# 2022

## TOC

- [2022-01](#2022-01)
- [2022-02](#2022-02)
- [2022-03](#2022-03)
- [2022-04](#2022-04)
- [2022-05](#2022-05)
- [2022-06](#2022-06)
- [2022-07](#2022-07)
- [2022-08](#2022-08)
- [2022-09](#2022-09)
- [2022-10](#2022-10)
- [2022-11](#2022-11)
- [2022-12](#2022-12)

## 2022-01

<details>

<summary>2022-01-01 05:13:43 - Usability and Aesthetics: Better Together for Automated Repair of Web Pages</summary>

- *Thanh Le-Cong, Xuan Bach D. Le, Quyet-Thang Huynh, Phi-Le Nguyen*

- `2201.00117v1` - [abs](http://arxiv.org/abs/2201.00117v1) - [pdf](http://arxiv.org/pdf/2201.00117v1)

> With the recent explosive growth of mobile devices such as smartphones or tablets, guaranteeing consistent web appearance across all environments has become a significant problem. This happens simply because it is hard to keep track of the web appearance on different sizes and types of devices that render the web pages. Therefore, fixing the inconsistent appearance of web pages can be difficult, and the cost incurred can be huge, e.g., poor user experience and financial loss due to it. Recently, automated web repair techniques have been proposed to automatically resolve inconsistent web page appearance, focusing on improving usability. However, generated patches tend to disrupt the webpage's layout, rendering the repaired webpage aesthetically unpleasing, e.g., distorted images or misalignment of components.   In this paper, we propose an automated repair approach for web pages based on meta-heuristic algorithms that can assure both usability and aesthetics. The key novelty that empowers our approach is a novel fitness function that allows us to optimistically evolve buggy web pages to find the best solution that optimizes both usability and aesthetics at the same time. Empirical evaluations show that our approach is able to successfully resolve mobile-friendly problems in 94% of the evaluation subjects, significantly outperforming state-of-the-art baseline techniques in terms of both usability and aesthetics.

</details>

<details>

<summary>2022-01-01 16:15:34 - Leaky Frontends: Security Vulnerabilities in Processor Frontends</summary>

- *Shuwen Deng, Bowen Huang, Jakub Szefer*

- `2105.12224v3` - [abs](http://arxiv.org/abs/2105.12224v3) - [pdf](http://arxiv.org/pdf/2105.12224v3)

> This paper evaluates new security threats due to the processor frontend in modern Intel processors. The root causes of the security threats are the multiple paths in the processor frontend that the micro-operations can take: through the Micro-Instruction Translation Engine (MITE), through the Decode Stream Buffer (DSB), also called the Micro-operation Cache, or through the Loop Stream Detector (LSD). Each path has its own unique timing and power signatures, which lead to the side- and covert-channel attacks presented in this work. Especially, the switching between the different paths leads to observable timing or power differences which, as this work demonstrates, could be exploited by attackers. Because of the different paths, the switching, and way the components are shared in the frontend between hardware threads, two separate threads are able to be mutually influenced and timing or power can reveal activity on the other thread. The security threats are not limited to multi-threading, and this work further demonstrates new ways for leaking execution information about SGX enclaves or a new in-domain Spectre variant in single-thread setting. Finally, this work demonstrates a new method for fingerprinting the microcode patches of the processor by analyzing the behavior of different paths in the frontend. The findings of this work highlight the security threats associated with the processor frontend and the need for deployment of defenses for the modern processor frontend.

</details>

<details>

<summary>2022-01-01 18:32:07 - NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning</summary>

- *Buyun Liang, Tim Mitchell, Ju Sun*

- `2111.13984v2` - [abs](http://arxiv.org/abs/2111.13984v2) - [pdf](http://arxiv.org/pdf/2111.13984v2)

> Optimizing nonconvex (NCVX) problems, especially nonsmooth and constrained ones, is an essential part of machine learning. However, it can be hard to reliably solve such problems without optimization expertise. Existing general-purpose NCVX optimization packages are powerful but typically cannot handle nonsmoothness. GRANSO is among the first optimization solvers targeting general nonsmooth NCVX problems with nonsmooth constraints, but, as it is implemented in MATLAB and requires the user to provide analytical gradients, GRANSO is often not a convenient choice in machine learning (especially deep learning) applications. To greatly lower the technical barrier, we introduce a new software package called NCVX, whose initial release contains the solver PyGRANSO, a PyTorch-enabled port of GRANSO incorporating auto-differentiation, GPU acceleration, tensor input, and support for new QP solvers. NCVX is built on freely available and widely used open-source frameworks, and as a highlight, can solve general constrained deep learning problems, the first of its kind. NCVX is available at https://ncvx.org, with detailed documentation and numerous examples from machine learning and other fields.

</details>

<details>

<summary>2022-01-02 08:06:55 - Towards Transferable Adversarial Attacks on Vision Transformers</summary>

- *Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, Yu-Gang Jiang*

- `2109.04176v3` - [abs](http://arxiv.org/abs/2109.04176v3) - [pdf](http://arxiv.org/pdf/2109.04176v3)

> Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. % crafted in a similar fashion as CNNs. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance. Code is available at https://github.com/zhipeng-wei/PNA-PatchOut.

</details>

<details>

<summary>2022-01-03 16:10:36 - Exception-Driven Fault Localization for Automated Program Repair</summary>

- *Davide Ginelli, Oliviero Riganelli, Daniela Micucci, Leonardo Mariani*

- `2201.00736v1` - [abs](http://arxiv.org/abs/2201.00736v1) - [pdf](http://arxiv.org/pdf/2201.00736v1)

> Automated Program Repair (APR) techniques typically exploit spectrum-based fault localization (SBFL) to identify the program locations that should be patched, making the effectiveness of APR techniques dependent on the effectiveness of fault localization. Indeed, results show that SBFL often does not localize faults accurately, hindering the effectiveness of APR. In this paper, we propose EXCEPT, a technique that addresses the localization problem by focusing on the semantics of failures rather than on the correlation between the executed statements and the failed tests, as SBFL does. We focus on failures due to exceptions and we exploit their type and source to localize and guess the faults. Experiments with 43 exception-raising faults from the Defects4J benchmark show that EXCEPT can perform better than Ochiai and ssFix.

</details>

<details>

<summary>2022-01-03 18:53:23 - Robust Contrastive Learning Using Negative Samples with Diminished Semantics</summary>

- *Songwei Ge, Shlok Mishra, Haohan Wang, Chun-Liang Li, David Jacobs*

- `2110.14189v2` - [abs](http://arxiv.org/abs/2110.14189v2) - [pdf](http://arxiv.org/pdf/2110.14189v2)

> Unsupervised learning has recently made exceptional progress because of the development of more effective contrastive learning methods. However, CNNs are prone to depend on low-level features that humans deem non-semantic. This dependency has been conjectured to induce a lack of robustness to image perturbations or domain shift. In this paper, we show that by generating carefully designed negative samples, contrastive learning can learn more robust representations with less dependence on such features. Contrastive learning utilizes positive pairs that preserve semantic information while perturbing superficial features in the training images. Similarly, we propose to generate negative samples in a reversed way, where only the superfluous instead of the semantic features are preserved. We develop two methods, texture-based and patch-based augmentations, to generate negative samples. These samples achieve better generalization, especially under out-of-domain settings. We also analyze our method and the generated texture-based samples, showing that texture features are indispensable in classifying particular ImageNet classes and especially finer classes. We also show that model bias favors texture and shape features differently under different test settings. Our code, trained models, and ImageNet-Texture dataset can be found at https://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.

</details>

<details>

<summary>2022-01-04 11:12:39 - Short Range Correlation Transformer for Occluded Person Re-Identification</summary>

- *Yunbin Zhao, Songhao Zhu, Dongsheng Wang, Zhiwei Liang*

- `2201.01090v1` - [abs](http://arxiv.org/abs/2201.01090v1) - [pdf](http://arxiv.org/pdf/2201.01090v1)

> Occluded person re-identification is one of the challenging areas of computer vision, which faces problems such as inefficient feature representation and low recognition accuracy. Convolutional neural network pays more attention to the extraction of local features, therefore it is difficult to extract features of occluded pedestrians and the effect is not so satisfied. Recently, vision transformer is introduced into the field of re-identification and achieves the most advanced results by constructing the relationship of global features between patch sequences. However, the performance of vision transformer in extracting local features is inferior to that of convolutional neural network. Therefore, we design a partial feature transformer-based person re-identification framework named PFT. The proposed PFT utilizes three modules to enhance the efficiency of vision transformer. (1) Patch full dimension enhancement module. We design a learnable tensor with the same size as patch sequences, which is full-dimensional and deeply embedded in patch sequences to enrich the diversity of training samples. (2) Fusion and reconstruction module. We extract the less important part of obtained patch sequences, and fuse them with original patch sequence to reconstruct the original patch sequences. (3) Spatial Slicing Module. We slice and group patch sequences from spatial direction, which can effectively improve the short-range correlation of patch sequences. Experimental results over occluded and holistic re-identification datasets demonstrate that the proposed PFT network achieves superior performance consistently and outperforms the state-of-the-art methods.

</details>

<details>

<summary>2022-01-04 18:09:13 - On the effectiveness of adversarial training against common corruptions</summary>

- *Klim Kireev, Maksym Andriushchenko, Nicolas Flammarion*

- `2103.02325v2` - [abs](http://arxiv.org/abs/2103.02325v2) - [pdf](http://arxiv.org/pdf/2103.02325v2)

> The literature on robustness towards common corruptions shows no consensus on whether adversarial training can improve the performance in this setting. First, we show that, when used with an appropriately selected perturbation radius, $\ell_p$ adversarial training can serve as a strong baseline against common corruptions improving both accuracy and calibration. Then we explain why adversarial training performs better than data augmentation with simple Gaussian noise which has been observed to be a meaningful baseline on common corruptions. Related to this, we identify the $\sigma$-overfitting phenomenon when Gaussian augmentation overfits to a particular standard deviation used for training which has a significant detrimental effect on common corruption accuracy. We discuss how to alleviate this problem and then how to further enhance $\ell_p$ adversarial training by introducing an efficient relaxation of adversarial training with learned perceptual image patch similarity as the distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the $\ell_p$ adversarial training baseline but also has cumulative gains with data augmentation methods such as AugMix, DeepAugment, ANT, and SIN, leading to state-of-the-art performance on common corruptions.   The code of our experiments is publicly available at https://github.com/tml-epfl/adv-training-corruptions.

</details>

<details>

<summary>2022-01-05 21:37:43 - Multiple Sclerosis Lesions Segmentation using Attention-Based CNNs in FLAIR Images</summary>

- *Mehdi SadeghiBakhi, Hamidreza Pourreza, Hamidreza Mahyar*

- `2201.01832v1` - [abs](http://arxiv.org/abs/2201.01832v1) - [pdf](http://arxiv.org/pdf/2201.01832v1)

> Objective: Multiple Sclerosis (MS) is an autoimmune, and demyelinating disease that leads to lesions in the central nervous system. This disease can be tracked and diagnosed using Magnetic Resonance Imaging (MRI). Up to now a multitude of multimodality automatic biomedical approaches is used to segment lesions which are not beneficial for patients in terms of cost, time, and usability. The authors of the present paper propose a method employing just one modality (FLAIR image) to segment MS lesions accurately. Methods: A patch-based Convolutional Neural Network (CNN) is designed, inspired by 3D-ResNet and spatial-channel attention module, to segment MS lesions. The proposed method consists of three stages: (1) the contrast-limited adaptive histogram equalization (CLAHE) is applied to the original images and concatenated to the extracted edges in order to create 4D images; (2) the patches of size 80 * 80 * 80 * 2 are randomly selected from the 4D images; and (3) the extracted patches are passed into an attention-based CNN which is used to segment the lesions. Finally, the proposed method was compared to previous studies of the same dataset. Results: The current study evaluates the model, with a test set of ISIB challenge data. Experimental results illustrate that the proposed approach significantly surpasses existing methods in terms of Dice similarity and Absolute Volume Difference while the proposed method use just one modality (FLAIR) to segment the lesions. Conclusions: The authors have introduced an automated approach to segment the lesions which is based on, at most, two modalities as an input. The proposed architecture is composed of convolution, deconvolution, and an SCA-VoxRes module as an attention module. The results show, the proposed method outperforms well compare to other methods.

</details>

<details>

<summary>2022-01-05 22:33:43 - On the Real-World Adversarial Robustness of Real-Time Semantic Segmentation Models for Autonomous Driving</summary>

- *Giulio Rossolini, Federico Nesti, Gianluca D'Amico, Saasha Nair, Alessandro Biondi, Giorgio Buttazzo*

- `2201.01850v1` - [abs](http://arxiv.org/abs/2201.01850v1) - [pdf](http://arxiv.org/pdf/2201.01850v1)

> The existence of real-world adversarial examples (commonly in the form of patches) poses a serious threat for the use of deep learning models in safety-critical computer vision tasks such as visual perception in autonomous driving. This paper presents an extensive evaluation of the robustness of semantic segmentation models when attacked with different types of adversarial patches, including digital, simulated, and physical ones. A novel loss function is proposed to improve the capabilities of attackers in inducing a misclassification of pixels. Also, a novel attack strategy is presented to improve the Expectation Over Transformation method for placing a patch in the scene. Finally, a state-of-the-art method for detecting adversarial patch is first extended to cope with semantic segmentation models, then improved to obtain real-time performance, and eventually evaluated in real-world scenarios. Experimental results reveal that, even though the adversarial effect is visible with both digital and real-world attacks, its impact is often spatially confined to areas of the image around the patch. This opens to further questions about the spatial robustness of real-time semantic segmentation models.

</details>

<details>

<summary>2022-01-06 04:11:45 - ExSinGAN: Learning an Explainable Generative Model from a Single Image</summary>

- *ZiCheng Zhang, CongYing Han, TianDe Guo*

- `2105.07350v2` - [abs](http://arxiv.org/abs/2105.07350v2) - [pdf](http://arxiv.org/pdf/2105.07350v2)

> Generating images from a single sample, as a newly developing branch of image synthesis, has attracted extensive attention. In this paper, we formulate this problem as sampling from the conditional distribution of a single image, and propose a hierarchical framework that simplifies the learning of the intricate conditional distributions through the successive learning of the distributions about structure, semantics and texture, making the process of learning and generation comprehensible. On this basis, we design ExSinGAN composed of three cascaded GANs for learning an explainable generative model from a given image, where the cascaded GANs model the distributions about structure, semantics and texture successively. ExSinGAN is learned not only from the internal patches of the given image as the previous works did, but also from the external prior obtained by the GAN inversion technique. Benefiting from the appropriate combination of internal and external information, ExSinGAN has a more powerful capability of generation and competitive generalization ability for the image manipulation tasks compared with prior works.

</details>

<details>

<summary>2022-01-09 10:33:37 - Analyzing Thermal Buckling in Curvilinearly Stiffened Composite Plates with Arbitrary Shaped Cutouts Using Isogeometric Level Set Method</summary>

- *Balakrishnan Devarajan*

- `2104.05132v3` - [abs](http://arxiv.org/abs/2104.05132v3) - [pdf](http://arxiv.org/pdf/2104.05132v3)

> In this paper we develop a new simple and effective isogeometric analysis for modeling thermal buckling of stiffened laminated composite plates with cutouts using level sets. We employ a first order shear deformation theory to approximate the displacement field of the stiffeners and the plate. Numerical modeling with a treatment of trimmed objects, such as internal cutouts in terms of NURBS-based isogeometric analysis presents several challenges, primarily due to need for using the tensor product of the NURBS basis functions. Due to this feature, the refinement operations can only be performed globally on the domain and not locally around the cutout. The new approach can overcome the drawbacks in modeling complex geometries with multiple-patches as the level sets are used to describe the internal cutouts; while the numerical integration is used only inside the physical domain. Results of parametric studies are presented which show the influence of ply orientation, size and orientation of the cutout and the position and profile of the curvilinear stiffeners. The numerical examples show high reliability and efficiency of the present method compared with other published solutions and ABAQUS.

</details>

<details>

<summary>2022-01-09 16:55:22 - COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty</summary>

- *Masahiro Oda, Tong Zheng, Yuichiro Hayashi, Yoshito Otake, Masahiro Hashimoto, Toshiaki Akashi, Shigeki Aoki, Kensaku Mori*

- `2201.03053v1` - [abs](http://arxiv.org/abs/2201.03053v1) - [pdf](http://arxiv.org/pdf/2201.03053v1)

> This paper proposes a segmentation method of infection regions in the lung from CT volumes of COVID-19 patients. COVID-19 spread worldwide, causing many infected patients and deaths. CT image-based diagnosis of COVID-19 can provide quick and accurate diagnosis results. An automated segmentation method of infection regions in the lung provides a quantitative criterion for diagnosis. Previous methods employ whole 2D image or 3D volume-based processes. Infection regions have a considerable variation in their sizes. Such processes easily miss small infection regions. Patch-based process is effective for segmenting small targets. However, selecting the appropriate patch size is difficult in infection region segmentation. We utilize the scale uncertainty among various receptive field sizes of a segmentation FCN to obtain infection regions. The receptive field sizes can be defined as the patch size and the resolution of volumes where patches are clipped from. This paper proposes an infection segmentation network (ISNet) that performs patch-based segmentation and a scale uncertainty-aware prediction aggregation method that refines the segmentation result. We design ISNet to segment infection regions that have various intensity values. ISNet has multiple encoding paths to process patch volumes normalized by multiple intensity ranges. We collect prediction results generated by ISNets having various receptive field sizes. Scale uncertainty among the prediction results is extracted by the prediction aggregation method. We use an aggregation FCN to generate a refined segmentation result considering scale uncertainty among the predictions. In our experiments using 199 chest CT volumes of COVID-19 cases, the prediction aggregation method improved the dice similarity score from 47.6% to 62.1%.

</details>

<details>

<summary>2022-01-10 02:57:28 - D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation</summary>

- *Yixuan Wu, Kuanlun Liao, Jintai Chen, Jinhong Wang, Danny Z. Chen, Honghao Gao, Jian Wu*

- `2201.00462v2` - [abs](http://arxiv.org/abs/2201.00462v2) - [pdf](http://arxiv.org/pdf/2201.00462v2)

> Computer-aided medical image segmentation has been applied widely in diagnosis and treatment to obtain clinically useful information of shapes and volumes of target organs and tissues. In the past several years, convolutional neural network (CNN) based methods (e.g., U-Net) have dominated this area, but still suffered from inadequate long-range information capturing. Hence, recent work presented computer vision Transformer variants for medical image segmentation tasks and obtained promising performances. Such Transformers model long-range dependency by computing pair-wise patch relations. However, they incur prohibitive computational costs, especially on 3D medical images (e.g., CT and MRI). In this paper, we propose a new method called Dilated Transformer, which conducts self-attention for pair-wise patch relations captured alternately in local and global scopes. Inspired by dilated convolution kernels, we conduct the global self-attention in a dilated manner, enlarging receptive fields without increasing the patches involved and thus reducing computational costs. Based on this design of Dilated Transformer, we construct a U-shaped encoder-decoder hierarchical architecture called D-Former for 3D medical image segmentation. Experiments on the Synapse and ACDC datasets show that our D-Former model, trained from scratch, outperforms various competitive CNN-based or Transformer-based segmentation models at a low computational cost without time-consuming per-training process.

</details>

<details>

<summary>2022-01-10 12:38:22 - Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound</summary>

- *Daniel Wulff, Jannis Hagenah, Floris Ernst*

- `2201.03319v1` - [abs](http://arxiv.org/abs/2201.03319v1) - [pdf](http://arxiv.org/pdf/2201.03319v1)

> 3D ultrasound (3DUS) becomes more interesting for target tracking in radiation therapy due to its capability to provide volumetric images in real-time without using ionizing radiation. It is potentially usable for tracking without using fiducials. For this, a method for learning meaningful representations would be useful to recognize anatomical structures in different time frames in representation space (r-space). In this study, 3DUS patches are reduced into a 128-dimensional r-space using conventional autoencoder, variational autoencoder and sliced-wasserstein autoencoder. In the r-space, the capability of separating different ultrasound patches as well as recognizing similar patches is investigated and compared based on a dataset of liver images. Two metrics to evaluate the tracking capability in the r-space are proposed. It is shown that ultrasound patches with different anatomical structures can be distinguished and sets of similar patches can be clustered in r-space. The results indicate that the investigated autoencoders have different levels of usability for target tracking in 3DUS.

</details>

<details>

<summary>2022-01-11 07:24:37 - Asymptotic Optimality of the Greedy Patching Heuristic for Max TSP in Doubling Metrics</summary>

- *Vladimir Shenmaier*

- `2201.03813v1` - [abs](http://arxiv.org/abs/2201.03813v1) - [pdf](http://arxiv.org/pdf/2201.03813v1)

> The maximum traveling salesman problem (Max~TSP) consists of finding a Hamiltonian cycle with the maximum total weight of the edges in a given complete weighted graph. We prove that, in the case when the edge weights are induced by a metric space of bounded doubling dimension, asymptotically optimal solutions of the problem can be found by the simple greedy patching heuristic. Taking as a start point a maximum-weight cycle cover, this heuristic iteratively patches pairs of its cycles into one minimizing the weight loss at each step.

</details>

<details>

<summary>2022-01-11 11:47:13 - Reconstructing occluded Elevation Information in Terrain Maps with Self-supervised Learning</summary>

- *Maximilian StÃ¶lzle, Takahiro Miki, Levin Gerdes, Martin Azkarate, Marco Hutter*

- `2109.07150v2` - [abs](http://arxiv.org/abs/2109.07150v2) - [pdf](http://arxiv.org/pdf/2109.07150v2)

> Accurate and complete terrain maps enhance the awareness of autonomous robots and enable safe and optimal path planning. Rocks and topography often create occlusions and lead to missing elevation information in the Digital Elevation Map (DEM). Currently, these occluded areas are either fully avoided during motion planning or the missing values in the elevation map are filled-in using traditional interpolation, diffusion or patch-matching techniques. These methods cannot leverage the high-level terrain characteristics and the geometric constraints of line of sight we humans use intuitively to predict occluded areas. We introduce a self-supervised learning approach capable of training on real-world data without a need for ground-truth information to reconstruct the occluded areas in the DEMs. We accomplish this by adding artificial occlusion to the incomplete elevation maps constructed on a real robot by performing ray casting. We first evaluate a supervised learning approach on synthetic data for which we have the full ground-truth available and subsequently move to several real-world datasets. These real-world datasets were recorded during exploration of both structured and unstructured terrain with a legged robot, and additionally in a planetary scenario on Lunar analogue terrain. We state a significant improvement compared to the baseline methods both on synthetic terrain and for the real-world datasets. Our neural network is able to run in real-time on both CPU and GPU with suitable sampling rates for autonomous ground robots. We motivate the applicability of reconstructing occlusion in elevation maps with preliminary motion planning experiments.

</details>

<details>

<summary>2022-01-11 14:36:57 - Sorald: Automatic Patch Suggestions for SonarQube Static Analysis Violations</summary>

- *Khashayar Etemadi, Nicolas Harrand, Simon Larsen, Haris Adzemovic, Henry Luong Phu, Ashutosh Verma, Fernanda Madeiral, Douglas Wikstrom, Martin Monperrus*

- `2103.12033v2` - [abs](http://arxiv.org/abs/2103.12033v2) - [pdf](http://arxiv.org/pdf/2103.12033v2)

> Previous work has shown that early resolution of issues detected by static code analyzers can prevent major costs later on. However, developers often ignore such issues for two main reasons. First, many issues should be interpreted to determine if they correspond to actual flaws in the program. Second, static analyzers often do not present the issues in a way that is actionable. To address these problems, we present Sorald: a novel system that devise metaprogramming templates to transform the abstract syntax trees of programs and suggest fixes for static analysis warnings. Thus, the burden on the developer is reduced from interpreting and fixing static issues, to inspecting and approving full fledged solutions. Sorald fixes violations of 10 rules from SonarJava, one of the most widely used static analyzers for Java. We evaluate Sorald on a dataset of 161 popular repositories on Github. Our analysis shows the effectiveness of Sorald as it fixes 65% (852/1,307) of the violations that meets the repair preconditions. Overall, our experiments show it is possible to automatically fix notable violations of the static analysis rules produced by the state-of-the-art static analyzer SonarJava.

</details>

<details>

<summary>2022-01-12 05:24:29 - Robust Contrastive Learning against Noisy Views</summary>

- *Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelka, Yale Song*

- `2201.04309v1` - [abs](http://arxiv.org/abs/2201.04309v1) - [pdf](http://arxiv.org/pdf/2201.04309v1)

> Contrastive learning relies on an assumption that positive pairs contain related views, e.g., patches of an image or co-occurring multimodal signals of a video, that share certain underlying information about an instance. But what if this assumption is violated? The literature suggests that contrastive learning produces suboptimal representations in the presence of noisy views, e.g., false positive pairs with no apparent shared information. In this work, we propose a new contrastive loss function that is robust against noisy views. We provide rigorous theoretical justifications by showing connections to robust symmetric losses for noisy binary classification and by establishing a new contrastive bound for mutual information maximization based on the Wasserstein distance measure. The proposed loss is completely modality-agnostic and a simple drop-in replacement for the InfoNCE loss, which makes it easy to apply to existing contrastive frameworks. We show that our approach provides consistent improvements over the state-of-the-art on image, video, and graph contrastive learning benchmarks that exhibit a variety of real-world noise patterns.

</details>

<details>

<summary>2022-01-12 23:00:22 - Local2Global: A distributed approach for scaling representation learning on graphs</summary>

- *Lucas G. S. Jeub, Giovanni Colavizza, Xiaowen Dong, Marya Bazzi, Mihai Cucuringu*

- `2201.04729v1` - [abs](http://arxiv.org/abs/2201.04729v1) - [pdf](http://arxiv.org/pdf/2201.04729v1)

> We propose a decentralised "local2global"' approach to graph representation learning, that one can a-priori use to scale any embedding technique. Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or "patches") and training local representations for each patch independently. In a second step, we combine the local representations into a globally consistent representation by estimating the set of rigid motions that best align the local representations using information from the patch overlaps, via group synchronization. A key distinguishing feature of local2global relative to existing work is that patches are trained independently without the need for the often costly parameter synchronization during distributed training. This allows local2global to scale to large-scale industrial applications, where the input graph may not even fit into memory and may be stored in a distributed manner. We apply local2global on data sets of different sizes and show that our approach achieves a good trade-off between scale and accuracy on edge reconstruction and semi-supervised classification. We also consider the downstream task of anomaly detection and show how one can use local2global to highlight anomalies in cybersecurity networks.

</details>

<details>

<summary>2022-01-14 08:06:11 - Cross-token Modeling with Conditional Computation</summary>

- *Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, Yang You*

- `2109.02008v3` - [abs](http://arxiv.org/abs/2109.02008v3) - [pdf](http://arxiv.org/pdf/2109.02008v3)

> Mixture-of-Experts (MoE), a conditional computation architecture, achieved promising performance by scaling local module (i.e. feed-forward network) of transformer. However, scaling the cross-token module (i.e. self-attention) is challenging due to the unstable training. This work proposes Sparse-MLP, an all-MLP model which applies sparsely-activated MLPs to cross-token modeling. Specifically, in each Sparse block of our all-MLP model, we apply two stages of MoE layers: one with MLP experts mixing information within channels along image patch dimension, the other with MLP experts mixing information within patches along the channel dimension. In addition, by proposing importance-score routing strategy for MoE and redesigning the image representation shape, we further improve our model's computational efficiency. Experimentally, we are more computation-efficient than Vision Transformers with comparable accuracy. Also, our models can outperform MLP-Mixer by 2.5\% on ImageNet Top-1 accuracy with fewer parameters and computational cost. On downstream tasks, i.e. Cifar10 and Cifar100, our models can still achieve better performance than baselines.

</details>

<details>

<summary>2022-01-16 09:22:37 - An Investigation into Inconsistency of Software Vulnerability Severity across Data Sources</summary>

- *Roland Croft, M. Ali Babar, Li Li*

- `2112.10356v2` - [abs](http://arxiv.org/abs/2112.10356v2) - [pdf](http://arxiv.org/pdf/2112.10356v2)

> Software Vulnerability (SV) severity assessment is a vital task for informing SV remediation and triage. Ranking of SV severity scores is often used to advise prioritization of patching efforts. However, severity assessment is a difficult and subjective manual task that relies on expertise, knowledge, and standardized reporting schemes. Consequently, different data sources that perform independent analysis may provide conflicting severity rankings. Inconsistency across these data sources affects the reliability of severity assessment data, and can consequently impact SV prioritization and fixing. In this study, we investigate severity ranking inconsistencies over the SV reporting lifecycle. Our analysis helps characterize the nature of this problem, identify correlated factors, and determine the impacts of inconsistency on downstream tasks. Our findings observe that SV severity often lacks consideration or is underestimated during initial reporting, and such SVs consequently receive lower prioritization. We identify six potential attributes that are correlated to this misjudgment, and show that inconsistency in severity reporting schemes can severely degrade the performance of downstream severity prediction by up to 77%. Our findings help raise awareness of SV severity data inconsistencies and draw attention to this data quality problem. These insights can help developers better consider SV severity data sources, and improve the reliability of consequent SV prioritization. Furthermore, we encourage researchers to provide more attention to SV severity data selection.

</details>

<details>

<summary>2022-01-17 03:24:31 - Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems</summary>

- *Wei Jia, Zhaojun Lu, Haichun Zhang, Zhenglin Liu, Jie Wang, Gang Qu*

- `2201.06192v1` - [abs](http://arxiv.org/abs/2201.06192v1) - [pdf](http://arxiv.org/pdf/2201.06192v1)

> Adversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have received a lot of attention recently. However, majority of the research on AEs is in the digital domain and the adversarial patches are static, which is very different from many real-world DNN applications such as Traffic Sign Recognition (TSR) systems in autonomous vehicles. In TSR systems, object detectors use DNNs to process streaming video in real time. From the view of object detectors, the traffic sign`s position and quality of the video are continuously changing, rendering the digital AEs ineffective in the physical world.   In this paper, we propose a systematic pipeline to generate robust physical AEs against real-world object detectors. Robustness is achieved in three ways. First, we simulate the in-vehicle cameras by extending the distribution of image transformations with the blur transformation and the resolution transformation. Second, we design the single and multiple bounding boxes filters to improve the efficiency of the perturbation training. Third, we consider four representative attack vectors, namely Hiding Attack, Appearance Attack, Non-Target Attack and Target Attack.   We perform a comprehensive set of experiments under a variety of environmental conditions, and considering illuminations in sunny and cloudy weather as well as at night. The experimental results show that the physical AEs generated from our pipeline are effective and robust when attacking the YOLO v5 based TSR system. The attacks have good transferability and can deceive other state-of-the-art object detectors. We launched HA and NTA on a brand-new 2021 model vehicle. Both attacks are successful in fooling the TSR system, which could be a life-threatening case for autonomous vehicles. Finally, we discuss three defense mechanisms based on image preprocessing, AEs detection, and model enhancing.

</details>

<details>

<summary>2022-01-17 04:12:18 - Balancing Performance and Energy Consumption of Bagging Ensembles for the Classification of Data Streams in Edge Computing</summary>

- *Guilherme Cassales, Heitor Gomes, Albert Bifet, Bernhard Pfahringer, Hermes Senger*

- `2201.06205v1` - [abs](http://arxiv.org/abs/2201.06205v1) - [pdf](http://arxiv.org/pdf/2201.06205v1)

> In recent years, the Edge Computing (EC) paradigm has emerged as an enabling factor for developing technologies like the Internet of Things (IoT) and 5G networks, bridging the gap between Cloud Computing services and end-users, supporting low latency, mobility, and location awareness to delay-sensitive applications. Most solutions in EC employ machine learning (ML) methods to perform data classification and other information processing tasks on continuous and evolving data streams. Usually, such solutions have to cope with vast amounts of data that come as data streams while balancing energy consumption, latency, and the predictive performance of the algorithms. Ensemble methods achieve remarkable predictive performance when applied to evolving data streams due to the combination of several models and the possibility of selective resets. This work investigates strategies for optimizing the performance (i.e., delay, throughput) and energy consumption of bagging ensembles to classify data streams. The experimental evaluation involved six state-of-art ensemble algorithms (OzaBag, OzaBag Adaptive Size Hoeffding Tree, Online Bagging ADWIN, Leveraging Bagging, Adaptive RandomForest, and Streaming Random Patches) applying five widely used machine learning benchmark datasets with varied characteristics on three computer platforms. Such strategies can significantly reduce energy consumption in 96% of the experimental scenarios evaluated. Despite the trade-offs, it is possible to balance them to avoid significant loss in predictive performance.

</details>

<details>

<summary>2022-01-20 12:21:15 - S-Extension Patch: A simple and efficient way to extend an object detection model</summary>

- *Dishant Parikh*

- `2110.02670v2` - [abs](http://arxiv.org/abs/2110.02670v2) - [pdf](http://arxiv.org/pdf/2110.02670v2)

> While building convolutional network-based systems, the toll it takes to train the network is something that cannot be ignored. In cases where we need to append additional capabilities to the existing model, the attention immediately goes towards retraining techniques. In this paper, I show how to leverage knowledge about the dataset to append the class faster while maintaining the speed of inference as well as the accuracies; while reducing the amount of time and data required. The method can extend a class in the existing object detection model in 1/10th of the time compared to the other existing methods. S-Extension patch not only offers faster training but also speed and ease of adaptation, as it can be appended to any existing system, given it fulfills the similarity threshold condition.

</details>

<details>

<summary>2022-01-20 22:02:08 - Learning with Less Labels in Digital Pathology via Scribble Supervision from Natural Images</summary>

- *Eu Wern Teh, Graham W. Taylor*

- `2201.02627v2` - [abs](http://arxiv.org/abs/2201.02627v2) - [pdf](http://arxiv.org/pdf/2201.02627v2)

> A critical challenge of training deep learning models in the Digital Pathology (DP) domain is the high annotation cost by medical experts. One way to tackle this issue is via transfer learning from the natural image domain (NI), where the annotation cost is considerably cheaper. Cross-domain transfer learning from NI to DP is shown to be successful via class labels. One potential weakness of relying on class labels is the lack of spatial information, which can be obtained from spatial labels such as full pixel-wise segmentation labels and scribble labels. We demonstrate that scribble labels from NI domain can boost the performance of DP models on two cancer classification datasets (Patch Camelyon Breast Cancer and Colorectal Cancer dataset). Furthermore, we show that models trained with scribble labels yield the same performance boost as full pixel-wise segmentation labels despite being significantly easier and faster to collect.

</details>

<details>

<summary>2022-01-21 12:58:53 - Attack of the Clones: Measuring the Maintainability, Originality and Security of Bitcoin 'Forks' in the Wild</summary>

- *Jusop Choi, Wonseok Choi, William Aiken, Hyoungshick Kim, Jun Ho Huh, Taesoo Kim, Yongdae Kim, Ross Anderson*

- `2201.08678v1` - [abs](http://arxiv.org/abs/2201.08678v1) - [pdf](http://arxiv.org/pdf/2201.08678v1)

> Since Bitcoin appeared in 2009, over 6,000 different cryptocurrency projects have followed. The cryptocurrency world may be the only technology where a massive number of competitors offer similar services yet claim unique benefits, including scalability, fast transactions, and security. But are these projects really offering unique features and significant enhancements over their competitors? To answer this question, we conducted a large-scale empirical analysis of code maintenance activities, originality and security across 592 crypto projects. We found that about half of these projects have not been updated for the last six months; over two years, about three-quarters of them disappeared, or were reported as scams or inactive. We also investigated whether 11 security vulnerabilities patched in Bitcoin were also patched in other projects. We found that about 80% of 510 C-language-based cryptocurrency projects have at least one unpatched vulnerability, and the mean time taken to fix the vulnerability is 237.8 days. Among those 510 altcoins, we found that at least 157 altcoins are likely to have been forked from Bitcoin, about a third of them containing only slight changes from the Bitcoin version from which they were forked. As case studies, we did a deep dive into 20 altcoins (e.g., Litecoin, FujiCoin, and Feathercoin) similar to the version of Bitcoin used for the fork. About half of them did not make any technically meaningful change - failing to comply with the promises (e.g., about using Proof of Stake) made in their whitepapers.

</details>

<details>

<summary>2022-01-21 16:39:11 - GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses</summary>

- *Wei Ma, Mengjie Zhao, Ezekiel Soremekun, Qiang Hu, Jie Zhang, Mike Papadakis, Maxime Cordy, Xiaofei Xie, Yves Le Traon*

- `2112.01218v2` - [abs](http://arxiv.org/abs/2112.01218v2) - [pdf](http://arxiv.org/pdf/2112.01218v2)

> Code embedding is a keystone in the application of machine learning on several Software Engineering (SE) tasks. To effectively support a plethora of SE tasks, the embedding needs to capture program syntax and semantics in a way that is generic. To this end, we propose the first self-supervised pre-training approach (called GraphCode2Vec) which produces task-agnostic embedding of lexical and program dependence features. GraphCode2Vec achieves this via a synergistic combination of code analysis and Graph Neural Networks. GraphCode2Vec is generic, it allows pre-training, and it is applicable to several SE downstream tasks. We evaluate the effectiveness of GraphCode2Vec on four (4) tasks (method name prediction, solution classification, mutation testing and overfitted patch classification), and compare it with four (4) similarly generic code embedding baselines (Code2Seq, Code2Vec, CodeBERT, GraphCodeBERT) and 7 task-specific, learning-based methods. In particular, GraphCode2Vec is more effective than both generic and task-specific learning-based baselines. It is also complementary and comparable to GraphCodeBERT (a larger and more complex model). We also demonstrate through a probing and ablation study that GraphCode2Vec learns lexical and program dependence features and that self-supervised pre-training improves effectiveness.

</details>

<details>

<summary>2022-01-21 21:32:34 - Effective Learning of a GMRF Mixture Model</summary>

- *Shahaf E. Finder, Eran Treister, Oren Freifeld*

- `2005.09030v3` - [abs](http://arxiv.org/abs/2005.09030v3) - [pdf](http://arxiv.org/pdf/2005.09030v3)

> Learning a Gaussian Mixture Model (GMM) is hard when the number of parameters is too large given the amount of available data. As a remedy, we propose restricting the GMM to a Gaussian Markov Random Field Mixture Model (GMRF-MM), as well as a new method for estimating the latter's sparse precision (i.e., inverse covariance) matrices. When the sparsity pattern of each matrix is known, we propose an efficient optimization method for the Maximum Likelihood Estimate (MLE) of that matrix. When it is unknown, we utilize the popular Graphical Least Absolute Shrinkage and Selection Operator (GLASSO) to estimate that pattern. However, we show that even for a single Gaussian, when GLASSO is tuned to successfully estimate the sparsity pattern, it does so at the price of a substantial bias of the values of the nonzero entries of the matrix, and we show that this problem only worsens in a mixture setting. To overcome this, we discard the nonzero values estimated by GLASSO, keep only its pattern estimate and use it within the proposed MLE method. This yields an effective two-step procedure that removes the bias. We show that our "debiasing" approach outperforms GLASSO in both the single-GMRF and the GMRF-MM cases. We also show that when learning priors for image patches, our method outperforms GLASSO even if we merely use an educated guess about the sparsity pattern, and that our GMRF-MM outperforms the baseline GMM on real and synthetic high-dimensional datasets.

</details>

<details>

<summary>2022-01-22 10:06:21 - A Direct Slip Ratio Estimation Method based on an Intelligent Tire and Machine Learning</summary>

- *Nan Xu, Zepeng Tang, Hassan Askari, Jianfeng Zhou, Amir Khajepour*

- `2106.08961v3` - [abs](http://arxiv.org/abs/2106.08961v3) - [pdf](http://arxiv.org/pdf/2106.08961v3)

> Accurate estimation of the tire slip ratio is critical for vehicle safety, as it is necessary for vehicle control purposes. In this paper, an intelligent tire system is presented to develop a novel slip ratio estimation model using machine learning algorithms. The accelerations, generated by a triaxial accelerometer installed onto the inner liner of the tire, are varied when the tire rotates to update the contact patch. Meanwhile, the slip ratio reference value can be measured by the MTS Flat-Trac tire test platform. Then, by analyzing the variation between the accelerations and slip ratio, highly useful features are discovered, which are especially promising for assessing vertical acceleration. For these features, machine learning (ML) algorithms are trained to build the slip ratio estimation model, in which the ML algorithms include artificial neural networks (ANNs), gradient boosting machines (GBMs), random forests (RFs), and support vector machines (SVMs). Finally, the estimated NRMS errors are evaluated using 10-fold cross-validation (CV). The proposed estimation model is able to estimate the slip ratio continuously and stably using only the acceleration from the intelligent tire system, and the estimated slip ratio range can reach 30%. The estimation results have high robustness to vehicle velocity and load, where the best NRMS errors can reach 4.88%. In summary, the present study with the fusion of an intelligent tire system and machine learning paves the way for the accurate estimation of the tire slip ratio under different driving conditions, which create new opportunities for autonomous vehicles, intelligent tires, and tire slip ratio estimation.

</details>

<details>

<summary>2022-01-24 10:52:43 - PickNet: Real-Time Channel Selection for Ad Hoc Microphone Arrays</summary>

- *Takuya Yoshioka, Xiaofei Wang, Dongmei Wang*

- `2201.09586v1` - [abs](http://arxiv.org/abs/2201.09586v1) - [pdf](http://arxiv.org/pdf/2201.09586v1)

> This paper proposes PickNet, a neural network model for real-time channel selection for an ad hoc microphone array consisting of multiple recording devices like cell phones. Assuming at most one person to be vocally active at each time point, PickNet identifies the device that is spatially closest to the active person for each time frame by using a short spectral patch of just hundreds of milliseconds. The model is applied to every time frame, and the short time frame signals from the selected microphones are concatenated across the frames to produce an output signal. As the personal devices are usually held close to their owners, the output signal is expected to have higher signal-to-noise and direct-to-reverberation ratios on average than the input signals. Since PickNet utilizes only limited acoustic context at each time frame, the system using the proposed model works in real time and is robust to changes in acoustic conditions. Speech recognition-based evaluation was carried out by using real conversational recordings obtained with various smartphones. The proposed model yielded significant gains in word error rate with limited computational cost over systems using a block-online beamformer and a single distant microphone.

</details>

<details>

<summary>2022-01-24 16:42:56 - Patches Are All You Need?</summary>

- *Asher Trockman, J. Zico Kolter*

- `2201.09792v1` - [abs](http://arxiv.org/abs/2201.09792v1) - [pdf](http://arxiv.org/pdf/2201.09792v1)

> Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.

</details>

<details>

<summary>2022-01-24 19:32:20 - Challenges Porting a C++ Template-Metaprogramming Abstraction Layer to Directive-based Offloading</summary>

- *Jeffrey Kelling, Sergei Bastrakov, Alexander Debus, Thomas Kluge, Matt Leinhauser, Richard Pausch, Klaus Steiniger, Jan Stephan, RenÃ© Widera, Jeff Young, Michael Bussmann, Sunita Chandrasekaran, Guido Juckeland*

- `2110.08650v2` - [abs](http://arxiv.org/abs/2110.08650v2) - [pdf](http://arxiv.org/pdf/2110.08650v2)

> HPC systems employ a growing variety of compute accelerators with different architectures and from different vendors. Large scientific applications are required to run efficiently across these systems but need to retain a single code-base in order to not stifle development. Directive-based offloading programming models set out to provide the required portability, but, to existing codes, they themselves represent yet another API to port to. Here, we present our approach of porting the GPU-accelerated particle-in-cell code PIConGPU to OpenACC and OpenMP target by adding two new backends to its existing C++-template metaprogramming-based offloading abstraction layer alpaka and avoiding other modifications to the application code. We introduce our approach in the face of conflicts between requirements and available features in the standards as well as practical hurdles posed by immature compiler support.

</details>

<details>

<summary>2022-01-25 07:20:47 - Leveraging Structural Properties of Source Code Graphs for Just-In-Time Bug Prediction</summary>

- *Md Nadim, Debajyoti Mondal, Chanchal K. Roy*

- `2201.10137v1` - [abs](http://arxiv.org/abs/2201.10137v1) - [pdf](http://arxiv.org/pdf/2201.10137v1)

> The most common use of data visualization is to minimize the complexity for proper understanding. A graph is one of the most commonly used representations for understanding relational data. It produces a simplified representation of data that is challenging to comprehend if kept in a textual format. In this study, we propose a methodology to utilize the relational properties of source code in the form of a graph to identify Just-in-Time (JIT) bug prediction in software systems during different revisions of software evolution and maintenance. We presented a method to convert the source codes of commit patches to equivalent graph representations and named it Source Code Graph (SCG). To understand and compare multiple source code graphs, we extracted several structural properties of these graphs, such as the density, number of cycles, nodes, edges, etc. We then utilized the attribute values of those SCGs to visualize and detect buggy software commits. We process more than 246K software commits from 12 subject systems in this investigation. Our investigation on these 12 open-source software projects written in C++ and Java programming languages shows that if we combine the features from SCG with conventional features used in similar studies, we will get the increased performance of Machine Learning (ML) based buggy commit detection models. We also find the increase of F1~Scores in predicting buggy and non-buggy commits statistically significant using the Wilcoxon Signed Rank Test. Since SCG-based feature values represent the style or structural properties of source code updates or changes in the software system, it suggests the importance of careful maintenance of source code style or structure for keeping a software system bug-free.

</details>

<details>

<summary>2022-01-25 14:20:52 - Sparse bottleneck neural networks for exploratory non-linear visualization of Patch-seq data</summary>

- *Yves Bernaerts, Philipp Berens, Dmitry Kobak*

- `2006.10411v3` - [abs](http://arxiv.org/abs/2006.10411v3) - [pdf](http://arxiv.org/pdf/2006.10411v3)

> Patch-seq, a recently developed experimental technique, allows neuroscientists to obtain transcriptomic and electrophysiological information from the same neurons. Efficiently analyzing and visualizing such paired multivariate data in order to extract biologically meaningful interpretations has, however, remained a challenge. Here, we use sparse deep neural networks with and without a two-dimensional bottleneck to predict electrophysiological features from the transcriptomic ones using a group lasso penalty, yielding concise and biologically interpretable two-dimensional visualizations. In two large example data sets, this visualization reveals known neural classes and their marker genes without biological prior knowledge. We also demonstrate that our method is applicable to other kinds of multimodal data, such as paired transcriptomic and proteomic measurements provided by CITE-seq.

</details>

<details>

<summary>2022-01-25 15:19:30 - BLDNet: A Semi-supervised Change Detection Building Damage Framework using Graph Convolutional Networks and Urban Domain Knowledge</summary>

- *Ali Ismail, Mariette Awad*

- `2201.10389v1` - [abs](http://arxiv.org/abs/2201.10389v1) - [pdf](http://arxiv.org/pdf/2201.10389v1)

> Change detection is instrumental to localize damage and understand destruction in disaster informatics. While convolutional neural networks are at the core of recent change detection solutions, we present in this work, BLDNet, a novel graph formulation for building damage change detection and enable learning relationships and representations from both local patterns and non-stationary neighborhoods. More specifically, we use graph convolutional networks to efficiently learn these features in a semi-supervised framework with few annotated data. Additionally, BLDNet formulation allows for the injection of additional contextual building meta-features. We train and benchmark on the xBD dataset to validate the effectiveness of our approach. We also demonstrate on urban data from the 2020 Beirut Port Explosion that performance is improved by incorporating domain knowledge building meta-features.

</details>

<details>

<summary>2022-01-25 19:46:14 - The Unexplored Terrain of Compiler Warnings</summary>

- *Gunnar Kudrjavets, Aditya Kumar, Nachiappan Nagappan, Ayushi Rastogi*

- `2201.10599v1` - [abs](http://arxiv.org/abs/2201.10599v1) - [pdf](http://arxiv.org/pdf/2201.10599v1)

> The authors' industry experiences suggest that compiler warnings, a lightweight version of program analysis, are valuable early bug detection tools. Significant costs are associated with patches and security bulletins for issues that could have been avoided if compiler warnings were addressed. Yet, the industry's attitude towards compiler warnings is mixed. Practices range from silencing all compiler warnings to having a zero-tolerance policy as to any warnings. Current published data indicates that addressing compiler warnings early is beneficial. However, support for this value theory stems from grey literature or is anecdotal. Additional focused research is needed to truly assess the cost-benefit of addressing warnings.

</details>

<details>

<summary>2022-01-26 10:42:23 - A deep learning method based on patchwise training for reconstructing temperature field</summary>

- *Xingwen Peng, Xingchen Li, Zhiqiang Gong, Xiaoyu Zhao, Wen Yao*

- `2201.10860v1` - [abs](http://arxiv.org/abs/2201.10860v1) - [pdf](http://arxiv.org/pdf/2201.10860v1)

> Physical field reconstruction is highly desirable for the measurement and control of engineering systems. The reconstruction of the temperature field from limited observation plays a crucial role in thermal management for electronic equipment. Deep learning has been employed in physical field reconstruction, whereas the accurate estimation for the regions with large gradients is still diffcult. To solve the problem, this work proposes a novel deep learning method based on patchwise training to reconstruct the temperature field of electronic equipment accurately from limited observation. Firstly, the temperature field reconstruction (TFR) problem of the electronic equipment is modeled mathematically and transformed as an image-to-image regression task. Then a patchwise training and inference framework consisting of an adaptive UNet and a shallow multilayer perceptron (MLP) is developed to establish the mapping from the observation to the temperature field. The adaptive UNet is utilized to reconstruct the whole temperature field while the MLP is designed to predict the patches with large temperature gradients. Experiments employing finite element simulation data are conducted to demonstrate the accuracy of the proposed method. Furthermore, the generalization is evaluated by investigating cases under different heat source layouts, different power intensities, and different observation point locations. The maximum absolute errors of the reconstructed temperature field are less than 1K under the patchwise training approach.

</details>

<details>

<summary>2022-01-30 02:25:10 - Attention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ....</summary>

- *Prateek Verma*

- `2110.03183v5` - [abs](http://arxiv.org/abs/2110.03183v5) - [pdf](http://arxiv.org/pdf/2110.03183v5)

> This paper presents a way of doing large scale audio understanding without traditional state of the art neural architectures. Ever since the introduction of deep learning for understanding audio signals in the past decade, convolutional architectures have been able to achieve state of the art results surpassing traditional hand-crafted features. In the recent past, there has been a similar shift away from traditional convolutional and recurrent neural networks towards purely end-to-end Transformer architectures. We, in this work, explore an approach, based on Bag-of-Words model. Our approach does not have any convolutions, recurrence, attention, transformers or other approaches such as BERT. We utilize micro and macro level clustered vanilla embeddings, and use a MLP head for classification. We only use feed-forward encoder-decoder models to get the bottlenecks of spectral envelops, spectral patches and slices as well as multi-resolution spectra. A classification head (a feed-forward layer), similar to the approach in SimCLR is trained on a learned representation. Using simple codes learned on latent representations, we show how we surpass traditional convolutional neural network architectures, and come strikingly close to outperforming powerful Transformer architectures. This work hopefully would pave way for exciting advancements in the field of representation learning without massive, end-to-end neural architectures.

</details>

<details>

<summary>2022-01-31 17:57:44 - A Formal Model of Checked C</summary>

- *Liyi Li, Yiyun Liu, Deena L. Postol, Leonidas Lampropoulos, David Van Horn, Michael Hicks*

- `2201.13394v1` - [abs](http://arxiv.org/abs/2201.13394v1) - [pdf](http://arxiv.org/pdf/2201.13394v1)

> We present a formal model of Checked C, a dialect of C that aims to enforce spatial memory safety. Our model pays particular attention to the semantics of dynamically sized, potentially null-terminated arrays. We formalize this model in Coq, and prove that any spatial memory safety errors can be blamed on portions of the program labeled unchecked; this is a Checked C feature that supports incremental porting and backward compatibility. While our model's operational semantics uses annotated ("fat") pointers to enforce spatial safety, we show that such annotations can be safely erased: Using PLT Redex we formalize an executable version of our model and a compilation procedure from it to an untyped C-like language, and use randomized testing to validate that generated code faithfully simulates the original. Finally, we develop a custom random generator for well-typed and almost-well-typed terms in our Redex model, and use it to search for inconsistencies between our model and the Clang Checked C implementation. We find these steps to be a useful way to co-develop a language (Checked C is still in development) and a core model of it.

</details>


## 2022-02

<details>

<summary>2022-02-01 21:01:22 - Lagrangian Manifold Monte Carlo on Monge Patches</summary>

- *Marcelo Hartmann, Mark Girolami, Arto Klami*

- `2202.00755v1` - [abs](http://arxiv.org/abs/2202.00755v1) - [pdf](http://arxiv.org/pdf/2202.00755v1)

> The efficiency of Markov Chain Monte Carlo (MCMC) depends on how the underlying geometry of the problem is taken into account. For distributions with strongly varying curvature, Riemannian metrics help in efficient exploration of the target distribution. Unfortunately, they have significant computational overhead due to e.g. repeated inversion of the metric tensor, and current geometric MCMC methods using the Fisher information matrix to induce the manifold are in practice slow. We propose a new alternative Riemannian metric for MCMC, by embedding the target distribution into a higher-dimensional Euclidean space as a Monge patch and using the induced metric determined by direct geometric reasoning. Our metric only requires first-order gradient information and has fast inverse and determinants, and allows reducing the computational complexity of individual iterations from cubic to quadratic in the problem dimensionality. We demonstrate how Lagrangian Monte Carlo in this metric efficiently explores the target distributions.

</details>

<details>

<summary>2022-02-01 23:27:00 - Security Evaluation of Block-based Image Encryption for Vision Transformer against Jigsaw Puzzle Solver Attack</summary>

- *Tatsuya Chuman, Hitoshi Kiya*

- `2202.00806v1` - [abs](http://arxiv.org/abs/2202.00806v1) - [pdf](http://arxiv.org/pdf/2202.00806v1)

> The aim of this paper is to evaluate the security of a block-based image encryption for the vision transformer against jigsaw puzzle solver attacks. The vision transformer, a model for image classification based on the transformer architecture, is carried out by dividing an image into a grid of square patches. Some encryption schemes for the vision transformer have been proposed by applying block-based image encryption such as block scrambling and rotating to patches of the image. On the other hand, the security of encryption scheme for the vision transformer has never evaluated. In this paper, jigsaw puzzle solver attacks are utilized to evaluate the security of encrypted images by regarding the divided patches as pieces of a jigsaw puzzle. In experiments, an image is resized and divided into patches to apply block scrambling-based image encryption, and then the security of encrypted images for the vision transformer against jigsaw puzzle solver attacks is evaluated.

</details>

<details>

<summary>2022-02-02 12:31:05 - Gradient Variance Loss for Structure-Enhanced Image Super-Resolution</summary>

- *Lusine Abrahamyan, Anh Minh Truong, Wilfried Philips, Nikos Deligiannis*

- `2202.00997v1` - [abs](http://arxiv.org/abs/2202.00997v1) - [pdf](http://arxiv.org/pdf/2202.00997v1)

> Recent success in the field of single image super-resolution (SISR) is achieved by optimizing deep convolutional neural networks (CNNs) in the image space with the L1 or L2 loss. However, when trained with these loss functions, models usually fail to recover sharp edges present in the high-resolution (HR) images for the reason that the model tends to give a statistical average of potential HR solutions. During our research, we observe that gradient maps of images generated by the models trained with the L1 or L2 loss have significantly lower variance than the gradient maps of the original high-resolution images. In this work, we propose to alleviate the above issue by introducing a structure-enhancing loss function, coined Gradient Variance (GV) loss, and generate textures with perceptual-pleasant details. Specifically, during the training of the model, we extract patches from the gradient maps of the target and generated output, calculate the variance of each patch and form variance maps for these two images. Further, we minimize the distance between the computed variance maps to enforce the model to produce high variance gradient maps that will lead to the generation of high-resolution images with sharper edges. Experimental results show that the GV loss can significantly improve both Structure Similarity (SSIM) and peak signal-to-noise ratio (PSNR) performance of existing image super-resolution (SR) deep learning models.

</details>

<details>

<summary>2022-02-02 22:02:10 - Debug-Localize-Repair: A Symbiotic Construction for Heap Manipulations</summary>

- *Sahil Verma, Subhajit Roy*

- `2011.13396v2` - [abs](http://arxiv.org/abs/2011.13396v2) - [pdf](http://arxiv.org/pdf/2011.13396v2)

> We present Wolverine2, an integrated Debug-Localize-Repair environment for heap manipulating programs. Wolverine2 provides an interactive debugging environment: while concretely executing a program via on an interactive shell supporting common debugging facilities, Wolverine2 displays the abstract program states (as box-and-arrow diagrams) as a visual aid to the programmer, packages a novel, proof-directed repair algorithm to quickly synthesize the repair patches and a new bug localization algorithm to reduce the search space of repairs. Wolverine2 supports "hot-patching" of the generated patches to provide a seamless debugging environment, and also facilitates new debug-localize-repair possibilities: \textit{specification refinement} and \textit{checkpoint-based hopping}. We evaluate Wolverine2 on 6400 buggy programs (generated using automated fault injection) on a variety of data-structures like singly, doubly, and circular linked lists, AVL trees, Red-Black trees, Splay Trees and Binary Search Trees; Wolverine2 could repair all the buggy instances within realistic programmer wait-time (less than 5 sec in most cases). Wolverine2 could also repair more than 80\% of the 247 (buggy) student submissions where a reasonable attempt was made.

</details>

<details>

<summary>2022-02-03 09:57:22 - On the Utility of Marrying GIN and PMD for Improving Stack Overflow Code Snippets</summary>

- *Sherlock A. Licorish, Markus Wagner*

- `2202.01490v1` - [abs](http://arxiv.org/abs/2202.01490v1) - [pdf](http://arxiv.org/pdf/2202.01490v1)

> Software developers are increasingly dependent on question and answer portals and blogs for coding solutions. While such interfaces provide useful information, there are concerns that code hosted here is often incorrect, insecure or incomplete. Previous work indeed detected a range of faults in code provided on Stack Overflow through the use of static analysis. Static analysis may go a far way towards quickly establishing the health of software code available online. In addition, mechanisms that enable rapid automated program improvement may then enhance such code. Accordingly, we present this proof of concept. We use the PMD static analysis tool to detect performance faults for a sample of Stack Overflow Java code snippets, before performing mutations on these snippets using GIN. We then re-analyse the performance faults in these snippets after the GIN mutations. GIN's RandomSampler was used to perform 17,986 unique line and statement patches on 3,034 snippets where PMD violations were removed from 770 patched versions. Our outcomes indicate that static analysis techniques may be combined with automated program improvement methods to enhance publicly available code with very little resource requirements. We discuss our planned research agenda in this regard.

</details>

<details>

<summary>2022-02-03 10:37:09 - Sim2Real Object-Centric Keypoint Detection and Description</summary>

- *Chengliang Zhong, Chao Yang, Jinshan Qi, Fuchun Sun, Huaping Liu, Xiaodong Mu, Wenbing Huang*

- `2202.00448v2` - [abs](http://arxiv.org/abs/2202.00448v2) - [pdf](http://arxiv.org/pdf/2202.00448v2)

> Keypoint detection and description play a central role in computer vision. Most existing methods are in the form of scene-level prediction, without returning the object classes of different keypoints. In this paper, we propose the object-centric formulation, which, beyond the conventional setting, requires further identifying which object each interest point belongs to. With such fine-grained information, our framework enables more downstream potentials, such as object-level matching and pose estimation in a clustered environment. To get around the difficulty of label collection in the real world, we develop a sim2real contrastive learning mechanism that can generalize the model trained in simulation to real-world applications. The novelties of our training method are three-fold: (i) we integrate the uncertainty into the learning framework to improve feature description of hard cases, e.g., less-textured or symmetric patches; (ii) we decouple the object descriptor into two output branches -- intra-object salience and inter-object distinctness, resulting in a better pixel-wise description; (iii) we enforce cross-view semantic consistency for enhanced robustness in representation learning. Comprehensive experiments on image matching and 6D pose estimation verify the encouraging generalization ability of our method from simulation to reality. Particularly for 6D pose estimation, our method significantly outperforms typical unsupervised/sim2real methods, achieving a closer gap with the fully supervised counterpart. Additional results and videos can be found at https://zhongcl-thu.github.io/rock/

</details>

<details>

<summary>2022-02-03 15:11:00 - Estimating the Potential of Program Repair Search Spaces with Commit Analysis</summary>

- *Khashayar Etemadi, Niloofar Tarighat, Siddharth Yadav, Matias Martinez, Martin Monperrus*

- `2007.06986v2` - [abs](http://arxiv.org/abs/2007.06986v2) - [pdf](http://arxiv.org/pdf/2007.06986v2)

> The most natural method for evaluating program repair systems is to run them on bug datasets, such as Defects4J. Yet, using this evaluation technique on arbitrary real-world programs requires heavy configuration. In this paper, we propose a purely static method to evaluate the potential of the search space of repair approaches. This new method enables researchers and practitioners to encode the search spaces of repair approaches and select potentially useful ones without struggling with tool configuration and execution. We encode the search spaces by specifying the repair strategies they employ. Next, we use the specifications to check whether past commits lie in repair search spaces. For a repair approach, including many human-written past commits in its search space indicates its potential to generate useful patches. We implement our evaluation method in LighteR. LighteR gets a Git repository and outputs a list of commits whose source code changes lie in repair search spaces. We run LighteR on 55,309 commits from the history of 72 Github repositories with and show that LighteR's precision and recall are 77% and 92%, respectively. Overall, our experiments show that our novel method is both lightweight and effective to study the search space of program repair approaches.

</details>

<details>

<summary>2022-02-04 08:36:34 - Image-to-Image MLP-mixer for Image Reconstruction</summary>

- *Youssef Mansour, Kang Lin, Reinhard Heckel*

- `2202.02018v1` - [abs](http://arxiv.org/abs/2202.02018v1) - [pdf](http://arxiv.org/pdf/2202.02018v1)

> Neural networks are highly effective tools for image reconstruction problems such as denoising and compressive sensing. To date, neural networks for image reconstruction are almost exclusively convolutional. The most popular architecture is the U-Net, a convolutional network with a multi-resolution architecture. In this work, we show that a simple network based on the multi-layer perceptron (MLP)-mixer enables state-of-the art image reconstruction performance without convolutions and without a multi-resolution architecture, provided that the training set and the size of the network are moderately large. Similar to the original MLP-mixer, the image-to-image MLP-mixer is based exclusively on MLPs operating on linearly-transformed image patches. Contrary to the original MLP-mixer, we incorporate structure by retaining the relative positions of the image patches. This imposes an inductive bias towards natural images which enables the image-to-image MLP-mixer to learn to denoise images based on fewer examples than the original MLP-mixer. Moreover, the image-to-image MLP-mixer requires fewer parameters to achieve the same denoising performance than the U-Net and its parameters scale linearly in the image resolution instead of quadratically as for the original MLP-mixer. If trained on a moderate amount of examples for denoising, the image-to-image MLP-mixer outperforms the U-Net by a slight margin. It also outperforms the vision transformer tailored for image reconstruction and classical un-trained methods such as BM3D, making it a very effective tool for image reconstruction problems.

</details>

<details>

<summary>2022-02-04 18:14:39 - Towards Training Reproducible Deep Learning Models</summary>

- *Boyuan Chen, Mingzhi Wen, Yong Shi, Dayi Lin, Gopi Krishnan Rajbahadur, Zhen Ming, Jiang*

- `2202.02326v1` - [abs](http://arxiv.org/abs/2202.02326v1) - [pdf](http://arxiv.org/pdf/2202.02326v1)

> Reproducibility is an increasing concern in Artificial Intelligence (AI), particularly in the area of Deep Learning (DL). Being able to reproduce DL models is crucial for AI-based systems, as it is closely tied to various tasks like training, testing, debugging, and auditing. However, DL models are challenging to be reproduced due to issues like randomness in the software (e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There are various practices to mitigate some of the aforementioned issues. However, many of them are either too intrusive or can only work for a specific usage context. In this paper, we propose a systematic approach to training reproducible DL models. Our approach includes three main parts: (1) a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains, (2) a unified framework which leverages a record-and-replay technique to mitigate software-related randomness and a profile-and-patch technique to control hardware-related non-determinism, and (3) a reproducibility guideline which explains the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Case study results show our approach can successfully reproduce six open source and one commercial DL models.

</details>

<details>

<summary>2022-02-07 20:15:30 - DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning</summary>

- *E. Kloberdanz, K. G. Kloberdanz, W. Le*

- `2202.03493v1` - [abs](http://arxiv.org/abs/2202.03493v1) - [pdf](http://arxiv.org/pdf/2202.03493v1)

> Deep learning (DL) has become an integral part of solutions to various important problems, which is why ensuring the quality of DL systems is essential. One of the challenges of achieving reliability and robustness of DL software is to ensure that algorithm implementations are numerically stable. DL algorithms require a large amount and a wide variety of numerical computations. A naive implementation of numerical computation can lead to errors that may result in incorrect or inaccurate learning and results. A numerical algorithm or a mathematical formula can have several implementations that are mathematically equivalent, but have different numerical stability properties. Designing numerically stable algorithm implementations is challenging, because it requires an interdisciplinary knowledge of software engineering, DL, and numerical analysis. In this paper, we study two mature DL libraries PyTorch and Tensorflow with the goal of identifying unstable numerical methods and their solutions. Specifically, we investigate which DL algorithms are numerically unstable and conduct an in-depth analysis of the root cause, manifestation, and patches to numerical instabilities. Based on these findings, we launch, the first database of numerical stability issues and solutions in DL. Our findings and provide future references to developers and tool builders to prevent, detect, localize and fix numerically unstable algorithm implementations. To demonstrate that, using {\it DeepStability} we have located numerical stability issues in Tensorflow, and submitted a fix which has been accepted and merged in.

</details>

<details>

<summary>2022-02-08 00:32:43 - Sparse-RS: a versatile framework for query-efficient sparse black-box adversarial attacks</summary>

- *Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas Flammarion, Matthias Hein*

- `2006.12834v3` - [abs](http://arxiv.org/abs/2006.12834v3) - [pdf](http://arxiv.org/pdf/2006.12834v3)

> We propose a versatile framework based on random search, Sparse-RS, for score-based sparse targeted and untargeted attacks in the black-box setting. Sparse-RS does not rely on substitute models and achieves state-of-the-art success rate and query efficiency for multiple sparse attack models: $l_0$-bounded perturbations, adversarial patches, and adversarial frames. The $l_0$-version of untargeted Sparse-RS outperforms all black-box and even all white-box attacks for different models on MNIST, CIFAR-10, and ImageNet. Moreover, our untargeted Sparse-RS achieves very high success rates even for the challenging settings of $20\times20$ adversarial patches and $2$-pixel wide adversarial frames for $224\times224$ images. Finally, we show that Sparse-RS can be applied to generate targeted universal adversarial patches where it significantly outperforms the existing approaches. The code of our framework is available at https://github.com/fra31/sparse-rs.

</details>

<details>

<summary>2022-02-09 18:33:57 - How to Understand Masked Autoencoders</summary>

- *Shuhao Cao, Peng Xu, David A. Clifton*

- `2202.03670v2` - [abs](http://arxiv.org/abs/2202.03670v2) - [pdf](http://arxiv.org/pdf/2202.03670v2)

> "Masked Autoencoders (MAE) Are Scalable Vision Learners" revolutionizes the self-supervised learning method in that it not only achieves the state-of-the-art for image pre-training, but is also a milestone that bridges the gap between visual and linguistic masked autoencoding (BERT-style) pre-trainings. However, to our knowledge, to date there are no theoretical perspectives to explain the powerful expressivity of MAE. In this paper, we, for the first time, propose a unified theoretical framework that provides a mathematical understanding for MAE. Specifically, we explain the patch-based attention approaches of MAE using an integral kernel under a non-overlapping domain decomposition setting. To help the research community to further comprehend the main reasons of the great success of MAE, based on our framework, we pose five questions and answer them with mathematical rigor using insights from operator theory.

</details>

<details>

<summary>2022-02-09 23:05:48 - Providing Real-time Assistance for Repairing Runtime Exceptions using Stack Overflow Posts</summary>

- *Sonal Mahajan, Mukul R. Prasad*

- `2202.04762v1` - [abs](http://arxiv.org/abs/2202.04762v1) - [pdf](http://arxiv.org/pdf/2202.04762v1)

> Runtime Exceptions (REs) are an important class of bugs that occur frequently during code development. Traditional Automatic Program Repair (APR) tools are of limited use in this "in-development" use case, since they require a test-suite to be available as a patching oracle. Thus, developers typically tend to manually resolve their in-development REs, often by referring to technical forums, such as Stack Overflow (SO). To automate this manual process we extend our previous work, MAESTRO, to provide real-time assistance to developers for repairing Java REs by recommending a relevant patch-suggesting SO post and synthesizing a repair patch from this post to fix the RE in the developer's code. MAESTRO exploits a library of Runtime Exception Patterns (REPs) semi-automatically mined from SO posts, through a relatively inexpensive, one-time, incremental process. An REP is an abstracted sequence of statements that triggers a given RE. REPs are used to index SO posts, retrieve a post most relevant to the RE instance exhibited by a developer's code and then mediate the process of extracting a concrete repair from the SO post, abstracting out post-specific details, and concretizing the repair to the developer's buggy code. We evaluate MAESTRO on a published RE benchmark comprised of 78 instances. MAESTRO is able to generate a correct repair patch at the top position in 27% of the cases, within the top-3 in 40% of the cases and overall return a useful artifact in 81% of the cases. Further, the use of REPs proves instrumental to all aspects of MAESTRO's performance, from ranking and searching of SO posts to synthesizing patches from a given post. In particular, 45% of correct patches generated by MAESTRO could not be produced by a baseline technique not using REPs, even when provided with MAESTRO's SO-post ranking. MAESTRO is also fast, needing around 1 second, on average, to generate its output.

</details>

<details>

<summary>2022-02-09 23:10:26 - GhostTalk: Interactive Attack on Smartphone Voice System Through Power Line</summary>

- *Yuanda Wang, Hanqing Guo, Qiben Yan*

- `2202.02585v2` - [abs](http://arxiv.org/abs/2202.02585v2) - [pdf](http://arxiv.org/pdf/2202.02585v2)

> Inaudible voice command injection is one of the most threatening attacks towards voice assistants. Existing attacks aim at injecting the attack signals over the air, but they require the access to the authorized user's voice for activating the voice assistants. Moreover, the effectiveness of the attacks can be greatly deteriorated in a noisy environment. In this paper, we explore a new type of channel, the power line side-channel, to launch the inaudible voice command injection. By injecting the audio signals over the power line through a modified charging cable, the attack becomes more resilient against various environmental factors and liveness detection models. Meanwhile, the smartphone audio output can be eavesdropped through the modified cable, enabling a highly-interactive attack.   To exploit the power line side-channel, we present GhostTalk, a new hidden voice attack that is capable of injecting and eavesdropping simultaneously. Via a quick modification of the power bank cables, the attackers could launch interactive attacks by remotely making a phone call or capturing private information from the voice assistants. GhostTalk overcomes the challenge of bypassing the speaker verification system by stealthily triggering a switch component to simulate the press button on the headphone. In case when the smartphones are charged by an unaltered standard cable, we discover that it is possible to recover the audio signal from smartphone loudspeakers by monitoring the charging current on the power line. To demonstrate the feasibility, we design GhostTalk-SC, an adaptive eavesdropper system targeting smartphones charged in the public USB ports. To correctly recognize the private information in the audio, GhostTalk-SC carefully extracts audio spectra and integrates a neural network model to classify spoken digits in the speech.

</details>

<details>

<summary>2022-02-10 03:30:12 - Sparse-Dyn: Sparse Dynamic Graph Multi-representation Learning via Event-based Sparse Temporal Attention Network</summary>

- *Yan Pang, Chao Liu*

- `2201.01384v2` - [abs](http://arxiv.org/abs/2201.01384v2) - [pdf](http://arxiv.org/pdf/2201.01384v2)

> Dynamic graph neural networks have been widely used in modeling and representation learning of graph structure data. Current dynamic representation learning focuses on either discrete learning which results in temporal information loss or continuous learning that involves heavy computation. In this work, we proposed a novel dynamic graph neural network, Sparse-Dyn. It adaptively encodes temporal information into a sequence of patches with an equal amount of temporal-topological structure. Therefore, while avoiding the use of snapshots which causes information loss, it also achieves a finer time granularity, which is close to what continuous networks could provide. In addition, we also designed a lightweight module, Sparse Temporal Transformer, to compute node representations through both structural neighborhoods and temporal dynamics. Since the fully-connected attention conjunction is simplified, the computation cost is far lower than the current state-of-the-arts. Link prediction experiments are conducted on both continuous and discrete graph datasets. Through comparing with several state-of-the-art graph embedding baselines, the experimental results demonstrate that Sparse-Dyn has a faster inference speed while having competitive performance.

</details>

<details>

<summary>2022-02-10 21:39:18 - SSAST: Self-Supervised Audio Spectrogram Transformer</summary>

- *Yuan Gong, Cheng-I Jeff Lai, Yu-An Chung, James Glass*

- `2110.09784v2` - [abs](http://arxiv.org/abs/2110.09784v2) - [pdf](http://arxiv.org/pdf/2110.09784v2)

> Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST.   This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.

</details>

<details>

<summary>2022-02-11 01:49:24 - Trust Enhancement Issues in Program Repair</summary>

- *Yannic Noller, Ridwan Shariffdeen, Xiang Gao, Abhik Roychoudhury*

- `2108.13064v4` - [abs](http://arxiv.org/abs/2108.13064v4) - [pdf](http://arxiv.org/pdf/2108.13064v4)

> Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.

</details>

<details>

<summary>2022-02-11 12:05:37 - Very Pwnable Network: Cisco AnyConnect Security Analysis</summary>

- *Gerbert Roitburd, Matthias Ortmann, Matthias Hollick, Jiska Classen*

- `2202.05573v1` - [abs](http://arxiv.org/abs/2202.05573v1) - [pdf](http://arxiv.org/pdf/2202.05573v1)

> Corporate Virtual Private Networks (VPNs) enable users to work from home or while traveling. At the same time, VPNs are tied to a company's network infrastructure, forcing users to install proprietary clients for network compatibility reasons. VPN clients run with high privileges to encrypt and reroute network traffic. Thus, bugs in VPN clients pose a substantial risk to their users and in turn the corporate network. Cisco, the dominating vendor of enterprise network hardware, offers VPN connectivity with their AnyConnect client for desktop and mobile devices. While past security research primarily focused on the AnyConnect Windows client, we show that Linux and iOS are based on different architectures and have distinct security issues. Our reverse engineering as well as the follow-up design analysis and fuzzing reveal 13 new vulnerabilities. Seven of these are located in the Linux client. The root cause for privilege escalations on Linux is anchored so deep in the client's architecture that it only got patched with a partial workaround. A similar analysis on iOS uncovers three AnyConnect-specific bugs as well as three general issues in iOS network extensions, which apply to all kinds of VPNs and are not restricted to AnyConnect.

</details>

<details>

<summary>2022-02-11 15:48:52 - Collaborative Dispersion by Silent Robots</summary>

- *Barun Gorain, Partha Sarathi Mandal, Kaushik Mondal, Supantha Pandit*

- `2202.05710v1` - [abs](http://arxiv.org/abs/2202.05710v1) - [pdf](http://arxiv.org/pdf/2202.05710v1)

> In the dispersion problem, a set of $k$ co-located mobile robots must relocate themselves in distinct nodes of an unknown network. The network is modeled as an anonymous graph $G=(V,E)$, where the nodes of the graph are not labeled. The edges incident to a node $v$ with degree $d$ are labeled with port numbers in the range $0,1, \cdots, d-1$ at $v$. The robots have unique ids in the range $[0,L]$, where $L \ge k$, and are initially placed at a source node $s$. Each robot knows only its own id but does not know the ids of the other robots or the values of $L,k$. The task of dispersion was traditionally achieved with the assumption of two types of communication abilities: (a) when some robots are at the same node, they can communicate by exchanging messages between them (b) any two robots in the network can exchange messages between them.   In this paper, we ask whether this ability of communication among co-located robots is necessary to achieve dispersion. We show that even if the ability of communication is not available, the task of dispersion by a set of mobile robots can be achieved in a much weaker model where a robot at a node $v$ has the access of following very restricted information at the beginning of any round: (1) am I alone at $v$? (2) the number of robots at $v$ increased or decreased compare to the previous round?   We propose a deterministic algorithm that achieves dispersion on any given graph $G=(V,E)$ in time $O\left( k\log L+k^2 \log \Delta\right)$, where $\Delta$ is the maximum degree of a node in $G$. Each robot uses $O(\log L+ \log \Delta)$ additional memory. We also prove that the task of dispersion cannot be achieved by a set of mobile robots with $o(\log L + \log \Delta)$ additional memory.

</details>

<details>

<summary>2022-02-11 16:22:27 - Patch-NetVLAD+: Learned patch descriptor and weighted matching strategy for place recognition</summary>

- *Yingfeng Cai, Junqiao Zhao, Jiafeng Cui, Fenglin Zhang, Chen Ye, Tiantian Feng*

- `2202.05738v1` - [abs](http://arxiv.org/abs/2202.05738v1) - [pdf](http://arxiv.org/pdf/2202.05738v1)

> Visual Place Recognition (VPR) in areas with similar scenes such as urban or indoor scenarios is a major challenge. Existing VPR methods using global descriptors have difficulty capturing local specific regions (LSR) in the scene and are therefore prone to localization confusion in such scenarios. As a result, finding the LSR that are critical for location recognition becomes key. To address this challenge, we introduced Patch-NetVLAD+, which was inspired by patch-based VPR researches. Our method proposed a fine-tuning strategy with triplet loss to make NetVLAD suitable for extracting patch-level descriptors. Moreover, unlike existing methods that treat all patches in an image equally, our method extracts patches of LSR, which present less frequently throughout the dataset, and makes them play an important role in VPR by assigning proper weights to them. Experiments on Pittsburgh30k and Tokyo247 datasets show that our approach achieved up to 6.35\% performance improvement than existing patch-based methods.

</details>

<details>

<summary>2022-02-12 11:10:22 - Perspectives on risk prioritization of data center vulnerabilities using rank aggregation and multi-objective optimization</summary>

- *Bruno Grisci, Gabriela Kuhn, Felipe Colombelli, VÃ­tor Matter, Leomar Lima, Karine Heinen, Mauricio Pegoraro, Marcio Borges, Sandro Rigo, Jorge Barbosa, Rodrigo da Rosa Righi, Cristiano AndrÃ© da Costa, Gabriel de Oliveira Ramos*

- `2202.07466v1` - [abs](http://arxiv.org/abs/2202.07466v1) - [pdf](http://arxiv.org/pdf/2202.07466v1)

> Nowadays, data has become an invaluable asset to entities and companies, and keeping it secure represents a major challenge. Data centers are responsible for storing data provided by software applications. Nevertheless, the number of vulnerabilities has been increasing every day. Managing such vulnerabilities is essential for building a reliable and secure network environment. Releasing patches to fix security flaws in software is a common practice to handle these vulnerabilities. However, prioritization becomes crucial for organizations with an increasing number of vulnerabilities since time and resources to fix them are usually limited. This review intends to present a survey of vulnerability ranking techniques and promote a discussion on how multi-objective optimization could benefit the management of vulnerabilities risk prioritization. The state-of-the-art approaches for risk prioritization were reviewed, intending to develop an effective model for ranking vulnerabilities in data centers. The main contribution of this work is to point out multi-objective optimization as a not commonly explored but promising strategy to prioritize vulnerabilities, enabling better time management and increasing security.

</details>

<details>

<summary>2022-02-14 14:27:58 - NALABS: Detecting Bad Smells in Natural Language Requirements and Test Specifications</summary>

- *Kostadin Rajkovic, Eduard Enoiu*

- `2202.05641v2` - [abs](http://arxiv.org/abs/2202.05641v2) - [pdf](http://arxiv.org/pdf/2202.05641v2)

> In large-scale embedded system development, requirement and test specifications are often expressed in natural language. In the context of developing such products, requirement review is performed in many cases manually using these specifications as a basis for quality assurance. Low-quality specifications can have expensive consequences during the requirement engineering process. Especially, if feedback loops during requirement engineering are long, leading to artifacts that are not easily maintainable, are hard to understand, and are inefficient to port to other system variants. We use the idea of smells to specifications expressed in natural language, defining a set of specifications for bad smells. We developed a tool called NALABS (NAtural LAnguage Bad Smells), available on https://github.com/eduardenoiu/NALABS and used for automatically checking specifications. We discuss some of the decisions made for its implementation, and future work.

</details>

<details>

<summary>2022-02-14 17:16:53 - Leveraging Local Domains for Image-to-Image Translation</summary>

- *Anthony Dell'Eva, Fabio Pizzati, Massimo Bertozzi, Raoul de Charette*

- `2109.04468v3` - [abs](http://arxiv.org/abs/2109.04468v3) - [pdf](http://arxiv.org/pdf/2109.04468v3)

> Image-to-image (i2i) networks struggle to capture local changes because they do not affect the global scene structure. For example, translating from highway scenes to offroad, i2i networks easily focus on global color features but ignore obvious traits for humans like the absence of lane markings. In this paper, we leverage human knowledge about spatial domain characteristics which we refer to as 'local domains' and demonstrate its benefit for image-to-image translation. Relying on a simple geometrical guidance, we train a patch-based GAN on few source data and hallucinate a new unseen domain which subsequently eases transfer learning to target. We experiment on three tasks ranging from unstructured environments to adverse weather. Our comprehensive evaluation setting shows we are able to generate realistic translations, with minimal priors, and training only on a few images. Furthermore, when trained on our translations images we show that all tested proxy tasks are significantly improved, without ever seeing target domain at training.

</details>

<details>

<summary>2022-02-14 21:39:34 - A variational approach based on perturbed eigenvalue analysis for improving spectral properties of isogeometric multipatch discretizations</summary>

- *Thi-Hoa Nguyen, RenÃ© R. Hiemstra, Stein K. F. Stoter, Dominik Schillinger*

- `2111.06501v2` - [abs](http://arxiv.org/abs/2111.06501v2) - [pdf](http://arxiv.org/pdf/2111.06501v2)

> A key advantage of isogeometric discretizations is their accurate and well-behaved eigenfrequencies and eigenmodes. For degree two and higher, however, optical branches of spurious outlier frequencies and modes may appear due to boundaries or reduced continuity at patch interfaces. In this paper, we introduce a variational approach based on perturbed eigenvalue analysis that eliminates outlier frequencies without negatively affecting the accuracy in the remainder of the spectrum and modes. We then propose a pragmatic iterative procedure that estimates the perturbation parameters in such a way that the outlier frequencies are effectively reduced. We demonstrate that our approach allows for a much larger critical time-step size in explicit dynamics calculations. In addition, we show that the critical time-step size obtained with the proposed approach does not depend on the polynomial degree of spline basis functions.

</details>

<details>

<summary>2022-02-16 13:49:43 - O-ViT: Orthogonal Vision Transformer</summary>

- *Yanhong Fei, Yingjie Liu, Xian Wei, Mingsong Chen*

- `2201.12133v2` - [abs](http://arxiv.org/abs/2201.12133v2) - [pdf](http://arxiv.org/pdf/2201.12133v2)

> Inspired by the tremendous success of the self-attention mechanism in natural language processing, the Vision Transformer (ViT) creatively applies it to image patch sequences and achieves incredible performance. However, the scaled dot-product self-attention of ViT brings about scale ambiguity to the structure of the original feature space. To address this problem, we propose a novel method named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the geometric perspective. O-ViT limits parameters of self-attention blocks to be on the norm-keeping orthogonal manifold, which can keep the geometry of the feature space. Moreover, O-ViT achieves both orthogonal constraints and cheap optimization overhead by adopting a surjective mapping between the orthogonal group and its Lie algebra.We have conducted comparative experiments on image recognition tasks to demonstrate O-ViT's validity and experiments show that O-ViT can boost the performance of ViT by up to 3.6%.

</details>

<details>

<summary>2022-02-17 15:12:56 - Revisiting reopened bugs in open source software systems</summary>

- *Ankur Tagra, Haoxiang Zhang, Gopi Krishnan Rajbahadur, Ahmed E. Hassan*

- `2202.08701v1` - [abs](http://arxiv.org/abs/2202.08701v1) - [pdf](http://arxiv.org/pdf/2202.08701v1)

> Reopened bugs can degrade the overall quality of a software system since they require unnecessary rework by developers. Moreover, reopened bugs also lead to a loss of trust in the end-users regarding the quality of the software. Thus, predicting bugs that might be reopened could be extremely helpful for software developers to avoid rework. Prior studies on reopened bug prediction focus only on three open source projects (i.e., Apache, Eclipse, and OpenOffice) to generate insights. We observe that one out of the three projects (i.e., Apache) has a data leak issue -- the bug status of reopened was included as training data to predict reopened bugs. In addition, prior studies used an outdated prediction model pipeline (i.e., with old techniques for constructing a prediction model) to predict reopened bugs. Therefore, we revisit the reopened bugs study on a large scale dataset consisting of 47 projects tracked by JIRA using the modern techniques such as SMOTE, permutation importance together with 7 different machine learning models. We study the reopened bugs using a mixed methods approach (i.e., both quantitative and qualitative study). We find that: 1) After using an updated reopened bug prediction model pipeline, only 34% projects give an acceptable performance with AUC >= 0.7. 2) There are four major reasons for a bug getting reopened, that is, technical (i.e., patch/integration issues), documentation, human (i.e., due to incorrect bug assessment), and reasons not shown in the bug reports. 3) In projects with an acceptable AUC, 94% of the reopened bugs are due to patch issues (i.e., the usage of an incorrect patch) identified before bug reopening. Our study revisits reopened bugs and provides new insights into developer's bug reopening activities.

</details>

<details>

<summary>2022-02-18 02:55:59 - A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation</summary>

- *Yu Wang, Yarong Ji, Hongbing Xiao*

- `2202.06344v2` - [abs](http://arxiv.org/abs/2202.06344v2) - [pdf](http://arxiv.org/pdf/2202.06344v2)

> Automatic segmentation of glioma and its subregions is of great significance for diagnosis, treatment and monitoring of disease. In this paper, an augmentation method, called TensorMixup, was proposed and applied to the three dimensional U-Net architecture for brain tumor segmentation. The main ideas included that first, two image patches with size of 128 in three dimensions were selected according to glioma information of ground truth labels from the magnetic resonance imaging data of any two patients with the same modality. Next, a tensor in which all elements were independently sampled from Beta distribution was used to mix the image patches. Then the tensor was mapped to a matrix which was used to mix the one-hot encoded labels of the above image patches. Therefore, a new image and its one-hot encoded label were synthesized. Finally, the new data was used to train the model which could be used to segment glioma. The experimental results show that the mean accuracy of Dice scores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor core, and enhancing tumor segmentation, which proves that the proposed TensorMixup is feasible and effective for brain tumor segmentation.

</details>

<details>

<summary>2022-02-18 11:53:01 - MultiRes-NetVLAD: Augmenting Place Recognition Training with Low-Resolution Imagery</summary>

- *Ahmad Khaliq, Michael Milford, Sourav Garg*

- `2202.09146v1` - [abs](http://arxiv.org/abs/2202.09146v1) - [pdf](http://arxiv.org/pdf/2202.09146v1)

> Visual Place Recognition (VPR) is a crucial component of 6-DoF localization, visual SLAM and structure-from-motion pipelines, tasked to generate an initial list of place match hypotheses by matching global place descriptors. However, commonly-used CNN-based methods either process multiple image resolutions after training or use a single resolution and limit multi-scale feature extraction to the last convolutional layer during training. In this paper, we augment NetVLAD representation learning with low-resolution image pyramid encoding which leads to richer place representations. The resultant multi-resolution feature pyramid can be conveniently aggregated through VLAD into a single compact representation, avoiding the need for concatenation or summation of multiple patches in recent multi-scale approaches. Furthermore, we show that the underlying learnt feature tensor can be combined with existing multi-scale approaches to improve their baseline performance. Evaluation on 15 viewpoint-varying and viewpoint-consistent benchmarking datasets confirm that the proposed MultiRes-NetVLAD leads to state-of-the-art Recall@N performance for global descriptor based retrieval, compared against 11 existing techniques. Source code is publicly available at https://github.com/Ahmedest61/MultiRes-NetVLAD.

</details>

<details>

<summary>2022-02-22 02:24:46 - On the Effectiveness of Adversarial Training against Backdoor Attacks</summary>

- *Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao Xia, Gang Niu, Masashi Sugiyama*

- `2202.10627v1` - [abs](http://arxiv.org/abs/2202.10627v1) - [pdf](http://arxiv.org/pdf/2202.10627v1)

> DNNs' demand for massive data forces practitioners to collect data from the Internet without careful check due to the unacceptable cost, which brings potential risks of backdoor attacks. A backdoored model always predicts a target class in the presence of a predefined trigger pattern, which can be easily realized via poisoning a small amount of data. In general, adversarial training is believed to defend against backdoor attacks since it helps models to keep their prediction unchanged even if we perturb the input image (as long as within a feasible range). Unfortunately, few previous studies succeed in doing so. To explore whether adversarial training could defend against backdoor attacks or not, we conduct extensive experiments across different threat models and perturbation budgets, and find the threat model in adversarial training matters. For instance, adversarial training with spatial adversarial examples provides notable robustness against commonly-used patch-based backdoor attacks. We further propose a hybrid strategy which provides satisfactory robustness across different backdoor attacks.

</details>

<details>

<summary>2022-02-22 10:20:14 - VU-BERT: A Unified framework for Visual Dialog</summary>

- *Tong Ye, Shijing Si, Jianzong Wang, Rui Wang, Ning Cheng, Jing Xiao*

- `2202.10787v1` - [abs](http://arxiv.org/abs/2202.10787v1) - [pdf](http://arxiv.org/pdf/2202.10787v1)

> The visual dialog task attempts to train an agent to answer multi-turn questions given an image, which requires the deep understanding of interactions between the image and dialog history. Existing researches tend to employ the modality-specific modules to model the interactions, which might be troublesome to use. To fill in this gap, we propose a unified framework for image-text joint embedding, named VU-BERT, and apply patch projection to obtain vision embedding firstly in visual dialog tasks to simplify the model. The model is trained over two tasks: masked language modeling and next utterance retrieval. These tasks help in learning visual concepts, utterances dependence, and the relationships between these two modalities. Finally, our VU-BERT achieves competitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.

</details>

<details>

<summary>2022-02-23 09:01:27 - Deepfake Detection for Facial Images with Facemasks</summary>

- *Donggeun Ko, Sangjun Lee, Jinyong Park, Saebyeol Shin, Donghee Hong, Simon S. Woo*

- `2202.11359v1` - [abs](http://arxiv.org/abs/2202.11359v1) - [pdf](http://arxiv.org/pdf/2202.11359v1)

> Hyper-realistic face image generation and manipulation have givenrise to numerous unethical social issues, e.g., invasion of privacy,threat of security, and malicious political maneuvering, which re-sulted in the development of recent deepfake detection methods with the rising demands of deepfake forensics. Proposed deepfake detection methods to date have shown remarkable detection performance and robustness. However, none of the suggested deepfake detection methods assessed the performance of deepfakes with the facemask during the pandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly evaluate the performance of state-of-the-art deepfake detection models on the deepfakes with the facemask. Also, we propose two approaches to enhance the masked deepfakes detection: face-patch and face-crop. The experimental evaluations on both methods are assessed through the base-line deepfake detection models on the various deepfake datasets. Our extensive experiments show that, among the two methods, face-crop performs better than the face-patch, and could be a train method for deepfake detection models to detect fake faces with facemask in real world.

</details>

<details>

<summary>2022-02-23 23:27:35 - Using Deep Learning to Detect Digitally Encoded DNA Trigger for Trojan Malware in Bio-Cyber Attacks</summary>

- *Mohd Siblee Islam, Stepan Ivanov, Hamdan Awan, Jennifer Drohan, Sasitharan Balasubramaniam, Lee Coffey, Srivatsan Kidambi, Witty Sri-saan*

- `2202.11824v1` - [abs](http://arxiv.org/abs/2202.11824v1) - [pdf](http://arxiv.org/pdf/2202.11824v1)

> This article uses Deep Learning technologies to safeguard DNA sequencing against Bio-Cyber attacks. We consider a hybrid attack scenario where the payload is encoded into a DNA sequence to activate a Trojan malware implanted in a software tool used in the sequencing pipeline in order to allow the perpetrators to gain control over the resources used in that pipeline during sequence analysis. The scenario considered in the paper is based on perpetrators submitting synthetically engineered DNA samples that contain digitally encoded IP address and port number of the perpetrators machine in the DNA. Genetic analysis of the samples DNA will decode the address that is used by the software trojan malware to activate and trigger a remote connection. This approach can open up to multiple perpetrators to create connections to hijack the DNA sequencing pipeline. As a way of hiding the data, the perpetrators can avoid detection by encoding the address to maximise similarity with genuine DNAs, which we showed previously. However, in this paper we show how Deep Learning can be used to successfully detect and identify the trigger encoded data, in order to protect a DNA sequencing pipeline from trojan attacks. The result shows nearly up to 100% accuracy in detection in such a novel Trojan attack scenario even after applying fragmentation encryption and steganography on the encoded trigger data. In addition, feasibility of designing and synthesizing encoded DNA for such Trojan payloads is validated by a wet lab experiment.

</details>

<details>

<summary>2022-02-24 10:56:17 - Learning to Merge Tokens in Vision Transformers</summary>

- *Cedric Renggli, AndrÃ© Susano Pinto, Neil Houlsby, Basil Mustafa, Joan Puigcerver, Carlos Riquelme*

- `2202.12015v1` - [abs](http://arxiv.org/abs/2202.12015v1) - [pdf](http://arxiv.org/pdf/2202.12015v1)

> Transformers are widely applied to solve natural language understanding and computer vision tasks. While scaling up these architectures leads to improved performance, it often comes at the expense of much higher computational costs. In order for large-scale models to remain practical in real-world systems, there is a need for reducing their computational overhead. In this work, we present the PatchMerger, a simple module that reduces the number of patches or tokens the network has to process by merging them between two consecutive intermediate layers. We show that the PatchMerger achieves a significant speedup across various model sizes while matching the original performance both upstream and downstream after fine-tuning.

</details>

<details>

<summary>2022-02-24 13:29:45 - A Neural Solver for Variational Problems on CAD Geometries with Application to Electric Machine Simulation</summary>

- *Moritz von Tresckow, Stefan Kurz, Herbert De Gersem, Dimitrios Loukrezis*

- `2111.09005v2` - [abs](http://arxiv.org/abs/2111.09005v2) - [pdf](http://arxiv.org/pdf/2111.09005v2)

> This work presents a deep learning-based framework for the solution of partial differential equations on complex computational domains described with computer-aided design tools. To account for the underlying distribution of the training data caused by spline-based projections from the reference to the physical domain, a variational neural solver equipped with an importance sampling scheme is developed, such that the loss function based on the discretized energy functional obtained after the weak formulation is modified according to the sample distribution. To tackle multi-patch domains possibly leading to solution discontinuities, the variational neural solver is additionally combined with a domain decomposition approach based on the Discontinuous Galerkin formulation. The proposed neural solver is verified on a toy problem and then applied to a real-world engineering test case, namely that of electric machine simulation. The numerical results show clearly that the neural solver produces physics-conforming solutions of significantly improved accuracy.

</details>

<details>

<summary>2022-02-25 03:09:46 - Structure-aware Unsupervised Tagged-to-Cine MRI Synthesis with Self Disentanglement</summary>

- *Xiaofeng Liu, Fangxu Xing, Jerry L. Prince, Maureen Stone, Georges El Fakhri, Jonghye Woo*

- `2202.12474v1` - [abs](http://arxiv.org/abs/2202.12474v1) - [pdf](http://arxiv.org/pdf/2202.12474v1)

> Cycle reconstruction regularized adversarial training -- e.g., CycleGAN, DiscoGAN, and DualGAN -- has been widely used for image style transfer with unpaired training data. Several recent works, however, have shown that local distortions are frequent, and structural consistency cannot be guaranteed. Targeting this issue, prior works usually relied on additional segmentation or consistent feature extraction steps that are task-specific. To counter this, this work aims to learn a general add-on structural feature extractor, by explicitly enforcing the structural alignment between an input and its synthesized image. Specifically, we propose a novel input-output image patches self-training scheme to achieve a disentanglement of underlying anatomical structures and imaging modalities. The translator and structure encoder are updated, following an alternating training protocol. In addition, the information w.r.t. imaging modality can be eliminated with an asymmetric adversarial game. We train, validate, and test our network on 1,768, 416, and 1,560 unpaired subject-independent slices of tagged and cine magnetic resonance imaging from a total of twenty healthy subjects, respectively, demonstrating superior performance over competing methods.

</details>

<details>

<summary>2022-02-28 08:24:00 - GRAPHITE: Generating Automatic Physical Examples for Machine-Learning Attacks on Computer Vision Systems</summary>

- *Ryan Feng, Neal Mangaokar, Jiefeng Chen, Earlence Fernandes, Somesh Jha, Atul Prakash*

- `2002.07088v6` - [abs](http://arxiv.org/abs/2002.07088v6) - [pdf](http://arxiv.org/pdf/2002.07088v6)

> This paper investigates an adversary's ease of attack in generating adversarial examples for real-world scenarios. We address three key requirements for practical attacks for the real-world: 1) automatically constraining the size and shape of the attack so it can be applied with stickers, 2) transform-robustness, i.e., robustness of a attack to environmental physical variations such as viewpoint and lighting changes, and 3) supporting attacks in not only white-box, but also black-box hard-label scenarios, so that the adversary can attack proprietary models. In this work, we propose GRAPHITE, an efficient and general framework for generating attacks that satisfy the above three key requirements. GRAPHITE takes advantage of transform-robustness, a metric based on expectation over transforms (EoT), to automatically generate small masks and optimize with gradient-free optimization. GRAPHITE is also flexible as it can easily trade-off transform-robustness, perturbation size, and query count in black-box settings. On a GTSRB model in a hard-label black-box setting, we are able to find attacks on all possible 1,806 victim-target class pairs with averages of 77.8% transform-robustness, perturbation size of 16.63% of the victim images, and 126K queries per pair. For digital-only attacks where achieving transform-robustness is not a requirement, GRAPHITE is able to find successful small-patch attacks with an average of only 566 queries for 92.2% of victim-target pairs. GRAPHITE is also able to find successful attacks using perturbations that modify small areas of the input image against PatchGuard, a recently proposed defense against patch-based attacks.

</details>


## 2022-03

<details>

<summary>2022-03-01 16:03:25 - Multi-Channel Man-in-the-Middle Attacks Against Protected Wi-Fi Networks: A State of the Art Review</summary>

- *Manesh Thankappan, Helena RifÃ -Pous, Carles Garrigues*

- `2203.00579v1` - [abs](http://arxiv.org/abs/2203.00579v1) - [pdf](http://arxiv.org/pdf/2203.00579v1)

> Multi-Channel Man-in-the-Middle (MitM) attacks are special MitM attacks capable of manipulating encrypted Wi-Fi wireless frames between two legitimate endpoints. Since its inception in 2014, attackers have been targeting WPA Wi-Fi networks to perform different attacks, such as cipher downgrades, denial of service, key reinstallation Man-in-the-Middle (MitM) attacks (KRACK) in 2017, and recently FragAttacks in 2021, which widely impacted millions of Wi-Fi Multi-Channel MitM (MC-MitM) devices, especially IoT devices. To the best of our knowledge, there are no studies in the literature that KRACK holistically review the different types of Multi-Channel MitM enabled attacks and analyze their potential Internet of Things (IoT) impact. To this end, we evaluate the capabilities of Multi-Channel MitM and review every reported attack in Encryption the state of the art. We examine practical issues that hamper the total adoption of protection mechanisms, i.e., Security security patches and Protected Management Frames (PMF), and review available defense mechanisms in FragAttacks confronting the Multi-Channel MitM enabled attacks in the IoT context. Finally, we highlight the potential research problems and identify future research lines in this field.

</details>

<details>

<summary>2022-03-02 17:30:03 - Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images</summary>

- *Alexander Vieth, Anna Vilanova, Boudewijn Lelieveldt, Elmar Eisemann, Thomas HÃ¶llt*

- `2202.09179v2` - [abs](http://arxiv.org/abs/2202.09179v2) - [pdf](http://arxiv.org/pdf/2202.09179v2)

> High-dimensional imaging is becoming increasingly relevant in many fields from astronomy and cultural heritage to systems biology. Visual exploration of such high-dimensional data is commonly facilitated by dimensionality reduction. However, common dimensionality reduction methods do not include spatial information present in images, such as local texture features, into the construction of low-dimensional embeddings. Consequently, exploration of such data is typically split into a step focusing on the attribute space followed by a step focusing on spatial information, or vice versa. In this paper, we present a method for incorporating spatial neighborhood information into distance-based dimensionality reduction methods, such as t-Distributed Stochastic Neighbor Embedding (t-SNE). We achieve this by modifying the distance measure between high-dimensional attribute vectors associated with each pixel such that it takes the pixel's spatial neighborhood into account. Based on a classification of different methods for comparing image patches, we explore a number of different approaches. We compare these approaches from a theoretical and experimental point of view. Finally, we illustrate the value of the proposed methods by qualitative and quantitative evaluation on synthetic data and two real-world use cases.

</details>

<details>

<summary>2022-03-03 02:31:50 - PetsGAN: Rethinking Priors for Single Image Generation</summary>

- *Zicheng Zhang, Yinglu Liu, Congying Han, Hailin Shi, Tiande Guo, Bowen Zhou*

- `2203.01488v1` - [abs](http://arxiv.org/abs/2203.01488v1) - [pdf](http://arxiv.org/pdf/2203.01488v1)

> Single image generation (SIG), described as generating diverse samples that have similar visual content with the given single image, is first introduced by SinGAN which builds a pyramid of GANs to progressively learn the internal patch distribution of the single image. It also shows great potentials in a wide range of image manipulation tasks. However, the paradigm of SinGAN has limitations in terms of generation quality and training time. Firstly, due to the lack of high-level information, SinGAN cannot handle the object images well as it does on the scene and texture images. Secondly, the separate progressive training scheme is time-consuming and easy to cause artifact accumulation. To tackle these problems, in this paper, we dig into the SIG problem and improve SinGAN by fully-utilization of internal and external priors. The main contributions of this paper include: 1) We introduce to SIG a regularized latent variable model. To the best of our knowledge, it is the first time to give a clear formulation and optimization goal of SIG, and all the existing methods for SIG can be regarded as special cases of this model. 2) We design a novel Prior-based end-to-end training GAN (PetsGAN) to overcome the problems of SinGAN. Our method gets rid of the time-consuming progressive training scheme and can be trained end-to-end. 3) We construct abundant qualitative and quantitative experiments to show the superiority of our method on both generated image quality, diversity, and the training speed. Moreover, we apply our method to other image manipulation tasks (e.g., style transfer, harmonization), and the results further prove the effectiveness and efficiency of our method.

</details>

<details>

<summary>2022-03-03 12:08:18 - On Improving Adversarial Transferability of Vision Transformers</summary>

- *Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli*

- `2106.04169v3` - [abs](http://arxiv.org/abs/2106.04169v3) - [pdf](http://arxiv.org/pdf/2106.04169v3)

> Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs). This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs. Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models. (i) Self-Ensemble: We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. (ii) Token Refinement: We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT. Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens.

</details>

<details>

<summary>2022-03-03 15:14:13 - A multi-stream convolutional neural network for classification of progressive MCI in Alzheimer's disease using structural MRI images</summary>

- *Mona Ashtari-Majlan, Abbas Seifi, Mohammad Mahdi Dehshibi*

- `2203.01944v1` - [abs](http://arxiv.org/abs/2203.01944v1) - [pdf](http://arxiv.org/pdf/2203.01944v1)

> Early diagnosis of Alzheimer's disease and its prodromal stage, also known as mild cognitive impairment (MCI), is critical since some patients with progressive MCI will develop the disease. We propose a multi-stream deep convolutional neural network fed with patch-based imaging data to classify stable MCI and progressive MCI. First, we compare MRI images of Alzheimer's disease with cognitively normal subjects to identify distinct anatomical landmarks using a multivariate statistical test. These landmarks are then used to extract patches that are fed into the proposed multi-stream convolutional neural network to classify MRI images. Next, we train the architecture in a separate scenario using samples from Alzheimer's disease images, which are anatomically similar to the progressive MCI ones and cognitively normal images to compensate for the lack of progressive MCI training data. Finally, we transfer the trained model weights to the proposed architecture in order to fine-tune the model using progressive MCI and stable MCI data. Experimental results on the ADNI-1 dataset indicate that our method outperforms existing methods for MCI classification, with an F1-score of 85.96%.

</details>

<details>

<summary>2022-03-04 22:48:12 - Geodesic Gramian Denoising Applied to the Images Contaminated With Noise Sampled From Diverse Probability Distributions</summary>

- *Yonggi Park, Kelum Gajamannage, Alexey Sadovski*

- `2203.02600v1` - [abs](http://arxiv.org/abs/2203.02600v1) - [pdf](http://arxiv.org/pdf/2203.02600v1)

> As quotidian use of sophisticated cameras surges, people in modern society are more interested in capturing fine-quality images. However, the quality of the images might be inferior to people's expectations due to the noise contamination in the images. Thus, filtering out the noise while preserving vital image features is an essential requirement. Current existing denoising methods have their own assumptions on the probability distribution in which the contaminated noise is sampled for the method to attain its expected denoising performance. In this paper, we utilize our recent Gramian-based filtering scheme to remove noise sampled from five prominent probability distributions from selected images. This method preserves image smoothness by adopting patches partitioned from the image, rather than pixels, and retains vital image features by performing denoising on the manifold underlying the patch space rather than in the image domain. We validate its denoising performance, using three benchmark computer vision test images applied to two state-of-the-art denoising methods, namely BM3D and K-SVD.

</details>

<details>

<summary>2022-03-06 16:46:58 - Vulnerability Detection in Open Source Software: An Introduction</summary>

- *Stuart Millar*

- `2203.16428v1` - [abs](http://arxiv.org/abs/2203.16428v1) - [pdf](http://arxiv.org/pdf/2203.16428v1)

> This paper is an introductory discussion on the cause of open source software vulnerabilities, their importance in the cybersecurity ecosystem, and a selection of detection methods. A recent application security report showed 44% of applications contain critical vulnerabilities in an open source component, a concerning proportion. Most companies do not have a reliable way of being directly and promptly notified when zero-day vulnerabilities are found and then when patches are made available. This means attack vectors in open source exist longer than necessary. Conventional approaches to vulnerability detection are outlined alongside some newer research trends. A conclusion is made that it may not be possible to entirely replace expert human inspection of open source software, although it can be effectively augmented with techniques such as machine learning, IDE plug-ins and repository linking to make implementation and review less time intensive. Underpinning any technological advances should be better knowledge at the human level. Development teams need trained, coached and improved so they can implement open source more securely, know what vulnerabilities to look for and how to handle them. It is the use of this blended approach to detection which is key.

</details>

<details>

<summary>2022-03-07 17:22:30 - ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches</summary>

- *Maura Pintor, Daniele Angioni, Angelo Sotgiu, Luca Demetrio, Ambra Demontis, Battista Biggio, Fabio Roli*

- `2203.04412v1` - [abs](http://arxiv.org/abs/2203.04412v1) - [pdf](http://arxiv.org/pdf/2203.04412v1)

> Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch.

</details>

<details>

<summary>2022-03-08 02:29:32 - Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers</summary>

- *Han Joo Chae, Seunghwan Lee, Hyewon Son, Seungyeob Han, Taebin Lim*

- `2203.03814v1` - [abs](http://arxiv.org/abs/2203.03814v1) - [pdf](http://arxiv.org/pdf/2203.03814v1)

> We introduce AiD Regen, a novel system that generates 3D wound models combining 2D semantic segmentation with 3D reconstruction so that they can be printed via 3D bio-printers during the surgery to treat diabetic foot ulcers (DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D image capturing, semantic segmentation, boundary-guided point-cloud processing, 3D model reconstruction, and 3D printable G-code generation, into a single system that can be used out of the box. We developed a multi-stage data preprocessing method to handle small and unbalanced DFU image datasets. AiD Regen's human-in-the-loop machine learning interface enables clinicians to not only create 3D regenerative patches with just a few touch interactions but also customize and confirm wound boundaries. As evidenced by our experiments, our model outperforms prior wound segmentation models and our reconstruction algorithm is capable of generating 3D wound models with compelling accuracy. We further conducted a case study on a real DFU patient and demonstrated the effectiveness of AiD Regen in treating DFU wounds.

</details>

<details>

<summary>2022-03-08 10:14:51 - End-to-end Multiple Instance Learning with Gradient Accumulation</summary>

- *Axel Andersson, Nadezhda Koriakina, NataÅ¡a Sladoje, Joakim Lindblad*

- `2203.03981v1` - [abs](http://arxiv.org/abs/2203.03981v1) - [pdf](http://arxiv.org/pdf/2203.03981v1)

> Being able to learn on weakly labeled data, and provide interpretability, are two of the main reasons why attention-based deep multiple instance learning (ABMIL) methods have become particularly popular for classification of histopathological images. Such image data usually come in the form of gigapixel-sized whole-slide-images (WSI) that are cropped into smaller patches (instances). However, the sheer size of the data makes training of ABMIL models challenging. All the instances from one WSI cannot be processed at once by conventional GPUs. Existing solutions compromise training by relying on pre-trained models, strategic sampling or selection of instances, or self-supervised learning. We propose a training strategy based on gradient accumulation that enables direct end-to-end training of ABMIL models without being limited by GPU memory. We conduct experiments on both QMNIST and Imagenette to investigate the performance and training time, and compare with the conventional memory-expensive baseline and a recent sampled-based approach. This memory-efficient approach, although slower, reaches performance indistinguishable from the memory-expensive baseline.

</details>

<details>

<summary>2022-03-09 04:29:50 - Guidelines for cyber risk management in shipboard operational technology systems</summary>

- *Priyanga Rajaram, Mark Goh, Jianying Zhou*

- `2203.04072v2` - [abs](http://arxiv.org/abs/2203.04072v2) - [pdf](http://arxiv.org/pdf/2203.04072v2)

> Over the past few years, we have seen several cyber incidents being reported, where some of the primary causes were the lack of proper security controls onboard the ship and crew awareness on cybersecurity. In response to the growing cyber threat landscape in the maritime sector, we have developed a set of guidelines for maritime cyber risk management, focusing on four major shipboard Operational Technology (OT) systems that are crucial for the day-to-day operation of ships. These four OT systems are: Communication Systems, Propulsion, Machinery and Power Control Systems, Navigation Systems and Cargo Management Systems. The guidelines identify the cyber risks in each of the OT systems and recommend the necessary actions that can be taken to manage risks in each shipboard OT system. In this paper, we introduce the new guidelines, which include cyber risks, mitigation measures, cyber risk assessment, and a checklist to help shipowners and maritime authorities assess and enhance cyber hygiene of their vessels. Our guidelines have been disseminated by the Maritime and Port Authority of Singapore (MPA) to owners and operators of the Singapore Registry of Ships for their reference and use.

</details>

<details>

<summary>2022-03-09 08:15:14 - CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction</summary>

- *Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng Jing, Xiaoyao Liang*

- `2203.04570v1` - [abs](http://arxiv.org/abs/2203.04570v1) - [pdf](http://arxiv.org/pdf/2203.04570v1)

> Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices.   We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition.   In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning.   Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\% patches, our CP-ViT method reduces over 40\% FLOPs while maintaining accuracy loss within 1\%.

</details>

<details>

<summary>2022-03-09 09:12:48 - P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening</summary>

- *Yurong Chen*

- `2108.03815v4` - [abs](http://arxiv.org/abs/2108.03815v4) - [pdf](http://arxiv.org/pdf/2108.03815v4)

> Anomaly detection plays a pivotal role in numerous real-world scenarios, such as industrial automation and manufacturing intelligence. Recently, variational inference-based anomaly analysis has attracted researchers' and developers' attention. It aims to model the defect-free distribution so that anomalies can be classified as out-of-distribution samples. Nevertheless, there are two disturbing factors that need us to prioritize: (i) the simplistic prior latent distribution inducing limited expressive capability; (ii) the strong probability distance notion results in collapsed features. In this paper, we propose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to alleviate those challenges. In particular, a patch-wise variational inference model coupled with solving the jigsaw puzzle is designed, which is a simple yet effective way to increase the expressiveness of the latent manifold. This makes using the model on high-dimensional practical data possible. In addition, we leverage a weaker measure, sliced-Wasserstein distance, to achieve the equilibrium between the reconstruction fidelity and generalized representations. Comprehensive experiments, conducted on the MVTec AD dataset, demonstrate the superior performance of our proposed method.

</details>

<details>

<summary>2022-03-09 23:55:24 - Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice</summary>

- *Peihao Wang, Wenqing Zheng, Tianlong Chen, Zhangyang Wang*

- `2203.05962v1` - [abs](http://arxiv.org/abs/2203.05962v1) - [pdf](http://arxiv.org/pdf/2203.05962v1)

> Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains "for free" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing.

</details>

<details>

<summary>2022-03-10 01:45:08 - Curvature-guided dynamic scale networks for Multi-view Stereo</summary>

- *Khang Truong Giang, Soohwan Song, Sungho Jo*

- `2112.05999v3` - [abs](http://arxiv.org/abs/2112.05999v3) - [pdf](http://arxiv.org/pdf/2112.05999v3)

> Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by designing aggregated 3D cost volumes and their regularization. This paper focuses on learning a robust feature extraction network to enhance the performance of matching costs without heavy computation in the other steps. In particular, we present a dynamic scale feature extraction network, namely, CDSFNet. It is composed of multiple novel convolution layers, each of which can select a proper patch scale for each pixel guided by the normal curvature of the image surface. As a result, CDFSNet can estimate the optimal patch scales to learn discriminative features for accurate matching computation between reference and source images. By combining the robust extracted features with an appropriate cost formulation strategy, our resulting MVS architecture can estimate depth maps more precisely. Extensive experiments showed that the proposed method outperforms other state-of-the-art methods on complex outdoor scenes. It significantly improves the completeness of reconstructed models. As a result, the method can process higher resolution inputs within faster run-time and lower memory than other MVS methods. Our source code is available at url{https://github.com/TruongKhang/cds-mvsnet}.

</details>

<details>

<summary>2022-03-10 03:22:54 - Manifold Modeling in Quotient Space: Learning An Invariant Mapping with Decodability of Image Patches</summary>

- *Tatsuya Yokota, Hidekata Hontani*

- `2203.05134v1` - [abs](http://arxiv.org/abs/2203.05134v1) - [pdf](http://arxiv.org/pdf/2203.05134v1)

> This study proposes a framework for manifold learning of image patches using the concept of equivalence classes: manifold modeling in quotient space (MMQS). In MMQS, we do not consider a set of local patches of the image as it is, but rather the set of their canonical patches obtained by introducing the concept of equivalence classes and performing manifold learning on their canonical patches. Canonical patches represent equivalence classes, and their auto-encoder constructs a manifold in the quotient space. Based on this framework, we produce a novel manifold-based image model by introducing rotation-flip-equivalence relations. In addition, we formulate an image reconstruction problem by fitting the proposed image model to a corrupted observed image and derive an algorithm to solve it. Our experiments show that the proposed image model is effective for various self-supervised image reconstruction tasks, such as image inpainting, deblurring, super-resolution, and denoising.

</details>

<details>

<summary>2022-03-10 05:43:35 - Program Repair: Automated vs. Manual</summary>

- *Quanjun Zhang, Yuan Zhao, Weisong Sun, Chunrong Fang, Ziyuan Wang, Lingming Zhang*

- `2203.05166v1` - [abs](http://arxiv.org/abs/2203.05166v1) - [pdf](http://arxiv.org/pdf/2203.05166v1)

> Various automated program repair (APR) techniques have been proposed to fix bugs automatically in the last decade. Although recent researches have made significant progress on the effectiveness and efficiency, it is still unclear how APR techniques perform with human intervention in a real debugging scenario. To bridge this gap, we conduct an extensive study to compare three state-of-the-art APR tools with manual program repair, and further investigate whether the assistance of APR tools (i.e., repair reports) can improve manual program repair. To that end, we recruit 20 participants for a controlled experiment, resulting in a total of 160 manual repair tasks and a questionnaire survey. The experiment reveals several notable observations that (1) manual program repair may be influenced by the frequency of repair actions sometimes; (2) APR tools are more efficient in terms of debugging time, while manual program repair tends to generate a correct patch with fewer attempts; (3) APR tools can further improve manual program repair regarding the number of correctly-fixed bugs, while there exists a negative impact on the patch correctness; (4) participants are used to consuming more time to identify incorrect patches, while they are still misguided easily; (5) participants are positive about the tools' repair performance, while they generally lack confidence about the usability in practice. Besides, we provide some guidelines for improving the usability of APR tools (e.g., the misleading information in reports and the observation of feedback).

</details>

<details>

<summary>2022-03-12 00:08:54 - Peel $\mid$ Pile? Cross-Framework Portability of Quantum Software</summary>

- *Manuel SchÃ¶nberger, Maja Franz, Stefanie Scherzinger, Wolfgang Mauerer*

- `2203.06289v1` - [abs](http://arxiv.org/abs/2203.06289v1) - [pdf](http://arxiv.org/pdf/2203.06289v1)

> In recent years, various vendors have made quantum software frameworks available. Yet with vendor-specific frameworks, code portability seems at risk, especially in a field where hardware and software libraries have not yet reached a consolidated state, and even foundational aspects of the technologies are still in flux. Accordingly, the development of vendor-independent quantum programming languages and frameworks is often suggested. This follows the established architectural pattern of introducing additional levels of abstraction into software stacks, thereby piling on layers of abstraction. Yet software architecture also provides seemingly less abstract alternatives, namely to focus on hardware-specific formulations of problems that peel off unnecessary layers. In this article, we quantitatively and experimentally explore these strategic alternatives, and compare popular quantum frameworks from the software implementation perspective. We find that for several specific, yet generalisable problems, the mathematical formulation of the problem to be solved is not just sufficiently abstract and serves as precise description, but is likewise concrete enough to allow for deriving framework-specific implementations with little effort. Additionally, we argue, based on analysing dozens of existing quantum codes, that porting between frameworks is actually low-effort, since the quantum- and framework-specific portions are very manageable in terms of size, commonly in the order of mere hundreds of lines of code. Given the current state-of-the-art in quantum programming practice, this leads us to argue in favour of peeling off unnecessary abstraction levels.

</details>

<details>

<summary>2022-03-12 04:48:12 - The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy</summary>

- *Tianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, Zhangyang Wang*

- `2203.06345v1` - [abs](http://arxiv.org/abs/2203.06345v1) - [pdf](http://arxiv.org/pdf/2203.06345v1)

> Vision transformers (ViTs) have gained increasing popularity as they are commonly believed to own higher modeling capacity and representation flexibility, than traditional convolutional networks. However, it is questionable whether such potential has been fully unleashed in practice, as the learned ViTs often suffer from over-smoothening, yielding likely redundant models. Recent works made preliminary attempts to identify and alleviate such redundancy, e.g., via regularizing embedding similarity or re-injecting convolution-like structures. However, a "head-to-toe assessment" regarding the extent of redundancy in ViTs, and how much we could gain by thoroughly mitigating such, has been absent for this field. This paper, for the first time, systematically studies the ubiquitous existence of redundancy at all three levels: patch embedding, attention map, and weight space. In view of them, we advocate a principle of diversity for training ViTs, by presenting corresponding regularizers that encourage the representation diversity and coverage at each of those levels, that enabling capturing more discriminative information. Extensive experiments on ImageNet with a number of ViT backbones validate the effectiveness of our proposals, largely eliminating the observed ViT redundancy and significantly boosting the model generalization. For example, our diversified DeiT obtains 0.70%~1.76% accuracy boosts on ImageNet with highly reduced similarity. Our codes are fully available in https://github.com/VITA-Group/Diverse-ViT.

</details>

<details>

<summary>2022-03-14 00:34:11 - CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering</summary>

- *Nicholas Soucy, Salimeh Yasaei Sekeh*

- `2203.04873v2` - [abs](http://arxiv.org/abs/2203.04873v2) - [pdf](http://arxiv.org/pdf/2203.04873v2)

> Most semantic segmentation approaches of Hyperspectral images (HSIs) use and require preprocessing steps in the form of patching to accurately classify diversified land cover in remotely sensed images. These approaches use patching to incorporate the rich neighborhood information in images and exploit the simplicity and segmentability of the most common HSI datasets. In contrast, most landmasses in the world consist of overlapping and diffused classes, making neighborhood information weaker than what is seen in common HSI datasets. To combat this issue and generalize the segmentation models to more complex and diverse HSI datasets, in this work, we propose our novel flagship model: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to combine spectral information extracted from convolutional neural network (CNN) training on a cluster of landscape pixels. Our CEU-Net model outperforms existing state-of-the-art HSI semantic segmentation methods and gets competitive performance with and without patching when compared to baseline models. We highlight CEU-Net's high performance across Botswana, KSC, and Salinas datasets compared to HybridSN and AeroRIT methods.

</details>

<details>

<summary>2022-03-14 09:41:35 - Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection</summary>

- *Nicolae-Catalin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B. Moeslund, Mubarak Shah*

- `2111.09099v6` - [abs](http://arxiv.org/abs/2111.09099v6) - [pdf](http://arxiv.org/pdf/2111.09099v6)

> Anomaly detection is commonly pursued as a one-class classification problem, where models can only learn from normal training samples, while being evaluated on both normal and abnormal test samples. Among the successful approaches for anomaly detection, a distinguished category of methods relies on predicting masked information (e.g. patches, future frames, etc.) and leveraging the reconstruction error with respect to the masked information as an abnormality score. Different from related methods, we propose to integrate the reconstruction-based functionality into a novel self-supervised predictive architectural building block. The proposed self-supervised block is generic and can easily be incorporated into various state-of-the-art anomaly detection methods. Our block starts with a convolutional layer with dilated filters, where the center area of the receptive field is masked. The resulting activation maps are passed through a channel attention module. Our block is equipped with a loss that minimizes the reconstruction error with respect to the masked area in the receptive field. We demonstrate the generality of our block by integrating it into several state-of-the-art frameworks for anomaly detection on image and video, providing empirical evidence that shows considerable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We release our code as open source at https://github.com/ristea/sspcab.

</details>

<details>

<summary>2022-03-15 15:50:04 - Minimum Viable Device Drivers for ARM TrustZone</summary>

- *Liwei Guo, Felix Xiaozhu Lin*

- `2110.08303v2` - [abs](http://arxiv.org/abs/2110.08303v2) - [pdf](http://arxiv.org/pdf/2110.08303v2)

> While TrustZone can isolate IO hardware, it lacks drivers for modern IO devices. Rather than porting drivers, we propose a novel approach to deriving minimum viable drivers: developers exercise a full driver and record the driver/device interactions; the processed recordings, dubbed driverlets, are replayed in the TEE at run time to access IO devices.   Driverlets address two key challenges: correctness and expressiveness, for which they build on a key construct called interaction template. The interaction template ensures faithful reproduction of recorded IO jobs (albeit on new IO data); it accepts dynamic input values; it tolerates nondeterministic device behaviors.   We demonstrate driverlets on a series of sophisticated devices, making them accessible to TrustZone for the first time to our knowledge. Our experiments show that driverlets are secure, easy to build, and incur acceptable overhead (1.4x -2.7x compared to native drivers). Driverlets fill a critical gap in the TrustZone TEE, realizing its long-promised vision of secure IO.

</details>

<details>

<summary>2022-03-16 03:47:01 - Self-Supervised Deep Learning to Enhance Breast Cancer Detection on Screening Mammography</summary>

- *John D. Miller, Vignesh A. Arasu, Albert X. Pu, Laurie R. Margolies, Weiva Sieh, Li Shen*

- `2203.08812v1` - [abs](http://arxiv.org/abs/2203.08812v1) - [pdf](http://arxiv.org/pdf/2203.08812v1)

> A major limitation in applying deep learning to artificial intelligence (AI) systems is the scarcity of high-quality curated datasets. We investigate strong augmentation based self-supervised learning (SSL) techniques to address this problem. Using breast cancer detection as an example, we first identify a mammogram-specific transformation paradigm and then systematically compare four recent SSL methods representing a diversity of approaches. We develop a method to convert a pretrained model from making predictions on uniformly tiled patches to whole images, and an attention-based pooling method that improves the classification performance. We found that the best SSL model substantially outperformed the baseline supervised model. The best SSL model also improved the data efficiency of sample labeling by nearly 4-fold and was highly transferrable from one dataset to another. SSL represents a major breakthrough in computer vision and may help the AI for medical imaging field to shift away from supervised learning and dependency on scarce labels.

</details>

<details>

<summary>2022-03-16 05:20:29 - SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher</summary>

- *Thai Le, Noseong Park, Dongwon Lee*

- `2011.08908v2` - [abs](http://arxiv.org/abs/2011.08908v2) - [pdf](http://arxiv.org/pdf/2011.08908v2)

> Even though several methods have proposed to defend textual neural network (NN) models against black-box adversarial attacks, they often defend against a specific text perturbation strategy and/or require re-training the models from scratch. This leads to a lack of generalization in practice and redundant computation. In particular, the state-of-the-art transformer models (e.g., BERT, RoBERTa) require great time and computation resources. By borrowing an idea from software engineering, in order to address these limitations, we propose a novel algorithm, SHIELD, which modifies and re-trains only the last layer of a textual NN, and thus it "patches" and "transforms" the NN into a stochastic weighted ensemble of multi-expert prediction heads. Considering that most of current black-box attacks rely on iterative search mechanisms to optimize their adversarial perturbations, SHIELD confuses the attackers by automatically utilizing different weighted ensembles of predictors depending on the input. In other words, SHIELD breaks a fundamental assumption of the attack, which is a victim NN model remains constant during an attack. By conducting comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and RoBERTa-based textual NNs, once patched by SHIELD, exhibit a relative enhancement of 15%--70% in accuracy on average against 14 different black-box attacks, outperforming 6 defensive baselines across 3 public datasets. All codes are to be released.

</details>

<details>

<summary>2022-03-16 10:39:18 - Towards Practical Certifiable Patch Defense with Vision Transformer</summary>

- *Zhaoyu Chen, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, Wenqiang Zhang*

- `2203.08519v1` - [abs](http://arxiv.org/abs/2203.08519v1) - [pdf](http://arxiv.org/pdf/2203.08519v1)

> Patch attacks, one of the most threatening forms of physical attack in adversarial examples, can lead networks to induce misclassification by modifying pixels arbitrarily in a continuous region. Certifiable patch defense can guarantee robustness that the classifier is not affected by patch attacks. Existing certifiable patch defenses sacrifice the clean accuracy of classifiers and only obtain a low certified accuracy on toy datasets. Furthermore, the clean and certified accuracy of these methods is still significantly lower than the accuracy of normal classification networks, which limits their application in practice. To move towards a practical certifiable patch defense, we introduce Vision Transformer (ViT) into the framework of Derandomized Smoothing (DS). Specifically, we propose a progressive smoothed image modeling task to train Vision Transformer, which can capture the more discriminable local context of an image while preserving the global semantic information. For efficient inference and deployment in the real world, we innovatively reconstruct the global self-attention structure of the original ViT into isolated band unit self-attention. On ImageNet, under 2% area patch attacks our method achieves 41.70% certified accuracy, a nearly 1-fold increase over the previous best method (26.00%). Simultaneously, our method achieves 78.58% clean accuracy, which is quite close to the normal ResNet-101 accuracy. Extensive experiments show that our method obtains state-of-the-art clean and certified accuracy with inferring efficiently on CIFAR-10 and ImageNet.

</details>

<details>

<summary>2022-03-16 16:35:03 - Predicting Patch Correctness Based on the Similarity of Failing Test Cases</summary>

- *Haoye Tian, Yinghua Li, Weiguo Pian, Abdoul Kader KaborÃ©, Kui Liu, Andrew Habib, Jacques Klein, TegawendÃ© F. Bissyande*

- `2107.13296v2` - [abs](http://arxiv.org/abs/2107.13296v2) - [pdf](http://arxiv.org/pdf/2107.13296v2)

> Towards predicting patch correctness in APR, we propose a simple, but novel hypothesis on how the link between the patch behaviour and failing test specifications can be drawn: similar failing test cases should require similar patches. We then propose BATS, an unsupervised learning-based system to predict patch correctness by checking patch Behaviour Against failing Test Specification. BATS exploits deep representation learning models for code and patches: for a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases in order to identify the associated applied patches, which are then used as a proxy for assessing generated patch correctness. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1278 plausible patches (written by developers or generated by some 32 APR tools), we use BATS to predict correctness: BATS achieves an AUC between 0.557 to 0.718 and a recall between 0.562 and 0.854 in identifying correct patches. Compared against previous work, we demonstrate that our approach outperforms state-of-the-art performance in patch correctness prediction, without the need for large labeled patch datasets in contrast with prior machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: used in conjunction with a recent approach implementing supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS can be complementary to the state-of-the-art PATCH-SIM dynamic approach of identifying the correct patches for APR tools.

</details>

<details>

<summary>2022-03-17 08:51:09 - On Vision Features in Multimodal Machine Translation</summary>

- *Bei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma, JingBo Zhu*

- `2203.09173v1` - [abs](http://arxiv.org/abs/2203.09173v1) - [pdf](http://arxiv.org/pdf/2203.09173v1)

> Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models (such as Vision Transformer) and enhanced features (such as object-detection and image captioning). We develop a selective attention model to study the patch-level contribution of an image in MMT. On detailed probing tasks, we find that stronger vision models are helpful for learning translation from the visual modality. Our results also suggest the need of carefully examining MMT models, especially when current benchmarks are small-scale and biased. Our code could be found at \url{https://github.com/libeineu/fairseq_mmt}.

</details>

<details>

<summary>2022-03-18 00:56:50 - Cervical Optical Coherence Tomography Image Classification Based on Contrastive Self-Supervised Texture Learning</summary>

- *Kaiyi Chen, Qingbin Wang, Yutao Ma*

- `2108.05081v3` - [abs](http://arxiv.org/abs/2108.05081v3) - [pdf](http://arxiv.org/pdf/2108.05081v3)

> Background: Cervical cancer seriously affects the health of the female reproductive system. Optical coherence tomography (OCT) emerged as a non-invasive, high-resolution imaging technology for cervical disease detection. However, OCT image annotation is knowledge-intensive and time-consuming, which impedes the training process of deep-learning-based classification models. Purpose: This study aims to develop a computer-aided diagnosis (CADx) approach to classifying in-vivo cervical OCT images based on self-supervised learning. Methods: In addition to high-level semantic features extracted by a convolutional neural network (CNN), the proposed CADx approach leverages unlabeled cervical OCT images' texture features learned by contrastive texture learning. We conducted ten-fold cross-validation on the OCT image dataset from a multi-center clinical study on 733 patients from China. Results: In a binary classification task for detecting high-risk diseases, including high-grade squamous intraepithelial lesion and cervical cancer, our method achieved an area-under-the-curve value of 0.9798 plus or minus 0.0157 with a sensitivity of 91.17 plus or minus 4.99% and a specificity of 93.96 plus or minus 4.72% for OCT image patches; also, it outperformed two out of four medical experts on the test set. Furthermore, our method achieved a 91.53% sensitivity and 97.37% specificity on an external validation dataset containing 287 3D OCT volumes from 118 Chinese patients in a new hospital using a cross-shaped threshold voting strategy. Conclusions: The proposed contrastive-learning-based CADx method outperformed the end-to-end CNN models and provided better interpretability based on texture features, which holds great potential to be used in the clinical protocol of "see-and-treat."

</details>

<details>

<summary>2022-03-18 08:18:03 - AdIoTack: Quantifying and Refining Resilience of Decision Tree Ensemble Inference Models against Adversarial Volumetric Attacks on IoT Networks</summary>

- *Arman Pashamokhtari, Gustavo Batista, Hassan Habibi Gharakheili*

- `2203.09792v1` - [abs](http://arxiv.org/abs/2203.09792v1) - [pdf](http://arxiv.org/pdf/2203.09792v1)

> Machine Learning-based techniques have shown success in cyber intelligence. However, they are increasingly becoming targets of sophisticated data-driven adversarial attacks resulting in misprediction, eroding their ability to detect threats on network devices. In this paper, we present AdIoTack, a system that highlights vulnerabilities of decision trees against adversarial attacks, helping cybersecurity teams quantify and refine the resilience of their trained models for monitoring IoT networks. To assess the model for the worst-case scenario, AdIoTack performs white-box adversarial learning to launch successful volumetric attacks that decision tree ensemble models cannot flag. Our first contribution is to develop a white-box algorithm that takes a trained decision tree ensemble model and the profile of an intended network-based attack on a victim class as inputs. It then automatically generates recipes that specify certain packets on top of the indented attack packets (less than 15% overhead) that together can bypass the inference model unnoticed. We ensure that the generated attack instances are feasible for launching on IP networks and effective in their volumetric impact. Our second contribution develops a method to monitor the network behavior of connected devices actively, inject adversarial traffic (when feasible) on behalf of a victim IoT device, and successfully launch the intended attack. Our third contribution prototypes AdIoTack and validates its efficacy on a testbed consisting of a handful of real IoT devices monitored by a trained inference model. We demonstrate how the model detects all non-adversarial volumetric attacks on IoT devices while missing many adversarial ones. The fourth contribution develops systematic methods for applying patches to trained decision tree ensemble models, improving their resilience against adversarial volumetric attacks.

</details>

<details>

<summary>2022-03-18 13:52:57 - Improving Semantic Consistency of Variable Names with Use-Flow Graph Analysis</summary>

- *Yusuke Shinyama, Yoshitaka Arahori, Katsuhiko Gondow*

- `2203.09960v1` - [abs](http://arxiv.org/abs/2203.09960v1) - [pdf](http://arxiv.org/pdf/2203.09960v1)

> Consistency is one of the keys to maintainable source code and hence a successful software project. We propose a novel method of extracting the intent of programmers from source code of a large project (~300kLOC) and checking the semantic consistency of its variable names. Our system learns a project-specific naming convention for variables based on its role solely from source code, and suggest alternatives when it violates its internal consistency. The system can also show the reasoning why a certain variable should be named in a specific way. The system does not rely on any external knowledge. We applied our method to 12 open-source projects and evaluated its results with human reviewers. Our system proposed alternative variable names for 416 out of 1080 (39%) instances that are considered better than ones originally used by the developers. Based on the results, we created patches to correct the inconsistent names and sent them to its developers. Three open-source projects adopted it.

</details>

<details>

<summary>2022-03-18 20:05:36 - Approximation and Learning with Deep Convolutional Models: a Kernel Perspective</summary>

- *Alberto Bietti*

- `2102.10032v3` - [abs](http://arxiv.org/abs/2102.10032v3) - [pdf](http://arxiv.org/pdf/2102.10032v3)

> The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities.

</details>

<details>

<summary>2022-03-19 11:35:18 - CLIPstyler: Image Style Transfer with a Single Text Condition</summary>

- *Gihyun Kwon, Jong Chul Ye*

- `2112.00374v3` - [abs](http://arxiv.org/abs/2112.00374v3) - [pdf](http://arxiv.org/pdf/2112.00374v3)

> Existing neural style transfer methods require reference style images to transfer texture information of style images to content images. However, in many practical situations, users may not have reference style images but still be interested in transferring styles by just imagining them. In order to deal with such applications, we propose a new framework that enables a style transfer `without' a style image, but only with a text description of the desired style. Using the pre-trained text-image embedding model of CLIP, we demonstrate the modulation of the style of content images only with a single text condition. Specifically, we propose a patch-wise text-image matching loss with multiview augmentations for realistic texture transfer. Extensive experimental results confirmed the successful image style transfer with realistic textures that reflect semantic query texts.

</details>

<details>

<summary>2022-03-21 03:35:32 - LocATe: End-to-end Localization of Actions in 3D with Transformers</summary>

- *Jiankai Sun, Bolei Zhou, Michael J. Black, Arjun Chandrasekaran*

- `2203.10719v1` - [abs](http://arxiv.org/abs/2203.10719v1) - [pdf](http://arxiv.org/pdf/2203.10719v1)

> Understanding a person's behavior from their 3D motion is a fundamental problem in computer vision with many applications. An important component of this problem is 3D Temporal Action Localization (3D-TAL), which involves recognizing what actions a person is performing, and when. State-of-the-art 3D-TAL methods employ a two-stage approach in which the action span detection task and the action recognition task are implemented as a cascade. This approach, however, limits the possibility of error-correction. In contrast, we propose LocATe, an end-to-end approach that jointly localizes and recognizes actions in a 3D sequence. Further, unlike existing autoregressive models that focus on modeling the local context in a sequence, LocATe's transformer model is capable of capturing long-term correlations between actions in a sequence. Unlike transformer-based object-detection and classification models which consider image or patch features as input, the input in 3D-TAL is a long sequence of highly correlated frames. To handle the high-dimensional input, we implement an effective input representation, and overcome the diffuse attention across long time horizons by introducing sparse attention in the model. LocATe outperforms previous approaches on the existing PKU-MMD 3D-TAL benchmark (mAP=93.2%). Finally, we argue that benchmark datasets are most useful where there is clear room for performance improvement. To that end, we introduce a new, challenging, and more realistic benchmark dataset, BABEL-TAL-20 (BT20), where the performance of state-of-the-art methods is significantly worse. The dataset and code for the method will be available for research purposes.

</details>

<details>

<summary>2022-03-21 09:01:37 - AnoViT: Unsupervised Anomaly Detection and Localization with Vision Transformer-based Encoder-Decoder</summary>

- *Yunseung Lee, Pilsung Kang*

- `2203.10808v1` - [abs](http://arxiv.org/abs/2203.10808v1) - [pdf](http://arxiv.org/pdf/2203.10808v1)

> Image anomaly detection problems aim to determine whether an image is abnormal, and to detect anomalous areas. These methods are actively used in various fields such as manufacturing, medical care, and intelligent information. Encoder-decoder structures have been widely used in the field of anomaly detection because they can easily learn normal patterns in an unsupervised learning environment and calculate a score to identify abnormalities through a reconstruction error indicating the difference between input and reconstructed images. Therefore, current image anomaly detection methods have commonly used convolutional encoder-decoders to extract normal information through the local features of images. However, they are limited in that only local features of the image can be utilized when constructing a normal representation owing to the characteristics of convolution operations using a filter of fixed size. Therefore, we propose a vision transformer-based encoder-decoder model, named AnoViT, designed to reflect normal information by additionally learning the global relationship between image patches, which is capable of both image anomaly detection and localization. The proposed approach constructs a feature map that maintains the existing location information of individual patches by using the embeddings of all patches passed through multiple self-attention layers. The proposed AnoViT model performed better than the convolution-based model on three benchmark datasets. In MVTecAD, which is a representative benchmark dataset for anomaly localization, it showed improved results on 10 out of 15 classes compared with the baseline. Furthermore, the proposed method showed good performance regardless of the class and type of the anomalous area when localization results were evaluated qualitatively.

</details>

<details>

<summary>2022-03-21 16:32:44 - FGAN: Federated Generative Adversarial Networks for Anomaly Detection in Network Traffic</summary>

- *Sankha Das*

- `2203.11106v1` - [abs](http://arxiv.org/abs/2203.11106v1) - [pdf](http://arxiv.org/pdf/2203.11106v1)

> Over the last two decades, a lot of work has been done in improving network security, particularly in intrusion detection systems (IDS) and anomaly detection. Machine learning solutions have also been employed in IDSs to detect known and plausible attacks in incoming traffic. Parameters such as packet contents, sender IP and sender port, connection duration, etc. have been previously used to train these machine learning models to learn to differentiate genuine traffic from malicious ones. Generative Adversarial Networks (GANs) have been significantly successful in detecting such anomalies, mostly attributed to the adversarial training of the generator and discriminator in an attempt to bypass each other and in turn increase their own power and accuracy. However, in large networks having a wide variety of traffic at possibly different regions of the network and susceptible to a large number of potential attacks, training these GANs for a particular kind of anomaly may make it oblivious to other anomalies and attacks. In addition, the dataset required to train these models has to be made centrally available and publicly accessible, posing the obvious question of privacy of the communications of the respective participants of the network. The solution proposed in this work aims at tackling the above two issues by using GANs in a federated architecture in networks of such scale and capacity. In such a setting, different users of the network will be able to train and customize a centrally available adversarial model according to their own frequently faced conditions. Simultaneously, the member users of the network will also able to gain from the experiences of the other users in the network.

</details>

<details>

<summary>2022-03-22 02:48:40 - Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding</summary>

- *Haojun Jiang, Yuanze Lin, Dongchen Han, Shiji Song, Gao Huang*

- `2203.08481v2` - [abs](http://arxiv.org/abs/2203.08481v2) - [pdf](http://arxiv.org/pdf/2203.08481v2)

> Visual grounding, i.e., localizing objects in images according to natural language queries, is an important topic in visual language understanding. The most effective approaches for this task are based on deep learning, which generally require expensive manually labeled image-query or patch-query pairs. To eliminate the heavy dependence on human annotations, we present a novel method, named Pseudo-Q, to automatically generate pseudo language queries for supervised training. Our method leverages an off-the-shelf object detector to identify visual objects from unlabeled images, and then language queries for these objects are obtained in an unsupervised fashion with a pseudo-query generation module. Then, we design a task-related query prompt module to specifically tailor generated pseudo language queries for visual grounding tasks. Further, in order to fully capture the contextual relationships between images and language queries, we develop a visual-language model equipped with multi-level cross-modality attention mechanism. Extensive experimental results demonstrate that our method has two notable benefits: (1) it can reduce human annotation costs significantly, e.g., 31% on RefCOCO without degrading original model's performance under the fully supervised setting, and (2) without bells and whistles, it achieves superior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all the five datasets we have experimented. Code is available at https://github.com/LeapLabTHU/Pseudo-Q.

</details>

<details>

<summary>2022-03-22 09:25:48 - Occlusion Fields: An Implicit Representation for Non-Line-of-Sight Surface Reconstruction</summary>

- *Javier Grau, Markus Plack, Patrick Haehn, Michael Weinmann, Matthias Hullin*

- `2203.08657v2` - [abs](http://arxiv.org/abs/2203.08657v2) - [pdf](http://arxiv.org/pdf/2203.08657v2)

> Non-line-of-sight reconstruction (NLoS) is a novel indirect imaging modality that aims to recover objects or scene parts outside the field of view from measurements of light that is indirectly scattered off a directly visible, diffuse wall. Despite recent advances in acquisition and reconstruction techniques, the well-posedness of the problem at large, and the recoverability of objects and their shapes in particular, remains an open question. The commonly employed Fermat path criterion is rather conservative with this regard, as it classifies some surfaces as unrecoverable, although they contribute to the signal.   In this paper, we use a simpler necessary criterion for an opaque surface patch to be recoverable. Such piece of surface must be directly visible from some point on the wall, and it must occlude the space behind itself. Inspired by recent advances in neural implicit representations, we devise a new representation and reconstruction technique for NLoS scenes that unifies the treatment of recoverability with the reconstruction itself. Our approach, which we validate on various synthetic and experimental datasets, exhibits interesting properties. Unlike memory-inefficient volumetric representations, ours allows to infer adaptively tessellated surfaces from time-of-flight measurements of moderate resolution. It can further recover features beyond the Fermat path criterion, and it is robust to significant amounts of self-occlusion. We believe that this is the first time that these properties have been achieved in one system that, as an additional benefit, is trainable and hence suited for data-driven approaches.

</details>

<details>

<summary>2022-03-22 12:29:48 - Wisecr: Secure Simultaneous Code Disseminationto Many Batteryless Computational RFID Devices</summary>

- *Yang Su, Michael Chesser, Yansong Gao, Alanson P. Sample, Damith C. Ranasinghe*

- `2103.10671v3` - [abs](http://arxiv.org/abs/2103.10671v3) - [pdf](http://arxiv.org/pdf/2103.10671v3)

> Emerging ultra-low-power tiny scale computing devices in Cyber-Physical Systems %and Internet of Things (IoT) run on harvested energy, are intermittently powered, have limited computational capability, and perform sensing and actuation functions under the control of a dedicated firmware operating without the supervisory control of an operating system. Wirelessly updating or patching the firmware of such devices is inevitable. We consider the challenging problem of simultaneous and secure firmware updates or patching for a typical class of such devices -- Computational Radio Frequency Identification (CRFID) devices. We propose Wisecr, the first secure and simultaneous wireless code dissemination mechanism to multiple devices that prevent malicious code injection attacks and intellectual property (IP) theft, whilst enabling remote attestation of code installation. Importantly, Wisecr is engineered to comply with existing ISO compliant communication protocol standards employed by CRFID devices and systems. We comprehensively evaluate Wisecr's overhead, demonstrate its implementation over standards-compliant protocols, analyze its security and implement an end-to-end realization with popular CRFID devices -- the open-source code is released on GitHub.

</details>

<details>

<summary>2022-03-22 22:33:42 - DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification</summary>

- *Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E. Coupland, Yalin Zheng*

- `2203.12081v1` - [abs](http://arxiv.org/abs/2203.12081v1) - [pdf](http://arxiv.org/pdf/2203.12081v1)

> Multiple instance learning (MIL) has been increasingly used in the classification of histopathology whole slide images (WSIs). However, MIL approaches for this specific classification problem still face unique challenges, particularly those related to small sample cohorts. In these, there are limited number of WSI slides (bags), while the resolution of a single WSI is huge, which leads to a large number of patches (instances) cropped from this slide. To address this issue, we propose to virtually enlarge the number of bags by introducing the concept of pseudo-bags, on which a double-tier MIL framework is built to effectively use the intrinsic features. Besides, we also contribute to deriving the instance probability under the framework of attention-based MIL, and utilize the derivation to help construct and analyze the proposed framework. The proposed method outperforms other latest methods on the CAMELYON-16 by substantially large margins, and is also better in performance on the TCGA lung cancer dataset. The proposed framework is ready to be extended for wider MIL applications. The code is available at: https://github.com/hrzhang1123/DTFD-MIL

</details>

<details>

<summary>2022-03-23 02:16:17 - Online Encrypted Skype Identification Based on an Updating Mechanism</summary>

- *Shi Dong*

- `2203.12141v1` - [abs](http://arxiv.org/abs/2203.12141v1) - [pdf](http://arxiv.org/pdf/2203.12141v1)

> The machine learning algorithm is gaining prominence in traffic identification research as it offers a way to overcome the shortcomings of port-based and deep packet inspection, especially for P2P-based Skype. However,recent studies have focused mainly on traffic identification based on a full-packet dataset, which poses great challenges to identifying online network traffic. This study aims to provide a new flow identification algorithm by taking the sampled flow records as the object. The study constructs flow records from a Skype set as the dataset, considers the inherent NETFLOW and extended flow metrics as features, and uses a fast correlation-based filter algorithm to select highly correlated features. The study also proposes a new NFI method that adopts a Bayesian updating mechanism to improve the classifier model. The experimental results show that the proposed scheme can achieve much better identification performance than existing state-of-the-art traffic identification methods, and a typical feature metric is analyzed in the sampling environment. The NFI method improves identification accuracy and reduces false positives and false negatives compared to other methods.

</details>

<details>

<summary>2022-03-23 02:57:14 - Semi-Supervised Hybrid Spine Network for Segmentation of Spine MR Images</summary>

- *Meiyan Huang, Shuoling Zhou, Xiumei Chen, Haoran Lai, Qianjin Feng*

- `2203.12151v1` - [abs](http://arxiv.org/abs/2203.12151v1) - [pdf](http://arxiv.org/pdf/2203.12151v1)

> Automatic segmentation of vertebral bodies (VBs) and intervertebral discs (IVDs) in 3D magnetic resonance (MR) images is vital in diagnosing and treating spinal diseases. However, segmenting the VBs and IVDs simultaneously is not trivial. Moreover, problems exist, including blurry segmentation caused by anisotropy resolution, high computational cost, inter-class similarity and intra-class variability, and data imbalances. We proposed a two-stage algorithm, named semi-supervised hybrid spine network (SSHSNet), to address these problems by achieving accurate simultaneous VB and IVD segmentation. In the first stage, we constructed a 2D semi-supervised DeepLabv3+ by using cross pseudo supervision to obtain intra-slice features and coarse segmentation. In the second stage, a 3D full-resolution patch-based DeepLabv3+ was built. This model can be used to extract inter-slice information and combine the coarse segmentation and intra-slice features provided from the first stage. Moreover, a cross tri-attention module was applied to compensate for the loss of inter-slice and intra-slice information separately generated from 2D and 3D networks, thereby improving feature representation ability and achieving satisfactory segmentation results. The proposed SSHSNet was validated on a publicly available spine MR image dataset, and remarkable segmentation performance was achieved. Moreover, results show that the proposed method has great potential in dealing with the data imbalance problem. Based on previous reports, few studies have incorporated a semi-supervised learning strategy with a cross attention mechanism for spine segmentation. Therefore, the proposed method may provide a useful tool for spine segmentation and aid clinically in spinal disease diagnoses and treatments. Codes are publicly available at: https://github.com/Meiyan88/SSHSNet.

</details>

<details>

<summary>2022-03-23 18:29:44 - Powerful Physical Adversarial Examples Against Practical Face Recognition Systems</summary>

- *Inderjeet Singh, Toshinori Araki, Kazuya Kakizaki*

- `2203.15498v1` - [abs](http://arxiv.org/abs/2203.15498v1) - [pdf](http://arxiv.org/pdf/2203.15498v1)

> It is well-known that the most existing machine learning (ML)-based safety-critical applications are vulnerable to carefully crafted input instances called adversarial examples (AXs). An adversary can conveniently attack these target systems from digital as well as physical worlds. This paper aims to the generation of robust physical AXs against face recognition systems. We present a novel smoothness loss function and a patch-noise combo attack for realizing powerful physical AXs. The smoothness loss interjects the concept of delayed constraints during the attack generation process, thereby causing better handling of optimization complexity and smoother AXs for the physical domain. The patch-noise combo attack combines patch noise and imperceptibly small noises from different distributions to generate powerful registration-based physical AXs. An extensive experimental analysis found that our smoothness loss results in robust and more transferable digital and physical AXs than the conventional techniques. Notably, our smoothness loss results in a 1.17 and 1.97 times better mean attack success rate (ASR) in physical white-box and black-box attacks, respectively. Our patch-noise combo attack furthers the performance gains and results in 2.39 and 4.74 times higher mean ASR than conventional technique in physical world white-box and black-box attacks, respectively.

</details>

<details>

<summary>2022-03-24 15:06:01 - A Syntax-Guided Edit Decoder for Neural Program Repair</summary>

- *Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong, Lu Zhang*

- `2106.08253v6` - [abs](http://arxiv.org/abs/2106.08253v6) - [pdf](http://arxiv.org/pdf/2106.08253v6)

> Automated Program Repair (APR) helps improve the efficiency of software development and maintenance. Recent APR techniques use deep learning, particularly the encoder-decoder architecture, to generate patches. Though existing DL-based APR approaches have proposed different encoder architectures, the decoder remains to be the standard one, which generates a sequence of tokens one by one to replace the faulty statement. This decoder has multiple limitations: 1) allowing to generate syntactically incorrect programs, 2) inefficiently representing small edits, and 3) not being able to generate project-specific identifiers.   In this paper, we propose Recoder, a syntax-guided edit decoder with placeholder generation. Recoder is novel in multiple aspects: 1) Recoder generates edits rather than modified code, allowing efficient representation of small edits; 2) Recoder is syntax-guided, with the novel provider/decider architecture to ensure the syntactic correctness of the patched program and accurate generation; 3) Recoder generates placeholders that could be instantiated as project-specific identifiers later.   We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2 and 420 additional bugs from Defects4J v2.0. Our results show that Recoder repairs 53 bugs on Defects4J v1.2, which achieves 21.4% improvement over the previous state-of-the-art approach for single-hunk bugs (TBar). Importantly, to our knowledge, Recoder is the first DL-based APR approach that has outperformed the traditional APR approaches on this dataset. Furthermore, Recoder also repairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5% more than TBar (8 bugs) and 850% more than SimFix (2 bugs). This result suggests that Recoder has better generalizability than existing APR approaches.

</details>

<details>

<summary>2022-03-24 17:28:08 - Precipitaion Nowcasting using Deep Neural Network</summary>

- *Mohamed Chafik Bakkay, Mathieu Serrurier, Valentin Kivachuk Burda, Florian Dupuy, Naty Citlali Cabrera-Gutierrez, Michael Zamo, Maud-Alix Mader, Olivier Mestre, Guillaume Oller, Jean-Christophe Jouhaud, Laurent Terray*

- `2203.13263v1` - [abs](http://arxiv.org/abs/2203.13263v1) - [pdf](http://arxiv.org/pdf/2203.13263v1)

> Precipitation nowcasting is of great importance for weather forecast users, for activities ranging from outdoor activities and sports competitions to airport traffic management. In contrast to long-term precipitation forecasts which are traditionally obtained from numerical models, precipitation nowcasting needs to be very fast. It is therefore more challenging to obtain because of this time constraint. Recently, many machine learning based methods had been proposed. We propose the use three popular deep learning models (U-net, ConvLSTM and SVG-LP) trained on two-dimensional precipitation maps for precipitation nowcasting. We proposed an algorithm for patch extraction to obtain high resolution precipitation maps. We proposed a loss function to solve the blurry image issue and to reduce the influence of zero value pixels in precipitation maps.

</details>

<details>

<summary>2022-03-25 04:20:56 - C to Checked C by 3C</summary>

- *Aravind Machiry, John Kastner, Matt McCutchen, Aaron Eline, Kyle Headley, Michael Hicks*

- `2203.13445v1` - [abs](http://arxiv.org/abs/2203.13445v1) - [pdf](http://arxiv.org/pdf/2203.13445v1)

> Owing to the continued use of C (and C++), spatial safety violations (e.g., buffer overflows) still constitute one of today's most dangerous and prevalent security vulnerabilities. To combat these violations, Checked C extends C with bounds-enforced checked pointer types. Checked C is essentially a gradually typed spatially safe C - checked pointers are backwards-binary compatible with legacy pointers, and the language allows them to be added piecemeal, rather than necessarily all at once, so that safety retrofitting can be incremental. This paper presents a semi-automated process for porting a legacy C program to Checked C. The process centers on 3C, a static analysis-based annotation tool. 3C employs two novel static analysis algorithms - typ3c and boun3c - to annotate legacy pointers as checked pointers, and to infer array bounds annotations for pointers that need them. 3C performs a root cause analysis to direct a human developer to code that should be refactored; once done, 3C can be re-run to infer further annotations (and updated root causes). Experiments on 11 programs totaling 319KLoC show 3C to be effective at inferring checked pointer types, and experience with previously and newly ported code finds 3C works well when combined with human-driven refactoring.

</details>

<details>

<summary>2022-03-25 09:55:07 - DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance Improves Out-Of-Distribution Face Identification</summary>

- *Hai Phan, Anh Nguyen*

- `2112.04016v2` - [abs](http://arxiv.org/abs/2112.04016v2) - [pdf](http://arxiv.org/pdf/2112.04016v2)

> Face identification (FI) is ubiquitous and drives many high-stake decisions made by law enforcement. State-of-the-art FI approaches compare two images by taking the cosine similarity between their image embeddings. Yet, such an approach suffers from poor out-of-distribution (OOD) generalization to new types of images (e.g., when a query face is masked, cropped, or rotated) not included in the training set or the gallery. Here, we propose a re-ranking approach that compares two faces using the Earth Mover's Distance on the deep, spatial features of image patches. Our extra comparison stage explicitly examines image similarity at a fine-grained level (e.g., eyes to eyes) and is more robust to OOD perturbations and occlusions than traditional FI. Interestingly, without finetuning feature extractors, our method consistently improves the accuracy on all tested OOD queries: masked, cropped, rotated, and adversarial while obtaining similar results on in-distribution images.

</details>

<details>

<summary>2022-03-25 14:03:29 - Total Energy Shaping with Neural Interconnection and Damping Assignment -- Passivity Based Control</summary>

- *Santiago Sanchez-Escalonilla, Rodolfo Reyes-Baez, Bayu Jayawardhana*

- `2112.12999v2` - [abs](http://arxiv.org/abs/2112.12999v2) - [pdf](http://arxiv.org/pdf/2112.12999v2)

> In this work we exploit the universal approximation property of Neural Networks (NNs) to design interconnection and damping assignment (IDA) passivity-based control (PBC) schemes for fully-actuated mechanical systems in the port-Hamiltonian (pH) framework. To that end, we transform the IDA-PBC method into a supervised learning problem that solves the partial differential matching equations, and fulfills equilibrium assignment and Lyapunov stability conditions. A main consequence of this, is that the output of the learning algorithm has a clear control-theoretic interpretation in terms of passivity and Lyapunov stability. The proposed control design methodology is validated for mechanical systems of one and two degrees-of-freedom via numerical simulations.

</details>

<details>

<summary>2022-03-26 20:45:38 - NUNet: Deep Learning for Non-Uniform Super-Resolution of Turbulent Flows</summary>

- *Octavi Obiols-Sales, Abhinav Vishnu, Nicholas Malaya, Aparna Chandramowlishwaran*

- `2203.14154v1` - [abs](http://arxiv.org/abs/2203.14154v1) - [pdf](http://arxiv.org/pdf/2203.14154v1)

> Deep Learning (DL) algorithms are becoming increasingly popular for the reconstruction of high-resolution turbulent flows (aka super-resolution). However, current DL approaches perform spatially uniform super-resolution - a key performance limiter for scalability of DL-based surrogates for Computational Fluid Dynamics (CFD).   To address the above challenge, we introduce NUNet, a deep learning-based adaptive mesh refinement (AMR) framework for non-uniform super-resolution of turbulent flows. NUNet divides the input low-resolution flow field into patches, scores each patch, and predicts their target resolution. As a result, it outputs a spatially non-uniform flow field, adaptively refining regions of the fluid domain to achieve the target accuracy. We train NUNet with Reynolds-Averaged Navier-Stokes (RANS) solutions from three different canonical flows, namely turbulent channel flow, flat plate, and flow around ellipses. NUNet shows remarkable discerning properties, refining areas with complex flow features, such as near-wall domains and the wake region in flow around solid bodies, while leaving areas with smooth variations (such as the freestream) in the low-precision range. Hence, NUNet demonstrates an excellent qualitative and quantitative alignment with the traditional OpenFOAM AMR solver. Moreover, it reaches the same convergence guarantees as the AMR solver while accelerating it by 3.2-5.5x, including unseen-during-training geometries and boundary conditions, demonstrating its generalization capacities. Due to NUNet's ability to super-resolve only regions of interest, it predicts the same target 1024x1024 spatial resolution 7-28.5x faster than state-of-the-art DL methods and reduces the memory usage by 4.4-7.65x, showcasing improved scalability.

</details>

<details>

<summary>2022-03-28 00:38:17 - Relaxation Labeling Meets GANs: Solving Jigsaw Puzzles with Missing Borders</summary>

- *Marina Khoroshiltseva, Arianna Traviglia, Marcello Pelillo, Sebastiano Vascon*

- `2203.14428v1` - [abs](http://arxiv.org/abs/2203.14428v1) - [pdf](http://arxiv.org/pdf/2203.14428v1)

> This paper proposes JiGAN, a GAN-based method for solving Jigsaw puzzles with eroded or missing borders. Missing borders is a common real-world situation, for example, when dealing with the reconstruction of broken artifacts or ruined frescoes. In this particular condition, the puzzle's pieces do not align perfectly due to the borders' gaps; in this situation, the patches' direct match is unfeasible due to the lack of color and line continuations. JiGAN, is a two-steps procedure that tackles this issue: first, we repair the eroded borders with a GAN-based image extension model and measure the alignment affinity between pieces; then, we solve the puzzle with the relaxation labeling algorithm to enforce consistency in pieces positioning, hence, reconstructing the puzzle. We test the method on a large dataset of small puzzles and on three commonly used benchmark datasets to demonstrate the feasibility of the proposed approach.

</details>

<details>

<summary>2022-03-28 02:26:11 - GradViT: Gradient Inversion of Vision Transformers</summary>

- *Ali Hatamizadeh, Hongxu Yin, Holger Roth, Wenqi Li, Jan Kautz, Daguang Xu, Pavlo Molchanov*

- `2203.11894v3` - [abs](http://arxiv.org/abs/2203.11894v3) - [pdf](http://arxiv.org/pdf/2203.11894v3)

> In this work we demonstrate the vulnerability of vision transformers (ViTs) to gradient-based inversion attacks. During this attack, the original data batch is reconstructed given model weights and the corresponding gradients. We introduce a method, named GradViT, that optimizes random noise into naturally looking images via an iterative process. The optimization objective consists of (i) a loss on matching the gradients, (ii) image prior in the form of distance to batch-normalization statistics of a pretrained CNN model, and (iii) a total variation regularization on patches to guide correct recovery locations. We propose a unique loss scheduling function to overcome local minima during optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and observe unprecedentedly high fidelity and closeness to the original (hidden) data. During the analysis we find that vision transformers are significantly more vulnerable than previously studied CNNs due to the presence of the attention mechanism. Our method demonstrates new state-of-the-art results for gradient inversion in both qualitative and quantitative metrics. Project page at https://gradvit.github.io/.

</details>

<details>

<summary>2022-03-28 17:24:23 - Poisoning and Backdooring Contrastive Learning</summary>

- *Nicholas Carlini, Andreas Terzis*

- `2106.09667v2` - [abs](http://arxiv.org/abs/2106.09667v2) - [pdf](http://arxiv.org/pdf/2106.09667v2)

> Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.

</details>

<details>

<summary>2022-03-29 23:48:10 - ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation</summary>

- *Muhammad Asad, Lucas Fidon, Tom Vercauteren*

- `2201.04584v4` - [abs](http://arxiv.org/abs/2201.04584v4) - [pdf](http://arxiv.org/pdf/2201.04584v4)

> Automatic segmentation of lung lesions associated with COVID-19 in CT images requires large amount of annotated volumes. Annotations mandate expert knowledge and are time-intensive to obtain through fully manual segmentation methods. Additionally, lung lesions have large inter-patient variations, with some pathologies having similar visual appearance as healthy lung tissues. This poses a challenge when applying existing semi-automatic interactive segmentation techniques for data labelling. To address these challenges, we propose an efficient convolutional neural networks (CNNs) that can be learned online while the annotator provides scribble-based interaction. To accelerate learning from only the samples labelled through user-interactions, a patch-based approach is used for training the network. Moreover, we use weighted cross-entropy loss to address the class imbalance that may result from user-interactions. During online inference, the learned network is applied to the whole input volume using a fully convolutional approach. We compare our proposed method with state-of-the-art using synthetic scribbles and show that it outperforms existing methods on the task of annotating lung lesions associated with COVID-19, achieving 16% higher Dice score while reducing execution time by 3$\times$ and requiring 9000 lesser scribbles-based labelled voxels. Due to the online learning aspect, our approach adapts quickly to user input, resulting in high quality segmentation labels. Source code for ECONet is available at: https://github.com/masadcv/ECONet-MONAILabel.

</details>

<details>

<summary>2022-03-30 09:48:01 - POCO: Point Convolution for Surface Reconstruction</summary>

- *Alexandre Boulch, Renaud Marlet*

- `2201.01831v2` - [abs](http://arxiv.org/abs/2201.01831v2) - [pdf](http://arxiv.org/pdf/2201.01831v2)

> Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they loose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. To address these issues, we propose to use point cloud convolutions and compute latent vectors at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. Experiments on both object and scene datasets show that our approach significantly outperforms other methods on most classical metrics, producing finer details and better reconstructing thinner volumes. The code is available at https://github.com/valeoai/POCO.

</details>

<details>

<summary>2022-03-31 19:47:18 - Evolving Image Compositions for Feature Representation Learning</summary>

- *Paola Cascante-Bonilla, Arshdeep Sekhon, Yanjun Qi, Vicente Ordonez*

- `2106.09011v2` - [abs](http://arxiv.org/abs/2106.09011v2) - [pdf](http://arxiv.org/pdf/2106.09011v2)

> Convolutional neural networks for visual recognition require large amounts of training samples and usually benefit from data augmentation. This paper proposes PatchMix, a data augmentation method that creates new samples by composing patches from pairs of images in a grid-like pattern. These new samples are assigned label scores that are proportional to the number of patches borrowed from each image. We then add a set of additional losses at the patch-level to regularize and to encourage good representations at both the patch and image levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior transfer learning capabilities across a wide array of benchmarks. Although PatchMix can rely on random pairings and random grid-like patterns for mixing, we explore evolutionary search as a guiding strategy to jointly discover optimal grid-like patterns and image pairings. For this purpose, we conceive a fitness function that bypasses the need to re-train a model to evaluate each possible choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16).

</details>


## 2022-04

<details>

<summary>2022-04-01 02:19:40 - Federated Learning for the Classification of Tumor Infiltrating Lymphocytes</summary>

- *Ujjwal Baid, Sarthak Pati, Tahsin M. Kurc, Rajarsi Gupta, Erich Bremer, Shahira Abousamra, Siddhesh P. Thakur, Joel H. Saltz, Spyridon Bakas*

- `2203.16622v2` - [abs](http://arxiv.org/abs/2203.16622v2) - [pdf](http://arxiv.org/pdf/2203.16622v2)

> We evaluate the performance of federated learning (FL) in developing deep learning models for analysis of digitized tissue sections. A classification application was considered as the example use case, on quantifiying the distribution of tumor infiltrating lymphocytes within whole slide images (WSIs). A deep learning classification model was trained using 50*50 square micron patches extracted from the WSIs. We simulated a FL environment in which a dataset, generated from WSIs of cancer from numerous anatomical sites available by The Cancer Genome Atlas repository, is partitioned in 8 different nodes. Our results show that the model trained with the federated training approach achieves similar performance, both quantitatively and qualitatively, to that of a model trained with all the training data pooled at a centralized location. Our study shows that FL has tremendous potential for enabling development of more robust and accurate models for histopathology image analysis without having to collect large and diverse training data at a single location.

</details>

<details>

<summary>2022-04-01 08:56:59 - GrowliFlower: An image time series dataset for GROWth analysis of cauLIFLOWER</summary>

- *Jana Kierdorf, Laura Verena Junker-Frohn, Mike Delaney, Mariele Donoso Olave, Andreas Burkart, Hannah Jaenicke, Onno Muller, Uwe Rascher, Ribana Roscher*

- `2204.00294v1` - [abs](http://arxiv.org/abs/2204.00294v1) - [pdf](http://arxiv.org/pdf/2204.00294v1)

> This article presents GrowliFlower, a georeferenced, image-based UAV time series dataset of two monitored cauliflower fields of size 0.39 and 0.60 ha acquired in 2020 and 2021. The dataset contains RGB and multispectral orthophotos from which about 14,000 individual plant coordinates are derived and provided. The coordinates enable the dataset users the extraction of complete and incomplete time series of image patches showing individual plants. The dataset contains collected phenotypic traits of 740 plants, including the developmental stage as well as plant and cauliflower size. As the harvestable product is completely covered by leaves, plant IDs and coordinates are provided to extract image pairs of plants pre and post defoliation, to facilitate estimations of cauliflower head size. Moreover, the dataset contains pixel-accurate leaf and plant instance segmentations, as well as stem annotations to address tasks like classification, detection, segmentation, instance segmentation, and similar computer vision tasks. The dataset aims to foster the development and evaluation of machine learning approaches. It specifically focuses on the analysis of growth and development of cauliflower and the derivation of phenotypic traits to foster the development of automation in agriculture. Two baseline results of instance segmentation at plant and leaf level based on the labeled instance segmentation data are presented. The entire data set is publicly available.

</details>

<details>

<summary>2022-04-01 10:32:41 - The isogeometric collocated contact surface approach</summary>

- *Frederik Fahrendorf, Laura De Lorenzis*

- `2204.00338v1` - [abs](http://arxiv.org/abs/2204.00338v1) - [pdf](http://arxiv.org/pdf/2204.00338v1)

> We propose a frictionless contact formulation for isogeometric analysis, which combines a collocated formulation for the contact surfaces with a standard Galerkin treatment of the bulk. We denote it as isogeometric Collocated Contact Surface (CCS) formulation. The approach is based on a simple pointwise enforcement of the contact constraints, performed in this study with the penalty method. Unlike pointwise (node-to-surface or point-to-surface) contact algorithms in the Galerkin framework, the CCS formulation passes the contact patch test to machine precision by naturally exploiting the favorable properties of isogeometric collocation. Compared with approaches where the discretization of both bulk and contact surfaces is based on collocation, the CCS approach does not need enhancements to remove oscillations for highly non-uniform meshes. With respect to integral contact approaches, the CCS algorithm is less expensive, easier to code and can be added to a pre-existing isogeometric analysis code with minimal effort. Numerical examples in both small and large deformations are investigated to compare the CCS approach with some available contact formulations and to demonstrate its accuracy.

</details>

<details>

<summary>2022-04-01 14:26:09 - Aggregate Processes as Distributed Adaptive Services for the Industrial Internet of Things</summary>

- *Lorenzo Testa, Giorgio Audrito, Ferruccio Damiani, Gianluca Torta*

- `2204.00467v1` - [abs](http://arxiv.org/abs/2204.00467v1) - [pdf](http://arxiv.org/pdf/2204.00467v1)

> The Industrial Internet of Things (IIoT) promises to bring many benefits, including increased productivity, reduced costs, and increased safety to new generation manufacturing plants. The main ingredients of IIoT are the connected, communicating devices directly located in the workshop floor (far edge devices), as well as edge gateways that connect such devices to the Internet and, in particular, to cloud servers. The field of Edge Computing advocates that keeping computations as close as possible to the sources of data can be an effective means of reducing latency, preserving privacy, and improve the overall efficiency of the system, although building systems where (far) edge and cloud nodes cooperate is quite challenging. In the present work we propose the adoption of the Aggregate Programming (AP) paradigm (and, in particular, the "aggregate process" construct) as a way to simplify building distributed, intelligent services at the far edge of an IIoT architecture. We demonstrate the feasibility and efficacy of the approach with simulated experiments on FCPP (a C++ library for AP), and with some basic experiments on physical IIoT boards running an ad-hoc porting of FCPP.

</details>

<details>

<summary>2022-04-03 15:42:57 - TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?</summary>

- *Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, Anelia Angelova*

- `2106.11297v4` - [abs](http://arxiv.org/abs/2106.11297v4) - [pdf](http://arxiv.org/pdf/2106.11297v4)

> In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in images. Our experiments demonstrate strong performance on several challenging benchmarks for both image and video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced compute amount. We obtain comparable results to the state-of-the-arts on ImageNet while being computationally more efficient. We also confirm the effectiveness of the approach on multiple video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD.   The code is available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner

</details>

<details>

<summary>2022-04-04 05:25:51 - Patch Slimming for Efficient Vision Transformers</summary>

- *Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, Dacheng Tao*

- `2106.02852v2` - [abs](http://arxiv.org/abs/2106.02852v2) - [pdf](http://arxiv.org/pdf/2106.02852v2)

> This paper studies the efficiency problem for visual transformers by excavating redundant calculation in given networks. The recent transformer architecture has demonstrated its effectiveness for achieving excellent performance on a series of computer vision tasks. However, similar to that of convolutional neural networks, the huge computational cost of vision transformers is still a severe issue. Considering that the attention mechanism aggregates different patches layer-by-layer, we present a novel patch slimming approach that discards useless patches in a top-down paradigm. We first identify the effective patches in the last layer and then use them to guide the patch selection process of previous layers. For each layer, the impact of a patch on the final output feature is approximated and patches with less impact will be removed. Experimental results on benchmark datasets demonstrate that the proposed method can significantly reduce the computational costs of vision transformers without affecting their performances. For example, over 45% FLOPs of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the ImageNet dataset.

</details>

<details>

<summary>2022-04-04 17:50:41 - MultiMAE: Multi-modal Multi-task Masked Autoencoders</summary>

- *Roman Bachmann, David Mizrahi, Andrei Atanov, Amir Zamir*

- `2204.01678v1` - [abs](http://arxiv.org/abs/2204.01678v1) - [pdf](http://arxiv.org/pdf/2204.01678v1)

> We propose a pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two key aspects: I) it can optionally accept additional modalities of information in the input besides the RGB image (hence "multi-modal"), and II) its training objective accordingly includes predicting multiple outputs besides the RGB image (hence "multi-task").   We make use of masking (across image patches and input modalities) to make training MultiMAE tractable as well as to ensure cross-modality predictive coding is indeed learned by the network. We show this pre-training strategy leads to a flexible, simple, and efficient framework with improved transfer results to downstream tasks. In particular, the same exact pre-trained network can be flexibly used when additional information besides RGB images is available or when no information other than RGB is available - in all configurations yielding competitive to or significantly better results than the baselines. To avoid needing training datasets with multiple modalities and tasks, we train MultiMAE entirely using pseudo labeling, which makes the framework widely applicable to any RGB dataset.   The experiments are performed on multiple transfer tasks (image classification, semantic segmentation, depth estimation) and datasets (ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an intriguingly impressive capability by the model in cross-modal/task predictive coding and transfer.

</details>

<details>

<summary>2022-04-05 16:41:01 - UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation</summary>

- *Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, Daguang Xu*

- `2204.00631v2` - [abs](http://arxiv.org/abs/2204.00631v2) - [pdf](http://arxiv.org/pdf/2204.00631v2)

> Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions

</details>

<details>

<summary>2022-04-05 22:33:41 - Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study</summary>

- *Tatiana Castro VÃ©lez, Raffi Khatchadourian, Mehdi Bagherzadeh, Anita Raja*

- `2201.09953v3` - [abs](http://arxiv.org/abs/2201.09953v3) - [pdf](http://arxiv.org/pdf/2201.09953v3)

> Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code that supports symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development tends to produce DL code that is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, less error-prone imperative DL frameworks encouraging eager execution have emerged but at the expense of run-time performance. While hybrid approaches aim for the "best of both worlds," the challenges in applying them in the real world are largely unknown. We conduct a data-driven analysis of challenges -- and resultant bugs -- involved in writing reliable yet performant imperative DL code by studying 250 open-source projects, consisting of 19.7 MLOC, along with 470 and 446 manually examined code patches and bug reports, respectively. The results indicate that hybridization: (i) is prone to API misuse, (ii) can result in performance degradation -- the opposite of its intention, and (iii) has limited application due to execution mode incompatibility. We put forth several recommendations, best practices, and anti-patterns for effectively hybridizing imperative DL code, potentially benefiting DL practitioners, API designers, tool developers, and educators.

</details>

<details>

<summary>2022-04-07 15:14:21 - Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation</summary>

- *Sangjoon Park, Jong Chul Ye*

- `2204.03500v1` - [abs](http://arxiv.org/abs/2204.03500v1) - [pdf](http://arxiv.org/pdf/2204.03500v1)

> The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and shared learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (FeSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation. Instead of using a CNN based head as in FeSTA, p-FeSTA adopts a randomly permuting simple patch embedder, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging.

</details>

<details>

<summary>2022-04-07 15:34:19 - Distributed Reinforcement Learning for Robot Teams: A Review</summary>

- *Yutong Wang, Mehul Damani, Pamela Wang, Yuhong Cao, Guillaume Sartoretti*

- `2204.03516v1` - [abs](http://arxiv.org/abs/2204.03516v1) - [pdf](http://arxiv.org/pdf/2204.03516v1)

> Purpose of review: Recent advances in sensing, actuation, and computation have opened the door to multi-robot systems consisting of hundreds/thousands of robots, with promising applications to automated manufacturing, disaster relief, harvesting, last-mile delivery, port/airport operations, or search and rescue. The community has leveraged model-free multi-agent reinforcement learning (MARL) to devise efficient, scalable controllers for multi-robot systems (MRS). This review aims to provide an analysis of the state-of-the-art in distributed MARL for multi-robot cooperation.   Recent findings: Decentralized MRS face fundamental challenges, such as non-stationarity and partial observability. Building upon the "centralized training, decentralized execution" paradigm, recent MARL approaches include independent learning, centralized critic, value decomposition, and communication learning approaches. Cooperative behaviors are demonstrated through AI benchmarks and fundamental real-world robotic capabilities such as multi-robot motion/path planning.   Summary: This survey reports the challenges surrounding decentralized model-free MARL for multi-robot cooperation and existing classes of approaches. We present benchmarks and robotic applications along with a discussion on current open avenues for research.

</details>

<details>

<summary>2022-04-07 22:39:10 - Backports: Change Types, Challenges and Strategies</summary>

- *Debasish Chakroborti, Kevin A. Schneider, Chanchal K. Roy*

- `2204.03764v1` - [abs](http://arxiv.org/abs/2204.03764v1) - [pdf](http://arxiv.org/pdf/2204.03764v1)

> Source code repositories allow developers to manage multiple versions (or branches) of a software system. Pull-requests are used to modify a branch, and backporting is a regular activity used to port changes from a current development branch to other versions. In open-source software, backports are common and often need to be adapted by hand, which motivates us to explore backports and backporting challenges and strategies. In our exploration of 68,424 backports from 10 GitHub projects, we found that bug, test, document, and feature changes are commonly backported. We identified a number of backporting challenges, including that backports were inconsistently linked to their original pull-request (49%), that backports had incompatible code (13%), that backports failed to be accepted (10%), and that there were backporting delays (16 days to create, 5 days to merge). We identified some general strategies for addressing backporting issues. We also noted that backporting strategies depend on the project type and that further investigation is needed to determine their suitability. Furthermore, we created the first-ever backports dataset that can be used by other researchers and practitioners for investigating backports and backporting.

</details>

<details>

<summary>2022-04-08 07:56:32 - Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent Large Spatial-Temporal Data-Driven Approach -- Part 1</summary>

- *Deqing Zhai, Xiuju Fu, Xiao Feng Yin, Haiyan Xu, Wanbing Zhang, Ning Li*

- `2204.03899v1` - [abs](http://arxiv.org/abs/2204.03899v1) - [pdf](http://arxiv.org/pdf/2204.03899v1)

> In this study, a novel coordinative scheduling optimization approach is proposed to enhance port efficiency by reducing average wait time and turnaround time. The proposed approach consists of enhanced particle swarm optimization (ePSO) as kernel and augmented firefly algorithm (AFA) as global optimal search. Two paradigm methods of the proposed approach are investigated, which are batch method and rolling horizon method. The experimental results show that both paradigm methods of proposed approach can effectively enhance port efficiency. The average wait time could be significantly reduced by 86.0% - 95.5%, and the average turnaround time could eventually save 38.2% - 42.4% with respect to historical benchmarks. Moreover, the paradigm method of rolling horizon could reduce to 20 mins on running time over 3-month datasets, rather than 4 hrs on batch method at corresponding maximum performance.

</details>

<details>

<summary>2022-04-08 09:30:23 - Optimizing Coordinative Schedules for Tanker Terminals: An Intelligent Large Spatial-Temporal Data-Driven Approach -- Part 2</summary>

- *Deqing Zhai, Xiuju Fu, Xiao Feng Yin, Haiyan Xu, Wanbing Zhang, Ning Li*

- `2204.03955v1` - [abs](http://arxiv.org/abs/2204.03955v1) - [pdf](http://arxiv.org/pdf/2204.03955v1)

> In this study, a novel coordinative scheduling optimization approach is proposed to enhance port efficiency by reducing weighted average turnaround time. The proposed approach is developed as a heuristic algorithm applied and investigated through different observation windows with weekly rolling horizon paradigm method. The experimental results show that the proposed approach is effective and promising on mitigating the turnaround time of vessels. The results demonstrate that largest potential savings of turnaround time (weighted average) are around 17 hours (28%) reduction on baseline of 1-week observation, 45 hours (37%) reduction on baseline of 2-week observation and 70 hours (40%) reduction on baseline of 3-week observation. Even though the experimental results are based on historical datasets, the results potentially present significant benefits if real-time applications were applied under a quadratic computational complexity.

</details>

<details>

<summary>2022-04-08 18:52:45 - PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier</summary>

- *Chong Xiang, Saeed Mahloujifar, Prateek Mittal*

- `2108.09135v2` - [abs](http://arxiv.org/abs/2108.09135v2) - [pdf](http://arxiv.org/pdf/2108.09135v2)

> The adversarial patch attack against image classification models aims to inject adversarially crafted pixels within a restricted image region (i.e., a patch) for inducing model misclassification. This attack can be realized in the physical world by printing and attaching the patch to the victim object; thus, it imposes a real-world threat to computer vision systems. To counter this threat, we design PatchCleanser as a certifiably robust defense against adversarial patches. In PatchCleanser, we perform two rounds of pixel masking on the input image to neutralize the effect of the adversarial patch. This image-space operation makes PatchCleanser compatible with any state-of-the-art image classifier for achieving high accuracy. Furthermore, we can prove that PatchCleanser will always predict the correct class labels on certain images against any adaptive white-box attacker within our threat model, achieving certified robustness. We extensively evaluate PatchCleanser on the ImageNet, ImageNette, CIFAR-10, CIFAR-100, SVHN, and Flowers-102 datasets and demonstrate that our defense achieves similar clean accuracy as state-of-the-art classification models and also significantly improves certified robustness from prior works. Remarkably, PatchCleanser achieves 83.9% top-1 clean accuracy and 62.1% top-1 certified robust accuracy against a 2%-pixel square patch anywhere on the image for the 1000-class ImageNet dataset.

</details>

<details>

<summary>2022-04-09 13:23:32 - A Deep Learning Approach for Predicting Two-dimensional Soil Consolidation Using Physics-Informed Neural Networks (PINN)</summary>

- *Yue Lu, Gang Mei, Francesco Piccialli*

- `2205.05710v1` - [abs](http://arxiv.org/abs/2205.05710v1) - [pdf](http://arxiv.org/pdf/2205.05710v1)

> Soil consolidation is closely related to seepage, stability, and settlement of geotechnical buildings and foundations, and directly affects the use and safety of superstructures. Nowadays, the unidirectional consolidation theory of soils is widely used in certain conditions and approximate calculations. The multi-directional theory of soil consolidation is more reasonable than the unidirectional theory in practical applications, but it is much more complicated in terms of index determination and solution. To address the above problem, in this paper, we propose a deep learning method using physics-informed neural networks (PINN) to predict the excess pore water pressure of two-dimensional soil consolidation. In the proposed method, (1) a fully connected neural network is constructed, (2) the computational domain, partial differential equation (PDE), and constraints are defined to generate data for model training, and (3) the PDE of two-dimensional soil consolidation and the model of the neural network is connected to reduce the loss of the model. The effectiveness of the proposed method is verified by comparison with the numerical solution of PDE for two-dimensional consolidation. Using this method, the excess pore water pressure could be predicted simply and efficiently. In addition, the method was applied to predict the soil excess pore water pressure in the foundation in a real case at Tianjin port, China. The proposed deep learning approach can be used to investigate the large and complex multi-directional soil consolidation.

</details>

<details>

<summary>2022-04-10 17:28:27 - Neural Program Repair with Execution-based Backpropagation</summary>

- *He Ye, Matias Martinez, Martin Monperrus*

- `2105.04123v3` - [abs](http://arxiv.org/abs/2105.04123v3) - [pdf](http://arxiv.org/pdf/2105.04123v3)

> Neural machine translation (NMT) architectures have achieved promising results for automatic program repair. Yet, they have the limitation of generating low-quality patches (e.g., not compilable patches). This is because the existing works only optimize a purely syntactic loss function based on characters and tokens without incorporating program-specific information during neural network weight optimization. In this paper, we propose a novel program repair model called RewardRepair. The core novelty of RewardRepair is to improve NMT-based program repair with a loss function based on program compilation and test execution information, rewarding the network to produce patches that compile and that do not overfit. We conduct several experiments to evaluate RewardRepair showing that it is feasible and effective to use compilation and test execution results to optimize the underlying neural repair model. RewardRepair correctly repairs 207 bugs over four benchmarks. we report on repair success for 121 bugs that are fixed for the first time in the literature. Also, RewardRepair produces up to 45.3% of compilable patches, an improvement over the 39% by the state-of-the-art.

</details>

<details>

<summary>2022-04-11 04:38:34 - SUMD: Super U-shaped Matrix Decomposition Convolutional neural network for Image denoising</summary>

- *QiFan Li*

- `2204.04861v1` - [abs](http://arxiv.org/abs/2204.04861v1) - [pdf](http://arxiv.org/pdf/2204.04861v1)

> In this paper, we propose a novel and efficient CNN-based framework that leverages local and global context information for image denoising. Due to the limitations of convolution itself, the CNN-based method is generally unable to construct an effective and structured global feature representation, usually called the long-distance dependencies in the Transformer-based method. To tackle this problem, we introduce the matrix decomposition module(MD) in the network to establish the global context feature, comparable to the Transformer based method performance. Inspired by the design of multi-stage progressive restoration of U-shaped architecture, we further integrate the MD module into the multi-branches to acquire the relative global feature representation of the patch range at the current stage. Then, the stage input gradually rises to the overall scope and continuously improves the final feature. Experimental results on various image denoising datasets: SIDD, DND, and synthetic Gaussian noise datasets show that our model(SUMD) can produce comparable visual quality and accuracy results with Transformer-based methods.

</details>

<details>

<summary>2022-04-12 02:44:14 - AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</summary>

- *Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov, Nikita Orlov, Humphrey Shi, Gao Huang*

- `2112.14238v2` - [abs](http://arxiv.org/abs/2112.14238v2) - [pdf](http://arxiv.org/pdf/2112.14238v2)

> Recent works have shown that the computational efficiency of video recognition can be significantly improved by reducing the spatial redundancy. As a representative work, the adaptive focus method (AdaFocus) has achieved a favorable trade-off between accuracy and inference speed by dynamically identifying and attending to the informative regions in each video frame. However, AdaFocus requires a complicated three-stage training pipeline (involving reinforcement learning), leading to slow convergence and is unfriendly to practitioners. This work reformulates the training of AdaFocus as a simple one-stage algorithm by introducing a differentiable interpolation-based patch selection operation, enabling efficient end-to-end optimization. We further present an improved training scheme to address the issues introduced by the one-stage formulation, including the lack of supervision, input diversity and training stability. Moreover, a conditional-exit technique is proposed to perform temporal adaptive computation on top of AdaFocus without additional training. Extensive experiments on six benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, and Jester) demonstrate that our model significantly outperforms the original AdaFocus and other competitive baselines, while being considerably more simple and efficient to train. Code is available at https://github.com/LeapLabTHU/AdaFocusV2.

</details>

<details>

<summary>2022-04-12 16:31:27 - A Machine Learning Approach to Determine the Semantic Versioning Type of npm Packages Releases</summary>

- *Rabe Abdalkareem, Md Atique Reza Chowdhury, Emad Shihab*

- `2204.05929v1` - [abs](http://arxiv.org/abs/2204.05929v1) - [pdf](http://arxiv.org/pdf/2204.05929v1)

> Semantic versioning policy is widely used to indicate the level of changes in a package release. Unfortunately, there are many cases where developers do not respect the semantic versioning policy, leading to the breakage of dependent applications. To reduce such cases, we proposed using machine learning (ML) techniques to effectively predict the new release type, i.e., patch, minor, major, in order to properly determine the semantic versioning type. To perform our prediction, we mined and used a number of features about a release, such as the complexity of the changed code, change types, and development activities. We then used four ML classifiers. To evaluate the performance of the proposed ML classifiers, we conducted an empirical study on 31 JavaScript packages containing a total of approximately 6,260 releases. We started by extracting 41 release level features from historical data of packages' source code and repositories. Then, we used four machine learning classifiers, namely XGBoost, Random Forest, Decision Tree, and Logistic Regression. We found that the XGBoost classifiers performed the best, achieving median ROC AUC values of 0.78, 0.69, and 0.74 for major, minor, and patch releases, respectively. We also found that features related to the change types in a release are the best predictors group of features in determining the semantic versioning type. Finally, we studied the generalizability of determining the semantic versioning type by applying cross-package validation. Our results showed that the general classifier achieved median ROC AUC values of 0.76, 0.69, and 0.75 for major, minor, and patch releases.

</details>

<details>

<summary>2022-04-13 17:22:41 - Software Supply Chain Map: How Reuse Networks Expand</summary>

- *Hideaki Hata, Takashi Ishio*

- `2204.06531v1` - [abs](http://arxiv.org/abs/2204.06531v1) - [pdf](http://arxiv.org/pdf/2204.06531v1)

> Clone-and-own is a typical code reuse approach because of its simplicity and efficiency. Cloned software components are maintained independently by a new owner. These clone-and-own operations can be occurred sequentially, that is, cloned components can be cloned again and owned by other new owners on the supply chain. In general, code reuse is not documented well, consequently, appropriate changes like security patches cannot be propagated to descendant software projects. However, the OpenChain Project defined identifying and tracking source code reuses as responsibilities of FLOSS software staffs. Hence supporting source code reuse awareness is in a real need. This paper studies software reuse relations in FLOSS ecosystem. Technically, clone-and-own reuses of source code can be identified by file-level clone set detection. Since change histories are associated with files, we can determine origins and destinations in reusing across multiple software by considering times. By building software supply chain maps, we find that clone-and-own is prevalent in FLOSS development, and set of files are reused widely and repeatedly. These observations open up future challenges of maintaining and tracking global software genealogies.

</details>

<details>

<summary>2022-04-13 23:46:08 - Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations</summary>

- *Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, Pengtao Xie*

- `2202.07800v2` - [abs](http://arxiv.org/abs/2202.07800v2) - [pdf](http://arxiv.org/pdf/2202.07800v2)

> Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit

</details>

<details>

<summary>2022-04-14 06:13:11 - GLAD: Neural Predicate Synthesis to Repair Omission Faults</summary>

- *Sungmin Kang, Shin Yoo*

- `2204.06771v1` - [abs](http://arxiv.org/abs/2204.06771v1) - [pdf](http://arxiv.org/pdf/2204.06771v1)

> Existing template and learning-based APR tools have successfully found patches for many benchmark faults. However, our analysis of existing results shows that omission faults pose a significant challenge to these techniques. For template based approaches, omission faults provide no location to apply templates to; for learning based approaches that formulate repair as Neural Machine Translation (NMT), omission faults similarly do not provide the faulty code to translate. To address these issues, we propose GLAD, a novel learning-based repair technique that specifically targets if-clause synthesis. GLAD does not require a faulty line as it is based on generative Language Models (LMs) instead of machine translation; consequently, it can repair omission faults. GLAD intelligently constrains the language model using a type-based grammar. Further, it efficiently reduces the validation cost by performing dynamic ranking of candidate patches using a debugger. Thanks to the shift from translation to synthesis, GLAD is highly orthogonal to existing techniques: GLAD can correctly fix 16 Defects4J v1.2 faults that previous NMT-based techniques could not, while maintaining a reasonable runtime cost, underscoring its utility as an APR tool and potential to complement existing tools in practice. An inspection of the bugs that GLAD fixes reveals that GLAD can quickly generate expressions that would be challenging for other techniques.

</details>

<details>

<summary>2022-04-14 17:54:13 - Masked Siamese Networks for Label-Efficient Learning</summary>

- *Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas*

- `2204.07141v1` - [abs](http://arxiv.org/abs/2204.07141v1) - [pdf](http://arxiv.org/pdf/2204.07141v1)

> We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our approach matches the representation of an image view containing randomly masked patches to the representation of the original unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures, while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance, on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy, and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark. Our code is publicly available.

</details>

<details>

<summary>2022-04-14 20:13:12 - A Case for Microservices Orchestration Using Workflow Engines</summary>

- *Anas Nadeem, Muhammad Zubair Malik*

- `2204.07210v1` - [abs](http://arxiv.org/abs/2204.07210v1) - [pdf](http://arxiv.org/pdf/2204.07210v1)

> Microservices have become the de-facto software architecture for cloud-native applications. A contentious architectural decision in microservices is to compose them using choreography or orchestration. In choreography, every service works independently, whereas, in orchestration, there is a controller that coordinates service interactions. This paper makes a case for orchestration. The promise of microservices is that each microservice can be independently developed, deployed, tested, upgraded, and scaled. This makes them suitable for systems running on cloud infrastructures. However, microservice-based systems become complicated due to the complex interactions of various services, concurrent events, failing components, developers' lack of global view, and configurations of the environment. This makes maintaining and debugging such systems very challenging. We hypothesize that orchestrated services are easier to debug and to test this we ported the largest publicly available microservices' benchmark TrainTicket, which is implemented using choreography, to a fault-oblivious stateful workflow framework Temporal. We report our experience in porting the code from traditional choreographed microservice architecture to one orchestrated by Temporal and present our initial findings of time to debug the 22 bugs present in the benchmark. Our findings suggest that an effort towards making a transition to orchestrated approach is worthwhile, making the ported code easier to debug.

</details>

<details>

<summary>2022-04-15 12:49:46 - Recovery by discretization corrected particle strength exchange (DC PSE) operators</summary>

- *Benjamin F. Zwick, George C. Bourantas, Farah Alkhatib, Adam Wittek, Karol Miller*

- `2204.14089v1` - [abs](http://arxiv.org/abs/2204.14089v1) - [pdf](http://arxiv.org/pdf/2204.14089v1)

> A new recovery technique based on discretization corrected particle strength exchange (DC PSE) operators is developed in this paper. DC PSE is a collocation method that can be used to compute derivatives directly at nodal points, instead of by projection from Gauss points as is done in many finite element-based recovery techniques. The proposed method is truly meshless and does not require patches of elements to be defined, which makes it generally applicable to point clouds and arbitrary element topologies. Numerical examples show that the proposed method is accurate and robust.

</details>

<details>

<summary>2022-04-15 14:36:57 - Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information</summary>

- *Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, Ruoxi Jia*

- `2204.05255v2` - [abs](http://arxiv.org/abs/2204.05255v2) - [pdf](http://arxiv.org/pdf/2204.05255v2)

> Backdoor attacks insert malicious data into a training set so that, during inference time, it misclassifies inputs that have been patched with a backdoor trigger as the malware specified label. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as "clean-label attacks." Existing clean-label backdoor attacks require knowledge of the entire training set to be effective. Obtaining such knowledge is difficult or impossible because training data are often gathered from multiple sources (e.g., face images from different users). It remains a question whether backdoor attacks still present a real threat.   This paper provides an affirmative answer to this question by designing an algorithm to mount clean-label backdoor attacks based only on the knowledge of representative examples from the target class. With poisoning equal to or less than 0.5% of the target-class data and 0.05% of the training set, we can train a model to classify test examples from arbitrary classes into the target class when the examples are patched with a backdoor trigger. Our attack works well across datasets and models, even when the trigger presents in the physical world.   We explore the space of defenses and find that, surprisingly, our attack can evade the latest state-of-the-art defenses in their vanilla form, or after a simple twist, we can adapt to the downstream defenses. We study the cause of the intriguing effectiveness and find that because the trigger synthesized by our attack contains features as persistent as the original semantic features of the target class, any attempt to remove such triggers would inevitably hurt the model accuracy first.

</details>

<details>

<summary>2022-04-15 18:14:11 - Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification</summary>

- *Zuzanna Szafranowska, Richard Osuala, Bennet Breier, Kaisar Kushibar, Karim Lekadir, Oliver Diaz*

- `2203.04961v2` - [abs](http://arxiv.org/abs/2203.04961v2) - [pdf](http://arxiv.org/pdf/2203.04961v2)

> Early detection of breast cancer in mammography screening via deep-learning based computer-aided detection systems shows promising potential in improving the curability and mortality rates of breast cancer. However, many clinical centres are restricted in the amount and heterogeneity of available data to train such models to (i) achieve promising performance and to (ii) generalise well across acquisition protocols and domains. As sharing data between centres is restricted due to patient privacy concerns, we propose a potential solution: sharing trained generative models between centres as substitute for real patient data. In this work, we use three well known mammography datasets to simulate three different centres, where one centre receives the trained generator of Generative Adversarial Networks (GANs) from the two remaining centres in order to augment the size and heterogeneity of its training dataset. We evaluate the utility of this approach on mammography patch classification on the test set of the GAN-receiving centre using two different classification models, (a) a convolutional neural network and (b) a transformer neural network. Our experiments demonstrate that shared GANs notably increase the performance of both transformer and convolutional classification models and highlight this approach as a viable alternative to inter-centre data sharing.

</details>

<details>

<summary>2022-04-17 00:50:55 - On Reporting Performance and Accuracy Bugs for Deep Learning Frameworks: An Exploratory Study from GitHub</summary>

- *Guoming Long, Tao Chen*

- `2204.07893v1` - [abs](http://arxiv.org/abs/2204.07893v1) - [pdf](http://arxiv.org/pdf/2204.07893v1)

> The tremendous success of Deep Learning (DL) has significantly boosted the number of open-sourced DL frameworks hosted on GitHub. Among others, performance and accuracy bugs are critical factors that affect the reputation of these DL frameworks, therefore understanding the practice of discovering and investigating them for DL is important. In this paper, we conduct an exploratory study on the nature of reporting performance and accuracy bugs bugs for DL frameworks, aiming to improve our knowledge on this topic. Our study covers 10 most popular open-sourced DL frameworks on GitHub (e.g., TensorFlow, Keras, and PyTorch), based on which we sample 664 representative performance and accuracy bugs bug reports out of a total population of 22,522. Through systematic analysis of these samples, our key findings are: (1) low speed is the primary reason that a performance bug related report is submitted but we see no consistent pattern for accuracy related ones; (2) most of the reports are about issues encountered in the training stage; (3) only a small proportion of the reports provide insufficient information to investigate; (4) the majority of the performance and accuracy bugs bug reports (from 69% to 100%) are not related to the actual bug or regarded as unclassified; (5) around 50% of the performance and accuracy bug reports, which indeed reveal bugs, are not resolved by direct patches. Deriving from the above, we discuss a set of actionable implications to the researchers, maintainers, and report submitters on this subject. To promote open science, the labeled dataset has been made publicly available at https://tinyurl.com/4x3tap9w.

</details>

<details>

<summary>2022-04-18 03:46:06 - Simultaneous Multiple-Prompt Guided Generation Using Differentiable Optimal Transport</summary>

- *Yingtao Tian, Marco Cuturi, David Ha*

- `2204.08472v1` - [abs](http://arxiv.org/abs/2204.08472v1) - [pdf](http://arxiv.org/pdf/2204.08472v1)

> Recent advances in deep learning, such as powerful generative models and joint text-image embeddings, have provided the computational creativity community with new tools, opening new perspectives for artistic pursuits. Text-to-image synthesis approaches that operate by generating images from text cues provide a case in point. These images are generated with a latent vector that is progressively refined to agree with text cues. To do so, patches are sampled within the generated image, and compared with the text prompts in the common text-image embedding space; The latent vector is then updated, using gradient descent, to reduce the mean (average) distance between these patches and text cues. While this approach provides artists with ample freedom to customize the overall appearance of images, through their choice in generative models, the reliance on a simple criterion (mean of distances) often causes mode collapse: The entire image is drawn to the average of all text cues, thereby losing their diversity. To address this issue, we propose using matching techniques found in the optimal transport (OT) literature, resulting in images that are able to reflect faithfully a wide diversity of prompts. We provide numerous illustrations showing that OT avoids some of the pitfalls arising from estimating vectors with mean distances, and demonstrate the capacity of our proposed method to perform better in experiments, qualitatively and quantitatively.

</details>

<details>

<summary>2022-04-18 12:17:01 - Magnifying Networks for Images with Billions of Pixels</summary>

- *Neofytos Dimitriou, Ognjen Arandjelovic*

- `2112.06121v2` - [abs](http://arxiv.org/abs/2112.06121v2) - [pdf](http://arxiv.org/pdf/2112.06121v2)

> The shift towards end-to-end deep learning has brought unprecedented advances in many areas of computer vision. However, deep neural networks are trained on images with resolutions that rarely exceed $1,000 \times 1,000$ pixels. The growing use of scanners that create images with extremely high resolutions (average can be $100,000 \times 100,000$ pixels) thereby presents novel challenges to the field. Most of the published methods preprocess high-resolution images into a set of smaller patches, imposing an a priori belief on the best properties of the extracted patches (magnification, field of view, location, etc.). Herein, we introduce Magnifying Networks (MagNets) as an alternative deep learning solution for gigapixel image analysis that does not rely on a preprocessing stage nor requires the processing of billions of pixels. MagNets can learn to dynamically retrieve any part of a gigapixel image, at any magnification level and field of view, in an end-to-end fashion with minimal ground truth (a single global, slide-level label). Our results on the publicly available Camelyon16 and Camelyon17 datasets corroborate to the effectiveness and efficiency of MagNets and the proposed optimization framework for whole slide image classification. Importantly, MagNets process far less patches from each slide than any of the existing approaches ($10$ to $300$ times less).

</details>

<details>

<summary>2022-04-20 12:58:18 - Hephaestus: A large scale multitask dataset towards InSAR understanding</summary>

- *Nikolaos Ioannis Bountos, Ioannis Papoutsis, Dimitrios Michail, Andreas Karavias, Panagiotis Elias, Isaak Parcharidis*

- `2204.09435v1` - [abs](http://arxiv.org/abs/2204.09435v1) - [pdf](http://arxiv.org/pdf/2204.09435v1)

> Synthetic Aperture Radar (SAR) data and Interferometric SAR (InSAR) products in particular, are one of the largest sources of Earth Observation data. InSAR provides unique information on diverse geophysical processes and geology, and on the geotechnical properties of man-made structures. However, there are only a limited number of applications that exploit the abundance of InSAR data and deep learning methods to extract such knowledge. The main barrier has been the lack of a large curated and annotated InSAR dataset, which would be costly to create and would require an interdisciplinary team of experts experienced on InSAR data interpretation. In this work, we put the effort to create and make available the first of its kind, manually annotated dataset that consists of 19,919 individual Sentinel-1 interferograms acquired over 44 different volcanoes globally, which are split into 216,106 InSAR patches. The annotated dataset is designed to address different computer vision problems, including volcano state classification, semantic segmentation of ground deformation, detection and classification of atmospheric signals in InSAR imagery, interferogram captioning, text to InSAR generation, and InSAR image quality assessment.

</details>

<details>

<summary>2022-04-21 13:35:38 - R2-Trans:Fine-Grained Visual Categorization with Redundancy Reduction</summary>

- *Yu Wang, Shuo Ye, Shujian Yu, Xinge You*

- `2204.10095v1` - [abs](http://arxiv.org/abs/2204.10095v1) - [pdf](http://arxiv.org/pdf/2204.10095v1)

> Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories, whose main challenge is the large intraclass diversities and subtle inter-class differences. Existing FGVC methods usually select discriminant regions found by a trained model, which is prone to neglect other potential discriminant information. On the other hand, the massive interactions between the sequence of image patches in ViT make the resulting class-token contain lots of redundant information, which may also impacts FGVC performance. In this paper, we present a novel approach for FGVC, which can simultaneously make use of partial yet sufficient discriminative information in environmental cues and also compress the redundant information in class-token with respect to the target. Specifically, our model calculates the ratio of high-weight regions in a batch, adaptively adjusts the masking threshold and achieves moderate extraction of background information in the input space. Moreover, we also use the Information Bottleneck~(IB) approach to guide our network to learn a minimum sufficient representations in the feature space. Experimental results on three widely-used benchmark datasets verify that our approach can achieve outperforming performance than other state-of-the-art approaches and baseline models.

</details>

<details>

<summary>2022-04-24 01:47:29 - Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography</summary>

- *Binjie Qin, Haohao Mao, Yiming Liu, Jun Zhao, Yisong Lv, Yueqi Zhu, Song Ding, Xu Chen*

- `2204.08466v2` - [abs](http://arxiv.org/abs/2204.08466v2) - [pdf](http://arxiv.org/pdf/2204.08466v2)

> Although robust PCA has been increasingly adopted to extract vessels from X-ray coronary angiography (XCA) images, challenging problems such as inefficient vessel-sparsity modelling, noisy and dynamic background artefacts, and high computational cost still remain unsolved. Therefore, we propose a novel robust PCA unrolling network with sparse feature selection for super-resolution XCA vessel imaging. Being embedded within a patch-wise spatiotemporal super-resolution framework that is built upon a pooling layer and a convolutional long short-term memory network, the proposed network can not only gradually prune complex vessel-like artefacts and noisy backgrounds in XCA during network training but also iteratively learn and select the high-level spatiotemporal semantic information of moving contrast agents flowing in the XCA-imaged vessels. The experimental results show that the proposed method significantly outperforms state-of-the-art methods, especially in the imaging of the vessel network and its distal vessels, by restoring the intensity and geometry profiles of heterogeneous vessels against complex and dynamic backgrounds.

</details>

<details>

<summary>2022-04-24 15:01:53 - Optimize Deep Learning Models for Prediction of Gene Mutations Using Unsupervised Clustering</summary>

- *Zihan Chen, Xingyu Li, Miaomiao Yang, Hong Zhang, Xu Steven Xu*

- `2204.01593v2` - [abs](http://arxiv.org/abs/2204.01593v2) - [pdf](http://arxiv.org/pdf/2204.01593v2)

> Deep learning has become the mainstream methodological choice for analyzing and interpreting whole-slide digital pathology images (WSIs). It is commonly assumed that tumor regions carry most predictive information. In this paper, we proposed an unsupervised clustering-based multiple-instance learning, and apply our method to develop deep-learning models for prediction of gene mutations using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies (CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches could help identify predictive patches, exclude patches lack of predictive information, and therefore improve prediction on gene mutations in all three different cancer types, compared with the WSI based method without selection of image patches and models based on only tumor regions. Additionally, our proposed algorithm outperformed two recently published baseline algorithms leveraging unsupervised clustering to assist model prediction. The unsupervised-clustering-based approach for mutation prediction allows identification of the spatial regions related to mutation of a specific gene via the resolved probability scores, highlighting the heterogeneity of a predicted genotype in the tumor microenvironment. Finally, our study also demonstrated that selection of tumor regions of WSIs is not always the best way to identify patches for prediction of gene mutations, and other tissue types in the tumor micro-environment may provide better prediction ability for gene mutations than tumor tissues.

</details>

<details>

<summary>2022-04-25 12:02:18 - Discovering Gateway Ports in Maritime Using Temporal Graph Neural Network Port Classification</summary>

- *Dogan Altan, Mohammad Etemad, Dusica Marijan, Tetyana Kholodna*

- `2204.11855v1` - [abs](http://arxiv.org/abs/2204.11855v1) - [pdf](http://arxiv.org/pdf/2204.11855v1)

> Vessel navigation is influenced by various factors, such as dynamic environmental factors that change over time or static features such as vessel type or depth of the ocean. These dynamic and static navigational factors impose limitations on vessels, such as long waiting times in regions outside the actual ports, and we call these waiting regions gateway ports. Identifying gateway ports and their associated features such as congestion and available utilities can enhance vessel navigation by planning on fuel optimization or saving time in cargo operation. In this paper, we propose a novel temporal graph neural network (TGNN) based port classification method to enable vessels to discover gateway ports efficiently, thus optimizing their operations. The proposed method processes vessel trajectory data to build dynamic graphs capturing spatio-temporal dependencies between a set of static and dynamic navigational features in the data, and it is evaluated in terms of port classification accuracy on a real-world data set collected from ten vessels operating in Halifax, NS, Canada. The experimental results indicate that our TGNN-based port classification method provides an f-score of 95% in classifying ports.

</details>

<details>

<summary>2022-04-25 22:12:13 - Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch</summary>

- *Zhi Deng, Yang Liu, Hao Pan, Wassim Jabi, Juyong Zhang, Bailin Deng*

- `2201.09367v4` - [abs](http://arxiv.org/abs/2201.09367v4) - [pdf](http://arxiv.org/pdf/2201.09367v4)

> The freeform architectural modeling process often involves two important stages: concept design and digital modeling. In the first stage, architects usually sketch the overall 3D shape and the panel layout on a physical or digital paper briefly. In the second stage, a digital 3D model is created using the sketch as a reference. The digital model needs to incorporate geometric requirements for its components, such as the planarity of panels due to consideration of construction costs, which can make the modeling process more challenging. In this work, we present a novel sketch-based system to bridge the concept design and digital modeling of freeform roof-like shapes represented as planar quadrilateral (PQ) meshes. Our system allows the user to sketch the surface boundary and contour lines under axonometric projection and supports the sketching of occluded regions. In addition, the user can sketch feature lines to provide directional guidance to the PQ mesh layout. Given the 2D sketch input, we propose a deep neural network to infer in real-time the underlying surface shape along with a dense conjugate direction field, both of which are used to extract the final PQ mesh. To train and validate our network, we generate a large synthetic dataset that mimics architect sketching of freeform quadrilateral patches. The effectiveness and usability of our system are demonstrated with quantitative and qualitative evaluation as well as user studies.

</details>

<details>

<summary>2022-04-26 08:56:39 - Mixed Strategies for Security Games with General Defending Requirements</summary>

- *Rufan Bai, Haoxing Lin, Xinyu Yang, Xiaowei Wu, Minming Li, Weijia Jia*

- `2204.12158v1` - [abs](http://arxiv.org/abs/2204.12158v1) - [pdf](http://arxiv.org/pdf/2204.12158v1)

> The Stackelberg security game is played between a defender and an attacker, where the defender needs to allocate a limited amount of resources to multiple targets in order to minimize the loss due to adversarial attack by the attacker. While allowing targets to have different values, classic settings often assume uniform requirements to defend the targets. This enables existing results that study mixed strategies (randomized allocation algorithms) to adopt a compact representation of the mixed strategies.   In this work, we initiate the study of mixed strategies for the security games in which the targets can have different defending requirements. In contrast to the case of uniform defending requirement, for which an optimal mixed strategy can be computed efficiently, we show that computing the optimal mixed strategy is NP-hard for the general defending requirements setting. However, we show that strong upper and lower bounds for the optimal mixed strategy defending result can be derived. We propose an efficient close-to-optimal Patching algorithm that computes mixed strategies that use only few pure strategies. We also study the setting when the game is played on a network and resource sharing is enabled between neighboring targets. Our experimental results demonstrate the effectiveness of our algorithm in several large real-world datasets.

</details>

<details>

<summary>2022-04-28 02:48:04 - Genetic Improvement in the Shackleton Framework for Optimizing LLVM Pass Sequences</summary>

- *Shuyue Stella Li, Hannah Peeler, Andrew N. Sloss, Kenneth N. Reid, Wolfgang Banzhaf*

- `2204.13261v1` - [abs](http://arxiv.org/abs/2204.13261v1) - [pdf](http://arxiv.org/pdf/2204.13261v1)

> Genetic improvement is a search technique that aims to improve a given acceptable solution to a problem. In this paper, we present the novel use of genetic improvement to find problem-specific optimized LLVM pass sequences. We develop a pass-level patch representation in the linear genetic programming framework, Shackleton, to evolve the modifications to be applied to the default optimization pass sequences. Our GI-evolved solution has a mean of 3.7% runtime improvement compared to the -O3 optimization level in the default code generation options which optimizes on runtime. The proposed GI method provides an automatic way to find a problem-specific optimization sequence that improves upon a general solution without any expert domain knowledge. In this paper, we discuss the advantages and limitations of the GI feature in the Shackleton Framework and present our results.

</details>

<details>

<summary>2022-04-28 20:41:49 - Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting--Full Version</summary>

- *Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, Shirui Pan*

- `2204.13767v1` - [abs](http://arxiv.org/abs/2204.13767v1) - [pdf](http://arxiv.org/pdf/2204.13767v1)

> A variety of real-world applications rely on far future information to make decisions, thus calling for efficient and accurate long sequence multivariate time series forecasting. While recent attention-based forecasting models show strong abilities in capturing long-term dependencies, they still suffer from two key limitations. First, canonical self attention has a quadratic complexity w.r.t. the input time series length, thus falling short in efficiency. Second, different variables' time series often have distinct temporal dynamics, which existing studies fail to capture, as they use the same model parameter space, e.g., projection matrices, for all variables' time series, thus falling short in accuracy. To ensure high efficiency and accuracy, we propose Triformer, a triangular, variable-specific attention. (i) Linear complexity: we introduce a novel patch attention with linear complexity. When stacking multiple layers of the patch attentions, a triangular structure is proposed such that the layer sizes shrink exponentially, thus maintaining linear complexity. (ii) Variable-specific parameters: we propose a light-weight method to enable distinct sets of model parameters for different variables' time series to enhance accuracy without compromising efficiency and memory usage. Strong empirical evidence on four datasets from multiple domains justifies our design choices, and it demonstrates that Triformer outperforms state-of-the-art methods w.r.t. both accuracy and efficiency. This is an extended version of "Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting", to appear in IJCAI 2022 [Cirstea et al., 2022a], including additional experimental results.

</details>

<details>

<summary>2022-04-29 00:57:39 - Noise-reducing attention cross fusion learning transformer for histological image classification of osteosarcoma</summary>

- *Liangrui Pan, Hetian Wang, Lian Wang, Boya Ji, Mingting Liu, Mitchai Chongcheawchamnan, Jin Yuan, Shaoliang Peng*

- `2204.13838v1` - [abs](http://arxiv.org/abs/2204.13838v1) - [pdf](http://arxiv.org/pdf/2204.13838v1)

> The degree of malignancy of osteosarcoma and its tendency to metastasize/spread mainly depend on the pathological grade (determined by observing the morphology of the tumor under a microscope). The purpose of this study is to use artificial intelligence to classify osteosarcoma histological images and to assess tumor survival and necrosis, which will help doctors reduce their workload, improve the accuracy of osteosarcoma cancer detection, and make a better prognosis for patients. The study proposes a typical transformer image classification framework by integrating noise reduction convolutional autoencoder and feature cross fusion learning (NRCA-FCFL) to classify osteosarcoma histological images. Noise reduction convolutional autoencoder could well denoise histological images of osteosarcoma, resulting in more pure images for osteosarcoma classification. Moreover, we introduce feature cross fusion learning, which integrates two scale image patches, to sufficiently explore their interactions by using additional classification tokens. As a result, a refined fusion feature is generated, which is fed to the residual neural network for label predictions. We conduct extensive experiments to evaluate the performance of the proposed approach. The experimental results demonstrate that our method outperforms the traditional and deep learning approaches on various evaluation metrics, with an accuracy of 99.17% to support osteosarcoma diagnosis.

</details>

<details>

<summary>2022-04-30 11:34:05 - SOFT: Softmax-free Transformer with Linear Complexity</summary>

- *Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang*

- `2110.11945v3` - [abs](http://arxiv.org/abs/2110.11945v3) - [pdf](http://arxiv.org/pdf/2110.11945v3)

> Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.

</details>


## 2022-05

<details>

<summary>2022-05-01 20:06:00 - Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers</summary>

- *Miguel Saavedra-Ruiz, Sacha Morin, Liam Paull*

- `2203.03682v2` - [abs](http://arxiv.org/abs/2203.03682v2) - [pdf](http://arxiv.org/pdf/2203.03682v2)

> In this work, we consider the problem of learning a perception model for monocular robot navigation using few annotated images. Using a Vision Transformer (ViT) pretrained with a label-free self-supervised method, we successfully train a coarse image segmentation model for the Duckietown environment using 70 training images. Our model performs coarse image segmentation at the 8x8 patch level, and the inference resolution can be adjusted to balance prediction granularity and real-time perception constraints. We study how best to adapt a ViT to our task and environment, and find that some lightweight architectures can yield good single-image segmentation at a usable frame rate, even on CPU. The resulting perception model is used as the backbone for a simple yet robust visual servoing agent, which we deploy on a differential drive mobile robot to perform two tasks: lane following and obstacle avoidance.

</details>

<details>

<summary>2022-05-02 14:59:39 - Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection</summary>

- *Jiang Liu, Alexander Levine, Chun Pong Lau, Rama Chellappa, Soheil Feizi*

- `2112.04532v2` - [abs](http://arxiv.org/abs/2112.04532v2) - [pdf](http://arxiv.org/pdf/2112.04532v2)

> Object detection plays a key role in many security-critical systems. Adversarial patch attacks, which are easy to implement in the physical world, pose a serious threat to state-of-the-art object detectors. Developing reliable defenses for object detectors against patch attacks is critical but severely understudied. In this paper, we propose Segment and Complete defense (SAC), a general framework for defending object detectors against patch attacks through detection and removal of adversarial patches. We first train a patch segmenter that outputs patch masks which provide pixel-level localization of adversarial patches. We then propose a self adversarial training algorithm to robustify the patch segmenter. In addition, we design a robust shape completion algorithm, which is guaranteed to remove the entire patch from the images if the outputs of the patch segmenter are within a certain Hamming distance of the ground-truth patch masks. Our experiments on COCO and xView datasets demonstrate that SAC achieves superior robustness even under strong adaptive attacks with no reduction in performance on clean images, and generalizes well to unseen patch shapes, attack budgets, and unseen attack methods. Furthermore, we present the APRICOT-Mask dataset, which augments the APRICOT dataset with pixel-level annotations of adversarial patches. We show SAC can significantly reduce the targeted attack success rate of physical patch attacks. Our code is available at https://github.com/joellliu/SegmentAndComplete.

</details>

<details>

<summary>2022-05-04 02:29:40 - DEAR: A Novel Deep Learning-based Approach for Automated Program Repair</summary>

- *Yi Li, Shaohua Wang, Tien N. Nguyen*

- `2205.01859v1` - [abs](http://arxiv.org/abs/2205.01859v1) - [pdf](http://arxiv.org/pdf/2205.01859v1)

> The existing deep learning (DL)-based automated program repair (APR) models are limited in fixing general software defects. % We present {\tool}, a DL-based approach that supports fixing for the general bugs that require dependent changes at once to one or multiple consecutive statements in one or multiple hunks of code. % We first design a novel fault localization (FL) technique for multi-hunk, multi-statement fixes that combines traditional spectrum-based (SB) FL with deep learning and data-flow analysis. It takes the buggy statements returned by the SBFL model, detects the buggy hunks to be fixed at once, and expands a buggy statement $s$ in a hunk to include other suspicious statements around $s$. We design a two-tier, tree-based LSTM model that incorporates cycle training and uses a divide-and-conquer strategy to learn proper code transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees. We conducted several experiments to evaluate {\tool} on three datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset, {\tool} outperforms the baselines from 42\%--683\% in terms of the number of auto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes 31--145 more bugs than existing DL-based APR models with the top-1 patches. On CPatMiner dataset, among 667 fixed bugs, there are 169 (25.3\%) multi-hunk/multi-statement bugs. {\tool} fixes 71 and 164 more bugs, including 52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art, DL-based APR models.

</details>

<details>

<summary>2022-05-04 11:54:01 - Repairnator patches programs automatically</summary>

- *Martin Monperrus, Simon Urli, Thomas Durieux, Matias Martinez, Benoit Baudry, Lionel Seinturier*

- `1910.06247v2` - [abs](http://arxiv.org/abs/1910.06247v2) - [pdf](http://arxiv.org/pdf/1910.06247v2)

> Repairnator is a bot. It constantly monitors software bugs discovered during continuous integration of open-source software and tries to fix them automatically. If it succeeds in synthesizing a valid patch, Repairnator proposes the patch to the human developers, disguised under a fake human identity. To date, Repairnator has been able to producepatches that were accepted by the human developers and permanently merged into the code base. This is a milestone for human-competitiveness in software engineering research on automatic program repair.

</details>

<details>

<summary>2022-05-05 13:33:17 - 1-to-1 or 1-to-n? Investigating the effect of function inlining on binary similarity analysis</summary>

- *Ang Jia, Ming Fan, Wuxia Jin, Xi Xu, Zhaohui Zhou, Qiyi Tang, Sen Nie, Shi Wu, Ting Liu*

- `2112.12928v2` - [abs](http://arxiv.org/abs/2112.12928v2) - [pdf](http://arxiv.org/pdf/2112.12928v2)

> Binary similarity analysis is critical to many code-reuse-related issues and "1-to-1" mechanism is widely applied, where one function in a binary file is matched against one function in a source file or binary file. However, we discover that function mapping is a more complex problem of "1-to-n" or even "n-to-n" due to the existence of function inlining.   In this paper, we investigate the effect of function inlining on binary similarity analysis. We first construct 4 inlining-oriented datasets for four similarity analysis tasks, including code search, OSS reuse detection, vulnerability detection, and patch presence test. Then, we further study the extent of function inlining, the performance of existing works under function inlining, and the effectiveness of existing inlining-simulation strategies. Results show that the proportion of function inlining can reach nearly 70%, while most existing works neglect it and use "1-to-1" mechanism. The mismatches cause a 30% loss in performance during code search and a 40% loss during vulnerability detection. Moreover, two existing inlining-simulation strategies can only recover 60% of the inlined functions. We discover that inlining is usually cumulative when optimization increases. Conditional inlining and incremental inlining are suggested to design low-cost and high-coverage inlining-simulation strategies.

</details>

<details>

<summary>2022-05-05 14:20:29 - A Deep Reinforcement Learning Framework for Rapid Diagnosis of Whole Slide Pathological Images</summary>

- *Tingting Zheng, Weixing chen, Shuqin Li, Hao Quan, Qun Bai, Tianhang Nan, Song Zheng, Xinghua Gao, Yue Zhao, Xiaoyu Cui*

- `2205.02850v1` - [abs](http://arxiv.org/abs/2205.02850v1) - [pdf](http://arxiv.org/pdf/2205.02850v1)

> The deep neural network is a research hotspot for histopathological image analysis, which can improve the efficiency and accuracy of diagnosis for pathologists or be used for disease screening. The whole slide pathological image can reach one gigapixel and contains abundant tissue feature information, which needs to be divided into a lot of patches in the training and inference stages. This will lead to a long convergence time and large memory consumption. Furthermore, well-annotated data sets are also in short supply in the field of digital pathology. Inspired by the pathologist's clinical diagnosis process, we propose a weakly supervised deep reinforcement learning framework, which can greatly reduce the time required for network inference. We use neural network to construct the search model and decision model of reinforcement learning agent respectively. The search model predicts the next action through the image features of different magnifications in the current field of view, and the decision model is used to return the predicted probability of the current field of view image. In addition, an expert-guided model is constructed by multi-instance learning, which not only provides rewards for search model, but also guides decision model learning by the knowledge distillation method. Experimental results show that our proposed method can achieve fast inference and accurate prediction of whole slide images without any pixel-level annotations.

</details>

<details>

<summary>2022-05-05 14:56:19 - Negative Evidence Matters in Interpretable Histology Image Classification</summary>

- *Soufiane Belharbi, Marco Pedersoli, Ismail Ben Ayed, Luke McCaffrey, Eric Granger*

- `2201.02445v3` - [abs](http://arxiv.org/abs/2201.02445v3) - [pdf](http://arxiv.org/pdf/2201.02445v3)

> Using only global image-class labels, weakly-supervised learning methods, such as class activation mapping, allow training CNNs to jointly classify an image, and locate regions of interest associated with the predicted class. However, without any guidance at the pixel level, such methods may yield inaccurate regions. This problem is known to be more challenging with histology images than with natural ones, since objects are less salient, structures have more variations, and foreground and background regions have stronger similarities. Therefore, computer vision methods for visual interpretation of CNNs may not directly apply. In this paper, a simple yet efficient method based on a composite loss is proposed to learn information from the fully negative samples (i.e., samples without positive regions), and thereby reduce false positives/negatives. Our new loss function contains two complementary terms: the first exploits positive evidence collected from the CNN classifier, while the second leverages the fully negative samples from training data. In particular, a pre-trained CNN is equipped with a decoder that allows refining the regions of interest. The CNN is exploited to collect both positive and negative evidence at the pixel level to train the decoder. Our method called NEGEV benefits from the fully negative samples that naturally occur in the data, without any additional supervision signals beyond image-class labels. Extensive experiments show that our proposed method can substantial outperform related state-of-art methods on GlaS (public benchmark for colon cancer), and Camelyon16 (patch-based benchmark for breast cancer using three different backbones). Our results highlight the benefits of using both positive and negative evidence, the first obtained from a classifier, and the other naturally available in datasets.

</details>

<details>

<summary>2022-05-05 23:52:44 - Working memory inspired hierarchical video decomposition with transformative representations</summary>

- *Binjie Qin, Haohao Mao, Ruipeng Zhang, Yueqi Zhu, Song Ding, Xu Chen*

- `2204.10105v3` - [abs](http://arxiv.org/abs/2204.10105v3) - [pdf](http://arxiv.org/pdf/2204.10105v3)

> Video decomposition is very important to extract moving foreground objects from complex backgrounds in computer vision, machine learning, and medical imaging, e.g., extracting moving contrast-filled vessels from the complex and noisy backgrounds of X-ray coronary angiography (XCA). However, the challenges caused by dynamic backgrounds, overlapping heterogeneous environments and complex noises still exist in video decomposition. To solve these problems, this study is the first to introduce a flexible visual working memory model in video decomposition tasks to provide interpretable and high-performance hierarchical deep architecture, integrating the transformative representations between sensory and control layers from the perspective of visual and cognitive neuroscience. Specifically, robust PCA unrolling networks acting as a structure-regularized sensor layer decompose XCA into sparse/low-rank structured representations to separate moving contrast-filled vessels from noisy and complex backgrounds. Then, patch recurrent convolutional LSTM networks with a backprojection module embody unstructured random representations of the control layer in working memory, recurrently projecting spatiotemporally decomposed nonlocal patches into orthogonal subspaces for heterogeneous vessel retrieval and interference suppression. This video decomposition deep architecture effectively restores the heterogeneous profiles of intensity and the geometries of moving objects against the complex background interferences. Experiments show that the proposed method significantly outperforms state-of-the-art methods in accurate moving contrast-filled vessel extraction with excellent flexibility and computational efficiency.

</details>

<details>

<summary>2022-05-09 00:37:23 - Frictional Authors</summary>

- *Devlin Gualtieri*

- `2206.05016v1` - [abs](http://arxiv.org/abs/2206.05016v1) - [pdf](http://arxiv.org/pdf/2206.05016v1)

> I present a method for text analysis based on an analogy with the dynamic friction of sliding surfaces. One surface is an array of points with a 'friction coefficient' derived from the distribution frequency of a text's alphabetic characters. The other surface is a test patch having points with this friction coefficient equal to a median value. Examples are presented from an analysis of a broad range of public domain texts, and comparison is made to the Flesch Reading Ease. Source code for the analysis program is provided.

</details>

<details>

<summary>2022-05-09 14:01:37 - Do You Think You Can Hold Me? The Real Challenge of Problem-Space Evasion Attacks</summary>

- *Harel Berger, Amit Dvir, Chen Hajaj, Rony Ronen*

- `2205.04293v1` - [abs](http://arxiv.org/abs/2205.04293v1) - [pdf](http://arxiv.org/pdf/2205.04293v1)

> Android malware is a spreading disease in the virtual world. Anti-virus and detection systems continuously undergo patches and updates to defend against these threats. Most of the latest approaches in malware detection use Machine Learning (ML). Against the robustifying effort of detection systems, raise the \emph{evasion attacks}, where an adversary changes its targeted samples so that they are misclassified as benign. This paper considers two kinds of evasion attacks: feature-space and problem-space. \emph{Feature-space} attacks consider an adversary who manipulates ML features to evade the correct classification while minimizing or constraining the total manipulations. \textit{Problem-space} attacks refer to evasion attacks that change the actual sample. Specifically, this paper analyzes the gap between these two types in the Android malware domain. The gap between the two types of evasion attacks is examined via the retraining process of classifiers using each one of the evasion attack types. The experiments show that the gap between these two types of retrained classifiers is dramatic and may increase to 96\%. Retrained classifiers of feature-space evasion attacks have been found to be either less effective or completely ineffective against problem-space evasion attacks. Additionally, exploration of different problem-space evasion attacks shows that retraining of one problem-space evasion attack may be effective against other problem-space evasion attacks.

</details>

<details>

<summary>2022-05-10 00:44:21 - Blockchain-assisted Undisclosed IIoT Vulnerabilities Trusted Sharing Protection with Dynamic Token</summary>

- *Wenbo Zhang, Jing Zhang, Yifei Shi, Jingyu Feng*

- `2103.08908v3` - [abs](http://arxiv.org/abs/2103.08908v3) - [pdf](http://arxiv.org/pdf/2103.08908v3)

> With the large-scale deployment of industrial internet of things (IIoT) devices, the number of vulnerabilities that threaten IIoT security is also growing dramatically, including a mass of undisclosed IIoT vulnerabilities that lack mitigation measures. Coordination Vulnerabilities Disclosure (CVD) is one of the most popular vulnerabilities sharing solutions, in which some security workers (SWs) can develop undisclosed vulnerabilities patches together. However, CVD assumes that sharing participants (SWs) are all honest, and thus offering chances for dishonest SWs to leak undisclosed IIoT vulnerabilities. To combat such threats, we propose an Undisclosed IIoT Vulnerabilities Trusted Sharing Protection (UIV-TSP) scheme with dynamic token. In this article, a dynamic token is an implicit access credential for an SW to acquire an undisclosed vulnerability information, which is only held by the system and constantly updated as the SW access. Meanwhile, the latest updated token can be stealthily sneaked into the acquired information as the traceability token. Once the undisclosed vulnerability information leaves the SW host, the embedded self-destruct program will be automatically triggered to prevent leaks since the destination MAC address in the traceability token has changed. To quickly distinguish dishonest SWs, trust mechanism is adopted to evaluate the trust value of SWs. Moreover, we design a blockchain-assisted continuous logs storage method to achieve the tamper-proofing of dynamic token and the transparency of undisclosed IIoT vulnerabilities sharing. The simulation results indicate that our proposed scheme is resilient to suppress dishonest SWs and protect the IoT undisclosed vulnerabilities effectively.

</details>

<details>

<summary>2022-05-11 10:07:55 - REIN-2: Giving Birth to Prepared Reinforcement Learning Agents Using Reinforcement Learning Agents</summary>

- *Aristotelis Lazaridis, Ioannis Vlahavas*

- `2110.05128v2` - [abs](http://arxiv.org/abs/2110.05128v2) - [pdf](http://arxiv.org/pdf/2110.05128v2)

> Deep Reinforcement Learning (Deep RL) has been in the spotlight for the past few years, due to its remarkable abilities to solve problems which were considered to be practically unsolvable using traditional Machine Learning methods. However, even state-of-the-art Deep RL algorithms have various weaknesses that prevent them from being used extensively within industry applications, with one such major weakness being their sample-inefficiency. In an effort to patch these issues, we integrated a meta-learning technique in order to shift the objective of learning to solve a task into the objective of learning how to learn to solve a task (or a set of tasks), which we empirically show that improves overall stability and performance of Deep RL algorithms. Our model, named REIN-2, is a meta-learning scheme formulated within the RL framework, the goal of which is to develop a meta-RL agent (meta-learner) that learns how to produce other RL agents (inner-learners) that are capable of solving given environments. For this task, we convert the typical interaction of an RL agent with the environment into a new, single environment for the meta-learner to interact with. Compared to traditional state-of-the-art Deep RL algorithms, experimental results show remarkable performance of our model in popular OpenAI Gym environments in terms of scoring and sample efficiency, including the Mountain Car hard-exploration environment.

</details>

<details>

<summary>2022-05-12 07:17:56 - Weakly-supervised segmentation of referring expressions</summary>

- *Robin Strudel, Ivan Laptev, Cordelia Schmid*

- `2205.04725v2` - [abs](http://arxiv.org/abs/2205.04725v2) - [pdf](http://arxiv.org/pdf/2205.04725v2)

> Visual grounding localizes regions (boxes or segments) in the image corresponding to given referring expressions. In this work we address image segmentation from referring expressions, a problem that has so far only been addressed in a fully-supervised setting. A fully-supervised setup, however, requires pixel-wise supervision and is hard to scale given the expense of manual annotation. We therefore introduce a new task of weakly-supervised image segmentation from referring expressions and propose Text grounded semantic SEGgmentation (TSEG) that learns segmentation masks directly from image-level referring expressions without pixel-level annotations. Our transformer-based method computes patch-text similarities and guides the classification objective during training with a new multi-label patch assignment mechanism. The resulting visual grounding model segments image regions corresponding to given natural language expressions. Our approach TSEG demonstrates promising results for weakly-supervised referring expression segmentation on the challenging PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.

</details>

<details>

<summary>2022-05-12 13:36:59 - From IP to transport and beyond: cross-layer attacks against applications</summary>

- *Tianxiang Dai, Philipp Jeitner, Haya Shulman, Michael Waidner*

- `2205.06085v1` - [abs](http://arxiv.org/abs/2205.06085v1) - [pdf](http://arxiv.org/pdf/2205.06085v1)

> We perform the first analysis of methodologies for launching DNS cache poisoning: manipulation at the IP layer, hijack of the inter-domain routing and probing open ports via side channels. We evaluate these methodologies against DNS resolvers in the Internet and compare them with respect to effectiveness, applicability and stealth. Our study shows that DNS cache poisoning is a practical and pervasive threat.   We then demonstrate cross-layer attacks that leverage DNS cache poisoning for attacking popular systems, ranging from security mechanisms, such as RPKI, to applications, such as VoIP. In addition to more traditional adversarial goals, most notably impersonation and Denial of Service, we show for the first time that DNS cache poisoning can even enable adversaries to bypass cryptographic defences: we demonstrate how DNS cache poisoning can facilitate BGP prefix hijacking of networks protected with RPKI even when all the other networks apply route origin validation to filter invalid BGP announcements. Our study shows that DNS plays a much more central role in the Internet security than previously assumed.   We recommend mitigations for securing the applications and for preventing cache poisoning.

</details>

<details>

<summary>2022-05-12 15:11:39 - The jsRealB Text Realizer: Organization and Use Cases -- Revised version</summary>

- *Guy Lapalme*

- `2012.15425v2` - [abs](http://arxiv.org/abs/2012.15425v2) - [pdf](http://arxiv.org/pdf/2012.15425v2)

> This paper describes the design principles behind jsRealB (Version 4.0), a surface realizer written JavaScript for English or French sentences from a specification inspired by the constituent syntax formalism but for which a dependency-based input notation is also available. jsRealB can be used either within a web page or as a node.js module. We show that the seemingly simple process of text realization involves many interesting implementation challenges in order to take into account the specifics of each language. jsRealB has a large coverage of English and French and has been used to develop realistic data-to-text applications and to reproduce existing literary texts and sentences from Universal Dependency annotations. Its source code and that of its applications are available on GitHub. The port of this approach to Python (pyrealb) is also presented.

</details>

<details>

<summary>2022-05-12 15:26:20 - Smooth-Reduce: Leveraging Patches for Improved Certified Robustness</summary>

- *Ameya Joshi, Minh Pham, Minsu Cho, Leonid Boytsov, Filipe Condessa, J. Zico Kolter, Chinmay Hegde*

- `2205.06154v1` - [abs](http://arxiv.org/abs/2205.06154v1) - [pdf](http://arxiv.org/pdf/2205.06154v1)

> Randomized smoothing (RS) has been shown to be a fast, scalable technique for certifying the robustness of deep neural network classifiers. However, methods based on RS require augmenting data with large amounts of noise, which leads to significant drops in accuracy. We propose a training-free, modified smoothing approach, Smooth-Reduce, that leverages patching and aggregation to provide improved classifier certificates. Our algorithm classifies overlapping patches extracted from an input image, and aggregates the predicted logits to certify a larger radius around the input. We study two aggregation schemes -- max and mean -- and show that both approaches provide better certificates in terms of certified accuracy, average certified radii and abstention rates as compared to concurrent approaches. We also provide theoretical guarantees for such certificates, and empirically show significant improvements over other randomized smoothing methods that require expensive retraining. Further, we extend our approach to videos and provide meaningful certificates for video classifiers. A project page can be found at https://nyu-dice-lab.github.io/SmoothReduce/

</details>

<details>

<summary>2022-05-13 09:19:55 - Self-Sampling for Neural Point Cloud Consolidation</summary>

- *Gal Metzer, Rana Hanocka, Raja Giryes, Daniel Cohen-Or*

- `2008.06471v3` - [abs](http://arxiv.org/abs/2008.06471v3) - [pdf](http://arxiv.org/pdf/2008.06471v3)

> We introduce a novel technique for neural point cloud consolidation which learns from only the input point cloud. Unlike other point upsampling methods which analyze shapes via local patches, in this work, we learn from global subsets. We repeatedly self-sample the input point cloud with global subsets that are used to train a deep neural network. Specifically, we define source and target subsets according to the desired consolidation criteria (e.g., generating sharp points or points in sparse regions). The network learns a mapping from source to target subsets, and implicitly learns to consolidate the point cloud. During inference, the network is fed with random subsets of points from the input, which it displaces to synthesize a consolidated point set. We leverage the inductive bias of neural networks to eliminate noise and outliers, a notoriously difficult problem in point cloud consolidation. The shared weights of the network are optimized over the entire shape, learning non-local statistics and exploiting the recurrence of local-scale geometries. Specifically, the network encodes the distribution of the underlying shape surface within a fixed set of local kernels, which results in the best explanation of the underlying shape surface. We demonstrate the ability to consolidate point sets from a variety of shapes, while eliminating outliers and noise.

</details>

<details>

<summary>2022-05-13 13:29:13 - Constructing Trajectory and Predicting Estimated Time of Arrival for Long Distance Travelling Vessels: A Probability Density-based Scanning Approach</summary>

- *Deqing Zhai, Xiuju Fu, Xiao Feng Yin, Haiyan Xu, Wanbing Zhang, Ning Li*

- `2205.07945v1` - [abs](http://arxiv.org/abs/2205.07945v1) - [pdf](http://arxiv.org/pdf/2205.07945v1)

> In this study, a probability density-based approach for constructing trajectories is proposed and validated through an typical use-case application: Estimated Time of Arrival (ETA) prediction given origin-destination pairs. The ETA prediction is based on physics and mathematical laws given by the extracted information of probability density-based trajectories constructed. The overall ETA prediction errors are about 0.106 days (i.e. 2.544 hours) on average with 0.549 days (i.e. 13.176 hours) standard deviation, and the proposed approach has an accuracy of 92.08% with 0.959 R-Squared value for overall trajectories between Singapore and Australia ports selected.

</details>

<details>

<summary>2022-05-16 13:35:55 - SGBA: A Stealthy Scapegoat Backdoor Attack against Deep Neural Networks</summary>

- *Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong*

- `2104.01026v3` - [abs](http://arxiv.org/abs/2104.01026v3) - [pdf](http://arxiv.org/pdf/2104.01026v3)

> Outsourced deep neural networks have been demonstrated to suffer from patch-based trojan attacks, in which an adversary poisons the training sets to inject a backdoor in the obtained model so that regular inputs can be still labeled correctly while those carrying a specific trigger are falsely given a target label. Due to the severity of such attacks, many backdoor detection and containment systems have recently, been proposed for deep neural networks. One major category among them are various model inspection schemes, which hope to detect backdoors before deploying models from non-trusted third-parties. In this paper, we show that such state-of-the-art schemes can be defeated by a so-called Scapegoat Backdoor Attack, which introduces a benign scapegoat trigger in data poisoning to prevent the defender from reversing the real abnormal trigger. In addition, it confines the values of network parameters within the same variances of those from clean model during training, which further significantly enhances the difficulty of the defender to learn the differences between legal and illegal models through machine-learning approaches. Our experiments on 3 popular datasets show that it can escape detection by all five state-of-the-art model inspection schemes. Moreover, this attack brings almost no side-effects on the attack effectiveness and guarantees the universal feature of the trigger compared with original patch-based trojan attacks.

</details>

<details>

<summary>2022-05-18 11:46:01 - Predicting Berth Stay for Tanker Terminals: A Systematic and Dynamic Approach</summary>

- *Deqing Zhai, Xiuju Fu, Xiao Feng Yin, Haiyan Xu, Wanbing Zhang*

- `2204.04085v2` - [abs](http://arxiv.org/abs/2204.04085v2) - [pdf](http://arxiv.org/pdf/2204.04085v2)

> Given the trend of digitization and increasing number of maritime transport, prediction of vessel berth stay has been triggered for requirements of operation research and scheduling optimization problem in the era of maritime big data, which takes a significant part in port efficiency and maritime logistics enhancement. This study proposes a systematic and dynamic approach of predicting berth stay for tanker terminals. The approach covers three innovative aspects: 1) Data source employed is multi-faceted, including cargo operation data from tanker terminals, time-series data from automatic identification system (AIS), etc. 2) The process of berth stay is decomposed into multiple blocks according to data analysis and information extraction innovatively, and practical operation scenarios are also developed accordingly. 3) The predictive models of berth stay are developed on the basis of prior data analysis and information extraction under two methods, including regression and decomposed distribution. The models are evaluated under four dynamic scenarios with certain designated cargoes among two different terminals. The evaluation results show that the proposed approach can predict berth stay with the accuracy up to 98.81% validated by historical baselines, and also demonstrate the proposed approach has dynamic capability of predicting berth stay among the scenarios. The model may be potentially applied for short-term pilot-booking or scheduling optimizations within a reasonable time frame for advancement of port intelligence and logistics efficiency.

</details>

<details>

<summary>2022-05-19 13:50:49 - Time Series Generation with Masked Autoencoder</summary>

- *Mengyue Zha, SiuTim Wong, Mengqi Liu, Tong Zhang, Kani Chen*

- `2201.07006v3` - [abs](http://arxiv.org/abs/2201.07006v3) - [pdf](http://arxiv.org/pdf/2201.07006v3)

> This paper shows that masked autoencoder with extrapolator (ExtraMAE) is a scalable self-supervised model for time series generation. ExtraMAE randomly masks some patches of the original time series and learns temporal dynamics by recovering the masked patches. Our approach has two core designs. First, ExtraMAE is self-supervised. Supervision allows ExtraMAE to effectively and efficiently capture the temporal dynamics of the original time series. Second, ExtraMAE proposes an extrapolator to disentangle two jobs of the decoder: recovering latent representations and mapping them back into the feature space. These unique designs enable ExtraMAE to consistently and significantly outperform state-of-the-art (SoTA) benchmarks in time series generation. The lightweight architecture also makes ExtraMAE fast and scalable. ExtraMAE shows outstanding behavior in various downstream tasks such as time series classification, prediction, and imputation. As a self-supervised generative model, ExtraMAE allows explicit management of the synthetic data. We hope this paper will usher in a new era of time series generation with self-supervised models.

</details>

<details>

<summary>2022-05-20 05:53:23 - A Unified and Biologically-Plausible Relational Graph Representation of Vision Transformers</summary>

- *Yuzhong Chen, Yu Du, Zhenxiang Xiao, Lin Zhao, Lu Zhang, David Weizhong Liu, Dajiang Zhu, Tuo Zhang, Xintao Hu, Tianming Liu, Xi Jiang*

- `2206.11073v1` - [abs](http://arxiv.org/abs/2206.11073v1) - [pdf](http://arxiv.org/pdf/2206.11073v1)

> Vision transformer (ViT) and its variants have achieved remarkable successes in various visual tasks. The key characteristic of these ViT models is to adopt different aggregation strategies of spatial patch information within the artificial neural networks (ANNs). However, there is still a key lack of unified representation of different ViT architectures for systematic understanding and assessment of model representation performance. Moreover, how those well-performing ViT ANNs are similar to real biological neural networks (BNNs) is largely unexplored. To answer these fundamental questions, we, for the first time, propose a unified and biologically-plausible relational graph representation of ViT models. Specifically, the proposed relational graph representation consists of two key sub-graphs: aggregation graph and affine graph. The former one considers ViT tokens as nodes and describes their spatial interaction, while the latter one regards network channels as nodes and reflects the information communication between channels. Using this unified relational graph representation, we found that: a) a sweet spot of the aggregation graph leads to ViTs with significantly improved predictive performance; b) the graph measures of clustering coefficient and average path length are two effective indicators of model prediction performance, especially when applying on the datasets with small samples; c) our findings are consistent across various ViT architectures and multiple datasets; d) the proposed relational graph representation of ViT has high similarity with real BNNs derived from brain science data. Overall, our work provides a novel unified and biologically-plausible paradigm for more interpretable and effective representation of ViT ANNs.

</details>

<details>

<summary>2022-05-21 08:49:22 - On the Feasibility and Generality of Patch-based Adversarial Attacks on Semantic Segmentation Problems</summary>

- *Soma Kontar, Andras Horvath*

- `2205.10539v1` - [abs](http://arxiv.org/abs/2205.10539v1) - [pdf](http://arxiv.org/pdf/2205.10539v1)

> Deep neural networks were applied with success in a myriad of applications, but in safety critical use cases adversarial attacks still pose a significant threat. These attacks were demonstrated on various classification and detection tasks and are usually considered general in a sense that arbitrary network outputs can be generated by them.   In this paper we will demonstrate through simple case studies both in simulation and in real-life, that patch based attacks can be utilised to alter the output of segmentation networks. Through a few examples and the investigation of network complexity, we will also demonstrate that the number of possible output maps which can be generated via patch-based attacks of a given size is typically smaller than the area they effect or areas which should be attacked in case of practical applications.   We will prove that based on these results most patch-based attacks cannot be general in practice, namely they can not generate arbitrary output maps or if they could, they are spatially limited and this limit is significantly smaller than the receptive field of the patches.

</details>

<details>

<summary>2022-05-24 18:30:24 - PORTFILER: Port-Level Network Profiling for Self-Propagating Malware Detection</summary>

- *Talha Ongun, Oliver Spohngellert, Benjamin Miller, Simona Boboila, Alina Oprea, Tina Eliassi-Rad, Jason Hiser, Alastair Nottingham, Jack Davidson, Malathi Veeraraghavan*

- `2112.13798v2` - [abs](http://arxiv.org/abs/2112.13798v2) - [pdf](http://arxiv.org/pdf/2112.13798v2)

> Recent self-propagating malware (SPM) campaigns compromised hundred of thousands of victim machines on the Internet. It is challenging to detect these attacks in their early stages, as adversaries utilize common network services, use novel techniques, and can evade existing detection mechanisms. We propose PORTFILER (PORT-Level Network Traffic ProFILER), a new machine learning system applied to network traffic for detecting SPM attacks. PORTFILER extracts port-level features from the Zeek connection logs collected at a border of a monitored network, applies anomaly detection techniques to identify suspicious events, and ranks the alerts across ports for investigation by the Security Operations Center (SOC). We propose a novel ensemble methodology for aggregating individual models in PORTFILER that increases resilience against several evasion strategies compared to standard ML baselines. We extensively evaluate PORTFILER on traffic collected from two university networks, and show that it can detect SPM attacks with different patterns, such as WannaCry and Mirai, and performs well under evasion. Ranking across ports achieves precision over 0.94 with low false positive rates in the top ranked alerts. When deployed on the university networks, PORTFILER detected anomalous SPM-like activity on one of the campus networks, confirmed by the university SOC as malicious. PORTFILER also detected a Mirai attack recreated on the two university networks with higher precision and recall than deep-learning-based autoencoder methods.

</details>

<details>

<summary>2022-05-25 15:41:01 - Image Colorization using U-Net with Skip Connections and Fusion Layer on Landscape Images</summary>

- *Muhammad Hisyam Zayd, Novanto Yudistira, Randy Cahya Wihandika*

- `2205.12867v1` - [abs](http://arxiv.org/abs/2205.12867v1) - [pdf](http://arxiv.org/pdf/2205.12867v1)

> We present a novel technique to automatically colorize grayscale images that combine the U-Net model and Fusion Layer features. This approach allows the model to learn the colorization of images from pre-trained U-Net. Moreover, the Fusion layer is applied to merge local information results dependent on small image patches with global priors of an entire image on each class, forming visually more compelling colorization results. Finally, we validate our approach with a user study evaluation and compare it against state-of-the-art, resulting in improvements.

</details>

<details>

<summary>2022-05-26 11:02:29 - Future Computer Systems and Networking Research in the Netherlands: A Manifesto</summary>

- *Alexandru Iosup, Fernando Kuipers, Ana Lucia Varbanescu, Paola Grosso, Animesh Trivedi, Jan Rellermeyer, Lin Wang, Alexandru Uta, Francesco Regazzoni*

- `2206.03259v1` - [abs](http://arxiv.org/abs/2206.03259v1) - [pdf](http://arxiv.org/pdf/2206.03259v1)

> Our modern society and competitive economy depend on a strong digital foundation and, in turn, on sustained research and innovation in computer systems and networks (CompSys). With this manifesto, we draw attention to CompSys as a vital part of ICT. Among ICT technologies, CompSys covers all the hardware and all the operational software layers that enable applications; only application-specific details, and often only application-specific algorithms, are not part of CompSys. Each of the Top Sectors of the Dutch Economy, each route in the National Research Agenda, and each of the UN Sustainable Development Goals pose challenges that cannot be addressed without groundbreaking CompSys advances. Looking at the 2030-2035 horizon, important new applications will emerge only when enabled by CompSys developments. Triggered by the COVID-19 pandemic, millions moved abruptly online, raising infrastructure scalability and data sovereignty issues; but governments processing social data and responsible social networks still require a paradigm shift in data sovereignty and sharing. AI already requires massive computer systems which can cost millions per training task, but the current technology leaves an unsustainable energy footprint including large carbon emissions. Computational sciences such as bioinformatics, and "Humanities for all" and "citizen data science", cannot become affordable and efficient until computer systems take a generational leap. Similarly, the emerging quantum internet depends on (traditional) CompSys to bootstrap operation for the foreseeable future. Large commercial sectors, including finance and manufacturing, require specialized computing and networking or risk becoming uncompetitive. And, at the core of Dutch innovation, promising technology hubs, deltas, ports, and smart cities, could see their promise stagger due to critical dependency on non-European technology.

</details>

<details>

<summary>2022-05-26 14:15:19 - BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning</summary>

- *Zhenting Wang, Juan Zhai, Shiqing Ma*

- `2205.13383v1` - [abs](http://arxiv.org/abs/2205.13383v1) - [pdf](http://arxiv.org/pdf/2205.13383v1)

> Deep neural networks are vulnerable to Trojan attacks. Existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose stealthy and efficient Trojan attacks, BppAttack. Based on existing biology literature on human visual systems, we propose to use image quantization and dithering as the Trojan trigger, making imperceptible changes. It is a stealthy and efficient attack without training auxiliary models. Due to the small changes made to images, it is hard to inject such triggers during training. To alleviate this problem, we propose a contrastive learning based approach that leverages adversarial attacks to generate negative sample pairs so that the learned trigger is precise and accurate. The proposed method achieves high attack success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. It also effectively bypasses existing Trojan defenses and human inspection. Our code can be found in https://github.com/RU-System-Software-and-Security/BppAttack.

</details>

<details>

<summary>2022-05-27 16:36:45 - Patching Leaks in the Charformer for Efficient Character-Level Generation</summary>

- *Lukas Edman, Antonio Toral, Gertjan van Noord*

- `2205.14086v1` - [abs](http://arxiv.org/abs/2205.14086v1) - [pdf](http://arxiv.org/pdf/2205.14086v1)

> Character-based representations have important advantages over subword-based ones for morphologically rich languages. They come with increased robustness to noisy input and do not need a separate tokenization step. However, they also have a crucial disadvantage: they notably increase the length of text sequences. The GBST method from Charformer groups (aka downsamples) characters to solve this, but allows information to leak when applied to a Transformer decoder. We solve this information leak issue, thereby enabling character grouping in the decoder. We show that Charformer downsampling has no apparent benefits in NMT over previous downsampling methods in terms of translation quality, however it can be trained roughly 30% faster. Promising performance on English--Turkish translation indicate the potential of character-level models for morphologically-rich languages.

</details>

<details>

<summary>2022-05-28 05:13:45 - Object-wise Masked Autoencoders for Fast Pre-training</summary>

- *Jiantao Wu, Shentong Mo*

- `2205.14338v1` - [abs](http://arxiv.org/abs/2205.14338v1) - [pdf](http://arxiv.org/pdf/2205.14338v1)

> Self-supervised pre-training for images without labels has recently achieved promising performance in image classification. The success of transformer-based methods, ViT and MAE, draws the community's attention to the design of backbone architecture and self-supervised task. In this work, we show that current masked image encoding models learn the underlying relationship between all objects in the whole scene, instead of a single object representation. Therefore, those methods bring a lot of compute time for self-supervised pre-training. To solve this issue, we introduce a novel object selection and division strategy to drop non-object patches for learning object-wise representations by selective reconstruction with interested region masks. We refer to this method ObjMAE. Extensive experiments on four commonly-used datasets demonstrate the effectiveness of our model in reducing the compute cost by 72% while achieving competitive performance. Furthermore, we investigate the inter-object and intra-object relationship and find that the latter is crucial for self-supervised pre-training.

</details>

<details>

<summary>2022-05-29 05:30:33 - ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation</summary>

- *Haoxiang Guo, Shilin Liu, Hao Pan, Yang Liu, Xin Tong, Baining Guo*

- `2205.14573v1` - [abs](http://arxiv.org/abs/2205.14573v1) - [pdf](http://arxiv.org/pdf/2205.14573v1)

> We view the reconstruction of CAD models in the boundary representation (B-Rep) as the detection of geometric primitives of different orders, i.e. vertices, edges and surface patches, and the correspondence of primitives, which are holistically modeled as a chain complex, and show that by modeling such comprehensive structures more complete and regularized reconstructions can be achieved. We solve the complex generation problem in two steps. First, we propose a novel neural framework that consists of a sparse CNN encoder for input point cloud processing and a tri-path transformer decoder for generating geometric primitives and their mutual relationships with estimated probabilities. Second, given the probabilistic structure predicted by the neural network, we recover a definite B-Rep chain complex by solving a global optimization maximizing the likelihood under structural validness constraints and applying geometric refinements. Extensive tests on large scale CAD datasets demonstrate that the modeling of B-Rep chain complex structure enables more accurate detection for learning and more constrained reconstruction for optimization, leading to structurally more faithful and complete CAD B-Rep models than previous results.

</details>

<details>

<summary>2022-05-29 15:29:14 - DivSwapper: Towards Diversified Patch-based Arbitrary Style Transfer</summary>

- *Zhizhong Wang, Lei Zhao, Haibo Chen, Zhiwen Zuo, Ailin Li, Wei Xing, Dongming Lu*

- `2101.06381v2` - [abs](http://arxiv.org/abs/2101.06381v2) - [pdf](http://arxiv.org/pdf/2101.06381v2)

> Gram-based and patch-based approaches are two important research lines of style transfer. Recent diversified Gram-based methods have been able to produce multiple and diverse stylized outputs for the same content and style images. However, as another widespread research interest, the diversity of patch-based methods remains challenging due to the stereotyped style swapping process based on nearest patch matching. To resolve this dilemma, in this paper, we dive into the crux of existing patch-based methods and propose a universal and efficient module, termed DivSwapper, for diversified patch-based arbitrary style transfer. The key insight is to use an essential intuition that neural patches with higher activation values could contribute more to diversity. Our DivSwapper is plug-and-play and can be easily integrated into existing patch-based and Gram-based methods to generate diverse results for arbitrary styles. We conduct theoretical analyses and extensive experiments to demonstrate the effectiveness of our method, and compared with state-of-the-art algorithms, it shows superiority in diversity, quality, and efficiency.

</details>

<details>

<summary>2022-05-30 23:20:33 - Few-Shot Diffusion Models</summary>

- *Giorgio Giannone, Didrik Nielsen, Ole Winther*

- `2205.15463v1` - [abs](http://arxiv.org/abs/2205.15463v1) - [pdf](http://arxiv.org/pdf/2205.15463v1)

> Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability. These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class. We empirically show that FSDM can perform few-shot generation and transfer to new datasets. We benchmark variants of our method on complex vision datasets for few-shot learning and compare to unconditional and conditional DDPM baselines. Additionally, we show how conditioning the model on patch-based input set information improves training convergence.

</details>

<details>

<summary>2022-05-31 04:45:59 - Do Customized Android Frameworks Keep Pace with Android?</summary>

- *Pei Liu, Mattia Fazzini, John Grundy, Li Li*

- `2205.15535v1` - [abs](http://arxiv.org/abs/2205.15535v1) - [pdf](http://arxiv.org/pdf/2205.15535v1)

> To satisfy varying customer needs, device vendors and OS providers often rely on the open-source nature of the Android OS and offer customized versions of the Android OS. When a new version of the Android OS is released, device vendors and OS providers need to merge the changes from the Android OS into their customizations to account for its bug fixes, security patches, and new features. Because developers of customized OSs might have made changes to code locations that were also modified by the developers of the Android OS, the merge task can be characterized by conflicts, which can be time-consuming and error-prone to resolve.   To provide more insight into this critical aspect of the Android ecosystem, we present an empirical study that investigates how eight open-source customizations of the Android OS merge the changes from the Android OS into their projects. The study analyzes how often the developers from the customized OSs merge changes from the Android OS, how often the developers experience textual merge conflicts, and the characteristics of these conflicts. Furthermore, to analyze the effect of the conflicts, the study also analyzes how the conflicts can affect a randomly selected sample of 1,000 apps. After analyzing 1,148 merge operations, we identified that developers perform these operations for 9.7\% of the released versions of the Android OS, developers will encounter at least one conflict in 41.3\% of the merge operations, 58.1\% of the conflicts required developers to change the customized OSs, and 64.4\% of the apps considered use at least one method affected by a conflict. In addition to detailing our results, the paper also discusses the implications of our findings and provides insights for researchers and practitioners working with Android and its customizations.

</details>

<details>

<summary>2022-05-31 14:41:01 - Surface Analysis with Vision Transformers</summary>

- *Simon Dahan, Logan Z. J. Williams, Abdulah Fawaz, Daniel Rueckert, Emma C. Robinson*

- `2205.15836v1` - [abs](http://arxiv.org/abs/2205.15836v1) - [pdf](http://arxiv.org/pdf/2205.15836v1)

> The extension of convolutional neural networks (CNNs) to non-Euclidean geometries has led to multiple frameworks for studying manifolds. Many of those methods have shown design limitations resulting in poor modelling of long-range associations, as the generalisation of convolutions to irregular surfaces is non-trivial. Recent state-of-the-art performance of Vision Transformers (ViTs) demonstrates that a general-purpose architecture, which implements self-attention, could replace the local feature learning operations of CNNs. Motivated by the success of attention-modelling in computer vision, we extend ViTs to surfaces by reformulating the task of surface learning as a sequence-to-sequence problem and propose a patching mechanism for surface meshes. We validate the performance of the proposed Surface Vision Transformer (SiT) on two brain age prediction tasks in the developing Human Connectome Project (dHCP) dataset and investigate the impact of pre-training on model performance. Experiments show that the SiT outperforms many surface CNNs, while indicating some evidence of general transformation invariance. Code available at https://github.com/metrics-lab/surface-vision-transformers

</details>

<details>

<summary>2022-05-31 15:45:29 - A robust and lightweight deep attention multiple instance learning algorithm for predicting genetic alterations</summary>

- *Bangwei Guo, Xingyu Li, Miaomiao Yang, Hong Zhang, Xu Steven Xu*

- `2206.00455v1` - [abs](http://arxiv.org/abs/2206.00455v1) - [pdf](http://arxiv.org/pdf/2206.00455v1)

> Deep-learning models based on whole-slide digital pathology images (WSIs) become increasingly popular for predicting molecular biomarkers. Instance-based models has been the mainstream strategy for predicting genetic alterations using WSIs although bag-based models along with self-attention mechanism-based algorithms have been proposed for other digital pathology applications. In this paper, we proposed a novel Attention-based Multiple Instance Mutation Learning (AMIML) model for predicting gene mutations. AMIML was comprised of successive 1-D convolutional layers, a decoder, and a residual weight connection to facilitate further integration of a lightweight attention mechanism to detect the most predictive image patches. Using data for 24 clinically relevant genes from four cancer cohorts in The Cancer Genome Atlas (TCGA) studies (UCEC, BRCA, GBM and KIRC), we compared AMIML with one popular instance-based model and four recently published bag-based models (e.g., CHOWDER, HE2RNA, etc.). AMIML demonstrated excellent robustness, not only outperforming all the five baseline algorithms in the vast majority of the tested genes (17 out of 24), but also providing near-best-performance for the other seven genes. Conversely, the performance of the baseline published algorithms varied across different cancers/genes. In addition, compared to the published models for genetic alterations, AMIML provided a significant improvement for predicting a wide range of genes (e.g., KMT2C, TP53, and SETD2 for KIRC; ERBB2, BRCA1, and BRCA2 for BRCA; JAK1, POLE, and MTOR for UCEC) as well as produced outstanding predictive models for other clinically relevant gene mutations, which have not been reported in the current literature. Furthermore, with the flexible and interpretable attention-based MIL pooling mechanism, AMIML could further zero-in and detect predictive image patches.

</details>

<details>

<summary>2022-05-31 20:25:21 - MAD-EN: Microarchitectural Attack Detection through System-wide Energy Consumption</summary>

- *Debopriya Roy Dipta, Berk Gulmezoglu*

- `2206.00101v1` - [abs](http://arxiv.org/abs/2206.00101v1) - [pdf](http://arxiv.org/pdf/2206.00101v1)

> Microarchitectural attacks have become more threatening the hardware security than before with the increasing diversity of attacks such as Spectre and Meltdown. Vendor patches cannot keep up with the pace of the new threats, which makes the dynamic anomaly detection tools more evident than before. Unfortunately, previous studies utilize hardware performance counters that lead to high performance overhead and profile limited number of microarchitectural attacks due to the small number of counters that can be profiled concurrently. This yields those detection tools inefficient in real-world scenarios.   In this study, we introduce MAD-EN dynamic detection tool that leverages system-wide energy consumption traces collected from a generic Intel RAPL tool to detect ongoing anomalies in a system. In our experiments, we show that CNN-based MAD-EN can detect 10 different microarchitectural attacks with a total of 15 variants with the highest F1 score of 0.999, which makes our tool the most generic attack detection tool so far. Moreover, individual attacks can be distinguished with a 98% accuracy after an anomaly is detected in a system. We demonstrate that MAD-EN introduces 69.3% less performance overhead compared to performance counter-based detection mechanisms.

</details>


## 2022-06

<details>

<summary>2022-06-01 10:41:11 - A comparative study between vision transformers and CNNs in digital pathology</summary>

- *Luca Deininger, Bernhard Stimpel, Anil Yuce, Samaneh Abbasi-Sureshjani, Simon SchÃ¶nenberger, Paolo Ocampo, Konstanty Korski, Fabien Gaire*

- `2206.00389v1` - [abs](http://arxiv.org/abs/2206.00389v1) - [pdf](http://arxiv.org/pdf/2206.00389v1)

> Recently, vision transformers were shown to be capable of outperforming convolutional neural networks when pretrained on sufficient amounts of data. In comparison to convolutional neural networks, vision transformers have a weaker inductive bias and therefore allow a more flexible feature detection. Due to their promising feature detection, this work explores vision transformers for tumor detection in digital pathology whole slide images in four tissue types, and for tissue type identification. We compared the patch-wise classification performance of the vision transformer DeiT-Tiny to the state-of-the-art convolutional neural network ResNet18. Due to the sparse availability of annotated whole slide images, we further compared both models pretrained on large amounts of unlabeled whole-slide images using state-of-the-art self-supervised approaches. The results show that the vision transformer performed slightly better than the ResNet18 for three of four tissue types for tumor detection while the ResNet18 performed slightly better for the remaining tasks. The aggregated predictions of both models on slide level were correlated, indicating that the models captured similar imaging features. All together, the vision transformer models performed on par with the ResNet18 while requiring more effort to train. In order to surpass the performance of convolutional neural networks, vision transformers might require more challenging tasks to benefit from their weak inductive bias.

</details>

<details>

<summary>2022-06-01 18:50:56 - Not so immutable: Upgradeability of Smart Contracts on Ethereum</summary>

- *Mehdi Salehi, Jeremy Clark, Mohammad Mannan*

- `2206.00716v1` - [abs](http://arxiv.org/abs/2206.00716v1) - [pdf](http://arxiv.org/pdf/2206.00716v1)

> A smart contract that is deployed to a blockchain system like Ethereum is, under reasonable circumstances, expected to be immutable and tamper-proof. This is both a feature (promoting integrity and transparency) and a bug (preventing security patches and feature updates). Modern smart contracts use software tricks to enable upgradeability, raising the research questions of how upgradeability is achieved and who is authorized to make changes. In this paper, we summarize and evaluate six upgradeability patterns. We develop a measurement framework for finding how many upgradeable contracts are on Ethereum that use certain prominent upgrade patters. We find 1.4 million proxy contracts which 8,225 of them are unique upgradeable proxy contracts. We also measure how they implement access control over their upgradeability: about 50% are controlled by a single Externally Owned Address (EOA), and about 14% are controlled by multi-signature wallets in which a limited number of persons can change the whole logic of the contract.

</details>

<details>

<summary>2022-06-02 03:06:35 - Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images</summary>

- *Daniel Flores-Araiza, Francisco Lopez-Tiro, Elias Villalvazo-Avila, Jonathan El-Beze, Jacques Hubert, Gilberto Ochoa-Ruiz, Christian Daul*

- `2206.00252v2` - [abs](http://arxiv.org/abs/2206.00252v2) - [pdf](http://arxiv.org/pdf/2206.00252v2)

> Identifying the type of kidney stones can allow urologists to determine their formation cause, improving the early prescription of appropriate treatments to diminish future relapses. However, currently, the associated ex-vivo diagnosis (known as morpho-constitutional analysis, MCA) is time-consuming, expensive, and requires a great deal of experience, as it requires a visual analysis component that is highly operator dependant. Recently, machine learning methods have been developed for in-vivo endoscopic stone recognition. Shallow methods have been demonstrated to be reliable and interpretable but exhibit low accuracy, while deep learning-based methods yield high accuracy but are not explainable. However, high stake decisions require understandable computer-aided diagnosis (CAD) to suggest a course of action based on reasonable evidence, rather than merely prescribe one. Herein, we investigate means for learning part-prototypes (PPs) that enable interpretable models. Our proposal suggests a classification for a kidney stone patch image and provides explanations in a similar way as those used on the MCA method.

</details>

<details>

<summary>2022-06-02 14:20:22 - FV-UPatches: Enhancing Universality in Finger Vein Recognition</summary>

- *Ziyan Chen, Jiazhen Liu, Changwen Cao, Changlong Jin, Hakil Kim*

- `2206.01061v1` - [abs](http://arxiv.org/abs/2206.01061v1) - [pdf](http://arxiv.org/pdf/2206.01061v1)

> Many deep learning-based models have been introduced in finger vein recognition in recent years. These solutions, however, suffer from data dependency and are difficult to achieve model generalization. To address this problem, we are inspired by the idea of domain adaptation and propose a universal learning-based framework, which achieves generalization while training with limited data. To reduce differences between data distributions, a compressed U-Net is introduced as a domain mapper to map the raw region of interest image onto a target domain. The concentrated target domain is a unified feature space for the subsequent matching, in which a local descriptor model SOSNet is employed to embed patches into descriptors measuring the similarity of matching pairs. In the proposed framework, the domain mapper is an approximation to a specific extraction function thus the training is only a one-time effort with limited data. Moreover, the local descriptor model can be trained to be representative enough based on a public dataset of non-finger-vein images. The whole pipeline enables the framework to be well generalized, making it possible to enhance universality and helps to reduce costs of data collection, tuning and retraining. The comparable experimental results to state-of-the-art (SOTA) performance in five public datasets prove the effectiveness of the proposed framework. Furthermore, the framework shows application potential in other vein-based biometric recognition as well.

</details>

<details>

<summary>2022-06-03 03:53:59 - An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Location Deep Features</summary>

- *Ziquan Wei, Shenghua Cheng, Junbo Hu, Li Chen, Shaoqun Zeng, Xiuli Liu*

- `2106.15113v3` - [abs](http://arxiv.org/abs/2106.15113v3) - [pdf](http://arxiv.org/pdf/2106.15113v3)

> Digital gigapixel whole slide image (WSI) is widely used in clinical diagnosis, and automated WSI analysis is key for computer-aided diagnosis. Currently, analyzing the integrated descriptor of probabilities or feature maps from massive local patches encoded by ResNet classifier is the main manner for WSI-level prediction. Feature representations of the sparse and tiny lesion cells in cervical slides, however, are still challenging, while the unused location representations are available to supply the semantics classification. This study designs a novel and efficient framework with a new module InCNet constructed lightweight model YOLCO (You Only Look Cytology Once). It directly extracts feature inside the single cell (cluster) instead of the traditional way that from image tile with a fixed size. The InCNet (Inline Connection Network) enriches the multi-scale connectivity without efficiency loss. The proposal allows the input size enlarged to megapixel that can stitch the WSI by the average repeats decreased from $10^3\sim10^4$ to $10^1\sim10^2$ for collecting features and predictions at two scales. Based on Transformer for classifying the integrated multi-scale multi-task WSI features, the experimental results appear $0.872$ AUC score better than the best conventional model on our dataset ($n$=2,019) from four scanners. The code is available at https://github.com/Chrisa142857/You-Only-Look-Cytopathology-Once , where the deployment version has the speed $\sim$70 s/WSI.

</details>

<details>

<summary>2022-06-03 06:23:03 - Wigner-Smith Time Delay Matrix for Electromagnetics: Guiding and Periodic Systems with Evanescent Modes</summary>

- *Yiqian Mao, Utkarsh R. Patel, Eric Michielssen*

- `2206.02571v1` - [abs](http://arxiv.org/abs/2206.02571v1) - [pdf](http://arxiv.org/pdf/2206.02571v1)

> The Wigner-Smith (WS) time delay matrix relates an electromagnetic system's scattering matrix and its frequency derivative. Previous work showed that the entries of WS time delay matrices of systems excited by propagating waves consist of volume integrals of energy-like field quantities. This paper introduces a generalized WS relationship that applies to systems excited by mixtures of propagating and evanescent fields. Just like its predecessor, the generalized WS relationship allows for the identification of so-called WS modes that interact with the system with well-defined time delays. Furthermore, a technique is developed to compute the WS time delay matrix of a composite system from the WS time delay matrices of its subsystems. Numerical examples demonstrate the usefulness of the generalized WS method when characterizing time delays experienced by fields interacting with guiding and periodic structures that have ports supporting evanescent modes.

</details>

<details>

<summary>2022-06-06 07:26:41 - Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling</summary>

- *Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu*

- `2111.14819v2` - [abs](http://arxiv.org/abs/2111.14819v2) - [pdf](http://arxiv.org/pdf/2111.14819v2)

> We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT

</details>

<details>

<summary>2022-06-06 15:31:35 - Separable Self-attention for Mobile Vision Transformers</summary>

- *Sachin Mehta, Mohammad Rastegari*

- `2206.02680v1` - [abs](http://arxiv.org/abs/2206.02680v1) - [pdf](http://arxiv.org/pdf/2206.02680v1)

> Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires $O(k^2)$ time complexity with respect to the number of tokens (or patches) $k$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. $O(k)$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running $3.2\times$ faster on a mobile device.   Our source code is available at: \url{https://github.com/apple/ml-cvnets}

</details>

<details>

<summary>2022-06-07 07:40:05 - Patch-based image Super Resolution using generalized Gaussian mixture model</summary>

- *Dang-Phuong-Lan Nguyen, Jean-FranÃ§ois Aujol, Yannick Berthoumieu*

- `2206.03069v1` - [abs](http://arxiv.org/abs/2206.03069v1) - [pdf](http://arxiv.org/pdf/2206.03069v1)

> Single Image Super Resolution (SISR) methods aim to recover the clean images in high resolution from low resolution observations.A family of patch-based approaches have received considerable attention and development. The minimum mean square error (MMSE) methodis a powerful image restoration method that uses a probability model on the patches of images. This paper proposes an algorithm to learn a jointgeneralized Gaussian mixture model (GGMM) from a pair of the low resolution patches and the corresponding high resolution patches fromthe reference data. We then reconstruct the high resolution image based on the MMSE method. Our numerical evaluations indicate that theMMSE-GGMM method competes with other state of the art methods.

</details>

<details>

<summary>2022-06-07 12:07:18 - Deep Neural Patchworks: Coping with Large Segmentation Tasks</summary>

- *Marco Reisert, Maximilian Russe, Samer Elsheikh, Elias Kellner, Henrik Skibbe*

- `2206.03210v1` - [abs](http://arxiv.org/abs/2206.03210v1) - [pdf](http://arxiv.org/pdf/2206.03210v1)

> Convolutional neural networks are the way to solve arbitrary image segmentation tasks. However, when images are large, memory demands often exceed the available resources, in particular on a common GPU. Especially in biomedical imaging, where 3D images are common, the problems are apparent. A typical approach to solve this limitation is to break the task into smaller subtasks by dividing images into smaller image patches. Another approach, if applicable, is to look at the 2D image sections separately, and to solve the problem in 2D. Often, the loss of global context makes such approaches less effective; important global information might not be present in the current image patch, or the selected 2D image section. Here, we propose Deep Neural Patchworks (DNP), a segmentation framework that is based on hierarchical and nested stacking of patch-based networks that solves the dilemma between global context and memory limitations.

</details>

<details>

<summary>2022-06-07 13:31:11 - DeepMTS: Deep Multi-task Learning for Survival Prediction in Patients with Advanced Nasopharyngeal Carcinoma using Pretreatment PET/CT</summary>

- *Mingyuan Meng, Bingxin Gu, Lei Bi, Shaoli Song, David Dagan Feng, Jinman Kim*

- `2109.07711v2` - [abs](http://arxiv.org/abs/2109.07711v2) - [pdf](http://arxiv.org/pdf/2109.07711v2)

> Nasopharyngeal Carcinoma (NPC) is a malignant epithelial cancer arising from the nasopharynx. Survival prediction is a major concern for NPC patients, as it provides early prognostic information to plan treatments. Recently, deep survival models based on deep learning have demonstrated the potential to outperform traditional radiomics-based survival prediction models. Deep survival models usually use image patches covering the whole target regions (e.g., nasopharynx for NPC) or containing only segmented tumor regions as the input. However, the models using the whole target regions will also include non-relevant background information, while the models using segmented tumor regions will disregard potentially prognostic information existing out of primary tumors (e.g., local lymph node metastasis and adjacent tissue invasion). In this study, we propose a 3D end-to-end Deep Multi-Task Survival model (DeepMTS) for joint survival prediction and tumor segmentation in advanced NPC from pretreatment PET/CT. Our novelty is the introduction of a hard-sharing segmentation backbone to guide the extraction of local features related to the primary tumors, which reduces the interference from non-relevant background information. In addition, we also introduce a cascaded survival network to capture the prognostic information existing out of primary tumors and further leverage the global tumor information (e.g., tumor size, shape, and locations) derived from the segmentation backbone. Our experiments with two clinical datasets demonstrate that our DeepMTS can consistently outperform traditional radiomics-based survival prediction models and existing deep survival models.

</details>

<details>

<summary>2022-06-07 22:16:48 - Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images</summary>

- *Zhenzhen Wang, Carla Saoud, Sintawat Wangsiricharoen, Aaron W. James, Aleksander S. Popel, Jeremias Sulam*

- `2109.10778v2` - [abs](http://arxiv.org/abs/2109.10778v2) - [pdf](http://arxiv.org/pdf/2109.10778v2)

> Annotating cancerous regions in whole-slide images (WSIs) of pathology samples plays a critical role in clinical diagnosis, biomedical research, and machine learning algorithms development. However, generating exhaustive and accurate annotations is labor-intensive, challenging, and costly. Drawing only coarse and approximate annotations is a much easier task, less costly, and it alleviates pathologists' workload. In this paper, we study the problem of refining these approximate annotations in digital pathology to obtain more accurate ones. Some previous works have explored obtaining machine learning models from these inaccurate annotations, but few of them tackle the refinement problem where the mislabeled regions should be explicitly identified and corrected, and all of them require a -- often very large -- number of training samples. We present a method, named Label Cleaning Multiple Instance Learning (LC-MIL), to refine coarse annotations on a single WSI without the need of external training data. Patches cropped from a WSI with inaccurate labels are processed jointly within a multiple instance learning framework, mitigating their impact on the predictive model and refining the segmentation. Our experiments on a heterogeneous WSI set with breast cancer lymph node metastasis, liver cancer, and colorectal cancer samples show that LC-MIL significantly refines the coarse annotations, outperforming state-of-the-art alternatives, even while learning from a single slide. Moreover, we demonstrate how real annotations drawn by pathologists can be efficiently refined and improved by the proposed approach. All these results demonstrate that LC-MIL is a promising, light-weight tool to provide fine-grained annotations from coarsely annotated pathology sets.

</details>

<details>

<summary>2022-06-08 07:42:17 - Low-power option Greeks: Efficiency-driven market risk analysis using FPGAs</summary>

- *Mark Klaisoongnoen, Nick Brown, Oliver Thomson Brown*

- `2206.03719v1` - [abs](http://arxiv.org/abs/2206.03719v1) - [pdf](http://arxiv.org/pdf/2206.03719v1)

> Quantitative finance is the use of mathematical models to analyse financial markets and securities. Typically requiring significant amounts of computation, an important question is the role that novel architectures can play in accelerating these models. In this paper we explore the acceleration of the industry standard Securities Technology Analysis Center's (STAC) derivatives risk analysis benchmark STAC-A2\texttrademark{} by porting the Heston stochastic volatility model and Longstaff and Schwartz path reduction onto a Xilinx Alveo U280 FPGA with a focus on efficiency-driven computing.   Describing in detail the steps undertaken to optimise the algorithm for the FPGA, we then leverage the flexibility provided by the reconfigurable architecture to explore choices around numerical precision and representation. Insights gained are then exploited in our final performance and energy measurements, where for the efficiency improvement metric we achieve between an 8 times and 185 times improvement on the FPGA compared to two 24-core Intel Xeon Platinum CPUs. The result of this work is not only a show-case for the market risk analysis workload on FPGAs, but furthermore a set of efficiency driven techniques and lessons learnt that can be applied to quantitative finance and computational workloads on reconfigurable architectures more generally.

</details>

<details>

<summary>2022-06-08 20:16:20 - Deep Estimation of Speckle Statistics Parametric Images</summary>

- *Ali K. Z. Tehrani, Ivan M. Rosado-Mendez, Hassan Rivaz*

- `2206.04145v1` - [abs](http://arxiv.org/abs/2206.04145v1) - [pdf](http://arxiv.org/pdf/2206.04145v1)

> Quantitative Ultrasound (QUS) provides important information about the tissue properties. QUS parametric image can be formed by dividing the envelope data into small overlapping patches and computing different speckle statistics such as parameters of the Nakagami and Homodyned K-distributions (HK-distribution). The calculated QUS parametric images can be erroneous since only a few independent samples are available inside the patches. Another challenge is that the envelope samples inside the patch are assumed to come from the same distribution, an assumption that is often violated given that the tissue is usually not homogenous. In this paper, we propose a method based on Convolutional Neural Networks (CNN) to estimate QUS parametric images without patching. We construct a large dataset sampled from the HK-distribution, having regions with random shapes and QUS parameter values. We then use a well-known network to estimate QUS parameters in a multi-task learning fashion. Our results confirm that the proposed method is able to reduce errors and improve border definition in QUS parametric images.

</details>

<details>

<summary>2022-06-09 07:56:57 - CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization</summary>

- *Sungwook Lee, Seunghyun Lee, Byung Cheol Song*

- `2206.04325v1` - [abs](http://arxiv.org/abs/2206.04325v1) - [pdf](http://arxiv.org/pdf/2206.04325v1)

> For a long time, anomaly localization has been widely used in industries. Previous studies focused on approximating the distribution of normal features without adaptation to a target dataset. However, since anomaly localization should precisely discriminate normal and abnormal features, the absence of adaptation may make the normality of abnormal features overestimated. Thus, we propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes sophisticated anomaly localization using features adapted to the target dataset. CFA consists of (1) a learnable patch descriptor that learns and embeds target-oriented features and (2) scalable memory bank independent of the size of the target dataset. And, CFA adopts transfer learning to increase the normal feature density so that abnormal features can be clearly distinguished by applying patch descriptor and memory bank to a pre-trained CNN. The proposed method outperforms the previous methods quantitatively and qualitatively. For example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in anomaly localization of MVTec AD benchmark. In addition, this paper points out the negative effects of biased features of pre-trained CNNs and emphasizes the importance of the adaptation to the target dataset. The code is publicly available at https://github.com/sungwool/CFA_for_anomaly_localization.

</details>

<details>

<summary>2022-06-09 11:34:45 - Contrastive Counterfactual Visual Explanations With Overdetermination</summary>

- *Adam White, Kwun Ho Ngan, James Phelan, Saman Sadeghi Afgeh, Kevin Ryan, Constantino Carlos Reyes-Aldasoro, Artur d'Avila Garcez*

- `2106.14556v3` - [abs](http://arxiv.org/abs/2106.14556v3) - [pdf](http://arxiv.org/pdf/2106.14556v3)

> A novel explainable AI method called CLEAR Image is introduced in this paper. CLEAR Image is based on the view that a satisfactory explanation should be contrastive, counterfactual and measurable. CLEAR Image explains an image's classification probability by contrasting the image with a corresponding image generated automatically via adversarial learning. This enables both salient segmentation and perturbations that faithfully determine each segment's importance. CLEAR Image was successfully applied to a medical imaging case study where it outperformed methods such as Grad-CAM and LIME by an average of 27% using a novel pointing game metric. CLEAR Image excels in identifying cases of "causal overdetermination" where there are multiple patches in an image, any one of which is sufficient by itself to cause the classification probability to be close to one.

</details>

<details>

<summary>2022-06-09 14:08:17 - Velocity Level Approximation of Pressure Field Contact Patches</summary>

- *Joseph Masterjohn, Damrong Guoy, John Shepherd, Alejandro Castro*

- `2110.04157v2` - [abs](http://arxiv.org/abs/2110.04157v2) - [pdf](http://arxiv.org/pdf/2110.04157v2)

> Pressure Field Contact (PFC) was recently introduced as a method for detailed modeling of contact interface regions at rates much faster than elasticity-theory models, while at the same time predicting essential trends and capturing rich contact behavior. The PFC model was designed to work in conjunction with error-controlled integration at the acceleration level. Therefore a vast majority of existent multibody codes using solvers at the velocity level cannot incorporate PFC in its original form. In this work we introduce a discrete in time approximation of PFC making it suitable for use with existent velocity-level time steppers and enabling execution at real-time rates. We evaluate the accuracy and performance gains of our approach and demonstrate its effectiveness in simulating relevant manipulation tasks. The method is available in open source as part of Drake's Hydroelastic Contact model.

</details>

<details>

<summary>2022-06-10 13:50:34 - Saccade Mechanisms for Image Classification, Object Detection and Tracking</summary>

- *Saurabh Farkya, Zachary Daniels, Aswin Nadamuni Raghavan, David Zhang, Michael Piacentino*

- `2206.05102v1` - [abs](http://arxiv.org/abs/2206.05102v1) - [pdf](http://arxiv.org/pdf/2206.05102v1)

> We examine how the saccade mechanism from biological vision can be used to make deep neural networks more efficient for classification and object detection problems. Our proposed approach is based on the ideas of attention-driven visual processing and saccades, miniature eye movements influenced by attention. We conduct experiments by analyzing: i) the robustness of different deep neural network (DNN) feature extractors to partially-sensed images for image classification and object detection, and ii) the utility of saccades in masking image patches for image classification and object tracking. Experiments with convolutional nets (ResNet-18) and transformer-based models (ViT, DETR, TransTrack) are conducted on several datasets (CIFAR-10, DAVSOD, MSCOCO, and MOT17). Our experiments show intelligent data reduction via learning to mimic human saccades when used in conjunction with state-of-the-art DNNs for classification, detection, and tracking tasks. We observed minimal drop in performance for the classification and detection tasks while only using about 30\% of the original sensor data. We discuss how the saccade mechanism can inform hardware design via ``in-pixel'' processing.

</details>

<details>

<summary>2022-06-10 17:58:00 - Is Self-Supervised Learning More Robust Than Supervised Learning?</summary>

- *Yuanyi Zhong, Haoran Tang, Junkun Chen, Jian Peng, Yu-Xiong Wang*

- `2206.05259v1` - [abs](http://arxiv.org/abs/2206.05259v1) - [pdf](http://arxiv.org/pdf/2206.05259v1)

> Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pre-training data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixel-level gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning.

</details>

<details>

<summary>2022-06-13 18:16:40 - Compositional Mixture Representations for Vision and Text</summary>

- *Stephan Alaniz, Marco Federici, Zeynep Akata*

- `2206.06404v1` - [abs](http://arxiv.org/abs/2206.06404v1) - [pdf](http://arxiv.org/pdf/2206.06404v1)

> Learning a common representation space between vision and language allows deep networks to relate objects in the image to the corresponding semantic meaning. We present a model that learns a shared Gaussian mixture representation imposing the compositionality of the text onto the visual domain without having explicit location supervision. By combining the spatial transformer with a representation learning approach we learn to split images into separately encoded patches to associate visual and textual representations in an interpretable manner. On variations of MNIST and CIFAR10, our model is able to perform weakly supervised object detection and demonstrates its ability to extrapolate to unseen combination of objects.

</details>

<details>

<summary>2022-06-14 14:00:17 - Evaluating histopathology transfer learning with ChampKit</summary>

- *Jakub R. Kaczmarzyk, Tahsin M. Kurc, Shahira Abousamra, Rajarsi Gupta, Joel H. Saltz, Peter K. Koo*

- `2206.06862v1` - [abs](http://arxiv.org/abs/2206.06862v1) - [pdf](http://arxiv.org/pdf/2206.06862v1)

> Histopathology remains the gold standard for diagnosis of various cancers. Recent advances in computer vision, specifically deep learning, have facilitated the analysis of histopathology images for various tasks, including immune cell detection and microsatellite instability classification. The state-of-the-art for each task often employs base architectures that have been pretrained for image classification on ImageNet. The standard approach to develop classifiers in histopathology tends to focus narrowly on optimizing models for a single task, not considering the aspects of modeling innovations that improve generalization across tasks. Here we present ChampKit (Comprehensive Histopathology Assessment of Model Predictions toolKit): an extensible, fully reproducible benchmarking toolkit that consists of a broad collection of patch-level image classification tasks across different cancers. ChampKit enables a way to systematically document the performance impact of proposed improvements in models and methodology. ChampKit source code and data are freely accessible at https://github.com/kaczmarj/champkit .

</details>

<details>

<summary>2022-06-14 16:01:05 - Low-Rank Hankel Tensor Completion for Traffic Speed Estimation</summary>

- *Xudong Wang, Yuankai Wu, Dingyi Zhuang, Lijun Sun*

- `2105.11335v2` - [abs](http://arxiv.org/abs/2105.11335v2) - [pdf](http://arxiv.org/pdf/2105.11335v2)

> This paper studies the traffic state estimation (TSE) problem using sparse observations from mobile sensors. Most existing TSE methods either rely on well-defined physical traffic flow models or require large amounts of simulation data as input to train machine learning models. Different from previous studies, we propose a purely data-driven and model-free solution in this paper. We consider the TSE as a spatiotemporal matrix completion/interpolation problem, and apply spatiotemporal delay embedding to transform the original incomplete matrix into a fourth-order Hankel structured tensor. By imposing a low-rank assumption on this tensor structure, we can approximate and characterize both global and local spatiotemporal patterns in a data-driven manner. We use the truncated nuclear norm of a balanced spatiotemporal unfolding -- in which each column represents the vectorization of a small patch in the original matrix -- to approximate the tensor rank. An efficient solution algorithm based on the Alternating Direction Method of Multipliers (ADMM) is developed for model learning. The proposed framework only involves two hyperparameters, spatial and temporal window lengths, which are easy to set given the degree of data sparsity. We conduct numerical experiments on real-world high-resolution trajectory data, and our results demonstrate the effectiveness and superiority of the proposed model in some challenging scenarios.

</details>

<details>

<summary>2022-06-14 22:47:25 - Data-driven discovery of intrinsic dynamics</summary>

- *Daniel Floryan, Michael D. Graham*

- `2108.05928v2` - [abs](http://arxiv.org/abs/2108.05928v2) - [pdf](http://arxiv.org/pdf/2108.05928v2)

> Dynamical models underpin our ability to understand and predict the behavior of natural systems. Whether dynamical models are developed from first-principles derivations or from observational data, they are predicated on our choice of state variables. The choice of state variables is driven by convenience and intuition, and in the data-driven case the observed variables are often chosen to be the state variables. The dimensionality of these variables (and consequently the dynamical models) can be arbitrarily large, obscuring the underlying behavior of the system. In truth, these variables are often highly redundant and the system is driven by a much smaller set of latent intrinsic variables. In this study, we combine the mathematical theory of manifolds with the representational capacity of neural networks to develop a method that learns a system's intrinsic state variables directly from time series data, and also learns predictive models for their dynamics. What distinguishes our method is its ability to reduce data to the intrinsic dimensionality of the nonlinear manifold they live on. This ability is enabled by the concepts of charts and atlases from the theory of manifolds, whereby a manifold is represented by a collection of patches that are sewn together -- a necessary representation to attain intrinsic dimensionality. We demonstrate this approach on several high-dimensional systems with low-dimensional behavior. The resulting framework provides the ability to develop dynamical models of the lowest possible dimension, capturing the essence of a system.

</details>

<details>

<summary>2022-06-15 09:30:20 - TLDR: Twin Learning for Dimensionality Reduction</summary>

- *Yannis Kalantidis, Carlos Lassance, Jon Almazan, Diane Larlus*

- `2110.09455v2` - [abs](http://arxiv.org/abs/2110.09455v2) - [pdf](http://arxiv.org/pdf/2110.09455v2)

> Dimensionality reduction methods are unsupervised approaches which learn low-dimensional spaces where some properties of the initial space, typically the notion of "neighborhood", are preserved. Such methods usually require propagation on large k-NN graphs or complicated optimization solvers. On the other hand, self-supervised learning approaches, typically used to learn representations from scratch, rely on simple and more scalable frameworks for learning. In this paper, we propose TLDR, a dimensionality reduction method for generic input spaces that is porting the recent self-supervised learning framework of Zbontar et al. (2021) to the specific task of dimensionality reduction, over arbitrary representations. We propose to use nearest neighbors to build pairs from a training set and a redundancy reduction loss to learn an encoder that produces representations invariant across such pairs. TLDR is a method that is simple, easy to train, and of broad applicability; it consists of an offline nearest neighbor computation step that can be highly approximated, and a straightforward learning process. Aiming for scalability, we focus on improving linear dimensionality reduction, and show consistent gains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on ROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining it with a 10x compression.

</details>

<details>

<summary>2022-06-15 13:59:31 - Ripple Attention for Visual Perception with Sub-quadratic Complexity</summary>

- *Lin Zheng, Huijie Pan, Lingpeng Kong*

- `2110.02453v2` - [abs](http://arxiv.org/abs/2110.02453v2) - [pdf](http://arxiv.org/pdf/2110.02453v2)

> Transformer architectures are now central to sequence modeling tasks. At its heart is the attention mechanism, which enables effective modeling of long-term dependencies in a sequence. Recently, transformers have been successfully applied in the computer vision domain, where 2D images are first segmented into patches and then treated as 1D sequences. Such linearization, however, impairs the notion of spatial locality in images, which bears important visual clues. To bridge the gap, we propose ripple attention, a sub-quadratic attention mechanism for vision transformers. Built upon the recent kernel-based efficient attention mechanisms, we design a novel dynamic programming algorithm that weights contributions of different tokens to a query with respect to their relative spatial distances in the 2D space in linear observed time. Extensive experiments and analyses demonstrate the effectiveness of ripple attention on various visual tasks.

</details>

<details>

<summary>2022-06-16 17:06:47 - Adversarial Patch Attacks and Defences in Vision-Based Tasks: A Survey</summary>

- *Abhijith Sharma, Yijun Bian, Phil Munz, Apurva Narayan*

- `2206.08304v1` - [abs](http://arxiv.org/abs/2206.08304v1) - [pdf](http://arxiv.org/pdf/2206.08304v1)

> Adversarial attacks in deep learning models, especially for safety-critical systems, are gaining more and more attention in recent years, due to the lack of trust in the security and robustness of AI models. Yet the more primitive adversarial attacks might be physically infeasible or require some resources that are hard to access like the training data, which motivated the emergence of patch attacks. In this survey, we provide a comprehensive overview to cover existing techniques of adversarial patch attacks, aiming to help interested researchers quickly catch up with the progress in this field. We also discuss existing techniques for developing detection and defences against adversarial patches, aiming to help the community better understand this field and its applications in the real world.

</details>

<details>

<summary>2022-06-16 22:55:32 - Backdoor Attacks on Vision Transformers</summary>

- *Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash*

- `2206.08477v1` - [abs](http://arxiv.org/abs/2206.08477v1) - [pdf](http://arxiv.org/pdf/2206.08477v1)

> Vision Transformers (ViT) have recently demonstrated exemplary performance on a variety of vision tasks and are being used as an alternative to CNNs. Their design is based on a self-attention mechanism that processes images as a sequence of patches, which is quite different compared to CNNs. Hence it is interesting to study if ViTs are vulnerable to backdoor attacks. Backdoor attacks happen when an attacker poisons a small part of the training data for malicious purposes. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. To the best of our knowledge, we are the first to show that ViTs are vulnerable to backdoor attacks. We also find an intriguing difference between ViTs and CNNs - interpretation algorithms effectively highlight the trigger on test images for ViTs but not for CNNs. Based on this observation, we propose a test-time image blocking defense for ViTs which reduces the attack success rate by a large margin. Code is available here: https://github.com/UCDvision/backdoor_transformer.git

</details>

<details>

<summary>2022-06-17 16:32:08 - CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer</summary>

- *Yao Mu, Shoufa Chen, Mingyu Ding, Jianyu Chen, Runjian Chen, Ping Luo*

- `2206.08883v1` - [abs](http://arxiv.org/abs/2206.08883v1) - [pdf](http://arxiv.org/pdf/2206.08883v1)

> Transformer has achieved great successes in learning vision and language representation, which is general across various downstream tasks. In visual control, learning transferable state representation that can transfer between different control tasks is important to reduce the training sample size. However, porting Transformer to sample-efficient visual control remains a challenging and unsolved problem. To this end, we propose a novel Control Transformer (CtrlFormer), possessing many appealing benefits that prior arts do not have. Firstly, CtrlFormer jointly learns self-attention mechanisms between visual tokens and policy tokens among different control tasks, where multitask representation can be learned and transferred without catastrophic forgetting. Secondly, we carefully design a contrastive reinforcement learning paradigm to train CtrlFormer, enabling it to achieve high sample efficiency, which is important in control problems. For example, in the DMControl benchmark, unlike recent advanced methods that failed by producing a zero score in the "Cartpole" task after transfer learning with 100k samples, CtrlFormer can achieve a state-of-the-art score with only 100k samples while maintaining the performance of previous tasks. The code and models are released in our project homepage.

</details>

<details>

<summary>2022-06-18 19:46:06 - DECK: Model Hardening for Defending Pervasive Backdoors</summary>

- *Guanhong Tao, Yingqi Liu, Siyuan Cheng, Shengwei An, Zhuo Zhang, Qiuling Xu, Guangyu Shen, Xiangyu Zhang*

- `2206.09272v1` - [abs](http://arxiv.org/abs/2206.09272v1) - [pdf](http://arxiv.org/pdf/2206.09272v1)

> Pervasive backdoors are triggered by dynamic and pervasive input perturbations. They can be intentionally injected by attackers or naturally exist in normally trained models. They have a different nature from the traditional static and localized backdoors that can be triggered by perturbing a small input area with some fixed pattern, e.g., a patch with solid color. Existing defense techniques are highly effective for traditional backdoors. However, they may not work well for pervasive backdoors, especially regarding backdoor removal and model hardening. In this paper, we propose a novel model hardening technique against pervasive backdoors, including both natural and injected backdoors. We develop a general pervasive attack based on an encoder-decoder architecture enhanced with a special transformation layer. The attack can model a wide range of existing pervasive backdoor attacks and quantify them by class distances. As such, using the samples derived from our attack in adversarial training can harden a model against these backdoor vulnerabilities. Our evaluation on 9 datasets with 15 model structures shows that our technique can enlarge class distances by 59.65% on average with less than 1% accuracy degradation and no robustness loss, outperforming five hardening techniques such as adversarial training, universal adversarial training, MOTH, etc. It can reduce the attack success rate of six pervasive backdoor attacks from 99.06% to 1.94%, surpassing seven state-of-the-art backdoor removal techniques.

</details>

<details>

<summary>2022-06-19 03:27:07 - Patch-based Object-centric Transformers for Efficient Video Generation</summary>

- *Wilson Yan, Ryo Okumura, Stephen James, Pieter Abbeel*

- `2206.04003v2` - [abs](http://arxiv.org/abs/2206.04003v2) - [pdf](http://arxiv.org/pdf/2206.04003v2)

> In this work, we present Patch-based Object-centric Video Transformer (POVT), a novel region-based video generation architecture that leverages object-centric information to efficiently model temporal dynamics in videos. We build upon prior work in video prediction via an autoregressive transformer over the discrete latent space of compressed videos, with an added modification to model object-centric information via bounding boxes. Due to better compressibility of object-centric representations, we can improve training efficiency by allowing the model to only access object information for longer horizon temporal information. When evaluated on various difficult object-centric datasets, our method achieves better or equal performance to other video generation models, while remaining computationally more efficient and scalable. In addition, we show that our method is able to perform object-centric controllability through bounding box manipulation, which may aid downstream tasks such as video editing, or visual planning. Samples are available at https://sites.google.com/view/povt-public

</details>

<details>

<summary>2022-06-21 03:54:21 - Transformers Improve Breast Cancer Diagnosis from Unregistered Multi-View Mammograms</summary>

- *Xuxin Chen, Ke Zhang, Neman Abdoli, Patrik W. Gilley, Ximin Wang, Hong Liu, Bin Zheng, Yuchen Qiu*

- `2206.10096v1` - [abs](http://arxiv.org/abs/2206.10096v1) - [pdf](http://arxiv.org/pdf/2206.10096v1)

> Deep convolutional neural networks (CNNs) have been widely used in various medical imaging tasks. However, due to the intrinsic locality of convolution operation, CNNs generally cannot model long-range dependencies well, which are important for accurately identifying or mapping corresponding breast lesion features computed from unregistered multiple mammograms. This motivates us to leverage the architecture of Multi-view Vision Transformers to capture long-range relationships of multiple mammograms from the same patient in one examination. For this purpose, we employ local Transformer blocks to separately learn patch relationships within four mammograms acquired from two-view (CC/MLO) of two-side (right/left) breasts. The outputs from different views and sides are concatenated and fed into global Transformer blocks, to jointly learn patch relationships between four images representing two different views of the left and right breasts. To evaluate the proposed model, we retrospectively assembled a dataset involving 949 sets of mammograms, which include 470 malignant cases and 479 normal or benign cases. We trained and evaluated the model using a five-fold cross-validation method. Without any arduous preprocessing steps (e.g., optimal window cropping, chest wall or pectoral muscle removal, two-view image registration, etc.), our four-image (two-view-two-side) Transformer-based model achieves case classification performance with an area under ROC curve (AUC = 0.818), which significantly outperforms AUC = 0.784 achieved by the state-of-the-art multi-view CNNs (p = 0.009). It also outperforms two one-view-two-side models that achieve AUC of 0.724 (CC view) and 0.769 (MLO view), respectively. The study demonstrates the potential of using Transformers to develop high-performing computer-aided diagnosis schemes that combine four mammograms.

</details>

<details>

<summary>2022-06-22 04:05:13 - A Feature Memory Rearrangement Network for Visual Inspection of Textured Surface Defects Toward Edge Intelligent Manufacturing</summary>

- *Haiming Yao, Wenyong Yu, Xue Wang*

- `2206.10830v1` - [abs](http://arxiv.org/abs/2206.10830v1) - [pdf](http://arxiv.org/pdf/2206.10830v1)

> Recent advances in the industrial inspection of textured surfaces-in the form of visual inspection-have made such inspections possible for efficient, flexible manufacturing systems. We propose an unsupervised feature memory rearrangement network (FMR-Net) to accurately detect various textural defects simultaneously. Consistent with mainstream methods, we adopt the idea of background reconstruction; however, we innovatively utilize artificial synthetic defects to enable the model to recognize anomalies, while traditional wisdom relies only on defect-free samples. First, we employ an encoding module to obtain multiscale features of the textured surface. Subsequently, a contrastive-learning-based memory feature module (CMFM) is proposed to obtain discriminative representations and construct a normal feature memory bank in the latent space, which can be employed as a substitute for defects and fast anomaly scores at the patch level. Next, a novel global feature rearrangement module (GFRM) is proposed to further suppress the reconstruction of residual defects. Finally, a decoding module utilizes the restored features to reconstruct the normal texture background. In addition, to improve inspection performance, a two-phase training strategy is utilized for accurate defect restoration refinement, and we exploit a multimodal inspection method to achieve noise-robust defect localization. We verify our method through extensive experiments and test its practical deployment in collaborative edge--cloud intelligent manufacturing scenarios by means of a multilevel detection method, demonstrating that FMR-Net exhibits state-of-the-art inspection accuracy and shows great potential for use in edge-computing-enabled smart industries.

</details>

<details>

<summary>2022-06-23 17:10:28 - An Unconstrained Convex Formulation of Compliant Contact</summary>

- *Alejandro Castro, Frank Permenter, Xuchen Han*

- `2110.10107v2` - [abs](http://arxiv.org/abs/2110.10107v2) - [pdf](http://arxiv.org/pdf/2110.10107v2)

> We present a convex formulation of compliant frictional contact and a robust, performant method to solve it in practice. By analytically eliminating contact constraints, we obtain an unconstrained convex problem. Our solver has proven global convergence and warm-starts effectively, enabling simulation at interactive rates. We develop compact analytical expressions of contact forces allowing us to describe our model in clear physical terms and to rigorously characterize our approximations. Moreover, this enables us not only to model point contact, but also to incorporate sophisticated models of compliant contact patches. Our time stepping scheme includes the midpoint rule, which we demonstrate achieves second order accuracy even with frictional contact. We introduce a number of accuracy metrics and show our method outperforms existing commercial and open source alternatives without sacrificing accuracy. Finally, we demonstrate robust simulation of robotic manipulation tasks at interactive rates, with accurately resolved stiction and contact transitions, as required for meaningful sim-to-real transfer. Our method is implemented in the open source robotics toolkit Drake.

</details>

<details>

<summary>2022-06-24 14:39:11 - PoCaP Corpus: A Multimodal Dataset for Smart Operating Room Speech Assistant using Interventional Radiology Workflow Analysis</summary>

- *Kubilay Can Demir, Matthias May, Axel Schmid, Michael Uder, Katharina Breininger, Tobias Weise, Andreas Maier, Seung Hee Yang*

- `2206.12320v1` - [abs](http://arxiv.org/abs/2206.12320v1) - [pdf](http://arxiv.org/pdf/2206.12320v1)

> This paper presents a new multimodal interventional radiology dataset, called PoCaP (Port Catheter Placement) Corpus. This corpus consists of speech and audio signals in German, X-ray images, and system commands collected from 31 PoCaP interventions by six surgeons with average duration of 81.4 $\pm$ 41.0 minutes. The corpus aims to provide a resource for developing a smart speech assistant in operating rooms. In particular, it may be used to develop a speech controlled system that enables surgeons to control the operation parameters such as C-arm movements and table positions. In order to record the dataset, we acquired consent by the institutional review board and workers council in the University Hospital Erlangen and by the patients for data privacy. We describe the recording set-up, data structure, workflow and preprocessing steps, and report the first PoCaP Corpus speech recognition analysis results with 11.52 $\%$ word error rate using pretrained models. The findings suggest that the data has the potential to build a robust command recognition system and will allow the development of a novel intervention support systems using speech and image processing in the medical domain.

</details>

<details>

<summary>2022-06-25 16:57:59 - An Empirical Study of Bugs in Eclipse Stable Internal Interfaces</summary>

- *Simon Kawuma, Evarist Nabaasa*

- `2203.09134v2` - [abs](http://arxiv.org/abs/2203.09134v2) - [pdf](http://arxiv.org/pdf/2203.09134v2)

> TThe Eclipse framework is a popular and widely used framework that has been evolving for over a decade. The framework provides both stable interfaces (APIs) and unstable interfaces (non-APIs). Despite being discouraged by Eclipse, application developers often use non-APIs which cause their systems to fail when ported to new framework releases. Previous studies showed that applications using relatively old non-APIs are more likely to be compatible with new releases compared to the ones that used newly introduced non-APIs. Furthermore, from our previous study about the stability of Eclipse internal interfaces, we discovered that there exist 327K stable non-API methods as the Eclipse framework evolves. In the same study, we recommended that 327K stable non-API methods can be used by Eclipse interface providers as possible candidates for promotion to stable interfaces. However, since non-APIs are unsupported and considered to be immature i.e., can contain bugs, to this end there exist a need to first investigate the stable non-APIs for possible bugs before they can be promoted to APIs. In this study, we empirically investigated the stable non-API for possible bugs using Sonarqube software quality tool. We discovered that over 79.8% classes containing old stable non-APIs methods have zero bugs. Results from this study can be used by both interface providers and users as a starting point to analyze which interfaces are well tested and also estimate how much work could be involved when performing bug fixing for a given eclipse release.

</details>

<details>

<summary>2022-06-26 13:26:20 - Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer</summary>

- *Lalithkumar Seenivasan, Mobarakol Islam, Adithya K Krishna, Hongliang Ren*

- `2206.11053v2` - [abs](http://arxiv.org/abs/2206.11053v2) - [pdf](http://arxiv.org/pdf/2206.11053v2)

> Visual question answering (VQA) in surgery is largely unexplored. Expert surgeons are scarce and are often overloaded with clinical and academic workloads. This overload often limits their time answering questionnaires from patients, medical students or junior residents related to surgical procedures. At times, students and junior residents also refrain from asking too many questions during classes to reduce disruption. While computer-aided simulators and recording of past surgical procedures have been made available for them to observe and improve their skills, they still hugely rely on medical experts to answer their questions. Having a Surgical-VQA system as a reliable 'second opinion' could act as a backup and ease the load on the medical experts in answering these questions. The lack of annotated medical data and the presence of domain-specific terms has limited the exploration of VQA for surgical procedures. In this work, we design a Surgical-VQA task that answers questionnaires on surgical procedures based on the surgical scene. Extending the MICCAI endoscopic vision challenge 2018 dataset and workflow recognition dataset further, we introduce two Surgical-VQA datasets with classification and sentence-based answers. To perform Surgical-VQA, we employ vision-text transformers models. We further introduce a residual MLP-based VisualBert encoder model that enforces interaction between visual and text tokens, improving performance in classification-based answering. Furthermore, we study the influence of the number of input image patches and temporal visual features on the model performance in both classification and sentence-based answering.

</details>

<details>

<summary>2022-06-27 16:22:16 - Simplifying Semantic Annotations of SMCalFlow</summary>

- *Joram Meron*

- `2206.13425v1` - [abs](http://arxiv.org/abs/2206.13425v1) - [pdf](http://arxiv.org/pdf/2206.13425v1)

> SMCalFlow is a large corpus of semantically detailed annotations of task-oriented natural dialogues. The annotations use a dataflow approach, in which the annotations are programs which represent user requests. Despite the availability, size and richness of this annotated corpus, it has seen only very limited use in dialogue systems research work, at least in part due to the difficulty in understanding and using the annotations. To address these difficulties, this paper suggests a simplification of the SMCalFlow annotations, as well as releases code needed to inspect the execution of the annotated dataflow programs, which should allow researchers of dialogue systems an easy entry point to experiment with various dataflow based implementations and annotations.

</details>

<details>

<summary>2022-06-27 16:27:53 - ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration</summary>

- *Neel Dey, Jo Schlemper, Seyed Sadegh Mohseni Salehi, Bo Zhou, Guido Gerig, Michal Sofka*

- `2206.13434v1` - [abs](http://arxiv.org/abs/2206.13434v1) - [pdf](http://arxiv.org/pdf/2206.13434v1)

> Establishing voxelwise semantic correspondence across distinct imaging modalities is a foundational yet formidable computer vision task. Current multi-modality registration techniques maximize hand-crafted inter-domain similarity functions, are limited in modeling nonlinear intensity-relationships and deformations, and may require significant re-engineering or underperform on new tasks, datasets, and domain pairs. This work presents ContraReg, an unsupervised contrastive representation learning approach to multi-modality deformable registration. By projecting learned multi-scale local patch features onto a jointly learned inter-domain embedding space, ContraReg obtains representations useful for non-rigid multi-modality alignment. Experimentally, ContraReg achieves accurate and robust results with smooth and invertible deformations across a series of baselines and ablations on a neonatal T1-T2 brain MRI registration task with all methods validated over a wide range of deformation regularization strengths.

</details>

<details>

<summary>2022-06-27 20:35:52 - DeepPERF: A Deep Learning-Based Approach For Improving Software Performance</summary>

- *Spandan Garg, Roshanak Zilouchian Moghaddam, Colin B. Clement, Neel Sundaresan, Chen Wu*

- `2206.13619v1` - [abs](http://arxiv.org/abs/2206.13619v1) - [pdf](http://arxiv.org/pdf/2206.13619v1)

> Improving software performance is an important yet challenging part of the software development cycle. Today, the majority of performance inefficiencies are identified and patched by performance experts. Recent advancements in deep learning approaches and the wide-spread availability of open source data creates a great opportunity to automate the identification and patching of performance problems. In this paper, we present DeepPERF, a transformer-based approach to suggest performance improvements for C# applications. We pretrain DeepPERF on English and Source code corpora and followed by finetuning for the task of generating performance improvement patches for C# applications. Our evaluation shows that our model can generate the same performance improvement suggestion as the developer fix in ~53% of the cases, getting ~34% of them verbatim in our expert-verified dataset of performance changes made by C# developers. Additionally, we evaluate DeepPERF on 50 open source C# repositories on GitHub using both benchmark and unit tests and find that our model is able to suggest valid performance improvements that can improve both CPU usage and Memory allocations. So far we've submitted 19 pull-requests with 28 different performance optimizations and 11 of these PRs have been approved by the project owners.

</details>

<details>

<summary>2022-06-27 20:52:53 - Patch Selection for Melanoma Classification</summary>

- *Guillaume Lachaud, Patricia Conde-Cespedes, Maria Trocan*

- `2206.13626v1` - [abs](http://arxiv.org/abs/2206.13626v1) - [pdf](http://arxiv.org/pdf/2206.13626v1)

> In medical image processing, the most important information is often located on small parts of the image. Patch-based approaches aim at using only the most relevant parts of the image. Finding ways to automatically select the patches is a challenge. In this paper, we investigate two criteria to choose patches: entropy and a spectral similarity criterion. We perform experiments at different levels of patch size. We train a Convolutional Neural Network on the subsets of patches and analyze the training time. We find that, in addition to requiring less preprocessing time, the classifiers trained on the datasets of patches selected based on entropy converge faster than on those selected based on the spectral similarity criterion and, furthermore, lead to higher accuracy. Moreover, patches of high entropy lead to faster convergence and better accuracy than patches of low entropy.

</details>

<details>

<summary>2022-06-28 03:30:05 - Clone-based code method usage pattern mining</summary>

- *Zhipeng Xue, Yuanliang Zhang, Rulin Xu*

- `2109.13099v4` - [abs](http://arxiv.org/abs/2109.13099v4) - [pdf](http://arxiv.org/pdf/2109.13099v4)

> When programmers retrieve a code method and want to reuse it, they need to understand the usage patterns of the retrieved method. However, it is difficult to obtain usage information of the retrieved method since this method may only have a brief comment and few available usage examples. In this paper, we propose an approach, called LUPIN (cLone-based Usage Pattern mIniNg), to mine the usage patterns of these methods, which do not widely appeared in the code repository. The key idea of LUPIN is that the cloned code of the target method may have a similar usage pattern, and we can collect more usage information of the target method from cloned code usage examples. From the amplified usage examples, we mine the usage pattern of the target method by frequent subsequence mining after program slicing and code normalization. Our evaluation shows that LUPIN can mine four categories of usage patterns with an average precision of 0.65.

</details>

<details>

<summary>2022-06-28 08:46:56 - Automated Repair of Resource Leaks in Android Applications</summary>

- *Bhargav Nagaraja Bhatt, Carlo A. Furia*

- `2003.03201v3` - [abs](http://arxiv.org/abs/2003.03201v3) - [pdf](http://arxiv.org/pdf/2003.03201v3)

> Resource leaks -- a program does not release resources it previously acquired -- are a common kind of bug in Android applications. Even with the help of existing techniques to automatically detect leaks, writing a leak-free program remains tricky. One of the reasons is Android's event-driven programming model, which complicates the understanding of an application's overall control flow.   In this paper, we present PlumbDroid: a technique to automatically detect and fix resource leaks in Android applications. PlumbDroid uses static analysis to find execution traces that may leak a resource. The information built for detection also undergirds automatically building a fix -- consisting of release operations performed at appropriate locations -- that removes the leak and does not otherwise affect the application's usage of the resource.   An empirical evaluation on resource leaks from the DroidLeaks curated collection demonstrates that PlumbDroid's approach is scalable, precise, and produces correct fixes for a variety of resource leak bugs: PlumbDroid automatically found and repaired 50 leaks that affect 9 widely used resources of the Android system, including all those collected by DroidLeaks for those resources; on average, it took just 2 minutes to detect and repair a leak. PlumbDroid also compares favorably to Relda2/RelFix -- the only other fully automated approach to repair Android resource leaks -- since it usually detects more leaks with higher precision and producing smaller fixes. These results indicate that PlumbDroid can provide valuable support to enhance the quality of Android applications in practice.

</details>

<details>

<summary>2022-06-28 08:53:21 - Building a Secure Software Supply Chain with GNU Guix</summary>

- *Ludovic CourtÃ¨s*

- `2206.14606v1` - [abs](http://arxiv.org/abs/2206.14606v1) - [pdf](http://arxiv.org/pdf/2206.14606v1)

> The software supply chain is becoming a widespread analogy to designate the series of steps taken to go from source code published by developers to executables running on the users? computers. A security vulnerability in any of these steps puts users at risk, and evidence shows that attacks on the supply chain are becoming more common. The consequences of an attack on the software supply chain can be tragic in a society that relies on many interconnected software systems, and this has led research interest as well as governmental incentives for supply chain security to rise.   GNU Guix is a software deployment tool and software distribution that supports provenance tracking, reproducible builds, and reproducible software environments. Unlike many software distributions, it consists exclusively of source code: it provides a set of package definitions that describe how to build code from source. Together, these properties set it apart from many deployment tools that center on the distribution of binaries.   This paper focuses on one research question: how can Guix and similar systems allow users to securely update their software? Guix source code is distributed using the Git version control system; updating Guix-installed software packages means, first, updating the local copy of the Guix source code. Prior work on secure software updates focuses on systems very different from Guix -- systems such as Debian, Fedora, or PyPI where updating consists in fetching metadata about the latest binary artifacts available -- and is largely inapplicable in the context of Guix. By contrast, the main threats for Guix are attacks on its source code repository, which could lead users to run inauthentic code or to downgrade their system. Deployment tools that more closely resemble Guix, from Nix to Portage, either lack secure update mechanisms or suffer from shortcomings.   Our main contribution is a model and tool to authenticate new Git revisions. We further show how, building on Git semantics, we build protections against downgrade attacks and related threats. We explain implementation choices. This work has been deployed in production two years ago, giving us insight on its actual use at scale every day. The Git checkout authentication at its core is applicable beyond the specific use case of Guix, and we think it could benefit to developer teams that use Git.   As attacks on the software supply chain appear, security research is now looking at every link of the supply chain. Secure updates are one important aspect of the supply chain, but this paper also looks at the broader context: how Guix models and implements the supply chain, from upstream source code to binaries running on computers. While much recent work focuses on attestation -- certifying each link of the supply chain -- Guix takes a more radical approach: enabling independent verification of each step, building on reproducible builds, "bootstrappable" builds, and provenance tracking. The big picture shows how Guix can be used as the foundation of secure software supply chains.

</details>

<details>

<summary>2022-06-28 14:58:51 - Deep Neural Networks pruning via the Structured Perspective Regularization</summary>

- *Matteo Cacciola, Antonio Frangioni, Xinlin Li, Andrea Lodi*

- `2206.14056v1` - [abs](http://arxiv.org/abs/2206.14056v1) - [pdf](http://arxiv.org/pdf/2206.14056v1)

> In Machine Learning, Artificial Neural Networks (ANNs) are a very powerful tool, broadly used in many applications. Often, the selected (deep) architectures include many layers, and therefore a large amount of parameters, which makes training, storage and inference expensive. This motivated a stream of research about compressing the original networks into smaller ones without excessively sacrificing performances. Among the many proposed compression approaches, one of the most popular is \emph{pruning}, whereby entire elements of the ANN (links, nodes, channels, \ldots) and the corresponding weights are deleted. Since the nature of the problem is inherently combinatorial (what elements to prune and what not), we propose a new pruning method based on Operational Research tools. We start from a natural Mixed-Integer-Programming model for the problem, and we use the Perspective Reformulation technique to strengthen its continuous relaxation. Projecting away the indicator variables from this reformulation yields a new regularization term, which we call the Structured Perspective Regularization, that leads to structured pruning of the initial architecture. We test our method on some ResNet architectures applied to CIFAR-10, CIFAR-100 and ImageNet datasets, obtaining competitive performances w.r.t.~the state of the art for structured pruning.

</details>

<details>

<summary>2022-06-28 15:53:13 - Locally checkable problems parameterized by clique-width</summary>

- *Narmina Baghirova, Carolina LucÃ­a Gonzalez, Bernard Ries, David Schindl*

- `2203.02992v2` - [abs](http://arxiv.org/abs/2203.02992v2) - [pdf](http://arxiv.org/pdf/2203.02992v2)

> We continue the study initiated by Bonomo-Braberman and Gonzalez in 2020 on $r$-locally checkable problems. We propose a dynamic programming algorithm that takes as input a graph with an associated clique-width expression and solves a $1$-locally checkable problem under certain restrictions. We show that it runs in polynomial time in graphs of bounded clique-width, when the number of colors of the locally checkable problem is fixed. Furthermore, we present a first extension of our framework to global properties by taking into account the sizes of the color classes, and consequently enlarge the set of problems solvable in polynomial time with our approach in graphs of bounded clique-width. As examples, we apply this setting to show that, when parameterized by clique-width, the $[k]-$Roman domination problem is FPT, and the $k$-community problem, Max PDS and other variants are XP.

</details>

<details>

<summary>2022-06-28 16:59:28 - Memory Safe Computations with XLA Compiler</summary>

- *Artem Artemev, Tilman Roeder, Mark van der Wilk*

- `2206.14148v1` - [abs](http://arxiv.org/abs/2206.14148v1) - [pdf](http://arxiv.org/pdf/2206.14148v1)

> Software packages like TensorFlow and PyTorch are designed to support linear algebra operations, and their speed and usability determine their success. However, by prioritising speed, they often neglect memory requirements. As a consequence, the implementations of memory-intensive algorithms that are convenient in terms of software design can often not be run for large problems due to memory overflows. Memory-efficient solutions require complex programming approaches with significant logic outside the computational framework. This impairs the adoption and use of such algorithms. To address this, we developed an XLA compiler extension that adjusts the computational data-flow representation of an algorithm according to a user-specified memory limit. We show that k-nearest neighbour and sparse Gaussian process regression methods can be run at a much larger scale on a single device, where standard implementations would have failed. Our approach leads to better use of hardware resources. We believe that further focus on removing memory constraints at a compiler level will widen the range of machine learning methods that can be developed in the future.

</details>

<details>

<summary>2022-06-28 17:43:11 - Harnessing the Power of Ego Network Layers for Link Prediction in Online Social Networks</summary>

- *Mustafa Toprak, Chiara Boldrini, Andrea Passarella, Marco Conti*

- `2109.09190v2` - [abs](http://arxiv.org/abs/2109.09190v2) - [pdf](http://arxiv.org/pdf/2109.09190v2)

> Being able to recommend links between users in online social networks is important for users to connect with like-minded individuals as well as for the platforms themselves and third parties leveraging social media information to grow their business. Predictions are typically based on unsupervised or supervised learning, often leveraging simple yet effective graph topological information, such as the number of common neighbors. However, we argue that richer information about personal social structure of individuals might lead to better predictions. In this paper, we propose to leverage well-established social cognitive theories to improve link prediction performance. According to these theories, individuals arrange their social relationships along, on average, five concentric circles of decreasing intimacy. We postulate that relationships in different circles have different importance in predicting new links. In order to validate this claim, we focus on popular feature-extraction prediction algorithms (both unsupervised and supervised) and we extend them to include social-circles awareness. We validate the prediction performance of these circle-aware algorithms against several benchmarks (including their baseline versions as well as node-embedding- and GNN-based link prediction), leveraging two Twitter datasets comprising a community of video gamers and generic users. We show that social-awareness generally provides significant improvements in the prediction performance, beating also state-of-the-art solutions like node2vec and SEAL, and without increasing the computational complexity. Finally, we show that social-awareness can be used in place of using a classifier (which may be costly or impractical) for targeting a specific category of users.

</details>

<details>

<summary>2022-06-28 19:36:44 - Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</summary>

- *Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica*

- `2201.12023v3` - [abs](http://arxiv.org/abs/2201.12023v3) - [pdf](http://arxiv.org/pdf/2201.12023v3)

> Alpa automates model-parallel training of large deep learning (DL) models by generating execution plans that unify data, operator, and pipeline parallelism. Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations. They do not suffice to scale out complex DL models on distributed compute devices. Alpa distributes the training of large DL models by viewing parallelisms as two hierarchical levels: inter-operator and intra-operator parallelisms. Based on it, Alpa constructs a new hierarchical space for massive model-parallel execution plans. Alpa designs a number of compilation passes to automatically derive efficient parallel execution plans at each parallelism level. Alpa implements an efficient runtime to orchestrate the two-level parallel execution on distributed compute devices. Our evaluation shows Alpa generates parallelization plans that match or outperform hand-tuned model-parallel training systems even on models they are designed for. Unlike specialized systems, Alpa also generalizes to models with heterogeneous architectures and models without manually-designed plans. Alpa's source code is publicly available at https://github.com/alpa-projects/alpa

</details>

<details>

<summary>2022-06-28 19:44:40 - Online Anomaly Detection Based On Reservoir Sampling and LOF for IoT devices</summary>

- *Tomasz Szydlo*

- `2206.14265v1` - [abs](http://arxiv.org/abs/2206.14265v1) - [pdf](http://arxiv.org/pdf/2206.14265v1)

> The growing number of IoT devices and their use to monitor the operation of machines and equipment increases interest in anomaly detection algorithms running on devices. However, the difficulty is the limitations of the available computational and memory resources on the devices. In the case of microcontrollers (MCUs), these are single megabytes of program and several hundred kilobytes of working memory. Consequently, algorithms must be appropriately matched to the capabilities of the devices. In the paper, we analyse the processing pipeline for anomaly detection and implementation of the Local Outliner Factor (LOF) algorithm on a MCU. We also show that it is possible to train such an algorithm directly on the device, which gives great potential to use the solution in real devices.

</details>

<details>

<summary>2022-06-28 21:14:36 - MurTree: Optimal Classification Trees via Dynamic Programming and Search</summary>

- *Emir DemiroviÄ, Anna Lukina, Emmanuel Hebrard, Jeffrey Chan, James Bailey, Christopher Leckie, Kotagiri Ramamohanarao, Peter J. Stuckey*

- `2007.12652v4` - [abs](http://arxiv.org/abs/2007.12652v4) - [pdf](http://arxiv.org/pdf/2007.12652v4)

> Decision tree learning is a widely used approach in machine learning, favoured in applications that require concise and interpretable models. Heuristic methods are traditionally used to quickly produce models with reasonably high accuracy. A commonly criticised point, however, is that the resulting trees may not necessarily be the best representation of the data in terms of accuracy and size. In recent years, this motivated the development of optimal classification tree algorithms that globally optimise the decision tree in contrast to heuristic methods that perform a sequence of locally optimal decisions. We follow this line of work and provide a novel algorithm for learning optimal classification trees based on dynamic programming and search. Our algorithm supports constraints on the depth of the tree and number of nodes. The success of our approach is attributed to a series of specialised techniques that exploit properties unique to classification trees. Whereas algorithms for optimal classification trees have traditionally been plagued by high runtimes and limited scalability, we show in a detailed experimental study that our approach uses only a fraction of the time required by the state-of-the-art and can handle datasets with tens of thousands of instances, providing several orders of magnitude improvements and notably contributing towards the practical realisation of optimal decision trees.

</details>

<details>

<summary>2022-06-28 22:09:10 - Indistinguishability Obfuscation of Circuits and its Application in Security</summary>

- *Shilun Li, Zijing Di*

- `2206.14304v1` - [abs](http://arxiv.org/abs/2206.14304v1) - [pdf](http://arxiv.org/pdf/2206.14304v1)

> Under discussion in the paper is an $i\mathcal{O}$ (indistinguishability obfuscator) for circuits in Nick's Class. The obfuscator is constructed by encoding the Branching Program given by Barrington's theorem using Multilinear Jigsaw Puzzle framework. We will show under various indistinguishability hardness assumptions, the constructed obfuscator is an $i\mathcal{O}$ for Nick's Class. Using Fully Homomorphic Encryption, we will amplify the result and construct an $i\mathcal{O}$ for $\textbf{P}/poly$, which are circuits of polynomial size. Discussion on $i\mathcal{O}$ and Functional Encryption is also included in this paper.

</details>

<details>

<summary>2022-06-29 06:35:06 - Automatic Synthesis of Neurons for Recurrent Neural Nets</summary>

- *Roland Olsson, Chau Tran, Lars Magnusson*

- `2207.03577v1` - [abs](http://arxiv.org/abs/2207.03577v1) - [pdf](http://arxiv.org/pdf/2207.03577v1)

> We present a new class of neurons, ARNs, which give a cross entropy on test data that is up to three times lower than the one achieved by carefully optimized LSTM neurons. The explanations for the huge improvements that often are achieved are elaborate skip connections through time, up to four internal memory states per neuron and a number of novel activation functions including small quadratic forms. The new neurons were generated using automatic programming and are formulated as pure functional programs that easily can be transformed. We present experimental results for eight datasets and found excellent improvements for seven of them, but LSTM remained the best for one dataset. The results are so promising that automatic programming to generate new neurons should become part of the standard operating procedure for any machine learning practitioner who works on time series data such as sensor signals.

</details>

<details>

<summary>2022-06-29 09:13:21 - Representation and Synthesis of C++ Programs for Generalized Planning</summary>

- *Javier Segovia-Aguas, Yolanda E-MartÃ­n, Sergio JimÃ©nez*

- `2206.14480v1` - [abs](http://arxiv.org/abs/2206.14480v1) - [pdf](http://arxiv.org/pdf/2206.14480v1)

> The paper introduces a novel representation for Generalized Planning (GP) problems, and their solutions, as C++ programs. Our C++ representation allows to formally proving the termination of generalized plans, and to specifying their asymptotic complexity w.r.t. the number of world objects. Characterizing the complexity of C++ generalized plans enables the application of a combinatorial search that enumerates the space of possible GP solutions in order of complexity. Experimental results show that our implementation of this approach, which we call BFGP++, outperforms the previous GP as heuristic search approach for the computation of generalized plans represented as compiler-styled programs. Last but not least, the execution of a C++ program on a classical planning instance is a deterministic grounding-free and search-free process, so our C++ representation allows us to automatically validate the computed solutions on large test instances of thousands of objects, where off-the-shelf classical planners get stuck either in the pre-processing or in the search.

</details>

<details>

<summary>2022-06-29 12:10:18 - A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA</summary>

- *Renhui Zhang, Youwei Zhang, Yao Yu*

- `2207.06490v1` - [abs](http://arxiv.org/abs/2207.06490v1) - [pdf](http://arxiv.org/pdf/2207.06490v1)

> Numerical reasoning is required when solving most problems in our life, but it has been neglected in previous artificial intelligence researches. FinQA challenge has been organized to strengthen the study on numerical reasoning where the participants are asked to predict the numerical reasoning program to solve financial question. The result of FinQA will be evaluated by both execution accuracy and program accuracy. In this paper, we present our approach to tackle the task objective by developing models with different specialized capabilities and fusing their strength. Overall, our approach achieves the 1st place in FinQA challenge, with 71.93% execution accuracy and 67.03% program accuracy.

</details>

<details>

<summary>2022-06-29 12:23:16 - The Online Min-Sum Set Cover Problem</summary>

- *Dimitris Fotakis, Loukas Kavouras, Grigorios Koumoutsos, Stratis Skoulakis, Manolis Vardas*

- `2003.02161v2` - [abs](http://arxiv.org/abs/2003.02161v2) - [pdf](http://arxiv.org/pdf/2003.02161v2)

> We consider the online Min-Sum Set Cover (MSSC), a natural and intriguing generalization of the classical list update problem. In Online MSSC, the algorithm maintains a permutation on $n$ elements based on subsets $S_1, S_2, \ldots$ arriving online. The algorithm serves each set $S_t$ upon arrival, using its current permutation $\pi_{t}$, incurring an access cost equal to the position of the first element of $S_t$ in $\pi_{t}$. Then, the algorithm may update its permutation to $\pi_{t+1}$, incurring a moving cost equal to the Kendall tau distance of $\pi_{t}$ to $\pi_{t+1}$. The objective is to minimize the total access and moving cost for serving the entire sequence. We consider the $r$-uniform version, where each $S_t$ has cardinality $r$. List update is the special case where $r = 1$.   We obtain tight bounds on the competitive ratio of deterministic online algorithms for MSSC against a static adversary, that serves the entire sequence by a single permutation. First, we show a lower bound of $(r+1)(1-\frac{r}{n+1})$ on the competitive ratio. Then, we consider several natural generalizations of successful list update algorithms and show that they fail to achieve any interesting competitive guarantee. On the positive side, we obtain a $O(r)$-competitive deterministic algorithm using ideas from online learning and the multiplicative weight updates (MWU) algorithm.   Furthermore, we consider efficient algorithms. We propose a memoryless online algorithm, called Move-All-Equally, which is inspired by the Double Coverage algorithm for the $k$-server problem. We show that its competitive ratio is $\Omega(r^2)$ and $2^{O(\sqrt{\log n \cdot \log r})}$, and conjecture that it is $f(r)$-competitive. We also compare Move-All-Equally against the dynamic optimal solution and obtain (almost) tight bounds by showing that it is $\Omega(r \sqrt{n})$ and $O(r^{3/2} \sqrt{n})$-competitive.

</details>

<details>

<summary>2022-06-29 13:44:27 - InvAASTCluster: On Applying Invariant-Based Program Clustering to Introductory Programming Assignments</summary>

- *Pedro Orvalho, MikolÃ¡Å¡ Janota, Vasco Manquinho*

- `2206.14175v2` - [abs](http://arxiv.org/abs/2206.14175v2) - [pdf](http://arxiv.org/pdf/2206.14175v2)

> Due to the vast number of students enrolled in Massive Open Online Courses (MOOCs), there has been an increasing number of automated program repair techniques focused on introductory programming assignments (IPAs). Such state-of-the-art techniques use program clustering to take advantage of previous correct student implementations to repair a given new incorrect submission. Usually, these repair techniques use clustering methods since analyzing all available correct student submissions to repair a program is not feasible. The clustering methods use program representations based on several features such as abstract syntax tree (AST), syntax, control flow, and data flow. However, these features are sometimes brittle when representing semantically similar programs.   This paper proposes InvAASTCluster, a novel approach for program clustering that takes advantage of dynamically generated program invariants observed over several program executions to cluster semantically equivalent IPAs. Our main objective is to find a more suitable representation of programs using a combination of the program's semantics, through its invariants, and its structure, through its anonymized abstract syntax tree. The evaluation of InvAASTCluster shows that the proposed program representation outperforms syntax-based representations when clustering a set of different correct IPAs. Furthermore, we integrate InvAASTCluster into a state-of-the-art clustering-based program repair tool and evaluate it on a set of IPAs. Our results show that InvAASTCluster advances the current state-of-the-art when used by clustering-based program repair tools by repairing a larger number of students' programs in a shorter amount of time.

</details>

<details>

<summary>2022-06-29 18:52:58 - Quant-BnB: A Scalable Branch-and-Bound Method for Optimal Decision Trees with Continuous Features</summary>

- *Rahul Mazumder, Xiang Meng, Haoyue Wang*

- `2206.11844v2` - [abs](http://arxiv.org/abs/2206.11844v2) - [pdf](http://arxiv.org/pdf/2206.11844v2)

> Decision trees are one of the most useful and popular methods in the machine learning toolbox. In this paper, we consider the problem of learning optimal decision trees, a combinatorial optimization problem that is challenging to solve at scale. A common approach in the literature is to use greedy heuristics, which may not be optimal. Recently there has been significant interest in learning optimal decision trees using various approaches (e.g., based on integer programming, dynamic programming) -- to achieve computational scalability, most of these approaches focus on classification tasks with binary features. In this paper, we present a new discrete optimization method based on branch-and-bound (BnB) to obtain optimal decision trees. Different from existing customized approaches, we consider both regression and classification tasks with continuous features. The basic idea underlying our approach is to split the search space based on the quantiles of the feature distribution -- leading to upper and lower bounds for the underlying optimization problem along the BnB iterations. Our proposed algorithm Quant-BnB shows significant speedups compared to existing approaches for shallow optimal trees on various real datasets.

</details>

<details>

<summary>2022-06-29 21:12:19 - The Hiatus Between Organism and Machine Evolution: Contrasting Mixed Microbial Communities with Robots</summary>

- *Andrea Roli, Stuart A. Kauffman*

- `2206.14916v1` - [abs](http://arxiv.org/abs/2206.14916v1) - [pdf](http://arxiv.org/pdf/2206.14916v1)

> Mixed microbial communities, usually composed of various bacterial and fungal species, are fundamental in a plethora of environments, from soil to human gut and skin. Their evolution is a paradigmatic example of intertwined dynamics, where not just the relations among species plays a role, but also the opportunities -- and possible harms -- that each species presents to the others. These opportunities are in fact \textit{affordances}, which can be seized by heritable variation and selection. In this paper, starting from a systemic viewpoint of mixed microbial communities, we focus on the pivotal role of affordances in evolution and we contrast it to the artificial evolution of programs and robots. We maintain that the two realms are neatly separated, in that natural evolution proceeds by extending the space of its possibilities in a completely open way, while the latter is inherently limited by the algorithmic framework it is defined. This discrepancy characterises also an envisioned setting in which robots evolve in the physical world. We present arguments supporting our claim and we propose an experimental setting for assessing our statements. Rather than just discussing the limitations of the artificial evolution of machines, the aim of this contribution is to emphasize the tremendous potential of the evolution of the biosphere, beautifully represented by the evolution of communities of microbes.

</details>

<details>

<summary>2022-06-29 21:26:28 - Barrier Certified Safety Learning Control: When Sum-of-Square Programming Meets Reinforcement Learning</summary>

- *Hejun Huang, Zhenglong Li, Dongkun Han*

- `2206.07915v2` - [abs](http://arxiv.org/abs/2206.07915v2) - [pdf](http://arxiv.org/pdf/2206.07915v2)

> Safety guarantee is essential in many engineering implementations. Reinforcement learning provides a useful way to strengthen safety. However, reinforcement learning algorithms cannot completely guarantee safety over realistic operations. To address this issue, this work adopts control barrier functions over reinforcement learning, and proposes a compensated algorithm to completely maintain safety. Specifically, a sum-of-squares programming has been exploited to search for the optimal controller, and tune the learning hyperparameters simultaneously. Thus, the control actions are pledged to be always within the safe region. The effectiveness of proposed method is demonstrated via an inverted pendulum model. Compared to quadratic programming based reinforcement learning methods, our sum-of-squares programming based reinforcement learning has shown its superiority.

</details>

<details>

<summary>2022-06-30 00:30:38 - LAORAM: A Look Ahead ORAM Architecture for Training Large Embedding Tables</summary>

- *Rachit Rajat, Yongqin Wang, Murali Annavaram*

- `2107.08094v2` - [abs](http://arxiv.org/abs/2107.08094v2) - [pdf](http://arxiv.org/pdf/2107.08094v2)

> Data confidentiality is becoming a significant concern, especially in the cloud computing era. Memory access patterns have been demonstrated to leak critical information such as security keys and a program's spatial and temporal information. This information leak poses an even more significant privacy challenge in machine learning models with embedding tables. Embedding tables are routinely used to learn categorical features from training data. Even knowing the locations of the embedding table entries accessed, not the data within the embedding table, will compromise categorical input data to the model. Embedding entries are privacy-sensitive since they disclose valuable properties about the user. Oblivious RAM (ORAM), and its enhanced variants such as PathORAM have emerged as viable solutions to hide leakage from memory access streams.   In this work, we present LAORAM, an ORAM framework explicitly designed to protect user privacy during embedding table training. LAORAM exploits the unique property of training, the training samples used in the future are known beforehand. LAORAM preprocesses the training samples to identify the memory blocks which are accessed together in the near future. The system tries to assign these blocks to as few paths as possible within the PathORAM infrastructure.   LAORAM does this operation by combining multiple blocks accessed together as superblocks. To further increase performance, LAORAM uses a fat-tree structure for PathORAM reducing the number of background evictions required, which improves the stash usage. We have evaluated LAORAM using both a recommendation model (DLRM) and a NLP model (XLM-R) embedding table configurations. LAORAM performs 5 times faster than PathORAM on a recommendation dataset (Kaggle) and 5.4x faster on a NLP dataset (XNLI), while guaranteeing the same security guarantees as the original PathORAM.

</details>

<details>

<summary>2022-06-30 02:01:26 - Multiple Targets Directed Greybox Fuzzing</summary>

- *Hongliang Liang, Xianglin Cheng, Jie Liu, Jin Li*

- `2206.14977v1` - [abs](http://arxiv.org/abs/2206.14977v1) - [pdf](http://arxiv.org/pdf/2206.14977v1)

> Directed greybox fuzzing (DGF) can quickly discover or reproduce bugs in programs by seeking to reach a program location or explore some locations in order. However, due to their static stage division and coarse-grained energy scheduling, prior DGF tools perform poorly when facing multiple target locations (targets for short).   In this paper, we present multiple targets directed greybox fuzzing which aims to reach multiple programs locations in a fuzzing campaign. Specifically, we propose a novel strategy to adaptively coordinate exploration and exploitation stages, and a novel energy scheduling strategy by considering more relations between seeds and target locations. We implement our approaches in a tool called LeoFuzz and evaluate it on crash reproduction, true positives verification, and vulnerability exposure in real-world programs. Experimental results show that LeoFuzz outperforms six state-of-the-art fuzzers, i.e., QYSM, AFLGo, Lolly, Berry, Beacon and WindRanger in terms of effectiveness and efficiency. Moreover, LeoFuzz has detected 23 new vulnerabilities in real-world programs, and 11 of them have been assigned CVE IDs.

</details>

<details>

<summary>2022-06-30 05:54:51 - xFuzz: Machine Learning Guided Cross-Contract Fuzzing</summary>

- *Yinxing Xue, Jiaming Ye, Wei Zhang, Jun Sun, Lei Ma, Haijun Wang, Jianjun Zhao*

- `2111.12423v2` - [abs](http://arxiv.org/abs/2111.12423v2) - [pdf](http://arxiv.org/pdf/2111.12423v2)

> Smart contract transactions are increasingly interleaved by cross-contract calls. While many tools have been developed to identify a common set of vulnerabilities, the cross-contract vulnerability is overlooked by existing tools. Cross-contract vulnerabilities are exploitable bugs that manifest in the presence of more than two interacting contracts. Existing methods are however limited to analyze a maximum of two contracts at the same time. Detecting cross-contract vulnerabilities is highly non-trivial. With multiple interacting contracts, the search space is much larger than that of a single contract. To address this problem, we present xFuzz, a machine learning guided smart contract fuzzing framework. The machine learning models are trained with novel features (e.g., word vectors and instructions) and are used to filter likely benign program paths. Comparing with existing static tools, machine learning model is proven to be more robust, avoiding directly adopting manually-defined rules in specific tools. We compare xFuzz with three state-of-the-art tools on 7,391 contracts. xFuzz detects 18 exploitable cross-contract vulnerabilities, of which 15 vulnerabilities are exposed for the first time. Furthermore, our approach is shown to be efficient in detecting non-cross-contract vulnerabilities as well -- using less than 20% time as that of other fuzzing tools, xFuzz detects twice as many vulnerabilities.

</details>

<details>

<summary>2022-06-30 07:00:11 - Story-thinking, computational-thinking, programming and software engineering</summary>

- *Austen Rainer, Catherine Menon*

- `2206.15066v1` - [abs](http://arxiv.org/abs/2206.15066v1) - [pdf](http://arxiv.org/pdf/2206.15066v1)

> Working with stories and working with computations require very different modes of thought. We call the first mode "story-thinking" and the second "computational-thinking". The aim of this curiosity-driven paper is to explore the nature of these two modes of thinking, and to do so in relation to programming, including software engineering as programming-in-the-large. We suggest that story-thinking and computational-thinking may be understood as two ways of attending to the world, and that each both contributes and neglects the world, though in different ways and for different ends. We formulate two fundamental problems, i.e., the problem of "neglectful representations" and the problem of oppositional ways of thinking. We briefly suggest two ways in which these problems might be tackled and identify candidate hypotheses about the current state of the world, one assertion about a possible future state, and several research questions for future research.

</details>

<details>

<summary>2022-06-30 10:44:12 - Bio-inspired Machine Learning: programmed death and replication</summary>

- *Andrey Grabovsky, Vitaly Vanchurin*

- `2207.04886v1` - [abs](http://arxiv.org/abs/2207.04886v1) - [pdf](http://arxiv.org/pdf/2207.04886v1)

> We analyze algorithmic and computational aspects of biological phenomena, such as replication and programmed death, in the context of machine learning. We use two different measures of neuron efficiency to develop machine learning algorithms for adding neurons to the system (i.e. replication algorithm) and removing neurons from the system (i.e. programmed death algorithm). We argue that the programmed death algorithm can be used for compression of neural networks and the replication algorithm can be used for improving performance of the already trained neural networks. We also show that a combined algorithm of programmed death and replication can improve the learning efficiency of arbitrary machine learning systems. The computational advantages of the bio-inspired algorithms are demonstrated by training feedforward neural networks on the MNIST dataset of handwritten images.

</details>

<details>

<summary>2022-06-30 15:35:50 - Why we do need Explainable AI for Healthcare</summary>

- *Giovanni CinÃ , Tabea RÃ¶ber, Rob Goedhart, Ilker Birbil*

- `2206.15363v1` - [abs](http://arxiv.org/abs/2206.15363v1) - [pdf](http://arxiv.org/pdf/2206.15363v1)

> The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology. One thread of such debate concerns Explainable AI and its promise to render AI devices more transparent and trustworthy. A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques, questioning their use and inclusion in guidelines and standards. Revisiting such criticisms, this article offers a balanced and comprehensive perspective on the utility of Explainable AI, focusing on the specificity of clinical applications of AI and placing them in the context of healthcare interventions. Against its detractors and despite valid concerns, we argue that the Explainable AI research program is still central to human-machine interaction and ultimately our main tool against loss of control, a danger that cannot be prevented by rigorous clinical validation alone.

</details>

<details>

<summary>2022-06-30 15:58:40 - Implementing a Chatbot Solution for Learning Management System</summary>

- *Dimitrios Chaskopoulos, Jonas Eilertsen HÃ¦gdahl, Petter Sagvold, Claire Trinquet, Maryam Edalati*

- `2206.13187v2` - [abs](http://arxiv.org/abs/2206.13187v2) - [pdf](http://arxiv.org/pdf/2206.13187v2)

> Innovation is a key component in trying new solutions for the students to learn efficiently and in ways that correspond to their own experience, where chatbots are one of these new solutions. One of the main problem that chatbots face today is to mimic human language, where they try to find the best answer to an input, which is not how a human conversation usually works, rather taking into account the previous messages and building onto them. Extreme programming methodology was chosen to use integrate ChatterBot, Pyside2, web scraping and Tampermonkey into Blackboard as a test case. Problems occurred with the bot and more training was needed for the bot to work perfectly, but the integration and web scraping worked, giving us a chatbot that was able to talk with. We showed the plausibility of integrating an AI bot in an educational setting.

</details>

<details>

<summary>2022-06-30 16:19:21 - j-Wave: An open-source differentiable wave simulator</summary>

- *Antonio Stanziola, Simon R. Arridge, Ben T. Cox, Bradley E. Treeby*

- `2207.01499v1` - [abs](http://arxiv.org/abs/2207.01499v1) - [pdf](http://arxiv.org/pdf/2207.01499v1)

> We present an open-source differentiable acoustic simulator, j-Wave, which can solve time-varying and time-harmonic acoustic problems. It supports automatic differentiation, which is a program transformation technique that has many applications, especially in machine learning and scientific computing. j-Wave is composed of modular components that can be easily customized and reused. At the same time, it is compatible with some of the most popular machine learning libraries, such as JAX and TensorFlow. The accuracy of the simulation results for known configurations is evaluated against the widely used k-Wave toolbox and a cohort of acoustic simulation software. j-Wave is available from https://github.com/ucl-bug/jwave.

</details>

<details>

<summary>2022-06-30 22:09:47 - Predicting Ulnar Collateral Ligament Injury in Rookie Major League Baseball Pitchers</summary>

- *Sean A. Rendar, Fenglong Ma*

- `2207.00585v1` - [abs](http://arxiv.org/abs/2207.00585v1) - [pdf](http://arxiv.org/pdf/2207.00585v1)

> In the growing world of machine learning and data analytics, scholars are finding new and innovative ways to solve real-world problems. One solution comes by way of an intersection between healthcare, sports statistics, and data sciences. Within the realm of Major League Baseball (MLB), pitchers are regarded as the most important roster position. They often are among the highest paid players and are crucial to a franchise's success, but they are more at risk to suffer an injury that sidelines them for over a complete season. The ulnar collateral ligament (UCL) is a small ligament in the elbow that controls the strength and stability of a pitcher's throwing arm. Due to repetitive strain, it is not uncommon for pitchers to tear it partially or completely during their careers. Repairing this injury requires UCL reconstruction surgery, as known informally as Tommy John surgery. In this podium abstract, we want to investigate whether we can use machine learning techniques to predict UCL injury by analyzing online pitcher data.

</details>


## 2022-07

<details>

<summary>2022-07-01 09:49:17 - Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes</summary>

- *Cedric Richter, Heike Wehrheim*

- `2207.00301v1` - [abs](http://arxiv.org/abs/2207.00301v1) - [pdf](http://arxiv.org/pdf/2207.00301v1)

> Real bug fixes found in open source repositories seem to be the perfect source for learning to localize and repair real bugs. However, the absence of large scale bug fix collections has made it difficult to effectively exploit real bug fixes in the training of larger neural models in the past. In contrast, artificial bugs -- produced by mutating existing source code -- can be easily obtained at a sufficient scale and are therefore often preferred in the training of existing approaches. Still, localization and repair models that are trained on artificial bugs usually underperform when faced with real bugs. This raises the question whether bug localization and repair models trained on real bug fixes are more effective in localizing and repairing real bugs.   We address this question by introducing RealiT, a pre-train-and-fine-tune approach for effectively learning to localize and repair real bugs from real bug fixes. RealiT is first pre-trained on a large number of artificial bugs produced by traditional mutation operators and then fine-tuned on a smaller set of real bug fixes. Fine-tuning does not require any modifications of the learning algorithm and hence can be easily adopted in various training scenarios for bug localization or repair (even when real training data is scarce). In addition, we found that training on real bug fixes with RealiT is empirically powerful by nearly doubling the localization performance of an existing model on real bugs while maintaining or even improving the repair performance.

</details>

<details>

<summary>2022-07-02 07:04:20 - Abstraction and Refinement: Towards Scalable and Exact Verification of Neural Networks</summary>

- *Jiaxiang Liu, Yunhan Xing, Xiaomu Shi, Fu Song, Zhiwu Xu, Zhong Ming*

- `2207.00759v1` - [abs](http://arxiv.org/abs/2207.00759v1) - [pdf](http://arxiv.org/pdf/2207.00759v1)

> As a new programming paradigm, deep neural networks (DNNs) have been increasingly deployed in practice, but the lack of robustness hinders their applications in safety-critical domains. While there are techniques for verifying DNNs with formal guarantees, they are limited in scalability and accuracy. In this paper, we present a novel abstraction-refinement approach for scalable and exact DNN verification. Specifically, we propose a novel abstraction to break down the size of DNNs by over-approximation. The result of verifying the abstract DNN is always conclusive if no spurious counterexample is reported. To eliminate spurious counterexamples introduced by abstraction, we propose a novel counterexample-guided refinement that refines the abstract DNN to exclude a given spurious counterexample while still over-approximating the original one. Our approach is orthogonal to and can be integrated with many existing verification techniques. For demonstration, we implement our approach using two promising and exact tools Marabou and Planet as the underlying verification engines, and evaluate on widely-used benchmarks ACAS Xu, MNIST and CIFAR-10. The results show that our approach can boost their performance by solving more problems and reducing up to 86.3% and 78.0% verification time, respectively. Compared to the most relevant abstraction-refinement approach, our approach is 11.6-26.6 times faster.

</details>

<details>

<summary>2022-07-02 07:18:04 - Neuromorphic Computing with Ferroelectric FinFETs in the Presence of Temperature, Process Variation, Device Aging and Flicker Noise</summary>

- *Sourav De, Bo-Han Qiu, Wei-Xuan Bu, Md. Aftab Baig, Chung-Jun Su, Yao-Jen Lee, Darsen Lu*

- `2103.13302v2` - [abs](http://arxiv.org/abs/2103.13302v2) - [pdf](http://arxiv.org/pdf/2103.13302v2)

> This paper reports a comprehensive study on the impacts of temperature-change, process variation, flicker noise and device aging on the inference accuracy of pre-trained all-ferroelectric (FE) FinFET deep neural networks. Multiple-level-cell (MLC) operation with a novel adaptive-program-and-read algorithm with 100ns write pulse has been experimentally demonstrated in 5 nm thick hafnium zirconium oxide (HZO)-based FE-FinFET. With pre-trained neural network (NN) with 97.5% inference accuracy on MNIST dataset as baseline, device to device variation is shown to have negligible impact. Flicker noise characterization at various bias conditions depicts that drain current fluctuation is less than 0.7% with virtually no inference accuracy degradation. The conductance drift of a programmed cell, as an aftermath of temperature change, was captured by a compact model over a wide range of gate biases. Despite significant inference accuracy degradation at 233K for a NN trained at 300K, gate bias optimization for recovering the accuracy is demonstrated. Endurance above 10$^8$ cycles and extrapolated retention above 10 years are shown, which paves the way for edge device artificial intelligence with FE-FinFETs.

</details>

<details>

<summary>2022-07-02 14:34:54 - Combinatory Adjoints and Differentiation</summary>

- *Martin Elsman, Fritz Henglein, Robin Kaarsgaard, Mikkel Kragh Mathiesen, Robert Schenck*

- `2207.00847v1` - [abs](http://arxiv.org/abs/2207.00847v1) - [pdf](http://arxiv.org/pdf/2207.00847v1)

> We develop a compositional approach for automatic and symbolic differentiation based on categorical constructions in functional analysis where derivatives are linear functions on abstract vectors rather than being limited to scalars, vectors, matrices or tensors represented as multi-dimensional arrays. We show that both symbolic and automatic differentiation can be performed using a differential calculus for generating linear functions representing Fr\'echet derivatives based on rules for primitive, constant, linear and bilinear functions as well as their sequential and parallel composition. Linear functions are represented in a combinatory domain-specific language. Finally, we provide a calculus for symbolically computing the adjoint of a derivative without using matrices, which are too inefficient to use on high-dimensional spaces. The resulting symbolic representation of a derivative retains the data-parallel operations from the input program. The combination of combinatory differentiation and computing formal adjoints turns out to be behaviorally equivalent to reverse-mode automatic differentiation. In particular, it provides opportunities for optimizations where matrices are too inefficient to represent linear functions.

</details>

<details>

<summary>2022-07-02 14:35:33 - Tableless Calculation of Circular Functions on Dyadic Rationals</summary>

- *Peter Kourzanov*

- `2207.00849v1` - [abs](http://arxiv.org/abs/2207.00849v1) - [pdf](http://arxiv.org/pdf/2207.00849v1)

> I would like to tell a story. A story about a beautiful mathematical relationship that elucidates the computational view on the classic subject of trigonometry. All stories need a language, and for this particular story an algorithmic language ought to do well. What makes a language algorithmic? From our perspective as the functional programming community, an algorithmic language provides means to express computation in terms of functions, with no implementation-imposed limitations. We develop a new algorithm for the computation of trigonometric functions on dyadic rationals, together with the language used to express it, in Scheme. We provide a mechanically-derived algorithm for the computation of the inverses of our target functions. We address efficiency and accuracy concerns that pertain to the implementation of the proposed algorithm either in hardware or software.

</details>

<details>

<summary>2022-07-02 22:59:34 - Approximating Dynamic Time Warping Distance Between Run-Length Encoded Strings</summary>

- *Zoe Xi, William Kuszmaul*

- `2207.00915v1` - [abs](http://arxiv.org/abs/2207.00915v1) - [pdf](http://arxiv.org/pdf/2207.00915v1)

> Dynamic Time Warping (DTW) is a widely used similarity measure for comparing strings that encode time series data, with applications to areas including bioinformatics, signature verification, and speech recognition. The standard dynamic-programming algorithm for DTW takes $O(n^2)$ time, and there are conditional lower bounds showing that no algorithm can do substantially better.   In many applications, however, the strings $x$ and $y$ may contain long runs of repeated letters, meaning that they can be compressed using run-length encoding. A natural question is whether the DTW-distance between these compressed strings can be computed efficiently in terms of the lengths $k$ and $\ell$ of the compressed strings. Recent work has shown how to achieve $O(k\ell^2 + \ell k^2)$ time, leaving open the question of whether a near-quadratic $\tilde{O}(k\ell)$-time algorithm might exist.   We show that, if a small approximation loss is permitted, then a near-quadratic time algorithm is indeed possible: our algorithm computes a $(1 + \epsilon)$-approximation for $DTW(x, y)$ in $\tilde{O}(k\ell / \epsilon^3)$ time, where $k$ and $\ell$ are the number of runs in $x$ and $y$. Our algorithm allows for $DTW$ to be computed over any metric space $(\Sigma, \delta)$ in which distances are $O(log(n))$-bit integers. Surprisingly, the algorithm also works even if $\delta$ does not induce a metric space on $\Sigma$ (e.g., $\delta$ need not satisfy the triangle inequality).

</details>

<details>

<summary>2022-07-02 23:39:45 - An AlphaZero-Inspired Approach to Solving Search Problems</summary>

- *Evgeny Dantsin, Vladik Kreinovich, Alexander Wolpert*

- `2207.00919v1` - [abs](http://arxiv.org/abs/2207.00919v1) - [pdf](http://arxiv.org/pdf/2207.00919v1)

> AlphaZero and its extension MuZero are computer programs that use machine-learning techniques to play at a superhuman level in chess, go, and a few other games. They achieved this level of play solely with reinforcement learning from self-play, without any domain knowledge except the game rules. It is a natural idea to adapt the methods and techniques used in AlphaZero for solving search problems such as the Boolean satisfiability problem (in its search version). Given a search problem, how to represent it for an AlphaZero-inspired solver? What are the "rules of solving" for this search problem? We describe possible representations in terms of easy-instance solvers and self-reductions, and we give examples of such representations for the satisfiability problem. We also describe a version of Monte Carlo tree search adapted for search problems.

</details>

<details>

<summary>2022-07-02 23:55:50 - Auto-active Verification of Floating-point Programs via Nonlinear Real Provers</summary>

- *Junaid Rasheed, Michal KoneÄnÃ½*

- `2207.00921v1` - [abs](http://arxiv.org/abs/2207.00921v1) - [pdf](http://arxiv.org/pdf/2207.00921v1)

> We give a process for verifying numerical programs against their functional specifications. Our implementation is capable of automatically verifying programs against tight error bounds featuring common elementary functions. We demonstrate and evaluate our implementation on several examples, yielding the first fully verified SPARK implementations of the sine and square root functions. The process integrates existing tools using a series of transformations and derivations, building on the proving process in SPARK where Why3 produces Verification Conditions (VCs) and tools such as SMT solvers attempt to verify them. We add steps aimed specifically at VCs that contain inequalities with both floating-point operations and exact real functions. PropaFP is our open-source implementation of these steps. The steps include symbolic simplifications, deriving bounds via interval arithmetic, and safely replacing floating-point operations with exact operations, utilizing tools such as FPTaylor or Gappa to bound the compound rounding errors of expressions. Finally, the VCs are passed to provers such as dReal, MetiTarski or LPPaver which attempt to complete the proof or suggest possible counter-examples.

</details>

<details>

<summary>2022-07-03 00:06:39 - Computable Contracts in the Financial Services Industry</summary>

- *Vinay K Chaudhri*

- `2208.04685v1` - [abs](http://arxiv.org/abs/2208.04685v1) - [pdf](http://arxiv.org/pdf/2208.04685v1)

> A computable contract is a contract that a computer can read, understand and execute. The financial services industry makes extensive use of contracts, for example, mortgage agreements, derivatives contracts, arbitration agreements, etc. Most of these contracts exist as text documents, making it difficult to automatically query, execute and analyze them. In this vision paper, we argue that the use of computable contracts in the financial services industry will lead to substantial improvements in customer experience, reductions in the cost of doing legal transactions, make it easier to respond to changing laws, and provide a much better framework for making decisions impacted by contracts. Using a simple payment agreement, we illustrate a Contract Definition Language, sketch several use cases and discuss their benefits to the financial services industry.

</details>

<details>

<summary>2022-07-03 13:56:44 - Histopathological Imaging Classification of Breast Tissue for Cancer Diagnosis Support Using Deep Learning Models</summary>

- *Tat-Bao-Thien Nguyen, Minh-Vuong Ngo, Van-Phong Nguyen*

- `2207.05057v1` - [abs](http://arxiv.org/abs/2207.05057v1) - [pdf](http://arxiv.org/pdf/2207.05057v1)

> According to some medical imaging techniques, breast histopathology images called Hematoxylin and Eosin are considered as the gold standard for cancer diagnoses. Based on the idea of dividing the pathologic image (WSI) into multiple patches, we used the window [512,512] sliding from left to right and sliding from top to bottom, each sliding step overlapping by 50% to augmented data on a dataset of 400 images which were gathered from the ICIAR 2018 Grand Challenge. Then use the EffficientNet model to classify and identify the histopathological images of breast cancer into 4 types: Normal, Benign, Carcinoma, Invasive Carcinoma. The EffficientNet model is a recently developed model that uniformly scales the width, depth, and resolution of the network with a set of fixed scaling factors that are well suited for training images with high resolution. And the results of this model give a rather competitive classification efficiency, achieving 98% accuracy on the training set and 93% on the evaluation set.

</details>

<details>

<summary>2022-07-03 14:03:09 - An Empirical Study of Flaky Tests in JavaScript</summary>

- *Negar Hashemi, Amjed Tahir, Shawn Rasheed*

- `2207.01047v1` - [abs](http://arxiv.org/abs/2207.01047v1) - [pdf](http://arxiv.org/pdf/2207.01047v1)

> Flaky tests (tests with non-deterministic outcomes) can be problematic for testing efficiency and software reliability. Flaky tests in test suites can also significantly delay software releases. There have been several studies that attempt to quantify the impact of test flakiness in different programming languages (e.g., Java and Python) and application domains (e.g., mobile and GUI-based). In this paper, we conduct an empirical study of the state of flaky tests in JavaScript. We investigate two aspects of flaky tests in JavaScript projects: the main causes of flaky tests in these projects and common fixing strategies. By analysing 452 commits from large, top-scoring JavaScript projects from GitHub, we found that flakiness caused by concurrency-related issues (e.g., async wait, race conditions or deadlocks) is the most dominant reason for test flakiness. The other top causes of flaky tests are operating system-specific (e.g., features that work on specific OS or OS versions) and network stability (e.g., internet availability or bad socket management). In terms of how flaky tests are dealt with, the majority of those flaky tests (>80%) are fixed to eliminate flaky behaviour and developers sometimes skip, quarantine or remove flaky tests.

</details>

<details>

<summary>2022-07-03 21:13:55 - Chimera: A Hybrid Machine Learning Driven Multi-Objective Design Space Exploration Tool for FPGA High-Level Synthesis</summary>

- *Mang Yu, Sitao Huang, Deming Chen*

- `2207.07917v1` - [abs](http://arxiv.org/abs/2207.07917v1) - [pdf](http://arxiv.org/pdf/2207.07917v1)

> In recent years, hardware accelerators based on field-programmable gate arrays (FPGAs) have been widely adopted, thanks to FPGAs' extraordinary flexibility. However, with the high flexibility comes the difficulty in design and optimization. Conventionally, these accelerators are designed with low-level hardware descriptive languages, which means creating large designs with complex behavior is extremely difficult. Therefore, high-level synthesis (HLS) tools were created to simplify hardware designs for FPGAs. They enable the user to create hardware designs using high-level languages and provide various optimization directives to help to improve the performance of the synthesized hardware. However, applying these optimizations to achieve high performance is time-consuming and usually requires expert knowledge. To address this difficulty, we present an automated design space exploration tool for applying HLS optimization directives, called Chimera, which significantly reduces the human effort and expertise needed for creating high-performance HLS designs. It utilizes a novel multi-objective exploration method that seamlessly integrates active learning, evolutionary algorithm, and Thompson sampling, making it capable of finding a set of optimized designs on a Pareto curve with only a small number of design points evaluated during the exploration. In the experiments, in less than 24 hours, this hybrid method explored design points that have the same or superior performance compared to highly optimized hand-tuned designs created by expert HLS users from the Rosetta benchmark suite. In addition to discovering the extreme points, it also explores a Pareto frontier, where the elbow point can potentially save up to 26\% of Flip-Flop resource with negligibly higher latency.

</details>

<details>

<summary>2022-07-04 07:30:51 - Satellite downlink scheduling under breakpoint resume mode</summary>

- *Zhongxiang Chang, Yuning Chen, Zhongbao Zhou*

- `2207.01239v1` - [abs](http://arxiv.org/abs/2207.01239v1) - [pdf](http://arxiv.org/pdf/2207.01239v1)

> A novel problem called satellite downlink scheduling problem (SDSP) under breakpoint resume mode (SDSP-BRM) is studied in our paper. Compared to the traditional SDSP where an imaging data has to be completely downloaded at one time, SDSP-BRM allows the data of an imaging data be broken into a number of pieces which can be downloaded in different playback windows. By analyzing the characteristics of SDSP-BRM, we first propose a mixed integer programming model for its formulation and then prove the NP-hardness of SDSP-BRM. To solve the problem, we design a simple and effective heuristic algorithm (SEHA) where a number of problem-tailored move operators are proposed for local searching. Numerical results on a set of well-designed scenarios demonstrate the efficiency of the proposed algorithm in comparison to the general purpose CPLEX solver. We conduct additional experiments to shed light on the impact of the segmental strategy on the overall performance of the proposed SEHA.

</details>

<details>

<summary>2022-07-04 10:42:13 - Advantages and Disadvantages of (Dedicated) Model Transformation Languages A Qualitative Interview Study</summary>

- *Stefan HÃ¶ppner, Yves Haas, Matthias Tichy, Katharina Juhnke*

- `2201.13348v3` - [abs](http://arxiv.org/abs/2201.13348v3) - [pdf](http://arxiv.org/pdf/2201.13348v3)

> Model driven development envisages the use of model transformations to evolve models. Model transformation languages, developed for this task, are touted with many benefits over general purpose programming languages. However, a large number of these claims have not yet been substantiated. They are also made without the context necessary to be able to critically assess their merit or built meaningful empirical studies around them. The objective of our work is to elicit the reasoning, influences and background knowledge that lead people to assume benefits or drawbacks of model transformation languages. We conducted a large-scale interview study involving 56 participants from research and industry. Interviewees were presented with claims about model transformation languages and were asked to provide reasons for their assessment thereof. We qualitatively analysed the responses to find factors that influence the properties of model transformation languages as well as explanations as to how exactly they do so. Our interviews show, that general purpose expressiveness of GPLs, domain specific capabilities of MTLs as well as tooling all have strong influences on how people view properties of model transformation languages. Moreover, the Choice of MTL, the Use Case for which a transformation should be developed as well as the Skills of involved stakeholders have a moderating effect on the influences, by changing the context to consider. There is a broad body of experience, that suggests positive and negative influences for properties of MTLs. Our data suggests, that much needs to be done in order to convey the viability of model transformation languages. Efforts to provide more empirical substance need to be undergone and lackluster language capabilities and tooling need to be improved upon. We suggest several approaches for this that can be based on the results of the presented study.

</details>

<details>

<summary>2022-07-04 16:43:05 - RegMiner: Towards Constructing a Large Regression Dataset from Code Evolution History</summary>

- *Xuezhi Song, Yun Lin, Siang Hwee Ng, Yijian Wu, Xin Peng, Jin Song Dong, Hong Mei*

- `2109.12389v2` - [abs](http://arxiv.org/abs/2109.12389v2) - [pdf](http://arxiv.org/pdf/2109.12389v2)

> Bug datasets consisting of real-world bugs are important artifacts for researchers and programmers, which lay empirical and experimental foundation for various SE/PL research such as fault localization, software testing, and program repair. All known state-of-the-art datasets are constructed manually, which inevitably limits their scalability, representativeness, and the support for the emerging data-driven research. In this work, we propose an approach to automate the process of harvesting replicable regression bugs from the code evolutionary history. We focus on regression bug dataset, as they (1) manifest how a bug is introduced and fixed (as normal bugs), (2) support regression bug analysis, and (3) incorporate a much stronger specification (i.e., the original passing version) for general bug analysis. Technically, we address an information retrieval problem on code evolution history. Given a code repository, we search for regressions where a test can pass a regression-fixing commit, fail a regressioninducing commit, and pass a working commit. In this work, we address the challenges of (1) identifying potential regression-fixing commits from the code evolution history, (2) migrating the test and its code dependencies over the history, and (3) minimizing the compilation overhead during the regression search. We build our tool, RegMiner, which harvested 537 regressions over 66 projects for 3 weeks, created the largest replicable regression dataset within shortest period, to the best of our knowledge. Moreover, our empirical study on our regression dataset shows a gap between the popular regression fault localization techniques (e.g, delta-debugging) and the real fix, revealing new data-driven research opportunities.

</details>

<details>

<summary>2022-07-04 20:59:11 - Elysium: Context-Aware Bytecode-Level Patching to Automatically Heal Vulnerable Smart Contracts</summary>

- *Christof Ferreira Torres, Hugo Jonker, Radu State*

- `2108.10071v3` - [abs](http://arxiv.org/abs/2108.10071v3) - [pdf](http://arxiv.org/pdf/2108.10071v3)

> Fixing bugs is easiest by patching source code. However, source code is not always available: only 0.3% of the ~49M smart contracts that are currently deployed on Ethereum have their source code publicly available. Moreover, since contracts may call functions from other contracts, security flaws in closed-source contracts may affect open-source contracts as well. However, current state-of-the-art approaches that operate on closed-source contracts (i.e., EVM bytecode), such as EVMPatch and SmartShield, make use of purely hard-coded templates that leverage fix patching patterns. As a result, they cannot dynamically adapt to the bytecode that is being patched, which severely limits their flexibility and scalability. For instance, when patching integer overflows using hard-coded templates, a particular patch template needs to be employed as the bounds to be checked are different for each integer size. In this paper, we propose Elysium, a scalable approach towards automatic smart contract repair at the bytecode level. Elysium combines template-based and semantic-based patching by inferring context information from bytecode. Elysium is currently able to patch 7 different types of vulnerabilities in smart contracts automatically and can easily be extended with new templates and new bug-finding tools. We evaluate its effectiveness and correctness using 3 different datasets by replaying more than 500K transactions on patched contracts. We find that Elysium outperforms existing tools by patching at least 30% more contracts correctly. Finally, we also compare the overhead of Elysium in terms of deployment and transaction cost. In comparison to other tools, we find that generally Elysium minimizes the runtime cost (i.e., transaction cost) up to a factor of 1.7, for only a marginally higher deployment cost, where deployment cost is a one-time cost as compared to the runtime cost.

</details>

<details>

<summary>2022-07-05 07:39:32 - DiffML: End-to-end Differentiable ML Pipelines</summary>

- *Benjamin Hilprecht, Christian Hammacher, Eduardo Reis, Mohamed Abdelaal, Carsten Binnig*

- `2207.01269v2` - [abs](http://arxiv.org/abs/2207.01269v2) - [pdf](http://arxiv.org/pdf/2207.01269v2)

> In this paper, we present our vision of differentiable ML pipelines called DiffML to automate the construction of ML pipelines in an end-to-end fashion. The idea is that DiffML allows to jointly train not just the ML model itself but also the entire pipeline including data preprocessing steps, e.g., data cleaning, feature selection, etc. Our core idea is to formulate all pipeline steps in a differentiable way such that the entire pipeline can be trained using backpropagation. However, this is a non-trivial problem and opens up many new research questions. To show the feasibility of this direction, we demonstrate initial ideas and a general principle of how typical preprocessing steps such as data cleaning, feature selection and dataset selection can be formulated as differentiable programs and jointly learned with the ML model. Moreover, we discuss a research roadmap and core challenges that have to be systematically tackled to enable fully differentiable ML pipelines.

</details>

<details>

<summary>2022-07-05 10:47:20 - Static Deadlock Detection in Low-Level C Code</summary>

- *Dominik Harmim, VladimÃ­r Marcin, Lucie SvobodovÃ¡, TomÃ¡Å¡ Vojnar*

- `2207.01948v1` - [abs](http://arxiv.org/abs/2207.01948v1) - [pdf](http://arxiv.org/pdf/2207.01948v1)

> We present a novel scalable deadlock analyser L2D2 capable of handling C code with low-level unstructured lock manipulation. L2D2 runs along the call tree of a program, starting from its leaves, and analyses each function just once, without any knowledge of the call context. L2D2 builds function summaries recording information about locks that are assumed or known to be locked or unlocked at the entry, inside, and at the exit of functions, together with lock dependencies, and reports warnings about possible deadlocks when cycles in the lock dependencies are detected. We implemented L2D2 as a plugin of the Facebook/Meta Infer framework and report results of experiments on a large body of C as well as C++ code illustrating the effectiveness and efficiency of L2D2.

</details>

<details>

<summary>2022-07-05 12:00:57 - Insights into the origin of halo mass profiles from machine learning</summary>

- *Luisa Lucie-Smith, Susmita Adhikari, Risa H. Wechsler*

- `2205.04474v2` - [abs](http://arxiv.org/abs/2205.04474v2) - [pdf](http://arxiv.org/pdf/2205.04474v2)

> The mass distribution of dark matter haloes is the result of the hierarchical growth of initial density perturbations through mass accretion and mergers. We use an interpretable machine-learning framework to provide physical insights into the origin of the spherically-averaged mass profile of dark matter haloes. We train a gradient-boosted-trees algorithm to predict the final mass profiles of cluster-sized haloes, and measure the importance of the different inputs provided to the algorithm. We find two primary scales in the initial conditions (ICs) that impact the final mass profile: the density at approximately the scale of the haloes' Lagrangian patch $R_L$ ($R\sim 0.7\, R_L$) and that in the large-scale environment ($R\sim 1.7~R_L$). The model also identifies three primary time-scales in the halo assembly history that affect the final profile: (i) the formation time of the virialized, collapsed material inside the halo, (ii) the dynamical time, which captures the dynamically unrelaxed, infalling component of the halo over its first orbit, (iii) a third, most recent time-scale, which captures the impact on the outer profile of recent massive merger events. While the inner profile retains memory of the ICs, this information alone is insufficient to yield accurate predictions for the outer profile. As we add information about the haloes' mass accretion history, we find a significant improvement in the predicted profiles at all radii. Our machine-learning framework provides novel insights into the role of the ICs and the mass assembly history in determining the final mass profile of cluster-sized haloes.

</details>

<details>

<summary>2022-07-05 12:24:21 - Vulpedia: Detecting Vulnerable Ethereum Smart Contracts via Abstracted Vulnerability Signatures</summary>

- *Jiaming Ye, Mingliang Ma, Yun Lin, Lei Ma, Yinxing Xue, Jianjun Zhao*

- `1912.04466v2` - [abs](http://arxiv.org/abs/1912.04466v2) - [pdf](http://arxiv.org/pdf/1912.04466v2)

> Recent years have seen smart contracts are getting increasingly popular in building trustworthy decentralized applications. Previous research has proposed static and dynamic techniques to detect vulnerabilities in smart contracts. These tools check vulnerable contracts against several predefined rules. However, the emerging new vulnerable types and programming skills to prevent possible vulnerabilities emerging lead to a large number of false positive and false negative reports of tools. To address this, we propose Vulpedia, which mines expressive vulnerability signatures from contracts. Vulpedia is based on the relaxed assumption that the owner of contract is not malicious. Specifically, we extract structural program features from vulnerable and benign contracts as vulnerability signatures, and construct a systematic detection method based on detection rules composed of vulnerability signatures. Compared with the rules defined by state-of-the-arts, our approach can extract more expressive rules to achieve better completeness (i.e., detection recall) and soundness (i.e., precision). We further evaluate Vulpedia with four baselines (i.e., Slither, Securify, SmartCheck and Oyente) on the testing dataset consisting of 17,770 contracts. The experiment results show that Vulpedia achieves best performance of precision on 4 types of vulnerabilities and leading recall on 3 types of vulnerabilities meanwhile exhibiting the great efficiency performance.

</details>

<details>

<summary>2022-07-05 14:46:47 - Learning to Accelerate Approximate Methods for Solving Integer Programming via Early Fixing</summary>

- *Longkang Li, Baoyuan Wu*

- `2207.02087v1` - [abs](http://arxiv.org/abs/2207.02087v1) - [pdf](http://arxiv.org/pdf/2207.02087v1)

> Integer programming (IP) is an important and challenging problem. Approximate methods have shown promising performance on both effectiveness and efficiency for solving the IP problem. However, we observed that a large fraction of variables solved by some iterative approximate methods fluctuate around their final converged discrete states in very long iterations. Inspired by this observation, we aim to accelerate these approximate methods by early fixing these fluctuated variables to their converged states while not significantly harming the solution accuracy. To this end, we propose an early fixing framework along with the approximate method. We formulate the whole early fixing process as a Markov decision process, and train it using imitation learning. A policy network will evaluate the posterior probability of each free variable concerning its discrete candidate states in each block of iterations. Specifically, we adopt the powerful multi-headed attention mechanism in the policy network. Extensive experiments on our proposed early fixing framework are conducted to three different IP applications: constrained linear programming, MRF energy minimization and sparse adversarial attack. The former one is linear IP problem, while the latter two are quadratic IP problems. We extend the problem scale from regular size to significantly large size. The extensive experiments reveal the competitiveness of our early fixing framework: the runtime speeds up significantly, while the solution quality does not degrade much, even in some cases it is available to obtain better solutions. Our proposed early fixing framework can be regarded as an acceleration extension of ADMM methods for solving integer programming. The source codes are available at \url{https://github.com/SCLBD/Accelerated-Lpbox-ADMM}.

</details>

<details>

<summary>2022-07-05 15:26:48 - Simultaneous Contact-Rich Grasping and Locomotion via Distributed Optimization Enabling Free-Climbing for Multi-Limbed Robots</summary>

- *Yuki Shirai, Xuan Lin, Alexander Schperberg, Yusuke Tanaka, Hayato Kato, Varit Vichathorn, Dennis Hong*

- `2207.01418v2` - [abs](http://arxiv.org/abs/2207.01418v2) - [pdf](http://arxiv.org/pdf/2207.01418v2)

> While motion planning of locomotion for legged robots has shown great success, motion planning for legged robots with dexterous multi-finger grasping is not mature yet. We present an efficient motion planning framework for simultaneously solving locomotion (e.g., centroidal dynamics), grasping (e.g., patch contact), and contact (e.g., gait) problems. To accelerate the planning process, we propose distributed optimization frameworks based on Alternating Direction Methods of Multipliers (ADMM) to solve the original large-scale Mixed-Integer NonLinear Programming (MINLP). The resulting frameworks use Mixed-Integer Quadratic Programming (MIQP) to solve contact and NonLinear Programming (NLP) to solve nonlinear dynamics, which are more computationally tractable and less sensitive to parameters. Also, we explicitly enforce patch contact constraints from limit surfaces with micro-spine grippers. We demonstrate our proposed framework in the hardware experiments, showing that the multi-limbed robot is able to realize various motions including free-climbing at a slope angle 45{\deg} with a much shorter planning time.

</details>

<details>

<summary>2022-07-05 15:49:29 - ADVISER: AI-Driven Vaccination Intervention Optimiser for Increasing Vaccine Uptake in Nigeria</summary>

- *Vineet Nair, Kritika Prakash, Michael Wilbur, Aparna Taneja, Corinne Namblard, Oyindamola Adeyemo, Abhishek Dubey, Abiodun Adereni, Milind Tambe, Ayan Mukhopadhyay*

- `2204.13663v3` - [abs](http://arxiv.org/abs/2204.13663v3) - [pdf](http://arxiv.org/pdf/2204.13663v3)

> More than 5 million children under five years die from largely preventable or treatable medical conditions every year, with an overwhelmingly large proportion of deaths occurring in under-developed countries with low vaccination uptake. One of the United Nations' sustainable development goals (SDG 3) aims to end preventable deaths of newborns and children under five years of age. We focus on Nigeria, where the rate of infant mortality is appalling. We collaborate with HelpMum, a large non-profit organization in Nigeria to design and optimize the allocation of heterogeneous health interventions under uncertainty to increase vaccination uptake, the first such collaboration in Nigeria. Our framework, ADVISER: AI-Driven Vaccination Intervention Optimiser, is based on an integer linear program that seeks to maximize the cumulative probability of successful vaccination. Our optimization formulation is intractable in practice. We present a heuristic approach that enables us to solve the problem for real-world use-cases. We also present theoretical bounds for the heuristic method. Finally, we show that the proposed approach outperforms baseline methods in terms of vaccination uptake through experimental evaluation. HelpMum is currently planning a pilot program based on our approach to be deployed in the largest city of Nigeria, which would be the first deployment of an AI-driven vaccination uptake program in the country and hopefully, pave the way for other data-driven programs to improve health outcomes in Nigeria.

</details>

<details>

<summary>2022-07-05 18:02:17 - Authentication Devices in Fog-Mobile Edge Computing Environments Through a Wireless Grid Resource Sharing Protocol</summary>

- *Tyson Brooks*

- `2207.03346v1` - [abs](http://arxiv.org/abs/2207.03346v1) - [pdf](http://arxiv.org/pdf/2207.03346v1)

> The rapid growth of the Internet of Things (IoT), cloud computing, Fog computing, mobile edge computing and wireless grids has resulted in the widespread deployment of relatively immature technology. These technologies, which will primarily use 5G wireless communication networks, are becoming popular because they can be deployed quickly with little infrastructure and lends themselves to environments utilizing numerous internet connected devices (ICD). Because of the threat of exploitation, these networks have to be protected by a robust security architecture due to these technologies being plagued with security problems. The authentication of smart ICDs to IoT networks is a critical mechanism for achieving security on these new information system platforms. This article identifies an authentication process required for these ICDs, which will need to prove their identity to authenticate to an IoT fog-mobile edge computing (FMEC) cloud network through a wireless grid authentication process. The purpose of this article is to hypothesize a generic authentication methodology for these FMEC clouds uses in an IoT architecture. The proposed methodology, called wg-IoT, must include the integration of Fog computing, wireless grids and mobile edge computing clouds to create this new IoT architecture. An authentication process developed from the resource sharing protocol (RSP) from a wireless grid is first developed and proposed for the authentication of ICDs. The wireless grid core components must be embedded in IoT devices or sensors depending on their capability to handle five primary functions: management of identification [ID] and presence, permissions management, data transferability, application-programming interface [API] and security.

</details>

<details>

<summary>2022-07-05 19:21:57 - Clustering with Semidefinite Programming and Fixed Point Iteration</summary>

- *Pedro Felzenszwalb, Caroline Klivans, Alice Paul*

- `2012.09202v3` - [abs](http://arxiv.org/abs/2012.09202v3) - [pdf](http://arxiv.org/pdf/2012.09202v3)

> We introduce a novel method for clustering using a semidefinite programming (SDP) relaxation of the Max k-Cut problem. The approach is based on a new methodology for rounding the solution of an SDP relaxation using iterated linear optimization. We show the vertices of the Max k-Cut relaxation correspond to partitions of the data into at most k sets. We also show the vertices are attractive fixed points of iterated linear optimization. Each step of this iterative process solves a relaxation of the closest vertex problem and leads to a new clustering problem where the underlying clusters are more clearly defined. Our experiments show that using fixed point iteration for rounding the Max k-Cut SDP relaxation leads to significantly better results when compared to randomized rounding.

</details>

<details>

<summary>2022-07-05 21:31:18 - Quantum Logic Gate Synthesis as a Markov Decision Process</summary>

- *M. Sohaib Alam, Noah F. Berthusen, Peter P. Orth*

- `1912.12002v2` - [abs](http://arxiv.org/abs/1912.12002v2) - [pdf](http://arxiv.org/pdf/1912.12002v2)

> Reinforcement learning has witnessed recent applications to a variety of tasks in quantum programming. The underlying assumption is that those tasks could be modeled as Markov Decision Processes (MDPs). Here, we investigate the feasibility of this assumption by exploring its consequences for two fundamental tasks in quantum programming: state preparation and gate compilation. By forming discrete MDPs, focusing exclusively on the single-qubit case (both with and without noise), we solve for the optimal policy exactly through policy iteration. We find optimal paths that correspond to the shortest possible sequence of gates to prepare a state, or compile a gate, up to some target accuracy. As an example, we find sequences of $H$ and $T$ gates with length as small as $11$ producing $\sim 99\%$ fidelity for states of the form $(HT)^{n} |0\rangle$ with values as large as $n=10^{10}$. In the presence of gate noise, we demonstrate how the optimal policy adapts to the effects of noisy gates in order to achieve a higher state fidelity. Our work shows that one can meaningfully impose a discrete, stochastic and Markovian nature to a continuous, deterministic and non-Markovian quantum evolution, and provides theoretical insight into why reinforcement learning may be successfully used to find optimally short gate sequences in quantum programming.

</details>

<details>

<summary>2022-07-05 22:39:47 - NatGen: Generative pre-training by "Naturalizing" source code</summary>

- *Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar Devanbu, Baishakhi Ray*

- `2206.07585v2` - [abs](http://arxiv.org/abs/2206.07585v2) - [pdf](http://arxiv.org/pdf/2206.07585v2)

> Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, "Naturalizing" of source code, exploiting code's bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code's bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce un-natural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow).

</details>

<details>

<summary>2022-07-06 08:44:50 - Transaction Monitoring of Smart Contracts</summary>

- *Margarita Capretto, Martin Ceresa, Cesar Sanchez*

- `2207.02517v1` - [abs](http://arxiv.org/abs/2207.02517v1) - [pdf](http://arxiv.org/pdf/2207.02517v1)

> Blockchains are modern distributed systems that provide decentralized financial capabilities with trustable guarantees. Smart contracts are programs written in specialized programming languages running on a blockchain and govern how tokens and cryptocurrency are sent and received. Smart contracts can invoke other contracts during the execution of transactions initiated by external users.   Once deployed, smart contracts cannot be modified and their pitfalls can cause malfunctions and losses, for example by attacks from malicious users. Runtime verification is a very appealing technique to improve the reliability of smart contracts. One approach consists of specifying undesired executions (never claims) and detecting violations of the specification on the fly. This can be done by extending smart contracts with additional instructions corresponding to monitor specified properties, resulting in an onchain monitoring approach.   In this paper, we study transaction monitoring that consists of detecting violations of complete transaction executions and not of individual operations within transactions. Our main contributions are to show that transaction monitoring is not possible in most blockchains and propose different execution mechanisms that would enable transaction monitoring.

</details>

<details>

<summary>2022-07-06 09:14:12 - Exploration of Artificial Intelligence-oriented Power System Dynamic Simulators</summary>

- *Tannan Xiao, Ying Chen, Jianquan Wang, Shaowei Huang, Weilin Tong, Tirui He*

- `2110.00931v4` - [abs](http://arxiv.org/abs/2110.00931v4) - [pdf](http://arxiv.org/pdf/2110.00931v4)

> With the rapid development of artificial intelligence (AI), it is foreseeable that the accuracy and efficiency of dynamic analysis for future power system will be greatly improved by the integration of dynamic simulators and AI. To explore the interaction mechanism of power system dynamic simulations and AI, a general design of an AI-oriented power system dynamic simulator is proposed, which consists of a high-performance simulator with neural network supportability and flexible external and internal application programming interfaces (APIs). With the support of APIs, simulation-assisted AI and AI-assisted simulation form a comprehensive interaction mechanism between power system dynamic simulations and AI. A prototype of this design is implemented and made public based on a highly efficient electromechanical simulator. Tests of this prototype are carried out under four scenarios including sample generation, AI-based stability prediction, data-driven dynamic component modeling, and AI-aided stability control, which prove the validity, flexibility, and efficiency of the design and implementation of the AI-oriented power system dynamic simulator.

</details>

<details>

<summary>2022-07-06 09:33:16 - Adversarial Masking for Self-Supervised Learning</summary>

- *Yuge Shi, N. Siddharth, Philip H. S. Torr, Adam R. Kosiorek*

- `2201.13100v3` - [abs](http://arxiv.org/abs/2201.13100v3) - [pdf](http://arxiv.org/pdf/2201.13100v3)

> We propose ADIOS, a masked image model (MIM) framework for self-supervised learning, which simultaneously learns a masking function and an image encoder using an adversarial objective. The image encoder is trained to minimise the distance between representations of the original and that of a masked image. The masking function, conversely, aims at maximising this distance. ADIOS consistently improves on state-of-the-art self-supervised learning (SSL) methods on a variety of tasks and datasets -- including classification on ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao et al., 2021) -- while generating semantically meaningful masks. Unlike modern MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch tokenisation construction of Vision Transformers, and can be implemented with convolutional backbones. We further demonstrate that the masks learned by ADIOS are more effective in improving representation learning of SSL methods than masking schemes used in popular MIM models. Code is available at https://github.com/YugeTen/adios.

</details>

<details>

<summary>2022-07-06 15:16:01 - Characterizing disruptions in online gaming behavior following software patches</summary>

- *Xiaozhe "Arcadia" Zhang, Brian C. Keegan*

- `2207.02736v1` - [abs](http://arxiv.org/abs/2207.02736v1) - [pdf](http://arxiv.org/pdf/2207.02736v1)

> Multiplayer online games are ideal settings for studying the effects of technological disruptions on social behavior. Software patches to online games cause significant changes to the game's rules and require players to develop new strategies to cope with these disruptions. We surveyed players, analyzed the content of software patch notes, and analyzed changes to the character selection behaviors in more than 53 million matches of Dota 2 in the days before and after software patches over a 30-month period. We found that the severity of patches is correlated with the magnitude of behavioral changes following a patch. We discuss the opportunities of leveraging software patches to online games as a valuable but overlooked empirical instrument for measuring behavioral dynamics.

</details>

<details>

<summary>2022-07-06 18:41:07 - HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC</summary>

- *Michael Riera, Erfan Bank Tavakoli, Masudul Hassan Quraishi, Fengbo Ren*

- `2011.10896v5` - [abs](http://arxiv.org/abs/2011.10896v5) - [pdf](http://arxiv.org/pdf/2011.10896v5)

> This paper presents HALO 1.0, an open-ended extensible multi-agent software framework that implements a set of proposed hardware-agnostic accelerator orchestration (HALO) principles. HALO implements a novel compute-centric message passing interface (C^2MPI) specification for enabling the performance portable execution of a hardware-agnostic host application across heterogeneous accelerators. The experiment results of evaluating eight widely used HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs, and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified control flow for host programs to run across all the computing devices with a consistently top performance portability score, which is up to five orders of magnitude higher than the OpenCL-based solution.

</details>

<details>

<summary>2022-07-06 19:38:01 - Learning Optimal Solutions via an LSTM-Optimization Framework</summary>

- *Dogacan Yilmaz, Ä°. Esra BÃ¼yÃ¼ktahtakÄ±n*

- `2207.02937v1` - [abs](http://arxiv.org/abs/2207.02937v1) - [pdf](http://arxiv.org/pdf/2207.02937v1)

> In this study, we present a deep learning-optimization framework to tackle dynamic mixed-integer programs. Specifically, we develop a bidirectional Long Short Term Memory (LSTM) framework that can process information forward and backward in time to learn optimal solutions to sequential decision-making problems. We demonstrate our approach in predicting the optimal decisions for the single-item capacitated lot-sizing problem (CLSP), where a binary variable denotes whether to produce in a period or not. Due to the dynamic nature of the problem, the CLSP can be treated as a sequence labeling task where a recurrent neural network can capture the problem's temporal dynamics. Computational results show that our LSTM-Optimization (LSTM-Opt) framework significantly reduces the solution time of benchmark CLSP problems without much loss in feasibility and optimality. For example, the predictions at the 85\% level reduce the CPLEX solution time by a factor of 9 on average for over 240,000 test instances with an optimality gap of less than 0.05\% and 0.4\% infeasibility in the test set. Also, models trained using shorter planning horizons can successfully predict the optimal solution of the instances with longer planning horizons. For the hardest data set, the LSTM predictions at the 25\% level reduce the solution time of 70 CPU hours to less than 2 CPU minutes with an optimality gap of 0.8\% and without any infeasibility. The LSTM-Opt framework outperforms classical ML algorithms, such as the logistic regression and random forest, in terms of the solution quality, and exact approaches, such as the ($\ell$, S) and dynamic programming-based inequalities, with respect to the solution time improvement. Our machine learning approach could be beneficial in tackling sequential decision-making problems similar to CLSP, which need to be solved repetitively, frequently, and in a fast manner.

</details>

<details>

<summary>2022-07-07 04:40:41 - Causality-based Neural Network Repair</summary>

- *Bing Sun, Jun Sun, Hong Long Pham, Jie Shi*

- `2204.09274v2` - [abs](http://arxiv.org/abs/2204.09274v2) - [pdf](http://arxiv.org/pdf/2204.09274v2)

> Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (\textbf{CA}usality-based \textbf{RE}pair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the `guilty' neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by $61.91\%$ on average. For backdoor removal tasks, CARE reduces the attack success rate from over $98\%$ to less than $1\%$. For safety property repair tasks, CARE reduces the property violation rate to less than $1\%$. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.

</details>

<details>

<summary>2022-07-07 11:37:18 - Entropy-Based Feature Extraction For Real-Time Semantic Segmentation</summary>

- *Lusine Abrahamyan, Nikos Deligiannis*

- `2207.03233v1` - [abs](http://arxiv.org/abs/2207.03233v1) - [pdf](http://arxiv.org/pdf/2207.03233v1)

> This paper introduces an efficient patch-based computational module, coined Entropy-based Patch Encoder (EPE) module, for resource-constrained semantic segmentation. The EPE module consists of three lightweight fully-convolutional encoders, each extracting features from image patches with a different amount of entropy. Patches with high entropy are being processed by the encoder with the largest number of parameters, patches with moderate entropy are processed by the encoder with a moderate number of parameters, and patches with low entropy are processed by the smallest encoder. The intuition behind the module is the following: as patches with high entropy contain more information, they need an encoder with more parameters, unlike low entropy patches, which can be processed using a small encoder. Consequently, processing part of the patches via the smaller encoder can significantly reduce the computational cost of the module. Experiments show that EPE can boost the performance of existing real-time semantic segmentation models with a slight increase in the computational cost. Specifically, EPE increases the mIOU performance of DFANet A by 0.9% with only 1.2% increase in the number of parameters and the mIOU performance of EDANet by 1% with 10% increase of the model parameters.

</details>

<details>

<summary>2022-07-07 13:31:35 - Shell Language Processing: Unix command parsing for Machine Learning</summary>

- *Dmitrijs Trizna*

- `2107.02438v3` - [abs](http://arxiv.org/abs/2107.02438v3) - [pdf](http://arxiv.org/pdf/2107.02438v3)

> In this article, we present a Shell Language Preprocessing (SLP) library, which implements tokenization and encoding directed at parsing Unix and Linux shell commands. We describe the rationale behind the need for a new approach with specific examples of when conventional Natural Language Processing (NLP) pipelines fail. Furthermore, we evaluate our methodology on a security classification task against widely accepted information and communications technology (ICT) tokenization techniques and achieve significant improvement of an F1 score from 0.392 to 0.874.

</details>

<details>

<summary>2022-07-07 18:18:00 - Data-driven Numerical Invariant Synthesis with Automatic Generation of Attributes</summary>

- *Ahmed Bouajjani, Wael-Amine Boutglay, Peter Habermehl*

- `2205.14943v3` - [abs](http://arxiv.org/abs/2205.14943v3) - [pdf](http://arxiv.org/pdf/2205.14943v3)

> We propose a data-driven algorithm for numerical invariant synthesis and verification. The algorithm is based on the ICE-DT schema for learning decision trees from samples of positive and negative states and implications corresponding to program transitions. The main issue we address is the discovery of relevant attributes to be used in the learning process of numerical invariants. We define a method for solving this problem guided by the data sample. It is based on the construction of a separator that covers positive states and excludes negative ones, consistent with the implications. The separator is constructed using an abstract domain representation of convex sets. The generalization mechanism of the decision tree learning from the constraints of the separator allows the inference of general invariants, accurate enough for proving the targeted property. We implemented our algorithm and showed its efficiency.

</details>

<details>

<summary>2022-07-07 19:23:40 - Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework</summary>

- *Elias B. Khalil, Pashootan Vaezipoor, Bistra Dilkina*

- `2110.08423v2` - [abs](http://arxiv.org/abs/2110.08423v2) - [pdf](http://arxiv.org/pdf/2110.08423v2)

> In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a "small" subset of an instance's integer variables with the following property: in a branch-and-bound procedure, the instance can be solved to global optimality by branching only on the variables in the backdoor. Constructing datasets of pre-computed backdoors for widely used MIP benchmark sets or particular problem families can enable new questions around novel structural properties of a MIP, or explain why a problem that is hard in theory can be solved efficiently in practice. Existing algorithms for finding backdoors rely on sampling candidate variable subsets in various ways, an approach which has demonstrated the existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010. However, these algorithms fall short of consistently succeeding at the task due to an imbalance between exploration and exploitation. We propose BaMCTS, a Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive algorithmic engineering, hybridization with traditional MIP concepts, and close integration with the CPLEX solver have enabled our method to outperform baselines on MIPLIB2017 instances, finding backdoors more frequently and more efficiently.

</details>

<details>

<summary>2022-07-08 03:48:02 - Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis</summary>

- *Jiancheng Yang, Rui Shi, Udaranga Wickramasinghe, Qikui Zhu, Bingbing Ni, Pascal Fua*

- `2206.15328v2` - [abs](http://arxiv.org/abs/2206.15328v2) - [pdf](http://arxiv.org/pdf/2206.15328v2)

> The human annotations are imperfect, especially when produced by junior practitioners. Multi-expert consensus is usually regarded as golden standard, while this annotation protocol is too expensive to implement in many real-world projects. In this study, we propose a method to refine human annotation, named Neural Annotation Refinement (NeAR). It is based on a learnable implicit function, which decodes a latent vector into represented shape. By integrating the appearance as an input of implicit functions, the appearance-aware NeAR fixes the annotation artefacts. Our method is demonstrated on the application of adrenal gland analysis. We first show that the NeAR can repair distorted golden standards on a public adrenal gland segmentation dataset. Besides, we develop a new Adrenal gLand ANalysis (ALAN) dataset with the proposed NeAR, where each case consists of a 3D shape of adrenal gland and its diagnosis label (normal vs. abnormal) assigned by experts. We show that models trained on the shapes repaired by the NeAR can diagnose adrenal glands better than the original ones. The ALAN dataset will be open-source, with 1,584 shapes for adrenal gland diagnosis, which serves as a new benchmark for medical shape analysis. Code and dataset are available at https://github.com/M3DV/NeAR.

</details>

<details>

<summary>2022-07-08 10:30:50 - BF++: a language for general-purpose program synthesis</summary>

- *Vadim Liventsev, Aki HÃ¤rmÃ¤, Milan PetkoviÄ*

- `2101.09571v6` - [abs](http://arxiv.org/abs/2101.09571v6) - [pdf](http://arxiv.org/pdf/2101.09571v6)

> Most state of the art decision systems based on Reinforcement Learning (RL) are data-driven black-box neural models, where it is often difficult to incorporate expert knowledge into the models or let experts review and validate the learned decision mechanisms. Knowledge-insertion and model review are important requirements in many applications involving human health and safety. One way to bridge the gap between data and knowledge driven systems is program synthesis: replacing a neural network that outputs decisions with a symbolic program generated by a neural network or by means of genetic programming. We propose a new programming language, BF++, designed specifically for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.

</details>

<details>

<summary>2022-07-08 13:13:51 - Constrained Training of Neural Networks via Theorem Proving</summary>

- *Mark Chevallier, Matthew Whyte, Jacques D. Fleuriot*

- `2207.03880v1` - [abs](http://arxiv.org/abs/2207.03880v1) - [pdf](http://arxiv.org/pdf/2207.03880v1)

> We introduce a theorem proving approach to the specification and generation of temporal logical constraints for training neural networks. We formalise a deep embedding of linear temporal logic over finite traces (LTL$_f$) and an associated evaluation function characterising its semantics within the higher-order logic of the Isabelle theorem prover. We then proceed to formalise a loss function $\mathcal{L}$ that we formally prove to be sound, and differentiable to a function $d\mathcal{L}$. We subsequently use Isabelle's automatic code generation mechanism to produce OCaml versions of LTL$_f$, $\mathcal{L}$ and $d\mathcal{L}$ that we integrate with PyTorch via OCaml bindings for Python. We show that, when used for training in an existing deep learning framework for dynamic movement, our approach produces expected results for common movement specification patterns such as obstacle avoidance and patrolling. The distinctive benefit of our approach is the fully rigorous method for constrained training, eliminating many of the risks inherent to ad-hoc implementations of logical aspects directly in an "unsafe" programming language such as Python.

</details>

<details>

<summary>2022-07-08 14:19:36 - Towards Semantic Communication Protocols: A Probabilistic Logic Perspective</summary>

- *Sejin Seo, Jihong Park, Seung-Woo Ko, Jinho Choi, Mehdi Bennis, Seong-Lyun Kim*

- `2207.03920v1` - [abs](http://arxiv.org/abs/2207.03920v1) - [pdf](http://arxiv.org/pdf/2207.03920v1)

> Classical medium access control (MAC) protocols are interpretable, yet their task-agnostic control signaling messages (CMs) are ill-suited for emerging mission-critical applications. By contrast, neural network (NN) based protocol models (NPMs) learn to generate task-specific CMs, but their rationale and impact lack interpretability. To fill this void, in this article we propose, for the first time, a semantic protocol model (SPM) constructed by transforming an NPM into an interpretable symbolic graph written in the probabilistic logic programming language (ProbLog). This transformation is viable by extracting and merging common CMs and their connections while treating the NPM as a CM generator. By extensive simulations, we corroborate that the SPM tightly approximates its original NPM while occupying only 0.02% memory. By leveraging its interpretability and memory-efficiency, we demonstrate several SPM-enabled applications such as SPM reconfiguration for collision-avoidance, as well as comparing different SPMs via semantic entropy calculation and storing multiple SPMs to cope with non-stationary environments.

</details>

<details>

<summary>2022-07-08 15:55:06 - Generalization-Memorization Machines</summary>

- *Zhen Wang, Yuan-Hai Shao*

- `2207.03976v1` - [abs](http://arxiv.org/abs/2207.03976v1) - [pdf](http://arxiv.org/pdf/2207.03976v1)

> Classifying the training data correctly without over-fitting is one of the goals in machine learning. In this paper, we propose a generalization-memorization mechanism, including a generalization-memorization decision and a memory modeling principle. Under this mechanism, error-based learning machines improve their memorization abilities of training data without over-fitting. Specifically, the generalization-memorization machines (GMM) are proposed by applying this mechanism. The optimization problems in GMM are quadratic programming problems and could be solved efficiently. It should be noted that the recently proposed generalization-memorization kernel and the corresponding support vector machines are the special cases of our GMM. Experimental results show the effectiveness of the proposed GMM both on memorization and generalization.

</details>

<details>

<summary>2022-07-08 17:37:56 - Lessons from Deep Learning applied to Scholarly Information Extraction: What Works, What Doesn't, and Future Directions</summary>

- *Raquib Bin Yousuf, Subhodip Biswas, Kulendra Kumar Kaushal, James Dunham, Rebecca Gelles, Sathappan Muthiah, Nathan Self, Patrick Butler, Naren Ramakrishnan*

- `2207.04029v1` - [abs](http://arxiv.org/abs/2207.04029v1) - [pdf](http://arxiv.org/pdf/2207.04029v1)

> Understanding key insights from full-text scholarly articles is essential as it enables us to determine interesting trends, give insight into the research and development, and build knowledge graphs. However, some of the interesting key insights are only available when considering full-text. Although researchers have made significant progress in information extraction from short documents, extraction of scientific entities from full-text scholarly literature remains a challenging problem. This work presents an automated End-to-end Research Entity Extractor called EneRex to extract technical facets such as dataset usage, objective task, method from full-text scholarly research articles. Additionally, we extracted three novel facets, e.g., links to source code, computing resources, programming language/libraries from full-text articles. We demonstrate how EneRex is able to extract key insights and trends from a large-scale dataset in the domain of computer science. We further test our pipeline on multiple datasets and found that the EneRex improves upon a state of the art model. We highlight how the existing datasets are limited in their capacity and how EneRex may fit into an existing knowledge graph. We also present a detailed discussion with pointers for future research. Our code and data are publicly available at https://github.com/DiscoveryAnalyticsCenter/EneRex.

</details>

<details>

<summary>2022-07-08 20:55:01 - Language for Description of Worlds</summary>

- *Dimiter Dobrev*

- `2010.16243v4` - [abs](http://arxiv.org/abs/2010.16243v4) - [pdf](http://arxiv.org/pdf/2010.16243v4)

> We will reduce the task of creating AI to the task of finding an appropriate language for description of the world. This will not be a programing language because programing languages describe only computable functions, while our language will describe a somewhat broader class of functions. Another specificity of this language will be that the description will consist of separate modules. This will enable us look for the description of the world automatically such that we discover it module after module. Our approach to the creation of this new language will be to start with a particular world and write the description of that particular world. The point is that the language which can describe this particular world will be appropriate for describing any world.

</details>

<details>

<summary>2022-07-09 07:05:53 - Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain</summary>

- *Chang Yue, Peizhuo Lv, Ruigang Liang, Kai Chen*

- `2207.04209v1` - [abs](http://arxiv.org/abs/2207.04209v1) - [pdf](http://arxiv.org/pdf/2207.04209v1)

> With the broad application of deep neural networks (DNNs), backdoor attacks have gradually attracted attention. Backdoor attacks are insidious, and poisoned models perform well on benign samples and are only triggered when given specific inputs, which cause the neural network to produce incorrect outputs. The state-of-the-art backdoor attack work is implemented by data poisoning, i.e., the attacker injects poisoned samples into the dataset, and the models trained with that dataset are infected with the backdoor. However, most of the triggers used in the current study are fixed patterns patched on a small fraction of an image and are often clearly mislabeled, which is easily detected by humans or defense methods such as Neural Cleanse and SentiNet. Also, it's difficult to be learned by DNNs without mislabeling, as they may ignore small patterns. In this paper, we propose a generalized backdoor attack method based on the frequency domain, which can implement backdoor implantation without mislabeling and accessing the training process. It is invisible to human beings and able to evade the commonly used defense methods. We evaluate our approach in the no-label and clean-label cases on three datasets (CIFAR-10, STL-10, and GTSRB) with two popular scenarios (self-supervised learning and supervised learning). The results show our approach can achieve a high attack success rate (above 90%) on all the tasks without significant performance degradation on main tasks. Also, we evaluate the bypass performance of our approach for different kinds of defenses, including the detection of training data (i.e., Activation Clustering), the preprocessing of inputs (i.e., Filtering), the detection of inputs (i.e., SentiNet), and the detection of models (i.e., Neural Cleanse). The experimental results demonstrate that our approach shows excellent robustness to such defenses.

</details>

<details>

<summary>2022-07-09 07:07:12 - Reflections on the Evolution of Computer Science Education</summary>

- *Sreekrishnan Venkateswaran*

- `2208.04713v1` - [abs](http://arxiv.org/abs/2208.04713v1) - [pdf](http://arxiv.org/pdf/2208.04713v1)

> Computer Science education has been evolving over the years to reflect applied realities. Until about a decade ago, theory of computation, algorithm design and system software dominated the curricula. Most courses were considered core and were hence mandatory; the programme structure did not allow much of a choice or variety. This column analyses why this changed Circa 2010 when elective subjects across scores of topics become part of mainstream education to reflect the on-going lateral acceleration of Computer Science. Fundamental discoveries in artificial intelligence, machine learning, virtualization and cloud computing are several decades old. Many core theories in data science are centuries old. Yet their leverage exploded only after Circa 2010, when the stage got set for people-centric problem solving in massive scale. This was due in part to the rush of innovative real-world applications that reached the common man through the ubiquitous smart phone. AI/ML modules arrived in popular programming languages; they could be used to build and train models on powerful - yet affordable - compute on public clouds reachable through high-speed Internet connectivity. Academia responded by adapting Computer Science curricula to align it with the changing technology landscape. The goal of this experiential piece is to trigger a lively discussion on the past and future of Computer Science education.

</details>

<details>

<summary>2022-07-09 07:46:39 - Subclasses of Class Function used to Implement Transformations of Statistical Models</summary>

- *Lloyd Allison*

- `2207.04218v1` - [abs](http://arxiv.org/abs/2207.04218v1) - [pdf](http://arxiv.org/pdf/2207.04218v1)

> A library of software for inductive inference guided by the Minimum Message Length (MML) principle was created previously. It contains various (object-oriented-) classes and subclasses of statistical Model and can be used to infer Models from given data sets in machine learning problems. Here transformations of statistical Models are considered and implemented within the library so as to have desirable properties from the object-oriented programming and mathematical points of view. The subclasses of class Function needed to do such transformations are defined.

</details>

<details>

<summary>2022-07-09 15:02:39 - A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities</summary>

- *Yaoxian Li, Shiyi Qi, Cuiyun Gao, Yun Peng, David Lo, Zenglin Xu, Michael R. Lyu*

- `2207.04285v1` - [abs](http://arxiv.org/abs/2207.04285v1) - [pdf](http://arxiv.org/pdf/2207.04285v1)

> Transformer-based models have demonstrated state-of-the-art performance in many intelligent coding tasks such as code comment generation and code completion. Previous studies show that deep learning models are sensitive to the input variations, but few studies have systematically studied the robustness of Transformer under perturbed input code. In this work, we empirically study the effect of semantic-preserving code transformation on the performance of Transformer. Specifically, 24 and 27 code transformation strategies are implemented for two popular programming languages, Java and Python, respectively. For facilitating analysis, the strategies are grouped into five categories: block transformation, insertion/deletion transformation, grammatical statement transformation, grammatical token transformation, and identifier transformation. Experiments on three popular code intelligence tasks, including code completion, code summarization and code search, demonstrate insertion/deletion transformation and identifier transformation show the greatest impact on the performance of Transformer. Our results also suggest that Transformer based on abstract syntax trees (ASTs) shows more robust performance than the model based on only code sequence under most code transformations. Besides, the design of positional encoding can impact the robustness of Transformer under code transformation. Based on our findings, we distill some insights about the challenges and opportunities for Transformer-based code intelligence.

</details>

<details>

<summary>2022-07-09 18:21:32 - Improving Diffusion Model Efficiency Through Patching</summary>

- *Troy Luhman, Eric Luhman*

- `2207.04316v1` - [abs](http://arxiv.org/abs/2207.04316v1) - [pdf](http://arxiv.org/pdf/2207.04316v1)

> Diffusion models are a powerful class of generative models that iteratively denoise samples to produce data. While many works have focused on the number of iterations in this sampling procedure, few have focused on the cost of each iteration. We find that adding a simple ViT-style patching transformation can considerably reduce a diffusion model's sampling time and memory usage. We justify our approach both through an analysis of the diffusion model objective, and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024. We provide implementations in Tensorflow and Pytorch.

</details>

<details>

<summary>2022-07-09 22:26:51 - Gas Gauge: A Security Analysis Tool for Smart Contract Out-of-Gas Vulnerabilities</summary>

- *Behkish Nassirzadeh, Huaiying Sun, Sebastian Banescu, Vijay Ganesh*

- `2112.14771v2` - [abs](http://arxiv.org/abs/2112.14771v2) - [pdf](http://arxiv.org/pdf/2112.14771v2)

> In recent years we have witnessed a dramatic increase in the adoption and application of smart contracts in a variety of contexts such as decentralized finance, supply chain management, and identity management. However, a critical stumbling block to the further adoption of smart contracts is their security. A particularly widespread class of security vulnerabilities that afflicts Ethereum smart contracts is the gas limit denial of service(DoS) on a contract via unbounded operations. These vulnerabilities result in a failed transaction with an out-of-gas error and are often present in contracts containing loops whose bounds are affected by end-user input. Note that such vulnerabilities differ from gas limit DoS on the network via block stuffing. Therefore, we present Gas Gauge, a tool aimed at detecting Out-of-Gas DoS vulnerabilities in Ethereum smart contracts. Gas Gauge consists of three major components: the Detection, Identification, and Correction Phases. The Detection Phase consists of an accurate static analysis approach that finds and summarizes all the loops in a smart contract. The Identification Phase uses a white-box fuzzing approach to generate a set of inputs that causes the contract to run out of gas. The Correction Phase uses static analysis and run-time verification to predict the maximum loop bounds consistent with allowable gas usage and suggest appropriate repairs to the user of the tool. Each part of the tool can be used separately for different purposes or all together to detect, identify and help repair the contracts vulnerable to Out-of-Gas DoS vulnerabilities. Gas Gauge was tested on 1,000 real-world solidity smart contracts deployed on the Ethereum Mainnet. The results were compared to seven state-of-the-art static and symbolic tools, and it was empirically demonstrated that Gas Gauge is far more effective than competing state-of-the-art tools.

</details>

<details>

<summary>2022-07-10 08:58:47 - Vision Transformer for Contrastive Clustering</summary>

- *Hua-Bao Ling, Bowen Zhu, Dong Huang, Ding-Hua Chen, Chang-Dong Wang, Jian-Huang Lai*

- `2206.12925v2` - [abs](http://arxiv.org/abs/2206.12925v2) - [pdf](http://arxiv.org/pdf/2206.12925v2)

> Vision Transformer (ViT) has shown its advantages over the convolutional neural network (CNN) with its ability to capture global long-range dependencies for visual representation learning. Besides ViT, contrastive learning is another popular research topic recently. While previous contrastive learning works are mostly based on CNNs, some recent studies have attempted to combine ViT and contrastive learning for enhanced self-supervised learning. Despite the considerable progress, these combinations of ViT and contrastive learning mostly focus on the instance-level contrastiveness, which often overlook the global contrastiveness and also lack the ability to directly learn the clustering result (e.g., for images). In view of this, this paper presents a novel deep clustering approach termed Vision Transformer for Contrastive Clustering (VTCC), which for the first time, to our knowledge, unifies the Transformer and the contrastive learning for the image clustering task. Specifically, with two random augmentations performed on each image, we utilize a ViT encoder with two weight-sharing views as the backbone. To remedy the potential instability of the ViT, we incorporate a convolutional stem to split each augmented sample into a sequence of patches, which uses multiple stacked small convolutions instead of a big convolution in the patch projection layer. By learning the feature representations for the sequences of patches via the backbone, an instance projector and a cluster projector are further utilized to perform the instance-level contrastive learning and the global clustering structure learning, respectively. Experiments on eight image datasets demonstrate the stability (during the training-from-scratch) and the superiority (in clustering performance) of our VTCC approach over the state-of-the-art.

</details>

<details>

<summary>2022-07-10 09:41:44 - Robust deep learning-based semantic organ segmentation in hyperspectral images</summary>

- *Silvia Seidlitz, Jan Sellner, Jan Odenthal, Berkin ÄÂzdemir, Alexander Studier-Fischer, Samuel KnÄÅdler, Leonardo Ayala, Tim J. Adler, Hannes G. Kenngott, Minu Tizabi, Martin Wagner, Felix Nickel, Beat P. MÄÅºller-Stich, Lena Maier-Hein*

- `2111.05408v2` - [abs](http://arxiv.org/abs/2111.05408v2) - [pdf](http://arxiv.org/pdf/2111.05408v2)

> Semantic image segmentation is an important prerequisite for context-awareness and autonomous robotics in surgery. The state of the art has focused on conventional RGB video data acquired during minimally invasive surgery, but full-scene semantic segmentation based on spectral imaging data and obtained during open surgery has received almost no attention to date. To address this gap in the literature, we are investigating the following research questions based on hyperspectral imaging (HSI) data of pigs acquired in an open surgery setting: (1) What is an adequate representation of HSI data for neural network-based fully automated organ segmentation, especially with respect to the spatial granularity of the data (pixels vs. superpixels vs. patches vs. full images)? (2) Is there a benefit of using HSI data compared to other modalities, namely RGB data and processed HSI data (e.g. tissue parameters like oxygenation), when performing semantic organ segmentation? According to a comprehensive validation study based on 506 HSI images from 20 pigs, annotated with a total of 19 classes, deep learning-based segmentation performance increases, consistently across modalities, with the spatial context of the input data. Unprocessed HSI data offers an advantage over RGB data or processed data from the camera provider, with the advantage increasing with decreasing size of the input to the neural network. Maximum performance (HSI applied to whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04), which is in the range of the inter-rater variability (DSC of 0.89 ((standard deviation (SD)) 0.07)). We conclude that HSI could become a powerful image modality for fully-automatic surgical scene understanding with many advantages over traditional imaging, including the ability to recover additional functional tissue information. Code and pre-trained models: https://github.com/IMSY-DKFZ/htc.

</details>

<details>

<summary>2022-07-11 05:09:34 - swTVM: Towards Optimized Tensor Code Generation for Deep Learning on Sunway Many-Core Processor</summary>

- *Mingzhen Li, Changxi Liu, Jianjin Liao, Xuegui Zheng, Hailong Yang, Rujun Sun, Jun Xu, Lin Gan, Guangwen Yang, Zhongzhi Luan, Depei Qian*

- `1904.07404v3` - [abs](http://arxiv.org/abs/1904.07404v3) - [pdf](http://arxiv.org/pdf/1904.07404v3)

> The flourish of deep learning frameworks and hardware platforms has been demanding an efficient compiler that can shield the diversity in both software and hardware in order to provide application portability. Among the existing deep learning compilers, TVM is well known for its efficiency in code generation and optimization across diverse hardware devices. In the meanwhile, the Sunway many-core processor renders itself as a competitive candidate for its attractive computational power in both scientific computing and deep learning workloads. This paper combines the trends in these two directions. Specifically, we propose swTVM that extends the original TVM to support ahead-of-time compilation for architecture requiring cross-compilation such as Sunway. In addition, we leverage the architecture features during the compilation such as core group for massive parallelism, DMA for high bandwidth memory transfer and local device memory for data locality, in order to generate efficient codes for deep learning workloads on Sunway. The experiment results show that the codes generated by swTVM achieves 1.79x on average compared to the state-of-the-art deep learning framework on Sunway, across six representative benchmarks. This work is the first attempt from the compiler perspective to bridge the gap of deep learning and Sunway processor particularly with productivity and efficiency in mind. We believe this work will encourage more people to embrace the power of deep learning and Sunway many-core processor.

</details>

<details>

<summary>2022-07-11 05:32:26 - Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm</summary>

- *Meena Jagadeesan, Ilya Razenshteyn, Suriya Gunasekar*

- `2102.12238v4` - [abs](http://arxiv.org/abs/2102.12238v4) - [pdf](http://arxiv.org/pdf/2102.12238v4)

> We provide a function space characterization of the inductive bias resulting from minimizing the $\ell_2$ norm of the weights in multi-channel convolutional neural networks with linear activations and empirically test our resulting hypothesis on ReLU networks trained using gradient descent. We define an induced regularizer in the function space as the minimum $\ell_2$ norm of weights of a network required to realize a function. For two layer linear convolutional networks with $C$ output channels and kernel size $K$, we show the following: (a) If the inputs to the network are single channeled, the induced regularizer for any $K$ is independent of the number of output channels $C$. Furthermore, we derive the regularizer is a norm given by a semidefinite program (SDP). (b) In contrast, for multi-channel inputs, multiple output channels can be necessary to merely realize all matrix-valued linear functions and thus the inductive bias does depend on $C$. However, for sufficiently large $C$, the induced regularizer is again given by an SDP that is independent of $C$. In particular, the induced regularizer for $K=1$ and $K=D$ (input dimension) is given in closed form as the nuclear norm and the $\ell_{2,1}$ group-sparse norm, respectively, of the Fourier coefficients of the linear predictor. We investigate the broader applicability of our theoretical results to implicit regularization from gradient descent on linear and ReLU networks through experiments on MNIST and CIFAR-10 datasets.

</details>

<details>

<summary>2022-07-11 10:41:39 - ACERAC: Efficient reinforcement learning in fine time discretization</summary>

- *Jakub Åyskawa, PaweÅ WawrzyÅski*

- `2104.04004v4` - [abs](http://arxiv.org/abs/2104.04004v4) - [pdf](http://arxiv.org/pdf/2104.04004v4)

> One of the main goals of reinforcement learning (RL) is to provide a~way for physical machines to learn optimal behavior instead of being programmed. However, effective control of the machines usually requires fine time discretization. The most common RL methods apply independent random elements to each action, which is not suitable in that setting. It is not feasible because it causes the controlled system to jerk, and does not ensure sufficient exploration since a~single action is not long enough to create a~significant experience that could be translated into policy improvement. In our view these are the main obstacles that prevent application of RL in contemporary control systems. To address these pitfalls, in this paper we introduce an RL framework and adequate analytical tools for actions that may be stochastically dependent in subsequent time instances. We also introduce an RL algorithm that approximately optimizes a~policy that produces such actions. It applies experience replay to adjust likelihood of sequences of previous actions to optimize expected $n$-step returns the policy yields. The efficiency of this algorithm is verified against four other RL methods (CDAU, PPO, SAC, ACER) in four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) in diverse time discretization. The algorithm introduced here outperforms the competitors in most cases considered.

</details>

<details>

<summary>2022-07-11 15:36:10 - Simple Dynamic Spanners with Near-optimal Recourse against an Adaptive Adversary</summary>

- *Sayan Bhattacharya, Thatchaphol Saranurak, Pattara Sukprasert*

- `2207.04954v1` - [abs](http://arxiv.org/abs/2207.04954v1) - [pdf](http://arxiv.org/pdf/2207.04954v1)

> Designing dynamic algorithms against an adaptive adversary whose performance match the ones assuming an oblivious adversary is a major research program in the field of dynamic graph algorithms. One of the prominent examples whose oblivious-vs-adaptive gap remains maximally large is the \emph{fully dynamic spanner} problem; there exist algorithms assuming an oblivious adversary with near-optimal size-stretch trade-off using only $\operatorname{polylog}(n)$ update time [Baswana, Khurana, and Sarkar TALG'12; Forster and Goranci STOC'19; Bernstein, Forster, and Henzinger SODA'20], while against an adaptive adversary, even when we allow infinite time and only count recourse (i.e. the number of edge changes per update in the maintained spanner), all previous algorithms with stretch at most $\log^{5}(n)$ require at least $\Omega(n)$ amortized recourse [Ausiello, Franciosa, and Italiano ESA'05].   In this paper, we completely close this gap with respect to recourse by showing algorithms against an adaptive adversary with near-optimal size-stretch trade-off and recourse. More precisely, for any $k\ge1$, our algorithm maintains a $(2k-1)$-spanner of size $O(n^{1+1/k}\log n)$ with $O(\log n)$ amortized recourse, which is optimal in all parameters up to a $O(\log n)$ factor. As a step toward algorithms with small update time (not just recourse), we show another algorithm that maintains a $3$-spanner of size $\tilde O(n^{1.5})$ with $\operatorname{polylog}(n)$ amortized recourse \emph{and} simultaneously $\tilde O(\sqrt{n})$ worst-case update time.

</details>

<details>

<summary>2022-07-11 16:03:51 - Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning</summary>

- *Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, Tao Mei*

- `2207.04978v1` - [abs](http://arxiv.org/abs/2207.04978v1) - [pdf](http://arxiv.org/pdf/2207.04978v1)

> Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \url{https://github.com/YehLi/ImageNetModel}.

</details>

<details>

<summary>2022-07-11 17:18:49 - The Complexity of Finding Fair Many-to-One Matchings</summary>

- *Niclas Boehmer, Tomohiro Koana*

- `2206.06988v2` - [abs](http://arxiv.org/abs/2206.06988v2) - [pdf](http://arxiv.org/pdf/2206.06988v2)

> We analyze the (parameterized) computational complexity of "fair" variants of bipartite many-to-one matching, where each vertex from the "left" side is matched to exactly one vertex and each vertex from the "right" side may be matched to multiple vertices. We want to find a "fair" matching, in which each vertex from the right side is matched to a "fair" set of vertices. Assuming that each vertex from the left side has one color modeling its attribute, we study two fairness criteria. In one of them, we deem a vertex set fair if for any two colors, the difference between the numbers of their occurrences does not exceed a given threshold. Fairness is relevant when finding many-to-one matchings between students and colleges, voters and constituencies, and applicants and firms. Here colors may model sociodemographic attributes, party memberships, and qualifications, respectively. We show that finding a fair many-to-one matching is NP-hard even for three colors and maximum degree five. Our main contribution is the design of fixed-parameter tractable algorithms with respect to the number of vertices on the right side. Our algorithms make use of a variety of techniques including color coding. At the core lie integer linear programs encoding Hall like conditions. To establish the correctness of our integer programs, we prove a new separation result, inspired by Frank's separation theorem [Frank, Discrete Math. 1982], which may also be of independent interest. We further obtain complete complexity dichotomies regarding the number of colors and the maximum degree of each side.

</details>

<details>

<summary>2022-07-11 21:06:29 - Patch-level instance-group discrimination with pretext-invariant learning for colitis scoring</summary>

- *Ziang Xu, Sharib Ali, Soumya Gupta, Simon Leedham, James E East, Jens Rittscher*

- `2207.05192v1` - [abs](http://arxiv.org/abs/2207.05192v1) - [pdf](http://arxiv.org/pdf/2207.05192v1)

> Inflammatory bowel disease (IBD), in particular ulcerative colitis (UC), is graded by endoscopists and this assessment is the basis for risk stratification and therapy monitoring. Presently, endoscopic characterisation is largely operator dependant leading to sometimes undesirable clinical outcomes for patients with IBD. We focus on the Mayo Endoscopic Scoring (MES) system which is widely used but requires the reliable identification of subtle changes in mucosal inflammation. Most existing deep learning classification methods cannot detect these fine-grained changes which make UC grading such a challenging task. In this work, we introduce a novel patch-level instance-group discrimination with pretext-invariant representation learning (PLD-PIRL) for self-supervised learning (SSL). Our experiments demonstrate both improved accuracy and robustness compared to the baseline supervised network and several state-of-the-art SSL methods. Compared to the baseline (ResNet50) supervised classification our proposed PLD-PIRL obtained an improvement of 4.75% on hold-out test data and 6.64% on unseen center test data for top-1 accuracy.

</details>

<details>

<summary>2022-07-12 06:32:27 - WheaCha: A Method for Explaining the Predictions of Models of Code</summary>

- *Yu Wang, Ke Wang, Linzhang Wang*

- `2102.04625v3` - [abs](http://arxiv.org/abs/2102.04625v3) - [pdf](http://arxiv.org/pdf/2102.04625v3)

> Attribution methods have emerged as a popular approach to interpreting model predictions based on the relevance of input features. Although the feature importance ranking can provide insights of how models arrive at a prediction from a raw input, they do not give a clear-cut definition of the key features models use for the prediction. In this paper, we present a new method, called WheaCha, for explaining the predictions of code models. Although WheaCha employs the same mechanism of tracing model predictions back to the input features, it differs from all existing attribution methods in crucial ways. Specifically, WheaCha divides an input program into "wheat" (i.e., the defining features that are the reason for which models predict the label that they predict) and the rest "chaff" for any prediction of a learned code model. We realize WheaCha in a tool, HuoYan, and use it to explain four prominent code models: code2vec, seq-GNN, GGNN, and CodeBERT. Results show (1) HuoYan is efficient - taking on average under twenty seconds to compute the wheat for an input program in an end-to-end fashion (i.e., including model prediction time); (2) the wheat that all models use to predict input programs is made of simple syntactic or even lexical properties (i.e., identifier names); (3) Based on wheat, we present a novel approach to explaining the predictions of code models through the lens of training data.

</details>

<details>

<summary>2022-07-12 08:51:41 - Transformer Compressed Sensing via Global Image Tokens</summary>

- *Marlon Bran Lorenzana, Craig Engstrom, Shekhar S. Chandra*

- `2203.12861v3` - [abs](http://arxiv.org/abs/2203.12861v3) - [pdf](http://arxiv.org/pdf/2203.12861v3)

> Convolutional neural networks (CNN) have demonstrated outstanding Compressed Sensing (CS) performance compared to traditional, hand-crafted methods. However, they are broadly limited in terms of generalisability, inductive bias and difficulty to model long distance relationships. Transformer neural networks (TNN) overcome such issues by implementing an attention mechanism designed to capture dependencies between inputs. However, high-resolution tasks typically require vision Transformers (ViT) to decompose an image into patch-based tokens, limiting inputs to inherently local contexts. We propose a novel image decomposition that naturally embeds images into low-resolution inputs. These Kaleidoscope tokens (KD) provide a mechanism for global attention, at the same computational cost as a patch-based approach. To showcase this development, we replace CNN components in a well-known CS-MRI neural network with TNN blocks and demonstrate the improvements afforded by KD. We also propose an ensemble of image tokens, which enhance overall image quality and reduces model size. Supplementary material is available: https://github.com/uqmarlonbran/TCS.git

</details>

<details>

<summary>2022-07-12 10:09:48 - Computing NP-hard Repetitiveness Measures via MAX-SAT</summary>

- *Hideo Bannai, Keisuke Goto, Masakazu Ishihata, Shunsuke Kanda, Dominik KÃ¶ppl, Takaaki Nishimoto*

- `2207.02571v2` - [abs](http://arxiv.org/abs/2207.02571v2) - [pdf](http://arxiv.org/pdf/2207.02571v2)

> Repetitiveness measures reveal profound characteristics of datasets, and give rise to compressed data structures and algorithms working in compressed space. Alas, the computation of some of these measures is NP-hard, and straight-forward computation is infeasible for datasets of even small sizes. Three such measures are the smallest size of a string attractor, the smallest size of a bidirectional macro scheme, and the smallest size of a straight-line program. While a vast variety of implementations for heuristically computing approximations exist, exact computation of these measures has received little to no attention. In this paper, we present MAX-SAT formulations that provide the first non-trivial implementations for exact computation of smallest string attractors, smallest bidirectional macro schemes, and smallest straight-line programs. Computational experiments show that our implementations work for texts of length up to a few hundred for straight-line programs and bidirectional macro schemes, and texts even over a million for string attractors.

</details>

<details>

<summary>2022-07-12 12:09:31 - Popular Matchings with One-Sided Bias</summary>

- *Telikepalli Kavitha*

- `2207.05488v1` - [abs](http://arxiv.org/abs/2207.05488v1) - [pdf](http://arxiv.org/pdf/2207.05488v1)

> Let $G = (A \cup B,E)$ be a bipartite graph where the set $A$ consists of agents or main players and the set $B$ consists of jobs or secondary players. Every vertex has a strict ranking of its neighbors. A matching $M$ is popular if for any matching $N$, the number of vertices that prefer $M$ to $N$ is at least the number that prefer $N$ to $M$. Popular matchings always exist in $G$ since every stable matching is popular.   A matching $M$ is $A$-popular if for any matching $N$, the number of agents (i.e., vertices in $A$) that prefer $M$ to $N$ is at least the number of agents that prefer $N$ to $M$. Unlike popular matchings, $A$-popular matchings need not exist in a given instance $G$ and there is a simple linear time algorithm to decide if $G$ admits an $A$-popular matching and compute one, if so.   We consider the problem of deciding if $G$ admits a matching that is both popular and $A$-popular and finding one, if so. We call such matchings fully popular. A fully popular matching is useful when $A$ is the more important side -- so along with overall popularity, we would like to maintain ``popularity within the set $A$''. A fully popular matching is not necessarily a min-size/max-size popular matching and all known polynomial-time algorithms for popular matching problems compute either min-size or max-size popular matchings. Here we show a linear time algorithm for the fully popular matching problem, thus our result shows a new tractable subclass of popular matchings.

</details>

<details>

<summary>2022-07-12 13:47:57 - Fuzzing Deep-Learning Libraries via Automated Relational API Inference</summary>

- *Yinlin Deng, Chenyuan Yang, Anjiang Wei, Lingming Zhang*

- `2207.05531v1` - [abs](http://arxiv.org/abs/2207.05531v1) - [pdf](http://arxiv.org/pdf/2207.05531v1)

> A growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily "borrow" test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic or semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).

</details>

<details>

<summary>2022-07-12 15:30:46 - Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic Python Code with Pythonic Idioms</summary>

- *Zejun Zhang, Zhenchang Xing, Xin Xia, Xiwei Xu, Liming Zhu*

- `2207.05613v1` - [abs](http://arxiv.org/abs/2207.05613v1) - [pdf](http://arxiv.org/pdf/2207.05613v1)

> Compared to other programming languages (e.g., Java), Python has more idioms to make Python code concise and efficient. Although pythonic idioms are well accepted in the Python community, Python programmers are often faced with many challenges in using them, for example, being unaware of certain pythonic idioms or do not know how to use them properly. Based on an analysis of 7,638 Python repositories on GitHub, we find that non-idiomatic Python code that can be implemented with pythonic idioms occurs frequently and widely. Unfortunately, there is no tool for automatically refactoring such non-idiomatic code into idiomatic code. In this paper, we design and implement an automatic refactoring tool to make Python code idiomatic. We identify nine pythonic idioms by systematically contrasting the abstract syntax grammar of Python and Java. Then we define the syntactic patterns for detecting non-idiomatic code for each pythonic idiom. Finally, we devise atomic AST-rewriting operations and refactoring steps to refactor non-idiomatic code into idiomatic code. We test and review over 4,115 refactorings applied to 1,065 Python projects from GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to 84 projects. These evaluations confirm the high-accuracy, practicality and usefulness of our refactoring tool on real-world Python code. Our refactoring tool can be accessed at 47.242.131.128:5000.

</details>

<details>

<summary>2022-07-12 19:07:42 - Surrogate Likelihoods for Variational Annealed Importance Sampling</summary>

- *Martin Jankowiak, Du Phan*

- `2112.12194v2` - [abs](http://arxiv.org/abs/2112.12194v2) - [pdf](http://arxiv.org/pdf/2112.12194v2)

> Variational inference is a powerful paradigm for approximate Bayesian inference with a number of appealing properties, including support for model learning and data subsampling. By contrast MCMC methods like Hamiltonian Monte Carlo do not share these properties but remain attractive since, contrary to parametric methods, MCMC is asymptotically unbiased. For these reasons researchers have sought to combine the strengths of both classes of algorithms, with recent approaches coming closer to realizing this vision in practice. However, supporting data subsampling in these hybrid methods can be a challenge, a shortcoming that we address by introducing a surrogate likelihood that can be learned jointly with other variational parameters. We argue theoretically that the resulting algorithm permits the user to make an intuitive trade-off between inference fidelity and computational cost. In an extensive empirical comparison we show that our method performs well in practice and that it is well-suited for black-box inference in probabilistic programming frameworks.

</details>

<details>

<summary>2022-07-12 21:27:59 - Compactly Restrictable Metric Policy Optimization Problems</summary>

- *Victor D. Dorobantu, Kamyar Azizzadenesheli, Yisong Yue*

- `2207.05850v1` - [abs](http://arxiv.org/abs/2207.05850v1) - [pdf](http://arxiv.org/pdf/2207.05850v1)

> We study policy optimization problems for deterministic Markov decision processes (MDPs) with metric state and action spaces, which we refer to as Metric Policy Optimization Problems (MPOPs). Our goal is to establish theoretical results on the well-posedness of MPOPs that can characterize practically relevant continuous control systems. To do so, we define a special class of MPOPs called Compactly Restrictable MPOPs (CR-MPOPs), which are flexible enough to capture the complex behavior of robotic systems but specific enough to admit solutions using dynamic programming methods such as value iteration. We show how to arrive at CR-MPOPs using forward-invariance. We further show that our theoretical results on CR-MPOPs can be used to characterize feedback linearizable control affine systems.

</details>

<details>

<summary>2022-07-12 22:34:39 - A Novel DeBERTa-based Model for Financial Question Answering Task</summary>

- *Yanbo J. Wang, Yuming Li, Hui Qin, Yuhang Guan, Sheng Chen*

- `2207.05875v1` - [abs](http://arxiv.org/abs/2207.05875v1) - [pdf](http://arxiv.org/pdf/2207.05875v1)

> As a rising star in the field of natural language processing, question answering systems (Q&A Systems) are widely used in all walks of life. Compared with other scenarios, the applicationin financial scenario has strong requirements in the traceability and interpretability of the Q&A systems. In addition, since the demand for artificial intelligence technology has gradually shifted from the initial computational intelligence to cognitive intelligence, this research mainly focuses on the financial numerical reasoning dataset - FinQA. In the shared task, the objective is to generate the reasoning program and the final answer according to the given financial report containing text and tables. We use the method based on DeBERTa pre-trained language model, with additional optimization methods including multi-model fusion, training set combination on this basis. We finally obtain an execution accuracy of 68.99 and a program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge.

</details>

<details>

<summary>2022-07-13 01:12:04 - NumS: Scalable Array Programming for the Cloud</summary>

- *Melih Elibol, Vinamra Benara, Samyu Yagati, Lianmin Zheng, Alvin Cheung, Michael I. Jordan, Ion Stoica*

- `2206.14276v2` - [abs](http://arxiv.org/abs/2206.14276v2) - [pdf](http://arxiv.org/pdf/2206.14276v2)

> Scientists increasingly rely on Python tools to perform scalable distributed memory array operations using rich, NumPy-like expressions. However, many of these tools rely on dynamic schedulers optimized for abstract task graphs, which often encounter memory and network bandwidth-related bottlenecks due to sub-optimal data and operator placement decisions. Tools built on the message passing interface (MPI), such as ScaLAPACK and SLATE, have better scaling properties, but these solutions require specialized knowledge to use. In this work, we present NumS, an array programming library which optimizes NumPy-like expressions on task-based distributed systems. This is achieved through a novel scheduler called Load Simulated Hierarchical Scheduling (LSHS). LSHS is a local search method which optimizes operator placement by minimizing maximum memory and network load on any given node within a distributed system. Coupled with a heuristic for load balanced data layouts, our approach is capable of attaining communication lower bounds on some common numerical operations, and our empirical study shows that LSHS enhances performance on Ray by decreasing network load by a factor of 2x, requiring 4x less memory, and reducing execution time by 10x on the logistic regression problem. On terabyte-scale data, NumS achieves competitive performance to SLATE on DGEMM, up to 20x speedup over Dask on a key operation for tensor factorization, and a 2x speedup on logistic regression compared to Dask ML and Spark's MLlib.

</details>

<details>

<summary>2022-07-13 01:19:16 - Measurement and applications of position bias in a marketplace search engine</summary>

- *Richard Demsyn-Jones*

- `2206.11720v2` - [abs](http://arxiv.org/abs/2206.11720v2) - [pdf](http://arxiv.org/pdf/2206.11720v2)

> Search engines intentionally influence user behavior by picking and ranking the list of results. Users engage with the highest results both because of their prominent placement and because they are typically the most relevant documents. Search engine ranking algorithms need to identify relevance while incorporating the influence of the search engine itself. This paper describes our efforts at Thumbtack to understand the impact of ranking, including the empirical results of a randomization program. In the context of a consumer marketplace we discuss practical details of model choice, experiment design, bias calculation, and machine learning model adaptation. We include a novel discussion of how ranking bias may not only affect labels, but also model features. The randomization program led to improved models, motivated internal scenario analysis, and enabled user-facing scenario tooling.

</details>

<details>

<summary>2022-07-13 03:54:43 - Machine Learning Assisted Approach for Security-Constrained Unit Commitment</summary>

- *Arun Venkatesh Ramesh, Xingpeng Li*

- `2111.09824v2` - [abs](http://arxiv.org/abs/2111.09824v2) - [pdf](http://arxiv.org/pdf/2111.09824v2)

> Security-constrained unit commitment (SCUC) is solved for power system day-ahead generation scheduling, which is a large-scale mixed-integer linear programming problem and is very computationally intensive. Model reduction of SCUC may bring significant time savings. In this work, a novel approach is proposed to effectively utilize machine learning (ML) to reduce the problem size of SCUC. An ML model using logistic regression (LR) algorithm is proposed and trained with historical nodal demand profiles and the respective commitment schedules. The ML outputs are processed and analyzed to reduce variables and constraints in SCUC. The proposed approach is validated on several standard test systems including IEEE 24-bus system, IEEE 73-bus system, IEEE 118-bus system, synthetic South Carolina 500-bus system and Polish 2383-bus system. Simulation results demonstrate that the use of the prediction from the proposed LR model in SCUC model reduction can substantially reduce the computing time while maintaining solution quality.

</details>

<details>

<summary>2022-07-13 06:44:26 - Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation</summary>

- *Yupei Lin, Sen Zhang, Tianshui Chen, Yongyi Lu, Guangping Li, Yukai Shi*

- `2204.11018v2` - [abs](http://arxiv.org/abs/2204.11018v2) - [pdf](http://arxiv.org/pdf/2204.11018v2)

> Unpaired image-to-image translation aims to find a mapping between the source domain and the target domain. To alleviate the problem of the lack of supervised labels for the source images, cycle-consistency based methods have been proposed for image structure preservation by assuming a reversible relationship between unpaired images. However, this assumption only uses limited correspondence between image pairs. Recently, contrastive learning (CL) has been used to further investigate the image correspondence in unpaired image translation by using patch-based positive/negative learning. Patch-based contrastive routines obtain the positives by self-similarity computation and recognize the rest patches as negatives. This flexible learning paradigm obtains auxiliary contextualized information at a low cost. As the negatives own an impressive sample number, with curiosity, we make an investigation based on a question: are all negatives necessary for feature contrastive learning? Unlike previous CL approaches that use negatives as much as possible, in this paper, we study the negatives from an information-theoretic perspective and introduce a new negative Pruning technology for Unpaired image-to-image Translation (PUT) by sparsifying and ranking the patches. The proposed algorithm is efficient, flexible and enables the model to learn essential information between corresponding patches stably. By putting quality over quantity, only a few negative patches are required to achieve better results. Lastly, we validate the superiority, stability, and versatility of our model through comparative experiments.

</details>

<details>

<summary>2022-07-13 06:46:22 - Realizability Makes a Difference: A Complexity Gap for Sink-Finding in USOs</summary>

- *Simon Weber, Joel Widmer*

- `2207.05985v1` - [abs](http://arxiv.org/abs/2207.05985v1) - [pdf](http://arxiv.org/pdf/2207.05985v1)

> Algorithms for finding the sink in Unique Sink Orientations (USOs) of the hypercube can be used to solve many algebraic and geometric problems, most importantly including the P-Matrix Linear Complementarity Problem and Linear Programming. The realizable USOs are those that arise from the reductions of these problems to the USO sink-finding problem. Finding the sink of realizable USOs is thus highly practically relevant, yet it is unknown whether realizability can be exploited algorithmically to find the sink more quickly. However, all (non-trivial) known unconditional lower bounds for sink-finding make use of USOs that are provably not realizable. This indicates that the sink-finding problem might indeed be strictly easier on realizable USOs.   In this paper we show that this is true for a subclass of all USOs. We consider the class of Matou\v{s}ek-type USOs, which are a translation of Matou\v{s}ek's LP-type problems into the language of USOs. We show a query complexity gap between sink-finding in all, and sink-finding in only the realizable $n$-dimensional Matou\v{s}ek-type USOs. We provide concrete deterministic algorithms and lower bounds for both cases, and show that in the realizable case $O(log^2 n)$ vertex evaluation queries suffice, while in general exactly $n$ queries are needed. The Matou\v{s}ek-type USOs are the first USO class found to admit such a gap.

</details>

<details>

<summary>2022-07-13 12:47:00 - Dress Code: High-Resolution Multi-Category Virtual Try-On</summary>

- *Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, Rita Cucchiara*

- `2204.08532v2` - [abs](http://arxiv.org/abs/2204.08532v2) - [pdf](http://arxiv.org/pdf/2204.08532v2)

> Image-based virtual try-on strives to transfer the appearance of a clothing item onto the image of a target person. Prior work focuses mainly on upper-body clothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body items. This shortcoming arises from a main factor: current publicly available datasets for image-based virtual try-on do not account for this variety, thus limiting progress in the field. To address this deficiency, we introduce Dress Code, which contains images of multi-category clothes. Dress Code is more than 3x larger than publicly available datasets for image-based virtual try-on and features high-resolution paired images (1024x768) with front-view, full-body reference models. To generate HD try-on images with high visual quality and rich in details, we propose to learn fine-grained discriminating features. Specifically, we leverage a semantic-aware discriminator that makes predictions at pixel-level instead of image- or patch-level. Extensive experimental evaluation demonstrates that the proposed approach surpasses the baselines and state-of-the-art competitors in terms of visual quality and quantitative results. The Dress Code dataset is publicly available at https://github.com/aimagelab/dress-code.

</details>

<details>

<summary>2022-07-13 17:10:47 - Iterative Linear Quadratic Optimization for Nonlinear Control: Differentiable Programming Algorithmic Templates</summary>

- *Vincent Roulet, Siddhartha Srinivasa, Maryam Fazel, Zaid Harchaoui*

- `2207.06362v1` - [abs](http://arxiv.org/abs/2207.06362v1) - [pdf](http://arxiv.org/pdf/2207.06362v1)

> We present the implementation of nonlinear control algorithms based on linear and quadratic approximations of the objective from a functional viewpoint. We present a gradient descent, a Gauss-Newton method, a Newton method, differential dynamic programming approaches with linear quadratic or quadratic approximations, various line-search strategies, and regularized variants of these algorithms. We derive the computational complexities of all algorithms in a differentiable programming framework and present sufficient optimality conditions. We compare the algorithms on several benchmarks, such as autonomous car racing using a bicycle model of a car. The algorithms are coded in a differentiable programming language in a publicly available package.

</details>

<details>

<summary>2022-07-13 21:34:11 - Off-line approximate dynamic programming for the vehicle routing problem with a highly variable customer basis and stochastic demands</summary>

- *Mohsen Dastpak, Fausto Errico, Ola Jabali*

- `2109.10200v2` - [abs](http://arxiv.org/abs/2109.10200v2) - [pdf](http://arxiv.org/pdf/2109.10200v2)

> We study a stochastic variant of the vehicle routing problem arising in the context of domestic donor collection services. The problem we consider combines the following attributes. Customers requesting services are variable, in the sense that the customers are stochastic but are not restricted to a predefined set, as they may appear anywhere in a given service area. Furthermore, demand volumes are stochastic and observed upon visiting the customer. The objective is to maximize the expected served demands while meeting vehicle capacity and time restrictions. We call this problem the VRP with a highly Variable Customer basis and Stochastic Demands (VRP-VCSD). For this problem, we first propose a Markov Decision Process (MDP) formulation representing the classical centralized decision-making perspective where one decision-maker establishes the routes of all vehicles. While the resulting formulation turns out to be intractable, it provides us with the ground to develop a new MDP formulation, which we call partially decentralized. In this formulation, the action-space is decomposed by vehicle. However, the decentralization is incomplete as we enforce identical vehicle-specific policies while optimizing the collective reward. We propose several strategies to reduce the dimension of the state and action spaces associated with the partially decentralized formulation. These yield a considerably more tractable problem, which we solve via Reinforcement Learning. In particular, we develop a Q-learning algorithm called DecQN, featuring state-of-the-art acceleration techniques. We conduct a thorough computational analysis. Results show that DecQN considerably outperforms three benchmark policies. Moreover, we show that our approach can compete with specialized methods developed for the particular case of the VRP-VCSD, where customer locations and expected demands are known in advance.

</details>

<details>

<summary>2022-07-14 02:04:42 - Deepfake Video Detection with Spatiotemporal Dropout Transformer</summary>

- *Daichi Zhang, Fanzhao Lin, Yingying Hua, Pengju Wang, Dan Zeng, Shiming Ge*

- `2207.06612v1` - [abs](http://arxiv.org/abs/2207.06612v1) - [pdf](http://arxiv.org/pdf/2207.06612v1)

> While the abuse of deepfake technology has caused serious concerns recently, how to detect deepfake videos is still a challenge due to the high photo-realistic synthesis of each frame. Existing image-level approaches often focus on single frame and ignore the spatiotemporal cues hidden in deepfake videos, resulting in poor generalization and robustness. The key of a video-level detector is to fully exploit the spatiotemporal inconsistency distributed in local facial regions across different frames in deepfake videos. Inspired by that, this paper proposes a simple yet effective patch-level approach to facilitate deepfake video detection via spatiotemporal dropout transformer. The approach reorganizes each input video into bag of patches that is then fed into a vision transformer to achieve robust representation. Specifically, a spatiotemporal dropout operation is proposed to fully explore patch-level spatiotemporal cues and serve as effective data augmentation to further enhance model's robustness and generalization ability. The operation is flexible and can be easily plugged into existing vision transformers. Extensive experiments demonstrate the effectiveness of our approach against 25 state-of-the-arts with impressive robustness, generalizability, and representation ability.

</details>

<details>

<summary>2022-07-14 05:35:14 - Automated Change Rule Inference for Distance-Based API Misuse Detection</summary>

- *Sebastian Nielebock, Paul Blockhaus, Jacob KrÃ¼ger, Frank Ortmeier*

- `2207.06665v1` - [abs](http://arxiv.org/abs/2207.06665v1) - [pdf](http://arxiv.org/pdf/2207.06665v1)

> Developers build on Application Programming Interfaces (APIs) to reuse existing functionalities of code libraries. Despite the benefits of reusing established libraries (e.g., time savings, high quality), developers may diverge from the API's intended usage; potentially causing bugs or, more specifically, API misuses. Recent research focuses on developing techniques to automatically detect API misuses, but many suffer from a high false-positive rate. In this article, we improve on this situation by proposing ChaRLI (Change RuLe Inference), a technique for automatically inferring change rules from developers' fixes of API misuses based on API Usage Graphs (AUGs). By subsequently applying graph-distance algorithms, we use change rules to discriminate API misuses from correct usages. This allows developers to reuse others' fixes of an API misuse at other code locations in the same or another project. We evaluated the ability of change rules to detect API misuses based on three datasets and found that the best mean relative precision (i.e., for testable usages) ranges from 77.1 % to 96.1 % while the mean recall ranges from 0.007 % to 17.7 % for individual change rules. These results underpin that ChaRLI and our misuse detection are helpful complements to existing API misuse detectors.

</details>

<details>

<summary>2022-07-14 07:32:15 - problexity -- an open-source Python library for binary classification problem complexity assessment</summary>

- *Joanna Komorniczak, Pawel Ksieniewicz*

- `2207.06709v1` - [abs](http://arxiv.org/abs/2207.06709v1) - [pdf](http://arxiv.org/pdf/2207.06709v1)

> The classification problem's complexity assessment is an essential element of many topics in the supervised learning domain. It plays a significant role in meta-learning -- becoming the basis for determining meta-attributes or multi-criteria optimization -- allowing the evaluation of the training set resampling without needing to rebuild the recognition model. The tools currently available for the academic community, which would enable the calculation of problem complexity measures, are available only as libraries of the C++ and R languages. This paper describes the software module that allows for the estimation of 22 complexity measures for the Python language -- compatible with the scikit-learn programming interface -- allowing for the implementation of research using them in the most popular programming environment of the machine learning community.

</details>

<details>

<summary>2022-07-14 09:33:18 - The complexity of finding and enumerating optimal subgraphs to represent spatial correlation</summary>

- *Jessica Enright, Duncan Lee, Kitty Meeks, William Pettersson, John Sylvester*

- `2010.10314v6` - [abs](http://arxiv.org/abs/2010.10314v6) - [pdf](http://arxiv.org/pdf/2010.10314v6)

> Understanding spatial correlation is vital in many fields including epidemiology and social science. Lee, Meeks and Pettersson (Stat. Comput. 2021) recently demonstrated that improved inference for areal unit count data can be achieved by carrying out modifications to a graph representing spatial correlations; specifically, they delete edges of the planar graph derived from border-sharing between geographic regions in order to maximise a specific objective function. In this paper we address the computational complexity of the associated graph optimisation problem. We demonstrate that this problem cannot be solved in polynomial time unless P = NP; we further show intractability for two simpler variants of the problem. We follow these results with two parameterised algorithms that exactly solve the problem. Both of these solve not only the decision problem, but also enumerate all solutions with polynomial time precalculation, delay, and postcalculation time in respective restricted settings. For this problem, efficient enumeration allows the uncertainty in the spatial correlation to be utilised in the modelling. The first enumeration algorithm utilises dynamic programming on a tree decomposition, and has polynomial time precalculation and linear delay if both the treewidth and maximum degree are bounded. The second algorithm is restricted to problem instances with maximum degree three, as may arise from triangulations of planar surfaces, but can output all solutions with FPT precalculation time and linear delay when the maximum number of edges that can be removed is taken as the parameter.

</details>

<details>

<summary>2022-07-14 13:26:47 - Neural Networks for Encoding Dynamic Security-Constrained Optimal Power Flow</summary>

- *Ilgiz Murzakhanov, Andreas Venzke, George S. Misyris, Spyros Chatzivasileiadis*

- `2003.07939v5` - [abs](http://arxiv.org/abs/2003.07939v5) - [pdf](http://arxiv.org/pdf/2003.07939v5)

> This paper introduces a framework to capture previously intractable optimization constraints and transform them to a mixed-integer linear program, through the use of neural networks. We encode the feasible space of optimization problems characterized by both tractable and intractable constraints, e.g. differential equations, to a neural network. Leveraging an exact mixed-integer reformulation of neural networks, we solve mixed-integer linear programs that accurately approximate solutions to the originally intractable non-linear optimization problem. We apply our methods to the AC optimal power flow problem (AC-OPF), where directly including dynamic security constraints renders the AC-OPF intractable. Our proposed approach has the potential to be significantly more scalable than traditional approaches. We demonstrate our approach for power system operation considering N-1 security and small-signal stability, showing how it can efficiently obtain cost-optimal solutions which at the same time satisfy both static and dynamic security constraints.

</details>

<details>

<summary>2022-07-14 15:33:40 - Robot Program Parameter Inference via Differentiable Shadow Program Inversion</summary>

- *Benjamin Alt, Darko Katic, Rainer JÃ¤kel, Asil Kaan Bozcuoglu, Michael Beetz*

- `2103.14452v2` - [abs](http://arxiv.org/abs/2103.14452v2) - [pdf](http://arxiv.org/pdf/2103.14452v2)

> Challenging manipulation tasks can be solved effectively by combining individual robot skills, which must be parameterized for the concrete physical environment and task at hand. This is time-consuming and difficult for human programmers, particularly for force-controlled skills. To this end, we present Shadow Program Inversion (SPI), a novel approach to infer optimal skill parameters directly from data. SPI leverages unsupervised learning to train an auxiliary differentiable program representation ("shadow program") and realizes parameter inference via gradient-based model inversion. Our method enables the use of efficient first-order optimizers to infer optimal parameters for originally non-differentiable skills, including many skill variants currently used in production. SPI zero-shot generalizes across task objectives, meaning that shadow programs do not need to be retrained to infer parameters for different task variants. We evaluate our methods on three different robots and skill frameworks in industrial and household scenarios. Code and examples are available at https://innolab.artiminds.com/icra2021.

</details>

<details>

<summary>2022-07-14 17:36:44 - Learning port-Hamiltonian systems -- algorithms</summary>

- *Vladimir Salnikov, Antoine Falaize, Daria Loziienko*

- `2207.07124v1` - [abs](http://arxiv.org/abs/2207.07124v1) - [pdf](http://arxiv.org/pdf/2207.07124v1)

> In this article we study the possibilities of recovering the structure of port-Hamiltonian systems starting from ``unlabelled'' ordinary differential equations describing mechanical systems. The algorithm we suggest solves the problem in two phases. It starts by constructing the connectivity structure of the system using machine learning methods -- producing thus a graph of interconnected subsystems. Then this graph is enhanced by recovering the Hamiltonian structure of each subsystem as well as the corresponding ports. This second phase relies heavily on results from symplectic and Poisson geometry that we briefly sketch. And the precise solutions can be constructed using methods of computer algebra and symbolic computations. The algorithm permits to extend the port-Hamiltonian formalism to generic ordinary differential equations, hence introducing eventually a new concept of normal forms of ODEs.

</details>

<details>

<summary>2022-07-14 17:59:58 - Bootstrapped Masked Autoencoders for Vision BERT Pretraining</summary>

- *Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu*

- `2207.07116v1` - [abs](http://arxiv.org/abs/2207.07116v1) - [pdf](http://arxiv.org/pdf/2207.07116v1)

> We propose bootstrapped masked autoencoders (BootMAE), a new approach for vision BERT pretraining. BootMAE improves the original masked autoencoders (MAE) with two core designs: 1) momentum encoder that provides online feature as extra BERT prediction targets; 2) target-aware decoder that tries to reduce the pressure on the encoder to memorize target-specific information in BERT pretraining. The first design is motivated by the observation that using a pretrained MAE to extract the features as the BERT prediction target for masked tokens can achieve better pretraining performance. Therefore, we add a momentum encoder in parallel with the original MAE encoder, which bootstraps the pretraining performance by using its own representation as the BERT prediction target. In the second design, we introduce target-specific information (e.g., pixel values of unmasked patches) from the encoder directly to the decoder to reduce the pressure on the encoder of memorizing the target-specific information. Thus, the encoder focuses on semantic modeling, which is the goal of BERT pretraining, and does not need to waste its capacity in memorizing the information of unmasked tokens related to the prediction target. Through extensive experiments, our BootMAE achieves $84.2\%$ Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\%$ under the same pre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic segmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object detection and segmentation on COCO dataset. Code is released at https://github.com/LightDXY/BootMAE.

</details>

<details>

<summary>2022-07-14 20:00:20 - A Scalable AutoML Approach Based on Graph Neural Networks</summary>

- *Mossad Helali, Essam Mansour, Ibrahim Abdelaziz, Julian Dolby, Kavitha Srinivas*

- `2111.00083v4` - [abs](http://arxiv.org/abs/2111.00083v4) - [pdf](http://arxiv.org/pdf/2111.00083v4)

> AutoML systems build machine learning models automatically by performing a search over valid data transformations and learners, along with hyper-parameter optimization for each learner. Many AutoML systems use meta-learning to guide search for optimal pipelines. In this work, we present a novel meta-learning system called KGpip which, (1) builds a database of datasets and corresponding pipelines by mining thousands of scripts with program analysis, (2) uses dataset embeddings to find similar datasets in the database based on its content instead of metadata-based features, (3) models AutoML pipeline creation as a graph generation problem, to succinctly characterize the diverse pipelines seen for a single dataset. KGpip's meta-learning is a sub-component for AutoML systems. We demonstrate this by integrating KGpip with two AutoML systems. Our comprehensive evaluation using 126 datasets, including those used by the state-of-the-art systems, shows that KGpip significantly outperforms these systems.

</details>

<details>

<summary>2022-07-14 23:27:43 - A Theoretically Grounded Benchmark for Evaluating Machine Commonsense</summary>

- *Henrique Santos, Ke Shen, Alice M. Mulvehill, Yasaman Razeghi, Deborah L. McGuinness, Mayank Kejriwal*

- `2203.12184v2` - [abs](http://arxiv.org/abs/2203.12184v2) - [pdf](http://arxiv.org/pdf/2203.12184v2)

> Programming machines with commonsense reasoning (CSR) abilities is a longstanding challenge in the Artificial Intelligence community. Current CSR benchmarks use multiple-choice (and in relatively fewer cases, generative) question-answering instances to evaluate machine commonsense. Recent progress in transformer-based language representation models suggest that considerable progress has been made on existing benchmarks. However, although tens of CSR benchmarks currently exist, and are growing, it is not evident that the full suite of commonsense capabilities have been systematically evaluated. Furthermore, there are doubts about whether language models are 'fitting' to a benchmark dataset's training partition by picking up on subtle, but normatively irrelevant (at least for CSR), statistical features to achieve good performance on the testing partition. To address these challenges, we propose a benchmark called Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based on discriminative question answering, but with questions designed to evaluate diverse aspects of commonsense, such as space, time, and world states. TG-CSR is based on a subset of commonsense categories first proposed as a viable theory of commonsense by Gordon and Hobbs. The benchmark is also designed to be few-shot (and in the future, zero-shot), with only a few training and validation examples provided. This report discusses the structure and construction of the benchmark. Preliminary results suggest that the benchmark is challenging even for advanced language representation models designed for discriminative CSR question answering tasks.   Benchmark access and leaderboard: https://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website: https://usc-isi-i2.github.io/TGCSR/

</details>

<details>

<summary>2022-07-15 07:11:19 - A Controlled Experiment of Different Code Representations for Learning-Based Bug Repair</summary>

- *Marjane Namavar, Noor Nashid, Ali Mesbah*

- `2110.14081v3` - [abs](http://arxiv.org/abs/2110.14081v3) - [pdf](http://arxiv.org/pdf/2110.14081v3)

> Training a deep learning model on source code has gained significant traction recently. Since such models reason about vectors of numbers, source code needs to be converted to a code representation before vectorization. Numerous approaches have been proposed to represent source code, from sequences of tokens to abstract syntax trees. However, there is no systematic study to understand the effect of code representation on learning performance. Through a controlled experiment, we examine the impact of various code representations on model accuracy and usefulness in deep learning-based program repair. We train 21 different generative models that suggest fixes for name-based bugs, including 14 different homogeneous code representations, four mixed representations for the buggy and fixed code, and three different embeddings. We assess if fix suggestions produced by the model in various code representations are automatically patchable, meaning they can be transformed to a valid code that is ready to be applied to the buggy code to fix it. We also conduct a developer study to qualitatively evaluate the usefulness of inferred fixes in different code representations. Our results highlight the importance of code representation and its impact on learning and usefulness. Our findings indicate that (1) while code abstractions help the learning process, they can adversely impact the usefulness of inferred fixes from a developer's point of view; this emphasizes the need to look at the patches generated from the practitioner's perspective, which is often neglected in the literature, (2) mixed representations can outperform homogeneous code representations, (3) bug type can affect the effectiveness of different code representations; although current techniques use a single code representation for all bug types, there is no single best code representation applicable to all bug types.

</details>

<details>

<summary>2022-07-15 08:48:40 - Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection</summary>

- *Svetlana Pavlitskaya, Bianca-Marina CodÃÂu, J. Marius ZÄÅllner*

- `2207.07347v1` - [abs](http://arxiv.org/abs/2207.07347v1) - [pdf](http://arxiv.org/pdf/2207.07347v1)

> Standard approaches for adversarial patch generation lead to noisy conspicuous patterns, which are easily recognizable by humans. Recent research has proposed several approaches to generate naturalistic patches using generative adversarial networks (GANs), yet only a few of them were evaluated on the object detection use case. Moreover, the state of the art mostly focuses on suppressing a single large bounding box in input by overlapping it with the patch directly. Suppressing objects near the patch is a different, more complex task. In this work, we have evaluated the existing approaches to generate inconspicuous patches. We have adapted methods, originally developed for different computer vision tasks, to the object detection use case with YOLOv3 and the COCO dataset. We have evaluated two approaches to generate naturalistic patches: by incorporating patch generation into the GAN training process and by using the pretrained GAN. For both cases, we have assessed a trade-off between performance and naturalistic patch appearance. Our experiments have shown, that using a pre-trained GAN helps to gain realistic-looking patches while preserving the performance similar to conventional adversarial patches.

</details>

<details>

<summary>2022-07-15 13:02:08 - ODFNet: Using orientation distribution functions to characterize 3D point clouds</summary>

- *Yusuf H. Sahin, Alican Mertan, Gozde Unal*

- `2012.04708v2` - [abs](http://arxiv.org/abs/2012.04708v2) - [pdf](http://arxiv.org/pdf/2012.04708v2)

> Learning new representations of 3D point clouds is an active research area in 3D vision, as the order-invariant point cloud structure still presents challenges to the design of neural network architectures. Recent works explored learning either global or local features or both for point clouds, however none of the earlier methods focused on capturing contextual shape information by analysing local orientation distribution of points. In this paper, we leverage on point orientation distributions around a point in order to obtain an expressive local neighborhood representation for point clouds. We achieve this by dividing the spherical neighborhood of a given point into predefined cone volumes, and statistics inside each volume are used as point features. In this way, a local patch can be represented by not only the selected point's nearest neighbors, but also considering a point density distribution defined along multiple orientations around the point. We are then able to construct an orientation distribution function (ODF) neural network that involves an ODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet model achieves state-of the-art accuracy for object classification on ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS datasets.

</details>

<details>

<summary>2022-07-15 13:27:07 - Open-source software for electrical engineering applications requiring consideration of electrodynamics: elecode</summary>

- *Dmitry Kuklin*

- `2207.06908v2` - [abs](http://arxiv.org/abs/2207.06908v2) - [pdf](http://arxiv.org/pdf/2207.06908v2)

> The work presents elecode, open-source software for various electrical engineering applications that require considering electromagnetic processes. The primary focus of the software is power engineering applications. However, the software does not impose any specific limitations preventing other uses. In contrast to other open-source software based on the Finite Difference Time Domain (FDTD) method, elecode implements various thin wire modeling techniques which allow simulating complex objects consisting of wires. In addition, implemented graphical user interface (GUI) helps modify models conveniently. The software provides auxiliary numerical methods for simulations and measurements of the electrical soil properties, allows conducting lightning-related simulations (including those involving isolation breakdown models), and calculations of grounding characteristics. The part of the code responsible for FDTD simulations is well tested in previous works. Recently, the code was rewritten in order to add a convenient interface for using it as a library, command-line program, or GUI program. Finally, the code was released under an open-source license. The main capabilities of the software are described in the work. Several simulation examples covering main software features are presented. elecode is available at https://gitlab.com/dmika/elecode.

</details>

<details>

<summary>2022-07-15 15:06:40 - Dynamic Algorithms for Packing-Covering LPs via Multiplicative Weight Updates</summary>

- *Sayan Bhattacharya, Peter Kiss, Thatchaphol Saranurak*

- `2207.07519v1` - [abs](http://arxiv.org/abs/2207.07519v1) - [pdf](http://arxiv.org/pdf/2207.07519v1)

> In the dynamic linear program (LP) problem, we are given an LP undergoing updates and we need to maintain an approximately optimal solution. Recently, significant attention (e.g., [Gupta et al. STOC'17; Arar et al. ICALP'18, Wajc STOC'20]) has been devoted to the study of special cases of dynamic packing and covering LPs, such as the dynamic fractional matching and set cover problems. But until now, there is no non-trivial dynamic algorithm for general packing and covering LPs.   In this paper, we settle the complexity of dynamic packing and covering LPs, up to a polylogarithmic factor in update time. More precisely, in the partially dynamic setting (where updates can either only relax or only restrict the feasible region), we give near-optimal deterministic $\epsilon$-approximation algorithms with polylogarithmic amortized update time. Then, we show that both partially dynamic updates and amortized update time are necessary; without any of these conditions, the trivial algorithm that recomputes the solution from scratch after every update is essentially the best possible, assuming SETH.   To obtain our results, we initiate a systematic study of the multiplicative weights update (MWU) method in the dynamic setting. As by-products of our techniques, we also obtain the first online $(1+\epsilon)$-competitive algorithms for both covering and packing LPs with polylogarithmic recourse, and the first streaming algorithms for covering and packing LPs with linear space and polylogarithmic passes.

</details>

<details>

<summary>2022-07-15 16:43:14 - Teaching Networks to Solve Optimization Problems</summary>

- *Xinran Liu, Yuzhe Lu, Ali Abbasi, Meiyi Li, Javad Mohammadi, Soheil Kolouri*

- `2202.04104v2` - [abs](http://arxiv.org/abs/2202.04104v2) - [pdf](http://arxiv.org/pdf/2202.04104v2)

> Leveraging machine learning to facilitate the optimization process is an emerging field that holds the promise to bypass the fundamental computational bottleneck caused by classic iterative solvers in critical applications requiring near-real-time optimization. The majority of existing approaches focus on learning data-driven optimizers that lead to fewer iterations in solving an optimization. In this paper, we take a different approach and propose to replace the iterative solvers altogether with a trainable parametric set function, that outputs the optimal arguments/parameters of an optimization problem in a single feed forward. We denote our method as Learning to Optimize the Optimization Process (LOOP). We show the feasibility of learning such parametric (set) functions to solve various classic optimization problems including linear/nonlinear regression, principal component analysis, transport-based coreset, and quadratic programming in supply management applications. In addition, we propose two alternative approaches for learning such parametric functions, with and without a solver in the LOOP. Finally, through various numerical experiments, we show that the trained solvers could be orders of magnitude faster than the classic iterative solvers while providing near optimal solutions.

</details>

<details>

<summary>2022-07-15 17:04:24 - Feature Learning in Infinite-Width Neural Networks</summary>

- *Greg Yang, Edward J. Hu*

- `2011.14522v3` - [abs](http://arxiv.org/abs/2011.14522v3) - [pdf](http://arxiv.org/pdf/2011.14522v3)

> As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.   More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.

</details>

<details>

<summary>2022-07-15 18:21:51 - Support Vector Machines with the Hard-Margin Loss: Optimal Training via Combinatorial Benders' Cuts</summary>

- *Ãtalo Santana, Breno Serrano, Maximilian Schiffer, Thibaut Vidal*

- `2207.07690v1` - [abs](http://arxiv.org/abs/2207.07690v1) - [pdf](http://arxiv.org/pdf/2207.07690v1)

> The classical hinge-loss support vector machines (SVMs) model is sensitive to outlier observations due to the unboundedness of its loss function. To circumvent this issue, recent studies have focused on non-convex loss functions, such as the hard-margin loss, which associates a constant penalty to any misclassified or within-margin sample. Applying this loss function yields much-needed robustness for critical applications but it also leads to an NP-hard model that makes training difficult, since current exact optimization algorithms show limited scalability, whereas heuristics are not able to find high-quality solutions consistently. Against this background, we propose new integer programming strategies that significantly improve our ability to train the hard-margin SVM model to global optimality. We introduce an iterative sampling and decomposition approach, in which smaller subproblems are used to separate combinatorial Benders' cuts. Those cuts, used within a branch-and-cut algorithm, permit to converge much more quickly towards a global optimum. Through extensive numerical analyses on classical benchmark data sets, our solution algorithm solves, for the first time, 117 new data sets to optimality and achieves a reduction of 50% in the average optimality gap for the hardest datasets of the benchmark.

</details>

<details>

<summary>2022-07-15 18:36:29 - POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging</summary>

- *Shishir G. Patil, Paras Jain, Prabal Dutta, Ion Stoica, Joseph E. Gonzalez*

- `2207.07697v1` - [abs](http://arxiv.org/abs/2207.07697v1) - [pdf](http://arxiv.org/pdf/2207.07697v1)

> Fine-tuning models on edge devices like mobile phones would enable privacy-preserving personalization over sensitive data. However, edge training has historically been limited to relatively small models with simple architectures because training is both memory and energy intensive. We present POET, an algorithm to enable training large neural networks on memory-scarce battery-operated edge devices. POET jointly optimizes the integrated search search spaces of rematerialization and paging, two algorithms to reduce the memory consumption of backpropagation. Given a memory budget and a run-time constraint, we formulate a mixed-integer linear program (MILP) for energy-optimal training. Our approach enables training significantly larger models on embedded devices while reducing energy consumption while not modifying mathematical correctness of backpropagation. We demonstrate that it is possible to fine-tune both ResNet-18 and BERT within the memory constraints of a Cortex-M class embedded device while outperforming current edge training methods in energy efficiency. POET is an open-source project available at https://github.com/ShishirPatil/poet

</details>

<details>

<summary>2022-07-15 19:04:43 - Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis</summary>

- *Shounak Naik, Rajaswa Patil, Swati Agarwal, Veeky Baths*

- `2207.07706v1` - [abs](http://arxiv.org/abs/2207.07706v1) - [pdf](http://arxiv.org/pdf/2207.07706v1)

> Representational Similarity Analysis is a method from cognitive neuroscience, which helps in comparing representations from two different sources of data. In this paper, we propose using Representational Similarity Analysis to probe the semantic grounding in language models of code. We probe representations from the CodeBERT model for semantic grounding by using the data from the IBM CodeNet dataset. Through our experiments, we show that current pre-training methods do not induce semantic grounding in language models of code, and instead focus on optimizing form-based patterns. We also show that even a little amount of fine-tuning on semantically relevant tasks increases the semantic grounding in CodeBERT significantly. Our ablations with the input modality to the CodeBERT model show that using bimodal inputs (code and natural language) over unimodal inputs (only code) gives better semantic grounding and sample efficiency during semantic fine-tuning. Finally, our experiments with semantic perturbations in code reveal that CodeBERT is able to robustly distinguish between semantically correct and incorrect code.

</details>

<details>

<summary>2022-07-15 19:17:55 - A No-Code Low-Code Paradigm for Authoring Business Automations Using Natural Language</summary>

- *Michael Desmond, Evelyn Duesterwald, Vatche Isahagian, Vinod Muthusamy*

- `2207.10648v1` - [abs](http://arxiv.org/abs/2207.10648v1) - [pdf](http://arxiv.org/pdf/2207.10648v1)

> Most business process automation is still developed using traditional automation technologies such as workflow engines. These systems provide domain specific languages that require both business knowledge and programming skills to effectively use. As such, business users often lack adequate programming skills to fully leverage these code oriented environments. We propose a paradigm for the construction of business automations using natural language. The approach applies a large language model to translate business rules and automations described in natural language, into a domain specific language interpretable by a business rule engine. We compare the performance of various language model configurations, across various target domains, and explore the use of constrained decoding to ensure syntactically correct generation of output.

</details>

<details>

<summary>2022-07-16 00:10:12 - BCRLSP: An Offline Reinforcement Learning Framework for Sequential Targeted Promotion</summary>

- *Fanglin Chen, Xiao Liu, Bo Tang, Feiyu Xiong, Serim Hwang, Guomian Zhuang*

- `2207.07790v1` - [abs](http://arxiv.org/abs/2207.07790v1) - [pdf](http://arxiv.org/pdf/2207.07790v1)

> We utilize an offline reinforcement learning (RL) model for sequential targeted promotion in the presence of budget constraints in a real-world business environment. In our application, the mobile app aims to boost customer retention by sending cash bonuses to customers and control the costs of such cash bonuses during each time period. To achieve the multi-task goal, we propose the Budget Constrained Reinforcement Learning for Sequential Promotion (BCRLSP) framework to determine the value of cash bonuses to be sent to users. We first find out the target policy and the associated Q-values that maximizes the user retention rate using an RL model. A linear programming (LP) model is then added to satisfy the constraints of promotion costs. We solve the LP problem by maximizing the Q-values of actions learned from the RL model given the budget constraints. During deployment, we combine the offline RL model with the LP model to generate a robust policy under the budget constraints. Using both online and offline experiments, we demonstrate the efficacy of our approach by showing that BCRLSP achieves a higher long-term customer retention rate and a lower cost than various baselines. Taking advantage of the near real-time cost control method, the proposed framework can easily adapt to data with a noisy behavioral policy and/or meet flexible budget constraints.

</details>

<details>

<summary>2022-07-16 06:13:26 - Efficient Online ML API Selection for Multi-Label Classification Tasks</summary>

- *Lingjiao Chen, Matei Zaharia, James Zou*

- `2102.09127v2` - [abs](http://arxiv.org/abs/2102.09127v2) - [pdf](http://arxiv.org/pdf/2102.09127v2)

> Multi-label classification tasks such as OCR and multi-object recognition are a major focus of the growing machine learning as a service industry. While many multi-label prediction APIs are available, it is challenging for users to decide which API to use for their own data and budget, due to the heterogeneity in those APIs' price and performance. Recent work shows how to select from single-label prediction APIs. However the computation complexity of the previous approach is exponential in the number of labels and hence is not suitable for settings like OCR. In this work, we propose FrugalMCT, a principled framework that adaptively selects the APIs to use for different data in an online fashion while respecting user's budget. The API selection problem is cast as an integer linear program, which we show has a special structure that we leverage to develop an efficient online API selector with strong performance guarantees. We conduct systematic experiments using ML APIs from Google, Microsoft, Amazon, IBM, Tencent and other providers for tasks including multi-label image classification, scene text recognition and named entity recognition. Across diverse tasks, FrugalMCT can achieve over 90% cost reduction while matching the accuracy of the best single API, or up to 8% better accuracy while matching the best API's cost.

</details>

<details>

<summary>2022-07-16 17:14:24 - Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure</summary>

- *Jason M. Altschuler, Enric Boix-Adsera*

- `2008.03006v4` - [abs](http://arxiv.org/abs/2008.03006v4) - [pdf](http://arxiv.org/pdf/2008.03006v4)

> Multimarginal Optimal Transport (MOT) has attracted significant interest due to applications in machine learning, statistics, and the sciences. However, in most applications, the success of MOT is severely limited by a lack of efficient algorithms. Indeed, MOT in general requires exponential time in the number of marginals k and their support sizes n. This paper develops a general theory about what "structure" makes MOT solvable in poly(n,k) time.   We develop a unified algorithmic framework for solving MOT in poly(n,k) time by characterizing the "structure" that different algorithms require in terms of simple variants of the dual feasibility oracle. This framework has several benefits. First, it enables us to show that the Sinkhorn algorithm, which is currently the most popular MOT algorithm, requires strictly more structure than other algorithms do to solve MOT in poly(n,k) time. Second, our framework makes it much simpler to develop poly(n,k) time algorithms for a given MOT problem. In particular, it is necessary and sufficient to (approximately) solve the dual feasibility oracle -- which is much more amenable to standard algorithmic techniques.   We illustrate this ease-of-use by developing poly(n,k) time algorithms for three general classes of MOT cost structures: (1) graphical structure; (2) set-optimization structure; and (3) low-rank plus sparse structure. For structure (1), we recover the known result that Sinkhorn has poly(n,k) runtime; moreover, we provide the first poly(n,k) time algorithms for computing solutions that are exact and sparse. For structures (2)-(3), we give the first poly(n,k) time algorithms, even for approximate computation. Together, these three structures encompass many -- if not most -- current applications of MOT.

</details>

<details>

<summary>2022-07-16 21:15:21 - Do Fewer Tiers Mean Fewer Tears? Eliminating Web Stack Components to Improve Interoperability</summary>

- *Adrian Ramsingh, Jeremy Singer, Phil Trinder*

- `2207.08019v1` - [abs](http://arxiv.org/abs/2207.08019v1) - [pdf](http://arxiv.org/pdf/2207.08019v1)

> Web applications are structured as multi-tier stacks of components. Each component may be written in a different language and interoperate using a variety of protocols. Such interoperation increases developer effort, can introduce security vulnerabilities, may reduce performance and require additional resources. A range of approaches have been explored to minimise web stack interoperation.   This paper explores a pragmatic approach to reducing web stack interoperation, namely eliminating a tier/component. That is, we explore the implications of eliminating the Apache web server in a JAPyL web stack: Jupyter Notebook, Apache, Python, Linux, and replacing it with PHP libraries. We conduct a systematic study to investigate the implications for web stack performance, resource consumption, security, and programming effort.

</details>

<details>

<summary>2022-07-17 01:14:43 - Risk-averse Stochastic Optimization for Farm Management Practices and Cultivar Selection Under Uncertainty</summary>

- *Faezeh Akhavizadegan, Javad Ansarifar, Lizhi Wang, Sotirios V. Archontoulis*

- `2208.04840v1` - [abs](http://arxiv.org/abs/2208.04840v1) - [pdf](http://arxiv.org/pdf/2208.04840v1)

> Optimizing management practices and selecting the best cultivar for planting play a significant role in increasing agricultural food production and decreasing environmental footprint. In this study, we develop optimization frameworks under uncertainty using conditional value-at-risk in the stochastic programming objective function. We integrate the crop model, APSIM, and a parallel Bayesian optimization algorithm to optimize the management practices and select the best cultivar at different levels of risk aversion. This approach integrates the power of optimization in determining the best decisions and crop model in simulating nature's output corresponding to various decisions. As a case study, we set up the crop model for 25 locations across the US Corn Belt. We optimized the management options (planting date, N fertilizer amount, fertilizing date, and plant density in the farm) and cultivar options (cultivars with different maturity days) three times: a) before, b) at planting and c) after a growing season with known weather. Results indicated that the proposed model produced meaningful connections between weather and optima decisions. Also, we found risk-tolerance farmers get more expected yield than risk-averse ones in wet and non-wet weathers.

</details>

<details>

<summary>2022-07-17 01:28:23 - Repairing Systematic Outliers by Learning Clean Subspaces in VAEs</summary>

- *Simao Eduardo, Kai Xu, Alfredo Nazabal, Charles Sutton*

- `2207.08050v1` - [abs](http://arxiv.org/abs/2207.08050v1) - [pdf](http://arxiv.org/pdf/2207.08050v1)

> Data cleaning often comprises outlier detection and data repair. Systematic errors result from nearly deterministic transformations that occur repeatedly in the data, e.g. specific image pixels being set to default values or watermarks. Consequently, models with enough capacity easily overfit to these errors, making detection and repair difficult. Seeing as a systematic outlier is a combination of patterns of a clean instance and systematic error patterns, our main insight is that inliers can be modelled by a smaller representation (subspace) in a model than outliers. By exploiting this, we propose Clean Subspace Variational Autoencoder (CLSVAE), a novel semi-supervised model for detection and automated repair of systematic errors. The main idea is to partition the latent space and model inlier and outlier patterns separately. CLSVAE is effective with much less labelled data compared to previous related models, often with less than 2% of the data. We provide experiments using three image datasets in scenarios with different levels of corruption and labelled set sizes, comparing to relevant baselines. CLSVAE provides superior repairs without human intervention, e.g. with just 0.25% of labelled data we see a relative error decrease of 58% compared to the closest baseline.

</details>

<details>

<summary>2022-07-17 05:14:21 - A compact butterfly-style silicon photonic-electronic neural chip for hardware-efficient deep learning</summary>

- *Chenghao Feng, Jiaqi Gu, Hanqing Zhu, Zhoufeng Ying, Zheng Zhao, David Z. Pan, Ray T. Chen*

- `2111.06705v2` - [abs](http://arxiv.org/abs/2111.06705v2) - [pdf](http://arxiv.org/pdf/2111.06705v2)

> The optical neural network (ONN) is a promising hardware platform for next-generation neurocomputing due to its high parallelism, low latency, and low energy consumption. Previous ONN architectures are mainly designed for general matrix multiplication (GEMM), leading to unnecessarily large area cost and high control complexity. Here, we move beyond classical GEMM-based ONNs and propose an optical subspace neural network (OSNN) architecture, which trades the universality of weight representation for lower optical component usage, area cost, and energy consumption. We devise a butterfly-style photonic-electronic neural chip to implement our OSNN with up to 7x fewer trainable optical components compared to GEMM-based ONNs. Additionally, a hardware-aware training framework is provided to minimize the required device programming precision, lessen the chip area, and boost the noise robustness. We experimentally demonstrate the utility of our neural chip in practical image recognition tasks, showing that a measured accuracy of 94.16% can be achieved in hand-written digit recognition tasks with 3-bit weight programming precision.

</details>

<details>

<summary>2022-07-17 10:53:46 - An Empirical Study of Automated Unit Test Generation for Python</summary>

- *Stephan Lukasczyk, Florian KroiÃ, Gordon Fraser*

- `2111.05003v2` - [abs](http://arxiv.org/abs/2111.05003v2) - [pdf](http://arxiv.org/pdf/2111.05003v2)

> Various mature automated test generation tools exist for statically typed programming languages such as Java. Automatically generating unit tests for dynamically typed programming languages such as Python, however, is substantially more difficult due to the dynamic nature of these languages as well as the lack of type information. Our Pynguin framework provides automated unit test generation for Python. In this paper, we extend our previous work on Pynguin to support more aspects of the Python language, and by studying a larger variety of well-established state of the art test-generation algorithms, namely DynaMOSA, MIO, and MOSA. Furthermore, we improved our Pynguin tool to generate regression assertions, whose quality we also evaluate. Our experiments confirm that evolutionary algorithms can outperform random test generation also in the context of Python, and similar to the Java world, DynaMOSA yields the highest coverage results. However, our results also demonstrate that there are still fundamental remaining issues, such as inferring type information for code without this information, currently limiting the effectiveness of test generation for Python.

</details>

<details>

<summary>2022-07-17 11:19:44 - An Automated Testing and Debugging Toolkit for Gate-Level Logic Synthesis Applications</summary>

- *Siang-Yun Lee, Heinz Riener, Giovanni De Micheli*

- `2207.13487v1` - [abs](http://arxiv.org/abs/2207.13487v1) - [pdf](http://arxiv.org/pdf/2207.13487v1)

> Correctness and robustness are essential for logic synthesis applications, but they are often only tested with a limited set of benchmarks. Moreover, when the application fails on a large benchmark, the debugging process may be tedious and time-consuming. In some fields such as compiler construction, automatic testing and debugging tools are well-developed to support developers and provide minimal guarantees on program quality. In this paper, we adapt fuzz testing and delta debugging techniques and specialize them for gate-level netlists commonly used in logic synthesis. Our toolkit improves over similar tools specialized for the AIGER format by supporting other gate-level netlist formats and by allowing a tight integration to provide 10x speed-up. Experimental results show that our fuzzer captures defects in mockturtle, ABC, and LSOracle with 10x smaller testcases and our testcase minimizer extracts minimal failure-inducing cores using 2x fewer oracle calls.

</details>

<details>

<summary>2022-07-17 12:42:24 - Automated Repair of Neural Networks</summary>

- *Dor Cohen, Ofer Strichman*

- `2207.08157v1` - [abs](http://arxiv.org/abs/2207.08157v1) - [pdf](http://arxiv.org/pdf/2207.08157v1)

> Over the last decade, Neural Networks (NNs) have been widely used in numerous applications including safety-critical ones such as autonomous systems. Despite their emerging adoption, it is well known that NNs are susceptible to Adversarial Attacks. Hence, it is highly important to provide guarantees that such systems work correctly. To remedy these issues we introduce a framework for repairing unsafe NNs w.r.t. safety specification, that is by utilizing satisfiability modulo theories (SMT) solvers. Our method is able to search for a new, safe NN representation, by modifying only a few of its weight values. In addition, our technique attempts to maximize the similarity to original network with regard to its decision boundaries. We perform extensive experiments which demonstrate the capability of our proposed framework to yield safe NNs w.r.t. the Adversarial Robustness property, with only a mild loss of accuracy (in terms of similarity). Moreover, we compare our method with a naive baseline to empirically prove its effectiveness. To conclude, we provide an algorithm to automatically repair NNs given safety properties, and suggest a few heuristics to improve its computational performance. Currently, by following this approach we are capable of producing small-sized (i.e., with up to few hundreds of parameters) correct NNs, composed of the piecewise linear ReLU activation function. Nevertheless, our framework is general in the sense that it can synthesize NNs w.r.t. any decidable fragment of first-order logic specification.

</details>

<details>

<summary>2022-07-18 01:32:16 - Extracting Densest Sub-hypergraph with Convex Edge-weight Functions</summary>

- *Yi Zhou, Shan Hu, Zimo Sheng*

- `2207.08340v1` - [abs](http://arxiv.org/abs/2207.08340v1) - [pdf](http://arxiv.org/pdf/2207.08340v1)

> The densest subgraph problem (DSG) aiming at finding an induced subgraph such that the average edge-weights of the subgraph is maximized, is a well-studied problem. However, when the input graph is a hypergraph, the existing notion of DSG fails to capture the fact that a hyperedge partially belonging to an induced sub-hypergraph is also a part of the sub-hypergraph. To resolve the issue, we suggest a function $f_e:\mathbb{Z}_{\ge0}\rightarrow \mathbb{R}_{\ge 0}$ to represent the partial edge-weight of a hyperedge $e$ in the input hypergraph $\mathcal{H}=(V,\mathcal{E},f)$ and formulate a generalized densest sub-hypergraph problem (GDSH) as $\max_{S\subseteq V}\frac{\sum_{e\in \mathcal{E}}{f_e(|e\cap S|)}}{|S|}$. We demonstrate that, when all the edge-weight functions are non-decreasing convex, GDSH can be solved in polynomial-time by the linear program-based algorithm, the network flow-based algorithm and the greedy $\frac{1}{r}$-approximation algorithm where $r$ is the rank of the input hypergraph. Finally, we investigate the computational tractability of GDSH where some edge-weight functions are non-convex.

</details>

<details>

<summary>2022-07-18 03:26:41 - CausNet : Generational orderings based search for optimal Bayesian networks via dynamic programming with parent set constraints</summary>

- *Nand Sharma, Joshua Millstein*

- `2207.08365v1` - [abs](http://arxiv.org/abs/2207.08365v1) - [pdf](http://arxiv.org/pdf/2207.08365v1)

> Finding a globally optimal Bayesian Network using exhaustive search is a problem with super-exponential complexity, which severely restricts the number of variables that it can work for. We implement a dynamic programming based algorithm with built-in dimensionality reduction and parent set identification. This reduces the search space drastically and can be applied to large-dimensional data. We use what we call generational orderings based search for optimal networks, which is a novel way to efficiently search the space of possible networks given the possible parent sets. The algorithm supports both continuous and categorical data, and categorical as well as survival outcomes. We demonstrate the efficacy of our algorithm on both synthetic and real data. In simulations, our algorithm performs better than three state-of-art algorithms that are currently used extensively. We then apply it to an Ovarian Cancer gene expression dataset with 513 genes and a survival outcome. Our algorithm is able to find an optimal network describing the disease pathway consisting of 6 genes leading to the outcome node in a few minutes on a basic computer. Our generational orderings based search for optimal networks, is both efficient and highly scalable approach to finding optimal Bayesian Networks, that can be applied to 1000s of variables. Using specifiable parameters - correlation, FDR cutoffs, and in-degree - one can increase or decrease the number of nodes and density of the networks. Availability of two scoring option-BIC and Bge-and implementation of survival outcomes and mixed data types makes our algorithm very suitable for many types of high dimensional biomedical data to find disease pathways.

</details>

<details>

<summary>2022-07-18 07:11:28 - FewGAN: Generating from the Joint Distribution of a Few Images</summary>

- *Lior Ben-Moshe, Sagie Benaim, Lior Wolf*

- `2207.11226v1` - [abs](http://arxiv.org/abs/2207.11226v1) - [pdf](http://arxiv.org/pdf/2207.11226v1)

> We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.

</details>

<details>

<summary>2022-07-18 09:33:04 - What does Transformer learn about source code?</summary>

- *Kechi Zhang, Ge Li, Zhi Jin*

- `2207.08466v1` - [abs](http://arxiv.org/abs/2207.08466v1) - [pdf](http://arxiv.org/pdf/2207.08466v1)

> In the field of source code processing, the transformer-based representation models have shown great powerfulness and have achieved state-of-the-art (SOTA) performance in many tasks. Although the transformer models process the sequential source code, pieces of evidence show that they may capture the structural information (\eg, in the syntax tree, data flow, control flow, \etc) as well. We propose the aggregated attention score, a method to investigate the structural information learned by the transformer. We also put forward the aggregated attention graph, a new way to extract program graphs from the pre-trained models automatically. We measure our methods from multiple perspectives. Furthermore, based on our empirical findings, we use the automatically extracted graphs to replace those ingenious manual designed graphs in the Variable Misuse task. Experimental results show that the semantic graphs we extracted automatically are greatly meaningful and effective, which provide a new perspective for us to understand and use the information contained in the model.

</details>

<details>

<summary>2022-07-18 09:54:10 - Knights and Gold Stars: A Tale of InnerSource Incentivization</summary>

- *Tapajit Dey, Willem Jiang, Brian Fitzgerald*

- `2207.08475v1` - [abs](http://arxiv.org/abs/2207.08475v1) - [pdf](http://arxiv.org/pdf/2207.08475v1)

> Given the success of the open source phenomenon, it is not surprising that many organizations are seeking to emulate this success by adopting open source practices internally in what is termed InnerSource. However, while open source development and InnerSource are similar in some aspects, they differ significantly on others, and thus need to be implemented and managed differently. To the best of our knowledge, there is no significant account of a successful InnerSource incentivization program. Here we describe a comprehensive InnerSource incentivization program that was implemented at Huawei. The program is based on theories of motivation, both intrinsic and extrinsic, and also includes incentives at the individual, project, and divisional level, which helps to overcome the barriers that arise when implementing InnerSource. The program has had very impressive early results, leading to significant increases in the number of InnerSource projects, contributors, departments, and lines of code contributed.

</details>

<details>

<summary>2022-07-18 11:45:55 - A Variant of Concurrent Constraint Programming on GPU</summary>

- *Pierre Talbot, FrÃ©dÃ©ric Pinel, Pascal Bouvry*

- `2207.12116v1` - [abs](http://arxiv.org/abs/2207.12116v1) - [pdf](http://arxiv.org/pdf/2207.12116v1)

> The number of cores on graphical computing units (GPUs) is reaching thousands nowadays, whereas the clock speed of processors stagnates. Unfortunately, constraint programming solvers do not take advantage yet of GPU parallelism. One reason is that constraint solvers were primarily designed within the mental frame of sequential computation. To solve this issue, we take a step back and contribute to a simple, intrinsically parallel, lock-free and formally correct programming language based on concurrent constraint programming. We then re-examine parallel constraint solving on GPUs within this formalism, and develop Turbo, a simple constraint solver entirely programmed on GPUs. Turbo validates the correctness of our approach and compares positively to a parallel CPU-based solver.

</details>

<details>

<summary>2022-07-18 12:23:51 - Authentication Attacks on Projection-based Cancelable Biometric Schemes</summary>

- *Axel Durbet, Pascal Lafourcade, Denis Migdal, Kevin Thiry-Atighehchi, Paul-Marie Grollemund*

- `2110.15163v6` - [abs](http://arxiv.org/abs/2110.15163v6) - [pdf](http://arxiv.org/pdf/2110.15163v6)

> Cancelable biometric schemes aim at generating secure biometric templates by combining user specific tokens, such as password, stored secret or salt, along with biometric data. This type of transformation is constructed as a composition of a biometric transformation with a feature extraction algorithm. The security requirements of cancelable biometric schemes concern the irreversibility, unlinkability and revocability of templates, without losing in accuracy of comparison. While several schemes were recently attacked regarding these requirements, full reversibility of such a composition in order to produce colliding biometric characteristics, and specifically presentation attacks, were never demonstrated to the best of our knowledge. In this paper, we formalize these attacks for a traditional cancelable scheme with the help of integer linear programming (ILP) and quadratically constrained quadratic programming (QCQP). Solving these optimization problems allows an adversary to slightly alter its fingerprint image in order to impersonate any individual. Moreover, in an even more severe scenario, it is possible to simultaneously impersonate several individuals.

</details>

<details>

<summary>2022-07-18 12:30:24 - A Certifiable Security Patch for Object Tracking in Self-Driving Systems via Historical Deviation Modeling</summary>

- *Xudong Pan, Qifan Xiao, Mi Zhang, Min Yang*

- `2207.08556v1` - [abs](http://arxiv.org/abs/2207.08556v1) - [pdf](http://arxiv.org/pdf/2207.08556v1)

> Self-driving cars (SDC) commonly implement the perception pipeline to detect the surrounding obstacles and track their moving trajectories, which lays the ground for the subsequent driving decision making process. Although the security of obstacle detection in SDC is intensively studied, not until very recently the attackers start to exploit the vulnerability of the tracking module. Compared with solely attacking the object detectors, this new attack strategy influences the driving decision more effectively with less attack budgets. However, little is known on whether the revealed vulnerability remains effective in end-to-end self-driving systems and, if so, how to mitigate the threat.   In this paper, we present the first systematic research on the security of object tracking in SDC. Through a comprehensive case study on the full perception pipeline of a popular open-sourced self-driving system, Baidu's Apollo, we prove the mainstream multi-object tracker (MOT) based on Kalman Filter (KF) is unsafe even with an enabled multi-sensor fusion mechanism. Our root cause analysis reveals, the vulnerability is innate to the design of KF-based MOT, which shall error-handle the prediction results from the object detectors yet the adopted KF algorithm is prone to trust the observation more when its deviation from the prediction is larger. To address this design flaw, we propose a simple yet effective security patch for KF-based MOT, the core of which is an adaptive strategy to balance the focus of KF on observations and predictions according to the anomaly index of the observation-prediction deviation, and has certified effectiveness against a generalized hijacking attack model. Extensive evaluation on $4$ KF-based existing MOT implementations (including 2D and 3D, academic and Apollo ones) validate the defense effectiveness and the trivial performance overhead of our approach.

</details>

<details>

<summary>2022-07-18 13:17:13 - Positive Dependency Graphs Revisited</summary>

- *Jorge Fandinno, Vladimir Lifschitz*

- `2207.08579v1` - [abs](http://arxiv.org/abs/2207.08579v1) - [pdf](http://arxiv.org/pdf/2207.08579v1)

> Theory of stable models is the mathematical basis of answer set programming. Several results in that theory refer to the concept of the positive dependency graph of a logic program. We describe a modification of that concept and show that the new understanding of positive dependency makes it possible to strengthen some of these results. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2022-07-18 13:38:12 - Applying Incremental Answer Set Solving to Product Configuration</summary>

- *Richard Comploi-Taupe, Giulia Francescutto, Gottfried Schenner*

- `2207.08599v1` - [abs](http://arxiv.org/abs/2207.08599v1) - [pdf](http://arxiv.org/pdf/2207.08599v1)

> In this paper, we apply incremental answer set solving to product configuration. Incremental answer set solving is a step-wise incremental approach to Answer Set Programming (ASP). We demonstrate how to use this technique to solve product configurations problems incrementally. Every step of the incremental solving process corresponds to a predefined configuration action. Using complex domain-specific configuration actions makes it possible to tightly control the level of non-determinism and performance of the solving process. We show applications of this technique for reasoning about product configuration, like simulating the behavior of a deterministic configuration algorithm and describing user actions.

</details>

<details>

<summary>2022-07-18 15:49:19 - Physarum Inspired Dynamics to Solve Semi-Definite Programs</summary>

- *Yuan Gao, Hamidreza Kamkari, Andreas Karrenbauer, Kurt Mehlhorn, Mohammadamin Sharifi*

- `2111.02291v3` - [abs](http://arxiv.org/abs/2111.02291v3) - [pdf](http://arxiv.org/pdf/2111.02291v3)

> Physarum Polycephalum is a slime mold that can solve shortest path problems. A mathematical model based on Physarum's behavior, known as the Physarum Directed Dynamics, can solve positive linear programs. In this paper, we present a family of Physarum-based dynamics extending the previous work and introduce a new algorithm to solve positive Semi-Definite Programs (SDP). The Physarum dynamics are governed by orthogonal projections (w.r.t. time-dependent scalar products) on the affine subspace defined by the linear constraints. We present a natural generalization of the scalar products used in the LP case to the matrix space for SDPs, which boils down to the linear case when all matrices in the SDP are diagonal, thus, representing an LP. We investigate the behavior of the induced dynamics theoretically and experimentally, highlight challenges arising from the non-commutative nature of matrix products, and prove soundness and convergence under mild conditions. Moreover, we consider a more abstract view on the dynamics that suggests a slight variation to guarantee unconditional soundness and convergence-to-optimality. By simulating these dynamics using suitable discretizations, one obtains numerical algorithms for solving positive SDPs, which have applications in discrete optimization, e.g., for computing the Goemans-Williamson approximation for MaxCut or the Lovasz theta number for determining the clique/chromatic number in perfect graphs.

</details>

<details>

<summary>2022-07-18 18:41:36 - Prior Knowledge Guided Unsupervised Domain Adaptation</summary>

- *Tao Sun, Cheng Lu, Haibin Ling*

- `2207.08877v1` - [abs](http://arxiv.org/abs/2207.08877v1) - [pdf](http://arxiv.org/pdf/2207.08877v1)

> The waive of labels in the target domain makes Unsupervised Domain Adaptation (UDA) an attractive technique in many real-world applications, though it also brings great challenges as model adaptation becomes harder without labeled target data. In this paper, we address this issue by seeking compensation from target domain prior knowledge, which is often (partially) available in practice, e.g., from human expertise. This leads to a novel yet practical setting where in addition to the training data, some prior knowledge about the target class distribution are available. We term the setting as Knowledge-guided Unsupervised Domain Adaptation (KUDA). In particular, we consider two specific types of prior knowledge about the class distribution in the target domain: Unary Bound that describes the lower and upper bounds of individual class probabilities, and Binary Relationship that describes the relations between two class probabilities. We propose a general rectification module that uses such prior knowledge to refine model generated pseudo labels. The module is formulated as a Zero-One Programming problem derived from the prior knowledge and a smooth regularizer. It can be easily plugged into self-training based UDA methods, and we combine it with two state-of-the-art methods, SHOT and DINE. Empirical results on four benchmarks confirm that the rectification module clearly improves the quality of pseudo labels, which in turn benefits the self-training stage. With the guidance from prior knowledge, the performances of both methods are substantially boosted. We expect our work to inspire further investigations in integrating prior knowledge in UDA. Code is available at https://github.com/tsun/KUDA.

</details>

<details>

<summary>2022-07-18 21:48:58 - Recursive McCormick Linearization of Multilinear Programs</summary>

- *Arvind U Raghunathan, Carlos Cardonha, David Bergman, Carlos J Nohra*

- `2207.08955v1` - [abs](http://arxiv.org/abs/2207.08955v1) - [pdf](http://arxiv.org/pdf/2207.08955v1)

> Linear programming (LP) relaxations are widely employed in exact solution methods for multilinear programs (MLP). One example is the family of Recursive McCormick Linearization (RML) strategies, where bilinear products are substituted for artificial variables, which deliver a relaxation of the original problem when introduced together with concave and convex envelopes. In this article, we introduce the first systematic approach for identifying RMLs, in which we focus on the identification of linear relaxation with a small number of artificial variables and with strong LP bounds. We present a novel mechanism for representing all the possible RMLs, which we use to design an exact mixed-integer programming (MIP) formulation for the identification of minimum-size RMLs; we show that this problem is NP-hard in general, whereas a special case is fixed-parameter tractable. Moreover, we explore structural properties of our formulation to derive an exact MIP model that identifies RMLs of a given size with the best possible relaxation bound is optimal. Our numerical results on a collection of benchmarks indicate that our algorithms outperform the RML strategy implemented in state-of-the-art global optimization solvers.

</details>

<details>

<summary>2022-07-18 23:53:55 - Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation</summary>

- *Tomas Cabezon Pedroso, Javier Del Ser, Natalia Diaz-RodrÄ±guez*

- `2207.08989v1` - [abs](http://arxiv.org/abs/2207.08989v1) - [pdf](http://arxiv.org/pdf/2207.08989v1)

> Rendering programs have changed the design process completely as they permit to see how the products will look before they are fabricated. However, the rendering process is complicated and takes a significant amount of time, not only in the rendering itself but in the setting of the scene as well. Materials, lights and cameras need to be set in order to get the best quality results. Nevertheless, the optimal output may not be obtained in the first render. This all makes the rendering process a tedious process. Since Goodfellow et al. introduced Generative Adversarial Networks (GANs) in 2014 [1], they have been used to generate computer-assigned synthetic data, from non-existing human faces to medical data analysis or image style transfer. GANs have been used to transfer image textures from one domain to another. However, paired data from both domains was needed. When Zhu et al. introduced the CycleGAN model, the elimination of this expensive constraint permitted transforming one image from one domain into another, without the need for paired data. This work validates the applicability of CycleGANs on style transfer from an initial sketch to a final render in 2D that represents a 3D design, a step that is paramount in every product design process. We inquiry the possibilities of including CycleGANs as part of the design pipeline, more precisely, applied to the rendering of ring designs. Our contribution entails a crucial part of the process as it allows the customer to see the final product before buying. This work sets a basis for future research, showing the possibilities of GANs in design and establishing a starting point for novel applications to approach crafts design.

</details>

<details>

<summary>2022-07-19 01:56:28 - Enhancing Security Patch Identification by Capturing Structures in Commits</summary>

- *Bozhi Wu, Shangqing Liu, Ruitao Feng, Xiaofei Xie, Jingkai Siow, Shang-Wei Lin*

- `2207.09022v1` - [abs](http://arxiv.org/abs/2207.09022v1) - [pdf](http://arxiv.org/pdf/2207.09022v1)

> With the rapid increasing number of open source software (OSS), the majority of the software vulnerabilities in the open source components are fixed silently, which leads to the deployed software that integrated them being unable to get a timely update. Hence, it is critical to design a security patch identification system to ensure the security of the utilized software. However, most of the existing works for security patch identification just consider the changed code and the commit message of a commit as a flat sequence of tokens with simple neural networks to learn its semantics, while the structure information is ignored. To address these limitations, in this paper, we propose our well-designed approach E-SPI, which extracts the structure information hidden in a commit for effective identification. Specifically, it consists of the code change encoder to extract the syntactic of the changed code with the BiLSTM to learn the code representation and the message encoder to construct the dependency graph for the commit message with the graph neural network (GNN) to learn the message representation. We further enhance the code change encoder by embedding contextual information related to the changed code. To demonstrate the effectiveness of our approach, we conduct the extensive experiments against six state-of-the-art approaches on the existing dataset and from the real deployment environment. The experimental results confirm that our approach can significantly outperform current state-of-the-art baselines.

</details>

<details>

<summary>2022-07-19 03:02:49 - A Comprehensive Empirical Investigation on Failure Clustering in Parallel Debugging</summary>

- *Yi Song, Xiaoyuan Xie, Quanming Liu, Xihao Zhang, Xi Wu*

- `2207.07992v2` - [abs](http://arxiv.org/abs/2207.07992v2) - [pdf](http://arxiv.org/pdf/2207.07992v2)

> The clustering technique has attracted a lot of attention as a promising strategy for parallel debugging in multi-fault scenarios, this heuristic approach (i.e., failure indexing or fault isolation) enables developers to perform multiple debugging tasks simultaneously through dividing failed test cases into several disjoint groups. When using statement ranking representation to model failures for better clustering, several factors influence clustering effectiveness, including the risk evaluation formula (REF), the number of faults (NOF), the fault type (FT), and the number of successful test cases paired with one individual failed test case (NSP1F). In this paper, we present the first comprehensive empirical study of how these four factors influence clustering effectiveness. We conduct extensive controlled experiments on 1060 faulty versions of 228 simulated faults and 141 real faults, and the results reveal that: 1) GP19 is highly competitive across all REFs, 2) clustering effectiveness decreases as NOF increases, 3) higher clustering effectiveness is easier to achieve when a program contains only predicate faults, and 4) clustering effectiveness remains when the scale of NSP1F is reduced to 20%.

</details>

<details>

<summary>2022-07-19 04:35:38 - Automated Black-Box Boundary Value Detection</summary>

- *Felix Dobslaw, Robert Feldt, Francisco de Oliveira Neto*

- `2207.09065v1` - [abs](http://arxiv.org/abs/2207.09065v1) - [pdf](http://arxiv.org/pdf/2207.09065v1)

> The input domain of software systems can typically be divided into sub-domains for which the outputs are similar. To ensure high quality it is critical to test the software on the boundaries between these sub-domains. Consequently, boundary value analysis and testing has been part of the toolbox of software testers for long and is typically taught early to students. However, despite its many argued benefits, boundary value analysis for a given specification or piece of software is typically described in abstract terms which allow for variation in how testers apply it.   Here we propose an automated, black-box boundary value detection method to support software testers in systematic boundary value analysis with consistent results. The method builds on a metric to quantify the level of boundariness of test inputs: the program derivative. By coupling it with search algorithms we find and rank pairs of inputs as good boundary candidates, i.e. inputs close together but with outputs far apart. We implement our AutoBVA approach and evaluate it on a curated dataset of example programs. Our results indicate that even with a simple and generic program derivative variant in combination with broad sampling over the input space, interesting boundary candidates can be identified.

</details>

<details>

<summary>2022-07-19 04:52:24 - Patch-level Representation Learning for Self-supervised Vision Transformers</summary>

- *Sukmin Yun, Hankook Lee, Jaehyung Kim, Jinwoo Shin*

- `2206.07990v3` - [abs](http://arxiv.org/abs/2206.07990v3) - [pdf](http://arxiv.org/pdf/2206.07990v3)

> Recent self-supervised learning (SSL) methods have shown impressive results in learning visual representations from unlabeled images. This paper aims to improve their performance further by utilizing the architectural advantages of the underlying neural network, as the current state-of-the-art visual pretext tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic. In particular, we focus on Vision Transformers (ViTs), which have gained much attention recently as a better architectural choice, often outperforming convolutional networks for various visual tasks. The unique characteristic of ViT is that it takes a sequence of disjoint patches from an image and processes patch-level representations internally. Inspired by this, we design a simple yet effective visual pretext task, coined SelfPatch, for learning better patch-level representations. To be specific, we enforce invariance against each patch and its neighbors, i.e., each patch treats similar neighboring patches as positive samples. Consequently, training ViTs with SelfPatch learns more semantically meaningful relations among patches (without using human-annotated labels), which can be beneficial, in particular, to downstream tasks of a dense prediction type. Despite its simplicity, we demonstrate that it can significantly improve the performance of existing SSL methods for various visual tasks, including object detection and semantic segmentation. Specifically, SelfPatch significantly improves the recent self-supervised ViT, DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance segmentation, and +2.9 mIoU on ADE20K semantic segmentation.

</details>

<details>

<summary>2022-07-19 06:41:15 - LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</summary>

- *Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei*

- `2204.08387v3` - [abs](http://arxiv.org/abs/2204.08387v3) - [pdf](http://arxiv.org/pdf/2204.08387v3)

> Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose \textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at \url{https://aka.ms/layoutlmv3}.

</details>

<details>

<summary>2022-07-19 07:25:43 - MONet: Multi-scale Overlap Network for Duplication Detection in Biomedical Images</summary>

- *Ekraam Sabir, Soumyaroop Nandi, Wael AbdAlmageed, Prem Natarajan*

- `2207.09107v1` - [abs](http://arxiv.org/abs/2207.09107v1) - [pdf](http://arxiv.org/pdf/2207.09107v1)

> Manipulation of biomedical images to misrepresent experimental results has plagued the biomedical community for a while. Recent interest in the problem led to the curation of a dataset and associated tasks to promote the development of biomedical forensic methods. Of these, the largest manipulation detection task focuses on the detection of duplicated regions between images. Traditional computer-vision based forensic models trained on natural images are not designed to overcome the challenges presented by biomedical images. We propose a multi-scale overlap detection model to detect duplicated image regions. Our model is structured to find duplication hierarchically, so as to reduce the number of patch operations. It achieves state-of-the-art performance overall and on multiple biomedical image categories.

</details>

<details>

<summary>2022-07-19 13:48:44 - Dynamic programming with incomplete information to overcome navigational uncertainty in a nautical environment</summary>

- *Chris Beeler, Xinkai Li, Colin Bellinger, Mark Crowley, Maia Fraser, Isaac Tamblyn*

- `2112.14657v2` - [abs](http://arxiv.org/abs/2112.14657v2) - [pdf](http://arxiv.org/pdf/2112.14657v2)

> Using a novel toy nautical navigation environment, we show that dynamic programming can be used when only incomplete information about a partially observed Markov decision process (POMDP) is known. By incorporating uncertainty into our model, we show that navigation policies can be constructed that maintain safety, outperforming the baseline performance of traditional dynamic programming for Markov decision processes (MDPs). Adding in controlled sensing methods, we show that these policies can also lower measurement costs at the same time.

</details>

<details>

<summary>2022-07-19 14:28:36 - Digital Therapeutics for Mental Health: Is Attrition the Achilles Heel?</summary>

- *Adaora Nwosu, Samantha Boardman, Mustafa M. Husain, P. Murali Doraiswamy*

- `2207.05179v2` - [abs](http://arxiv.org/abs/2207.05179v2) - [pdf](http://arxiv.org/pdf/2207.05179v2)

> Digit therapeutics are novel software devices that clinicians may utilize in delivering quality mental health care and ensuring positive outcomes. However, uptake of digital therapeutics and clinically tested software-based programs remains low. This article presents possible reasons for attrition and low engagement in clinical studies investigating digital therapeutics, analyses of studies in which engagement was high, and design constructs that may encourage user engagement. The aim is to shed light on the importance of real-world attrition data of digital therapeutics, and important characteristics of medical devices that have positively influenced user engagement. The findings presented in this article will be useful to relevant stakeholders and medical device experts tasked with addressing the gap between software medical design and user engagement present in digital therapeutic clinical trials.

</details>

<details>

<summary>2022-07-19 15:07:17 - MLGOPerf: An ML Guided Inliner to Optimize Performance</summary>

- *Amir H. Ashouri, Mostafa Elhoushi, Yuzhe Hua, Xiang Wang, Muhammad Asif Manzoor, Bryan Chan, Yaoqing Gao*

- `2207.08389v2` - [abs](http://arxiv.org/abs/2207.08389v2) - [pdf](http://arxiv.org/pdf/2207.08389v2)

> For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning.   This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.

</details>

<details>

<summary>2022-07-19 15:27:10 - Multi-parametric Analysis for Mixed Integer Linear Programming: An Application to Transmission Planning and Congestion Control</summary>

- *Jian Liu, Rui Bo, Siyuan Wang*

- `2207.09325v1` - [abs](http://arxiv.org/abs/2207.09325v1) - [pdf](http://arxiv.org/pdf/2207.09325v1)

> Enhancing existing transmission lines is a useful tool to combat transmission congestion and guarantee transmission security with increasing demand and boosting the renewable energy source. This study concerns the selection of lines whose capacity should be expanded and by how much from the perspective of independent system operator (ISO) to minimize the system cost with the consideration of transmission line constraints and electricity generation and demand balance conditions, and incorporating ramp-up and startup ramp rates, shutdown ramp rates, ramp-down rate limits and minimum up and minimum down times. For that purpose, we develop the ISO unit commitment and economic dispatch model and show it as a right-hand side uncertainty multiple parametric analysis for the mixed integer linear programming (MILP) problem. We first relax the binary variable to continuous variables and employ the Lagrange method and Karush-Kuhn-Tucker conditions to obtain optimal solutions (optimal decision variables and objective function) and critical regions associated with active and inactive constraints. Further, we extend the traditional branch and bound method for the large-scale MILP problem by determining the upper bound of the problem at each node, then comparing the difference between the upper and lower bounds and reaching the approximate optimal solution within the decision makers' tolerated error range. In additional, the objective function's first derivative on the parameters of each line is used to inform the selection of lines to ease congestion and maximize social welfare. Finally, the amount of capacity upgrade will be chosen by balancing the cost-reduction rate of the objective function on parameters and the cost of the line upgrade. Our findings are supported by numerical simulation and provide transmission line planners with decision-making guidance.

</details>

<details>

<summary>2022-07-19 18:44:56 - TestSelector: Automatic Test Suite Selection for Student Projects -- Extended Version</summary>

- *Filipe Marques, AntÃ³nio Morgado, JosÃ© Fragoso Santos, MikolÃ¡Å¡ Janota*

- `2207.09509v1` - [abs](http://arxiv.org/abs/2207.09509v1) - [pdf](http://arxiv.org/pdf/2207.09509v1)

> Computer Science course instructors routinely have to create comprehensive test suites to assess programming assignments. The creation of such test suites is typically not trivial as it involves selecting a limited number of tests from a set of (semi-)randomly generated ones. Manual strategies for test selection do not scale when considering large testing inputs needed, for instance, for the assessment of algorithms exercises. To facilitate this process, we present TestSelector, a new framework for automatic selection of optimal test suites for student projects. The key advantage of TestSelector over existing approaches is that it is easily extensible with arbitrarily complex code coverage measures, not requiring these measures to be encoded into the logic of an exact constraint solver. We demonstrate the flexibility of TestSelector by extending it with support for a range of classical code coverage measures and using it to select test suites for a number of real-world algorithms projects, further showing that the selected test suites outperform randomly selected ones in finding bugs in students' code.

</details>

<details>

<summary>2022-07-19 19:10:25 - QuerTCI: A Tool Integrating GitHub Issue Querying with Comment Classification</summary>

- *Ye Paing, Tatiana Castro VÃ©lez, Raffi Khatchadourian*

- `2202.08761v2` - [abs](http://arxiv.org/abs/2202.08761v2) - [pdf](http://arxiv.org/pdf/2202.08761v2)

> Empirical Software Engineering (ESE) researchers study (open-source) project issues and the comments and threads within to discover -- among others -- challenges developers face when incorporating new technologies, platforms, and programming language constructs. However, such threads accumulate, becoming unwieldy and hindering any insight researchers may gain. While existing approaches alleviate this burden by classifying issue thread comments, there is a gap between searching popular open-source software repositories (e.g., those on GitHub) for issues containing particular keywords and feeding the results into a classification model. This paper demonstrates a research infrastructure tool called QuerTCI that bridges this gap by integrating the GitHub issue comment search API with the classification models found in existing approaches. Using queries, ESE researchers can retrieve GitHub issues containing particular keywords, e.g., those related to a specific programming language construct, and, subsequently, classify the discussions occurring in those issues. We hope ESE researchers can use our tool to uncover challenges related to particular technologies using specific keywords through popular open-source repositories more seamlessly than previously possible. A tool demonstration video may be found at: https://youtu.be/fADKSxn0QUk.

</details>

<details>

<summary>2022-07-20 05:45:36 - ExoSGAN and ExoACGAN: Exoplanet Detection using Adversarial Training Algorithms</summary>

- *Cicy K Agnes, Akthar Naveed V, Anitha Mary M O Chacko*

- `2207.09665v1` - [abs](http://arxiv.org/abs/2207.09665v1) - [pdf](http://arxiv.org/pdf/2207.09665v1)

> Exoplanet detection opens the door to the discovery of new habitable worlds and helps us understand how planets were formed. With the objective of finding earth-like habitable planets, NASA launched Kepler space telescope and its follow up mission K2. The advancement of observation capabilities has increased the range of fresh data available for research, and manually handling them is both time-consuming and difficult. Machine learning and deep learning techniques can greatly assist in lowering human efforts to process the vast array of data produced by the modern instruments of these exoplanet programs in an economical and unbiased manner. However, care should be taken to detect all the exoplanets precisely while simultaneously minimizing the misclassification of non-exoplanet stars. In this paper, we utilize two variations of generative adversarial networks, namely semi-supervised generative adversarial networks and auxiliary classifier generative adversarial networks, to detect transiting exoplanets in K2 data. We find that the usage of these models can be helpful for the classification of stars with exoplanets. Both of our techniques are able to categorize the light curves with a recall and precision of 1.00 on the test data. Our semi-supervised technique is beneficial to solve the cumbersome task of creating a labeled dataset.

</details>

<details>

<summary>2022-07-20 07:27:47 - Towards Plug'n Play Task-Level Autonomy for Robotics Using POMDPs and Generative Models</summary>

- *Or Wertheim, Dan R. Suissa, Ronen I. Brafman*

- `2207.09713v1` - [abs](http://arxiv.org/abs/2207.09713v1) - [pdf](http://arxiv.org/pdf/2207.09713v1)

> To enable robots to achieve high level objectives, engineers typically write scripts that apply existing specialized skills, such as navigation, object detection and manipulation to achieve these goals. Writing good scripts is challenging since they must intelligently balance the inherent stochasticity of a physical robot's actions and sensors, and the limited information it has. In principle, AI planning can be used to address this challenge and generate good behavior policies automatically. But this requires passing three hurdles. First, the AI must understand each skill's impact on the world. Second, we must bridge the gap between the more abstract level at which we understand what a skill does and the low-level state variables used within its code. Third, much integration effort is required to tie together all components. We describe an approach for integrating robot skills into a working autonomous robot controller that schedules its skills to achieve a specified task and carries four key advantages. 1) Our Generative Skill Documentation Language (GSDL) makes code documentation simpler, compact, and more expressive using ideas from probabilistic programming languages. 2) An expressive abstraction mapping (AM) bridges the gap between low-level robot code and the abstract AI planning model. 3) Any properly documented skill can be used by the controller without any additional programming effort, providing a Plug'n Play experience. 4) A POMDP solver schedules skill execution while properly balancing partial observability, stochastic behavior, and noisy sensing.

</details>

<details>

<summary>2022-07-20 10:26:06 - ManiFest: Manifold Deformation for Few-shot Image Translation</summary>

- *Fabio Pizzati, Jean-FranÃ§ois Lalonde, Raoul de Charette*

- `2111.13681v3` - [abs](http://arxiv.org/abs/2111.13681v3) - [pdf](http://arxiv.org/pdf/2111.13681v3)

> Most image-to-image translation methods require a large number of training images, which restricts their applicability. We instead propose ManiFest: a framework for few-shot image translation that learns a context-aware representation of a target domain from a few images only. To enforce feature consistency, our framework learns a style manifold between source and proxy anchor domains (assumed to be composed of large numbers of images). The learned manifold is interpolated and deformed towards the few-shot target domain via patch-based adversarial and feature statistics alignment losses. All of these components are trained simultaneously during a single end-to-end loop. In addition to the general few-shot translation task, our approach can alternatively be conditioned on a single exemplar image to reproduce its specific style. Extensive experiments demonstrate the efficacy of ManiFest on multiple tasks, outperforming the state-of-the-art on all metrics and in both the general- and exemplar-based scenarios. Our code is available at https://github.com/cv-rits/Manifest .

</details>

<details>

<summary>2022-07-20 13:26:30 - ESBMC-Jimple: Verifying Kotlin Programs via Jimple Intermediate Representation</summary>

- *Rafael Menezes, Daniel Moura, Helena Cavalcante, Rosiane de Freitas, Lucas C. Cordeiro*

- `2206.04397v2` - [abs](http://arxiv.org/abs/2206.04397v2) - [pdf](http://arxiv.org/pdf/2206.04397v2)

> In this work, we describe and evaluate the first model checker for verifying Kotlin programs through the Jimple intermediate representation. The verifier, named ESBMC-Jimple, is built on top of the Efficient SMT-based Context-Bounded Model Checker (ESBMC). It uses the Soot framework to obtain the Jimple IR, representing a simplified version of the Kotlin source code, containing a maximum of three operands per instruction. ESBMC-Jimple processes Kotlin source code together with a model of the standard Kotlin libraries and checks a set of safety properties. Experimental results show that ESBMC-Jimple can correctly verify a set of Kotlin benchmarks from the literature and that it is competitive with state-of-the-art Java bytecode verifiers. A demonstration is available at https://youtu.be/J6WhNfXvJNc.

</details>

<details>

<summary>2022-07-20 15:08:52 - AutoDiCE: Fully Automated Distributed CNN Inference at the Edge</summary>

- *Xiaotian Guo, Andy D. Pimentel, Todor Stefanov*

- `2207.12113v1` - [abs](http://arxiv.org/abs/2207.12113v1) - [pdf](http://arxiv.org/pdf/2207.12113v1)

> Deep Learning approaches based on Convolutional Neural Networks (CNNs) are extensively utilized and very successful in a wide range of application areas, including image classification and speech recognition. For the execution of trained CNNs, i.e. model inference, we nowadays witness a shift from the Cloud to the Edge. Unfortunately, deploying and inferring large, compute and memory intensive CNNs on edge devices is challenging because these devices typically have limited power budgets and compute/memory resources. One approach to address this challenge is to leverage all available resources across multiple edge devices to deploy and execute a large CNN by properly partitioning the CNN and running each CNN partition on a separate edge device. Although such distribution, deployment, and execution of large CNNs on multiple edge devices is a desirable and beneficial approach, there currently does not exist a design and programming framework that takes a trained CNN model, together with a CNN partitioning specification, and fully automates the CNN model splitting and deployment on multiple edge devices to facilitate distributed CNN inference at the Edge. Therefore, in this paper, we propose a novel framework, called AutoDiCE, for automated splitting of a CNN model into a set of sub-models and automated code generation for distributed and collaborative execution of these sub-models on multiple, possibly heterogeneous, edge devices, while supporting the exploitation of parallelism among and within the edge devices. Our experimental results show that AutoDiCE can deliver distributed CNN inference with reduced energy consumption and memory usage per edge device, and improved overall system throughput at the same time.

</details>

<details>

<summary>2022-07-20 18:46:22 - What Made This Test Flake? Pinpointing Classes Responsible for Test Flakiness</summary>

- *Sarra Habchi, Guillaume Haben, Jeongju Sohn, Adriano Franci, Mike Papadakis, Maxime Cordy, Yves Le Traon*

- `2207.10143v1` - [abs](http://arxiv.org/abs/2207.10143v1) - [pdf](http://arxiv.org/pdf/2207.10143v1)

> Flaky tests are defined as tests that manifest non-deterministic behaviour by passing and failing intermittently for the same version of the code. These tests cripple continuous integration with false alerts that waste developers' time and break their trust in regression testing. To mitigate the effects of flakiness, both researchers and industrial experts proposed strategies and tools to detect and isolate flaky tests. However, flaky tests are rarely fixed as developers struggle to localise and understand their causes. Additionally, developers working with large codebases often need to know the sources of non-determinism to preserve code quality, i.e., avoid introducing technical debt linked with non-deterministic behaviour, and to avoid introducing new flaky tests. To aid with these tasks, we propose re-targeting Fault Localisation techniques to the flaky component localisation problem, i.e., pinpointing program classes that cause the non-deterministic behaviour of flaky tests. In particular, we employ Spectrum-Based Fault Localisation (SBFL), a coverage-based fault localisation technique commonly adopted for its simplicity and effectiveness. We also utilise other data sources, such as change history and static code metrics, to further improve the localisation. Our results show that augmenting SBFL with change and code metrics ranks flaky classes in the top-1 and top-5 suggestions, in 26% and 47% of the cases. Overall, we successfully reduced the average number of classes inspected to locate the first flaky class to 19% of the total number of classes covered by flaky tests. Our results also show that localisation methods are effective in major flakiness categories, such as concurrency and asynchronous waits, indicating their general ability to identify flaky components.

</details>

<details>

<summary>2022-07-20 18:57:55 - Inducing Causal Structure for Interpretable Neural Networks</summary>

- *Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D. Goodman, Christopher Potts*

- `2112.00826v2` - [abs](http://arxiv.org/abs/2112.00826v2) - [pdf](http://arxiv.org/pdf/2112.00826v2)

> In many areas, we have well-founded insights about causal structure that would be useful to bring into our trained models while still allowing them to learn in a data-driven fashion. To achieve this, we present the new method of interchange intervention training (IIT). In IIT, we (1) align variables in a causal model (e.g., a deterministic program or Bayesian network) with representations in a neural model and (2) train the neural model to match the counterfactual behavior of the causal model on a base input when aligned representations in both models are set to be the value they would be for a source input. IIT is fully differentiable, flexibly combines with other objectives, and guarantees that the target causal model is a causal abstraction of the neural model when its loss is zero. We evaluate IIT on a structural vision task (MNIST-PVR), a navigational language task (ReaSCAN), and a natural language inference task (MQNLI). We compare IIT against multi-task training objectives and data augmentation. In all our experiments, IIT achieves the best results and produces neural models that are more interpretable in the sense that they more successfully realize the target causal model.

</details>

<details>

<summary>2022-07-20 19:30:22 - Constrained Prescriptive Trees via Column Generation</summary>

- *Shivaram Subramanian, Wei Sun, Youssef Drissi, Markus Ettl*

- `2207.10163v1` - [abs](http://arxiv.org/abs/2207.10163v1) - [pdf](http://arxiv.org/pdf/2207.10163v1)

> With the abundance of available data, many enterprises seek to implement data-driven prescriptive analytics to help them make informed decisions. These prescriptive policies need to satisfy operational constraints, and proactively eliminate rule conflicts, both of which are ubiquitous in practice. It is also desirable for them to be simple and interpretable, so they can be easily verified and implemented. Existing approaches from the literature center around constructing variants of prescriptive decision trees to generate interpretable policies. However, none of the existing methods are able to handle constraints. In this paper, we propose a scalable method that solves the constrained prescriptive policy generation problem. We introduce a novel path-based mixed-integer program (MIP) formulation which identifies a (near) optimal policy efficiently via column generation. The policy generated can be represented as a multiway-split tree which is more interpretable and informative than a binary-split tree due to its shorter rules. We demonstrate the efficacy of our method with extensive experiments on both synthetic and real datasets.

</details>

<details>

<summary>2022-07-20 21:47:15 - On the Robustness of 3D Object Detectors</summary>

- *Fatima Albreiki, Sultan Abughazal, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Fahad Khan*

- `2207.10205v1` - [abs](http://arxiv.org/abs/2207.10205v1) - [pdf](http://arxiv.org/pdf/2207.10205v1)

> In recent years, significant progress has been achieved for 3D object detection on point clouds thanks to the advances in 3D data collection and deep learning techniques. Nevertheless, 3D scenes exhibit a lot of variations and are prone to sensor inaccuracies as well as information loss during pre-processing. Thus, it is crucial to design techniques that are robust against these variations. This requires a detailed analysis and understanding of the effect of such variations. This work aims to analyze and benchmark popular point-based 3D object detectors against several data corruptions. To the best of our knowledge, we are the first to investigate the robustness of point-based 3D object detectors. To this end, we design and evaluate corruptions that involve data addition, reduction, and alteration. We further study the robustness of different modules against local and global variations. Our experimental results reveal several intriguing findings. For instance, we show that methods that integrate Transformers at a patch or object level lead to increased robustness, compared to using Transformers at the point level.

</details>

<details>

<summary>2022-07-21 11:06:42 - The Cavendish Computors: The women working in scientific computing for Radio Astronomy</summary>

- *Verity Allan*

- `2205.07267v2` - [abs](http://arxiv.org/abs/2205.07267v2) - [pdf](http://arxiv.org/pdf/2205.07267v2)

> A discussion of the history of scientific computing for Radio Astronomy in the Cavendish Laboratory of the University of Cambridge in the decades after the Second World War. This covers the development of the aperture synthesis technique for Radio Astronomy and how that required using the new computing technology developed by the University's Mathematical Laboratory: the EDSAC, EDSAC 2 and TITAN computers. It looks at the scientific advances made by the Radio Astronomy group, particularly the assembling of evidence which contradicted the Steady State Hypothesis. It also examines the software advances that allowed bigger telescopes to be built: the Fast Fourier Transform (FFT) and the degridding algorithm. Throughout, the contribution of women is uncovered, from the diagrams they drew for scientific publications, through programming and operating computers, to writing scientific papers.

</details>

<details>

<summary>2022-07-21 14:03:13 - EdiBERT, a generative model for image editing</summary>

- *Thibaut Issenhuth, Ugo Tanielian, JÃ©rÃ©mie Mary, David Picard*

- `2111.15264v3` - [abs](http://arxiv.org/abs/2111.15264v3) - [pdf](http://arxiv.org/pdf/2111.15264v3)

> Advances in computer vision are pushing the limits of im-age manipulation, with generative models sampling detailed images on various tasks. However, a specialized model is often developed and trained for each specific task, even though many image edition tasks share similarities. In denoising, inpainting, or image compositing, one always aims at generating a realistic image from a low-quality one. In this paper, we aim at making a step towards a unified approach for image editing. To do so, we propose EdiBERT, a bi-directional transformer trained in the discrete latent space built by a vector-quantized auto-encoder. We argue that such a bidirectional model is suited for image manipulation since any patch can be re-sampled conditionally to the whole image. Using this unique and straightforward training objective, we show that the resulting model matches state-of-the-art performances on a wide variety of tasks: image denoising, image completion, and image composition.

</details>

<details>

<summary>2022-07-21 15:08:18 - NeRF-SR: High-Quality Neural Radiance Fields using Supersampling</summary>

- *Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang, Yu-Wing Tai, Shi-Min Hu*

- `2112.01759v3` - [abs](http://arxiv.org/abs/2112.01759v3) - [pdf](http://arxiv.org/pdf/2112.01759v3)

> We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis with mostly low-resolution (LR) inputs. Our method is built upon Neural Radiance Fields (NeRF) that predicts per-point density and color with a multi-layer perceptron. While producing images at arbitrary scales, NeRF struggles with resolutions that go beyond observed images. Our key insight is that NeRF benefits from 3D consistency, which means an observed pixel absorbs information from nearby views. We first exploit it by a supersampling strategy that shoots multiple rays at each image pixel, which further enforces multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can further boost the performance of supersampling by a refinement network that leverages the estimated depth at hand to hallucinate details from related patches on only one HR reference image. Experiment results demonstrate that NeRF-SR generates high-quality results for novel view synthesis at HR on both synthetic and real-world datasets without any external information.

</details>

<details>

<summary>2022-07-21 16:59:57 - Symbolic Regression in Materials Science: Discovering Interatomic Potentials from Data</summary>

- *Bogdan Burlacu, Michael Kommenda, Gabriel Kronberger, Stephan Winkler, Michael Affenzeller*

- `2206.06422v2` - [abs](http://arxiv.org/abs/2206.06422v2) - [pdf](http://arxiv.org/pdf/2206.06422v2)

> Particle-based modeling of materials at atomic scale plays an important role in the development of new materials and understanding of their properties. The accuracy of particle simulations is determined by interatomic potentials, which allow to calculate the potential energy of an atomic system as a function of atomic coordinates and potentially other properties. First-principles-based ab initio potentials can reach arbitrary levels of accuracy, however their aplicability is limited by their high computational cost.   Machine learning (ML) has recently emerged as an effective way to offset the high computational costs of ab initio atomic potentials by replacing expensive models with highly efficient surrogates trained on electronic structure data. Among a plethora of current methods, symbolic regression (SR) is gaining traction as a powerful "white-box" approach for discovering functional forms of interatomic potentials.   This contribution discusses the role of symbolic regression in Materials Science (MS) and offers a comprehensive overview of current methodological challenges and state-of-the-art results. A genetic programming-based approach for modeling atomic potentials from raw data (consisting of snapshots of atomic positions and associated potential energy) is presented and empirically validated on ab initio electronic structure data.

</details>

<details>

<summary>2022-07-21 22:52:05 - Solving the optimal stopping problem with reinforcement learning: an application in financial option exercise</summary>

- *Leonardo Kanashiro Felizardo, Elia Matsumoto, Emilio Del-Moral-Hernandez*

- `2208.00765v1` - [abs](http://arxiv.org/abs/2208.00765v1) - [pdf](http://arxiv.org/pdf/2208.00765v1)

> The optimal stopping problem is a category of decision problems with a specific constrained configuration. It is relevant to various real-world applications such as finance and management. To solve the optimal stopping problem, state-of-the-art algorithms in dynamic programming, such as the least-squares Monte Carlo (LSMC), are employed. This type of algorithm relies on path simulations using only the last price of the underlying asset as a state representation. Also, the LSMC was thinking for option valuation where risk-neutral probabilities can be employed to account for uncertainty. However, the general optimal stopping problem goals may not fit the requirements of the LSMC showing auto-correlated prices. We employ a data-driven method that uses Monte Carlo simulation to train and test artificial neural networks (ANN) to solve the optimal stopping problem. Using ANN to solve decision problems is not entirely new. We propose a different architecture that uses convolutional neural networks (CNN) to deal with the dimensionality problem that arises when we transform the whole history of prices into a Markovian state. We present experiments that indicate that our proposed architecture improves results over the previous implementations under specific simulated time series function sets. Lastly, we employ our proposed method to compare the optimal exercise of the financial options problem with the LSMC algorithm. Our experiments show that our method can capture more accurate exercise opportunities when compared to the LSMC. We have outstandingly higher (above 974\% improvement) expected payoff from these exercise policies under the many Monte Carlo simulations that used the real-world return database on the out-of-sample (test) data.

</details>

<details>

<summary>2022-07-22 01:24:34 - Sound and Complete Neural Network Repair with Minimality and Locality Guarantees</summary>

- *Feisi Fu, Wenchao Li*

- `2110.07682v3` - [abs](http://arxiv.org/abs/2110.07682v3) - [pdf](http://arxiv.org/pdf/2110.07682v3)

> We present a novel methodology for repairing neural networks that use ReLU activation functions. Unlike existing methods that rely on modifying the weights of a neural network which can induce a global change in the function space, our approach applies only a localized change in the function space while still guaranteeing the removal of the buggy behavior. By leveraging the piecewise linear nature of ReLU networks, our approach can efficiently construct a patch network tailored to the linear region where the buggy input resides, which when combined with the original network, provably corrects the behavior on the buggy input. Our method is both sound and complete -- the repaired network is guaranteed to fix the buggy input, and a patch is guaranteed to be found for any buggy input. Moreover, our approach preserves the continuous piecewise linear nature of ReLU networks, automatically generalizes the repair to all the points including other undetected buggy inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees that outputs on inputs away from the repair region are unaltered. On several benchmarks, we show that our approach significantly outperforms existing methods in terms of locality and limiting negative side effects. Our code is available on GitHub: https://github.com/BU-DEPEND-Lab/REASSURE.

</details>

<details>

<summary>2022-07-22 01:44:49 - World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator</summary>

- *Tatsuya Matsushima, Yuki Noguchi, Jumpei Arima, Toshiki Aoki, Yuki Okita, Yuya Ikeda, Koki Ishimoto, Shohei Taniguchi, Yuki Yamashita, Shoichi Seto, Shixiang Shane Gu, Yusuke Iwasawa, Yutaka Matsuo*

- `2207.10106v2` - [abs](http://arxiv.org/abs/2207.10106v2) - [pdf](http://arxiv.org/pdf/2207.10106v2)

> Tidying up a household environment using a mobile manipulator poses various challenges in robotics, such as adaptation to large real-world environmental variations, and safe and robust deployment in the presence of humans.The Partner Robot Challenge in World Robot Challenge (WRC) 2020, a global competition held in September 2021, benchmarked tidying tasks in the real home environments, and importantly, tested for full system performances.For this challenge, we developed an entire household service robot system, which leverages a data-driven approach to adapt to numerous edge cases that occur during the execution, instead of classical manual pre-programmed solutions. In this paper, we describe the core ingredients of the proposed robot system, including visual recognition, object manipulation, and motion planning. Our robot system won the second prize, verifying the effectiveness and potential of data-driven robot systems for mobile manipulation in home environments.

</details>

<details>

<summary>2022-07-22 09:45:42 - Classical and Quantum Algorithms for Variants of Subset-Sum via Dynamic Programming</summary>

- *Jonathan Allcock, Yassine Hamoudi, Antoine Joux, Felix KlingelhÃ¶fer, Miklos Santha*

- `2111.07059v2` - [abs](http://arxiv.org/abs/2111.07059v2) - [pdf](http://arxiv.org/pdf/2111.07059v2)

> Subset-Sum is an NP-complete problem where one must decide if a multiset of $n$ integers contains a subset whose elements sum to a target value $m$. The best-known classical and quantum algorithms run in time $\tilde{O}(2^{n/2})$ and $\tilde{O}(2^{n/3})$, respectively, based on the well-known meet-in-the-middle technique. Here we introduce a novel classical dynamic-programming-based data structure with applications to Subset-Sum and a number of variants, including Equal-Sums (where one seeks two disjoint subsets with the same sum), 2-Subset-Sum (a relaxed version of Subset-Sum where each item in the input set can be used twice in the summation), and Shifted-Sums, a generalization of both of these variants, where one seeks two disjoint subsets whose sums differ by some specified value.   Given any modulus $p$, our data structure can be constructed in time $O(n^2p)$, after which queries can be made in time $O(n^2)$ to the lists of subsets summing to any value modulo $p$. We use this data structure in combination with variable-time amplitude amplification and a new quantum pair finding algorithm, extending the quantum claw finding algorithm to the multiple solutions case, to give an $O(2^{0.504n})$ quantum algorithm for Shifted-Sums, an improvement on the best-known $O(2^{0.773n})$ classical running time. Incidentally, we obtain new $\tilde{O}(2^{n/2})$ and $\tilde{O}(2^{n/3})$ classical and quantum algorithms for Subset-Sum, not based on the seminal meet-in-the-middle method. We also study Pigeonhole Equal-Sums and Pigeonhole Modular Equal-Sums, where the existence of a solution is guaranteed by the pigeonhole principle. For the former problem, we give faster classical and quantum algorithms with running time $\tilde{O}(2^{n/2})$ and $\tilde{O}(2^{2n/5})$, respectively. For the more general modular problem, we give a classical algorithm that also runs in time $\tilde{O}(2^{n/2})$.

</details>

<details>

<summary>2022-07-22 14:20:02 - Code Structure Guided Transformer for Source Code Summarization</summary>

- *Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, Xin Xia, Michael R. Lyu*

- `2104.09340v2` - [abs](http://arxiv.org/abs/2104.09340v2) - [pdf](http://arxiv.org/pdf/2104.09340v2)

> Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this paper, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., data flow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% in terms of METEOR score, a metric widely used for measuring generation quality, respectively on two benchmark datasets.

</details>

<details>

<summary>2022-07-22 15:17:43 - Exact Matrix Factorization Updates for Nonlinear Programming</summary>

- *Adolfo R. Escobedo*

- `2202.00520v2` - [abs](http://arxiv.org/abs/2202.00520v2) - [pdf](http://arxiv.org/pdf/2202.00520v2)

> LU and Cholesky matrix factorization algorithms are core subroutines used to solve systems of linear equations (SLEs) encountered while solving an optimization problem. Standard factorization algorithms are highly efficient but remain susceptible to the accumulation of roundoff errors, which can lead solvers to return feasibility and optimality claims that are actually invalid. This paper introduces a novel approach for solving sequences of closely related SLEs encountered in nonlinear programming efficiently and without roundoff errors. Specifically, it introduces rank-one update algorithms for the roundoff-error-free (REF) factorization framework, a toolset built on integer-preserving arithmetic that has led to the development and implementation of fail-proof SLE solution subroutines for linear programming. The formal guarantees of the proposed algorithms are established through the derivation of theoretical insights. Their advantages are supported with computational experiments, which demonstrate upwards of 75x-improvements over exact factorization run-times on fully dense matrices with over one million entries. A significant advantage of the methodology is that the length of any coefficient calculated via the proposed algorithms is bounded polynomially in the size of the inputs without having to resort to greatest common divisor operations, which are required by and thereby hinder an efficient implementation of exact rational arithmetic approaches.

</details>

<details>

<summary>2022-07-22 18:08:16 - PanGu-Coder: Program Synthesis with Function-Level Language Modeling</summary>

- *Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang, Qun Liu*

- `2207.11280v1` - [abs](http://arxiv.org/abs/2207.11280v1) - [pdf](http://arxiv.org/pdf/2207.11280v1)

> We present PanGu-Coder, a pretrained decoder-only language model adopting the PanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description. We train PanGu-Coder using a two-stage strategy: the first stage employs Causal Language Modelling (CLM) to pre-train on raw programming language data, while the second stage uses a combination of Causal Language Modelling and Masked Language Modelling (MLM) training objectives that focus on the downstream task of text-to-code generation and train on loosely curated pairs of natural language program definitions and code functions. Finally, we discuss PanGu-Coder-FT, which is fine-tuned on a combination of competitive programming problems and code with continuous integration tests. We evaluate PanGu-Coder with a focus on whether it generates functionally correct programs and demonstrate that it achieves equivalent or better performance than similarly sized models, such as CodeX, while attending a smaller context window and training on less data.

</details>

<details>

<summary>2022-07-22 18:38:11 - Receptive Field-based Segmentation for Distributed CNN Inference Acceleration in Collaborative Edge Computing</summary>

- *Nan Li, Alexandros Iosifidis, Qi Zhang*

- `2207.11293v1` - [abs](http://arxiv.org/abs/2207.11293v1) - [pdf](http://arxiv.org/pdf/2207.11293v1)

> This paper studies inference acceleration using distributed convolutional neural networks (CNNs) in collaborative edge computing network. To avoid inference accuracy loss in inference task partitioning, we propose receptive field-based segmentation (RFS). To reduce the computation time and communication overhead, we propose a novel collaborative edge computing using fused-layer parallelization to partition a CNN model into multiple blocks of convolutional layers. In this scheme, the collaborative edge servers (ESs) only need to exchange small fraction of the sub-outputs after computing each fused block. In addition, to find the optimal solution of partitioning a CNN model into multiple blocks, we use dynamic programming, named as dynamic programming for fused-layer parallelization (DPFP). The experimental results show that DPFP can accelerate inference of VGG-16 up to 73% compared with the pre-trained model, which outperforms the existing work MoDNN in all tested scenarios. Moreover, we evaluate the service reliability of DPFP under time-variant channel, which shows that DPFP is an effective solution to ensure high service reliability with strict service deadline.

</details>

<details>

<summary>2022-07-22 19:01:48 - JAM: The JavaScript Agent Machine for Distributed Computing and Simulation with reactive and mobile Multi-agent Systems -- A Technical Report</summary>

- *Stefan Bosse*

- `2207.11300v1` - [abs](http://arxiv.org/abs/2207.11300v1) - [pdf](http://arxiv.org/pdf/2207.11300v1)

> Agent-based modelling (ABM), simulation (ABS), and distributed computation (ABC) are established methods. The Internet and Web-based technologies are suitable carriers. This paper is a technical report with some tutorial aspects of the JavaScript Agent Machine (JAM) platform and the programming of agents with AgentJS, a sub-set of the widely used JavaScript programming language for the programming of mobile state-based reactive agents. In addition to explaining the motivation for particular design choices and introducing core concepts of the architecture and the programming of agents in JavaScript, short examples illustrate the power of the JAM platform and its components for the deployment of large-scale multi-agent system in strong heterogeneous environments like the Internet. JAM is suitable for the deployment in strong heterogeneous and mobile environments. Finally, JAM can be used for ABC as well as for ABS in an unified methodology, finally enabling mobile crowd sensing coupled with simulation (ABS).

</details>

<details>

<summary>2022-07-23 04:28:45 - Satellite Detection in Unresolved Space Imagery for Space Domain Awareness Using Neural Networks</summary>

- *Jarred Jordan, Daniel Posada, David Zuehlke, Angelica Radulovic, Aryslan Malik, Troy Henderson*

- `2207.11412v1` - [abs](http://arxiv.org/abs/2207.11412v1) - [pdf](http://arxiv.org/pdf/2207.11412v1)

> This work utilizes a MobileNetV2 Convolutional Neural Network (CNN) for fast, mobile detection of satellites, and rejection of stars, in cluttered unresolved space imagery. First, a custom database is created using imagery from a synthetic satellite image program and labeled with bounding boxes over satellites for "satellite-positive" images. The CNN is then trained on this database and the inference is validated by checking the accuracy of the model on an external dataset constructed of real telescope imagery. In doing so, the trained CNN provides a method of rapid satellite identification for subsequent utilization in ground-based orbit estimation.

</details>

<details>

<summary>2022-07-23 21:05:06 - Experience with Abrupt Transition to Remote Teaching of Embedded Systems</summary>

- *Jan Koniarik, Daniel Dlhopolcek, Martin Ukrop*

- `2207.11603v1` - [abs](http://arxiv.org/abs/2207.11603v1) - [pdf](http://arxiv.org/pdf/2207.11603v1)

> Due to the pandemic of COVID-19, many university courses had to abruptly transform to enable remote teaching. Adjusting courses on embedded systems and micro-controllers was extra challenging since interaction with real hardware is their integral part. We start by comparing our experience with four basic alternatives of teaching embedded systems: 1) interacting with hardware at school, 2) having remote access to hardware, 3) lending hardware to students for at-home work and 4) virtualizing hardware. Afterward, we evaluate in detail our experience of the fast transition from traditional, offline at-school hardware programming course to using remote access to real hardware present in the lab. The somewhat unusual remote hardware access approach turned out to be a fully viable alternative for teaching embedded systems, enabling a relatively low-effort transition. Our setup is based on existing solutions and stable open technologies without the need for custom-developed applications that require high maintenance. We evaluate the experience of both the students and teachers and condense takeaways for future courses. The specific environment setup is available online as an inspiration for others.

</details>

<details>

<summary>2022-07-23 22:37:22 - A Historical Interaction between Artificial Intelligence and Philosophy</summary>

- *Youheng Zhang*

- `2208.04148v1` - [abs](http://arxiv.org/abs/2208.04148v1) - [pdf](http://arxiv.org/pdf/2208.04148v1)

> This paper reviews the historical development of AI and representative philosophical thinking from the perspective of the research paradigm. Additionally, it considers the methodology and applications of AI from a philosophical perspective and anticipates its continued advancement. In the history of AI, Symbolism and connectionism are the two main paradigms in AI research. Symbolism holds that the world can be explained by symbols and dealt with through precise, logical processes, but connectionism believes this process should be implemented through artificial neural networks. Regardless of how intelligent machines or programs should achieve their smart goals, the historical development of AI demonstrates the best answer at this time. Still, it is not the final answer of AI research.

</details>

<details>

<summary>2022-07-24 04:54:17 - Lyra: A Benchmark for Turducken-Style Code Generation</summary>

- *Qingyuan Liang, Zeyu Sun, Qihao Zhu, Wenjie Zhang, Lian Yu, Yingfei Xiong, Lu Zhang*

- `2108.12144v3` - [abs](http://arxiv.org/abs/2108.12144v3) - [pdf](http://arxiv.org/pdf/2108.12144v3)

> Recently, neural techniques have been used to generate source code automatically. While promising for declarative languages, these approaches achieve much poorer performance on datasets for imperative languages. Since a declarative language is typically embedded in an imperative language (i.e., the turducken-style programming) in real-world software development, the promising results on declarative languages can hardly lead to significant reduction of manual software development efforts. In this paper, we define a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative language with an embedded declarative language. To our knowledge, this is the first turducken-style code generation task. For this task, we present Lyra: a dataset in Python with embedded SQL. This dataset contains 2,000 carefully annotated database manipulation programs from real-world projects. Each program is paired with both a Chinese comment and an English comment. In our experiment, we adopted Transformer, BERT-style, and GPT-style models as baselines. In the best setting, the generation performance of GPT-style models is better than others, where the AST exact matching accuracy is 24% and 25.5% when using Chinese and English comments, respectively. Therefore, we believe that Lyra provides a new challenge for code generation. Yet, overcoming this challenge may significantly boost the applicability of code generation techniques for real-world software development.

</details>

<details>

<summary>2022-07-24 07:45:47 - PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training</summary>

- *Zihang Xu, Zhenghua Xu, Shuo Zhang, Thomas Lukasiewicz*

- `2207.11683v1` - [abs](http://arxiv.org/abs/2207.11683v1) - [pdf](http://arxiv.org/pdf/2207.11683v1)

> Deep learning based semi-supervised learning (SSL) methods have achieved strong performance in medical image segmentation, which can alleviate doctors' expensive annotation by utilizing a large amount of unlabeled data. Unlike most existing semi-supervised learning methods, adversarial training based methods distinguish samples from different sources by learning the data distribution of the segmentation map, leading the segmenter to generate more accurate predictions. We argue that the current performance restrictions for such approaches are the problems of feature extraction and learning preference. In this paper, we propose a new semi-supervised adversarial method called Patch Confidence Adversarial Training (PCA) for medical image segmentation. Rather than single scalar classification results or pixel-level confidence maps, our proposed discriminator creates patch confidence maps and classifies them at the scale of the patches. The prediction of unlabeled data learns the pixel structure and context information in each patch to get enough gradient feedback, which aids the discriminator in convergent to an optimal state and improves semi-supervised segmentation performance. Furthermore, at the discriminator's input, we supplement semantic information constraints on images, making it simpler for unlabeled data to fit the expected data distribution. Extensive experiments on the Automated Cardiac Diagnosis Challenge (ACDC) 2017 dataset and the Brain Tumor Segmentation (BraTS) 2019 challenge dataset show that our method outperforms the state-of-the-art semi-supervised methods, which demonstrates its effectiveness for medical image segmentation.

</details>

<details>

<summary>2022-07-24 11:10:50 - An Answer to the Bose-Nelson Sorting Problem for 11 and 12 Channels</summary>

- *Jannis Harder*

- `2012.04400v3` - [abs](http://arxiv.org/abs/2012.04400v3) - [pdf](http://arxiv.org/pdf/2012.04400v3)

> We show that 11-channel sorting networks have at least 35 comparators and that 12-channel sorting networks have at least 39 comparators. This positively settles the optimality of the corresponding sorting networks given in The Art of Computer Programming vol. 3 and closes the two smallest open instances of the Bose-Nelson sorting problem. We obtain these bounds by generalizing a result of Van Voorhis from sorting networks to a more general class of comparator networks. From this we derive a dynamic programming algorithm that computes the optimal size for a sorting network with a given number of channels. From an execution of this algorithm we construct a certificate containing a derivation of the corresponding lower size bound, which we check using a program formally verified using the Isabelle/HOL proof assistant.

</details>

<details>

<summary>2022-07-24 14:47:50 - CLIP-CLOP: CLIP-Guided Collage and Photomontage</summary>

- *Piotr Mirowski, Dylan Banarse, Mateusz Malinowski, Simon Osindero, Chrisantha Fernando*

- `2205.03146v3` - [abs](http://arxiv.org/abs/2205.03146v3) - [pdf](http://arxiv.org/pdf/2205.03146v3)

> The unabated mystique of large-scale neural networks, such as the CLIP dual image-and-text encoder, popularized automatically generated art. Increasingly more sophisticated generators enhanced the artworks' realism and visual appearance, and creative prompt engineering enabled stylistic expression. Guided by an artist-in-the-loop ideal, we design a gradient-based generator to produce collages. It requires the human artist to curate libraries of image patches and to describe (with prompts) the whole image composition, with the option to manually adjust the patches' positions during generation, thereby allowing humans to reclaim some control of the process and achieve greater creative freedom. We explore the aesthetic potentials of high-resolution collages, and provide an open-source Google Colab as an artistic tool.

</details>

<details>

<summary>2022-07-24 15:38:38 - Why Non-myopic Bayesian Optimization is Promising and How Far Should We Look-ahead? A Study via Rollout</summary>

- *Xubo Yue, Raed Al Kontar*

- `1911.01004v3` - [abs](http://arxiv.org/abs/1911.01004v3) - [pdf](http://arxiv.org/pdf/1911.01004v3)

> Lookahead, also known as non-myopic, Bayesian optimization (BO) aims to find optimal sampling policies through solving a dynamic program (DP) that maximizes a long-term reward over a rolling horizon. Though promising, lookahead BO faces the risk of error propagation through its increased dependence on a possibly mis-specified model. In this work we focus on the rollout approximation for solving the intractable DP. We first prove the improving nature of rollout in tackling lookahead BO and provide a sufficient condition for the used heuristic to be rollout improving. We then provide both a theoretical and practical guideline to decide on the rolling horizon stagewise. This guideline is built on quantifying the negative effect of a mis-specified model. To illustrate our idea, we provide case studies on both single and multi-information source BO. Empirical results show the advantageous properties of our method over several myopic and non-myopic BO algorithms.

</details>

<details>

<summary>2022-07-24 15:56:03 - Neurosymbolic Repair for Low-Code Formula Languages</summary>

- *Rohan Bavishi, Harshit Joshi, JosÃ© Pablo Cambronero SÃ¡nchez, Anna Fariha, Sumit Gulwani, Vu Le, Ivan Radicek, Ashish Tiwari*

- `2207.11765v1` - [abs](http://arxiv.org/abs/2207.11765v1) - [pdf](http://arxiv.org/pdf/2207.11765v1)

> Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify and fix, even though they can be resolved with just a few edits. We formalize the problem of producing such edits as the last-mile repair problem. To address this problem, we developed LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages. LaMirage takes a grammar and a set of domain-specific constraints/rules, which jointly approximate the target language, and uses these to generate a repair engine that can fix formulas in that language. To tackle the challenges of localizing the errors and ranking the candidate repairs, LaMirage leverages neural techniques, whereas it relies on symbolic methods to generate candidate repairs. This combination allows LaMirage to find repairs that satisfy the provided grammar and constraints, and then pick the most natural repair. We compare LaMirage to state-of-the-art neural and symbolic approaches on 400 real Excel and PowerFx formulas, where LaMirage outperforms all baselines. We release these benchmarks to encourage subsequent work in low-code domains.

</details>

<details>

<summary>2022-07-24 16:33:04 - Scaling Structured Inference with Randomization</summary>

- *Yao Fu, John P. Cunningham, Mirella Lapata*

- `2112.03638v3` - [abs](http://arxiv.org/abs/2112.03638v3) - [pdf](http://arxiv.org/pdf/2112.03638v3)

> Deep discrete structured models have seen considerable progress recently, but traditional inference using dynamic programming (DP) typically works with a small number of states (less than hundreds), which severely limits model capacity. At the same time, across machine learning, there is a recent trend of using randomized truncation techniques to accelerate computations involving large sums. Here, we propose a family of randomized dynamic programming (RDP) algorithms for scaling structured models to tens of thousands of latent states. Our method is widely applicable to classical DP-based inference (partition, marginal, reparameterization, entropy) and different graph structures (chains, trees, and more general hypergraphs). It is also compatible with automatic differentiation: it can be integrated with neural networks seamlessly and learned with gradient-based optimizers. Our core technique approximates the sum-product by restricting and reweighting DP on a small subset of nodes, which reduces computation by orders of magnitude. We further achieve low bias and variance via Rao-Blackwellization and importance sampling. Experiments over different graphs demonstrate the accuracy and efficiency of our approach. Furthermore, when using RDP for training a structured variational autoencoder with a scaled inference network, we achieve better test likelihood than baselines and successfully prevent posterior collapse. code at: https://github.com/FranxYao/RDP

</details>

<details>

<summary>2022-07-24 16:49:30 - Optimal Time-Backlog Tradeoffs for the Variable-Processor Cup Game</summary>

- *William Kuszmaul, Shyam Narayanan*

- `2205.01722v2` - [abs](http://arxiv.org/abs/2205.01722v2) - [pdf](http://arxiv.org/pdf/2205.01722v2)

> The \emph{$ p$-processor cup game} is a classic and widely studied scheduling problem that captures the setting in which a $p$-processor machine must assign tasks to processors over time in order to ensure that no individual task ever falls too far behind. The problem is formalized as a multi-round game in which two players, a filler (who assigns work to tasks) and an emptier (who schedules tasks) compete. The emptier's goal is to minimize backlog, which is the maximum amount of outstanding work for any task.   Recently, Kuszmaul and Westover (ITCS, 2021) proposed the \emph{variable-processor cup game}, which considers the same problem, except that the amount of resources available to the players (i.e., the number $p$ of processors) fluctuates between rounds of the game. They showed that this seemingly small modification fundamentally changes the dynamics of the game: whereas the optimal backlog in the fixed $p$-processor game is $\Theta(\log n)$, independent of $p$, the optimal backlog in the variable-processor game is $\Theta(n)$. The latter result was only known to apply to games with \emph{exponentially many} rounds, however, and it has remained an open question what the optimal tradeoff between time and backlog is for shorter games.   This paper establishes a tight trade-off curve between time and backlog in the variable-processor cup game. Importantly, we prove that for a game consisting of $t$ rounds, the optimal backlog is $\Theta(n)$ if and only if $t \ge \Omega(n^3)$. Our techniques also allow for us to resolve several other open questions concerning how the variable-processor cup game behaves in beyond-worst-case-analysis settings.

</details>

<details>

<summary>2022-07-24 19:59:28 - An Exact Algorithm for Semi-supervised Minimum Sum-of-Squares Clustering</summary>

- *Veronica Piccialli, Anna Russo Russo, Antonio M. Sudoso*

- `2111.15571v2` - [abs](http://arxiv.org/abs/2111.15571v2) - [pdf](http://arxiv.org/pdf/2111.15571v2)

> The minimum sum-of-squares clustering (MSSC), or k-means type clustering, is traditionally considered an unsupervised learning task. In recent years, the use of background knowledge to improve the cluster quality and promote interpretability of the clustering process has become a hot research topic at the intersection of mathematical optimization and machine learning research. The problem of taking advantage of background information in data clustering is called semi-supervised or constrained clustering. In this paper, we present a branch-and-cut algorithm for semi-supervised MSSC, where background knowledge is incorporated as pairwise must-link and cannot-link constraints. For the lower bound procedure, we solve the semidefinite programming relaxation of the MSSC discrete optimization model, and we use a cutting-plane procedure for strengthening the bound. For the upper bound, instead, by using integer programming tools, we use an adaptation of the k-means algorithm to the constrained case. For the first time, the proposed global optimization algorithm efficiently manages to solve real-world instances up to 800 data points with different combinations of must-link and cannot-link constraints and with a generic number of features. This problem size is about four times larger than the one of the instances solved by state-of-the-art exact algorithms.

</details>

<details>

<summary>2022-07-25 00:16:31 - Greedy Algorithm for Multiway Matching with Bounded Regret</summary>

- *Varun Gupta*

- `2112.04622v3` - [abs](http://arxiv.org/abs/2112.04622v3) - [pdf](http://arxiv.org/pdf/2112.04622v3)

> In this paper we prove the efficacy of a simple greedy algorithm for a finite horizon online resource allocation/matching problem, when the corresponding static planning linear program (SPP) exhibits a non-degeneracy condition called the general position gap (GPG). The key intuition that we formalize is that the solution of the reward maximizing SPP is the same as a feasibility Linear Program restricted to the optimal basic activities, and under GPG this solution can be tracked with bounded regret by a greedy algorithm, i.e., without the commonly used technique of periodically resolving the SPP.   The goal of the decision maker is to combine resources (from a finite set of resource types) into configurations (from a finite set of feasible configurations) where each configuration is specified by the number of resources consumed of each type and a reward. The resources are further subdivided into three types - offline (whose quantity is known and available at time 0), online-queueable (which arrive online and can be stored in a buffer), and online-nonqueueable (which arrive online and must be matched on arrival or lost). Under GRG we prove that, (i) our greedy algorithm gets bounded any-time regret of $\mathcal{O}(1/\epsilon_0)$ for matching reward ($\epsilon_0$ is a measure of the GPG) when no configuration contains both an online-queueable and an online-nonqueueable resource, and (ii) $\mathcal{O}(\log t)$ expected any-time regret otherwise (we also prove a matching lower bound). By considering the three types of resources, our matching framework encompasses several well-studied problems such as dynamic multi-sided matching, network revenue management, online stochastic packing, and multiclass queueing systems.

</details>

<details>

<summary>2022-07-25 00:17:45 - Online Stochastic Optimization with Wasserstein Based Non-stationarity</summary>

- *Jiashuo Jiang, Xiaocheng Li, Jiawei Zhang*

- `2012.06961v3` - [abs](http://arxiv.org/abs/2012.06961v3) - [pdf](http://arxiv.org/pdf/2012.06961v3)

> We consider a general online stochastic optimization problem with multiple budget constraints over a horizon of finite time periods. In each time period, a reward function and multiple cost functions are revealed, and the decision maker needs to specify an action from a convex and compact action set to collect the reward and consume the budget. Each cost function corresponds to the consumption of one budget. In each period, the reward and cost functions are drawn from an unknown distribution, which is non-stationary across time. The objective of the decision maker is to maximize the cumulative reward subject to the budget constraints. This formulation captures a wide range of applications including online linear programming and network revenue management, among others. In this paper, we consider two settings: (i) a data-driven setting where the true distribution is unknown but a prior estimate (possibly inaccurate) is available; (ii) an uninformative setting where the true distribution is completely unknown. We propose a unified Wasserstein-distance based measure to quantify the inaccuracy of the prior estimate in setting (i) and the non-stationarity of the system in setting (ii). We show that the proposed measure leads to a necessary and sufficient condition for the attainability of a sublinear regret in both settings. For setting (i), we propose a new algorithm, which takes a primal-dual perspective and integrates the prior information of the underlying distributions into an online gradient descent procedure in the dual space. The algorithm also naturally extends to the uninformative setting (ii). Under both settings, we show the corresponding algorithm achieves a regret of optimal order. In numerical experiments, we demonstrate how the proposed algorithms can be naturally integrated with the re-solving technique to further boost the empirical performance.

</details>

<details>

<summary>2022-07-25 03:24:58 - Versatile Weight Attack via Flipping Limited Bits</summary>

- *Jiawang Bai, Baoyuan Wu, Zhifeng Li, Shu-tao Xia*

- `2207.12405v1` - [abs](http://arxiv.org/abs/2207.12405v1) - [pdf](http://arxiv.org/pdf/2207.12405v1)

> To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage. Considering the effectiveness and stealthiness goals, we provide a general formulation to perform the bit-flip based weight attack, where the effectiveness term could be customized depending on the attacker's purpose. Furthermore, we present two cases of the general formulation with different malicious purposes, i.e., single sample attack (SSA) and triggered samples attack (TSA). To this end, we formulate this problem as a mixed integer programming (MIP) to jointly determine the state of the binary bits (0 or 1) in the memory and learn the sample modification. Utilizing the latest technique in integer programming, we equivalently reformulate this MIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of SSA and TSA in attacking DNNs.

</details>

<details>

<summary>2022-07-25 04:05:29 - IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning</summary>

- *Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu*

- `2110.13214v4` - [abs](http://arxiv.org/abs/2110.13214v4) - [pdf](http://arxiv.org/pdf/2110.13214v4)

> Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.

</details>

<details>

<summary>2022-07-25 04:45:16 - Minimax Rates for Robust Community Detection</summary>

- *Allen Liu, Ankur Moitra*

- `2207.11903v1` - [abs](http://arxiv.org/abs/2207.11903v1) - [pdf](http://arxiv.org/pdf/2207.11903v1)

> In this work, we study the problem of community detection in the stochastic block model with adversarial node corruptions. Our main result is an efficient algorithm that can tolerate an $\epsilon$-fraction of corruptions and achieves error $O(\epsilon) + e^{-\frac{C}{2} (1 \pm o(1))}$ where $C = (\sqrt{a} - \sqrt{b})^2$ is the signal-to-noise ratio and $a/n$ and $b/n$ are the inter-community and intra-community connection probabilities respectively. These bounds essentially match the minimax rates for the SBM without corruptions. We also give robust algorithms for $\mathbb{Z}_2$-synchronization. At the heart of our algorithm is a new semidefinite program that uses global information to robustly boost the accuracy of a rough clustering. Moreover, we show that our algorithms are doubly-robust in the sense that they work in an even more challenging noise model that mixes adversarial corruptions with unbounded monotone changes, from the semi-random model.

</details>

<details>

<summary>2022-07-25 11:29:01 - On the Foundations of Grounding in Answer Set Programming</summary>

- *Roland Kaminski, Torsten Schaub*

- `2108.04769v3` - [abs](http://arxiv.org/abs/2108.04769v3) - [pdf](http://arxiv.org/pdf/2108.04769v3)

> We provide a comprehensive elaboration of the theoretical foundations of variable instantiation, or grounding, in Answer Set Programming (ASP). Building on the semantics of ASP's modeling language, we introduce a formal characterization of grounding algorithms in terms of (fixed point) operators. A major role is played by dedicated well-founded operators whose associated models provide semantic guidance for delineating the result of grounding along with on-the-fly simplifications. We address an expressive class of logic programs that incorporates recursive aggregates and thus amounts to the scope of existing ASP modeling languages. This is accompanied with a plain algorithmic framework detailing the grounding of recursive aggregates. The given algorithms correspond essentially to the ones used in the ASP grounder gringo.

</details>

<details>

<summary>2022-07-25 15:41:50 - A Solver + Gradient Descent Training Algorithm for Deep Neural Networks</summary>

- *Dhananjay Ashok, Vineel Nagisetty, Christopher Srinivasa, Vijay Ganesh*

- `2207.03264v2` - [abs](http://arxiv.org/abs/2207.03264v2) - [pdf](http://arxiv.org/pdf/2207.03264v2)

> We present a novel hybrid algorithm for training Deep Neural Networks that combines the state-of-the-art Gradient Descent (GD) method with a Mixed Integer Linear Programming (MILP) solver, outperforming GD and variants in terms of accuracy, as well as resource and data efficiency for both regression and classification tasks. Our GD+Solver hybrid algorithm, called GDSolver, works as follows: given a DNN $D$ as input, GDSolver invokes GD to partially train $D$ until it gets stuck in a local minima, at which point GDSolver invokes an MILP solver to exhaustively search a region of the loss landscape around the weight assignments of $D$'s final layer parameters with the goal of tunnelling through and escaping the local minima. The process is repeated until desired accuracy is achieved. In our experiments, we find that GDSolver not only scales well to additional data and very large model sizes, but also outperforms all other competing methods in terms of rates of convergence and data efficiency. For regression tasks, GDSolver produced models that, on average, had 31.5% lower MSE in 48% less time, and for classification tasks on MNIST and CIFAR10, GDSolver was able to achieve the highest accuracy over all competing methods, using only 50% of the training data that GD baselines required.

</details>

<details>

<summary>2022-07-25 18:24:58 - Overwatch: Learning Patterns in Code Edit Sequences</summary>

- *Yuhao Zhang, Yasharth Bajpai, Priyanshu Gupta, Ameya Ketkar, Miltiadis Allamanis, Titus Barik, Sumit Gulwani, Arjun Radhakrishna, Mohammad Raza, Gustavo Soares, Ashish Tiwari*

- `2207.12456v1` - [abs](http://arxiv.org/abs/2207.12456v1) - [pdf](http://arxiv.org/pdf/2207.12456v1)

> Integrated Development Environments (IDEs) provide tool support to automate many source code editing tasks. Traditionally, IDEs use only the spatial context, i.e., the location where the developer is editing, to generate candidate edit recommendations. However, spatial context alone is often not sufficient to confidently predict the developer's next edit, and thus IDEs generate many suggestions at a location. Therefore, IDEs generally do not actively offer suggestions and instead, the developer is usually required to click on a specific icon or menu and then select from a large list of potential suggestions. As a consequence, developers often miss the opportunity to use the tool support because they are not aware it exists or forget to use it.   To better understand common patterns in developer behavior and produce better edit recommendations, we can additionally use the temporal context, i.e., the edits that a developer was recently performing. To enable edit recommendations based on temporal context, we present Overwatch, a novel technique for learning edit sequence patterns from traces of developers' edits performed in an IDE. Our experiments show that Overwatch has 78% precision and that Overwatch not only completed edits when developers missed the opportunity to use the IDE tool support but also predicted new edits that have no tool support in the IDE.

</details>

<details>

<summary>2022-07-26 00:02:45 - A Retrospective on ICSE 2022</summary>

- *Cailin Winston, Caleb Winston, Chloe Winston, Claris Winston, Cleah Winston*

- `2207.12578v1` - [abs](http://arxiv.org/abs/2207.12578v1) - [pdf](http://arxiv.org/pdf/2207.12578v1)

> The 44th International Conference on Software Engineering (ICSE 2022) was held in person from May 22 to May 27, 2022 in Pittsburgh, PA, USA. Here, we summarize themes of research and the direction of research in the field of software engineering and testing that we observed at the conference.

</details>

<details>

<summary>2022-07-26 02:09:18 - Improved Throughput for All-or-Nothing Multicommodity Flows with Arbitrary Demands</summary>

- *Anya Chaturvedi, Chandra Chekuri, Mengxue Liu, AndrÃ©a W. Richa, Mattias Rost, Stefan Schmid, Jamison Weber*

- `2005.04533v7` - [abs](http://arxiv.org/abs/2005.04533v7) - [pdf](http://arxiv.org/pdf/2005.04533v7)

> Throughput is a main performance objective in communication networks. This paper considers a fundamental maximum throughput routing problem -- the all-or-nothing multicommodity flow (ANF) problem -- in arbitrary directed graphs and in the practically relevant but challenging setting where demands can be (much) larger than the edge capacities. Hence, in addition to assigning requests to valid flows for each routed commodity, an admission control mechanism is required which prevents overloading the network when routing commodities. We make several contributions. On the theoretical side we obtain substantially improved bi-criteria approximation algorithms for this NP-hard problem. We present two non-trivial linear programming relaxations and show how to convert their fractional solutions into integer solutions via randomized rounding. One is an exponential-size formulation (solvable in polynomial time using a separation oracle) that considers a "packing" view and allows a more flexible approach, while the other is a compact (polynomial-size) edge-flow formulation that allows for easy solving via standard LP solvers. We obtain a polynomial-time randomized algorithm that yields an arbitrarily good approximation on the weighted throughput, while violating the edge capacity constraints by only a small multiplicative factor. We also describe a deterministic rounding algorithm by derandomization, using the method of pessimistic estimators. We complement our theoretical results with a proof of concept empirical evaluation.

</details>

<details>

<summary>2022-07-26 02:21:11 - TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems</summary>

- *Bao Gia Doan, Minhui Xue, Shiqing Ma, Ehsan Abbasnejad, Damith C. Ranasinghe*

- `2111.09999v2` - [abs](http://arxiv.org/abs/2111.09999v2) - [pdf](http://arxiv.org/pdf/2111.09999v2)

> Deep neural networks are vulnerable to attacks from adversarial inputs and, more recently, Trojans to misguide or hijack the model's decision. We expose the existence of an intriguing class of spatially bounded, physically realizable, adversarial examples -- Universal NaTuralistic adversarial paTches -- we call TnTs, by exploring the superset of the spatially bounded adversarial example space and the natural input space within generative adversarial networks. Now, an adversary can arm themselves with a patch that is naturalistic, less malicious-looking, physically realizable, highly effective achieving high attack success rates, and universal. A TnT is universal because any input image captured with a TnT in the scene will: i) misguide a network (untargeted attack); or ii) force the network to make a malicious decision (targeted attack). Interestingly, now, an adversarial patch attacker has the potential to exert a greater level of control -- the ability to choose a location-independent, natural-looking patch as a trigger in contrast to being constrained to noisy perturbations -- an ability is thus far shown to be only possible with Trojan attack methods needing to interfere with the model building processes to embed a backdoor at the risk discovery; but, still realize a patch deployable in the physical world. Through extensive experiments on the large-scale visual classification task, ImageNet with evaluations across its entire validation set of 50,000 images, we demonstrate the realistic threat from TnTs and the robustness of the attack. We show a generalization of the attack to create patches achieving higher attack success rates than existing state-of-the-art methods. Our results show the generalizability of the attack to different visual classification tasks (CIFAR-10, GTSRB, PubFig) and multiple state-of-the-art deep neural networks such as WideResnet50, Inception-V3 and VGG-16.

</details>

<details>

<summary>2022-07-26 04:09:22 - OCTAL: Graph Representation Learning for LTL Model Checking</summary>

- *Prasita Mukherjee, Haoteng Yin, Susheel Suresh, Tiark Rompf*

- `2207.11649v2` - [abs](http://arxiv.org/abs/2207.11649v2) - [pdf](http://arxiv.org/pdf/2207.11649v2)

> Model Checking is widely applied in verifying the correctness of complex and concurrent systems against a specification. Pure symbolic approaches while popular, still suffer from the state space explosion problem that makes them impractical for large scale systems and/or specifications. In this paper, we propose to use graph representation learning (GRL) for solving linear temporal logic (LTL) model checking, where the system and the specification are expressed by a B\"uchi automaton and an LTL formula respectively. A novel GRL-based framework OCTAL, is designed to learn the representation of the graph-structured system and specification, which reduces the model checking problem to binary classification in the latent space. The empirical experiments show that OCTAL achieves comparable accuracy against canonical SOTA model checkers on three different datasets, with up to $5\times$ overall speedup and above $63\times$ for satisfiability checking alone.

</details>

<details>

<summary>2022-07-26 07:44:24 - Design of Classes I</summary>

- *Marco T. MorazÃ¡n*

- `2207.12697v1` - [abs](http://arxiv.org/abs/2207.12697v1) - [pdf](http://arxiv.org/pdf/2207.12697v1)

> The use of functional programming languages in the first programming course at many universities is well-established and effective. Invariably, however, students must progress to study object-oriented programming. This article presents how the first steps of this transition have been successfully implemented at Seton Hall University. The developed methodology builds on the students' experience with type-based design acquired in their previous introduction to programming courses. The transition is made smooth by explicitly showing students that the design lessons they have internalized are relevant in object-oriented programming. This allows for new abstractions offered by object-oriented programming languages to be more easily taught and used by students. Empirical evidence collected from students in the course suggests that the approach developed is effective and that the transition is smooth.

</details>

<details>

<summary>2022-07-26 07:44:43 - Reimplementing the Wheel: Teaching Compilers with a Small Self-Contained One</summary>

- *Daniil Berezun, Dmitry Boulytchev*

- `2207.12698v1` - [abs](http://arxiv.org/abs/2207.12698v1) - [pdf](http://arxiv.org/pdf/2207.12698v1)

> We report on a one-semester compiler construction course based on the idea of implementing a small self-contained compiler for a small model language from scratch, not using other compiler construction frameworks. The course is built around an evolving family of languages with increasing expressiveness and complexity, which finally is crowned by a language with first-class functions, S-expressions, pattern matching, and garbage collection. The code generation technique is based on the idea of symbolic interpreters, which allows to implement a robust albeit not a very efficient native code generator. We give the motivation for the course, describe its structure, and report some results of teaching based on students' post-course surveys.

</details>

<details>

<summary>2022-07-26 07:45:20 - Introduction to Functional Classes in CS1</summary>

- *Marco T. MorazÃ¡n*

- `2207.12700v1` - [abs](http://arxiv.org/abs/2207.12700v1) - [pdf](http://arxiv.org/pdf/2207.12700v1)

> Students introduced to programming using a design-based approach and a functional programming language become familiar with first-class functions. They rarely, however, connect first-class functions to objects and object-oriented program design. This is a missed opportunity because students inevitably go on to courses using an object-oriented programming language. This article describes how students are introduced to objects within the setting of a design-based introduction to programming that uses a functional language. The methodology exposes students to interfaces, classes, objects, and polymorphic dispatch. Initial student feedback suggests that students benefit from the approach.

</details>

<details>

<summary>2022-07-26 07:45:44 - Teaching Interaction using State Diagrams</summary>

- *Padma Pasupathi, Christopher W. Schankula, Nicole DiVincenzo, Sarah Coker, Christopher Kumar Anand*

- `2207.12701v1` - [abs](http://arxiv.org/abs/2207.12701v1) - [pdf](http://arxiv.org/pdf/2207.12701v1)

> To make computational thinking appealing to young learners, initial programming instruction looks very different now than a decade ago, with increasing use of graphics and robots both real and virtual. After the first steps, children want to create interactive programs, and they need a model for this. State diagrams provide such a model.   This paper documents the design and implementation of a Model-Driven Engineering tool, SD Draw, that allows even primary-aged children to draw and understand state diagrams, and create modifiable app templates in the Elm programming language using the model-view-update pattern standard in Elm programs. We have tested this with grade 4 and 5 students. In our initial test, we discovered that children quickly understand the motivation and use of state diagrams using this tool, and will independently discover abstract states even if they are only taught to model using concrete states. To determine whether this approach is appropriate for children of this age we wanted to know: do children understand state diagrams, do they understand the role of reachability, and are they engaged by them? We found that they are able to translate between different representations of state diagrams, strongly indicating that they do understand them. We found with confidence p<0.001 that they do understand reachability by refuting the null hypothesis that they are creating diagrams randomly. And we found that they were engaged by the concept, with many students continuing to develop their diagrams on their own time after school and on the weekend.

</details>

<details>

<summary>2022-07-26 07:46:58 - Teaching Simple Constructive Proofs with Haskell Programs</summary>

- *Matthew Farrugia-Roberts, Bryn Jeffries, Harald SÃ¸ndergaard*

- `2208.04699v1` - [abs](http://arxiv.org/abs/2208.04699v1) - [pdf](http://arxiv.org/pdf/2208.04699v1)

> In recent years we have explored using Haskell alongside a traditional mathematical formalism in our large-enrolment university course on topics including logic and formal languages, aiming to offer our students a programming perspective on these mathematical topics. We have found it possible to offer almost all formative and summative assessment through an interactive learning platform, using Haskell as a lingua franca for digital exercises across our broad syllabus. One of the hardest exercises to convert into this format are traditional written proofs conveying constructive arguments. In this paper we reflect on the digitisation of this kind of exercise. We share many examples of Haskell exercises designed to target similar skills to written proof exercises across topics in propositional logic and formal languages, discussing various aspects of the design of such exercises. We also catalogue a sample of student responses to such exercises. This discussion contributes to our broader exploration of programming problems as a flexible digital medium for learning and assessment.

</details>

<details>

<summary>2022-07-26 07:47:22 - Engaging, Large-Scale Functional Programming Education in Physical and Virtual Space</summary>

- *Kevin Kappelmann, Jonas RÃ¤dle, Lukas Stevens*

- `2207.12703v1` - [abs](http://arxiv.org/abs/2207.12703v1) - [pdf](http://arxiv.org/pdf/2207.12703v1)

> Worldwide, computer science departments have experienced a dramatic increase in the number of student enrolments. Moreover, the ongoing COVID-19 pandemic requires institutions to radically replace the traditional way of on-site teaching, moving interaction from physical to virtual space. We report on our strategies and experience tackling these issues as part of a Haskell-based functional programming and verification course, accommodating over 2000 students in the course of two semesters. Among other things, we fostered engagement with weekly programming competitions and creative homework projects, workshops with industry partners, and collaborative pair-programming tutorials. To offer such an extensive programme to hundreds of students, we automated feedback for programming as well as inductive proof exercises. We explain and share our tools and exercises so that they can be reused by other educators.

</details>

<details>

<summary>2022-07-26 08:37:41 - On the Interaction between Test-Suite Reduction and Regression-Test Selection Strategies</summary>

- *Sebastian Ruland, Malte Lochau*

- `2207.12733v1` - [abs](http://arxiv.org/abs/2207.12733v1) - [pdf](http://arxiv.org/pdf/2207.12733v1)

> Unit testing is one of the most established quality-assurance techniques for software development. One major advantage of unit testing is the adjustable trade-off between efficiency (i.e., testing effort) and effectiveness (i.e., fault-detection probability). To this end, various strategies have been proposed to exploit this trade-off. In particular, test-suite reduction (TSR) reduces the number of (presumably redundant) test cases while testing a single program version. Regression-test selection (RTS) selects test cases for testing consecutive program revisions. However, both TSR and RTS may influence -- or even obstruct -- each others' performance when used in combination. For instance, test cases discarded during TSR for a particular program version may become relevant again for RTS. However, finding a combination of both strategies leading to a reasonable trade-off throughout the version history of a program is an open question. The goal of this paper is to gain a better understanding of the interactions between TSR and RTS with respect to efficiency and effectiveness. To this end, we present a configurable framework called RegreTS for automated unit-testing of C programs. The framework comprises different strategies for TSR and RTS and possible combinations thereof. We apply this framework to a collection of subject systems, delivering several crucial insights. First, TSR has almost always a negative impact on the effectiveness of RTS, yet a positive impact on efficiency. Second, test cases revealing to testers the effect of program modifications between consecutive program versions are far more effective than test cases simply covering modified code parts, yet causing much more testing effort.

</details>

<details>

<summary>2022-07-26 08:51:47 - Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification</summary>

- *Yudi Zhao, Kuangrong Hao, Chaochen Gu, Bing Wei*

- `2207.12744v1` - [abs](http://arxiv.org/abs/2207.12744v1) - [pdf](http://arxiv.org/pdf/2207.12744v1)

> To address the trade-off problem of quality-diversity for the generated images in imbalanced classification tasks, we research on over-sampling based methods at the feature level instead of the data level and focus on searching the latent feature space for optimal distributions. On this basis, we propose an iMproved Estimation Distribution Algorithm based Latent featUre Distribution Evolution (MEDA_LUDE) algorithm, where a joint learning procedure is programmed to make the latent features both optimized and evolved by the deep neural networks and the evolutionary algorithm, respectively. We explore the effect of the Large-margin Gaussian Mixture (L-GM) loss function on distribution learning and design a specialized fitness function based on the similarities among samples to increase diversity. Extensive experiments on benchmark based imbalanced datasets validate the effectiveness of our proposed algorithm, which can generate images with both quality and diversity. Furthermore, the MEDA_LUDE algorithm is also applied to the industrial field and successfully alleviates the imbalanced issue in fabric defect classification.

</details>

<details>

<summary>2022-07-26 09:15:37 - Using Abstraction for Interpretable Robot Programs in Stochastic Domains</summary>

- *Till Hofmann, Vaishak Belle*

- `2207.12763v1` - [abs](http://arxiv.org/abs/2207.12763v1) - [pdf](http://arxiv.org/pdf/2207.12763v1)

> A robot's actions are inherently stochastic, as its sensors are noisy and its actions do not always have the intended effects. For this reason, the agent language Golog has been extended to models with degrees of belief and stochastic actions. While this allows more precise robot models, the resulting programs are much harder to comprehend, because they need to deal with the noise, e.g., by looping until some desired state has been reached with certainty, and because the resulting action traces consist of a large number of actions cluttered with sensor noise. To alleviate these issues, we propose to use abstraction. We define a high-level and nonstochastic model of the robot and then map the high-level model into the lower-level stochastic model. The resulting programs are much easier to understand, often do not require belief operators or loops, and produce much shorter action traces.

</details>

<details>

<summary>2022-07-26 11:49:55 - Folding over Neural Networks</summary>

- *Minh Nguyen, Nicolas Wu*

- `2207.01090v2` - [abs](http://arxiv.org/abs/2207.01090v2) - [pdf](http://arxiv.org/pdf/2207.01090v2)

> Neural networks are typically represented as data structures that are traversed either through iteration or by manual chaining of method calls. However, a deeper analysis reveals that structured recursion can be used instead, so that traversal is directed by the structure of the network itself. This paper shows how such an approach can be realised in Haskell, by encoding neural networks as recursive data types, and then their training as recursion scheme patterns. In turn, we promote a coherent implementation of neural networks that delineates between their structure and semantics, allowing for compositionality in both how they are built and how they are trained.

</details>

<details>

<summary>2022-07-26 12:59:29 - On Integer Programming, Discrepancy, and Convolution</summary>

- *Klaus Jansen, Lars Rohwedder*

- `1803.04744v4` - [abs](http://arxiv.org/abs/1803.04744v4) - [pdf](http://arxiv.org/pdf/1803.04744v4)

> Integer programs with m constraints are solvable in pseudo-polynomial time in $\Delta$, the largest coefficient in a constraint, when m is a fixed constant. We give a new algorithm with a running time of $O(\sqrt{m}\Delta)^{2m} + O(nm)$, which improves on the state-of-the-art. Moreover, we show that improving on our algorithm for any $m$ is equivalent to improving over the quadratic time algorithm for $(\min,~+)$-convolution. This is a strong evidence that our algorithm's running time is the best possible. We also present a specialized algorithm with running time $O(\sqrt{m} \Delta)^{(1 + o(1))m} + O(nm)$ for testing feasibility of an integer program and also give a tight lower bound, which is based on the SETH in this case.

</details>

<details>

<summary>2022-07-26 13:02:32 - A Deep Learning Framework for Wind Turbine Repair Action Prediction Using Alarm Sequences and Long Short Term Memory Algorithms</summary>

- *Connor Walker, Callum Rothon, Koorosh Aslansefat, Yiannis Papadopoulos, Nina Dethlefs*

- `2207.09457v2` - [abs](http://arxiv.org/abs/2207.09457v2) - [pdf](http://arxiv.org/pdf/2207.09457v2)

> With an increasing emphasis on driving down the costs of Operations and Maintenance (O&M) in the Offshore Wind (OSW) sector, comes the requirement to explore new methodology and applications of Deep Learning (DL) to the domain. Condition-based monitoring (CBM) has been at the forefront of recent research developing alarm-based systems and data-driven decision making. This paper provides a brief insight into the research being conducted in this area, with a specific focus on alarm sequence modelling and the associated challenges faced in its implementation. The paper proposes a novel idea to predict a set of relevant repair actions from an input sequence of alarm sequences, comparing Long Short-term Memory (LSTM) and Bidirectional LSTM (biLSTM) models. Achieving training accuracy results of up to 80.23%, and test accuracy results of up to 76.01% with biLSTM gives a strong indication to the potential benefits of the proposed approach that can be furthered in future research. The paper introduces a framework that integrates the proposed approach into O$\&$M procedures and discusses the potential benefits which include the reduction of a confusing plethora of alarms, as well as unnecessary vessel transfers to the turbines for fault diagnosis and correction.

</details>

<details>

<summary>2022-07-26 15:34:10 - Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-based Policy Learning</summary>

- *Zeren Huang, Wenhao Chen, Weinan Zhang, Chuhan Shi, Furui Liu, Hui-Ling Zhen, Mingxuan Yuan, Jianye Hao, Yong Yu, Jun Wang*

- `2207.13701v1` - [abs](http://arxiv.org/abs/2207.13701v1) - [pdf](http://arxiv.org/pdf/2207.13701v1)

> Deriving a good variable selection strategy in branch-and-bound is essential for the efficiency of modern mixed-integer programming (MIP) solvers. With MIP branching data collected during the previous solution process, learning to branch methods have recently become superior over heuristics. As branch-and-bound is naturally a sequential decision making task, one should learn to optimize the utility of the whole MIP solving process instead of being myopic on each step. In this work, we formulate learning to branch as an offline reinforcement learning (RL) problem, and propose a long-sighted hybrid search scheme to construct the offline MIP dataset, which values the long-term utilities of branching decisions. During the policy training phase, we deploy a ranking-based reward assignment scheme to distinguish the promising samples from the long-term or short-term view, and train the branching model named Branch Ranking via offline policy learning. Experiments on synthetic MIP benchmarks and real-world tasks demonstrate that Branch Rankink is more efficient and robust, and can better generalize to large scales of MIP instances compared to the widely used heuristics and state-of-the-art learning-based branching models.

</details>

<details>

<summary>2022-07-26 22:03:35 - Exploring Representation of Horn Clauses using GNNs (Extended Technical Report)</summary>

- *Chencheng Liang, Philipp RÃ¼mmer, Marc Brockschmidt*

- `2206.06986v4` - [abs](http://arxiv.org/abs/2206.06986v4) - [pdf](http://arxiv.org/pdf/2206.06986v4)

> Learning program semantics from raw source code is challenging due to the complexity of real-world programming language syntax and due to the difficulty of reconstructing long-distance relational information implicitly represented in programs using identifiers. Addressing the first point, we consider Constrained Horn Clauses (CHCs) as a standard representation of program verification problems, providing a simple and programming language-independent syntax. For the second challenge, we explore graph representations of CHCs, and propose a new Relational Hypergraph Neural Network (R-HyGNN) architecture to learn program features. We introduce two different graph representations of CHCs. One is called constraint graph (CG), and emphasizes syntactic information of CHCs by translating the symbols and their relations in CHCs as typed nodes and binary edges, respectively, and constructing the constraints as abstract syntax trees. The second one is called control- and data-flow hypergraph (CDHG), and emphasizes semantic information of CHCs by representing the control and data flow through ternary hyperedges. We then propose a new GNN architecture, R-HyGNN, extending Relational Graph Convolutional Networks, to handle hypergraphs. To evaluate the ability of R-HyGNN to extract semantic information from programs, we use R-HyGNNs to train models on the two graph representations, and on five proxy tasks with increasing difficulty, using benchmarks from CHC-COMP 2021 as training data. The most difficult proxy task requires the model to predict the occurrence of clauses in counter-examples, which subsumes satisfiability of CHCs. CDHG achieves 90.59% accuracy in this task. Furthermore, R-HyGNN has perfect predictions on one of the graphs consisting of more than 290 clauses. Overall, our experiments indicate that R-HyGNN can capture intricate program features for guiding verification problems.

</details>

<details>

<summary>2022-07-27 00:29:32 - A Multicriteria Evaluation for Data-Driven Programming Feedback Systems: Accuracy, Effectiveness, Fallibility, and Students' Response</summary>

- *Preya Shabrina, Samiha Marwan, Andrew Bennison, Min Chi, Thomas Price, Tiffany Barnes*

- `2208.05326v1` - [abs](http://arxiv.org/abs/2208.05326v1) - [pdf](http://arxiv.org/pdf/2208.05326v1)

> Data-driven programming feedback systems can help novices to program in the absence of a human tutor. Prior evaluations showed that these systems improve learning in terms of test scores, or task completion efficiency. However, crucial aspects which can impact learning or reveal insights important for future improvement of such systems are ignored in these evaluations. These aspects include inherent fallibility of current state-of-the-art, students' programming behavior in response to correct/incorrect feedback, and effective/ineffective system components. Consequently, a great deal of knowledge is yet to be discovered about such systems. In this paper, we apply a multi-criteria evaluation with 5 criteria on a data-driven feedback system integrated within a block-based novice programming environment. Each criterion in the evaluation reveals a unique pivotal aspect of the system: 1) How accurate the feedback system is; 2) How it guides students throughout programming tasks; 3) How it helps students in task completion; 4) What happens when it goes wrong; and 5) How students respond generally to the system. Our evaluation results showed that the system was helpful to students due to its effective design and feedback representation despite being fallible. However, novices can be negatively impacted by this fallibility due to high reliance and lack of self-evaluation. The negative impacts include increased working time, implementation, or submission of incorrect/partially correct solutions. The evaluation results reinforced the necessity of multi-criteria system evaluations while revealing important insights helpful to ensuring proper usage of data-driven feedback systems, designing fallibility mitigation steps, and driving research for future improvement.

</details>

<details>

<summary>2022-07-27 00:54:13 - Identifying hidden coalitions in the US House of Representatives by optimally partitioning signed networks based on generalized balance</summary>

- *Samin Aref, Zachary P. Neal*

- `2105.01913v4` - [abs](http://arxiv.org/abs/2105.01913v4) - [pdf](http://arxiv.org/pdf/2105.01913v4)

> In network science, identifying optimal partitions of a signed network into internally cohesive and mutually divisive clusters based on generalized balance theory is computationally challenging. We reformulate and generalize two binary linear programming models that tackle this challenge, demonstrating their practicality by applying them them to partition networks of collaboration in the US House of Representatives. These models guarantee a globally optimal network partition and can be practically applied to signed networks containing up to 30,000 edges. In the US House context, we find that a three-cluster partition is better than a conventional two-cluster partition, where the otherwise hidden third coalition is composed of highly effective legislators who are ideologically aligned with the majority party.

</details>

<details>

<summary>2022-07-27 02:47:07 - Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition</summary>

- *Wangmeng Xiang, Chao Li, Biao Wang, Xihan Wei, Xian-Sheng Hua, Lei Zhang*

- `2207.13259v1` - [abs](http://arxiv.org/abs/2207.13259v1) - [pdf](http://arxiv.org/pdf/2207.13259v1)

> Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 & V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https://github.com/MartinXM/TPS.

</details>

<details>

<summary>2022-07-27 08:32:23 - POSET-RL: Phase ordering for Optimizing Size and Execution Time using Reinforcement Learning</summary>

- *Shalini Jain, Yashas Andaluri, S. VenkataKeerthy, Ramakrishna Upadrasta*

- `2208.04238v1` - [abs](http://arxiv.org/abs/2208.04238v1) - [pdf](http://arxiv.org/pdf/2208.04238v1)

> The ever increasing memory requirements of several applications has led to increased demands which might not be met by embedded devices. Constraining the usage of memory in such cases is of paramount importance. It is important that such code size improvements should not have a negative impact on the runtime. Improving the execution time while optimizing for code size is a non-trivial but a significant task. The ordering of standard optimization sequences in modern compilers is fixed, and are heuristically created by the compiler domain experts based on their expertise. However, this ordering is sub-optimal, and does not generalize well across all the cases. We present a reinforcement learning based solution to the phase ordering problem, where the ordering improves both the execution time and code size. We propose two different approaches to model the sequences: one by manual ordering, and other based on a graph called Oz Dependence Graph (ODG). Our approach uses minimal data as training set, and is integrated with LLVM. We show results on x86 and AArch64 architectures on the benchmarks from SPEC-CPU 2006, SPEC-CPU 2017 and MiBench. We observe that the proposed model based on ODG outperforms the current Oz sequence both in terms of size and execution time by 6.19% and 11.99% in SPEC 2017 benchmarks, on an average.

</details>

<details>

<summary>2022-07-27 09:22:59 - FishFuzz: Throwing Larger Nets to Catch Deeper Bugs</summary>

- *Han Zheng, Jiayuan Zhang, Yuhang Huang, Zezhong Ren, He Wang, Chunjie Cao, Yuqing Zhang, Flavio Toffalini, Mathias Payer*

- `2207.13393v1` - [abs](http://arxiv.org/abs/2207.13393v1) - [pdf](http://arxiv.org/pdf/2207.13393v1)

> Greybox fuzzing is the de-facto standard to discover bugs during development. Fuzzers execute many inputs to maximize the amount of reached code. Recently, Directed Greybox Fuzzers (DGFs) propose an alternative strategy that goes beyond "just" coverage: driving testing toward specific code targets by selecting "closer" seeds. DGFs go through different phases: exploration (i.e., reaching interesting locations) and exploitation (i.e., triggering bugs). In practice, DGFs leverage coverage to directly measure exploration, while exploitation is, at best, measured indirectly by alternating between different targets. Specifically, we observe two limitations in existing DGFs: (i) they lack precision in their distance metric, i.e., averaging multiple paths and targets into a single score (to decide which seeds to prioritize), and (ii) they assign energy to seeds in a round-robin fashion without adjusting the priority of the targets (exhaustively explored targets should be dropped).   We propose FishFuzz, which draws inspiration from trawl fishing: first casting a wide net, scraping for high coverage, then slowly pulling it in to maximize the harvest. The core of our fuzzer is a novel seed selection strategy that builds on two concepts: (i) a novel multi-distance metric whose precision is independent of the number of targets, and (ii) a dynamic target ranking to automatically discard exhausted targets. This strategy allows FishFuzz to seamlessly scale to tens of thousands of targets and dynamically alternate between exploration and exploitation phases. We evaluate FishFuzz by leveraging all sanitizer labels as targets. Extensively comparing FishFuzz against modern DGFs and coverage-guided fuzzers shows that FishFuzz reached higher coverage compared to the direct competitors, reproduces existing bugs (70.2% faster), and finally discovers 25 new bugs (18 CVEs) in 44 programs.

</details>

<details>

<summary>2022-07-27 13:03:23 - The NP-hard problem of computing the maximal sample variance over interval data is solvable in almost linear time with high probability</summary>

- *Miroslav Rada, Michal ÄernÃ½, OndÅej Sokol*

- `1905.07821v4` - [abs](http://arxiv.org/abs/1905.07821v4) - [pdf](http://arxiv.org/pdf/1905.07821v4)

> We consider the algorithm by Ferson et al. (Reliable computing 11(3), p. 207-233, 2005) designed for solving the NP-hard problem of computing the maximal sample variance over interval data, motivated by robust statistics (in fact, the formulation can be written as a nonconvex quadratic program with a specific structure). First, we propose a new version of the algorithm improving its original time bound $O(n^2 2^\omega)$ to $O(n \log n+n\cdot 2^\omega)$, where $n$ is number of input data and $\omega$ is the clique number in a certain intersection graph. Then we treat input data as random variables as it is usual in statistics) and introduce a natural probabilistic data generating model. We get $2^\omega = O(n^{1/\log\log n})$ and $\omega = O(\log n / \log\log n)$ on average. This results in average computing time $O(n^{1+\epsilon})$ for $\epsilon > 0$ arbitrarily small, which may be considered as "surprisingly good" average time complexity for solving an NP-hard problem. Moreover, we prove the following tail bound on the distribution of computation time: hard instances, forcing the algorithm to compute in time $2^{\Omega(n)}$, occur rarely, with probability tending to zero at the rate $e^{-n\log\log n}$.

</details>

<details>

<summary>2022-07-27 17:33:29 - Unsupervised Training for Neural TSP Solver</summary>

- *ElÄ«za Gaile, Andis Draguns, EmÄ«ls OzoliÅÅ¡, KÄrlis Freivalds*

- `2207.13667v1` - [abs](http://arxiv.org/abs/2207.13667v1) - [pdf](http://arxiv.org/pdf/2207.13667v1)

> There has been a growing number of machine learning methods for approximately solving the travelling salesman problem. However, these methods often require solved instances for training or use complex reinforcement learning approaches that need a large amount of tuning. To avoid these problems, we introduce a novel unsupervised learning approach. We use a relaxation of an integer linear program for TSP to construct a loss function that does not require correct instance labels. With variable discretization, its minimum coincides with the optimal or near-optimal solution. Furthermore, this loss function is differentiable and thus can be used to train neural networks directly. We use our loss function with a Graph Neural Network and design controlled experiments on both Euclidean and asymmetric TSP. Our approach has the advantage over supervised learning of not requiring large labelled datasets. In addition, the performance of our approach surpasses reinforcement learning for asymmetric TSP and is comparable to reinforcement learning for Euclidean instances. Our approach is also more stable and easier to train than reinforcement learning.

</details>

<details>

<summary>2022-07-27 23:27:21 - Will AI Make Cyber Swords or Shields: A few mathematical models of technological progress</summary>

- *Andrew J Lohn, Krystal Alex Jackson*

- `2207.13825v1` - [abs](http://arxiv.org/abs/2207.13825v1) - [pdf](http://arxiv.org/pdf/2207.13825v1)

> We aim to demonstrate the value of mathematical models for policy debates about technological progress in cybersecurity by considering phishing, vulnerability discovery, and the dynamics between patching and exploitation. We then adjust the inputs to those mathematical models to match some possible advances in their underlying technology. We find that AI's impact on phishing may be overestimated but could lead to more attacks going undetected. Advances in vulnerability discovery have the potential to help attackers more than defenders. And automation that writes exploits is more useful to attackers than automation that writes patches, although advances that help deploy patches faster have the potential to be more impactful than either.

</details>

<details>

<summary>2022-07-27 23:36:22 - Declarative Smart Contracts</summary>

- *Haoxian Chen, Gerald Whitters, Mohammad Javad Amiri, Yuepeng Wang, Boon Thau Loo*

- `2207.13827v1` - [abs](http://arxiv.org/abs/2207.13827v1) - [pdf](http://arxiv.org/pdf/2207.13827v1)

> This paper presents DeCon, a declarative programming language for implementing smart contracts and specifying contract-level properties. Driven by the observation that smart contract operations and contract-level properties can be naturally expressed as relational constraints, DeCon models each smart contract as a set of relational tables that store transaction records. This relational representation of smart contracts enables convenient specification of contract properties, facilitates run-time monitoring of potential property violations, and brings clarity to contract debugging via data provenance. Specifically, a DeCon program consists of a set of declarative rules and violation query rules over the relational representation, describing the smart contract implementation and contract-level properties, respectively. We have developed a tool that can compile DeCon programs into executable Solidity programs, with instrumentation for run-time property monitoring. Our case studies demonstrate that DeCon can implement realistic smart contracts such as ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal overhead of DeCon compared to the open-source reference implementation, incurring 14% median gas overhead for execution, and another 16% median gas overhead for run-time verification.

</details>

<details>

<summary>2022-07-28 12:16:37 - Using Graph Neural Networks for Program Termination</summary>

- *Yoav Alon, Cristina David*

- `2207.14648v1` - [abs](http://arxiv.org/abs/2207.14648v1) - [pdf](http://arxiv.org/pdf/2207.14648v1)

> Termination analyses investigate the termination behavior of programs, intending to detect nontermination, which is known to cause a variety of program bugs (e.g. hanging programs, denial-of-service vulnerabilities). Beyond formal approaches, various attempts have been made to estimate the termination behavior of programs using neural networks. However, the majority of these approaches continue to rely on formal methods to provide strong soundness guarantees and consequently suffer from similar limitations. In this paper, we move away from formal methods and embrace the stochastic nature of machine learning models. Instead of aiming for rigorous guarantees that can be interpreted by solvers, our objective is to provide an estimation of a program's termination behavior and of the likely reason for nontermination (when applicable) that a programmer can use for debugging purposes. Compared to previous approaches using neural networks for program termination, we also take advantage of the graph representation of programs by employing Graph Neural Networks. To further assist programmers in understanding and debugging nontermination bugs, we adapt the notions of attention and semantic segmentation, previously used for other application domains, to programs. Overall, we designed and implemented classifiers for program termination based on Graph Convolutional Networks and Graph Attention Networks, as well as a semantic segmentation Graph Neural Network that localizes AST nodes likely to cause nontermination. We also illustrated how the information provided by semantic segmentation can be combined with program slicing to further aid debugging.

</details>

<details>

<summary>2022-07-28 16:49:03 - Electricity Price Forecasting Model based on Gated Recurrent Units</summary>

- *Nafise Rezaei, Roozbeh Rajabi, Abouzar Estebsari*

- `2207.14225v1` - [abs](http://arxiv.org/abs/2207.14225v1) - [pdf](http://arxiv.org/pdf/2207.14225v1)

> The participation of consumers and producers in demand response programs has increased in smart grids, which reduces investment and operation costs of power systems. Also, with the advent of renewable energy sources, the electricity market is becoming more complex and unpredictable. To effectively implement demand response programs, forecasting the future price of electricity is very crucial for producers in the electricity market. Electricity prices are very volatile and change under the influence of various factors such as temperature, wind speed, rainfall, intensity of commercial and daily activities, etc. Therefore, considering the influencing factors as dependent variables can increase the accuracy of the forecast. In this paper, a model for electricity price forecasting is presented based on Gated Recurrent Units. The electrical load consumption is considered as an input variable in this model. Noise in electricity price seriously reduces the efficiency and effectiveness of analysis. Therefore, an adaptive noise reducer is integrated into the model for noise reduction. The SAEs are then used to extract features from the de-noised electricity price. Finally, the de-noised features are fed into the GRU to train predictor. Results on real dataset shows that the proposed methodology can perform effectively in prediction of electricity price.

</details>

<details>

<summary>2022-07-28 16:51:14 - Language Model Cascades</summary>

- *David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton*

- `2207.10342v2` - [abs](http://arxiv.org/abs/2207.10342v2) - [pdf](http://arxiv.org/pdf/2207.10342v2)

> Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.

</details>

<details>

<summary>2022-07-28 18:15:15 - CheckINN: Wide Range Neural Network Verification in Imandra (Extended)</summary>

- *Remi Desmartin, Grant Passmore, Ekaterina Komendantskaya, Matthew Daggitt*

- `2207.10562v2` - [abs](http://arxiv.org/abs/2207.10562v2) - [pdf](http://arxiv.org/pdf/2207.10562v2)

> Neural networks are increasingly relied upon as components of complex safety-critical systems such as autonomous vehicles. There is high demand for tools and methods that embed neural network verification in a larger verification cycle. However, neural network verification is difficult due to a wide range of verification properties of interest, each typically only amenable to verification in specialised solvers. In this paper, we show how Imandra, a functional programming language and a theorem prover originally designed for verification, validation and simulation of financial infrastructure can offer a holistic infrastructure for neural network verification. We develop a novel library CheckINN that formalises neural networks in Imandra, and covers different important facets of neural network verification.

</details>

<details>

<summary>2022-07-29 03:01:06 - Designing Programming Exercises from Board Games</summary>

- *Maxim Mozgovoy, Marina Purgina*

- `2208.00823v1` - [abs](http://arxiv.org/abs/2208.00823v1) - [pdf](http://arxiv.org/pdf/2208.00823v1)

> This paper introduces a collection of board games specifically chosen to serve as a basis for programming exercises. We examine the attractiveness of board games in this context as well as features that make a particular game a good exercise. The collection is annotated across several dimensions to assist choosing a game suitable for the target topic and student level. We discuss possible changes into exercise tasks to make them more challenging and introduce new topics. The work relies on established topics taxonomy and board games resources which makes extending the current collection easy.

</details>

<details>

<summary>2022-07-29 10:21:05 - Pattern matching algorithms in Blockchain for network fees reduction</summary>

- *Robert Susik, Robert Nowotniak*

- `2207.14592v1` - [abs](http://arxiv.org/abs/2207.14592v1) - [pdf](http://arxiv.org/pdf/2207.14592v1)

> Blockchain received a vast amount of attention in recent years and is still growing. The second generation of blockchain, such as Ethereum, allows execution of almost any program in Ethereum Virtual Machine (EVM), making it a global protocol for distributed applications. The code deployment and each operation performed in EVM cost the network fee called gas, which price varies and can be significant. That is why code optimization and well-chosen algorithms are crucial in programming on the blockchain. This paper evaluates the gas usage of several exact pattern matching algorithms on the Ethereum Virtual Machine. We also propose an efficient implementation of the algorithms in the Solidity/YUL language. We evaluate the gas fees of all the algorithms for different parameters (such as pattern length, alphabet size, and text size). We show a significant gas fee and execution time reduction with up to 22-fold lower gas usage and 55-fold speed-up comparing to StringUtils (a popular Solidity string library).

</details>

<details>

<summary>2022-07-29 10:41:02 - To what extent can we analyze Kotlin programs using existing Java taint analysis tools? (Extended Version)</summary>

- *Ranjith Krishnamurthy, Goran Piskachev, Eric Bodden*

- `2207.09379v2` - [abs](http://arxiv.org/abs/2207.09379v2) - [pdf](http://arxiv.org/pdf/2207.09379v2)

> As an alternative to Java, Kotlin has gained rapid popularity since its introduction and has become the default choice for developing Android apps. However, due to its interoperability with Java, Kotlin programs may contain almost the same security vulnerabilities as their Java counterparts. Hence, we question: to what extent can one use an existing Java static taint analysis on Kotlin code? In this paper, we investigate the challenges in implementing a taint analysis for Kotlin compared to Java. To answer this question, we performed an exploratory study where each Kotlin construct was examined and compared to its Java equivalent. We identified 18 engineering challenges that static-analysis writers need to handle differently due to Kotlin's unique constructs or the differences in the generated bytecode between the Kotlin and Java compilers. For eight of them, we provide a conceptual solution, while six of those we implemented as part of SecuCheck-Kotlin, an extension to the existing Java taint analysis SecuCheck.

</details>

<details>

<summary>2022-07-29 14:13:29 - Philosophy-Guided Mathematical Formalism for Complex Systems Modelling</summary>

- *Patrik Christen, Olivier Del Fabbro*

- `2005.01192v5` - [abs](http://arxiv.org/abs/2005.01192v5) - [pdf](http://arxiv.org/pdf/2005.01192v5)

> We recently presented the so-called allagmatic method, which includes a system metamodel providing a framework for describing, modelling, simulating, and interpreting complex systems. Its development and programming was guided by philosophy, especially by Gilbert Simondon's philosophy of individuation, Alfred North Whitehead's philosophy of organism, and concepts from cybernetics. Here, a mathematical formalism is presented to better describe and define the system metamodel of the allagmatic method, thereby further generalising it and extending its reach to a more formal treatment and allowing more theoretical studies. By using the formalism, an example for such a further study is provided with mathematical definitions and proofs for model creation and equivalence of cellular automata and artificial neural networks.

</details>

<details>

<summary>2022-07-29 14:23:51 - Programming Data Structures for Large-Scale Desktop Simulations of Complex Systems</summary>

- *Patrik Christen*

- `2205.04837v2` - [abs](http://arxiv.org/abs/2205.04837v2) - [pdf](http://arxiv.org/pdf/2205.04837v2)

> The investigation of complex systems requires running large-scale simulations over many temporal iterations. It is therefore important to provide efficient implementations. The present study borrows philosophical concepts from Gilbert Simondon to identify data structures and algorithms that have the biggest impact on running time and memory usage. These are the entity $e$-tuple $\mathcal{E}$ and the intertwined update function $\phi$. Focusing on implementing data structures in C#, $\mathcal{E}$ is implemented as a list of objects according to current software engineering practice and as an array of pointers according to theoretical considerations. Cellular automaton simulations with $10^9$ entities over one iteration reveal that the object-list with dynamic typing and multi-state readiness has a drastic effect on running time and memory usage, especially dynamic typing as it has a big impact on the evolution time. Pointer-arrays are possible to implement in C# and are more running time and memory efficient as compared to the object-list implementation, however, they are cumbersome to implement. In conclusion, avoiding dynamic typing in object-list based implementations or using pointer-arrays gives evolution times that are acceptable in practice, even on desktop computers.

</details>

<details>

<summary>2022-07-29 16:31:59 - An Algorithmic Theory of Integer Programming</summary>

- *Friedrich Eisenbrand, Christoph HunkenschrÃ¶der, Kim-Manuel Klein, Martin KouteckÃ½, Asaf Levin, Shmuel Onn*

- `1904.01361v3` - [abs](http://arxiv.org/abs/1904.01361v3) - [pdf](http://arxiv.org/pdf/1904.01361v3)

> We study the general integer programming problem where the number of variables $n$ is a variable part of the input. We consider two natural parameters of the constraint matrix $A$: its numeric measure $a$ and its sparsity measure $d$. We show that integer programming can be solved in time $g(a,d)\textrm{poly}(n,L)$, where $g$ is some computable function of the parameters $a$ and $d$, and $L$ is the binary encoding length of the input. In particular, integer programming is fixed-parameter tractable parameterized by $a$ and $d$, and is solvable in polynomial time for every fixed $a$ and $d$. Our results also extend to nonlinear separable convex objective functions. Moreover, for linear objectives, we derive a strongly-polynomial algorithm, that is, with running time $g(a,d)\textrm{poly}(n)$, independent of the rest of the input data.   We obtain these results by developing an algorithmic framework based on the idea of iterative augmentation: starting from an initial feasible solution, we show how to quickly find augmenting steps which rapidly converge to an optimum. A central notion in this framework is the Graver basis of the matrix $A$, which constitutes a set of fundamental augmenting steps. The iterative augmentation idea is then enhanced via the use of other techniques such as new and improved bounds on the Graver basis, rapid solution of integer programs with bounded variables, proximity theorems and a new proximity-scaling algorithm, the notion of a reduced objective function, and others.   As a consequence of our work, we advance the state of the art of solving block-structured integer programs. In particular, we develop near-linear time algorithms for $n$-fold, tree-fold, and $2$-stage stochastic integer programs. We also discuss some of the many applications of these classes.

</details>

<details>

<summary>2022-07-29 19:25:39 - Capacitated Vehicle Routing Problem Using Conventional and Approximation Method</summary>

- *Apurv Choudhari, Ameya Ekbote, Prerona Chaudhuri*

- `2208.00046v1` - [abs](http://arxiv.org/abs/2208.00046v1) - [pdf](http://arxiv.org/pdf/2208.00046v1)

> This paper attempts to solve the famous Vehicle Routing Problem by considering multiple constraints including capacitated vehicles, single depot, and distance using two approaches namely, cluster first and route the second algorithm and using integer linear programming. A set of nodes are provided as input to the system and a feasible route is generated as output, giving clusters of nodes and the route to be traveled within the cluster. For clustering the nodes, we have adopted the DBSCAN algorithm, and the routing is done using the approximation algorithm, Christofide's algorithm. The solution generated can be employed for solving real-life situations, like delivery systems consisting of various demand nodes.

</details>

<details>

<summary>2022-07-29 20:33:23 - Topology-Driven Generative Completion of Lacunae in Molecular Data</summary>

- *Dmitry Yu. Zubarev, Petar Ristoski*

- `2208.00063v1` - [abs](http://arxiv.org/abs/2208.00063v1) - [pdf](http://arxiv.org/pdf/2208.00063v1)

> We introduce an approach to the targeted completion of lacunae in molecular data sets which is driven by topological data analysis, such as Mapper algorithm. Lacunae are filled in using scaffold-constrained generative models trained with different scoring functions. The approach enables addition of links and vertices to the skeletonized representations of the data, such as Mapper graph, and falls in the broad category of network completion methods. We illustrate application of the topology-driven data completion strategy by creating a lacuna in the data set of onium cations extracted from USPTO patents, and repairing it.

</details>

<details>

<summary>2022-07-29 21:59:29 - Low-complexity Approximate Convolutional Neural Networks</summary>

- *R. J. Cintra, S. Duffner, C. Garcia, A. Leite*

- `2208.00087v1` - [abs](http://arxiv.org/abs/2208.00087v1) - [pdf](http://arxiv.org/pdf/2208.00087v1)

> In this paper, we present an approach for minimizing the computational complexity of trained Convolutional Neural Networks (ConvNet). The idea is to approximate all elements of a given ConvNet and replace the original convolutional filters and parameters (pooling and bias coefficients; and activation function) with efficient approximations capable of extreme reductions in computational complexity. Low-complexity convolution filters are obtained through a binary (zero-one) linear programming scheme based on the Frobenius norm over sets of dyadic rationals. The resulting matrices allow for multiplication-free computations requiring only addition and bit-shifting operations. Such low-complexity structures pave the way for low-power, efficient hardware designs. We applied our approach on three use cases of different complexity: (i) a "light" but efficient ConvNet for face detection (with around 1000 parameters); (ii) another one for hand-written digit classification (with more than 180000 parameters); and (iii) a significantly larger ConvNet: AlexNet with $\approx$1.2 million matrices. We evaluated the overall performance on the respective tasks for different levels of approximations. In all considered applications, very low-complexity approximations have been derived maintaining an almost equal classification performance.

</details>

<details>

<summary>2022-07-29 22:39:54 - PennyLane: Automatic differentiation of hybrid quantum-classical computations</summary>

- *Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Shahnawaz Ahmed, Vishnu Ajith, M. Sohaib Alam, Guillermo Alonso-Linaje, B. AkashNarayanan, Ali Asadi, Juan Miguel Arrazola, Utkarsh Azad, Sam Banning, Carsten Blank, Thomas R Bromley, Benjamin A. Cordier, Jack Ceroni, Alain Delgado, Olivia Di Matteo, Amintor Dusko, Tanya Garg, Diego Guala, Anthony Hayes, Ryan Hill, Aroosa Ijaz, Theodor Isacsson, David Ittah, Soran Jahangiri, Prateek Jain, Edward Jiang, Ankit Khandelwal, Korbinian Kottmann, Robert A. Lang, Christina Lee, Thomas Loke, Angus Lowe, Keri McKiernan, Johannes Jakob Meyer, J. A. MontaÃ±ez-Barrera, Romain Moyard, Zeyue Niu, Lee James O'Riordan, Steven Oud, Ashish Panigrahi, Chae-Yeun Park, Daniel Polatajko, NicolÃ¡s Quesada, Chase Roberts, Nahum SÃ¡, Isidor Schoch, Borun Shi, Shuli Shu, Sukin Sim, Arshpreet Singh, Ingrid Strandberg, Jay Soni, Antal SzÃ¡va, Slimane Thabet, Rodrigo A. Vargas-HernÃ¡ndez, Trevor Vincent, Nicola Vitucci, Maurice Weber, David Wierichs, Roeland Wiersema, Moritz Willmann, Vincent Wong, Shaoming Zhang, Nathan Killoran*

- `1811.04968v4` - [abs](http://arxiv.org/abs/1811.04968v4) - [pdf](http://arxiv.org/pdf/1811.04968v4)

> PennyLane is a Python 3 software framework for differentiable programming of quantum computers. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for hardware providers including the Xanadu Cloud, Amazon Braket, and IBM Quantum, allowing PennyLane optimizations to be run on publicly accessible quantum devices. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, JAX, and Autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.

</details>

<details>

<summary>2022-07-30 16:14:36 - Mining unit test cases to synthesize API usage examples</summary>

- *Mohammad Ghafari, Konstantin Rubinov, Mohammad Mehdi Pourhashem K*

- `2208.00264v1` - [abs](http://arxiv.org/abs/2208.00264v1) - [pdf](http://arxiv.org/pdf/2208.00264v1)

> Software developers study and reuse existing source code to understand how to properly use application programming interfaces (APIs). However, manually finding sufficient and adequate code examples for a given API is a difficult and a time-consuming activity. Existing approaches to find or generate examples assume availability of a reasonable set of client code that uses the API. This assumption does not hold for newly released API libraries, non-widely used APIs, nor private ones. In this work we reuse the important information that is naturally present in test code to circumvent the lack of usage examples for an API when other sources of client code are not available. We propose an approach for automatically identifying the most representative API uses within each unit test case. We then develop an approach to synthesize API usage examples by extracting relevant statements representing the usage of such APIs. We compare the output of a prototype implementation of our approach to both human-written examples and to a state-of-the-art approach. The obtained results are encouraging; the examples automatically generated with our approach are superior to the state-of-the-art approach and highly similar to the manually constructed examples.

</details>

<details>

<summary>2022-07-30 21:41:10 - An Improved A* Search Algorithm for Road Networks Using New Heuristic Estimation</summary>

- *Kevin Y. Chen*

- `2208.00312v1` - [abs](http://arxiv.org/abs/2208.00312v1) - [pdf](http://arxiv.org/pdf/2208.00312v1)

> Finding the shortest path between two points in a graph is a fundamental problem that has been well-studied over the past several decades. Shortest path algorithms are commonly applied to modern navigation systems, so our study aims to improve the efficiency of an existing algorithm on large-scale Euclidean networks. The current literature lacks a deep understanding of certain algorithms' performance on these types of networks. Therefore, we incorporate a new heuristic function, called the $k$-step look-ahead, into the A* search algorithm and conduct a computational experiment to evaluate and compare the results on road networks of varying sizes. Our main findings are that this new heuristic yields a significant improvement in runtime, particularly for larger networks when compared to standard A*, as well as that a higher value of $k$ is needed to achieve optimal efficiency as network size increases. Future research can build upon this work by implementing a program that automatically chooses an optimal $k$ value given an input network. The results of this study can be applied to GPS routing technologies or other navigation devices to speed up the time needed to find the shortest path from an origin to a destination, an essential objective in daily life.

</details>

<details>

<summary>2022-07-31 02:07:33 - Tai-e: A Static Analysis Framework for Java by Harnessing the Best Designs of Classics</summary>

- *Tian Tan, Yue Li*

- `2208.00337v1` - [abs](http://arxiv.org/abs/2208.00337v1) - [pdf](http://arxiv.org/pdf/2208.00337v1)

> Static analysis is a mature field with applications to bug detection, security analysis, and code optimization, etc. To facilitate these applications, static analysis frameworks play an essential role by providing a series of fundamental services such as program abstraction, control flow graph construction, and points-to/alias information computation, etc. However, despite impressive progress of static analysis, and this field has seen several popular frameworks in the last decades, it is still not clear how a static analysis framework should be designed in a way that analysis developers could benefit more: for example, what a good IR (for analysis) ought to look like? What functionalities should the module of fundamental analyses provide to ease client analyses? How to develop and integrate new analysis conveniently? How to manage multiple analyses?   To answer these questions, in this work, we discuss the design trade-offs for the crucial components of a static analysis framework, and argue for the most appropriate design by following the HBDC (Harnessing the Best Designs of Classics) principle: for each crucial component, we compare the design choices made for it (possibly) by different classic frameworks such as Soot, WALA, SpotBugs and Doop, and choose arguably the best one, but if none is good enough, we then propose a better design. These selected or newly proposed designs finally constitute Tai-e, a new static analysis framework for Java. Specifically, Tai-e is novel in the designs of several aspects like IR, pointer analysis and development of new analyses, etc., leading to an easy-to-learn, easy-to-use and efficient system. To our knowledge, this is the first work that systematically explores the designs and implementations of various static analysis frameworks, and we believe it provides useful materials and viewpoints for building better static analysis infrastructures.

</details>

<details>

<summary>2022-07-31 14:27:14 - Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit</summary>

- *Shiyi Qi, Yaoxian Li, Cuiyun Gao, Xiaohong Su, Shuzheng Gao, Zibin Zheng, Chuanyi Liu*

- `2205.13522v3` - [abs](http://arxiv.org/abs/2205.13522v3) - [pdf](http://arxiv.org/pdf/2205.13522v3)

> Adapting Deep Learning (DL) techniques to automate non-trivial coding activities, such as code documentation and defect detection, has been intensively studied recently. Learning to predict code changes is one of the popular and essential investigations. Prior studies have shown that DL techniques such as Neural Machine Translation (NMT) can benefit meaningful code changes, including bug fixing and code refactoring. However, NMT models may encounter bottleneck when modeling long sequences, thus are limited in accurately predicting code changes. In this work, we design a Transformer-based approach, considering that Transformer has proven effective in capturing long-term dependencies. Specifically, we propose a novel model named DTrans. For better incorporating the local structure of code, i.e., statement-level information in this paper, DTrans is designed with dynamically relative position encoding in the multi-head attention of Transformer. Experiments on benchmark datasets demonstrate that DTrans can more accurately generate patches than the state-of-the-art methods, increasing the performance by at least 5.45\%-46.57\% in terms of the exact match metric on different datasets. Moreover, DTrans can locate the lines to change with 1.75\%-24.21\% higher accuracy than the existing methods.

</details>

<details>

<summary>2022-07-31 16:27:33 - $k$ disjoint $st$-paths activation in polynomial time</summary>

- *Zeev Nutov*

- `2111.04011v2` - [abs](http://arxiv.org/abs/2111.04011v2) - [pdf](http://arxiv.org/pdf/2111.04011v2)

> In activation network design problems we are given an undirected graph $G=(V,E)$ and a pair of activation costs $\{c_e^u,c_e^v\}$ for each $e=uv \in E$. The goal is to find an edge set $F \subseteq E$ that satisfies a prescribed property of minimum activation cost $\tau(F)=\sum_{v \in V} \max \{c_e^v: e \in F \mbox{ is incident to } v\}$. In the Activation $k$ Disjoint Paths problem we are given $s,t \in V$ and an integer $k$, and seek an edge set $F \subseteq E$ of $k$ internally disjoint $st$-paths of minimum activation cost. The problem admits an easy $2$-approximation algorithm. However, it was an open question whether the problem is in P even for $k=2$ and power activation costs, when $c_e^u=c_e^v$ for all $e=uv \in E$. Here we will answer this question by giving a polynomial time algorithm using linear programing. We will also mention several consequences, among them a polynomial time algorithm for the Activation 2 Edge Disjoint Paths problem, and improved approximation ratios for the Min-Power $k$-Connected Subgraph problem.

</details>

<details>

<summary>2022-07-31 18:15:24 - Repairing $\mathcal{EL}$ Ontologies Using Weakening and Completing</summary>

- *Ying Li, Patrick Lambrix*

- `2208.00486v1` - [abs](http://arxiv.org/abs/2208.00486v1) - [pdf](http://arxiv.org/pdf/2208.00486v1)

> The quality of ontologies in terms of their correctness and completeness is crucial for developing high-quality ontology-based applications. Traditional debugging techniques repair ontologies by removing unwanted axioms, but may thereby remove consequences that are correct in the domain of the ontology. In this paper we propose an interactive approach to mitigate this for $\mathcal{EL}$ ontologies by axiom weakening and completing. We present algorithms for weakening and completing and present the first approach for repairing that takes into account removing, weakening and completing. We show different combination strategies, discuss the influence on the final ontologies and show experimental results. We show that previous work has only considered special cases and that there is a trade-off between the amount of validation work for a domain expert and the quality of the ontology in terms of correctness and completeness.

</details>

<details>

<summary>2022-07-31 21:03:40 - Assessing The Performance of YOLOv5 Algorithm for Detecting Volunteer Cotton Plants in Corn Fields at Three Different Growth Stages</summary>

- *Pappu Kumar Yadav, J. Alex Thomasson, Stephen W. Searcy, Robert G. Hardin, Ulisses Braga-Neto, Sorin C. Popescu, Daniel E. Martin, Roberto Rodriguez, Karem Meza, Juan Enciso, Jorge Solorzano Diaz, Tianyi Wang*

- `2208.00519v1` - [abs](http://arxiv.org/abs/2208.00519v1) - [pdf](http://arxiv.org/pdf/2208.00519v1)

> The boll weevil (Anthonomus grandis L.) is a serious pest that primarily feeds on cotton plants. In places like Lower Rio Grande Valley of Texas, due to sub-tropical climatic conditions, cotton plants can grow year-round and therefore the left-over seeds from the previous season during harvest can continue to grow in the middle of rotation crops like corn (Zea mays L.) and sorghum (Sorghum bicolor L.). These feral or volunteer cotton (VC) plants when reach the pinhead squaring phase (5-6 leaf stage) can act as hosts for the boll weevil pest. The Texas Boll Weevil Eradication Program (TBWEP) employs people to locate and eliminate VC plants growing by the side of roads or fields with rotation crops but the ones growing in the middle of fields remain undetected. In this paper, we demonstrate the application of computer vision (CV) algorithm based on You Only Look Once version 5 (YOLOv5) for detecting VC plants growing in the middle of corn fields at three different growth stages (V3, V6, and VT) using unmanned aircraft systems (UAS) remote sensing imagery. All the four variants of YOLOv5 (s, m, l, and x) were used and their performances were compared based on classification accuracy, mean average precision (mAP), and F1-score. It was found that YOLOv5s could detect VC plants with a maximum classification accuracy of 98% and mAP of 96.3 % at the V6 stage of corn while YOLOv5s and YOLOv5m resulted in the lowest classification accuracy of 85% and YOLOv5m and YOLOv5l had the least mAP of 86.5% at the VT stage on images of size 416 x 416 pixels. The developed CV algorithm has the potential to effectively detect and locate VC plants growing in the middle of corn fields as well as expedite the management aspects of TBWEP.

</details>


## 2022-08

<details>

<summary>2022-08-01 04:06:14 - A Real-time Edge-AI System for Reef Surveys</summary>

- *Yang Li, Jiajun Liu, Brano Kusy, Ross Marchant, Brendan Do, Torsten Merz, Joey Crosswell, Andy Steven, Lachlan Tychsen-Smith, David Ahmedt-Aristizabal, Jeremy Oorloff, Peyman Moghadam, Russ Babcock, Megha Malpani, Ard Oerlemans*

- `2208.00598v1` - [abs](http://arxiv.org/abs/2208.00598v1) - [pdf](http://arxiv.org/pdf/2208.00598v1)

> Crown-of-Thorn Starfish (COTS) outbreaks are a major cause of coral loss on the Great Barrier Reef (GBR) and substantial surveillance and control programs are ongoing to manage COTS populations to ecologically sustainable levels. In this paper, we present a comprehensive real-time machine learning-based underwater data collection and curation system on edge devices for COTS monitoring. In particular, we leverage the power of deep learning-based object detection techniques, and propose a resource-efficient COTS detector that performs detection inferences on the edge device to assist marine experts with COTS identification during the data collection phase. The preliminary results show that several strategies for improving computational efficiency (e.g., batch-wise processing, frame skipping, model input size) can be combined to run the proposed detection model on edge hardware with low resource consumption and low information loss.

</details>

<details>

<summary>2022-08-01 10:25:52 - Performance Comparison of Deep RL Algorithms for Energy Systems Optimal Scheduling</summary>

- *Hou Shengren, Edgar Mauricio Salazar, Pedro P. Vergara, Peter Palensky*

- `2208.00728v1` - [abs](http://arxiv.org/abs/2208.00728v1) - [pdf](http://arxiv.org/pdf/2208.00728v1)

> Taking advantage of their data-driven and model-free features, Deep Reinforcement Learning (DRL) algorithms have the potential to deal with the increasing level of uncertainty due to the introduction of renewable-based generation. To deal simultaneously with the energy systems' operational cost and technical constraints (e.g, generation-demand power balance) DRL algorithms must consider a trade-off when designing the reward function. This trade-off introduces extra hyperparameters that impact the DRL algorithms' performance and capability of providing feasible solutions. In this paper, a performance comparison of different DRL algorithms, including DDPG, TD3, SAC, and PPO, are presented. We aim to provide a fair comparison of these DRL algorithms for energy systems optimal scheduling problems. Results show DRL algorithms' capability of providing in real-time good-quality solutions, even in unseen operational scenarios, when compared with a mathematical programming model of the energy system optimal scheduling problem. Nevertheless, in the case of large peak consumption, these algorithms failed to provide feasible solutions, which can impede their practical implementation.

</details>

<details>

<summary>2022-08-01 10:53:25 - e-Genia3 An AgentSpeak extension for empathic agents</summary>

- *Joaquin Taverner, Emilio Vivancos, Vicente Botti*

- `2208.00737v1` - [abs](http://arxiv.org/abs/2208.00737v1) - [pdf](http://arxiv.org/pdf/2208.00737v1)

> In this paper, we present e-Genia3 an extension of AgentSpeak to provide support to the development of empathic agents. The new extension modifies the agent's reasoning processes to select plans according to the analyzed event and the affective state and personality of the agent. In addition, our proposal allows a software agent to simulate the distinction between self and other agents through two different event appraisal processes: the empathic appraisal process, for eliciting emotions as a response to other agents emotions, and the regular affective appraisal process for other non-empathic affective events. The empathic regulation process adapts the elicited empathic emotion based on intrapersonal factors (e.g., the agent's personality and affective memory) and interpersonal characteristics of the agent (e.g., the affective link between the agents). The use of a memory of past events and their corresponding elicited emotions allows the maintaining of an affective link to support long-term empathic interaction between agents.

</details>

<details>

<summary>2022-08-01 19:18:24 - Flaky Test Sanitisation via On-the-Fly Assumption Inference for Tests with Network Dependencies</summary>

- *Jens Dietrich, Shawn Rasheed, Amjed Tahir*

- `2208.01106v1` - [abs](http://arxiv.org/abs/2208.01106v1) - [pdf](http://arxiv.org/pdf/2208.01106v1)

> Flaky tests cause significant problems as they can interrupt automated build processes that rely on all tests succeeding and undermine the trustworthiness of tests. Numerous causes of test flakiness have been identified, and program analyses exist to detect such tests. Typically, these methods produce advice to developers on how to refactor tests in order to make test outcomes deterministic. We argue that one source of flakiness is the lack of assumptions that precisely describe under which circumstances a test is meaningful. We devise a sanitisation technique that can isolate f laky tests quickly by inferring such assumptions on-the-fly, allowing automated builds to proceed as flaky tests are ignored. We demonstrate this approach for Java and Groovy programs by implementing it as extensions for three popular testing frameworks (JUnit4, JUnit5 and Spock) that can transparently inject the inferred assumptions. If JUnit5 is used, those extensions can be deployed without refactoring project source code. We demonstrate and evaluate the utility of our approach using a set of six popular real-world programs, addressing known test flakiness issues in these programs caused by dependencies of tests on network availability. We find that our method effectively sanitises failures induced by network connectivity problems with high precision and recall.

</details>

<details>

<summary>2022-08-01 19:50:07 - Reduction Rules and ILP Are All You Need: Minimal Directed Feedback Vertex Set</summary>

- *Alex Meiburg*

- `2208.01119v1` - [abs](http://arxiv.org/abs/2208.01119v1) - [pdf](http://arxiv.org/pdf/2208.01119v1)

> This note describes the development of an exact solver for Minimal Directed Feedback Vertex Set as part of the PACE 2022 competition. The solver is powered largely by aggressively trying to reduce the DFVS problem to a Minimal Cover problem, and applying reduction rules adapted from Vertex Cover literature. The resulting problem is solved as an Integer Linear Program (ILP) using SCIP. The resulting solver performed the second-best in the competition, although a bug at submission time disqualified it. As an additional note, we describe a new vertex cover reduction generalizing the Desk reduction rule.

</details>

<details>

<summary>2022-08-01 20:29:24 - VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics</summary>

- *Haresh Karnan, Kavan Singh Sikand, Pranav Atreya, Sadegh Rabiee, Xuesu Xiao, Garrett Warnell, Peter Stone, Joydeep Biswas*

- `2203.15983v2` - [abs](http://arxiv.org/abs/2203.15983v2) - [pdf](http://arxiv.org/pdf/2203.15983v2)

> One of the key challenges in high speed off road navigation on ground vehicles is that the kinodynamics of the vehicle terrain interaction can differ dramatically depending on the terrain. Previous approaches to addressing this challenge have considered learning an inverse kinodynamics (IKD) model, conditioned on inertial information of the vehicle to sense the kinodynamic interactions. In this paper, we hypothesize that to enable accurate high-speed off-road navigation using a learned IKD model, in addition to inertial information from the past, one must also anticipate the kinodynamic interactions of the vehicle with the terrain in the future. To this end, we introduce Visual-Inertial Inverse Kinodynamics (VI-IKD), a novel learning based IKD model that is conditioned on visual information from a terrain patch ahead of the robot in addition to past inertial information, enabling it to anticipate kinodynamic interactions in the future. We validate the effectiveness of VI-IKD in accurate high-speed off-road navigation experimentally on a scale 1/5 UT-AlphaTruck off-road autonomous vehicle in both indoor and outdoor environments and show that compared to other state-of-the-art approaches, VI-IKD enables more accurate and robust off-road navigation on a variety of different terrains at speeds of up to 3.5 m/s.

</details>

<details>

<summary>2022-08-02 01:31:30 - Classifying Unstructured Clinical Notes via Automatic Weak Supervision</summary>

- *Chufan Gao, Mononito Goswami, Jieshi Chen, Artur Dubrawski*

- `2206.12088v2` - [abs](http://arxiv.org/abs/2206.12088v2) - [pdf](http://arxiv.org/pdf/2206.12088v2)

> Healthcare providers usually record detailed notes of the clinical care delivered to each patient for clinical, research, and billing purposes. Due to the unstructured nature of these narratives, providers employ dedicated staff to assign diagnostic codes to patients' diagnoses using the International Classification of Diseases (ICD) coding system. This manual process is not only time-consuming but also costly and error-prone. Prior work demonstrated potential utility of Machine Learning (ML) methodology in automating this process, but it has relied on large quantities of manually labeled data to train the models. Additionally, diagnostic coding systems evolve with time, which makes traditional supervised learning strategies unable to generalize beyond local applications. In this work, we introduce a general weakly-supervised text classification framework that learns from class-label descriptions only, without the need to use any human-labeled documents. It leverages the linguistic domain knowledge stored within pre-trained language models and the data programming framework to assign code labels to individual texts. We demonstrate the efficacy and flexibility of our method by comparing it to state-of-the-art weak text classifiers across four real-world text classification datasets, in addition to assigning ICD codes to medical notes in the publicly available MIMIC-III database.

</details>

<details>

<summary>2022-08-02 06:44:51 - Automatic Classification of Bug Reports Based on Multiple Text Information and Reports' Intention</summary>

- *Fanqi Meng, Xuesong Wang, Jingdong Wang, Peifang Wang*

- `2208.01274v1` - [abs](http://arxiv.org/abs/2208.01274v1) - [pdf](http://arxiv.org/pdf/2208.01274v1)

> With the rapid growth of software scale and complexity, a large number of bug reports are submitted to the bug tracking system. In order to speed up defect repair, these reports need to be accurately classified so that they can be sent to the appropriate developers. However, the existing classification methods only use the text information of the bug report, which leads to their low performance. To solve the above problems, this paper proposes a new automatic classification method for bug reports. The innovation is that when categorizing bug reports, in addition to using the text information of the report, the intention of the report (i.e. suggestion or explanation) is also considered, thereby improving the performance of the classification. First, we collect bug reports from four ecosystems (Apache, Eclipse, Gentoo, Mozilla) and manually annotate them to construct an experimental data set. Then, we use Natural Language Processing technology to preprocess the data. On this basis, BERT and TF-IDF are used to extract the features of the intention and the multiple text information. Finally, the features are used to train the classifiers. The experimental result on five classifiers (including K-Nearest Neighbor, Naive Bayes, Logistic Regression, Support Vector Machine, and Random Forest) show that our proposed method achieves better performance and its F-Measure achieves from 87.3% to 95.5%.

</details>

<details>

<summary>2022-08-02 08:34:17 - Convex duality for stochastic shortest path problems in known and unknown environments</summary>

- *Kelli Francis-Staite*

- `2208.00330v2` - [abs](http://arxiv.org/abs/2208.00330v2) - [pdf](http://arxiv.org/pdf/2208.00330v2)

> This paper studies Stochastic Shortest Path (SSP) problems in known and unknown environments from the perspective of convex optimisation. It first recalls results in the known parameter case, and develops understanding through different proofs. It then focuses on the unknown parameter case, where it studies extended value iteration (EVI) operators. This includes the existing operators used in Rosenberg et al. [26] and Tarbouriech et al. [31] based on the l-1 norm and supremum norm, as well as defining EVI operators corresponding to other norms and divergences, such as the KL-divergence. This paper shows in general how the EVI operators relate to convex programs, and the form of their dual, where strong duality is exhibited.   This paper then focuses on whether the bounds from finite horizon research of Neu and Pike-Burke [21] can be applied to these extended value iteration operators in the SSP setting. It shows that similar bounds to [21] for these operators exist, however they lead to operators that are not in general monotone and have more complex convergence properties. In a special case we observe oscillating behaviour. This paper generates open questions on how research may progress, with several examples that require further examination.

</details>

<details>

<summary>2022-08-02 10:34:01 - Application of Blockchain Smart Contracts in E-Commerce and Government</summary>

- *Kamal Kishor Singh*

- `2208.01350v1` - [abs](http://arxiv.org/abs/2208.01350v1) - [pdf](http://arxiv.org/pdf/2208.01350v1)

> With technological advances and the establishment of e-commerce models, business challenges have shifted to online platforms. The promise of embedding self-executing and autonomous programs into blockchain technologies has attracted increased interest and its use in niche solutions. Using qualitative interviews, this paper sought the opinions of the eleven industry leaders regarding smart contracts. Findings reveal that the technology is gaining momentum in e-commerce, particularly in financial transfer, record-keeping, real estate, and property management, insurance, mortgage, supply chain management, data storage, authorization of credit, denaturalized intelligence, aviation sector, shipping of products, invoice financing and other domains. The significant benefits of widespread adoption and deployment of smart contracts include their capability to deliver decentralization, efficacy, cost-effectiveness, transparency, speed, autonomy, transparency, privacy, and security, encouraging the emergence of novel business models. Albeit these benefits that revolutionize online transactions, the technology faced multifaceted challenges. Smart technologies are only a decade old and are not advanced in security, transparency, cost-effectiveness, and regulatory framework. Furthermore, organizational, and technical challenges limit their deployment: incompatibility with legacy systems, scalability, bugs, speed, and lack of talent and understanding regarding smart contracts. Consequently, policymakers, developers, researchers, practitioners, and other stakeholders need to invest effort and time to foster the technologies and address pertinent issues to enable the global adoption of smart contracts by small and big businesses.

</details>

<details>

<summary>2022-08-02 13:57:53 - Graph-based Reinforcement Learning meets Mixed Integer Programs: An application to 3D robot assembly discovery</summary>

- *Niklas Funk, Svenja Menzenbach, Georgia Chalvatzaki, Jan Peters*

- `2203.04120v2` - [abs](http://arxiv.org/abs/2203.04120v2) - [pdf](http://arxiv.org/pdf/2203.04120v2)

> Robot assembly discovery is a challenging problem that lives at the intersection of resource allocation and motion planning. The goal is to combine a predefined set of objects to form something new while considering task execution with the robot-in-the-loop. In this work, we tackle the problem of building arbitrary, predefined target structures entirely from scratch using a set of Tetris-like building blocks and a robotic manipulator. Our novel hierarchical approach aims at efficiently decomposing the overall task into three feasible levels that benefit mutually from each other. On the high level, we run a classical mixed-integer program for global optimization of block-type selection and the blocks' final poses to recreate the desired shape. Its output is then exploited to efficiently guide the exploration of an underlying reinforcement learning (RL) policy. This RL policy draws its generalization properties from a flexible graph-based representation that is learned through Q-learning and can be refined with search. Moreover, it accounts for the necessary conditions of structural stability and robotic feasibility that cannot be effectively reflected in the previous layer. Lastly, a grasp and motion planner transforms the desired assembly commands into robot joint movements. We demonstrate our proposed method's performance on a set of competitive simulated RAD environments, showcase real-world transfer, and report performance and robustness gains compared to an unstructured end-to-end approach. Videos are available at https://sites.google.com/view/rl-meets-milp .

</details>

<details>

<summary>2022-08-02 14:38:58 - OLLIE: Derivation-based Tensor Program Optimizer</summary>

- *Liyan Zheng, Haojie Wang, Jidong Zhai, Muyan Hu, Zixuan Ma, Tuowei Wang, Shizhi Tang, Lei Xie, Kezhao Huang, Zhihao Jia*

- `2208.02025v1` - [abs](http://arxiv.org/abs/2208.02025v1) - [pdf](http://arxiv.org/pdf/2208.02025v1)

> Boosting the runtime performance of deep neural networks (DNNs) is critical due to their wide adoption in real-world tasks. Existing approaches to optimizing the tensor algebra expression of a DNN only consider expressions representable by a fixed set of predefined operators, missing possible optimization opportunities between general expressions. We propose OLLIE, the first derivation-based tensor program optimizer. OLLIE optimizes tensor programs by leveraging transformations between general tensor algebra expressions, enabling a significantly larger expression search space that includes those supported by prior work as special cases. OLLIE uses a hybrid derivation-based optimizer that effectively combines explorative and guided derivations to quickly discover highly optimized expressions. Evaluation on seven DNNs shows that OLLIE can outperform existing optimizers by up to 2.73$\times$ (1.46$\times$ on average) on an A100 GPU and up to 2.68$\times$ (1.51$\times$) on a V100 GPU, respectively.

</details>

<details>

<summary>2022-08-02 15:21:51 - A replication of a controlled experiment with two STRIDE variants</summary>

- *Winnie Mbaka, Katja Tuma*

- `2208.01524v1` - [abs](http://arxiv.org/abs/2208.01524v1) - [pdf](http://arxiv.org/pdf/2208.01524v1)

> To avoid costly security patching after software deployment, security-by-design techniques (e.g., STRIDE threat analysis) are adopted in organizations to root out security issues before the system is ever implemented. Despite the global gap in cybersecurity workforce and the high manual effort required for performing threat analysis, organizations are ramping up threat analysis activities. However, past experimental results were inconclusive regarding some performance indicators of threat analysis techniques thus practitioners have little evidence for choosing the technique to adopt. To address this issue, we replicated a controlled experiment with STRIDE. Our study was aimed at measuring and comparing the performance indicators (productivity and precision) of two STRIDE variants (element and interaction). We conclude the paper by comparing our results to the original study.

</details>

<details>

<summary>2022-08-02 20:18:32 - An Algorithm for Ennola's Second Theorem and Counting Smooth Numbers in Practice</summary>

- *Chloe Makdad, Jonathan P. Sorenson*

- `2208.01725v1` - [abs](http://arxiv.org/abs/2208.01725v1) - [pdf](http://arxiv.org/pdf/2208.01725v1)

> Let $\Psi(x,y)$ count the number of positive integers $n\le x$ such that every prime divisor of $n$ is at most $y$. Given inputs $x$ and $y$, what is the best way to estimate $\Psi(x,y)$? We address this problem in three ways: with a new algorithm to estimate $\Psi(x,y)$, with a performance improvement to an established algorithm, and with empirically based advice on how to choose an algorithm to estimate $\Psi$ for the given inputs.   Our new algorithm to estimate $\Psi(x,y)$ is based on Ennola's second theorem [Ennola69], which applies when $y< (\log x)^{3/4-\epsilon}$ for $\epsilon>0$. It takes $O(y^2/\log y)$ arithmetic operations of precomputation and $O(y\log y)$ operations per evaluation of $\Psi$.   We show how to speed up Algorithm HT, which is based on the saddle-point method of Hildebrand and Tenenbaum [1986], by a factor proportional to $\log\log x$, by applying Newton's method in a new way.   And finally we give our empirical advice based on five algorithms to compute estimates for $\Psi(x,y)$.The challenge here is that the boundaries of the ranges of applicability, as given in theorems, often include unknown constants or small values of $\epsilon>0$, for example, that cannot be programmed directly.

</details>

<details>

<summary>2022-08-03 00:53:12 - Text Mining Undergraduate Engineering Programs' Applications: the Role of Gender, Nationality, and Socio-economic Status</summary>

- *Bo Lin, Bissan Ghaddar, Ada Hurst*

- `2107.14034v4` - [abs](http://arxiv.org/abs/2107.14034v4) - [pdf](http://arxiv.org/pdf/2107.14034v4)

> Women, visible minorities, and other socially disadvantaged groups continue to be underrepresented in STEM education. Understanding students' motivations for pursuing a STEM major, and the roles gender, nationality, parental education attainment, and socio-economic background play in shaping students' motivations can support the design of more effective recruitment efforts towards these groups. In this paper, we propose and develop a novel text mining approach incorporating the Latent Dirichlet Allocation and word embeddings to analyze applicants' motivational factors for choosing an engineering program. We apply the proposed method to a dataset of 43,645 applications to the engineering school of a large Canadian university. We then investigate the relationship between applicants' gender, nationality, and family income and educational attainment, and their stated motivations for applying to their engineering program of choice. We find that interest in technology and the desire to make social impact are the two most powerful motivators for applicants. Additionally, while we find significant motivational differences related to applicants' nationality and family socio-economic status, gender has the strongest and the most robust impact on students' motivations for studying engineering.

</details>

<details>

<summary>2022-08-03 04:46:44 - Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey</summary>

- *Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley*

- `2106.15379v2` - [abs](http://arxiv.org/abs/2106.15379v2) - [pdf](http://arxiv.org/pdf/2106.15379v2)

> This is a tutorial and survey paper on unification of spectral dimensionality reduction methods, kernel learning by Semidefinite Programming (SDP), Maximum Variance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We first explain how the spectral dimensionality reduction methods can be unified as kernel Principal Component Analysis (PCA) with different kernels. This unification can be interpreted as eigenfunction learning or representation of kernel in terms of distance matrix. Then, since the spectral methods are unified as kernel PCA, we say let us learn the best kernel for unfolding the manifold of data to its maximum variance. We first briefly introduce kernel learning by SDP for the transduction task. Then, we explain MVU in detail. Various versions of supervised MVU using nearest neighbors graph, by class-wise unfolding, by Fisher criterion, and by colored MVU are explained. We also explain out-of-sample extension of MVU using eigenfunctions and kernel mapping. Finally, we introduce other variants of MVU including action respecting embedding, relaxed MVU, and landmark MVU for big data.

</details>

<details>

<summary>2022-08-03 05:39:43 - Revisiting local branching with a machine learning lens</summary>

- *Defeng Liu, Matteo Fischetti, Andrea Lodi*

- `2112.02195v2` - [abs](http://arxiv.org/abs/2112.02195v2) - [pdf](http://arxiv.org/pdf/2112.02195v2)

> Finding high-quality solutions to mixed-integer linear programming problems (MILPs) is of great importance for many practical applications. In this respect, the refinement heuristic local branching (LB) has been proposed to produce improving solutions and has been highly influential for the development of local search methods in MILP. The algorithm iteratively explores a sequence of solution neighborhoods defined by the so-called local branching constraint, namely, a linear inequality limiting the distance from a reference solution. For a LB algorithm, the choice of the neighborhood size is critical to performance. In this work, we study the relation between the size of the search neighborhood and the behavior of the underlying LB algorithm, and we devise a leaning based framework for predicting the best size for the specific instance to be solved. Furthermore, we have also investigated the relation between the time limit for exploring the LB neighborhood and the actual performance of LB scheme, and devised a strategy for adapting the time limit. We computationally show that the neighborhood size and time limit can indeed be learned, leading to improved performances and that the overall algorithm generalizes well both with respect to the instance size and, remarkably, across instances.

</details>

<details>

<summary>2022-08-03 08:03:33 - Understanding Adversarial Imitation Learning in Small Sample Regime: A Stage-coupled Analysis</summary>

- *Tian Xu, Ziniu Li, Yang Yu, Zhi-Quan Luo*

- `2208.01899v1` - [abs](http://arxiv.org/abs/2208.01899v1) - [pdf](http://arxiv.org/pdf/2208.01899v1)

> Imitation learning learns a policy from expert trajectories. While the expert data is believed to be crucial for imitation quality, it was found that a kind of imitation learning approach, adversarial imitation learning (AIL), can have exceptional performance. With as little as only one expert trajectory, AIL can match the expert performance even in a long horizon, on tasks such as locomotion control. There are two mysterious points in this phenomenon. First, why can AIL perform well with only a few expert trajectories? Second, why does AIL maintain good performance despite the length of the planning horizon? In this paper, we theoretically explore these two questions. For a total-variation-distance-based AIL (called TV-AIL), our analysis shows a horizon-free imitation gap $\mathcal O(\{\min\{1, \sqrt{|\mathcal S|/N} \})$ on a class of instances abstracted from locomotion control tasks. Here $|\mathcal S|$ is the state space size for a tabular Markov decision process, and $N$ is the number of expert trajectories. We emphasize two important features of our bound. First, this bound is meaningful in both small and large sample regimes. Second, this bound suggests that the imitation gap of TV-AIL is at most 1 regardless of the planning horizon. Therefore, this bound can explain the empirical observation. Technically, we leverage the structure of multi-stage policy optimization in TV-AIL and present a new stage-coupled analysis via dynamic programming

</details>

<details>

<summary>2022-08-03 12:31:55 - Character Generation through Self-Supervised Vectorization</summary>

- *Gokcen Gokceoglu, Emre Akbas*

- `2208.02012v1` - [abs](http://arxiv.org/abs/2208.02012v1) - [pdf](http://arxiv.org/pdf/2208.02012v1)

> The prevalent approach in self-supervised image generation is to operate on pixel level representations. While this approach can produce high quality images, it cannot benefit from the simplicity and innate quality of vectorization. Here we present a drawing agent that operates on stroke-level representation of images. At each time step, the agent first assesses the current canvas and decides whether to stop or keep drawing. When a 'draw' decision is made, the agent outputs a program indicating the stroke to be drawn. As a result, it produces a final raster image by drawing the strokes on a canvas, using a minimal number of strokes and dynamically deciding when to stop. We train our agent through reinforcement learning on MNIST and Omniglot datasets for unconditional generation and parsing (reconstruction) tasks. We utilize our parsing agent for exemplar generation and type conditioned concept generation in Omniglot challenge without any further training. We present successful results on all three generation tasks and the parsing task. Crucially, we do not need any stroke-level or vector supervision; we only use raster images for training.

</details>

<details>

<summary>2022-08-03 13:26:16 - Recovery of Future Data via Convolution Nuclear Norm Minimization</summary>

- *Guangcan Liu, Wayne Zhang*

- `1909.03889v7` - [abs](http://arxiv.org/abs/1909.03889v7) - [pdf](http://arxiv.org/pdf/1909.03889v7)

> This paper studies the problem of time series forecasting (TSF) from the perspective of compressed sensing. First of all, we convert TSF into a more inclusive problem called tensor completion with arbitrary sampling (TCAS), which is to restore a tensor from a subset of its entries sampled in an arbitrary manner. While it is known that, in the framework of Tucker low-rankness, it is theoretically impossible to identify the target tensor based on some arbitrarily selected entries, in this work we shall show that TCAS is indeed tackleable in the light of a new concept called convolutional low-rankness, which is a generalization of the well-known Fourier sparsity. Then we introduce a convex program termed Convolution Nuclear Norm Minimization (CNNM), and we prove that CNNM succeeds in solving TCAS as long as a sampling condition--which depends on the convolution rank of the target tensor--is obeyed. This theory provides a meaningful answer to the fundamental question of what is the minimum sampling size needed for making a given number of forecasts. Experiments on univariate time series, images and videos show encouraging results.

</details>

<details>

<summary>2022-08-03 14:34:12 - Active Learning on a Programmable Photonic Quantum Processor</summary>

- *Chen Ding, Xiao-Yue Xu, Yun-Fei Niu, Shuo Zhang, Wan-Su Bao, He-Liang Huang*

- `2208.02104v1` - [abs](http://arxiv.org/abs/2208.02104v1) - [pdf](http://arxiv.org/pdf/2208.02104v1)

> Training a quantum machine learning model generally requires a large labeled dataset, which incurs high labeling and computational costs. To reduce such costs, a selective training strategy, called active learning (AL), chooses only a subset of the original dataset to learn while maintaining the trained model's performance. Here, we design and implement two AL-enpowered variational quantum classifiers, to investigate the potential applications and effectiveness of AL in quantum machine learning. Firstly, we build a programmable free-space photonic quantum processor, which enables the programmed implementation of various hybrid quantum-classical computing algorithms. Then, we code the designed variational quantum classifier with AL into the quantum processor, and execute comparative tests for the classifiers with and without the AL strategy. The results validate the great advantage of AL in quantum machine learning, as it saves at most $85\%$ labeling efforts and $91.6\%$ percent computational efforts compared to the training without AL on a data classification task. Our results inspire AL's further applications in large-scale quantum machine learning to drastically reduce training data and speed up training, underpinning the exploration of practical quantum advantages in quantum physics or real-world applications.

</details>

<details>

<summary>2022-08-03 16:41:01 - Efficiently Computing Nash Equilibria in Adversarial Team Markov Games</summary>

- *Fivos Kalogiannis, Ioannis Anagnostides, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Vaggos Chatziafratis, Stelios Stavroulakis*

- `2208.02204v1` - [abs](http://arxiv.org/abs/2208.02204v1) - [pdf](http://arxiv.org/pdf/2208.02204v1)

> Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications. In this work, we depart from those prior results by investigating infinite-horizon \emph{adversarial team Markov games}, a natural and well-motivated class of games in which a team of identically-interested players -- in the absence of any explicit coordination or communication -- is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary $\epsilon$-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as $1/\epsilon$. The proposed algorithm is particularly natural and practical, and it is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers. Along the way, we significantly extend an important characterization of optimal policies in adversarial (normal-form) team games due to Von Stengel and Koller (GEB `97).

</details>

<details>

<summary>2022-08-03 22:44:18 - Empirical Characteristics of Affordable Care Act Risk Transfer Payments</summary>

- *Grace Guan, Mark Braverman*

- `2208.02372v1` - [abs](http://arxiv.org/abs/2208.02372v1) - [pdf](http://arxiv.org/pdf/2208.02372v1)

> Under the Affordable Care Act (ACA), insurers cannot engage in medical underwriting and thus face perverse incentives to engage in risk selection and discourage low-value patients from enrolling in their plans. One ACA program intended to reduce the effects of risk selection is risk adjustment. Under a risk adjustment program, insurers with less healthy enrollees receive risk transfer payments from insurers with healthier enrollees. Our goal is to understand the elements driving risk transfers. First, the distribution of risk transfers should be based on random health shocks, which are unpredictable events that negatively affect health status. Second, risk transfers could be influenced by factors unique to each insurer, such as certain plans attracting certain patients, the extent to which carriers engage in risk selection, and the degree of upcoding. We create a publicly available dataset using Centers for Medicare and Medicaid Services data that includes insurer risk transfer payments, costs, and premiums for the 2014-2017 benefit years. Using this dataset, we find that the empirical distribution of risk transfer payments is not consistent with the lack of risk selection as measured by the ACA risk transfer formula. Over all states included in our dataset, at least 60% of the volume of transfers cannot be accounted for by a purely normal model. Because we find that it is very unlikely that risk transfer payments are caused solely by random shocks that reflect health events of the population, our work raises important questions about the causes of heterogeneity in risk transfers.

</details>

<details>

<summary>2022-08-04 03:40:02 - Designing and developing tools to automatically identify parallelism</summary>

- *Fabian Mora Cordero*

- `2208.02428v1` - [abs](http://arxiv.org/abs/2208.02428v1) - [pdf](http://arxiv.org/pdf/2208.02428v1)

> In this work we present a dynamic analysis tool for analyzing regions of code and how those regions depend between each other via data dependencies encountered during the execution of the program. We also present an abstract method to analyze and study parallelism in a directed graph, by studying a Quotient Graph of the execution graph of a program, and give a simple algorithm for searching parallelism in execution graphs with a high degree of symmetry. Finally, we evaluate our approach selecting four dwarfs out of 13 Berkeleys computational dwarfs or otherwise known as parallel patterns.

</details>

<details>

<summary>2022-08-04 12:12:09 - Meta-learning from Learning Curves Challenge: Lessons learned from the First Round and Design of the Second Round</summary>

- *Manh Hung Nguyen, Lisheng Sun, Nathan Grinsztajn, Isabelle Guyon*

- `2208.02821v1` - [abs](http://arxiv.org/abs/2208.02821v1) - [pdf](http://arxiv.org/pdf/2208.02821v1)

> Meta-learning from learning curves is an important yet often neglected research area in the Machine Learning community. We introduce a series of Reinforcement Learning-based meta-learning challenges, in which an agent searches for the best suited algorithm for a given dataset, based on feedback of learning curves from the environment. The first round attracted participants both from academia and industry. This paper analyzes the results of the first round (accepted to the competition program of WCCI 2022), to draw insights into what makes a meta-learner successful at learning from learning curves. With the lessons learned from the first round and the feedback from the participants, we have designed the second round of our challenge with a new protocol and a new meta-dataset. The second round of our challenge is accepted at the AutoML-Conf 2022 and currently ongoing .

</details>

<details>

<summary>2022-08-04 14:06:30 - Information Flow Control-by-Construction for an Object-Oriented Language Using Type Modifiers</summary>

- *Tobias Runge, Alexander Kittelmann, Marco Servetto, Alex Potanin, Ina Schaefer*

- `2208.02672v1` - [abs](http://arxiv.org/abs/2208.02672v1) - [pdf](http://arxiv.org/pdf/2208.02672v1)

> In security-critical software applications, confidential information must be prevented from leaking to unauthorized sinks. Static analysis techniques are widespread to enforce a secure information flow by checking a program after construction. A drawback of these systems is that incomplete programs during construction cannot be checked properly. The user is not guided to a secure program by most systems. We introduce IFbCOO, an approach that guides users incrementally to a secure implementation by using refinement rules. In each refinement step, confidentiality or integrity (or both) is guaranteed alongside the functional correctness of the program, such that insecure programs are declined by construction. In this work, we formalize IFbCOO and prove soundness of the refinement rules. We implement IFbCOO in the tool CorC and conduct a feasibility study by successfully implementing case studies.

</details>

<details>

<summary>2022-08-04 14:36:47 - Proceedings 38th International Conference on Logic Programming</summary>

- *Yuliya Lierler, Jose F. Morales, Carmine Dodaro, Veronica Dahl, Martin Gebser, Tuncay Tekle*

- `2208.02685v1` - [abs](http://arxiv.org/abs/2208.02685v1) - [pdf](http://arxiv.org/pdf/2208.02685v1)

> ICLP is the premier international event for presenting research in logic programming. Contributions to ICLP 2022 were sought in all areas of logic programming, including but not limited to: Foundations: Semantics, Formalisms, Nonmonotonic reasoning, Knowledge representation. Languages issues: Concurrency, Objects, Coordination, Mobility, Higher order, Types, Modes, Assertions, Modules, Meta-programming, Logic-based domain-specific languages, Programming techniques. Programming support: Program analysis, Transformation, Validation, Verification, Debugging, Profiling, Testing, Execution visualization. Implementation: Compilation, Virtual machines, Memory management, Parallel and Distributed execution, Constraint handling rules, Tabling, Foreign interfaces, User interfaces. Related Paradigms and Synergies: Inductive and coinductive logic programming, Constraint logic programming, Answer set programming, Interaction with SAT, SMT and CSP solvers, Theorem proving, Argumentation, Probabilistic programming, Machine learning. Applications: Databases, Big data, Data integration and federation, Software engineering, Natural language processing, Web and semantic web, Agents, Artificial intelligence, Computational life sciences, Cyber-security, Robotics, Education.

</details>

<details>

<summary>2022-08-04 17:58:43 - MAGPIE: Machine Automated General Performance Improvement via Evolution of Software</summary>

- *Aymeric Blot, Justyna Petke*

- `2208.02811v1` - [abs](http://arxiv.org/abs/2208.02811v1) - [pdf](http://arxiv.org/pdf/2208.02811v1)

> Performance is one of the most important qualities of software. Several techniques have thus been proposed to improve it, such as program transformations, optimisation of software parameters, or compiler flags. Many automated software improvement approaches use similar search strategies to explore the space of possible improvements, yet available tooling only focuses on one approach at a time. This makes comparisons and exploration of interactions of the various types of improvement impractical.   We propose MAGPIE, a unified software improvement framework. It provides a common edit sequence based representation that isolates the search process from the specific improvement technique, enabling a much simplified synergistic workflow. We provide a case study using a basic local search to compare compiler optimisation, algorithm configuration, and genetic improvement. We chose running time as our efficiency measure and evaluated our approach on four real-world software, written in C, C++, and Java.   Our results show that, used independently, all techniques find significant running time improvements: up to 25% for compiler optimisation, 97% for algorithm configuration, and 61% for evolving source code using genetic improvement. We also show that up to 10% further increase in performance can be obtained with partial combinations of the variants found by the different techniques. Furthermore, the common representation also enables simultaneous exploration of all techniques, providing a competitive alternative to using each technique individually.

</details>

<details>

<summary>2022-08-04 17:59:45 - On-the-Fly Syntax Highlighting using Neural Networks</summary>

- *Marco Edoardo Palma, Pasquale Salza, Harald C. Gall*

- `2208.02815v1` - [abs](http://arxiv.org/abs/2208.02815v1) - [pdf](http://arxiv.org/pdf/2208.02815v1)

> With the presence of online collaborative tools for software developers, source code is shared and consulted frequently, from code viewers to merge requests and code snippets. Typically, code highlighting quality in such scenarios is sacrificed in favor of system responsiveness. In these on-the-fly settings, performing a formal grammatical analysis of the source code is not only expensive, but also intractable for the many times the input is an invalid derivation of the language. Indeed, current popular highlighters heavily rely on a system of regular expressions, typically far from the specification of the language's lexer. Due to their complexity, regular expressions need to be periodically updated as more feedback is collected from the users and their design unwelcome the detection of more complex language formations. This paper delivers a deep learning-based approach suitable for on-the-fly grammatical code highlighting of correct and incorrect language derivations, such as code viewers and snippets. It focuses on alleviating the burden on the developers, who can reuse the language's parsing strategy to produce the desired highlighting specification. Moreover, this approach is compared to nowadays online syntax highlighting tools and formal methods in terms of accuracy and execution time, across different levels of grammatical coverage, for three mainstream programming languages. The results obtained show how the proposed approach can consistently achieve near-perfect accuracy in its predictions, thereby outperforming regular expression-based strategies.

</details>

<details>

<summary>2022-08-04 20:16:12 - On Incorrectness Logic and Kleene Algebra with Top and Tests</summary>

- *Cheng Zhang, Arthur Azevedo de Amorim, Marco Gaboardi*

- `2108.07707v4` - [abs](http://arxiv.org/abs/2108.07707v4) - [pdf](http://arxiv.org/pdf/2108.07707v4)

> Kleene algebra with tests (KAT) is a foundational equational framework for reasoning about programs, which has found applications in program transformations, networking and compiler optimizations, among many other areas. In his seminal work, Kozen proved that KAT subsumes propositional Hoare logic, showing that one can reason about the (partial) correctness of while programs by means of the equational theory of KAT. In this work, we investigate the support that KAT provides for reasoning about incorrectness, instead, as embodied by Ohearn's recently proposed incorrectness logic. We show that KAT cannot directly express incorrectness logic. The main reason for this limitation can be traced to the fact that KAT cannot express explicitly the notion of codomain, which is essential to express incorrectness triples. To address this issue, we study Kleene Algebra with Top and Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT is powerful enough to express a codomain operation, to express incorrectness triples, and to prove all the rules of incorrectness logic sound. This shows that one can reason about the incorrectness of while-like programs by means of the equational theory of TopKAT.

</details>

<details>

<summary>2022-08-04 22:10:51 - Are Query-Based Ontology Debuggers Really Helping Knowledge Engineers?</summary>

- *Patrick Rodler, Dietmar Jannach, Konstantin Schekotihin, Philipp Fleiss*

- `1904.01484v2` - [abs](http://arxiv.org/abs/1904.01484v2) - [pdf](http://arxiv.org/pdf/1904.01484v2)

> Real-world semantic or knowledge-based systems, e.g., in the biomedical domain, can become large and complex. Tool support for the localization and repair of faults within knowledge bases of such systems can therefore be essential for their practical success. Correspondingly, a number of knowledge base debugging approaches, in particular for ontology-based systems, were proposed throughout recent years. Query-based debugging is a comparably recent interactive approach that localizes the true cause of an observed problem by asking knowledge engineers a series of questions. Concrete implementations of this approach exist, such as the OntoDebug plug-in for the ontology editor Prot\'eg\'e.   To validate that a newly proposed method is favorable over an existing one, researchers often rely on simulation-based comparisons. Such an evaluation approach however has certain limitations and often cannot fully inform us about a method's true usefulness. We therefore conducted different user studies to assess the practical value of query-based ontology debugging. One main insight from the studies is that the considered interactive approach is indeed more efficient than an alternative algorithmic debugging based on test cases. We also observed that users frequently made errors in the process, which highlights the importance of a careful design of the queries that users need to answer.

</details>

<details>

<summary>2022-08-05 00:22:11 - Abstract Interpretation for Generalized Heuristic Search in Model-Based Planning</summary>

- *Tan Zhi-Xuan, Joshua B. Tenenbaum, Vikash K. Mansinghka*

- `2208.02938v1` - [abs](http://arxiv.org/abs/2208.02938v1) - [pdf](http://arxiv.org/pdf/2208.02938v1)

> Domain-general model-based planners often derive their generality by constructing search heuristics through the relaxation or abstraction of symbolic world models. We illustrate how abstract interpretation can serve as a unifying framework for these abstraction-based heuristics, extending the reach of heuristic search to richer world models that make use of more complex datatypes and functions (e.g. sets, geometry), and even models with uncertainty and probabilistic effects. These heuristics can also be integrated with learning, allowing agents to jumpstart planning in novel world models via abstraction-derived information that is later refined by experience. This suggests that abstract interpretation can play a key role in building universal reasoning systems.

</details>

<details>

<summary>2022-08-05 02:32:18 - Any-resolution Training for High-resolution Image Synthesis</summary>

- *Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, Richard Zhang*

- `2204.07156v2` - [abs](http://arxiv.org/abs/2204.07156v2) - [pdf](http://arxiv.org/pdf/2204.07156v2)

> Generative models operate at fixed resolution, even though natural images come in a variety of sizes. As high-resolution details are downsampled away and low-resolution images are discarded altogether, precious supervision is lost. We argue that every pixel matters and create datasets with variable-size images, collected at their native resolutions. To take advantage of varied-size data, we introduce continuous-scale training, a process that samples patches at random scales to train a new generator with variable output resolutions. First, conditioning the generator on a target scale allows us to generate higher resolution images than previously possible, without adding layers to the model. Second, by conditioning on continuous coordinates, we can sample patches that still obey a consistent global layout, which also allows for scalable training at higher resolutions. Controlled FFHQ experiments show that our method can take advantage of multi-resolution training data better than discrete multi-scale approaches, achieving better FID scores and cleaner high-frequency details. We also train on other natural image domains including churches, mountains, and birds, and demonstrate arbitrary scale synthesis with both coherent global layouts and realistic local details, going beyond 2K resolution in our experiments. Our project page is available at: https://chail.github.io/anyres-gan/.

</details>

<details>

<summary>2022-08-05 03:23:29 - Bug-Fix Variants: Visualizing Unique Source Code Changes across GitHub Forks</summary>

- *Daigo Imamura, Takashi Ishio, Raula Gaikovina Kula, Kenichi Matsumoto*

- `2208.04074v1` - [abs](http://arxiv.org/abs/2208.04074v1) - [pdf](http://arxiv.org/pdf/2208.04074v1)

> Forking is a common practice for developers when building upon on already existing projects. These forks create variants, which have a common code base but then evolve the code in different directions, which is specific to that forked project requirements. An interesting side-effect of having multiple forks is the ability to select between different evolution directions of the code which is based on developers fixing bugs in the code base. However, the key issue that this decentralized form of information is difficult to analyze. In this study, we propose a visualization to analyze active changes in fork repositories that have not been merged back to the original project. Our visualization shows code commit activities in multiple forks with highlight on bug fix commits in the history of forks. While the commit activity of each repository is visualized similarly to the code frequency view of GitHub, our view shows only commits unique to fork repositories. To illustrate the effectiveness of our visualization, we have applied our view to two use cases: identifying forks from a repository no longer maintained, and identifying a bug fix among forks. In the first case, we identify a fork of a suspended project named Obfuscator-LLVM. Our view shows the original repository and its most active fork that continue the development on the top. In the second case, we identify a bug fix in a fork of Clipy project. Our view shows that the most active fork has its own bug fixes; we could easily identify a patch for the bug highlighted in the view. As a new ideas paper, we then present our outline of three research questions to spark real world use-cases and goals for our visualization has the potential to uncover. A prototype of our visualization is available at \textcolor{blue}{\url{https://naist-se.github.io/vissoft2022/}

</details>

<details>

<summary>2022-08-05 10:47:07 - A Fixpoint Characterization of Three-Valued Disjunctive Hybrid MKNF Knowledge Bases</summary>

- *Spencer Killen, Jia-Huai You*

- `2208.03087v1` - [abs](http://arxiv.org/abs/2208.03087v1) - [pdf](http://arxiv.org/pdf/2208.03087v1)

> The logic of hybrid MKNF (minimal knowledge and negation as failure) is a powerful knowledge representation language that elegantly pairs ASP (answer set programming) with ontologies. Disjunctive rules are a desirable extension to normal rule-based reasoning and typically semantic frameworks designed for normal knowledge bases need substantial restructuring to support disjunctive rules. Alternatively, one may lift characterizations of normal rules to support disjunctive rules by inducing a collection of normal knowledge bases, each with the same body and a single atom in its head. In this work, we refer to a set of such normal knowledge bases as a head-cut of a disjunctive knowledge base. The question arises as to whether the semantics of disjunctive hybrid MKNF knowledge bases can be characterized using fixpoint constructions with head-cuts. Earlier, we have shown that head-cuts can be paired with fixpoint operators to capture the two-valued MKNF models of disjunctive hybrid MKNF knowledge bases. Three-valued semantics extends two-valued semantics with the ability to express partial information. In this work, we present a fixpoint construction that leverages head-cuts using an operator that iteratively captures three-valued models of hybrid MKNF knowledge bases with disjunctive rules. This characterization also captures partial stable models of disjunctive logic programs since a program can be expressed as a disjunctive hybrid MKNF knowledge base with an empty ontology. We elaborate on a relationship between this characterization and approximators in AFT (approximation fixpoint theory) for normal hybrid MKNF knowledge bases.

</details>

<details>

<summary>2022-08-05 10:48:25 - A Preliminary Data-driven Analysis of Common Errors Encountered by Novice SPARC Programmers</summary>

- *Zach Hansen, Hanxiang Du, Wanli Xing, Rory Eckel, Justin Lugo, Yuanlin Zhang*

- `2208.03090v1` - [abs](http://arxiv.org/abs/2208.03090v1) - [pdf](http://arxiv.org/pdf/2208.03090v1)

> Answer Set Programming (ASP), a modern development of Logic Programming, enables a natural integration of Computing with STEM subjects. This integration addresses a widely acknowledged challenge in K-12 education, and early empirical results on ASP-based integration are promising. Although ASP is considered a simple language when compared with imperative programming languages, programming errors can still be a significant barrier for students. This is particularly true for K-12 students who are novice users of ASP. Categorizing errors and measuring their difficulty has yielded insights into imperative languages like Java. However, little is known about the types and difficulty of errors encountered by K-12 students using ASP. To address this, we collected high school student programs submitted during a 4-session seminar teaching an ASP language known as SPARC. From error messages in this dataset, we identify a collection of error classes, and measure how frequently each class occurs and how difficult it is to resolve.

</details>

<details>

<summary>2022-08-05 10:48:42 - On Model Reconciliation: How to Reconcile When Robot Does not Know Human's Model?</summary>

- *Ho Tuan Dung, Tran Cao Son*

- `2208.03091v1` - [abs](http://arxiv.org/abs/2208.03091v1) - [pdf](http://arxiv.org/pdf/2208.03091v1)

> The Model Reconciliation Problem (MRP) was introduced to address issues in explainable AI planning. A solution to a MRP is an explanation for the differences between the models of the human and the planning agent (robot). Most approaches to solving MRPs assume that the robot, who needs to provide explanations, knows the human model. This assumption is not always realistic in several situations (e.g., the human might decide to update her model and the robot is unaware of the updates).   In this paper, we propose a dialog-based approach for computing explanations of MRPs under the assumptions that (i) the robot does not know the human model; (ii) the human and the robot share the set of predicates of the planning domain and their exchanges are about action descriptions and fluents' values; (iii) communication between the parties is perfect; and (iv) the parties are truthful. A solution of a MRP is computed through a dialog, defined as a sequence of rounds of exchanges, between the robot and the human. In each round, the robot sends a potential explanation, called proposal, to the human who replies with her evaluation of the proposal, called response. We develop algorithms for computing proposals by the robot and responses by the human and implement these algorithms in a system that combines imperative means with answer set programming using the multi-shot feature of clingo.

</details>

<details>

<summary>2022-08-05 10:49:21 - A Gaze into the Internal Logic of Graph Neural Networks, with Logic</summary>

- *Paul Tarau*

- `2208.03093v1` - [abs](http://arxiv.org/abs/2208.03093v1) - [pdf](http://arxiv.org/pdf/2208.03093v1)

> Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data.   This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its nodes properties of new nodes, given their known connections to nodes with possibly similar properties. The problem is known as graph node property prediction and our approach will consist in emulating with help of a Prolog program the key information propagation steps of a Graph Neural Network's training and inference stages.   We test our a approach on the ogbn-arxiv node property inference benchmark. To infer class labels for nodes representing papers in a citation network, we distill the dependency trees of the text associated to each node into directed acyclic graphs that we encode as ground Prolog terms. Together with the set of their references to other papers, they become facts in a database on which we reason with help of a Prolog program that mimics the information propagation in graph neural networks predicting node properties. In the process, we invent ground term similarity relations that help infer labels in the test set by propagating node properties from similar nodes in the training set and we evaluate their effectiveness in comparison with that of the graph's link structure. Finally, we implement explanation generators that unveil performance upper bounds inherent to the dataset.   As a practical outcome, we obtain a logic program, that, when seen as machine learning algorithm, performs close to the state of the art on the node property prediction benchmark.

</details>

<details>

<summary>2022-08-05 10:50:03 - A Model-Oriented Approach for Lifting Symmetries in Answer Set Programming</summary>

- *Alice Tarzariol*

- `2208.03095v1` - [abs](http://arxiv.org/abs/2208.03095v1) - [pdf](http://arxiv.org/pdf/2208.03095v1)

> When solving combinatorial problems, pruning symmetric solution candidates from the search space is essential. Most of the existing approaches are instance-specific and focus on the automatic computation of Symmetry Breaking Constraints (SBCs) for each given problem instance. However, the application of such approaches to large-scale instances or advanced problem encodings might be problematic since the computed SBCs are propositional and, therefore, can neither be meaningfully interpreted nor transferred to other instances. As a result, a time-consuming recomputation of SBCs must be done before every invocation of a solver. To overcome these limitations, we introduce a new model-oriented approach for Answer Set Programming that lifts the SBCs of small problem instances into a set of interpretable first-order constraints using a form of machine learning called Inductive Logic Programming. After targeting simple combinatorial problems, we aim to extend our method to be applied also for advanced decision and optimization problems.

</details>

<details>

<summary>2022-08-05 10:50:21 - Tools and Methodologies for Verifying Answer Set Programs</summary>

- *Zach Hansen*

- `2208.03096v1` - [abs](http://arxiv.org/abs/2208.03096v1) - [pdf](http://arxiv.org/pdf/2208.03096v1)

> Answer Set Programming (ASP) is a powerful declarative programming paradigm commonly used for solving challenging search and optimization problems. The modeling languages of ASP are supported by sophisticated solving algorithms (solvers) that make the solution search efficient while enabling the programmer to model the problem at a high level of abstraction. As an approach to Knowledge Representation and Reasoning, ASP benefits from its simplicity, conciseness and rigorously defined semantics. These characteristics make ASP a straightforward way to develop formally verifiable programs. In the context of artificial intelligence (AI), the clarity of ASP programs lends itself to the construction of explainable, trustworthy AI. In support of these goals, my research is concerned with extending the theory and tools supporting the verification of ASP progams.

</details>

<details>

<summary>2022-08-05 10:50:38 - An ASP Framework for Efficient Urban Traffic Optimization</summary>

- *Matteo Cardellini*

- `2208.03097v1` - [abs](http://arxiv.org/abs/2208.03097v1) - [pdf](http://arxiv.org/pdf/2208.03097v1)

> Avoiding congestion and controlling traffic in urban scenarios is becoming nowadays of paramount importance due to the rapid growth of our cities' population and vehicles. The effective control of urban traffic as a means to mitigate congestion can be beneficial in an economic, environmental and health way. In this paper, a framework which allows to efficiently simulate and optimize traffic flow in a large roads' network with hundreds of vehicles is presented. The framework leverages on an Answer Set Programming (ASP) encoding to formally describe the movements of vehicles inside a network. Taking advantage of the ability to specify optimization constraints in ASP and the off-the-shelf solver Clingo, it is then possible to optimize the routes of vehicles inside the network to reduce a range of relevant metrics (e.g., travel times or emissions). Finally, an analysis on real-world traffic data is performed, utilizing the state-of-the-art Urban Mobility Simulator (SUMO) to keep track of the state of the network, test the correctness of the solution and to prove the efficiency and capabilities of the presented solution.

</details>

<details>

<summary>2022-08-05 10:51:02 - Planning and Scheduling in Digital Health with Answer Set Programming</summary>

- *Marco Mochi*

- `2208.03099v1` - [abs](http://arxiv.org/abs/2208.03099v1) - [pdf](http://arxiv.org/pdf/2208.03099v1)

> In the hospital world there are several complex combinatory problems, and solving these problems is important to increase the degree of patients' satisfaction and the quality of care offered. The problems in the healthcare are complex since to solve them several constraints and different type of resources should be taken into account. Moreover, the solutions must be evaluated in a small amount of time to ensure the usability in real scenarios. We plan to propose solutions to these kind of problems both expanding already tested solutions and by modelling solutions for new problems, taking into account the literature and by using real data when available. Solving these kind of problems is important but, since the European Commission established with the General Data Protection Regulation that each person has the right to ask for explanation of the decision taken by an AI, without developing Explainability methodologies the usage of AI based solvers e.g. those based on Answer Set programming will be limited. Thus, another part of the research will be devoted to study and propose new methodologies for explaining the solutions obtained.

</details>

<details>

<summary>2022-08-05 18:07:53 - Going Beyond Approximation: Encoding Constraints for Explainable Multi-hop Inference via Differentiable Combinatorial Solvers</summary>

- *Mokanarangan Thayaparan, Marco Valentino, AndrÃ© Freitas*

- `2208.03339v1` - [abs](http://arxiv.org/abs/2208.03339v1) - [pdf](http://arxiv.org/pdf/2208.03339v1)

> Integer Linear Programming (ILP) provides a viable mechanism to encode explicit and controllable assumptions about explainable multi-hop inference with natural language. However, an ILP formulation is non-differentiable and cannot be integrated into broader deep learning architectures. Recently, Thayaparan et al. (2021a) proposed a novel methodology to integrate ILP with Transformers to achieve end-to-end differentiability for complex multi-hop inference. While this hybrid framework has been demonstrated to deliver better answer and explanation selection than transformer-based and existing ILP solvers, the neuro-symbolic integration still relies on a convex relaxation of the ILP formulation, which can produce sub-optimal solutions. To improve these limitations, we propose Diff-Comb Explainer, a novel neuro-symbolic architecture based on Differentiable BlackBox Combinatorial solvers (DBCS) (Pogan\v{c}i\'c et al., 2019). Unlike existing differentiable solvers, the presented model does not require the transformation and relaxation of the explicit semantic constraints, allowing for direct and more efficient integration of ILP formulations. Diff-Comb Explainer demonstrates improved accuracy and explainability over non-differentiable solvers, Transformers and existing differentiable constraint-based multi-hop inference frameworks.

</details>

<details>

<summary>2022-08-06 08:09:42 - Data science in public health: building next generation capacity</summary>

- *Nicholas Mirin, Heather Mattie, Latifa Jackson, Zainab Samad, Rumi Chunara*

- `2208.03461v1` - [abs](http://arxiv.org/abs/2208.03461v1) - [pdf](http://arxiv.org/pdf/2208.03461v1)

> Rapidly evolving technology, data and analytic landscapes are permeating many fields and professions. In public health, the need for data science skills including data literacy is particularly prominent given both the potential of novel data types and analysis methods to fill gaps in existing public health research and intervention practices, as well as the potential of such data or methods to perpetuate or augment health disparities. Through a review of public health courses and programs at the top 10 U.S. and globally ranked schools of public health, this article summarizes existing educational efforts in public health data science. These existing practices serve to inform efforts for broadening such curricula to further schools and populations. Data science ethics course offerings are also examined in context of assessing how population health principles can be blended into training across levels of data involvement to augment the traditional core of public health curricula. Parallel findings from domestic and international 'outside the classroom' training programs are also synthesized to advance approaches for increasing diversity in public health data science. Based on these program reviews and their synthesis, a four-point formula is distilled for furthering public health data science education efforts, toward development of a critical and inclusive mass of practitioners with fluency to leverage data to advance goals of public health and improve quality of life in the digital age.

</details>

<details>

<summary>2022-08-06 16:55:09 - Teaching Qubits to Sing: Mission Impossible?</summary>

- *Eduardo Reck Miranda, Brian N. Siegelwax*

- `2207.08225v3` - [abs](http://arxiv.org/abs/2207.08225v3) - [pdf](http://arxiv.org/pdf/2207.08225v3)

> This paper introduces a system that learns to sing new tunes by listening to examples. It extracts sequencing rules from input music and uses these rules to generate new tunes, which are sung by a vocal synthesiser. We developed a method to represent rules for musical composition as quantum circuits. We claim that such musical rules are quantum native: they are naturally encodable in the amplitudes of quantum states. To evaluate a rule to generate a subsequent event, the system builds the respective quantum circuit dynamically and measures it. After a brief discussion about the vocal synthesis methods that we have been experimenting with, the paper introduces our novel generative music method through a practical example. The paper shows some experiments and concludes with a discussion about harnessing the creative potential of the system.

</details>

<details>

<summary>2022-08-06 20:19:08 - Revisiting Gaussian Neurons for Online Clustering with Unknown Number of Clusters</summary>

- *Ole Christian Eidheim*

- `2205.00920v2` - [abs](http://arxiv.org/abs/2205.00920v2) - [pdf](http://arxiv.org/pdf/2205.00920v2)

> Despite the recent success of artificial neural networks, more biologically plausible learning methods may be needed to resolve the weaknesses of backpropagation trained models such as catastrophic forgetting and adversarial attacks. Although these weaknesses are not specifically addressed, a novel local learning rule is presented that performs online clustering with an upper limit on the number of clusters to be found rather than a fixed cluster count. Instead of using orthogonal weight or output activation constraints, activation sparsity is achieved by mutual repulsion of lateral Gaussian neurons ensuring that multiple neuron centers cannot occupy the same location in the input domain. An update method is also presented for adjusting the widths of the Gaussian neurons in cases where the data samples can be represented by means and variances. The algorithms were applied on the MNIST and CIFAR-10 datasets to create filters capturing the input patterns of pixel patches of various sizes. The experimental results demonstrate stability in the learned parameters across a large number of training samples.

</details>

<details>

<summary>2022-08-07 00:49:36 - Cyber Pirates Ahoy! An Analysis of Cybersecurity Challenges in the Shipping Industry</summary>

- *George Grispos, William R. Mahoney*

- `2208.03607v1` - [abs](http://arxiv.org/abs/2208.03607v1) - [pdf](http://arxiv.org/pdf/2208.03607v1)

> Maritime shipping has become a trillion-dollar industry that now impacts the economy of virtually every country around the world. It is therefore no surprise that countries and companies have spent billions of dollars to modernize shipping vessels and ports with various technologies. However, the implementation of these technologies has also caught the attention of cybercriminals. For example, a cyberattack on one shipping company resulted in nearly $300 millions in financial losses. Hence, this paper describes cybersecurity vulnerabilities present in the international shipping business. The contribution of this paper is the identification and dissection of cyber vulnerabilities specific to the shipping industry, along with how and why these potential vulnerabilities exist.

</details>

<details>

<summary>2022-08-07 02:44:23 - Parabolic Relaxation for Quadratically-constrained Quadratic Programming -- Part I: Definitions & Basic Properties</summary>

- *Ramtin Madani, Mersedeh Ashraphijuo, Mohsen Kheirandishfard, Alper Atamturk*

- `2208.03622v1` - [abs](http://arxiv.org/abs/2208.03622v1) - [pdf](http://arxiv.org/pdf/2208.03622v1)

> For general quadratically-constrained quadratic programming (QCQP), we propose a parabolic relaxation described with convex quadratic constraints. An interesting property of the parabolic relaxation is that the original non-convex feasible set is contained on the boundary of the parabolic relaxation. Under certain assumptions, this property enables one to recover near-optimal feasible points via objective penalization. Moreover, through an appropriate change of coordinates that requires a one-time computation of an optimal basis, the easier-to-solve parabolic relaxation can be made as strong as a semidefinite programming (SDP) relaxation, which can be effective in accelerating algorithms that require solving a sequence of convex surrogates. The majority of theoretical and computational results are given in the next part of this work [57].

</details>

<details>

<summary>2022-08-07 02:58:04 - Parabolic Relaxation for Quadratically-constrained Quadratic Programming -- Part II: Theoretical & Computational Results</summary>

- *Ramtin Madani, Mersedeh Ashraphijuo, Mohsen Kheirandishfard, Alper Atamturk*

- `2208.03625v1` - [abs](http://arxiv.org/abs/2208.03625v1) - [pdf](http://arxiv.org/pdf/2208.03625v1)

> In the first part of this work [32], we introduce a convex parabolic relaxation for quadratically-constrained quadratic programs, along with a sequential penalized parabolic relaxation algorithm to recover near-optimal feasible solutions. In this second part, we show that starting from a feasible solution or a near-feasible solution satisfying certain regularity conditions, the sequential penalized parabolic relaxation algorithm convergences to a point which satisfies Karush-Kuhn-Tucker optimality conditions. Next, we present numerical experiments on benchmark non-convex QCQP problems as well as large-scale instances of system identification problem demonstrating the efficiency of the proposed approach.

</details>

<details>

<summary>2022-08-07 03:50:34 - An Enclave-based TEE for SE-in-SoC in RISC-V Industry</summary>

- *Xuanle Ren, Xiaoxia Cui*

- `2208.03631v1` - [abs](http://arxiv.org/abs/2208.03631v1) - [pdf](http://arxiv.org/pdf/2208.03631v1)

> Secure Element (SE) in SoC sees an increasing adoption in industry. Many applications in IoT devices are bound to the SE because it provides strong cryptographic functions and physical protection. Though SE-in-SoC provides strong proven isolation for software programs, it also brings more design complexity and higher cost to PCB board building. More, SE-in-SoC may still have security concerns, such as malware installation and user impersonation. In this work, we employ TEE, a hardware-backed security technique, for protecting SE-in-SoC and RISCV. In particular, we construct various enclaves for isolating applications and manipulating the SE, with the inherently-secure primitives provided by RISC-V. Using hardware and software co-design, the solution ensures trusted execution and secure communication among applications. The security of SE is further protected by enforcing the SE to be controlled by a trusted enclave and making the RISC-V core resilient to side-channel attacks.

</details>

<details>

<summary>2022-08-07 12:26:25 - Towards Distributed Logic Programming based on Computability Logic</summary>

- *Keehang Kwon*

- `1909.07036v3` - [abs](http://arxiv.org/abs/1909.07036v3) - [pdf](http://arxiv.org/pdf/1909.07036v3)

> {\em Computability logic} (CoL) is a powerful computational model which views computational problems as games played by a machine and its environment. In this paper, we show that CoL naturally supports multiagent programming models with distributed control. To be specific, we discuss a distributed logic programming model based on CoL (CL1 to be exact), which we call CL1^{\Omega}. The key feature of this model is that it supports $dynamic/evolving$ knowledgebase of an agent. This model turns out to be a promising approach to reaching both general AI and future computing model.

</details>

<details>

<summary>2022-08-07 15:54:19 - Learning Modular Structures That Generalize Out-of-Distribution</summary>

- *Arjun Ashok, Chaitanya Devaguptapu, Vineeth Balasubramanian*

- `2208.03753v1` - [abs](http://arxiv.org/abs/2208.03753v1) - [pdf](http://arxiv.org/pdf/2208.03753v1)

> Out-of-distribution (O.O.D.) generalization remains to be a key challenge for real-world machine learning systems. We describe a method for O.O.D. generalization that, through training, encourages models to only preserve features in the network that are well reused across multiple training domains. Our method combines two complementary neuron-level regularizers with a probabilistic differentiable binary mask over the network, to extract a modular sub-network that achieves better O.O.D. performance than the original network. Preliminary evaluation on two benchmark datasets corroborates the promise of our method.

</details>

<details>

<summary>2022-08-07 20:53:42 - Decomposable Non-Smooth Convex Optimization with Nearly-Linear Gradient Oracle Complexity</summary>

- *Sally Dong, Haotian Jiang, Yin Tat Lee, Swati Padmanabhan, Guanghao Ye*

- `2208.03811v1` - [abs](http://arxiv.org/abs/2208.03811v1) - [pdf](http://arxiv.org/pdf/2208.03811v1)

> Many fundamental problems in machine learning can be formulated by the convex program \[ \min_{\theta\in R^d}\ \sum_{i=1}^{n}f_{i}(\theta), \] where each $f_i$ is a convex, Lipschitz function supported on a subset of $d_i$ coordinates of $\theta$. One common approach to this problem, exemplified by stochastic gradient descent, involves sampling one $f_i$ term at every iteration to make progress. This approach crucially relies on a notion of uniformity across the $f_i$'s, formally captured by their condition number. In this work, we give an algorithm that minimizes the above convex formulation to $\epsilon$-accuracy in $\widetilde{O}(\sum_{i=1}^n d_i \log (1 /\epsilon))$ gradient computations, with no assumptions on the condition number. The previous best algorithm independent of the condition number is the standard cutting plane method, which requires $O(nd \log (1/\epsilon))$ gradient computations. As a corollary, we improve upon the evaluation oracle complexity for decomposable submodular minimization by Axiotis et al. (ICML 2021). Our main technical contribution is an adaptive procedure to select an $f_i$ term at every iteration via a novel combination of cutting-plane and interior-point methods.

</details>

<details>

<summary>2022-08-08 03:34:58 - Neural Optimization Machine: A Neural Network Approach for Optimization</summary>

- *Jie Chen, Yongming Liu*

- `2208.03897v1` - [abs](http://arxiv.org/abs/2208.03897v1) - [pdf](http://arxiv.org/pdf/2208.03897v1)

> A novel neural network (NN) approach is proposed for constrained optimization. The proposed method uses a specially designed NN architecture and training/optimization procedure called Neural Optimization Machine (NOM). The objective functions for the NOM are approximated with NN models. The optimization process is conducted by the neural network's built-in backpropagation algorithm. The NOM solves optimization problems by extending the architecture of the NN objective function model. This is achieved by appropriately designing the NOM's structure, activation function, and loss function. The NN objective function can have arbitrary architectures and activation functions. The application of the NOM is not limited to specific optimization problems, e.g., linear and quadratic programming. It is shown that the increase of dimension of design variables does not increase the computational cost significantly. Then, the NOM is extended for multiobjective optimization. Finally, the NOM is tested using numerical optimization problems and applied for the optimal design of processing parameters in additive manufacturing.

</details>

<details>

<summary>2022-08-08 05:45:40 - CSSAM:Code Search via Attention Matching of Code Semantics and Structures</summary>

- *Yi Hu, Bo Cai, Yaoxiang Yu*

- `2208.03922v1` - [abs](http://arxiv.org/abs/2208.03922v1) - [pdf](http://arxiv.org/pdf/2208.03922v1)

> Despite the continuous efforts in improving both the effectiveness and efficiency of code search, two issues remained unsolved. First, programming languages have inherent strong structural linkages, and feature mining of code as text form would omit the structural information contained inside it. Second, there is a potential semantic relationship between code and query, it is challenging to align code and text across sequences so that vectors are spatially consistent during similarity matching. To tackle both issues, in this paper, a code search model named CSSAM (Code Semantics and Structures Attention Matching) is proposed. By introducing semantic and structural matching mechanisms, CSSAM effectively extracts and fuses multidimensional code features. Specifically, the cross and residual layer was developed to facilitate high-latitude spatial alignment of code and query at the token level. By leveraging the residual interaction, a matching module is designed to preserve more code semantics and descriptive features, that enhances the adhesion between the code and its corresponding query text. Besides, to improve the model's comprehension of the code's inherent structure, a code representation structure named CSRG (Code Semantic Representation Graph) is proposed for jointly representing abstract syntax tree nodes and the data flow of the codes. According to the experimental results on two publicly available datasets containing 540k and 330k code segments, CSSAM significantly outperforms the baselines in terms of achieving the highest SR@1/5/10, MRR, and NDCG@50 on both datasets respectively. Moreover, the ablation study is conducted to quantitatively measure the impact of each key component of CSSAM on the efficiency and effectiveness of code search, which offers the insights into the improvement of advanced code search solutions.

</details>

<details>

<summary>2022-08-08 11:02:16 - ICAF: Iterative Contrastive Alignment Framework for Multimodal Abstractive Summarization</summary>

- *Zijian Zhang, Chang Shu, Youxin Chen, Jing Xiao, Qian Zhang, Lu Zheng*

- `2108.05123v3` - [abs](http://arxiv.org/abs/2108.05123v3) - [pdf](http://arxiv.org/pdf/2108.05123v3)

> Integrating multimodal knowledge for abstractive summarization task is a work-in-progress research area, with present techniques inheriting fusion-then-generation paradigm. Due to semantic gaps between computer vision and natural language processing, current methods often treat multiple data points as separate objects and rely on attention mechanisms to search for connection in order to fuse together. In addition, missing awareness of cross-modal matching from many frameworks leads to performance reduction. To solve these two drawbacks, we propose an Iterative Contrastive Alignment Framework (ICAF) that uses recurrent alignment and contrast to capture the coherences between images and texts. Specifically, we design a recurrent alignment (RA) layer to gradually investigate fine-grained semantical relationships between image patches and text tokens. At each step during the encoding process, cross-modal contrastive losses are applied to directly optimize the embedding space. According to ROUGE, relevance scores, and human evaluation, our model outperforms the state-of-the-art baselines on MSMO dataset. Experiments on the applicability of our proposed framework and hyperparameters settings have been also conducted.

</details>

<details>

<summary>2022-08-08 12:30:25 - Selectively Combining Multiple Coverage Goals in Search-Based Unit Test Generation</summary>

- *Zhichao Zhou, Yuming Zhou, Chunrong Fang, Zhenyu Chen, Yutian Tang*

- `2208.04096v1` - [abs](http://arxiv.org/abs/2208.04096v1) - [pdf](http://arxiv.org/pdf/2208.04096v1)

> Unit testing is a critical part of software development process, ensuring the correctness of basic programming units in a program (e.g., a method). Search-based software testing (SBST) is an automated approach to generating test cases. SBST generates test cases with genetic algorithms by specifying the coverage criterion (e.g., branch coverage). However, a good test suite must have different properties, which cannot be captured by using an individual coverage criterion. Therefore, the state-of-the-art approach combines multiple criteria to generate test cases. As combining multiple coverage criteria brings multiple objectives for optimization, it hurts the test suites' coverage for certain criteria compared with using the single criterion. To cope with this problem, we propose a novel approach named \textbf{smart selection}. Based on the coverage correlations among criteria and the coverage goals' subsumption relationships, smart selection selects a subset of coverage goals to reduce the number of optimization objectives and avoid missing any properties of all criteria. We conduct experiments to evaluate smart selection on $400$ Java classes with three state-of-the-art genetic algorithms. On average, smart selection outperforms combining all goals on $65.1\%$ of the classes having significant differences between the two approaches.

</details>

<details>

<summary>2022-08-08 14:45:53 - FASHION: Functional and Attack graph Secured HybrId Optimization of virtualized Networks</summary>

- *Devon Callahan, Timothy Curry, Hazel Davidson, Heytem Zitoun, Benjamin Fuller, Laurent Michel*

- `1910.07921v3` - [abs](http://arxiv.org/abs/1910.07921v3) - [pdf](http://arxiv.org/pdf/1910.07921v3)

> Maintaining a resilient computer network is a delicate task with conflicting priorities. Flows should be served while controlling risk due to attackers. Upon publication of a vulnerability, administrators scramble to manually mitigate risk while waiting for a patch.   We introduce FASHION: a linear optimizer that balances routing flows with the security risk posed by these flows. FASHION formalizes routing as a multi-commodity flow problem with side constraints. FASHION formulates security using two approximations of risk in a probabilistic attack graph (Frigault et al., Network Security Metrics 2017). FASHION's output is a set of software-defined networking rules consumable by Frenetic (Foster et al., ICFP 2011).   We introduce a topology generation tool that creates data center network instances including flows and vulnerabilities. FASHION is executed on instances of up to 600 devices, thousands of flows, and million edge attack graphs. Solve time averages 30 minutes on the largest instances (seconds on the smallest instances). To ensure the security objective is accurate, the output solution is assessed using risk as defined by Frigault et al.   FASHION allows enterprises to reconfigure their network in response to changes in functionality or security requirements.

</details>

<details>

<summary>2022-08-08 15:09:52 - Origami-based Zygote structure enables pluripotent shape-transforming deployable structure</summary>

- *Yu-Ki Lee, Yue Hao, Zhonghua Xi, Woongbae Kim, Youngmin Park, Kyu-Jin Cho, Jyh-Ming Lien, In-Suk Choi*

- `2208.04204v1` - [abs](http://arxiv.org/abs/2208.04204v1) - [pdf](http://arxiv.org/pdf/2208.04204v1)

> We propose an algorithmic framework of a pluripotent structure evolving from a simple compact structure into diverse complex 3-D structures for designing the shape transformable, reconfigurable, and deployable structures and robots. Our algorithmic approach suggests a way of transforming a compact structure consisting of uniform building blocks into a large, desired 3-D shape. Analogous to the pluripotent stem cells that can grow into a preprogrammed shape according to coded information, which we call DNA, compactly stacked panels named the zygote structure can evolve into arbitrary 3-D structures by programming their connection path. Our stacking algorithm obtains this coded sequence by inversely stacking the voxelized surface of the desired structure into a tree. Applying the connection path obtained by the stacking algorithm, the compactly stacked panels named the zygote structure can be deployed into diverse large 3-D structures. We conceptually demonstrated our pluripotent evolving structure by energy releasing commercial spring hinges and thermally actuated shape memory alloy (SMA) hinges, respectively. We also show that the proposed concept enables the fabrication of large structures in a significantly smaller workspace.

</details>

<details>

<summary>2022-08-08 15:11:48 - POTHER: Patch-Voted Deep Learning-Based Chest X-ray Bias Analysis for COVID-19 Detection</summary>

- *Tomasz SzczepaÅski, Arkadiusz Sitek, Tomasz TrzciÅski, Szymon PÅotka*

- `2201.09360v5` - [abs](http://arxiv.org/abs/2201.09360v5) - [pdf](http://arxiv.org/pdf/2201.09360v5)

> A critical step in the fight against COVID-19, which continues to have a catastrophic impact on peoples lives, is the effective screening of patients presented in the clinics with severe COVID-19 symptoms. Chest radiography is one of the promising screening approaches. Many studies reported detecting COVID-19 in chest X-rays accurately using deep learning. A serious limitation of many published approaches is insufficient attention paid to explaining decisions made by deep learning models. Using explainable artificial intelligence methods, we demonstrate that model decisions may rely on confounding factors rather than medical pathology. After an analysis of potential confounding factors found on chest X-ray images, we propose a novel method to minimise their negative impact. We show that our proposed method is more robust than previous attempts to counter confounding factors such as ECG leads in chest X-rays that often influence model classification decisions. In addition to being robust, our method achieves results comparable to the state-of-the-art. The source code and pre-trained weights are publicly available at (https://github.com/tomek1911/POTHER).

</details>

<details>

<summary>2022-08-08 15:44:09 - A Linear Programming Approach for Resource-Aware Information-Theoretic Tree Abstractions</summary>

- *Daniel T. Larsson, Dipankar Maity, Panagiotis Tsiotras*

- `2208.04220v1` - [abs](http://arxiv.org/abs/2208.04220v1) - [pdf](http://arxiv.org/pdf/2208.04220v1)

> In this chapter, an integer linear programming formulation for the problem of obtaining task-relevant, multi-resolution, environment abstractions for resource-constrained autonomous agents is presented. The formulation leverages concepts from information-theoretic signal compression, specifically, the information bottleneck (IB) method, to pose an abstraction problem as an optimal encoder search over the space of multi-resolution trees. The abstractions emerge in a task-relevant manner as a function of agent information-processing constraints. We detail our formulation, and show how hierarchical tree structures, signal encoders, and information-theoretic methods for signal compression can be unified under a common theme. A discussion delineating the benefits and drawbacks of our formulation is presented, as well as a detailed explanation how our approach can be interpreted within the context of generating abstractions for resource-constrained autonomous systems. It is shown that the resulting information-theoretic abstraction problem over the space of multi-resolution trees can be formulated as a integer linear programming (ILP) problem. We demonstrate the approach on a number of examples, and provide a discussion detailing the differences of the proposed framework compared to existing methods. Lastly, we consider a linear program relaxation of the ILP problem, thereby demonstrating that multi-resolution information-theoretic tree abstractions can be obtained by solving a convex program.

</details>

<details>

<summary>2022-08-08 17:25:55 - Continual Reinforcement Learning with TELLA</summary>

- *Neil Fendley, Cash Costello, Eric Nguyen, Gino Perrotta, Corey Lowman*

- `2208.04287v1` - [abs](http://arxiv.org/abs/2208.04287v1) - [pdf](http://arxiv.org/pdf/2208.04287v1)

> Training reinforcement learning agents that continually learn across multiple environments is a challenging problem. This is made more difficult by a lack of reproducible experiments and standard metrics for comparing different continual learning approaches. To address this, we present TELLA, a tool for the Test and Evaluation of Lifelong Learning Agents. TELLA provides specified, reproducible curricula to lifelong learning agents while logging detailed data for evaluation and standardized analysis. Researchers can define and share their own curricula over various learning environments or run against a curriculum created under the DARPA Lifelong Learning Machines (L2M) Program.

</details>

<details>

<summary>2022-08-08 20:49:22 - Multi Purpose Routing: New Perspectives and Approximation Algorithms</summary>

- *Majid Farhadi, Jai Moondra, Prasad Tetali, Alejandro Toriello*

- `2208.04410v1` - [abs](http://arxiv.org/abs/2208.04410v1) - [pdf](http://arxiv.org/pdf/2208.04410v1)

> The cost due to delay in services may be intrinsically different for various applications of vehicle routing such as medical emergencies, logistical operations, and ride-sharing. We study a fundamental generalization of the Traveling Salesman Problem, namely $L_p$ TSP, where the objective is to minimize an aggregated measure of the delay in services, quantified by the Minkowski $p$-norm of the delay vector. We present efficient combinatorial and Linear Programming algorithms for approximating $L_p$ TSP on general metrics. We provide several approximation algorithms for the $L_p$ TSP problem, including $4.27$ & $10.92$-approximation algorithms for single & multi vehicle $L_2$ TSP, called the Traveling Firefighter Problem. Among other contributions, we provide an $8$-approximation and a $1.78$ inapproximability for All-Norm TSP problem, addressing scenarios where one does not know the ideal cost function, or is seeking simultaneous approximation with respect to any cost function.

</details>

<details>

<summary>2022-08-08 21:09:24 - Formalization of a Stochastic Approximation Theorem</summary>

- *Koundinya Vajjha, Barry Trager, Avraham Shinnar, Vasily Pestun*

- `2202.05959v2` - [abs](http://arxiv.org/abs/2202.05959v2) - [pdf](http://arxiv.org/pdf/2202.05959v2)

> Stochastic approximation algorithms are iterative procedures which are used to approximate a target value in an environment where the target is unknown and direct observations are corrupted by noise. These algorithms are useful, for instance, for root-finding and function minimization when the target function or model is not directly known. Originally introduced in a 1951 paper by Robbins and Monro, the field of Stochastic approximation has grown enormously and has come to influence application domains from adaptive signal processing to artificial intelligence. As an example, the Stochastic Gradient Descent algorithm which is ubiquitous in various subdomains of Machine Learning is based on stochastic approximation theory. In this paper, we give a formal proof (in the Coq proof assistant) of a general convergence theorem due to Aryeh Dvoretzky, which implies the convergence of important classical methods such as the Robbins-Monro and the Kiefer-Wolfowitz algorithms. In the process, we build a comprehensive Coq library of measure-theoretic probability theory and stochastic processes.

</details>

<details>

<summary>2022-08-08 21:37:38 - DF-SCA: Dynamic Frequency Side Channel Attacks are Practical</summary>

- *Debopriya Roy Dipta, Berk Gulmezoglu*

- `2206.13660v2` - [abs](http://arxiv.org/abs/2206.13660v2) - [pdf](http://arxiv.org/pdf/2206.13660v2)

> The arm race between hardware security engineers and side-channel researchers has become more competitive with more sophisticated attacks and defenses in the last decade. While modern hardware features improve the system performance significantly, they may create new attack surfaces for malicious people to extract sensitive information about users without physical access to the victim device. Although many previously exploited hardware and OS features were patched by OS developers and chip vendors, any feature that is accessible from userspace applications can be exploited to perform software-based side-channel attacks.   In this paper, we present DF-SCA, which is a software-based dynamic frequency side-channel attack on Linux and Android OS devices. We exploit unprivileged access to cpufreq interface that exposes real-time CPU core frequency values directly correlated with the system utilization, creating a reliable side-channel for attackers. We show that Dynamic Voltage and Frequency Scaling (DVFS) feature in modern systems can be utilized to perform website fingerprinting attacks for Google Chrome and Tor browsers on modern Intel, AMD, and ARM architectures. We further extend our analysis to a wide selection of scaling governors on Intel and AMD CPUs, verifying that all scaling governors provide enough information on the visited web page. Moreover, we extract properties of keystroke patterns on frequency readings, that leads to 95% accuracy to distinguish the keystrokes from other activities on Android phones. We leverage inter-keystroke timings of a user by training a k-th nearest neighbor model, which achieves 88% password recovery rate in the first guess on Bank of America application. Finally, we propose several countermeasures to mask the user activity to mitigate DF-SCA on Linux-based systems.

</details>

<details>

<summary>2022-08-08 21:39:17 - Consistent Approximations in Composite Optimization</summary>

- *Johannes O. Royset*

- `2201.05250v2` - [abs](http://arxiv.org/abs/2201.05250v2) - [pdf](http://arxiv.org/pdf/2201.05250v2)

> Approximations of optimization problems arise in computational procedures and sensitivity analysis. The resulting effect on solutions can be significant, with even small approximations of components of a problem translating into large errors in the solutions. We specify conditions under which approximations are well behaved in the sense of minimizers, stationary points, and level-sets and this leads to a framework of consistent approximations. The framework is developed for a broad class of composite problems, which are neither convex nor smooth. We demonstrate the framework using examples from stochastic optimization, neural-network based machine learning, distributionally robust optimization, penalty and augmented Lagrangian methods, interior-point methods, homotopy methods, smoothing methods, extended nonlinear programming, difference-of-convex programming, and multi-objective optimization. An enhanced proximal method illustrates the algorithmic possibilities. A quantitative analysis supplements the development by furnishing rates of convergence.

</details>

<details>

<summary>2022-08-08 21:51:08 - Learning from Sparse Demonstrations</summary>

- *Wanxin Jin, Todd D. Murphey, Dana KuliÄ, Neta Ezer, Shaoshuai Mou*

- `2008.02159v3` - [abs](http://arxiv.org/abs/2008.02159v3) - [pdf](http://arxiv.org/pdf/2008.02159v3)

> This paper develops the method of Continuous Pontryagin Differentiable Programming (Continuous PDP), which enables a robot to learn an objective function from a few sparsely demonstrated keyframes. The keyframes, labeled with some time stamps, are the desired task-space outputs, which a robot is expected to follow sequentially. The time stamps of the keyframes can be different from the time of the robot's actual execution. The method jointly finds an objective function and a time-warping function such that the robot's resulting trajectory sequentially follows the keyframes with minimal discrepancy loss. The Continuous PDP minimizes the discrepancy loss using projected gradient descent, by efficiently solving the gradient of the robot trajectory with respect to the unknown parameters. The method is first evaluated on a simulated robot arm and then applied to a 6-DoF quadrotor to learn an objective function for motion planning in unmodeled environments. The results show the efficiency of the method, its ability to handle time misalignment between keyframes and robot execution, and the generalization of objective learning into unseen motion conditions.

</details>

<details>

<summary>2022-08-09 01:28:30 - Learning to Improve Code Efficiency</summary>

- *Binghong Chen, Daniel Tarlow, Kevin Swersky, Martin Maas, Pablo Heiber, Ashish Naik, Milad Hashemi, Parthasarathy Ranganathan*

- `2208.05297v1` - [abs](http://arxiv.org/abs/2208.05297v1) - [pdf](http://arxiv.org/pdf/2208.05297v1)

> Improvements in the performance of computing systems, driven by Moore's Law, have transformed society. As such hardware-driven gains slow down, it becomes even more important for software developers to focus on performance and efficiency during development. While several studies have demonstrated the potential from such improved code efficiency (e.g., 2x better generational improvements compared to hardware), unlocking these gains in practice has been challenging. Reasoning about algorithmic complexity and the interaction of coding patterns on hardware can be challenging for the average programmer, especially when combined with pragmatic constraints around development velocity and multi-person development.   This paper seeks to address this problem. We analyze a large competitive programming dataset from the Google Code Jam competition and find that efficient code is indeed rare, with a 2x runtime difference between the median and the 90th percentile of solutions. We propose using machine learning to automatically provide prescriptive feedback in the form of hints, to guide programmers towards writing high-performance code. To automatically learn these hints from the dataset, we propose a novel discrete variational auto-encoder, where each discrete latent variable represents a different learned category of code-edit that increases performance. We show that this method represents the multi-modal space of code efficiency edits better than a sequence-to-sequence baseline and generates a distribution of more efficient solutions.

</details>

<details>

<summary>2022-08-09 09:53:12 - Session Fidelity for ElixirST: A Session-Based Type System for Elixir Modules</summary>

- *Gerard Tabone, Adrian Francalanza*

- `2208.04631v1` - [abs](http://arxiv.org/abs/2208.04631v1) - [pdf](http://arxiv.org/pdf/2208.04631v1)

> This paper builds on prior work investigating the adaptation of session types to provide behavioural information about Elixir modules. A type system called ElixirST has been constructed to statically determine whether functions in an Elixir module observe their endpoint specifications, expressed as session types; a corresponding tool automating this typechecking has also been constructed. In this paper we formally validate this type system. An LTS-based operational semantics for the language fragment supported by the type system is developed, modelling its runtime behaviour when invoked by the module client. This operational semantics is then used to prove session fidelity for ElixirST.

</details>

<details>

<summary>2022-08-09 12:25:06 - STELLA: Sparse Taint Analysis for Enclave Leakage Detection</summary>

- *Yang Chen, Jianfeng Jiang, Shoumeng Yan, Hui Xu*

- `2208.04719v1` - [abs](http://arxiv.org/abs/2208.04719v1) - [pdf](http://arxiv.org/pdf/2208.04719v1)

> Intel SGX (Software Guard Extension) is a promising TEE (trusted execution environment) technique that can protect programs running in user space from being maliciously accessed by the host operating system. Although it provides hardware access control and memory encryption, the actual effectiveness also depends on the quality of the software. In particular, improper implementation of a code snippet running inside the enclave may still leak private data due to the invalid use of pointers. This paper serves as a first attempt to study the privacy leakage issues of enclave code and proposes a novel static sparse taint analysis approach to detect them. We first summarize five common patterns of leakage code. Based on these patterns, our approach performs forward analysis to recognize all taint sinks and then employs a backward approach to detect leakages. Finally, we have conducted experiments with several open-source enclave programs and found 78 vulnerabilities previously unknown in 13 projects.

</details>

<details>

<summary>2022-08-09 12:31:06 - A general theoretical scheme for shape-programming of incompressible hyperelastic shells through differential growth</summary>

- *Zhanfeng Li, Jiong Wang, Mokarram Hossain, Chennakesava Kadapa*

- `2210.06202v1` - [abs](http://arxiv.org/abs/2210.06202v1) - [pdf](http://arxiv.org/pdf/2210.06202v1)

> In this paper, we study the problem of shape-programming of incompressible hyperelastic shells through differential growth. The aim of the current work is to determine the growth tensor (or growth functions) that can produce the deformation of a shell to the desired shape. First, a consistent finite-strain shell theory is introduced. The shell equation system is established from the 3D governing system through a series expansion and truncation approach. Based on the shell theory, the problem of shape-programming is studied under the stress-free assumption. For a special case in which the parametric coordinate curves generate a net of curvature lines on the target surface, the sufficient condition to ensure the vanishing of the stress components is analyzed, from which the explicit expression of the growth tensor can be derived. In the general case, we conduct the variable changes and derive the total growth tensor by considering a two-step deformation of the shell. With these obtained results, a general theoretical scheme for shape-programming of thin hyperelastic shells through differential growth is proposed. To demonstrate the feasibility and efficiency of the proposed scheme, several nature-inspired examples are studied. The derived growth tensors in these examples have also been implemented in the numerical simulations to verify their correctness and accuracy. The simulation results show that the target shapes of the shell samples can be recovered completely. The scheme for shape-programming proposed in the current work is helpful in designing and manufacturing intelligent soft devices.

</details>

<details>

<summary>2022-08-09 15:25:24 - Robust Machine Learning for Malware Detection over Time</summary>

- *Daniele Angioni, Luca Demetrio, Maura Pintor, Battista Biggio*

- `2208.04838v1` - [abs](http://arxiv.org/abs/2208.04838v1) - [pdf](http://arxiv.org/pdf/2208.04838v1)

> The presence and persistence of Android malware is an on-going threat that plagues this information era, and machine learning technologies are now extensively used to deploy more effective detectors that can block the majority of these malicious programs. However, these algorithms have not been developed to pursue the natural evolution of malware, and their performances significantly degrade over time because of such concept-drift. Currently, state-of-the-art techniques only focus on detecting the presence of such drift, or they address it by relying on frequent updates of models. Hence, there is a lack of knowledge regarding the cause of the concept drift, and ad-hoc solutions that can counter the passing of time are still under-investigated. In this work, we commence to address these issues as we propose (i) a drift-analysis framework to identify which characteristics of data are causing the drift, and (ii) SVM-CB, a time-aware classifier that leverages the drift-analysis information to slow down the performance drop. We highlight the efficacy of our contribution by comparing its degradation over time with a state-of-the-art classifier, and we show that SVM-CB better withstands the distribution changes that naturally characterize the malware domain. We conclude by discussing the limitations of our approach and how our contribution can be taken as a first step towards more time-resistant classifiers that not only tackle, but also understand the concept drift that affects data.

</details>

<details>

<summary>2022-08-09 16:08:42 - The application of adaptive minimum match k-nearest neighbors to identify at-risk students in health professions education</summary>

- *Anshul Kumar, Taylor DiJohnson, Roger Edwards, Lisa Walker*

- `2108.07709v3` - [abs](http://arxiv.org/abs/2108.07709v3) - [pdf](http://arxiv.org/pdf/2108.07709v3)

> Purpose: When a learner fails to reach a milestone, educators often wonder if there had been any warning signs that could have allowed them to intervene sooner. Machine learning can predict which students are at risk of failing a high-stakes certification exam. If predictions can be made well in advance of the exam, then educators can meaningfully intervene before students take the exam to reduce the chances of a failing score.   Methods: Using already-collected, first-year student assessment data from five cohorts in a Master of Physician Assistant Studies program, the authors implement an "adaptive minimum match" version of the k-nearest neighbors algorithm (AMMKNN), using changing numbers of neighbors to predict each student's future exam scores on the Physician Assistant National Certifying Examination (PANCE). Validation occurred in two ways: Leave-one-out cross-validation (LOOCV) and evaluating the predictions in a new cohort.   Results: AMMKNN achieved an accuracy of 93% in LOOCV. AMMKNN generates a predicted PANCE score for each student, one year before they are scheduled to take the exam. Students can then be classified into extra support, optional extra support, or no extra support groups. The educator then has one year to provide the appropriate customized support to each category of student.   Conclusions: Predictive analytics can identify at-risk students, so they can receive additional support or remediation when preparing for high-stakes certification exams. Educators can use the included methods and code to generate predicted test outcomes for students. The authors recommend that educators use this or similar predictive methods responsibly and transparently, as one of many tools used to support students.

</details>

<details>

<summary>2022-08-09 16:44:01 - Online Malware Classification with System-Wide System Calls in Cloud IaaS</summary>

- *Phillip Brown, Austin Brown, Maanak Gupta, Mahmoud Abdelsalam*

- `2208.04891v1` - [abs](http://arxiv.org/abs/2208.04891v1) - [pdf](http://arxiv.org/pdf/2208.04891v1)

> Accurately classifying malware in an environment allows the creation of better response and remediation strategies by cyber analysts. However, classifying malware in a live environment is a difficult task due to the large number of system data sources. Collecting statistics from these separate sources and processing them together in a form that can be used by a machine learning model is difficult. Fortunately, all of these resources are mediated by the operating system's kernel. User programs, malware included, interacts with system resources by making requests to the kernel with system calls. Collecting these system calls provide insight to the interaction with many system resources in a single location. Feeding these system calls into a performant model such as a random forest allows fast, accurate classification in certain situations. In this paper, we evaluate the feasibility of using system call sequences for online malware classification in both low-activity and heavy-use Cloud IaaS. We collect system calls as they are received by the kernel and take n-gram sequences of calls to use as features for tree-based machine learning models. We discuss the performance of the models on baseline systems with no extra running services and systems under heavy load and the performance gap between them.

</details>

<details>

<summary>2022-08-09 21:47:01 - Limitations of Language Models in Arithmetic and Symbolic Induction</summary>

- *Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan*

- `2208.05051v1` - [abs](http://arxiv.org/abs/2208.05051v1) - [pdf](http://arxiv.org/pdf/2208.05051v1)

> Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.

</details>

<details>

<summary>2022-08-10 05:39:56 - Offline versus Online Triplet Mining based on Extreme Distances of Histopathology Patches</summary>

- *Milad Sikaroudi, Benyamin Ghojogh, Amir Safarpoor, Fakhri Karray, Mark Crowley, H. R. Tizhoosh*

- `2007.02200v3` - [abs](http://arxiv.org/abs/2007.02200v3) - [pdf](http://arxiv.org/pdf/2007.02200v3)

> We analyze the effect of offline and online triplet mining for colorectal cancer (CRC) histopathology dataset containing 100,000 patches. We consider the extreme, i.e., farthest and nearest patches to a given anchor, both in online and offline mining. While many works focus solely on selecting the triplets online (batch-wise), we also study the effect of extreme distances and neighbor patches before training in an offline fashion. We analyze extreme cases' impacts in terms of embedding distance for offline versus online mining, including easy positive, batch semi-hard, batch hard triplet mining, neighborhood component analysis loss, its proxy version, and distance weighted sampling. We also investigate online approaches based on extreme distance and comprehensively compare offline, and online mining performance based on the data patterns and explain offline mining as a tractable generalization of the online mining with large mini-batch size. As well, we discuss the relations of different colorectal tissue types in terms of extreme distances. We found that offline and online mining approaches have comparable performances for a specific architecture, such as ResNet-18 in this study. Moreover, we found the assorted case, including different extreme distances, is promising, especially in the online approach.

</details>

<details>

<summary>2022-08-10 08:07:10 - Quantum-Inspired Tensor Neural Networks for Partial Differential Equations</summary>

- *Raj Patel, Chia-Wei Hsing, Serkan Sahin, Saeed S. Jahromi, Samuel Palmer, Shivam Sharma, Christophe Michel, Vincent Porte, Mustafa Abid, Stephane Aubert, Pierre Castellani, Chi-Guhn Lee, Samuel Mugel, Roman Orus*

- `2208.02235v2` - [abs](http://arxiv.org/abs/2208.02235v2) - [pdf](http://arxiv.org/pdf/2208.02235v2)

> Partial Differential Equations (PDEs) are used to model a variety of dynamical systems in science and engineering. Recent advances in deep learning have enabled us to solve them in a higher dimension by addressing the curse of dimensionality in new ways. However, deep learning methods are constrained by training time and memory. To tackle these shortcomings, we implement Tensor Neural Networks (TNN), a quantum-inspired neural network architecture that leverages Tensor Network ideas to improve upon deep learning approaches. We demonstrate that TNN provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. We benchmark TNN by applying them to solve parabolic PDEs, specifically the Black-Scholes-Barenblatt equation, widely used in financial pricing theory, empirically showing the advantages of TNN over DNN. Further examples, such as the Hamilton-Jacobi-Bellman equation, are also discussed.

</details>

<details>

<summary>2022-08-10 12:50:12 - Proceedings End-to-End Compositional Models of Vector-Based Semantics</summary>

- *Michael Moortgat, Gijs Wijnholds*

- `2208.05313v1` - [abs](http://arxiv.org/abs/2208.05313v1) - [pdf](http://arxiv.org/pdf/2208.05313v1)

> The workshop End-to-End Compositional Models of Vector-Based Semantics was held at NUI Galway on 15 and 16 August 2022 as part of the 33rd European Summer School in Logic, Language and Information (ESSLLI 2022).   The workshop was sponsored by the research project 'A composition calculus for vector-based semantic modelling with a localization for Dutch' (Dutch Research Council 360-89-070, 2017-2022). The workshop program was made up of two parts, the first part reporting on the results of the aforementioned project, the second part consisting of contributed papers on related approaches. The present volume collects the contributed papers and the abstracts of the invited talks.

</details>

<details>

<summary>2022-08-10 15:36:12 - Knapsack Secretary Through Boosting</summary>

- *Andreas Abels, Leon Ladewig, Kevin Schewior, Moritz StinzendÃ¶rfer*

- `2208.05396v1` - [abs](http://arxiv.org/abs/2208.05396v1) - [pdf](http://arxiv.org/pdf/2208.05396v1)

> We revisit the knapsack-secretary problem (Babaioff et al.; APPROX 2007), a generalization of the classic secretary problem in which items have different sizes and multiple items may be selected if their total size does not exceed the capacity $B$ of a knapsack. Previous works show competitive ratios of $1/(10e)$ (Babaioff et al.), $1/8.06$ (Kesselheim et al.; STOC 2014), and $1/6.65$ (Albers, Khan, and Ladewig; APPROX 2019) for the general problem but no definitive answers for the achievable competitive ratio; the best known impossibility remains $1/e$ as inherited from the classic secretary problem. In an effort to make more qualitative progress, we take an orthogonal approach and give definitive answers for special cases.   Our main result is on the $1$-$2$-knapsack secretary problem, the special case in which $B=2$ and all items have sizes $1$ or $2$, arguably the simplest meaningful generalization of the secretary problem towards the knapsack secretary problem. Our algorithm is simple: It $\textit{boosts}$ the value of size-$1$ items by a factor $\alpha>1$ and then uses the size-oblivious approach by Albers, Khan, and Ladewig. We show by a nontrivial analysis that this algorithm achieves a competitive ratio of $1/e$ if and only if $1.40\lesssim\alpha\leq e/(e-1)\approx 1.58$.   Towards understanding the general case, we then consider the case when sizes are $1$ and $B$, and $B$ is large. While it remains unclear if $1/e$ can be achieved in that case, we show that algorithms based only on the relative ranks of the item values can achieve precisely a competitive ratio of $1/(e+1)$. To show the impossibility, we use a non-trivial generalization of the factor-revealing linear program for the secretary problem (Buchbinder, Jain, and Singh; IPCO 2010).

</details>

<details>

<summary>2022-08-10 15:53:42 - BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage</summary>

- *Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston*

- `2208.03188v3` - [abs](http://arxiv.org/abs/2208.03188v3) - [pdf](http://arxiv.org/pdf/2208.03188v3)

> We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.

</details>

<details>

<summary>2022-08-11 02:13:04 - Polynomial Optimization: Enhancing RLT relaxations with Conic Constraints</summary>

- *Brais GonzÃ¡lez-RodrÃ­guez, RaÃºl Alvite-PazÃ³, Samuel Alvite-PazÃ³, Bissan Ghaddar, Julio GonzÃ¡lez-DÃ­az*

- `2208.05608v1` - [abs](http://arxiv.org/abs/2208.05608v1) - [pdf](http://arxiv.org/pdf/2208.05608v1)

> Conic optimization has recently emerged as a powerful tool for designing tractable and guaranteed algorithms for non-convex polynomial optimization problems. On the one hand, tractability is crucial for efficiently solving large-scale problems and, on the other hand, strong bounds are needed to ensure high quality solutions. In this research, we investigate the strengthening of RLT relaxations of polynomial optimization problems through the addition of nine different types of constraints that are based on linear, second-order cone, and semidefinite programming to solve to optimality the instances of well established test sets of polynomial optimization problems. We describe how to design these conic constraints and their performance with respect to each other and with respect to the standard RLT relaxations. Our first finding is that the different variants of nonlinear constraints (second-order cone and semidefinite) are the best performing ones in around $50\%$ of the instances. Additionally, we present a machine learning approach to decide on the most suitable constraints to add for a given instance. The computational results show that the machine learning approach significantly outperforms each and every one of the nine individual approaches.

</details>

<details>

<summary>2022-08-11 04:12:50 - Best Policy Identification in Linear MDPs</summary>

- *Jerome Taupin, Yassir Jedra, Alexandre Proutiere*

- `2208.05633v1` - [abs](http://arxiv.org/abs/2208.05633v1) - [pdf](http://arxiv.org/pdf/2208.05633v1)

> We investigate the problem of best policy identification in discounted linear Markov Decision Processes in the fixed confidence setting under a generative model. We first derive an instance-specific lower bound on the expected number of samples required to identify an $\varepsilon$-optimal policy with probability $1-\delta$. The lower bound characterizes the optimal sampling rule as the solution of an intricate non-convex optimization program, but can be used as the starting point to devise simple and near-optimal sampling rules and algorithms. We devise such algorithms. One of these exhibits a sample complexity upper bounded by ${\cal O}({\frac{d}{(\varepsilon+\Delta)^2}} (\log(\frac{1}{\delta})+d))$ where $\Delta$ denotes the minimum reward gap of sub-optimal actions and $d$ is the dimension of the feature space. This upper bound holds in the moderate-confidence regime (i.e., for all $\delta$), and matches existing minimax and gap-dependent lower bounds. We extend our algorithm to episodic linear MDPs.

</details>

<details>

<summary>2022-08-11 07:26:18 - Correlating Effectiveness of Pointer Analysis Techniques with Patterns in Embedded System Code</summary>

- *Komal Pathade*

- `2208.05675v1` - [abs](http://arxiv.org/abs/2208.05675v1) - [pdf](http://arxiv.org/pdf/2208.05675v1)

> A pointer analysis maps the pointers in a program to the memory locations they point to. In this work, we study the effectiveness of the three flavors of pointer analysis namely flow sensitive, flow insensitive, and context sensitive analysis on seven embedded code sets used in the industry. We compare precision gain i.e., the reduction in the number of spurious memory locations pointed by a pointer in each of these settings. We found that in 90% of cases the pointer information was same in all three settings. In other cases, context sensitive analysis was 2.6% more precise than flow sensitive analysis which was 6.8% more precise than flow insensitive analysis on average. We correlate precision gain with coding patterns in the embedded systems-which we believe to be first of its kind activity.

</details>

<details>

<summary>2022-08-11 11:51:23 - Learning Computation Bounds for Branch-and-Bound Algorithms to k-plex Extraction</summary>

- *Yun-Ya Huang, Chih-Ya Shen*

- `2208.05763v1` - [abs](http://arxiv.org/abs/2208.05763v1) - [pdf](http://arxiv.org/pdf/2208.05763v1)

> k-plex is a representative definition of communities in networks. While the cliques is too stiff to applicate to real cases, the k-plex relaxes the notion of the clique, allowing each node to miss up to k connections. Although k-plexes are more flexible than cliques, finding them is more challenging as their number is greater. In this paper, we aim to detect the k-plex under the size and time constraints, leveraging the new vision of automated learning bounding strategy. We introduce the constraint learning concept to learn the bound strategy from the branch and bound process and develop it into a Mixed Integer Programming framework. While most of the work is dedicated on learn the branch strategy in branch and bound-based algorithms, we focus on the learn to bound strategy which needs to handle the problem that learned strategy might not examine the feasible solution. We adopted the MILP framework and design a set of variables relative to the k-plex property as our constraint space to learn the strategy. The learn to bound strategy learning the original strategy function also reduces the computation load of the bound process to accelerate the branch and bound algorithm. Note that the learn to bound concept can apply to any branch and bound based algorithm with the appropriate framework. We conduct the experiment on different networks, the results show that our learn to branch and bound method does accelerate the original branch and bound method and outperforms other baselines, while also being able to generalize on different graph properties.

</details>

<details>

<summary>2022-08-11 21:22:18 - CodeBERT-nt: code naturalness via CodeBERT</summary>

- *Ahmed Khanfir, Matthieu Jimenez, Mike Papadakis, Yves Le Traon*

- `2208.06042v1` - [abs](http://arxiv.org/abs/2208.06042v1) - [pdf](http://arxiv.org/pdf/2208.06042v1)

> Much of software-engineering research relies on the naturalness of code, the fact that code, in small code snippets, is repetitive and can be predicted using statistical language models like n-gram. Although powerful, training such models on large code corpus is tedious, time-consuming and sensitive to code patterns (and practices) encountered during training. Consequently, these models are often trained on a small corpora and estimate the language naturalness that is relative to a specific style of programming or type of project. To overcome these issues, we propose using pre-trained language models to infer code naturalness. Pre-trained models are often built on big data, are easy to use in an out-of-the-box way and include powerful learning associations mechanisms. Our key idea is to quantify code naturalness through its predictability, by using state-of-the-art generative pre-trained language models. To this end, we infer naturalness by masking (omitting) code tokens, one at a time, of code-sequences, and checking the models' ability to predict them. To this end, we evaluate three different predictability metrics; a) measuring the number of exact matches of the predictions, b) computing the embedding similarity between the original and predicted code, i.e., similarity at the vector space, and c) computing the confidence of the model when doing the token completion task irrespective of the outcome. We implement this workflow, named CodeBERT-nt, and evaluate its capability to prioritize buggy lines over non-buggy ones when ranking code based on its naturalness. Our results, on 2510 buggy versions of 40 projects from the SmartShark dataset, show that CodeBERT-nt outperforms both, random-uniform and complexity-based ranking techniques, and yields comparable results (slightly better) than the n-gram models.

</details>

<details>

<summary>2022-08-12 03:56:35 - Coarse to Fine Two-Stage Approach to Robust Tensor Completion of Visual Data</summary>

- *Yicong He, George K. Atia*

- `2106.10422v4` - [abs](http://arxiv.org/abs/2106.10422v4) - [pdf](http://arxiv.org/pdf/2106.10422v4)

> Tensor completion is the problem of estimating the missing values of high-order data from partially observed entries. Data corruption due to prevailing outliers poses major challenges to traditional tensor completion algorithms, which catalyzed the development of robust algorithms that alleviate the effect of outliers. However, existing robust methods largely presume that the corruption is sparse, which may not hold in practice. In this paper, we develop a two-stage robust tensor completion approach to deal with tensor completion of visual data with a large amount of gross corruption. A novel coarse-to-fine framework is proposed which uses a global coarse completion result to guide a local patch refinement process. To efficiently mitigate the effect of a large number of outliers on tensor recovery, we develop a new M-estimator-based robust tensor ring recovery method which can adaptively identify the outliers and alleviate their negative effect in the optimization. The experimental results demonstrate the superior performance of the proposed approach over state-of-the-art robust algorithms for tensor completion.

</details>

<details>

<summary>2022-08-12 04:02:45 - Patch Tracking-based Streaming Tensor Ring Completion for Visual Data Recovery</summary>

- *Yicong He, George K. Atia*

- `2105.14620v3` - [abs](http://arxiv.org/abs/2105.14620v3) - [pdf](http://arxiv.org/pdf/2105.14620v3)

> Tensor completion aims to recover the missing entries of a partially observed tensor by exploiting its low-rank structure, and has been applied to visual data recovery. In applications where the data arrives sequentially such as streaming video completion, the missing entries of the tensor need to be dynamically recovered in a streaming fashion. Traditional streaming tensor completion algorithms treat the entire visual data as a tensor, which may not work satisfactorily when there is a big change in the tensor subspace along the temporal dimension, such as due to strong motion across the video frames. In this paper, we develop a novel patch tracking-based streaming tensor ring completion framework for visual data recovery. Given a newly incoming frame, small patches are tracked from the previous frame. Meanwhile, for each tracked patch, a patch tensor is constructed by stacking similar patches from the new frame. Patch tensors are then completed using a streaming tensor ring completion algorithm, and the incoming frame is recovered using the completed patch tensors. We propose a new patch tracking strategy that can accurately and efficiently track the patches with missing data. Further, a new streaming tensor ring completion algorithm is proposed which can efficiently and accurately update the latent core tensors and complete the missing entries of the patch tensors. Extensive experimental results demonstrate the superior performance of the proposed algorithms compared with both batch and streaming state-of-the-art tensor completion methods.

</details>

<details>

<summary>2022-08-12 06:56:53 - How far are German companies in improving security through static program analysis tools?</summary>

- *Goran Piskachev, Stefan Dziwok, Thorsten Koch, Sven Merschjohan, Eric Bodden*

- `2208.06136v1` - [abs](http://arxiv.org/abs/2208.06136v1) - [pdf](http://arxiv.org/pdf/2208.06136v1)

> As security becomes more relevant for many companies, the popularity of static program analysis (SPA) tools is increasing. In this paper, we target the use of SPA tools among companies in Germany with a focus on security. We give insights on the current issues and the developers' willingness to configure the tools to overcome these issues. Compared to previous studies, our study considers the companies' culture and processes for using SPA tools. We conducted an online survey with 256 responses and semi-structured interviews with 17 product owners and executives from multiple companies. Our results show a diversity in the usage of tools. Only half of our survey participants use SPA tools. The free tools tend to be more popular among software developers. In most companies, software developers are encouraged to use free tools, whereas commercial tools can be requested. However, the product owners and executives in our interviews reported that their developers do not request new tools. We also find out that automatic security checks with tools are rarely performed on each release.

</details>

<details>

<summary>2022-08-12 15:17:42 - A Dataset Generation Framework for profiling Disassembly attacks using Side-Channel Leakages and Deep Neural Networks</summary>

- *Pouya Narimani, Seyed Amin Habibi, Mohammad Ali Akhaee*

- `2207.12068v2` - [abs](http://arxiv.org/abs/2207.12068v2) - [pdf](http://arxiv.org/pdf/2207.12068v2)

> Various studies among side-channel attacks have tried to extract information through leakages from electronic devices to reach the instruction flow of some appliances. However, previous methods highly depend on the resolution of traced data. Obtaining low-noise traces is not always feasible in real attack scenarios. This study proposes two deep models to extract low and high-level features from side-channel traces and classify them to related instructions. We aim to evaluate the accuracy of a side-channel attack on low-resolution data with a more robust feature extractor thanks to neural networks. As inves-tigated, instruction flow in real programs is predictable and follows specific distributions. This leads to proposing a LSTM model to estimate these distributions, which could expedite the reverse engineering process and also raise the accuracy. The proposed model for leakage classification reaches 54.58% accuracy on average and outperforms other existing methods on our datasets. Also, LSTM model reaches 94.39% accuracy for instruction prediction on standard implementation of cryptographic algorithms.

</details>

<details>

<summary>2022-08-12 15:38:15 - Multi-Model Probabilistic Programming</summary>

- *Ryan Bernstein*

- `2208.06329v1` - [abs](http://arxiv.org/abs/2208.06329v1) - [pdf](http://arxiv.org/pdf/2208.06329v1)

> Probabilistic programming makes it easy to represent a probabilistic model as a program. Building an individual model, however, is only one step of probabilistic modeling. The broader challenge of probabilistic modeling is in understanding and navigating spaces of alternative models. There is currently no good way to represent these spaces of alternative models, despite their central role. We present an extension of probabilistic programming that lets each program represent a network of interrelated probabilistic models. We give a formal semantics for these multi-model probabilistic programs, a collection of efficient algorithms for network-of-model operations, and an example implementation built on top of the popular probabilistic programming language Stan. This network-of-models representation opens many doors, including search and automation in model-space, tracking and communication of model development, and explicit modeler degrees of freedom to mitigate issues like p-hacking. We demonstrate automatic model search and model development tracking using our Stan implementation, and we propose many more possible applications.

</details>

<details>

<summary>2022-08-13 08:16:28 - Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer</summary>

- *Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang*

- `2208.06592v1` - [abs](http://arxiv.org/abs/2208.06592v1) - [pdf](http://arxiv.org/pdf/2208.06592v1)

> Backdoor attacks have been shown to be a serious security threat against deep learning models, and detecting whether a given model has been backdoored becomes a crucial task. Existing defenses are mainly built upon the observation that the backdoor trigger is usually of small size or affects the activation of only a few neurons. However, the above observations are violated in many cases especially for advanced backdoor attacks, hindering the performance and applicability of the existing defenses. In this paper, we propose a backdoor defense DTInspector built upon a new observation. That is, an effective backdoor attack usually requires high prediction confidence on the poisoned training samples, so as to ensure that the trained model exhibits the targeted behavior with a high probability. Based on this observation, DTInspector first learns a patch that could change the predictions of most high-confidence data, and then decides the existence of backdoor by checking the ratio of prediction changes after applying the learned patch on the low-confidence data. Extensive evaluations on five backdoor attacks, four datasets, and three advanced attacking types demonstrate the effectiveness of the proposed defense.

</details>

<details>

<summary>2022-08-13 13:54:30 - Intertwining Ecosystems: A Large Scale Empirical Study of Libraries that Cross Software Ecosystems</summary>

- *Kanchanok Kannee, Supatsara Wattanakriengkrai, Ruksit Rojpaisarnkit, Raula Gaikovina Kula, Kenichi Matsumoto*

- `2208.06655v1` - [abs](http://arxiv.org/abs/2208.06655v1) - [pdf](http://arxiv.org/pdf/2208.06655v1)

> An increase in diverse technology stacks and third-party library usage has led developers to inevitably switch technologies. To assist these developers, maintainers have started to release their libraries to multiple technologies, i.e., a cross-ecosystem library. Our goal is to explore the extent to which these cross-ecosystem libraries are intertwined between ecosystems. We perform a large-scale empirical study of 1.1 million libraries from five different software ecosystems, i.e., PyPI for Python, CRAN for R, Maven for Java, RubyGems for Ruby, and NPM for JavaScript to identify 4,146 GitHub projects that release libraries to these five ecosystems. Analyzing their contributions, we first find that a significant majority (median of 37.5%) of contributors of these cross-ecosystem libraries come from a single ecosystem, while also receiving a significant portion of contributions (median of 24.06%) from outside their target ecosystems. We also find that a cross-ecosystem library is written using multiple programming languages. Specifically, three (i.e., PyPI, CRAN, RubyGems) out of the five ecosystems has the majority of source code is written using languages not specific to that ecosystem. As ecosystems become intertwined, this opens up new avenues for research, such as whether or not cross-ecosystem libraries will solve the search for replacement libraries, or how these libraries fit within each ecosystem just to name a few.

</details>

<details>

<summary>2022-08-13 22:44:42 - Feasibility Layer Aided Machine Learning Approach for Day-Ahead Operations</summary>

- *Arun Venkatesh Ramesh, Xingpeng Li*

- `2208.06742v1` - [abs](http://arxiv.org/abs/2208.06742v1) - [pdf](http://arxiv.org/pdf/2208.06742v1)

> Day-ahead operations involves a complex and computationally intensive optimization process to determine the generator commitment schedule and dispatch. The optimization process is a mixed-integer linear program (MILP) also known as security-constrained unit commitment (SCUC). Independent system operators (ISOs) run SCUC daily and require state-of-the-art algorithms to speed up the process. Existing patterns in historical information can be leveraged for model reduction of SCUC, which can provide significant time savings. In this paper, machine learning (ML) based classification approaches, namely logistic regression, neural networks, random forest and K-nearest neighbor, were studied for model reduction of SCUC. The ML was then aided with a feasibility layer (FL) and post-process technique to ensure high-quality solutions. The proposed approach is validated on several test systems namely, IEEE 24-Bus system, IEEE-73 Bus system, IEEE 118-Bus system, 500-Bus system, and Polish 2383-Bus system. Moreover, model reduction of a stochastic SCUC (SSCUC) was demonstrated utilizing a modified IEEE 24-Bus system with renewable generation. Simulation results demonstrate a high training accuracy to identify commitment schedule while FL and post-process ensure ML predictions do not lead to infeasible solutions with minimal loss in solution quality.

</details>

<details>

<summary>2022-08-14 10:32:13 - Simply Logical -- Intelligent Reasoning by Example (Fully Interactive Online Edition)</summary>

- *Peter Flach, Kacper Sokol*

- `2208.06823v1` - [abs](http://arxiv.org/abs/2208.06823v1) - [pdf](http://arxiv.org/pdf/2208.06823v1)

> "Simply Logical -- Intelligent Reasoning by Example" by Peter Flach was first published by John Wiley in 1994. It could be purchased as book-only or with a 3.5 inch diskette containing the SWI-Prolog programmes printed in the book (for various operating systems). In 2007 the copyright reverted back to the author at which point the book and programmes were made freely available online; the print version is no longer distributed through John Wiley publishers. In 2015, as a pilot, we ported most of the original book into an online, interactive website using SWI-Prolog's SWISH platform. Since then, we launched the Simply Logical open source organisation committed to maintaining a suite of freely available interactive online educational resources about Artificial Intelligence and Logic Programming with Prolog. With the advent of new educational technologies we were inspired to rebuild the book from the ground up using the Jupyter Book platform enhanced with a collection of bespoke plugins that implement, among other things, interactive SWI-Prolog code blocks that can be executed directly in a web browser. This new version is more modular, easier to maintain, and can be split into custom teaching modules, in addition to being modern-looking, visually appealing, and compatible with a range of (mobile) devices of varying screen sizes.

</details>

<details>

<summary>2022-08-14 17:18:07 - Multi-Objective Provisioning of Network Slices using Deep Reinforcement Learning</summary>

- *Chien-Cheng Wu, Vasilis Friderikos, Cedomir Stefanovic*

- `2207.13821v4` - [abs](http://arxiv.org/abs/2207.13821v4) - [pdf](http://arxiv.org/pdf/2207.13821v4)

> Network Slicing (NS) is crucial for efficiently enabling divergent network applications in next generation networks. Nonetheless, the complex Quality of Service (QoS) requirements and diverse heterogeneity in network services entails high computational time for Network Slice Provisioning (NSP) optimization. The legacy optimization methods are challenging to meet the low latency and high reliability of network applications. To this end, we model the real-time NSP as an Online Network Slice Provisioning (ONSP) problem. Specifically, we formulate the ONSP problem as an online Multi-Objective Integer Programming Optimization (MOIPO) problem. Then, we approximate the solution of the MOIPO problem by applying the Proximal Policy Optimization (PPO) method to the traffic demand prediction. Our simulation results show the effectiveness of the proposed method compared to the state-of-the-art MOIPO solvers with a lower SLA violation rate and network operation cost.

</details>

<details>

<summary>2022-08-14 20:10:14 - Limits of an AI program for solving college math problems</summary>

- *Ernest Davis*

- `2208.06906v1` - [abs](http://arxiv.org/abs/2208.06906v1) - [pdf](http://arxiv.org/pdf/2208.06906v1)

> Drori et al. (2022) report that "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level ... [It] automatically answers 81\% of university-level mathematics problems." The system they describe is indeed impressive; however, the above description is very much overstated. The work of solving the problems is done, not by a neural network, but by the symbolic algebra package Sympy. Problems of various formats are excluded from consideration. The so-called "explanations" are just rewordings of lines of code. Answers are marked as correct that are not in the form specified in the problem. Most seriously, it seems that in many cases the system uses the correct answer given in the test corpus to guide its path to solving the problem.

</details>

<details>

<summary>2022-08-15 07:15:55 - SSLEM: A Simplifier for MBA Expressions based on Semi-linear MBA Expressions and Program Synthesis</summary>

- *Seong-Kyun Mok, Seoyeon Kang, Jeongwoo Kim, Eun-Sun Cho, Seokwoo Choi*

- `2208.05612v2` - [abs](http://arxiv.org/abs/2208.05612v2) - [pdf](http://arxiv.org/pdf/2208.05612v2)

> MBA (mixed boolean and arithmetic) expressions are hard to simplify, so used for malware obfuscation to hinder analysts' diagnosis. Some MBA simplification methods with high performance have been developed, but they narrowed the target to "linear" MBA expressions, which allows efficient solutions based on logic/term-rewriting. However such restrictions are not appropriate for general forms of MBA expressions usually appearing in malware. To overcome this limitation, we introduce a "semi-linear" MBA expression, a new class of MBA expression extended from a linear MBA expression, and propose a new MBA simplifier called "SSLEM", based on a simplification idea of semi-linear MBA expressions and program synthesis

</details>

<details>

<summary>2022-08-15 07:47:38 - Efficient Climate Simulation via Machine Learning Method</summary>

- *Xin Wang, Wei Xue, Yilun Han, Guangwen Yang*

- `2209.08151v1` - [abs](http://arxiv.org/abs/2209.08151v1) - [pdf](http://arxiv.org/pdf/2209.08151v1)

> Hybrid modeling combining data-driven techniques and numerical methods is an emerging and promising research direction for efficient climate simulation. However, previous works lack practical platforms, making developing hybrid modeling a challenging programming problem. Furthermore, the lack of standard data sets and evaluation metrics may hamper researchers from comprehensively comparing various algorithms under a uniform condition. To address these problems, we propose a framework called NeuroClim for hybrid modeling under the real-world scenario, a basic setting to simulate the real climate that we live in. NeuroClim consists of three parts: (1) Platform. We develop a user-friendly platform NeuroGCM for efficiently developing hybrid modeling in climate simulation. (2) Dataset. We provide an open-source dataset for data-driven methods in hybrid modeling. We investigate the characteristics of the data, i.e., heterogeneity and stiffness, which reveals the difficulty of regressing climate simulation data; (3) Metrics. We propose a methodology for quantitatively evaluating hybrid modeling, including the approximation ability of machine learning models and the stability during simulation. We believe that NeuroClim allows researchers to work without high level of climate-related expertise and focus only on machine learning algorithm design, which will accelerate hybrid modeling research in the AI-Climate intersection. The codes and data are released at https://github.com/x-w19/NeuroClim.

</details>

<details>

<summary>2022-08-15 07:49:58 - Self-Supervised Vision Transformers for Malware Detection</summary>

- *Sachith Seneviratne, Ridwan Shariffdeen, Sanka Rasnayaka, Nuran Kasthuriarachchi*

- `2208.07049v1` - [abs](http://arxiv.org/abs/2208.07049v1) - [pdf](http://arxiv.org/pdf/2208.07049v1)

> Malware detection plays a crucial role in cyber-security with the increase in malware growth and advancements in cyber-attacks. Previously unseen malware which is not determined by security vendors are often used in these attacks and it is becoming inevitable to find a solution that can self-learn from unlabeled sample data. This paper presents SHERLOCK, a self-supervision based deep learning model to detect malware based on the Vision Transformer (ViT) architecture. SHERLOCK is a novel malware detection method which learns unique features to differentiate malware from benign programs with the use of image-based binary representation. Experimental results using 1.2 million Android applications across a hierarchy of 47 types and 696 families, shows that self-supervised learning can achieve an accuracy of 97% for the binary classification of malware which is higher than existing state-of-the-art techniques. Our proposed model is also able to outperform state-of-the-art techniques for multi-class malware classification of types and family with macro-F1 score of .497 and .491 respectively.

</details>

<details>

<summary>2022-08-15 11:14:39 - Xscope: Hunting for Cross-Chain Bridge Attacks</summary>

- *Jiashuo Zhang, Jianbo Gao, Yue Li, Ziming Chen, Zhi Guan, Zhong Chen*

- `2208.07119v1` - [abs](http://arxiv.org/abs/2208.07119v1) - [pdf](http://arxiv.org/pdf/2208.07119v1)

> Cross-Chain bridges have become the most popular solution to support asset interoperability between heterogeneous blockchains. However, while providing efficient and flexible cross-chain asset transfer, the complex workflow involving both on-chain smart contracts and off-chain programs causes emerging security issues. In the past year, there have been more than ten severe attacks against cross-chain bridges, causing billions of loss. With few studies focusing on the security of cross-chain bridges, the community still lacks the knowledge and tools to mitigate this significant threat. To bridge the gap, we conduct the first study on the security of cross-chain bridges. We document three new classes of security bugs and propose a set of security properties and patterns to characterize them. Based on those patterns, we design Xscope, an automatic tool to find security violations in cross-chain bridges and detect real-world attacks. We evaluate Xscope on four popular cross-chain bridges. It successfully detects all known attacks and finds suspicious attacks unreported before. A video of Xscope is available at https://youtu.be/vMRO_qOqtXY.

</details>

<details>

<summary>2022-08-15 15:50:24 - Examining Zero-Shot Vulnerability Repair with Large Language Models</summary>

- *Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt*

- `2112.02125v3` - [abs](http://arxiv.org/abs/2112.02125v3) - [pdf](http://arxiv.org/pdf/2112.02125v3)

> Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, "off-the-shelf" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.

</details>

<details>

<summary>2022-08-15 16:05:05 - Alleviation of Temperature Variation Induced Accuracy Degradation in Ferroelectric FinFET Based Neural Network</summary>

- *Sourav De, Hoang-Hiep Le, Md. Aftab Baig, Yao-Jen Lee, Darsen D. Lu, Thomas KÃ¤mpfe*

- `2103.03111v5` - [abs](http://arxiv.org/abs/2103.03111v5) - [pdf](http://arxiv.org/pdf/2103.03111v5)

> This paper reports the impacts of temperature variation on the inference accuracy of pre-trained all-ferroelectric FinFET deep neural networks, along with plausible design techniques to abate these impacts. We adopted a pre-trained artificial neural network (N.N.) with 96.4% inference accuracy on the MNIST dataset as the baseline. As an aftermath of temperature change, a compact model captured the conductance drift of a programmed cell over a wide range of gate biases. We observed a significant inference accuracy degradation in the analog neural network at 233 K for an N.N. trained at 300 K. Finally, we deployed binary neural networks with "read voltage" optimization to ensure immunity of N.N. to accuracy degradation under temperature variation, maintaining an inference accuracy of 96%. Keywords: Ferroelectric memories

</details>

<details>

<summary>2022-08-15 16:16:42 - A Feasibility-Driven Approach to Control-Limited DDP</summary>

- *Carlos Mastalli, Wolfgang Merkt, Josep Marti-Saumell, Henrique Ferrolho, Joan Sola, Nicolas Mansard, Sethu Vijayakumar*

- `2010.00411v4` - [abs](http://arxiv.org/abs/2010.00411v4) - [pdf](http://arxiv.org/pdf/2010.00411v4)

> Differential dynamic programming (DDP) is a direct single shooting method for trajectory optimization. Its efficiency derives from the exploitation of temporal structure (inherent to optimal control problems) and explicit roll-out/integration of the system dynamics. However, it suffers from numerical instability and, when compared to direct multiple shooting methods, it has limited initialization options (allows initialization of controls, but not of states) and lacks proper handling of control constraints. In this work, we tackle these issues with a feasibility-driven approach that regulates the dynamic feasibility during the numerical optimization and ensures control limits. Our feasibility search emulates the numerical resolution of a direct multiple shooting problem with only dynamics constraints. We show that our approach (named BOX-FDDP) has better numerical convergence than BOX-DDP+ (a single shooting method), and that its convergence rate and runtime performance are competitive with state-of-the-art direct transcription formulations solved using the interior point and active set algorithms available in KNITRO. We further show that BOX-FDDP decreases the dynamic feasibility error monotonically--as in state-of-the-art nonlinear programming algorithms. We demonstrate the benefits of our approach by generating complex and athletic motions for quadruped and humanoid robots. Finally, we highlight that BOX-FDDP is suitable for model predictive control in legged robots.

</details>

<details>

<summary>2022-08-15 16:39:34 - Cross-scale Attention Guided Multi-instance Learning for Crohn's Disease Diagnosis with Pathological Images</summary>

- *Ruining Deng, Can Cui, Lucas W. Remedios, Shunxing Bao, R. Michael Womick, Sophie Chiron, Jia Li, Joseph T. Roland, Ken S. Lau, Qi Liu, Keith T. Wilson, Yaohong Wang, Lori A. Coburn, Bennett A. Landman, Yuankai Huo*

- `2208.07322v1` - [abs](http://arxiv.org/abs/2208.07322v1) - [pdf](http://arxiv.org/pdf/2208.07322v1)

> Multi-instance learning (MIL) is widely used in the computer-aided interpretation of pathological Whole Slide Images (WSIs) to solve the lack of pixel-wise or patch-wise annotations. Often, this approach directly applies "natural image driven" MIL algorithms which overlook the multi-scale (i.e. pyramidal) nature of WSIs. Off-the-shelf MIL algorithms are typically deployed on a single-scale of WSIs (e.g., 20x magnification), while human pathologists usually aggregate the global and local patterns in a multi-scale manner (e.g., by zooming in and out between different magnifications). In this study, we propose a novel cross-scale attention mechanism to explicitly aggregate inter-scale interactions into a single MIL network for Crohn's Disease (CD), which is a form of inflammatory bowel disease. The contribution of this paper is two-fold: (1) a cross-scale attention mechanism is proposed to aggregate features from different resolutions with multi-scale interaction; and (2) differential multi-scale attention visualizations are generated to localize explainable lesion patterns. By training ~250,000 H&E-stained Ascending Colon (AC) patches from 20 CD patient and 30 healthy control samples at different scales, our approach achieved a superior Area under the Curve (AUC) score of 0.8924 compared with baseline models. The official implementation is publicly available at https://github.com/hrlblab/CS-MIL.

</details>

<details>

<summary>2022-08-15 22:36:17 - A Library for Representing Python Programs as Graphs for Machine Learning</summary>

- *David Bieber, Kensen Shi, Petros Maniatis, Charles Sutton, Vincent Hellendoorn, Daniel Johnson, Daniel Tarlow*

- `2208.07461v1` - [abs](http://arxiv.org/abs/2208.07461v1) - [pdf](http://arxiv.org/pdf/2208.07461v1)

> Graph representations of programs are commonly a central element of machine learning for code research. We introduce an open source Python library python_graphs that applies static analysis to construct graph representations of Python programs suitable for training machine learning models. Our library admits the construction of control-flow graphs, data-flow graphs, and composite ``program graphs'' that combine control-flow, data-flow, syntactic, and lexical information about a program. We present the capabilities and limitations of the library, perform a case study applying the library to millions of competitive programming submissions, and showcase the library's utility for machine learning research.

</details>

<details>

<summary>2022-08-15 23:21:16 - Semidefinite Programming versus Burer-Monteiro Factorization for Matrix Sensing</summary>

- *Baturalp Yalcin, Ziye Ma, Javad Lavaei, Somayeh Sojoudi*

- `2208.07469v1` - [abs](http://arxiv.org/abs/2208.07469v1) - [pdf](http://arxiv.org/pdf/2208.07469v1)

> Many fundamental low-rank optimization problems, such as matrix completion, phase synchronization/retrieval, power system state estimation, and robust PCA, can be formulated as the matrix sensing problem. Two main approaches for solving matrix sensing are based on semidefinite programming (SDP) and Burer-Monteiro (B-M) factorization. The SDP method suffers from high computational and space complexities, whereas the B-M method may return a spurious solution due to the non-convexity of the problem. The existing theoretical guarantees for the success of these methods have led to similar conservative conditions, which may wrongly imply that these methods have comparable performances. In this paper, we shed light on some major differences between these two methods. First, we present a class of structured matrix completion problems for which the B-M methods fail with an overwhelming probability, while the SDP method works correctly. Second, we identify a class of highly sparse matrix completion problems for which the B-M method works and the SDP method fails. Third, we prove that although the B-M method exhibits the same performance independent of the rank of the unknown solution, the success of the SDP method is correlated to the rank of the solution and improves as the rank increases. Unlike the existing literature that has mainly focused on those instances of matrix sensing for which both SDP and B-M work, this paper offers the first result on the unique merit of each method over the alternative approach.

</details>

<details>

<summary>2022-08-15 23:27:54 - On the Adoption and Effects of Source Code Reuse on Defect Proneness and Maintenance Effort</summary>

- *Giammaria Giordano, Gerardo Festa, Gemma Catolino, Fabio Palomba, Filomena Ferrucci, Carmine Gravino*

- `2208.07471v1` - [abs](http://arxiv.org/abs/2208.07471v1) - [pdf](http://arxiv.org/pdf/2208.07471v1)

> Context. Software reusability mechanisms, like inheritance and delegation in Object-Oriented programming, are widely recognized as key instruments of software design. These are used to reduce the risks of source code being affected by defects, other than to reduce the effort required to maintain and evolve source code. Previous work has traditionally employed source code reuse metrics for prediction purposes, e.g., in the context of defect prediction. Objective. However, our research identifies two noticeable limitations of current literature. First, still little is known on the extent to which developers actually employ code reuse mechanisms over time. Second, it is still unclear how these mechanisms may contribute to explain defect-proneness and maintenance effort during software evolution. We aim at bridging this gap of knowledge, as an improved understanding of these aspects might provide insights into the actual support provided by these mechanisms, e.g., by suggesting whether and how to use them for prediction purposes. Method. We propose an exploratory study aiming at (1) assessing how developers use inheritance and delegation during software evolution; and (2) statistically analyze the impact of inheritance and delegation on fault proneness and maintenance effort. The study will be conducted on the commits of 17 Java projects of the DEFECTS4J dataset.

</details>

<details>

<summary>2022-08-16 06:07:57 - A meta-probabilistic-programming language for bisimulation of probabilistic and non-well-founded type systems</summary>

- *Jonathan Warrell, Alexey Potapov, Adam Vandervorst, Ben Goertzel*

- `2203.15970v3` - [abs](http://arxiv.org/abs/2203.15970v3) - [pdf](http://arxiv.org/pdf/2203.15970v3)

> We introduce a formal meta-language for probabilistic programming, capable of expressing both programs and the type systems in which they are embedded. We are motivated here by the desire to allow an AGI to learn not only relevant knowledge (programs/proofs), but also appropriate ways of reasoning (logics/type systems). We draw on the frameworks of cubical type theory and dependent typed metagraphs to formalize our approach. In doing so, we show that specific constructions within the meta-language can be related via bisimulation (implying path equivalence) to the type systems they correspond. This allows our approach to provide a convenient means of deriving synthetic denotational semantics for various type systems. Particularly, we derive bisimulations for pure type systems (PTS), and probabilistic dependent type systems (PDTS). We discuss further the relationship of PTS to non-well-founded set theory, and demonstrate the feasibility of our approach with an implementation of a bisimulation proof in a Guarded Cubical Type Theory type checker.

</details>

<details>

<summary>2022-08-16 09:56:08 - Educating Reflective Systems Developers at Scale: Towards productive feedback in a semi-capstone large-scale software engineering course</summary>

- *Torgeir DingsÃ¸yr*

- `2208.07640v1` - [abs](http://arxiv.org/abs/2208.07640v1) - [pdf](http://arxiv.org/pdf/2208.07640v1)

> Feedback is critical in education. This Innovative Practice Full Paper reports lessons learned from improving the quality of feedback in a semi-capstone software engineering course, with particular focus on how to deliver productive feedback in large scale during project work. The bachelor-level introduction to software engineering course is taken by about 500 students from eight study programs, organised into 72 project teams. The course aims to educate reflective systems developers. The teaching staff includes 29 teaching assistants as supervisor and product owners for teams. Project teams get feedback on seven deliverables as part of formative portfolio assessment. Students expressed frustration on feedback not being aligned, that they got critique on topics not stated in assignments and that teaching assistants were reluctant to discuss the feedback. This article provides a description of the course design, an assessment of the quality of feedback and lessons learned from three main changes: Revising assignments and rubrics, reorganising the teaching staff and increasing training of teaching assistants. In discussing the changes, we draw on a survey to students with 142 respondents, a survey to teaching assistants with 18 respondents, meeting minutes from a student reference group and experience reports from teaching assistants as well as literature and own experience. The article concludes with three actionable lessons learned for large-scale semi-capstone courses.

</details>

<details>

<summary>2022-08-16 22:08:20 - Arachne: Search Based Repair of Deep Neural Networks</summary>

- *Jeongju Sohn, Sungmin Kang, Shin Yoo*

- `1912.12463v2` - [abs](http://arxiv.org/abs/1912.12463v2) - [pdf](http://arxiv.org/pdf/1912.12463v2)

> The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This paper introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses Differential Evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond Convolutional Neural Networks.

</details>

<details>

<summary>2022-08-17 01:17:27 - Resource Allocation in Quantum Key Distribution (QKD) for Space-Air-Ground Integrated Networks</summary>

- *Rakpong Kaewpuang, Minrui Xu, Dusit Niyato, Han Yu, Zehui Xiong*

- `2208.08009v1` - [abs](http://arxiv.org/abs/2208.08009v1) - [pdf](http://arxiv.org/pdf/2208.08009v1)

> Space-air-ground integrated networks (SAGIN) are one of the most promising advanced paradigms in the sixth generation (6G) communication. SAGIN can support high data rates, low latency, and seamless network coverage for interconnected applications and services. However, communications in SAGIN are facing tremendous security threats from the ever-increasing capacity of quantum computers. Fortunately, quantum key distribution (QKD) for establishing secure communications in SAGIN, i.e., QKD over SAGIN, can provide information-theoretic security. To minimize the QKD deployment cost in SAGIN with heterogeneous nodes, in this paper, we propose a resource allocation scheme for QKD over SAGIN using stochastic programming. The proposed scheme is formulated via two-stage stochastic programming (SP), while considering uncertainties such as security requirements and weather conditions. Under extensive experiments, the results clearly show that the proposed scheme can achieve the optimal deployment cost under various security requirements and unpredictable weather conditions.

</details>

<details>

<summary>2022-08-17 04:50:51 - ASTRO: An AST-Assisted Approach for Generalizable Neural Clone Detection</summary>

- *Yifan Zhang, Junwen Yang, Haoyu Dong, Qingchen Wang, Huajie Shao, Kevin Leach, Yu Huang*

- `2208.08067v1` - [abs](http://arxiv.org/abs/2208.08067v1) - [pdf](http://arxiv.org/pdf/2208.08067v1)

> Neural clone detection has attracted the attention of software engineering researchers and practitioners. However, most neural clone detection methods do not generalize beyond the scope of clones that appear in the training dataset. This results in poor model performance, especially in terms of model recall. In this paper, we present an Abstract Syntax Tree (AST) assisted approach for generalizable neural clone detection, or ASTRO, a framework for finding clones in codebases reflecting industry practices. We present three main components: (1) an AST-inspired representation for source code that leverages program structure and semantics, (2) a global graph representation that captures the context of an AST among a corpus of programs, and (3) a graph embedding for programs that, in combination with extant large-scale language models, improves state-of-the-art code clone detection. Our experimental results show that ASTRO improves state-of-the-art neural clone detection approaches in both recall and F-1 scores.

</details>

<details>

<summary>2022-08-17 04:57:08 - Proof Engineering with Predicate Transformer Semantics</summary>

- *Christa Jenkins, Mark Moir, Harold Carr*

- `2208.08070v1` - [abs](http://arxiv.org/abs/2208.08070v1) - [pdf](http://arxiv.org/pdf/2208.08070v1)

> We present a lightweight, open source Agda framework for manually verifying effectful programs using predicate transformer semantics. We represent the abstract syntax trees (AST) of effectful programs with a generalized algebraic datatype (GADT) AST, whose generality enables even complex operations to be primitive AST nodes. Users can then assign bespoke predicate transformers to such operations to aid the proof effort, for example by automatically decomposing proof obligations for branching code. Our framework codifies and generalizes a proof engineering methodology used by the authors to reason about a prototype implementation of LibraBFT, a Byzantine fault tolerant consensus protocol in which code executed by participants may have effects such as updating state and sending messages. Successful use of our framework in this context demonstrates its practical applicability.

</details>

<details>

<summary>2022-08-17 05:37:42 - Evaluating virtual laboratory platforms for supporting on-line information security courses</summary>

- *Monther Aldwairi*

- `2208.12612v1` - [abs](http://arxiv.org/abs/2208.12612v1) - [pdf](http://arxiv.org/pdf/2208.12612v1)

> Distance education had existed for a long time, then it has undergone a renaissance with the advent of computers and the Internet. Distance education relied on physically delivered material and assessments to students, who work offline at home. More recently, online learning or e-learning introduced virtual classrooms, assessments, online tests and transformed the classroom an into interactive online classroom. Despite the large number of online degrees offered, face-to-face remained the dominant mode and e-learning was just used to complement the classroom. The Covid-19 pandemic continues to impact higher education, and online learning is a forgone conclusion. However, the digital divide hindered the disadvantaged schools and students efforts to transition to online learning. As the pandemic continues to change the education landscape, many challenges arise and prevent student from realising the full potential of e-learning. One of those is the access to physical labs in science, engineering, and computer science programs. This study evaluates practical solutions for virtual labs to be used in teaching information security and ethical hacking. The course ran over five semesters, and 164 students were surveyed. The survey measured perceptions, enjoyment, experiences and attitudes towards virtual labs, and the results were supporting adoption and acceptance of virtual labs.

</details>

<details>

<summary>2022-08-17 08:56:29 - On Establishing Robust Consistency in Answer Set Programs</summary>

- *Andre Thevapalan, Gabriele Kern-Isberner*

- `2208.08157v1` - [abs](http://arxiv.org/abs/2208.08157v1) - [pdf](http://arxiv.org/pdf/2208.08157v1)

> Answer set programs used in real-world applications often require that the program is usable with different input data. This, however, can often lead to contradictory statements and consequently to an inconsistent program. Causes for potential contradictions in a program are conflicting rules. In this paper, we show how to ensure that a program $\mathcal{P}$ remains non-contradictory given any allowed set of such input data. For that, we introduce the notion of conflict-resolving $\lambda$- extensions. A conflict-resolving $\lambda$-extension for a conflicting rule $r$ is a set $\lambda$ of (default) literals such that extending the body of $r$ by $\lambda$ resolves all conflicts of $r$ at once. We investigate the properties that suitable $\lambda$-extensions should possess and building on that, we develop a strategy to compute all such conflict-resolving $\lambda$-extensions for each conflicting rule in $\mathcal{P}$. We show that by implementing a conflict resolution process that successively resolves conflicts using $\lambda$-extensions eventually yields a program that remains non-contradictory given any allowed set of input data.

</details>

<details>

<summary>2022-08-17 09:21:50 - An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities</summary>

- *Imen Sayar, Alexandre Bartel, Eric Bodden, Yves Le Traon*

- `2208.08173v1` - [abs](http://arxiv.org/abs/2208.08173v1) - [pdf](http://arxiv.org/pdf/2208.08173v1)

> Nowadays, an increasing number of applications uses deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class -- such as making it public -- can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks. For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.

</details>

<details>

<summary>2022-08-17 11:29:16 - Input Repair via Synthesis and Lightweight Error Feedback</summary>

- *Lukas Kirschner, Ezekiel Soremekun, Rahul Gopinath, Andreas Zeller*

- `2208.08235v1` - [abs](http://arxiv.org/abs/2208.08235v1) - [pdf](http://arxiv.org/pdf/2208.08235v1)

> Often times, input data may ostensibly conform to a given input format, but cannot be parsed by a conforming program, for instance, due to human error or data corruption. In such cases, a data engineer is tasked with input repair, i.e., she has to manually repair the corrupt data such that it follows a given format, and hence can be processed by the conforming program. Such manual repair can be time-consuming and error-prone. In particular, input repair is challenging without an input specification (e.g., input grammar) or program analysis.   In this work, we show that incorporating lightweight failure feedback (e.g., input incompleteness) to parsers is sufficient to repair any corrupt input data with maximal closeness to the semantics of the input data. We propose an approach (called FSYNTH) that leverages lightweight error-feedback and input synthesis to repair invalid inputs. FSYNTH is grammar-agnostic, and it does not require program analysis. Given a conforming program, and any invalid input, FSYNTH provides a set of repairs prioritized by the distance of the repair from the original input. We evaluate FSYNTH on 806 (real-world) invalid inputs using four well-known input formats, namely INI, TinyC, SExp, and cJSON. In our evaluation, we found that FSYNTH recovers 91% of valid input data. FSYNTH is also highly effective and efficient in input repair: It repairs 77% of invalid inputs within four minutes. It is up to 35% more effective than DDMax, the previously best-known approach. Overall, our approach addresses several limitations of DDMax, both in terms of what it can repair, as well as in terms of the set of repairs offered.

</details>

<details>

<summary>2022-08-17 23:03:54 - Physical Computing for Materials Acceleration Platforms</summary>

- *Erik Peterson, Alexander Lavin*

- `2208.08566v1` - [abs](http://arxiv.org/abs/2208.08566v1) - [pdf](http://arxiv.org/pdf/2208.08566v1)

> A ''technology lottery'' describes a research idea or technology succeeding over others because it is suited to the available software and hardware, not necessarily because it is superior to alternative directions--examples abound, from the synergies of deep learning and GPUs to the disconnect of urban design and autonomous vehicles. The nascent field of Self-Driving Laboratories (SDL), particularly those implemented as Materials Acceleration Platforms (MAPs), is at risk of an analogous pitfall: the next logical step for building MAPs is to take existing lab equipment and workflows and mix in some AI and automation. In this whitepaper, we argue that the same simulation and AI tools that will accelerate the search for new materials, as part of the MAPs research program, also make possible the design of fundamentally new computing mediums. We need not be constrained by existing biases in science, mechatronics, and general-purpose computing, but rather we can pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. Here we outline a simulation-based MAP program to design computers that use physics itself to solve optimization problems. Such systems mitigate the hardware-software-substrate-user information losses present in every other class of MAPs and they perfect alignment between computing problems and computing mediums eliminating any technology lottery. We offer concrete steps toward early ''Physical Computing (PC) -MAP'' advances and the longer term cyber-physical R&D which we expect to introduce a new era of innovative collaboration between materials researchers and computer scientists.

</details>

<details>

<summary>2022-08-18 05:59:56 - Parallel Power System Restoration</summary>

- *Sunil Chopra, Feng Qiu, Sangho Shim*

- `2204.01837v2` - [abs](http://arxiv.org/abs/2204.01837v2) - [pdf](http://arxiv.org/pdf/2204.01837v2)

> Power system restoration is an essential activity for grid resilience, where grid operators restart generators, re-establish transmission paths, and restore loads after a blackout event. With a goal of restoring electric service in the shortest time, the core decisions in restoration planning are to partition the grid into sub-networks, each of which has an initial power source for black-start (called sectionalization problem), and then restart all generators in each network (called generator startup sequencing problem or GSS) as soon as possible. Due to the complexity of each problem, the sectionalization and GSS problems are usually solved separately, often resulting in a sub-optimal solution. Our paper develops models and computational methods to solve the two problems simultaneously. We first study the computational complexity of the GSS problem and develop an efficient integer linear programming formulation. We then integrate the GSS problem with the sectionalization problem and develop an integer linear programming formulation for the parallel power system restoration (PPSR) problem to find exact optimal solutions. To solve larger systems, we then develop bounding approaches that find good upper and lower bounds efficiently. Finally, to address computational challenges for very large power grids, we develop a randomized approach to find a high-quality feasible solution quickly. Our computational experiments demonstrate that the proposed approaches are able to find good solutions for PPSR in up to 2000-bus systems.

</details>

<details>

<summary>2022-08-18 06:57:54 - SDA-SNE: Spatial Discontinuity-Aware Surface Normal Estimation via Multi-Directional Dynamic Programming</summary>

- *Nan Ming, Yi Feng, Rui Fan*

- `2208.08667v1` - [abs](http://arxiv.org/abs/2208.08667v1) - [pdf](http://arxiv.org/pdf/2208.08667v1)

> The state-of-the-art (SoTA) surface normal estimators (SNEs) generally translate depth images into surface normal maps in an end-to-end fashion. Although such SNEs have greatly minimized the trade-off between efficiency and accuracy, their performance on spatial discontinuities, e.g., edges and ridges, is still unsatisfactory. To address this issue, this paper first introduces a novel multi-directional dynamic programming strategy to adaptively determine inliers (co-planar 3D points) by minimizing a (path) smoothness energy. The depth gradients can then be refined iteratively using a novel recursive polynomial interpolation algorithm, which helps yield more reasonable surface normals. Our introduced spatial discontinuity-aware (SDA) depth gradient refinement strategy is compatible with any depth-to-normal SNEs. Our proposed SDA-SNE achieves much greater performance than all other SoTA approaches, especially near/on spatial discontinuities. We further evaluate the performance of SDA-SNE with respect to different iterations, and the results suggest that it converges fast after only a few iterations. This ensures its high efficiency in various robotics and computer vision applications requiring real-time performance. Additional experiments on the datasets with different extents of random noise further validate our SDA-SNE's robustness and environmental adaptability. Our source code, demo video, and supplementary material are publicly available at mias.group/SDA-SNE.

</details>

<details>

<summary>2022-08-18 16:28:36 - NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems</summary>

- *Filippo Vicentini, Damian Hofmann, Attila SzabÃ³, Dian Wu, Christopher Roth, Clemens Giuliani, Gabriel Pescia, Jannes Nys, Vladimir Vargas-Calderon, Nikita Astrakhantsev, Giuseppe Carleo*

- `2112.10526v2` - [abs](http://arxiv.org/abs/2112.10526v2) - [pdf](http://arxiv.org/pdf/2112.10526v2)

> We introduce version 3 of NetKet, the machine learning toolbox for many-body quantum physics. NetKet is built around neural-network quantum states and provides efficient algorithms for their evaluation and optimization. This new version is built on top of JAX, a differentiable programming and accelerated linear algebra framework for the Python programming language. The most significant new feature is the possibility to define arbitrary neural network ans\"atze in pure Python code using the concise notation of machine-learning frameworks, which allows for just-in-time compilation as well as the implicit generation of gradients thanks to automatic differentiation. NetKet 3 also comes with support for GPU and TPU accelerators, advanced support for discrete symmetry groups, chunking to scale up to thousands of degrees of freedom, drivers for quantum dynamics applications, and improved modularity, allowing users to use only parts of the toolbox as a foundation for their own code.

</details>

<details>

<summary>2022-08-19 00:16:09 - Discovering Faint and High Apparent Motion Rate Near-Earth Asteroids Using A Deep Learning Program</summary>

- *Franklin Wang, Jian Ge, Kevin Willis*

- `2208.09098v1` - [abs](http://arxiv.org/abs/2208.09098v1) - [pdf](http://arxiv.org/pdf/2208.09098v1)

> Although many near-Earth objects have been found by ground-based telescopes, some fast-moving ones, especially those near detection limits, have been missed by observatories. We developed a convolutional neural network for detecting faint fast-moving near-Earth objects. It was trained with artificial streaks generated from simulations and was able to find these asteroid streaks with an accuracy of 98.7% and a false positive rate of 0.02% on simulated data. This program was used to search image data from the Zwicky Transient Facility (ZTF) in four nights in 2019, and it identified six previously undiscovered asteroids. The visual magnitudes of our detections range from ~19.0 - 20.3 and motion rates range from ~6.8 - 24 deg/day, which is very faint compared to other ZTF detections moving at similar motion rates. Our asteroids are also ~1 - 51 m diameter in size and ~5 - 60 lunar distances away at close approach, assuming their albedo values follow the albedo distribution function of known asteroids. The use of a purely simulated dataset to train our model enables the program to gain sensitivity in detecting faint and fast-moving objects while still being able to recover nearly all discoveries made by previously designed neural networks which used real detections to train neural networks. Our approach can be adopted by any observatory for detecting fast-moving asteroid streaks.

</details>

<details>

<summary>2022-08-19 02:10:03 - Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment</summary>

- *Cheng Yang, Gene Cheung, Guangtao Zhai*

- `2106.01642v3` - [abs](http://arxiv.org/abs/2106.01642v3) - [pdf](http://arxiv.org/pdf/2106.01642v3)

> In semi-supervised graph-based binary classifier learning, a subset of known labels $\hat{x}_i$ are used to infer unknown labels, assuming that the label signal $\mathbf{x}$ is smooth with respect to a similarity graph specified by a Laplacian matrix. When restricting labels $x_i$ to binary values, the problem is NP-hard. While a conventional semi-definite programming relaxation (SDR) can be solved in polynomial time using, for example, the alternating direction method of multipliers (ADMM), the complexity of projecting a candidate matrix $\mathbf{M}$ onto the positive semi-definite (PSD) cone ($\mathbf{M} \succeq 0$) per iteration remains high. In this paper, leveraging a recent linear algebraic theory called Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free method by solving a sequence of linear programs (LP) instead. Specifically, we first recast the SDR to its dual, where a feasible solution $\mathbf{H} \succeq 0$ is interpreted as a Laplacian matrix corresponding to a balanced signed graph minus the last node. To achieve graph balance, we split the last node into two, each retains the original positive / negative edges, resulting in a new Laplacian $\bar{\mathbf{H}}$. We repose the SDR dual for solution $\bar{\mathbf{H}}$, then replace the PSD cone constraint $\bar{\mathbf{H}} \succeq 0$ with linear constraints derived from GDPA -- sufficient conditions to ensure $\bar{\mathbf{H}}$ is PSD -- so that the optimization becomes an LP per iteration. Finally, we extract predicted labels from converged solution $\bar{\mathbf{H}}$. Experiments show that our algorithm enjoyed a $28\times$ speedup over the next fastest scheme while achieving comparable label prediction performance.

</details>

<details>

<summary>2022-08-19 07:02:50 - Improved Image Classification with Token Fusion</summary>

- *Keong Hun Choi, Jin Woo Kim, Yao Wang, Jong Eun Ha*

- `2208.09183v1` - [abs](http://arxiv.org/abs/2208.09183v1) - [pdf](http://arxiv.org/pdf/2208.09183v1)

> In this paper, we propose a method using the fusion of CNN and transformer structure to improve image classification performance. In the case of CNN, information about a local area on an image can be extracted well, but there is a limit to the extraction of global information. On the other hand, the transformer has an advantage in relatively global extraction, but has a disadvantage in that it requires a lot of memory for local feature value extraction. In the case of an image, it is converted into a feature map through CNN, and each feature map's pixel is considered a token. At the same time, the image is divided into patch areas and then fused with the transformer method that views them as tokens. For the fusion of tokens with two different characteristics, we propose three methods: (1) late token fusion with parallel structure, (2) early token fusion, (3) token fusion in a layer by layer. In an experiment using ImageNet 1k, the proposed method shows the best classification performance.

</details>

<details>

<summary>2022-08-19 07:39:31 - Real-Time Robust Video Object Detection System Against Physical-World Adversarial Attacks</summary>

- *Husheng Han, Xing Hu, Kaidi Xu, Pucheng Dang, Ying Wang, Yongwei Zhao, Zidong Du, Qi Guo, Yanzhi Yang, Tianshi Chen*

- `2208.09195v1` - [abs](http://arxiv.org/abs/2208.09195v1) - [pdf](http://arxiv.org/pdf/2208.09195v1)

> DNN-based video object detection (VOD) powers autonomous driving and video surveillance industries with rising importance and promising opportunities. However, adversarial patch attack yields huge concern in live vision tasks because of its practicality, feasibility, and powerful attack effectiveness. This work proposes Themis, a software/hardware system to defend against adversarial patches for real-time robust video object detection. We observe that adversarial patches exhibit extremely localized superficial feature importance in a small region with non-robust predictions, and thus propose the adversarial region detection algorithm for adversarial effect elimination. Themis also proposes a systematic design to efficiently support the algorithm by eliminating redundant computations and memory traffics. Experimental results show that the proposed methodology can effectively recover the system from the adversarial attack with negligible hardware overhead.

</details>

<details>

<summary>2022-08-19 07:41:28 - Towards Informed Design and Validation Assistance in Computer Games Using Imitation Learning</summary>

- *Alessandro Sestini, Joakim Bergdahl, Konrad Tollmar, Andrew D. Bagdanov, Linus GisslÃ©n*

- `2208.07811v2` - [abs](http://arxiv.org/abs/2208.07811v2) - [pdf](http://arxiv.org/pdf/2208.07811v2)

> In games, as in and many other domains, design validation and testing is a huge challenge as systems are growing in size and manual testing is becoming infeasible. This paper proposes a new approach to automated game validation and testing. Our method leverages a data-driven imitation learning technique, which requires little effort and time and no knowledge of machine learning or programming, that designers can use to efficiently train game testing agents. We investigate the validity of our approach through a user study with industry experts. The survey results show that our method is indeed a valid approach to game validation and that data-driven programming would be a useful aid to reducing effort and increasing quality of modern playtesting. The survey also highlights several open challenges. With the help of the most recent literature, we analyze the identified challenges and propose future research directions suitable for supporting and maximizing the utility of our approach.

</details>

<details>

<summary>2022-08-19 08:34:09 - Crowdsourced Fact-Checking at Twitter: How Does the Crowd Compare With Experts?</summary>

- *Mohammed Saeed, Nicolas Traub, Maelle Nicolas, Gianluca Demartini, Paolo Papotti*

- `2208.09214v1` - [abs](http://arxiv.org/abs/2208.09214v1) - [pdf](http://arxiv.org/pdf/2208.09214v1)

> Fact-checking is one of the effective solutions in fighting online misinformation. However, traditional fact-checking is a process requiring scarce expert human resources, and thus does not scale well on social media because of the continuous flow of new content to be checked. Methods based on crowdsourcing have been proposed to tackle this challenge, as they can scale with a smaller cost, but, while they have shown to be feasible, have always been studied in controlled environments. In this work, we study the first large-scale effort of crowdsourced fact-checking deployed in practice, started by Twitter with the Birdwatch program. Our analysis shows that crowdsourcing may be an effective fact-checking strategy in some settings, even comparable to results obtained by human experts, but does not lead to consistent, actionable results in others. We processed 11.9k tweets verified by the Birdwatch program and report empirical evidence of i) differences in how the crowd and experts select content to be fact-checked, ii) how the crowd and the experts retrieve different resources to fact-check, and iii) the edge the crowd shows in fact-checking scalability and efficiency as compared to expert checkers.

</details>

<details>

<summary>2022-08-19 10:06:20 - Styler: learning formatting conventions to repair Checkstyle violations</summary>

- *Benjamin Loriot, Fernanda Madeiral, Martin Monperrus*

- `1904.01754v5` - [abs](http://arxiv.org/abs/1904.01754v5) - [pdf](http://arxiv.org/pdf/1904.01754v5)

> Ensuring the consistent usage of formatting conventions is an important aspect of modern software quality assurance. While formatting convention violations can be automatically detected by format checkers implemented in linters, there is no satisfactory solution for repairing them. Manually fixing formatting convention violations is a waste of developer time and code formatters do not take into account the conventions adopted and configured by developers for the used linter. In this paper, we present Styler, a tool dedicated to fixing formatting rule violations raised by format checkers using a machine learning approach. For a given project, Styler first generates training data by injecting violations of the project-specific rules in violation-free source code files. Then, it learns fixes by feeding long short-term memory neural networks with the training data encoded into token sequences. Finally, it predicts fixes for real formatting violations with the trained models. Currently, Styler supports a single checker, Checkstyle, which is a highly configurable and popular format checker for Java. In an empirical evaluation, Styler repaired 41% of 26,791 Checkstyle violations mined from 104 GitHub projects. Moreover, we compared Styler with the IntelliJ plugin CheckStyle-IDEA and the machine-learning-based code formatters Naturalize and CodeBuff. We found out that Styler fixes violations of a diverse set of Checkstyle rules (24/25 rules), generates smaller repairs in comparison to the other systems, and predicts repairs in seconds once trained on a project. Through a manual analysis, we identified cases in which Styler does not succeed to generate correct repairs, which can guide further improvements in Styler. Finally, the results suggest that Styler can be useful to help developers repair Checkstyle formatting violations.

</details>

<details>

<summary>2022-08-19 10:43:30 - Awaiting for Godot: Stateless Model Checking that Avoids Executions where Nothing Happens</summary>

- *Bengt Jonsson, Magnus LÃ¥ng, Konstantinos Sagonas*

- `2208.09259v1` - [abs](http://arxiv.org/abs/2208.09259v1) - [pdf](http://arxiv.org/pdf/2208.09259v1)

> Stateless Model Checking (SMC) is a verification technique for concurrent programs that checks for safety violations by exploring all possible thread schedulings. It is highly effective when coupled with Dynamic Partial Order Reduction (DPOR), which introduces an equivalence on schedulings and need explore only one in each equivalence class. Even with DPOR, SMC often spends unnecessary effort in exploring loop iterations that are pure, i.e., have no effect on the program state. We present techniques for making SMC with DPOR more effective on programs with pure loop iterations. The first is a static program analysis to detect loop purity and an associated program transformation, called Partial Loop Purity Elimination, that inserts assume statements to block pure loop iterations. Subsequently, some of these assumes are turned into await statements that completely remove many assume-blocked executions. Finally, we present an extension of the standard DPOR equivalence, obtained by weakening the conflict relation between events. All these techniques are incorporated into a new DPOR algorithm, Optimal-DPOR-Await, which can handle both awaits and the weaker conflict relation, is optimal in the sense that it explores exactly one execution in each equivalence class, and can also diagnose livelocks. Our implementation in Nidhugg shows that these techniques can significantly speed up the analysis of concurrent programs that are currently challenging for SMC tools, both for exploring their complete set of interleavings, but even for detecting concurrency errors in them.

</details>

<details>

<summary>2022-08-19 14:28:13 - Dialogue Policies for Confusion Mitigation in Situated HRI</summary>

- *Na Li, Robert Ross*

- `2208.09367v1` - [abs](http://arxiv.org/abs/2208.09367v1) - [pdf](http://arxiv.org/pdf/2208.09367v1)

> Confusion is a mental state triggered by cognitive disequilibrium that can occur in many types of task-oriented interaction, including Human-Robot Interaction (HRI). People may become confused while interacting with robots due to communicative or even task-centred challenges. To build a smooth and engaging HRI, it is insufficient for an agent to simply detect confusion; instead, the system should aim to mitigate the situation. In light of this, in this paper, we present our approach to a linguistic design of dialogue policies to build a dialogue framework to alleviate interlocutor confusion. We also outline our sketch and discuss challenges with respect to its operationalisation.

</details>

<details>

<summary>2022-08-19 19:19:34 - Glass-Vault: A Generic Transparent Privacy-preserving Exposure Notification Analytics Platform</summary>

- *Lorenzo Martinico, Aydin Abadi, Thomas Zacharias, Thomas Win*

- `2208.09525v1` - [abs](http://arxiv.org/abs/2208.09525v1) - [pdf](http://arxiv.org/pdf/2208.09525v1)

> The highly transmissible COVID-19 disease is a serious threat to people's health and life. To automate tracing those who have been in close physical contact with newly infected people and/or to analyse tracing-related data, researchers have proposed various ad-hoc programs that require being executed on users' smartphones. Nevertheless, the existing solutions have two primary limitations: (1) lack of generality: for each type of analytic task, a certain kind of data needs to be sent to an analyst; (2) lack of transparency: parties who provide data to an analyst are not necessarily infected individuals; therefore, infected individuals' data can be shared with others (e.g., the analyst) without their fine-grained and direct consent. In this work, we present Glass-Vault, a protocol that addresses both limitations simultaneously. It allows an analyst to run authorised programs over the collected data of infectious users, without learning the input data. Glass-Vault relies on a new variant of generic Functional Encryption that we propose in this work. This new variant, called DD-Steel, offers these two additional properties: dynamic and decentralised. We illustrate the security of both Glass-Vault and DD-Steel in the Universal Composability setting. Glass-Vault is the first UC-secure protocol that allows analysing the data of Exposure Notification users in a privacy-preserving manner. As a sample application, we indicate how it can be used to generate "infection heatmaps".

</details>

<details>

<summary>2022-08-19 22:12:37 - Minecraft: An Engaging Platform to Learn Programming</summary>

- *Worasait Suwannik*

- `2208.09556v1` - [abs](http://arxiv.org/abs/2208.09556v1) - [pdf](http://arxiv.org/pdf/2208.09556v1)

> Teaching programming effectively is difficult. This paper explores the benefits of using Minecraft Education Edition to teach Python programming. Educators can use the game to teach various programming concepts ranging from fundamental programming concepts, object-oriented programming, event-driven programming, and parallel programming. It has several benefits, including being highly engaging, sharpen creativity and problem-solving skill, motivating the study of mathematics, and making students realizes the importance of programming.

</details>

<details>

<summary>2022-08-19 23:42:06 - Multiple Instance Neuroimage Transformer</summary>

- *Ayush Singla, Qingyu Zhao, Daniel K. Do, Yuyin Zhou, Kilian M. Pohl, Ehsan Adeli*

- `2208.09567v1` - [abs](http://arxiv.org/abs/2208.09567v1) - [pdf](http://arxiv.org/pdf/2208.09567v1)

> For the first time, we propose using a multiple instance learning based convolution-free transformer model, called Multiple Instance Neuroimage Transformer (MINiT), for the classification of T1weighted (T1w) MRIs. We first present several variants of transformer models adopted for neuroimages. These models extract non-overlapping 3D blocks from the input volume and perform multi-headed self-attention on a sequence of their linear projections. MINiT, on the other hand, treats each of the non-overlapping 3D blocks of the input MRI as its own instance, splitting it further into non-overlapping 3D patches, on which multi-headed self-attention is computed. As a proof-of-concept, we evaluate the efficacy of our model by training it to identify sex from T1w-MRIs of two public datasets: Adolescent Brain Cognitive Development (ABCD) and the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). The learned attention maps highlight voxels contributing to identifying sex differences in brain morphometry. The code is available at https://github.com/singlaayush/MINIT.

</details>

<details>

<summary>2022-08-20 11:25:54 - From Time Series to Networks in R with the ts2net Package</summary>

- *Leonardo N. Ferreira*

- `2208.09660v1` - [abs](http://arxiv.org/abs/2208.09660v1) - [pdf](http://arxiv.org/pdf/2208.09660v1)

> Network science established itself as a prominent tool for modeling time series and complex systems. This modeling process consists of transforming a set or a single time series into a network. Nodes may represent complete time series, segments, or single values, while links define associations or similarities between the represented parts. R is one of the main programming languages used in data science, statistics, and machine learning, with many packages available. However, no single package provides the necessary methods to transform time series into networks. This paper presents ts2net, an R package for modeling one or multiple time series into networks. The package provides the time series distance functions that can be easily computed in parallel and in supercomputers to process larger data sets and methods to transform distance matrices into networks. Ts2net also provides methods to transform a single time series into a network, such as recurrence networks, visibility graphs, and transition networks. Together with other packages, ts2net permits using network science and graph mining tools to extract information from time series.

</details>

<details>

<summary>2022-08-21 04:33:34 - Zeno: A Scalable Capability-Based Secure Architecture</summary>

- *Alan Ehret, Jacob Abraham, Mihailo Isakov, Michel A. Kinsy*

- `2208.09800v1` - [abs](http://arxiv.org/abs/2208.09800v1) - [pdf](http://arxiv.org/pdf/2208.09800v1)

> Despite the numerous efforts of security researchers, memory vulnerabilities remain a top issue for modern computing systems. Capability-based solutions aim to solve whole classes of memory vulnerabilities at the hardware level by encoding access permissions with each memory reference. While some capability systems have seen commercial adoption, little work has been done to apply a capability model to datacenter-scale systems. Cloud and high-performance computing often require programs to share memory across many compute nodes. This presents a challenge for existing capability models, as capabilities must be enforceable across multiple nodes. Each node must agree on what access permissions a capability has and overheads of remote memory access must remain manageable.   To address these challenges, we introduce Zeno, a new capability-based architecture. Zeno supports a Namespace-based capability model to support globally shareable capabilities in a large-scale, multi-node system. In this work, we describe the Zeno architecture, define Zeno's security properties, evaluate the scalability of Zeno as a large-scale capability architecture, and measure the hardware overhead with an FPGA implementation.

</details>

<details>

<summary>2022-08-21 05:10:19 - Friendliness Of Stack Overflow Towards Newbies</summary>

- *Aneesh Tickoo, Shweta Chauhan, Gagan Raj Gupta*

- `2208.10488v1` - [abs](http://arxiv.org/abs/2208.10488v1) - [pdf](http://arxiv.org/pdf/2208.10488v1)

> In today's modern digital world, we have a number of online Question and Answer platforms like Stack Exchange, Quora, and GFG that serve as a medium for people to communicate and help each other. In this paper, we analyzed the effectiveness of Stack Overflow in helping newbies to programming. Every user on this platform goes through a journey. For the first 12 months, we consider them to be a newbie. Post 12 months they come under one of the following categories: Experienced, Lurkers, or Inquisitive. Each question asked has tags assigned to it and we observe that questions with some specific tags have a faster response time indicating an active community in that field over others. The platform had a steady growth up to 2013 after which it started declining, but recently during the pandemic 2020, we can see rejuvenated activity on the platform.

</details>

<details>

<summary>2022-08-22 13:16:13 - Deterministic Graph-Walking Program Mining</summary>

- *Peter Belcak, Roger Wattenhofer*

- `2208.10290v1` - [abs](http://arxiv.org/abs/2208.10290v1) - [pdf](http://arxiv.org/pdf/2208.10290v1)

> Owing to their versatility, graph structures admit representations of intricate relationships between the separate entities comprising the data. We formalise the notion of connection between two vertex sets in terms of edge and vertex features by introducing graph-walking programs. We give two algorithms for mining of deterministic graph-walking programs that yield programs in the order of increasing length. These programs characterise linear long-distance relationships between the given two vertex sets in the context of the whole graph.

</details>

<details>

<summary>2022-08-22 15:13:38 - Towards an AI-based Early Warning System for Bridge Scour</summary>

- *Negin Yousefpour, Oscar Correa*

- `2208.10500v1` - [abs](http://arxiv.org/abs/2208.10500v1) - [pdf](http://arxiv.org/pdf/2208.10500v1)

> Scour is the number one cause of bridge failure in many parts of the world. Considering the lack of reliability in existing empirical equations for scour depth estimation and the complexity and uncertainty of scour as a physical phenomenon, it is essential to develop more reliable solutions for scour risk assessment. This study introduces a novel AI approach for early forecast of scour based on real-time monitoring data obtained from sonar and stage sensors installed at bridge piers. Long-short Term Memory networks (LSTMs), a prominent Deep Learning algorithm successfully used for time-series forecasting in other fields, were developed and trained using river stage and bed elevation readings for more than 11 years obtained from Alaska scour monitoring program. The capability of the AI models in scour prediction is shown for three case-study bridges. Results show that LSTMs can capture the temporal and seasonal patterns of both flow and river bed variations around bridge piers, through cycles of scour and filling and can provide reasonable predictions of upcoming scour depth as early as seven days in advance. It is expected that the proposed solution can be implemented by transportation authorities for development of emerging AI-based early warning systems, enabling superior bridge scour management.

</details>

<details>

<summary>2022-08-22 16:31:43 - Patient-level Microsatellite Stability Assessment from Whole Slide Images By Combining Momentum Contrast Learning and Group Patch Embeddings</summary>

- *Daniel Shats, Hadar Hezi, Guy Shani, Yosef E. Maruvka, Moti Freiman*

- `2208.10429v1` - [abs](http://arxiv.org/abs/2208.10429v1) - [pdf](http://arxiv.org/pdf/2208.10429v1)

> Assessing microsatellite stability status of a patient's colorectal cancer is crucial in personalizing treatment regime. Recently, convolutional-neural-networks (CNN) combined with transfer-learning approaches were proposed to circumvent traditional laboratory testing for determining microsatellite status from hematoxylin and eosin stained biopsy whole slide images (WSI). However, the high resolution of WSI practically prevent direct classification of the entire WSI. Current approaches bypass the WSI high resolution by first classifying small patches extracted from the WSI, and then aggregating patch-level classification logits to deduce the patient-level status. Such approaches limit the capacity to capture important information which resides at the high resolution WSI data. We introduce an effective approach to leverage WSI high resolution information by momentum contrastive learning of patch embeddings along with training a patient-level classifier on groups of those embeddings. Our approach achieves up to 7.4\% better accuracy compared to the straightforward patch-level classification and patient level aggregation approach with a higher stability (AUC, $0.91 \pm 0.01$ vs. $0.85 \pm 0.04$, p-value$<0.01$). Our code can be found at https://github.com/TechnionComputationalMRILab/colorectal_cancer_ai.

</details>

<details>

<summary>2022-08-23 03:58:47 - Lexicase Selection at Scale</summary>

- *Li Ding, Ryan Boldi, Thomas Helmuth, Lee Spector*

- `2208.10719v1` - [abs](http://arxiv.org/abs/2208.10719v1) - [pdf](http://arxiv.org/pdf/2208.10719v1)

> Lexicase selection is a semantic-aware parent selection method, which assesses individual test cases in a randomly-shuffled data stream. It has demonstrated success in multiple research areas including genetic programming, genetic algorithms, and more recently symbolic regression and deep learning. One potential drawback of lexicase selection and its variants is that the selection procedure requires evaluating training cases in a single data stream, making it difficult to handle tasks where the evaluation is computationally heavy or the dataset is large-scale, e.g., deep learning. In this work, we investigate how the weighted shuffle methods can be employed to improve the efficiency of lexicase selection. We propose a novel method, fast lexicase selection, which incorporates lexicase selection and weighted shuffle with partial evaluation. Experiments on both classic genetic programming and deep learning tasks indicate that the proposed method can significantly reduce the number of evaluation steps needed for lexicase selection to select an individual, improving its efficiency while maintaining the performance.

</details>

<details>

<summary>2022-08-23 07:08:54 - Adversarial Vulnerability of Temporal Feature Networks for Object Detection</summary>

- *Svetlana Pavlitskaya, Nikolai Polley, Michael Weber, J. Marius ZÃ¶llner*

- `2208.10773v1` - [abs](http://arxiv.org/abs/2208.10773v1) - [pdf](http://arxiv.org/pdf/2208.10773v1)

> Taking into account information across the temporal domain helps to improve environment perception in autonomous driving. However, it has not been studied so far whether temporally fused neural networks are vulnerable to deliberately generated perturbations, i.e. adversarial attacks, or whether temporal history is an inherent defense against them. In this work, we study whether temporal feature networks for object detection are vulnerable to universal adversarial attacks. We evaluate attacks of two types: imperceptible noise for the whole image and locally-bound adversarial patch. In both cases, perturbations are generated in a white-box manner using PGD. Our experiments confirm, that attacking even a portion of a temporal input suffices to fool the network. We visually assess generated perturbations to gain insights into the functioning of attacks. To enhance the robustness, we apply adversarial training using 5-PGD. Our experiments on KITTI and nuScenes datasets demonstrate, that a model robustified via K-PGD is able to withstand the studied attacks while keeping the mAP-based performance comparable to that of an unattacked model.

</details>

<details>

<summary>2022-08-23 12:20:39 - Optimal Network Charge for Peer-to-Peer Energy Trading: A Grid Perspective</summary>

- *Yu Yang, Yue Chen, Guoqiang Hu, Costas J. Spanos*

- `2205.01945v3` - [abs](http://arxiv.org/abs/2205.01945v3) - [pdf](http://arxiv.org/pdf/2205.01945v3)

> Peer-to-peer (P2P) energy trading is a promising market scheme to accommodate the increasing distributed energy resources (DERs). However, how P2P to be integrated into the existing power systems remains to be investigated. In this paper, we apply network charge as a means for the grid operator to attribute transmission loss and ensure network constraints for empowering P2P transaction. The interaction between the grid operator and the prosumers is modeled as a Stackelberg game, which yields a bi-level optimization problem. We prove that the Stackelberg game admits an equilibrium network charge price. Besides, we propose a method to obtain the network charge price by converting the bi-level optimization into a single-level mixed-integer quadratic programming (MIQP), which can handle a reasonable scale of prosumers efficiently. Simulations on the IEEE bus systems show that the proposed optimal network charge is favorable as it can benefit both the grid operator and the prosumers for empowering the P2P market, and achieves near-optimal social welfare. Moreover, the results show that the presence of energy storage will make the prosumers more sensitive to the network charge price changes.

</details>

<details>

<summary>2022-08-23 12:38:29 - Automated Test Generation for Scratch Programs</summary>

- *Adina Deiner, Patric Feldmeier, Gordon Fraser, Sebastian Schweikl, Wengran Wang*

- `2202.06274v2` - [abs](http://arxiv.org/abs/2202.06274v2) - [pdf](http://arxiv.org/pdf/2202.06274v2)

> The importance of programming education has lead to dedicated educational programming environments, where users visually arrange block-based programming constructs that typically control graphical, interactive game-like programs. The Scratch programming environment is particularly popular, with more than 70 million registered users at the time of this writing. While the block-based nature of Scratch helps learners by preventing syntactical mistakes, there nevertheless remains a need to provide feedback and support in order to implement desired functionality. To support individual learning and classroom settings, this feedback and support should ideally be provided in an automated fashion, which requires tests to enable dynamic program analysis. The Whisker framework enables automated testing of Scratch programs, but creating these automated tests for Scratch programs is challenging. In this paper, we therefore investigate how to automatically generate Whisker tests. This raises important challenges: First, game-like programs are typically randomised, leading to flaky tests. Second, Scratch programs usually consist of animations and interactions with long delays, inhibiting the application of classical test generation approaches. Evaluation on common programming exercises, a random sample of 1000 Scratch user programs, and the 1000 most popular Scratch programs demonstrates that our approach enables Whisker to reliably accelerate test executions, and even though many Scratch programs are small and easy to cover, there are many unique challenges for which advanced search-based test generation using many-objective algorithms is needed in order to achieve high coverage.

</details>

<details>

<summary>2022-08-23 16:12:32 - Transformer Network-based Reinforcement Learning Method for Power Distribution Network (PDN) Optimization of High Bandwidth Memory (HBM)</summary>

- *Hyunwook Park, Minsu Kim, Seongguk Kim, Keunwoo Kim, Haeyeon Kim, Taein Shin, Keeyoung Son, Boogyo Sim, Subin Kim, Seungtaek Jeong, Chulsoon Hwang, Joungho Kim*

- `2203.15722v2` - [abs](http://arxiv.org/abs/2203.15722v2) - [pdf](http://arxiv.org/pdf/2203.15722v2)

> In this article, for the first time, we propose a transformer network-based reinforcement learning (RL) method for power distribution network (PDN) optimization of high bandwidth memory (HBM). The proposed method can provide an optimal decoupling capacitor (decap) design to maximize the reduction of PDN self- and transfer impedance seen at multiple ports. An attention-based transformer network is implemented to directly parameterize decap optimization policy. The optimality performance is significantly improved since the attention mechanism has powerful expression to explore massive combinatorial space for decap assignments. Moreover, it can capture sequential relationships between the decap assignments. The computing time for optimization is dramatically reduced due to the reusable network on positions of probing ports and decap assignment candidates. This is because the transformer network has a context embedding process to capture meta-features including probing ports positions. In addition, the network is trained with randomly generated data sets. Therefore, without additional training, the trained network can solve new decap optimization problems. The computing time for training and data cost are critically decreased due to the scalability of the network. Thanks to its shared weight property, the network can adapt to a larger scale of problems without additional training. For verification, we compare the results with conventional genetic algorithm (GA), random search (RS), and all the previous RL-based methods. As a result, the proposed method outperforms in all the following aspects: optimality performance, computing time, and data efficiency.

</details>

<details>

<summary>2022-08-23 16:36:33 - Masked Image Modeling Advances 3D Medical Image Analysis</summary>

- *Zekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem Safta, Samit Hirawat, Venkat Sethuraman, Mariann Micsinai Balan, Kevin Brown*

- `2204.11716v2` - [abs](http://arxiv.org/abs/2204.11716v2) - [pdf](http://arxiv.org/pdf/2204.11716v2)

> Recently, masked image modeling (MIM) has gained considerable attention due to its capacity to learn from vast amounts of unlabeled data and has been demonstrated to be effective on a wide variety of vision tasks involving natural images. Meanwhile, the potential of self-supervised learning in modeling 3D medical images is anticipated to be immense due to the high quantities of unlabeled images, and the expense and difficulty of quality labels. However, MIM's applicability to medical images remains uncertain. In this paper, we demonstrate that masked image modeling approaches can also advance 3D medical images analysis in addition to natural images. We study how masked image modeling strategies leverage performance from the viewpoints of 3D medical image segmentation as a representative downstream task: i) when compared to naive contrastive learning, masked image modeling approaches accelerate the convergence of supervised training even faster (1.40$\times$) and ultimately produce a higher dice score; ii) predicting raw voxel values with a high masking ratio and a relatively smaller patch size is non-trivial self-supervised pretext-task for medical images modeling; iii) a lightweight decoder or projection head design for reconstruction is powerful for masked image modeling on 3D medical images which speeds up training and reduce cost; iv) finally, we also investigate the effectiveness of MIM methods under different practical scenarios where different image resolutions and labeled data ratios are applied.

</details>

<details>

<summary>2022-08-23 17:12:18 - Evaluating Synthetic Bugs</summary>

- *Joshua Bundt, Andrew Fasano, Brendan Dolan-Gavitt, William Robertson, Tim Leek*

- `2208.11088v1` - [abs](http://arxiv.org/abs/2208.11088v1) - [pdf](http://arxiv.org/pdf/2208.11088v1)

> Fuzz testing has been used to find bugs in programs since the 1990s, but despite decades of dedicated research, there is still no consensus on which fuzzing techniques work best. One reason for this is the paucity of ground truth: bugs in real programs with known root causes and triggering inputs are difficult to collect at a meaningful scale. Bug injection technologies that add synthetic bugs into real programs seem to offer a solution, but the differences in finding these synthetic bugs versus organic bugs have not previously been explored at a large scale. Using over 80 years of CPU time, we ran eight fuzzers across 20 targets from the Rode0day bug-finding competition and the LAVA-M corpus. Experiments were standardized with respect to compute resources and metrics gathered. These experiments show differences in fuzzer performance as well as the impact of various configuration options. For instance, it is clear that integrating symbolic execution with mutational fuzzing is very effective and that using dictionaries improves performance. Other conclusions are less clear-cut; for example, no one fuzzer beat all others on all tests. It is noteworthy that no fuzzer found any organic bugs (i.e., one reported in a CVE), despite 50 such bugs being available for discovery in the fuzzing corpus. A close analysis of results revealed a possible explanation: a dramatic difference between where synthetic and organic bugs live with respect to the ''main path'' discovered by fuzzers. We find that recent updates to bug injection systems have made synthetic bugs more difficult to discover, but they are still significantly easier to find than organic bugs in our target programs. Finally, this study identifies flaws in bug injection techniques and suggests a number of axes along which synthetic bugs should be improved.

</details>

<details>

<summary>2022-08-24 07:43:58 - Advanced Tools and Methods for Treewidth-Based Problem Solving -- Extended Abstract</summary>

- *Markus Hecher*

- `2208.11340v1` - [abs](http://arxiv.org/abs/2208.11340v1) - [pdf](http://arxiv.org/pdf/2208.11340v1)

> Computer programs, so-called solvers, for solving the well-known Boolean satisfiability problem (Sat) have been improving for decades. Among the reasons, why these solvers are so fast, is the implicit usage of the formula's structural properties during solving. One of such structural indicators is the so-called treewidth, which tries to measure how close a formula instance is to being easy (tree-like). This work focuses on logic-based problems and treewidth-based methods and tools for solving them. Many of these problems are also relevant for knowledge representation and reasoning (KR) as well as artificial intelligence (AI) in general. We present a new type of problem reduction, which is referred to by decomposition-guided (DG). This reduction type forms the basis to solve a problem for quantified Boolean formulas (QBFs) of bounded treewidth that has been open since 2004. The solution of this problem then gives rise to a new methodology for proving precise lower bounds for a range of further formalisms in logic, KR, and AI. Despite the established lower bounds, we implement an algorithm for solving extensions of Sat efficiently, by directly using treewidth. Our implementation is based on finding abstractions of instances, which are then incrementally refined in the process. Thereby, our observations confirm that treewidth is an important measure that should be considered in the design of modern solvers.

</details>

<details>

<summary>2022-08-24 07:52:23 - Boosting Performance Optimization with Interactive Data Movement Visualization</summary>

- *Philipp Schaad, Tal Ben-Nun, Torsten Hoefler*

- `2207.07433v2` - [abs](http://arxiv.org/abs/2207.07433v2) - [pdf](http://arxiv.org/pdf/2207.07433v2)

> Optimizing application performance in today's hardware architecture landscape is an important, but increasingly complex task, often requiring detailed performance analyses. In particular, data movement and reuse play a crucial role in optimization and are often hard to improve without detailed program inspection. Performance visualizations can assist in the diagnosis of performance problems, but generally rely on data gathered through lengthy program executions. In this paper, we present a performance visualization geared towards analyzing data movement and reuse to inform impactful optimization decisions, without requiring program execution. We propose an approach that combines static dataflow analysis with parameterized program simulations to analyze both global data movement and fine-grained data access and reuse behavior, and visualize insights in-situ on the program representation. Case studies analyzing and optimizing real-world applications demonstrate our tool's effectiveness in guiding optimization decisions and making the performance tuning process more interactive.

</details>

<details>

<summary>2022-08-24 09:46:36 - Usable Security for an IoT OS: Integrating the Zoo of Embedded Crypto Components Below a Common API</summary>

- *Lena Boeckmann, Peter Kietzmann, Leandro Lanzieri, Thomas Schmidt, Matthias WÃ¤hlisch*

- `2208.09281v2` - [abs](http://arxiv.org/abs/2208.09281v2) - [pdf](http://arxiv.org/pdf/2208.09281v2)

> IoT devices differ widely in crypto-supporting hardware, ranging from no hardware support to powerful accelerators supporting numerous of operations including protected key storage. An operating system should provide uniform access to these heterogeneous hardware features, which is a particular challenge in the resource constrained IoT. Effective security is tied to the usability of cryptographic interfaces. A thoughtful API design is challenging, and it is beneficial to re-use such an interface and to share the knowledge of programming embedded security widely.   In this paper, we integrate an emerging cryptographic interface into usable system-level calls for the IoT operating system RIOT, which runs on more than 240 platforms. This interface supports ID-based key handling to access key material in protected storage without exposing it to anyone. Our design foresees hardware acceleration on all available variants; our implementation integrates diverse cryptographic hardware and software backends via the uniform interface. Our performance measurements show that the overhead of the uniform API with integrated key management is negligible compared to the individual crypto operation. Our approach enhances the usability, portability, and flexibility of cryptographic support in the IoT.

</details>

<details>

<summary>2022-08-24 12:17:59 - Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages</summary>

- *Peter Hirsch, Caroline Malin-Mayor, Anthony Santella, Stephan Preibisch, Dagmar Kainmueller, Jan Funke*

- `2208.11467v1` - [abs](http://arxiv.org/abs/2208.11467v1) - [pdf](http://arxiv.org/pdf/2208.11467v1)

> Tracking all nuclei of an embryo in noisy and dense fluorescence microscopy data is a challenging task. We build upon a recent method for nuclei tracking that combines weakly-supervised learning from a small set of nuclei center point annotations with an integer linear program (ILP) for optimal cell lineage extraction. Our work specifically addresses the following challenging properties of C. elegans embryo recordings: (1) Many cell divisions as compared to benchmark recordings of other organisms, and (2) the presence of polar bodies that are easily mistaken as cell nuclei. To cope with (1), we devise and incorporate a learnt cell division detector. To cope with (2), we employ a learnt polar body detector. We further propose automated ILP weights tuning via a structured SVM, alleviating the need for tedious manual set-up of a respective grid search. Our method outperforms the previous leader of the cell tracking challenge on the Fluo-N3DH-CE embryo dataset. We report a further extensive quantitative evaluation on two more C. elegans datasets. We will make these datasets public to serve as an extended benchmark for future method development. Our results suggest considerable improvements yielded by our method, especially in terms of the correctness of division event detection and the number and length of fully correct track segments. Code: https://github.com/funkelab/linajea

</details>

<details>

<summary>2022-08-24 13:10:48 - Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer</summary>

- *Fengji Zhang, Jin Liu, Yao Wan, Xiao Yu, Xiao Liu, Jacky Keung*

- `2208.11523v1` - [abs](http://arxiv.org/abs/2208.11523v1) - [pdf](http://arxiv.org/pdf/2208.11523v1)

> Stack Overflow is one of the most popular programming communities where developers can seek help for their encountered problems. Nevertheless, if inexperienced developers fail to describe their problems clearly, it is hard for them to attract sufficient attention and get the anticipated answers. We propose M$_3$NSCT5, a novel approach to automatically generate multiple post titles from the given code snippets. Developers may use the generated titles to find closely related posts and complete their problem descriptions. M$_3$NSCT5 employs the CodeT5 backbone, which is a pre-trained Transformer model having an excellent language understanding and generation ability. To alleviate the ambiguity issue that the same code snippets could be aligned with different titles under varying contexts, we propose the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from. We build a large-scale dataset with 890,000 question posts covering eight programming languages to validate the effectiveness of M$_3$NSCT5. The automatic evaluation results on the BLEU and ROUGE metrics demonstrate the superiority of M$_3$NSCT5 over six state-of-the-art baseline models. Moreover, a human evaluation with trustworthy results also demonstrates the great potential of our approach for real-world application.

</details>

<details>

<summary>2022-08-24 15:40:11 - SPDY: Accurate Pruning with Speedup Guarantees</summary>

- *Elias Frantar, Dan Alistarh*

- `2201.13096v2` - [abs](http://arxiv.org/abs/2201.13096v2) - [pdf](http://arxiv.org/pdf/2201.13096v2)

> The recent focus on the efficiency of deep neural networks (DNNs) has led to significant work on model compression approaches, of which weight pruning is one of the most popular. At the same time, there is rapidly-growing computational support for efficiently executing the unstructured-sparse models obtained via pruning. Yet, most existing pruning methods minimize just the number of remaining weights, i.e. the size of the model, rather than optimizing for inference time. We address this gap by introducing SPDY, a new compression method which automatically determines layer-wise sparsity targets achieving a desired inference speedup on a given system, while minimizing accuracy loss. SPDY is composed of two new techniques: the first is an efficient dynamic programming algorithm for solving the speedup-constrained layer-wise compression problem assuming a set of given layer-wise sensitivity scores; the second is a local search procedure for determining accurate layer-wise sensitivity scores. Experiments across popular vision and language models show that SPDY guarantees speedups while recovering higher accuracy relative to existing strategies, both for one-shot and gradual pruning scenarios, and is compatible with most existing pruning approaches. We also extend our approach to the recently-proposed task of pruning with very little data, where we achieve the best known accuracy recovery when pruning to the GPU-supported 2:4 sparsity pattern.

</details>

<details>

<summary>2022-08-24 16:53:54 - Constraint-driven multi-task learning</summary>

- *Bogdan Cretu, Andrew Cropper*

- `2208.11656v1` - [abs](http://arxiv.org/abs/2208.11656v1) - [pdf](http://arxiv.org/pdf/2208.11656v1)

> Inductive logic programming is a form of machine learning based on mathematical logic that generates logic programs from given examples and background knowledge.   In this project, we extend the Popper ILP system to make use of multi-task learning. We implement the state-of-the-art approach and several new strategies to improve search performance. Furthermore, we introduce constraint preservation, a technique that improves overall performance for all approaches.   Constraint preservation allows the system to transfer knowledge between updates on the background knowledge set. Consequently, we reduce the amount of repeated work performed by the system. Additionally, constraint preservation allows us to transition from the current state-of-the-art iterative deepening search approach to a more efficient breadth first search approach.   Finally, we experiment with curriculum learning techniques and show their potential benefit to the field.

</details>

<details>

<summary>2022-08-24 18:05:57 - OMB-Py: Python Micro-Benchmarks for Evaluating Performance of MPI Libraries on HPC Systems</summary>

- *Nawras Alnaasan, Arpan Jain, Aamir Shafi, Hari Subramoni, Dhabaleswar K Panda*

- `2110.10659v2` - [abs](http://arxiv.org/abs/2110.10659v2) - [pdf](http://arxiv.org/pdf/2110.10659v2)

> Python has become a dominant programming language for emerging areas like Machine Learning (ML), Deep Learning (DL), and Data Science (DS). An attractive feature of Python is that it provides easy-to-use programming interface while allowing library developers to enhance performance of their applications by harnessing the computing power offered by High Performance Computing (HPC) platforms. Efficient communication is key to scaling applications on parallel systems, which is typically enabled by the Message Passing Interface (MPI) standard and compliant libraries on HPC hardware. mpi4py is a Python-based communication library that provides an MPI-like interface for Python applications allowing application developers to utilize parallel processing elements including GPUs. However, there is currently no benchmark suite to evaluate communication performance of mpi4py -- and Python MPI codes in general -- on modern HPC systems. In order to bridge this gap, we propose OMB-Py -- Python extensions to the open-source OSU Micro-Benchmark (OMB) suite -- aimed to evaluate communication performance of MPI-based parallel applications in Python. To the best of our knowledge, OMB-Py is the first communication benchmark suite for parallel Python applications. OMB-Py consists of a variety of point-to-point and collective communication benchmark tests that are implemented for a range of popular Python libraries including NumPy, CuPy, Numba, and PyCUDA. Our evaluation reveals that mpi4py introduces a small overhead when compared to native MPI libraries. We plan to publicly release OMB-Py to benefit the Python HPC community.

</details>

<details>

<summary>2022-08-24 22:04:51 - Pushing the limits of fairness impossibility: Who's the fairest of them all?</summary>

- *Brian Hsu, Rahul Mazumder, Preetam Nandy, Kinjal Basu*

- `2208.12606v1` - [abs](http://arxiv.org/abs/2208.12606v1) - [pdf](http://arxiv.org/pdf/2208.12606v1)

> The impossibility theorem of fairness is a foundational result in the algorithmic fairness literature. It states that outside of special cases, one cannot exactly and simultaneously satisfy all three common and intuitive definitions of fairness - demographic parity, equalized odds, and predictive rate parity. This result has driven most works to focus on solutions for one or two of the metrics. Rather than follow suit, in this paper we present a framework that pushes the limits of the impossibility theorem in order to satisfy all three metrics to the best extent possible. We develop an integer-programming based approach that can yield a certifiably optimal post-processing method for simultaneously satisfying multiple fairness criteria under small violations. We show experiments demonstrating that our post-processor can improve fairness across the different definitions simultaneously with minimal model performance reduction. We also discuss applications of our framework for model selection and fairness explainability, thereby attempting to answer the question: who's the fairest of them all?

</details>

<details>

<summary>2022-08-25 08:33:03 - Sparse Polynomial Optimization: Theory and Practice</summary>

- *Victor Magron, Jie Wang*

- `2208.11158v2` - [abs](http://arxiv.org/abs/2208.11158v2) - [pdf](http://arxiv.org/pdf/2208.11158v2)

> The problem of minimizing a polynomial over a set of polynomial inequalities is an NP-hard non-convex problem. Thanks to powerful results from real algebraic geometry, one can convert this problem into a nested sequence of finite-dimensional convex problems. At each step of the associated hierarchy, one needs to solve a fixed size semidefinite program, which can be in turn solved with efficient numerical tools. On the practical side however, there is no-free lunch and such optimization methods usually encompass severe scalability issues. Fortunately, for many applications, we can look at the problem in the eyes and exploit the inherent data structure arising from the cost and constraints describing the problem, for instance sparsity or symmetries.   This book presents several research efforts to tackle this scientific challenge with important computational implications, and provides the development of alternative optimization schemes that scale well in terms of computational complexity, at least in some identified class of problems. The presented algorithmic framework in this book mainly exploits the sparsity structure of the input data to solve large-scale polynomial optimization problems. We present sparsity-exploiting hierarchies of relaxations, for either unconstrained or constrained problems. By contrast with the dense hierarchies, they provide faster approximation of the solution in practice but also come with the same theoretical convergence guarantees. Our framework is not restricted to static polynomial optimization, and we expose hierarchies of approximations for values of interest arising from the analysis of dynamical systems. We also present various extensions to problems involving noncommuting variables, e.g., matrices of arbitrary size or quantum physic operators.

</details>

<details>

<summary>2022-08-25 09:08:32 - TEP-GNN: Accurate Execution Time Prediction of Functional Tests using Graph Neural Networks</summary>

- *Hazem Peter Samoaa, Antonio Longa, Mazen Mohamad, Morteza Haghir Chehreghani, Philipp Leitner*

- `2208.11947v1` - [abs](http://arxiv.org/abs/2208.11947v1) - [pdf](http://arxiv.org/pdf/2208.11947v1)

> Predicting the performance of production code prior to actually executing or benchmarking it is known to be highly challenging. In this paper, we propose a predictive model, dubbed TEP-GNN, which demonstrates that high-accuracy performance prediction is possible for the special case of predicting unit test execution times. TEP-GNN uses FA-ASTs, or flow-augmented ASTs, as a graph-based code representation approach, and predicts test execution times using a powerful graph neural network (GNN) deep learning model. We evaluate TEP-GNN using four real-life Java open source programs, based on 922 test files mined from the projects' public repositories. We find that our approach achieves a high Pearson correlation of 0.789, considerable outperforming a baseline deep learning model. However, we also find that more work is needed for trained models to generalize to unseen projects. Our work demonstrates that FA-ASTs and GNNs are a feasible approach for predicting absolute performance values, and serves as an important intermediary step towards being able to predict the performance of arbitrary code prior to execution.

</details>

<details>

<summary>2022-08-25 14:09:56 - Improving Stack Overflow question title generation with copying enhanced CodeBERT model and bi-modal information</summary>

- *Fengji Zhang, Xiao Yu, Jacky Keung, Fuyang Li, Zhiwen Xie, Zhen Yang, Caoyuan Ma, Zhimin Zhang*

- `2109.13073v2` - [abs](http://arxiv.org/abs/2109.13073v2) - [pdf](http://arxiv.org/pdf/2109.13073v2)

> Context: Stack Overflow is very helpful for software developers who are seeking answers to programming problems. Previous studies have shown that a growing number of questions are of low quality and thus obtain less attention from potential answerers. Gao et al. proposed an LSTM-based model (i.e., BiLSTM-CC) to automatically generate question titles from the code snippets to improve the question quality. However, only using the code snippets in the question body cannot provide sufficient information for title generation, and LSTMs cannot capture the long-range dependencies between tokens. Objective: This paper proposes CCBERT, a deep learning based novel model to enhance the performance of question title generation by making full use of the bi-modal information of the entire question body. Method: CCBERT follows the encoder-decoder paradigm and uses CodeBERT to encode the question body into hidden representations, a stacked Transformer decoder to generate predicted tokens, and an additional copy attention layer to refine the output distribution. Both the encoder and decoder perform the multi-head self-attention operation to better capture the long-range dependencies. This paper builds a dataset containing around 200,000 high-quality questions filtered from the data officially published by Stack Overflow to verify the effectiveness of the CCBERT model. Results: CCBERT outperforms all the baseline models on the dataset. Experiments on both code-only and low-resource datasets show the superiority of CCBERT with less performance degradation. The human evaluation also shows the excellent performance of CCBERT concerning both readability and correlation criteria.

</details>

<details>

<summary>2022-08-25 14:17:29 - Runtime reliability monitoring for complex fault-tolerance policies</summary>

- *Alessandro Fantechi, Gloria Gori, Marco Papini*

- `2208.12111v1` - [abs](http://arxiv.org/abs/2208.12111v1) - [pdf](http://arxiv.org/pdf/2208.12111v1)

> Reliability of complex Cyber-Physical Systems is necessary to guarantee availability and/or safety of the provided services. Diverse and complex fault tolerance policies are adopted to enhance reliability, that include a varied mix of redundancy and dynamic reconfiguration to address hardware reliability, as well as specific software reliability techniques like diversity or software rejuvenation. These complex policies call for flexible runtime health checks of system executions that go beyond conventional runtime monitoring of pre-programmed health conditions, also in order to minimize maintenance costs. Defining a suitable monitoring model in the application of this method in complex systems is still a challenge. In this paper we propose a novel approach, Reliability Based Monitoring (RBM), for a flexible runtime monitoring of reliability in complex systems, that exploits a hierarchical reliability model periodically applied to runtime diagnostics data: this allows to dynamically plan maintenance activities aimed at prevent failures. As a proof of concept, we show how to apply RBM to a 2oo3 software system implementing different fault-tolerant policies.

</details>

<details>

<summary>2022-08-25 16:13:29 - JAXFit: Trust Region Method for Nonlinear Least-Squares Curve Fitting on the GPU</summary>

- *Lucas R. Hofer, Milan KrstajiÄ, Robert P. Smith*

- `2208.12187v1` - [abs](http://arxiv.org/abs/2208.12187v1) - [pdf](http://arxiv.org/pdf/2208.12187v1)

> We implement a trust region method on the GPU for nonlinear least squares curve fitting problems using a new deep learning Python library called JAX. Our open source package, JAXFit, works for both unconstrained and constrained curve fitting problems and allows the fit functions to be defined in Python alone -- without any specialized knowledge of either the GPU or CUDA programming. Since JAXFit runs on the GPU, it is much faster than CPU based libraries and even other GPU based libraries, despite being very easy to use. Additionally, due to JAX's deep learning foundations, the Jacobian in JAXFit's trust region algorithm is calculated with automatic differentiation, rather than than using derivative approximations or requiring the user to define the fit function's partial derivatives.

</details>

<details>

<summary>2022-08-25 16:39:59 - Automating UAV Flight Readiness Approval using Goal-Directed Answer Set Programming</summary>

- *Sarat Chandra Varanasi, Baoluo Meng, Christopher Alexander, Szabolcs Borgyos, Brendan Hall*

- `2208.12199v1` - [abs](http://arxiv.org/abs/2208.12199v1) - [pdf](http://arxiv.org/pdf/2208.12199v1)

> We present a novel application of Goal-Directed Answer Set Programming that digitizes the model aircraft operator's compliance verification against the Academy of Model Aircrafts (AMA) safety code. The AMA safety code regulates how AMA flyers operate Unmanned Aerial Vehicles (UAVs) for limited recreational purposes. Flying drones and their operators are subject to various rules before and after the operation of the aircraft to ensure safe flights. In this paper, we leverage Answer Set Programming to encode the AMA safety code and automate compliance checks. To check compliance, we use the s(CASP) which is a goal-directed ASP engine. By using s(CASP) the operators can easily check for violations and obtain a justification tree explaining the cause of the violations in human-readable natural language. Further, we implement an algorithm to help the operators obtain the minimal set of conditions that need to be satisfied in order to pass the compliance check. We develop a front-end questionnaire interface that accepts various conditions and use the backend s(CASP) engine to evaluate whether the conditions adhere to the regulations. We also leverage s(CASP) implemented in SWI-Prolog, where SWI-Prolog exposes the reasoning capabilities of s(CASP) as a REST service. To the best of our knowledge, this is the first application of ASP in the AMA and Avionics Compliance and Certification space.

</details>

<details>

<summary>2022-08-25 18:01:04 - Learning Continuous Implicit Representation for Near-Periodic Patterns</summary>

- *Bowei Chen, Tiancheng Zhi, Martial Hebert, Srinivasa G. Narasimhan*

- `2208.12278v1` - [abs](http://arxiv.org/abs/2208.12278v1) - [pdf](http://arxiv.org/pdf/2208.12278v1)

> Near-Periodic Patterns (NPP) are ubiquitous in man-made scenes and are composed of tiled motifs with appearance differences caused by lighting, defects, or design elements. A good NPP representation is useful for many applications including image completion, segmentation, and geometric remapping. But representing NPP is challenging because it needs to maintain global consistency (tiled motifs layout) while preserving local variations (appearance differences). Methods trained on general scenes using a large dataset or single-image optimization struggle to satisfy these constraints, while methods that explicitly model periodicity are not robust to periodicity detection errors. To address these challenges, we learn a neural implicit representation using a coordinate-based MLP with single image optimization. We design an input feature warping module and a periodicity-guided patch loss to handle both global consistency and local variations. To further improve the robustness, we introduce a periodicity proposal module to search and use multiple candidate periodicities in our pipeline. We demonstrate the effectiveness of our method on more than 500 images of building facades, friezes, wallpapers, ground, and Mondrian patterns on single and multi-planar scenes.

</details>

<details>

<summary>2022-08-25 19:38:18 - Mask-Mediator-Wrapper: A revised mediator-wrapper architecture for heterogeneous data source integration</summary>

- *Juraj DonÄeviÄ, KreÅ¡imir Fertalj, Mario BrÄiÄ, Agneza Krajna*

- `2208.12319v1` - [abs](http://arxiv.org/abs/2208.12319v1) - [pdf](http://arxiv.org/pdf/2208.12319v1)

> This paper deals with the mediator-wrapper architecture. It is an important architectural pattern that enables a more flexible and modular architecture in opposition to monolithic architectures for data source integration systems. This paper identifies certain realistic and concrete scenarios where the mediator-wrapper architecture underperforms. These issues are addressed with the extension of the architecture via the mask component type. The mask component is detailed so it can be reasoned about without prescribing a concrete programming language or paradigm. The benefits of the new mask-mediator-wrapper architecture are analytically proven in relevant scenarios. One of the applications of the new architecture is envisioned for modern data sources integration systems backing Big data processing.

</details>

<details>

<summary>2022-08-25 21:38:08 - Understanding the Power of Evolutionary Computation for GPU Code Optimization</summary>

- *Jhe-Yu Liou, Muaaz Awan, Steven Hofmeyr, Stephanie Forrest, Carole-Jean Wu*

- `2208.12350v1` - [abs](http://arxiv.org/abs/2208.12350v1) - [pdf](http://arxiv.org/pdf/2208.12350v1)

> Achieving high performance for GPU codes requires developers to have significant knowledge in parallel programming and GPU architectures, and in-depth understanding of the application. This combination makes it challenging to find performance optimizations for GPU-based applications, especially in scientific computing. This paper shows that significant speedups can be achieved on two quite different scientific workloads using the tool, GEVO, to improve performance over human-optimized GPU code. GEVO uses evolutionary computation to find code edits that improve the runtime of a multiple sequence alignment kernel and a SARS-CoV-2 simulation by 28.9% and 29% respectively. Further, when GEVO begins with an early, unoptimized version of the sequence alignment program, it finds an impressive 30 times speedup -- a performance improvement similar to that of the hand-tuned version. This work presents an in-depth analysis of the discovered optimizations, revealing that the primary sources of improvement vary across applications; that most of the optimizations generalize across GPU architectures; and that several of the most important optimizations involve significant code interdependencies. The results showcase the potential of automated program optimization tools to help reduce the optimization burden for scientific computing developers and enhance performance portability for domain-specific accelerators.

</details>

<details>

<summary>2022-08-25 22:18:38 - Social Diversity for ATL Repair</summary>

- *Zahra Varaminybahnemiry, Jessie Galasso, Houari Sahraoui*

- `2208.12359v1` - [abs](http://arxiv.org/abs/2208.12359v1) - [pdf](http://arxiv.org/pdf/2208.12359v1)

> Model transformations play an essential role in the Model-Driven Engineering paradigm. Writing a correct transformation program requires to be proficient with the source and target modeling languages, to have a clear understanding of the mapping between the elements of the two, as well as to master the transformation language to properly describe the transformation. Transformation programs are thus complex and error-prone, and finding and fixing errors in such programs typically involve a tedious and time-consuming effort by developers. In this paper, we propose a novel search-based approach to automatically repair transformation programs containing many semantic errors. To prevent the fitness plateaus and the single fitness peak limitations, we leverage the notion of social diversity to promote repair patches tackling errors that are less covered by the other patches of the population. We evaluate our approach on 71 semantically incorrect transformation programs written in ATL, and containing up to five semantic errors simultaneously. The evaluation shows that integrating social diversity when searching for repair patches allows to improve the quality of those patches and to speed up the convergence even when up to five semantic errors are involved.

</details>

<details>

<summary>2022-08-26 01:58:26 - FuncFooler: A Practical Black-box Attack Against Learning-based Binary Code Similarity Detection Methods</summary>

- *Lichen Jia, Bowen Tang, Chenggang Wu, Zhe Wang, Zihan Jiang, Yuanming Lai, Yan Kang, Ning Liu, Jingfeng Zhang*

- `2208.14191v1` - [abs](http://arxiv.org/abs/2208.14191v1) - [pdf](http://arxiv.org/pdf/2208.14191v1)

> The binary code similarity detection (BCSD) method measures the similarity of two binary executable codes. Recently, the learning-based BCSD methods have achieved great success, outperforming traditional BCSD in detection accuracy and efficiency. However, the existing studies are rather sparse on the adversarial vulnerability of the learning-based BCSD methods, which cause hazards in security-related applications. To evaluate the adversarial robustness, this paper designs an efficient and black-box adversarial code generation algorithm, namely, FuncFooler. FuncFooler constrains the adversarial codes 1) to keep unchanged the program's control flow graph (CFG), and 2) to preserve the same semantic meaning. Specifically, FuncFooler consecutively 1) determines vulnerable candidates in the malicious code, 2) chooses and inserts the adversarial instructions from the benign code, and 3) corrects the semantic side effect of the adversarial code to meet the constraints. Empirically, our FuncFooler can successfully attack the three learning-based BCSD models, including SAFE, Asm2Vec, and jTrans, which calls into question whether the learning-based BCSD is desirable.

</details>

<details>

<summary>2022-08-26 03:53:48 - An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode</summary>

- *Sila Lertbanjongngam, Bodin Chinthanet, Takashi Ishio, Raula Gaikovina Kula, Pattara Leelaprute, Bundit Manaskasemsak, Arnon Rungsawang, Kenichi Matsumoto*

- `2208.08603v2` - [abs](http://arxiv.org/abs/2208.08603v2) - [pdf](http://arxiv.org/pdf/2208.08603v2)

> AlphaCode is a code generation system for assisting software developers in solving competitive programming problems using natural language problem descriptions. Despite the advantages of the code generating system, the open source community expressed concerns about practicality and data licensing. However, there is no research investigating generated codes in terms of code clone and performance. In this paper, we conduct an empirical study to find code similarities and performance differences between AlphaCode-generated codes and human codes. The results show that (i) the generated codes from AlphaCode are similar to human codes (i.e., the average maximum similarity score is 0.56) and (ii) the generated code performs on par with or worse than the human code in terms of execution time and memory usage. Moreover, AlphaCode tends to generate more similar codes to humans for low-difficulty problems (i.e., four cases have the exact same codes). It also employs excessive nested loops and unnecessary variable declarations for high-difficulty problems, which cause low performance regarding our manual investigation. The replication package is available at https:/doi.org/10.5281/zenodo.6820681

</details>

<details>

<summary>2022-08-26 05:09:56 - Temporal Fuzzy Utility Maximization with Remaining Measure</summary>

- *Shicheng Wan, Zhenqiang Ye, Wensheng Gan, Jiahui Chen*

- `2208.12439v1` - [abs](http://arxiv.org/abs/2208.12439v1) - [pdf](http://arxiv.org/pdf/2208.12439v1)

> High utility itemset mining approaches discover hidden patterns from large amounts of temporal data. However, an inescapable problem of high utility itemset mining is that its discovered results hide the quantities of patterns, which causes poor interpretability. The results only reflect the shopping trends of customers, which cannot help decision makers quantify collected information. In linguistic terms, computers use mathematical or programming languages that are precisely formalized, but the language used by humans is always ambiguous. In this paper, we propose a novel one-phase temporal fuzzy utility itemset mining approach called TFUM. It revises temporal fuzzy-lists to maintain less but major information about potential high temporal fuzzy utility itemsets in memory, and then discovers a complete set of real interesting patterns in a short time. In particular, the remaining measure is the first adopted in the temporal fuzzy utility itemset mining domain in this paper. The remaining maximal temporal fuzzy utility is a tighter and stronger upper bound than that of previous studies adopted. Hence, it plays an important role in pruning the search space in TFUM. Finally, we also evaluate the efficiency and effectiveness of TFUM on various datasets. Extensive experimental results indicate that TFUM outperforms the state-of-the-art algorithms in terms of runtime cost, memory usage, and scalability. In addition, experiments prove that the remaining measure can significantly prune unnecessary candidates during mining.

</details>

<details>

<summary>2022-08-26 08:23:38 - Privacy with Good Taste: A Case Study in Quantifying Privacy Risks in Genetic Scores</summary>

- *RaÃºl Pardo, Willard Rafnsson, Gregor Steinhorn, Denis Lavrov, Thomas Lumley, Christian W. Probst, Ilze Ziedins, Andrzej WÄsowski*

- `2208.12497v1` - [abs](http://arxiv.org/abs/2208.12497v1) - [pdf](http://arxiv.org/pdf/2208.12497v1)

> Analysis of genetic data opens up many opportunities for medical and scientific advances. The use of phenotypic information and polygenic risk scores to analyze genetic data is widespread. Most work on genetic privacy focuses on basic genetic data such as SNP values and specific genotypes. In this paper, we introduce a novel methodology to quantify and prevent privacy risks by focusing on polygenic scores and phenotypic information. Our methodology is based on the tool-supported privacy risk analysis method Privug. We demonstrate the use of Privug to assess privacy risks posed by disclosing a polygenic trait score for bitter taste receptors, encoded by TAS2R38 and TAS2R16, to a person's privacy in regards to their ethnicity. We provide an extensive privacy risks analysis of different programs for genetic data disclosure: taster phenotype, tasting polygenic score, and a polygenic score distorted with noise. Finally, we discuss the privacy/utility trade-offs of the polygenic score.

</details>

<details>

<summary>2022-08-26 10:12:27 - Computing Maximum Fixed Point Solutions over Feasible Paths in Data Flow Analyses</summary>

- *Komal Pathade, Uday Khedker*

- `2208.12561v1` - [abs](http://arxiv.org/abs/2208.12561v1) - [pdf](http://arxiv.org/pdf/2208.12561v1)

> The control flow graph (CFG) representation of a procedure used by virtually all flow-sensitive program analyses, admits a large number of infeasible control flow paths i.e., these paths do not occur in any execution of the program. Hence the information reaching along infeasible paths in an analysis is spurious. This affects the precision of the conventional maximum fixed point (MFP) solution of the data flow analysis, because it includes the information reaching along all control flow paths. The existing approaches for removing this imprecision are either specific to a data flow problem with no straightforward generalization or involve control flow graph restructuring which may exponentially blow up the size of the CFG.   We lift the notion of MFP solution to define the notion of feasible path MFP (FPMFP) solutions that exclude the data flowing along known infeasible paths. The notion of FPMFP is generic and does not involve CFG restructuring. Instead, it takes externally supplied information about infeasible paths and lifts any data flow analysis to an analysis that maintains the distinctions between different paths where these distinctions are beneficial, and ignores them where they are not. Thus it gets the benefit of a path-sensitive analysis where it is useful without performing a conventional path-sensitive analysis.   We evaluated the proposed feasible path MFP solutions for reaching definitions analysis and potentially uninitialized variable analysis on 30 benchmarks. The evaluation results indicate that precision improvement in these two analyses respectively reduce the number def-use pairs by up to 13.6% (average 2.87%, geometric mean 1.75%), and reduce the potentially uninitialized variable alarms by up to 100% (average 18.5%, geo. mean 3%). We found that the FPMFP computation time was 2.9X of MFP computation time on average.

</details>

<details>

<summary>2022-08-26 13:43:41 - ZK-SecreC: a Domain-Specific Language for Zero Knowledge Proofs</summary>

- *Dan Bogdanov, Joosep JÃ¤Ã¤ger, Peeter Laud, HÃ¤rmel Nestra, Martin Pettai, Jaak Randmets, Ville Sokk, Kert Tali, Sandhra-Mirella Valdma*

- `2203.15448v2` - [abs](http://arxiv.org/abs/2203.15448v2) - [pdf](http://arxiv.org/pdf/2203.15448v2)

> We present ZK-SecreC, a domain-specific language for zero-knowledge proofs. We present the rationale for its design, its syntax and semantics, and demonstrate its usefulness on the basis of a number of non-trivial examples. The design features a type system, where each piece of data is assigned both a confidentiality and an integrity type, which are not orthogonal to each other. We perform an empiric evaluation of the statements produced by its compiler in terms of their size. We also show the integration of the compiler with the implementation of a zero-knowledge proof technique, and evaluate the running time of both Prover and Verifier.

</details>

<details>

<summary>2022-08-26 15:27:21 - A Formal Comparison between Datalog-based Languages for Stream Reasoning (extended version)</summary>

- *Nicola Leone, Marco Manna, Maria Concetta Morelli, Simona Perri*

- `2208.12726v1` - [abs](http://arxiv.org/abs/2208.12726v1) - [pdf](http://arxiv.org/pdf/2208.12726v1)

> The paper investigates the relative expressiveness of two logic-based languages for reasoning over streams, namely LARS Programs -- the language of the Logic-based framework for Analytic Reasoning over Streams called LARS -- and LDSR -- the language of the recent extension of the I-DLV system for stream reasoning called I-DLV-sr. Although these two languages build over Datalog, they do differ both in syntax and semantics. To reconcile their expressive capabilities for stream reasoning, we define a comparison framework that allows us to show that, without any restrictions, the two languages are incomparable and to identify fragments of each language that can be expressed via the other one.

</details>

<details>

<summary>2022-08-26 15:46:36 - Defect Prediction Using Stylistic Metrics</summary>

- *Rafed Muhammad Yasir, Ahmedul Kabir*

- `2206.10959v3` - [abs](http://arxiv.org/abs/2206.10959v3) - [pdf](http://arxiv.org/pdf/2206.10959v3)

> Defect prediction is one of the most popular research topics due to its potential to minimize software quality assurance efforts. Existing approaches have examined defect prediction from various perspectives such as complexity and developer metrics. However, none of these consider programming style for defect prediction. This paper aims at analyzing the impact of stylistic metrics on both within-project and crossproject defect prediction. For prediction, 4 widely used machine learning algorithms namely Naive Bayes, Support Vector Machine, Decision Tree and Logistic Regression are used. The experiment is conducted on 14 releases of 5 popular, open source projects. F1, Precision and Recall are inspected to evaluate the results. Results reveal that stylistic metrics are a good predictor of defects.

</details>

<details>

<summary>2022-08-26 17:20:58 - Learning and Compositionality: a Unification Attempt via Connectionist Probabilistic Programming</summary>

- *Ximing Qiao, Hai Li*

- `2208.12789v1` - [abs](http://arxiv.org/abs/2208.12789v1) - [pdf](http://arxiv.org/pdf/2208.12789v1)

> We consider learning and compositionality as the key mechanisms towards simulating human-like intelligence. While each mechanism is successfully achieved by neural networks and symbolic AIs, respectively, it is the combination of the two mechanisms that makes human-like intelligence possible. Despite the numerous attempts on building hybrid neuralsymbolic systems, we argue that our true goal should be unifying learning and compositionality, the core mechanisms, instead of neural and symbolic methods, the surface approaches to achieve them. In this work, we review and analyze the strengths and weaknesses of neural and symbolic methods by separating their forms and meanings (structures and semantics), and propose Connectionist Probabilistic Program (CPPs), a framework that connects connectionist structures (for learning) and probabilistic program semantics (for compositionality). Under the framework, we design a CPP extension for small scale sequence modeling and provide a learning algorithm based on Bayesian inference. Although challenges exist in learning complex patterns without supervision, our early results demonstrate CPP's successful extraction of concepts and relations from raw sequential data, an initial step towards compositional learning.

</details>

<details>

<summary>2022-08-26 17:53:41 - Improving Counterexample Quality from Failed Program Verification</summary>

- *Li Huang, Bertrand Meyer, Manuel Oriol*

- `2208.10492v2` - [abs](http://arxiv.org/abs/2208.10492v2) - [pdf](http://arxiv.org/pdf/2208.10492v2)

> In software verification, a successful automated program proof is the ultimate triumph. The road to such success is, however, paved with many failed proof attempts. The message produced by the prover when a proof fails is often obscure, making it very hard to know how to proceed further. The work reported here attempts to help in such cases by providing immediately understandable counterexamples.   To this end, it introduces an approach called Counterexample Extraction and Minimization (CEAM). When a proof fails, CEAM turns the counterexample model generated by the prover into a a clearly understandable version; it can in addition simplify the counterexamples further by minimizing the integer values they contain. We have implemented the CEAM approach as an extension to the AutoProof verifier and demonstrate its application to a collection of examples.

</details>

<details>

<summary>2022-08-27 18:45:16 - Analysis of Validating and Verifying OpenACC Compilers 3.0 and Above</summary>

- *A. M. Jarmusch, A. Liu, C. Munley, D. Horta, V. Ravichandran, J. Denny, S. Chandrasekaran*

- `2208.13071v1` - [abs](http://arxiv.org/abs/2208.13071v1) - [pdf](http://arxiv.org/pdf/2208.13071v1)

> OpenACC is a high-level directive-based parallel programming model that can manage the sophistication of heterogeneity in architectures and abstract it from the users. The portability of the model across CPUs and accelerators has gained the model a wide variety of users. This means it is also crucial to analyze the reliability of the compilers' implementations. To address this challenge, the OpenACC Validation and Verification team has proposed a validation testsuite to verify the OpenACC implementations across various compilers with an infrastructure for a more streamlined execution. This paper will cover the following aspects: (a) the new developments since the last publication on the testsuite, (b) outline the use of the infrastructure, (c) discuss tests that highlight our workflow process, (d) analyze the results from executing the testsuite on various systems, and (e) outline future developments.

</details>

<details>

<summary>2022-08-28 04:09:36 - An Access Control Method with Secret Key for Semantic Segmentation Models</summary>

- *Teru Nagamori, Ryota Iijima, Hitoshi Kiya*

- `2208.13135v1` - [abs](http://arxiv.org/abs/2208.13135v1) - [pdf](http://arxiv.org/pdf/2208.13135v1)

> A novel method for access control with a secret key is proposed to protect models from unauthorized access in this paper. We focus on semantic segmentation models with the vision transformer (ViT), called segmentation transformer (SETR). Most existing access control methods focus on image classification tasks, or they are limited to CNNs. By using a patch embedding structure that ViT has, trained models and test images can be efficiently encrypted with a secret key, and then semantic segmentation tasks are carried out in the encrypted domain. In an experiment, the method is confirmed to provide the same accuracy as that of using plain images without any encryption to authorized users with a correct key and also to provide an extremely degraded accuracy to unauthorized users.

</details>

<details>

<summary>2022-08-28 13:03:06 - Measuring design compliance using neural language models -- an automotive case study</summary>

- *Dhasarathy Parthasarathy, Cecilia Ekelin, Anjali Karri, Jiapeng Sun, Panagiotis Moraitis*

- `2208.13215v1` - [abs](http://arxiv.org/abs/2208.13215v1) - [pdf](http://arxiv.org/pdf/2208.13215v1)

> As the modern vehicle becomes more software-defined, it is beginning to take significant effort to avoid serious regression in software design. This is because automotive software architects rely largely upon manual review of code to spot deviations from specified design principles. Such an approach is both inefficient and prone to error. In recent days, neural language models pre-trained on source code are beginning to be used for automating a variety of programming tasks. In this work, we extend the application of such a Programming Language Model (PLM) to automate the assessment of design compliance. Using a PLM, we construct a system that assesses whether a set of query programs comply with Controller-Handler, a design pattern specified to ensure hardware abstraction in automotive control software. The assessment is based upon measuring whether the geometrical arrangement of query program embeddings, extracted from the PLM, aligns with that of a set of known implementations of the pattern. The level of alignment is then transformed into an interpretable measure of compliance. Using a controlled experiment, we demonstrate that our technique determines compliance with a precision of 92%. Also, using expert review to calibrate the automated assessment, we introduce a protocol to determine the nature of the violation, helping eventual refactoring. Results from this work indicate that neural language models can provide valuable assistance to human architects in assessing and fixing violations in automotive software design.

</details>

<details>

<summary>2022-08-28 15:38:20 - How Segregation Patterns Affect the Availability of Fair District Plans</summary>

- *William Hager, Betseygail Rand*

- `2208.13235v1` - [abs](http://arxiv.org/abs/2208.13235v1) - [pdf](http://arxiv.org/pdf/2208.13235v1)

> We create 4200 synthetic cities which vary in percent minority population and their residential segregation patterns. Of these, 1200 are modeled on existing cities, and 3000 are rectangular grid cities. In each city, we consider single-member voting district plans for a hypothetical city council election. A fair district plan is defined as one where the number of minority-majority districts is proportional to the city-wide minority population. Thus each city is summarized by three traits: minority percent, a measure of segregation, and availability of a fair district plan. We find that when the minority population is around 25%-33%, there is a positive correlation between the degree of segregation and the availability of proportional district plan. Consistently, when the minority population lives in a more diffuse residential pattern, there are fewer available proportional district plans. Finally, we develop a new method to validate runtime and sample size of an ensemble of district plans created by the GerryChain software program.

</details>

<details>

<summary>2022-08-28 16:27:43 - Assessing the Impact of Execution Environment on Observation-Based Slicing</summary>

- *David Binkley, Leon Moonen*

- `2208.13244v1` - [abs](http://arxiv.org/abs/2208.13244v1) - [pdf](http://arxiv.org/pdf/2208.13244v1)

> Program slicing reduces a program to a smaller version that retains a chosen computation, referred to as a slicing criterion. One recent multi-lingual slicing approach, observation-based slicing (ORBS), speculatively deletes parts of the program and then executes the code. If the behavior of the slicing criteria is unchanged, the speculative deletion is made permanent.   While this makes ORBS language agnostic, it can lead to the production of some non-intuitive slices. One particular challenge is when the execution environment plays a role. For example, ORBS will delete the line "a = 0" if the memory location assigned to a contains zero before executing the statement, since deletion will not affect the value of a and thus the slicing criterion. Consequently, slices can differ between execution environments due to factors such as initialization and call stack reuse.   The technique considered, nVORBS, attempts to ameliorate this problem by validating a candidate slice in n different execution environments. We conduct an empirical study to collect initial insights into how often the execution environment leads to slice differences. Specifically, we compare and contrast the slices produced by seven different instantiations of nVORBS. Looking forward, the technique can be seen as a variation on metamorphic testing, and thus suggests how ideas from metamorphic testing might be used to improve dynamic program analysis.

</details>

<details>

<summary>2022-08-28 16:55:25 - FFCNN: Fast FPGA based Acceleration for Convolution neural network inference</summary>

- *F. Keddous, H-N. Nguyen, A. Nakib*

- `2208.13250v1` - [abs](http://arxiv.org/abs/2208.13250v1) - [pdf](http://arxiv.org/pdf/2208.13250v1)

> We present a new efficient OpenCL-based Accelerator for large scale Convolutional Neural Networks called Fast Inference on FPGAs for Convolution Neural Network (FFCNN). FFCNN is based on a deeply pipelined OpenCL kernels architecture. As pointed out before, high-level synthesis tools such as the OpenCL framework can easily port codes originally designed for CPUs and GPUs to FPGAs, but it is still difficult to make OpenCL codes run efficiently on FPGAs. This work aims to propose an efficient FPGA implementation of OpenCL High-Performance Computing Applications. To do so, a Data reuse and task mapping techniques are also presented to improve design efficiency. In addition, the following motivations were taken into account when developing FFCNN: 1) FFCNN has been designed to be easily implemented on Intel OpenCL SDK based FPGA design flow. 2) In FFFCN, different techniques have been integrated to improve the memory band with and throughput. A performance analysis is conducted on two deep CNN for Large-Scale Images classification. The obtained results, and the comparison with other works designed to accelerate the same types of architectures, show the efficiency and the competitiveness of the proposed accelerator design by significantly improved performance and resource utilization.

</details>

<details>

<summary>2022-08-29 00:06:25 - Fluorescence molecular optomic signatures improve identification of tumors in head and neck specimens</summary>

- *Yao Chen, Samuel S. Streeter, Brady Hunt, Hira S. Sardar, Jason R. Gunn, Laura J. Tafe, Joseph A. Paydarfar, Brian W. Pogue, Keith D. Paulsen, Kimberley S. Samkoe*

- `2208.13314v1` - [abs](http://arxiv.org/abs/2208.13314v1) - [pdf](http://arxiv.org/pdf/2208.13314v1)

> In this study, a radiomics approach was extended to optical fluorescence molecular imaging data for tissue classification, termed 'optomics'. Fluorescence molecular imaging is emerging for precise surgical guidance during head and neck squamous cell carcinoma (HNSCC) resection. However, the tumor-to-normal tissue contrast is confounded by intrinsic physiological limitations of heterogeneous expression of the target molecule, epidermal growth factor receptor (EGFR). Optomics seek to improve tumor identification by probing textural pattern differences in EGFR expression conveyed by fluorescence. A total of 1,472 standardized optomic features were extracted from fluorescence image samples. A supervised machine learning pipeline involving a support vector machine classifier was trained with 25 top-ranked features selected by minimum redundancy maximum relevance criterion. Model predictive performance was compared to fluorescence intensity thresholding method by classifying testing set image patches of resected tissue with histologically confirmed malignancy status. The optomics approach provided consistent improvement in prediction accuracy on all test set samples, irrespective of dose, compared to fluorescence intensity thresholding method (mean accuracies of 89% vs. 81%; P = 0.0072). The improved performance demonstrates that extending the radiomics approach to fluorescence molecular imaging data offers a promising image analysis technique for cancer detection in fluorescence-guided surgery.

</details>

<details>

<summary>2022-08-29 01:38:07 - Effective approaches to disaster evacuation during a COVID-like pandemic</summary>

- *Yi-Lin Tsai, Dymasius Y. Sitepu, Karyn E. Chappell, Rishi P. Mediratta, C. Jason Wang, Peter K. Kitanidis, Christopher B. Field*

- `2208.13326v1` - [abs](http://arxiv.org/abs/2208.13326v1) - [pdf](http://arxiv.org/pdf/2208.13326v1)

> Since COVID-19 vaccines became available, no studies have quantified how different disaster evacuation strategies can mitigate pandemic risks in shelters. Therefore, we applied an age-structured epidemiological model, known as the Susceptible-Exposed-Infectious-Recovered (SEIR) model, to investigate to what extent different vaccine uptake levels and the Diversion protocol implemented in Taiwan decrease infections and delay pandemic peak occurrences. Taiwan's Diversion protocol involves diverting those in self-quarantine due to exposure, thus preventing them from mingling with the general public at a congregate shelter. The Diversion protocol, combined with sufficient vaccine uptake, can decrease the maximum number of infections and delay outbreaks relative to scenarios without such strategies. When the diversion of all exposed people is not possible, or vaccine uptake is insufficient, the Diversion protocol is still valuable. Furthermore, a group of evacuees that consists primarily of a young adult population tends to experience pandemic peak occurrences sooner and have up to 180% more infections than does a majority elderly group when the Diversion protocol is implemented. However, when the Diversion protocol is not enforced, the majority elderly group suffers from up to 20% more severe cases than the majority young adult group.

</details>

<details>

<summary>2022-08-29 09:29:37 - Gender-dependent Contribution, Code and Creativity in a Virtual Programming Course</summary>

- *Isabella GraÃl, Gordon Fraser*

- `2208.13447v1` - [abs](http://arxiv.org/abs/2208.13447v1) - [pdf](http://arxiv.org/pdf/2208.13447v1)

> Since computer science is still mainly male dominated, academia, industry and education jointly seek ways to motivate and inspire girls, for example by introducing them to programming at an early age. The recent COVID-19 pandemic has forced many such endeavours to move to an online setting. While the gender-dependent differences in programming courses have been studied previously, for example revealing that girls may feel safer in same-sex groups, much less is known about gender-specific differences in online programming courses. In order to investigate whether gender-specific differences can be observed in online courses, we conducted an online introductory programming course for Scratch, in which we observed the gender-specific characteristics of participants with respect to how they interact, their enjoyment, the code they produce, and the creativity exposed by their programs. Overall, we observed no significant differences between how girls participated in all-female vs. mixed groups, and girls generally engaged with the course more actively than boys. This suggests that online courses can be a useful means to avoid gender-dependent group dynamics. However, when encouraging creative freedom in programming, girls and boys seem to fall back to socially inherited stereotypical behavior also in an online setting, influencing the choice of programming concepts applied. This may inhibit learning and is a challenge that needs to be addressed independently of whether courses are held online.

</details>

<details>

<summary>2022-08-29 09:35:24 - Common Patterns in Block-Based Robot Programs</summary>

- *Florian ObermÃ¼ller, Robert Pernerstorfer, Lisa Bailey, Ute Heuer, Gordon Fraser*

- `2208.13451v1` - [abs](http://arxiv.org/abs/2208.13451v1) - [pdf](http://arxiv.org/pdf/2208.13451v1)

> Programmable robots are engaging and fun to play with, interact with the real world, and are therefore well suited to introduce young learners to programming. Introductory robot programming languages often extend existing block-based languages such as Scratch. While teaching programming with such languages is well established, the interaction with the real world in robot programs leads to specific challenges, for which learners and educators may require assistance and feedback. A practical approach to provide this feedback is by identifying and pointing out patterns in the code that are indicative of good or bad solutions. While such patterns have been defined for regular block-based programs, robot-specific programming aspects have not been considered so far. The aim of this paper is therefore to identify patterns specific to robot programming for the Scratch-based mBlock programming language, which is used for the popular mBot and Codey Rocky robots. We identify: (1) 26 bug patterns, which indicate erroneous code; (2) three code smells, which indicate code that may work but is written in a confusing or difficult to understand way; and (3) 18 code perfumes, which indicate aspects of code that are likely good. We extend the LitterBox analysis framework to automatically identify these patterns in mBlock programs. Evaluated on a dataset of 3,540 mBlock programs, we find a total of 6,129 instances of bug patterns, 592 code smells and 14,495 code perfumes. This demonstrates the potential of our approach to provide feedback and assistance to learners and educators alike for their mBlock robot programs.

</details>

<details>

<summary>2022-08-29 09:51:53 - Common Problems and Effects of Feedback on Fun When Programming Ozobots in Primary School</summary>

- *Luisa Greifenstein, Isabella GraÃl, Ute Heuer, Gordon Fraser*

- `2208.13456v1` - [abs](http://arxiv.org/abs/2208.13456v1) - [pdf](http://arxiv.org/pdf/2208.13456v1)

> Computational thinking is increasingly introduced at primary school level, usually with some form of programming activity. In particular, educational robots provide an opportunity for engaging students with programming through hands-on experiences. However, primary school teachers might not be adequately prepared for teaching computer science related topics, and giving feedback to students can often be challenging: Besides the content of the feedback (e.g., what problems have to be handled), the way the feedback is given is also important, as it can lead to negative emotional effects. To support teachers with the way of giving feedback on common problems when teaching programming with robotics, we conducted a study consisting of seven workshops with three third and four fourth grade primary school classes. Within seven different activities, the 116 primary school children first programmed the Ozobot Evo robot in the pen-and-paper mode and then on a digital device. Throughout these activities we collected data on the problems the students encountered, the feedback given, and the fun they experienced. Our analysis reveals eight categories of problems, which we summarise in this paper together with corresponding possible feedback. We observed that problems that are urgent or can harm the students' self-efficacy have a negative impact on how enjoyable an activity is perceived. While direct instruction significantly decreased the experienced fun, hints had a positive effect. Generally, we found programming the Ozobot Evo to be encouraging for both girls and boys. To support teachers, we discuss ideas for giving encouraging feedback on common problems of Ozobot Evo programming activities and how our findings transfer to other robots.

</details>

<details>

<summary>2022-08-29 12:48:20 - From Fine- to Coarse-Grained Dynamic Information Flow Control and Back, a Tutorial on Dynamic Information Flow</summary>

- *Marco Vassena, Alejandro Russo, Deepak Garg, Vineet Rajani, Deian Stefan*

- `2208.13560v1` - [abs](http://arxiv.org/abs/2208.13560v1) - [pdf](http://arxiv.org/pdf/2208.13560v1)

> This tutorial provides a complete and homogeneous account of the latest advances in fine- and coarse-grained dynamic information-flow control (IFC) security. Since the 70s, the programming language and the operating system communities have proposed different IFC approaches. IFC operating systems track information flows in a coarse-grained fashion, at the granularity of a process. In contrast, traditional language-based approaches to IFC are fine-grained: they track information flows at the granularity of program variables. For decades, researchers believed coarse-grained IFC to be strictly less permissive than fine-grained IFC -- coarse-grained IFC systems seem inherently less precise because they track less information -- and so granularity appeared to be a fundamental feature of IFC systems. We show that the granularity of the tracking system does not fundamentally restrict how precise or permissive dynamic IFC systems can be. To this end, we mechanize two mostly standard languages, one with a fine-grained dynamic IFC system and the other with a coarse-grained dynamic IFC system, and prove a semantics-preserving translation from each language to the other. In addition, we derive the standard security property of non-interference of each language from that of the other via our verified translation. These translations stand to have important implications on the usability of IFC approaches. The coarse- to fine-grained direction can be used to remove the label annotation burden that fine-grained systems impose on developers, while the fine- to coarse-grained translation shows that coarse-grained systems -- which are easier to design and implement -- can track information as precisely as fine-grained systems and provides an algorithm for automatically retrofitting legacy applications to run on existing coarse-grained systems.

</details>

<details>

<summary>2022-08-29 14:27:21 - Neuroevolution-Based Generation of Tests and Oracles for Games</summary>

- *Patric Feldmeier, Gordon Fraser*

- `2208.13632v1` - [abs](http://arxiv.org/abs/2208.13632v1) - [pdf](http://arxiv.org/pdf/2208.13632v1)

> Game-like programs have become increasingly popular in many software engineering domains such as mobile apps, web applications, or programming education. However, creating tests for programs that have the purpose of challenging human players is a daunting task for automatic test generators. Even if test generation succeeds in finding a relevant sequence of events to exercise a program, the randomized nature of games means that it may neither be possible to reproduce the exact program behavior underlying this sequence, nor to create test assertions checking if observed randomized game behavior is correct. To overcome these problems, we propose Neatest, a novel test generator based on the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. Neatest systematically explores a program's statements, and creates neural networks that operate the program in order to reliably reach each statement -- that is, Neatest learns to play the game in a way to reliably cover different parts of the code. As the networks learn the actual game behavior, they can also serve as test oracles by evaluating how surprising the observed behavior of a program under test is compared to a supposedly correct version of the program. We evaluate this approach in the context of Scratch, an educational programming environment. Our empirical study on 25 non-trivial Scratch games demonstrates that our approach can successfully train neural networks that are not only far more resilient to random influences than traditional test suites consisting of static input sequences, but are also highly effective with an average mutation score of more than 65%.

</details>

<details>

<summary>2022-08-29 20:31:25 - Creating Interactive Visualizations of TopHat Programs</summary>

- *Mark Gerarts, Marc de Hoog, Nico Naus, Tim Steenvoorden*

- `2208.13870v1` - [abs](http://arxiv.org/abs/2208.13870v1) - [pdf](http://arxiv.org/pdf/2208.13870v1)

> Many companies and institutions have automated their business process in workflow management software. The novel programming paradigm Task-Oriented Programming (TOP) provides an abstraction for such software. The largest framework based on TOP, iTasks, has been used to develop real-world software.   Workflow software often includes critical systems. In such cases it is important to reason over the software to ascertain its correctness. The lack of a formal iTasks semantics makes it unsuitable for formal reasoning. To this end TopHat has been developed as a TOP language with a formal semantics. However, TopHat lacks a graphical user interface (GUI), making it harder to develop practical TopHat systems.   In this paper we present TopHat UI. By combining an existing server framework and user interface framework, we have developed a fully functioning proof of concept implementation in Haskell, on top of TopHat's semantics. We show that implementing a TOP framework is possible using a different host language than iTasks uses. None of TopHat's formal properties have been compromised, since the UI framework is completely separate from TopHat. We run several example programs and evaluate their generated GUI. Having such a system improves the quality and verifiability of TOP software in general.

</details>

<details>

<summary>2022-08-29 20:50:40 - Inference and Optimization for Engineering and Physical Systems</summary>

- *Mikhail Krechetov*

- `2208.13880v1` - [abs](http://arxiv.org/abs/2208.13880v1) - [pdf](http://arxiv.org/pdf/2208.13880v1)

> The central object of this PhD thesis is known under different names in the fields of computer science and statistical mechanics. In computer science, it is called the Maximum Cut problem, one of the famous twenty-one Karp's original NP-hard problems, while the same object from Physics is called the Ising Spin Glass model. This model of a rich structure often appears as a reduction or reformulation of real-world problems from computer science, physics and engineering. However, solving this model exactly (finding the maximal cut or the ground state) is likely to stay an intractable problem (unless $\textit{P} = \textit{NP}$) and requires the development of ad-hoc heuristics for every particular family of instances.   One of the bright and beautiful connections between discrete and continuous optimization is a Semidefinite Programming-based rounding scheme for Maximum Cut. This procedure allows us to find a provably near-optimal solution; moreover, this method is conjectured to be the best possible in polynomial time. In the first two chapters of this thesis, we investigate local non-convex heuristics intended to improve the rounding scheme.   In the last chapter of this thesis, we make one step further and aim to control the solution of the problem we wanted to solve in previous chapters. We formulate a bi-level optimization problem over the Ising model where we want to tweak the interactions as little as possible so that the ground state of the resulting Ising model satisfies the desired criteria. This kind of problem arises in pandemic modeling. We show that when the interactions are non-negative, our bi-level optimization is solvable in polynomial time using convex programming.

</details>

<details>

<summary>2022-08-30 07:55:53 - A General Purpose Exact Solution Method for Mixed Integer Concave Minimization Problems</summary>

- *Ankur Sinha, Arka Das, Guneshwar Anand, Sachin Jayaswal*

- `2208.09253v2` - [abs](http://arxiv.org/abs/2208.09253v2) - [pdf](http://arxiv.org/pdf/2208.09253v2)

> In this article, we discuss an exact algorithm for solving mixed integer concave minimization problems. A piecewise inner-approximation of the concave function is achieved using an auxiliary linear program that leads to a bilevel program, which provides a lower bound to the original problem. The bilevel program is reduced to a single level formulation with the help of Karush-Kuhn-Tucker (KKT) conditions. Incorporating the KKT conditions lead to complementary slackness conditions that are linearized using BigM, for which we identify a tight value for general problems. Multiple bilevel programs, when solved over iterations, guarantee convergence to the exact optimum of the original problem. Though the algorithm is general and can be applied to any optimization problem with concave function(s), in this paper, we solve two common classes of operations and supply chain problems; namely, the concave knapsack problem, and the concave production-transportation problem. The computational experiments indicate that our proposed approach outperforms the customized methods that have been used in the literature to solve the two classes of problems by an order of magnitude in most of the test cases.

</details>

<details>

<summary>2022-08-30 08:28:49 - Attack detection based on machine learning algorithms for different variants of Spectre attacks and different Meltdown attack implementations</summary>

- *Zhongkai Tong, Ziyuan Zhu, Yusha Zhang, Yuxin Liu, Dan Meng*

- `2208.14062v1` - [abs](http://arxiv.org/abs/2208.14062v1) - [pdf](http://arxiv.org/pdf/2208.14062v1)

> To improve the overall performance of processors, computer architects use various performance optimization techniques in modern processors, such as speculative execution, branch prediction, and chaotic execution. Both now and in the future, these optimization techniques are critical for improving the execution speed of processor instructions. However, researchers have discovered that these techniques introduce hidden inherent security flaws, such as meltdown and ghost attacks in recent years. They exploit techniques such as chaotic execution or speculative execution combined with cache-based side-channel attacks to leak protected data. The impact of these vulnerabilities is enormous because they are prevalent in existing or future processors. However, until today, meltdown and ghost have not been effectively addressed, but instead, multiple attack variants and different attack implementations have evolved from them. This paper proposes to optimize four different hardware performance events through feature selection and use machine learning algorithms to build a real-time detection mechanism for Spectre v1,v2,v4, and different implementations of meltdown attacks, ultimately achieving an accuracy rate of over 99\%. In order to verify the practicality of the attack detection model, this paper is tested with a variety of benign programs and different implementations of Spectre attacks different from the modeling process, and the absolute accuracy also exceeds 99\%, showing that this paper can cope with different attack variants and different implementations of the same attack that may occur daily.

</details>

<details>

<summary>2022-08-30 10:23:23 - Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers</summary>

- *Fangqi Li, Shilin Wang, Yun Zhu*

- `2208.14127v1` - [abs](http://arxiv.org/abs/2208.14127v1) - [pdf](http://arxiv.org/pdf/2208.14127v1)

> Backdoor-based watermarking schemes were proposed to protect the intellectual property of artificial intelligence models, especially deep neural networks, under the black-box setting. Compared with ordinary backdoors, backdoor-based watermarks need to digitally incorporate the owner's identity, which fact adds extra requirements to the trigger generation and verification programs. Moreover, these concerns produce additional security risks after the watermarking scheme has been published for as a forensics tool or the owner's evidence has been eavesdropped on. This paper proposes the capsulation attack, an efficient method that can invalidate most established backdoor-based watermarking schemes without sacrificing the pirated model's functionality. By encapsulating the deep neural network with a rule-based or Bayes filter, an adversary can block ownership probing and reject the ownership verification. We propose a metric, CAScore, to measure a backdoor-based watermarking scheme's security against the capsulation attack. This paper also proposes a new backdoor-based deep neural network watermarking scheme that is secure against the capsulation attack by reversing the encoding process and randomizing the exposure of triggers.

</details>

<details>

<summary>2022-08-30 14:38:27 - Approximation Algorithms for Drone Delivery Packing Problem</summary>

- *Saswata Jana, Partha Sarathi Mandal*

- `2208.14304v1` - [abs](http://arxiv.org/abs/2208.14304v1) - [pdf](http://arxiv.org/pdf/2208.14304v1)

> Recent advancements in unmanned aerial vehicles, also known as drones, have motivated logistics to use drones for multiple operations. Collaboration between drones and trucks in a last-mile delivery system has numerous benefits and reduces a number of challenges. In this paper, we introduce \textit{drone-delivery packing problem} (DDP), where we have a set of deliveries and respective customers with their prescribed locations, delivery time intervals, associated cost for deliveries, and a set of drones with identical battery budgets. The objective of the DDP is to find an assignment for all deliveries to the drones by using the minimum number of drones subject to the battery budget and compatibility of the assignment of each drone. We prove that DDP is NP-Hard and formulate the integer linear programming (ILP) formulation for it. We proposed two greedy approximation algorithms for DDP. The first algorithm uses at most $2OPT + (\Delta + 1)$ drones. The second algorithm uses at most $2OPT + \omega$ drones, where OPT is the optimum solution for DDP, $\omega$ is the maximum clique size, and $\Delta$ is the maximum degree of the interval graph $G$ constructed from the delivery time intervals.

</details>

<details>

<summary>2022-08-30 17:57:14 - Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond</summary>

- *Cheng-Yen Hsieh, Chih-Jung Chang, Fu-En Yang, Yu-Chiang Frank Wang*

- `2208.14439v1` - [abs](http://arxiv.org/abs/2208.14439v1) - [pdf](http://arxiv.org/pdf/2208.14439v1)

> While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with additional learners to observe and relate inherent semantic information within an image. In particular, we present a cross-scale patch-level correlation learning in SS-PRL, which allows the model to aggregate and associate information learned across patch scales. We show that, with our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection, and instance segmentation.

</details>

<details>

<summary>2022-08-30 20:34:53 - $MC^2$: Rigorous and Efficient Directed Greybox Fuzzing</summary>

- *Abhishek Shah, Dongdong She, Samanway Sadhu, Krish Singal, Peter Coffman, Suman Jana*

- `2208.14530v1` - [abs](http://arxiv.org/abs/2208.14530v1) - [pdf](http://arxiv.org/pdf/2208.14530v1)

> Directed greybox fuzzing is a popular technique for targeted software testing that seeks to find inputs that reach a set of target sites in a program. Most existing directed greybox fuzzers do not provide any theoretical analysis of their performance or optimality.   In this paper, we introduce a complexity-theoretic framework to pose directed greybox fuzzing as a oracle-guided search problem where some feedback about the input space (e.g., how close an input is to the target sites) is received by querying an oracle. Our framework assumes that each oracle query can return arbitrary content with a large but constant amount of information. Therefore, we use the number of oracle queries required by a fuzzing algorithm to find a target-reaching input as the performance metric. Using our framework, we design a randomized directed greybox fuzzing algorithm that makes a logarithmic (wrt. the number of all possible inputs) number of queries in expectation to find a target-reaching input. We further prove that the number of oracle queries required by our algorithm is optimal, i.e., no fuzzing algorithm can improve (i.e., minimize) the query count by more than a constant factor.   We implement our approach in MC$^2$ and outperform state-of-the-art directed greybox fuzzers on challenging benchmarks (Magma and Fuzzer Test Suite) by up to two orders of magnitude (i.e., $134\times$) on average. MC$^2$ also found 15 previously undiscovered bugs that other state-of-the-art directed greybox fuzzers failed to find.

</details>

<details>

<summary>2022-08-30 20:45:19 - Modeling Soft-Failure Evolution for Triggering Timely Repair with Low QoT Margins</summary>

- *Sadananda Behera, Tania Panayiotou, Georgios Ellinas*

- `2208.14535v1` - [abs](http://arxiv.org/abs/2208.14535v1) - [pdf](http://arxiv.org/pdf/2208.14535v1)

> In this work, the capabilities of an encoder-decoder learning framework are leveraged to predict soft-failure evolution over a long future horizon. This enables the triggering of timely repair actions with low quality-of-transmission (QoT) margins before a costly hard-failure occurs, ultimately reducing the frequency of repair actions and associated operational expenses. Specifically, it is shown that the proposed scheme is capable of triggering a repair action several days prior to the expected day of a hard-failure, contrary to soft-failure detection schemes utilizing rule-based fixed QoT margins, that may lead either to premature repair actions (i.e., several months before the event of a hard-failure) or to repair actions that are taken too late (i.e., after the hard failure has occurred). Both frameworks are evaluated and compared for a lightpath established in an elastic optical network, where soft-failure evolution can be modeled by analyzing bit-error-rate information monitored at the coherent receivers.

</details>

<details>

<summary>2022-08-30 21:42:24 - Foundations of Reasoning with Uncertainty via Real-valued Logics</summary>

- *Ronald Fagin, Ryan Riegel, Alexander Gray*

- `2008.02429v3` - [abs](http://arxiv.org/abs/2008.02429v3) - [pdf](http://arxiv.org/pdf/2008.02429v3)

> Real-valued logics underlie an increasing number of neuro-symbolic approaches, though typically their logical inference capabilities are characterized only qualitatively. We provide foundations for establishing the correctness and power of such systems. We give a sound and strongly complete axiomatization that can be parametrized to cover essentially every real-valued logic, including all the common fuzzy logics. Our class of sentences are very rich, and each describes a set of possible real values for a collection of formulas of the real-valued logic, including which combinations of real values are possible. Strong completeness allows us to derive exactly what information can be inferred about the combinations of real values of a collection of formulas given information about the combinations of real values of several other collections of formulas. We then extend the axiomatization to deal with weighted subformulas. Finally, we give a decision procedure based on linear programming for deciding, for certain real-valued logics and under certain natural assumptions, whether a set of our sentences logically implies another of our sentences.

</details>

<details>

<summary>2022-08-30 22:08:48 - TMIC: App Inventor Extension for the Deployment of Image Classification Models Exported from Teachable Machine</summary>

- *Fabiano Pereira de Oliveira, Christiane Gresse von Wangenheim, Jean C. R. Hauck*

- `2208.12637v2` - [abs](http://arxiv.org/abs/2208.12637v2) - [pdf](http://arxiv.org/pdf/2208.12637v2)

> TMIC is an App Inventor extension for the deployment of ML models for image classification developed with Google Teachable Machine in educational settings. Google Teachable Machine, is an intuitive visual tool that provides workflow-oriented support for the development of ML models for image classification. Aiming at the usage of models developed with Google Teachable Machine, the extension TMIC enables the deployment of the trained models exported as TensorFlow.js to Google Cloud as part of App Inventor, one of the most popular block-based programming environments for teaching computing in K-12. The extension was created with the App Inventor extension framework based on the extension PIC and is available under the BSD 3 license. It can be used for teaching ML in K-12, in introductory courses in higher education or by anyone interested in creating intelligent apps with image classification. The extension TMIC is being developed by the initiative Computa\c{c}\~ao na Escola of the Department of Informatics and Statistics at the Federal University of Santa Catarina/Brazil as part of a research effort aiming at introducing AI education in K-12.

</details>

<details>

<summary>2022-08-31 05:56:37 - Foundation Posteriors for Approximate Probabilistic Inference</summary>

- *Mike Wu, Noah Goodman*

- `2205.09735v2` - [abs](http://arxiv.org/abs/2205.09735v2) - [pdf](http://arxiv.org/pdf/2205.09735v2)

> Probabilistic programs provide an expressive representation language for generative models. Given a probabilistic program, we are interested in the task of posterior inference: estimating a latent variable given a set of observed variables. Existing techniques for inference in probabilistic programs often require choosing many hyper-parameters, are computationally expensive, and/or only work for restricted classes of programs. Here we formulate inference as masked language modeling: given a program, we generate a supervised dataset of variables and assignments, and randomly mask a subset of the assignments. We then train a neural network to unmask the random values, defining an approximate posterior distribution. By optimizing a single neural network across a range of programs we amortize the cost of training, yielding a "foundation" posterior able to do zero-shot inference for new programs. The foundation posterior can also be fine-tuned for a particular program and dataset by optimizing a variational inference objective. We show the efficacy of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.

</details>

<details>

<summary>2022-08-31 08:11:21 - Mapping aids using source location tracking increase novices' performance in programming cyber-physical systems</summary>

- *Thomas Witte, Andrea Vogt, Tina Seufert, Matthias Tichy*

- `2208.14679v1` - [abs](http://arxiv.org/abs/2208.14679v1) - [pdf](http://arxiv.org/pdf/2208.14679v1)

> Novices need to overcome initial barriers while programming cyber-physical systems behavior, like coding quadcopter missions, and should thus be supported by an adequately designed programming environment. Using multiple representations by including graphical previews is a common approach to ease coding and program understanding. However, novices struggle to map information of the code and graphical previews. Previous studies imply that mapping aids in a live programming environment might support novices while programming and foster a deeper understanding of the content. To implement these mapping aids in a domain independent way Source Location Tracking based on run-time information can be used. In our study, we tested N=82 participants while interacting and learning in an online programming environment. Using our 2x2 between-subject design study, we investigated the effects of two mapping aids: highlighting and dynamic linking on coding correctness including typical errors, and learning outcomes. Based on process data, successful strategies were analyzed. Combining both mapping aids compared to one aid resulted in higher performance. While highlights were more helpful for implementing the quadcopter missions, dynamic linking improved learning outcomes on the comprehension and application level . Traces of learning strategies were related to higher coding correctness and higher learning outcomes. Based on process data, users in the group with both aids had a higher chance of avoiding certain typical implementation mistakes. Implementing dynamic linking and highlighting through source location tracking is a promising approach to support novices to develop a better semantic understanding of the domain specific language. Depending on the coding tasks different mapping aids might be effective.

</details>

<details>

<summary>2022-08-31 11:52:58 - Computing all-vs-all MEMs in run-length encoded collections of HiFi reads</summary>

- *Diego DÃ­az-DomÃ­nguez, Simon J. Puglisi, Leena Salmela*

- `2208.14787v1` - [abs](http://arxiv.org/abs/2208.14787v1) - [pdf](http://arxiv.org/pdf/2208.14787v1)

> We describe an algorithm to find maximal exact matches (MEMs) among HiFi reads with homopolymer errors. The main novelty in our work is that we resort to run-length compression to help deal with errors. Our method receives as input a run-length-encoded string collection containing the HiFi reads along with their reverse complements. Subsequently, it splits the encoding into two arrays, one storing the sequence of symbols for equal-symbol runs and another storing the run lengths. The purpose of the split is to get the BWT of the run symbols and reorder their lengths accordingly. We show that this special BWT, as it encodes the HiFi reads and their reverse complements, supports bi-directional queries for the HiFi reads. Then, we propose a variation of the MEM algorithm of Belazzougui et al. (2013) that exploits the run-length encoding and the implicit bi-directional property of our BWT to compute approximate MEMs. Concretely, if the algorithm finds that two substrings, $a_1 \ldots a_p$ and $b_1 \ldots b_p$, have a MEM, then it reports the MEM only if their corresponding length sequences, $\ell^{a}_1 \ldots \ell^{a}_p$ and $\ell^{b}_1 \ldots \ell^{b}_p$, do not differ beyond an input threshold. We use a simple metric to calculate the similarity of the length sequences that we call the {\em run-length excess}. Our technique facilitates the detection of MEMs with homopolymer errors as it does not require dynamic programming to find approximate matches where the only edits are the lengths of the equal-symbol runs. Finally, we present a method that relies on a geometric data structure to report the text occurrences of the MEMs detected by our algorithm.

</details>

<details>

<summary>2022-08-31 12:29:17 - Predicting Flaky Tests Categories using Few-Shot Learning</summary>

- *Amal Akli, Guillaume Haben, Sarra Habchi, Mike Papadakis, Yves Le Traon*

- `2208.14799v1` - [abs](http://arxiv.org/abs/2208.14799v1) - [pdf](http://arxiv.org/pdf/2208.14799v1)

> Flaky tests are tests that yield different outcomes when run on the same version of a program. This non-deterministic behaviour plagues continuous integration with false signals, wasting developers' time and reducing their trust in test suites. Studies highlighted the importance of keeping tests flakiness-free. Recently, the research community has been pushing forward the detection of flaky tests by suggesting many static and dynamic approaches. While promising, those approaches mainly focus on classifying tests as flaky or not and, even when high performances are reported, it remains challenging to understand the cause of flakiness. This part is crucial for researchers and developers that aim to fix it. To help with the comprehension of a given flaky test, we propose FlakyCat, the first approach for classifying flaky tests based on their root cause category. FlakyCat relies on CodeBERT for code representation and leverages a Siamese network-based Few-Shot learning method to train a multi-class classifier with few data. We train and evaluate FlakyCat on a set of 343 flaky tests collected from open-source Java projects. Our evaluation shows that FlakyCat categorises flaky tests accurately, with a weighted F1 score of 70%. Furthermore, we investigate the performance of our approach for each category, revealing that Async waits, Unordered collections and Time-related flaky tests are accurately classified, while Concurrency-related flaky tests are more challenging to predict. Finally, to facilitate the comprehension of FlakyCat's predictions, we present a new technique for CodeBERT-based model interpretability that highlights code statements influencing the categorization.

</details>

<details>

<summary>2022-08-31 12:40:44 - Learning Automata-Based Complex Event Patterns in Answer Set Programming</summary>

- *Nikos Katzouris, Georgios Paliouras*

- `2208.14820v1` - [abs](http://arxiv.org/abs/2208.14820v1) - [pdf](http://arxiv.org/pdf/2208.14820v1)

> Complex Event Recognition and Forecasting (CER/F) techniques attempt to detect, or even forecast ahead of time, event occurrences in streaming input using predefined event patterns. Such patterns are not always known in advance, or they frequently change over time, making machine learning techniques, capable of extracting such patterns from data, highly desirable in CER/F. Since many CER/F systems use symbolic automata to represent such patterns, we propose a family of such automata where the transition-enabling conditions are defined by Answer Set Programming (ASP) rules, and which, thanks to the strong connections of ASP to symbolic learning, are directly learnable from data. We present such a learning approach in ASP and an incremental version thereof that trades optimality for efficiency and is capable to scale to large datasets. We evaluate our approach on two CER datasets and compare it to state-of-the-art automata learning techniques, demonstrating empirically a superior performance, both in terms of predictive accuracy and scalability.

</details>


## 2022-09

<details>

<summary>2022-09-01 00:04:10 - Prioritization of Metamorphic Relations to reduce the cost of testing</summary>

- *Madhusudan Srinivasan, Upulee Kanewala*

- `2209.00162v1` - [abs](http://arxiv.org/abs/2209.00162v1) - [pdf](http://arxiv.org/pdf/2209.00162v1)

> An oracle is a mechanism to decide whether the outputs of the program for the executed test cases are correct. For machine learning programs, such oracle is not available or too difficult to apply. Metamorphic testing is a testing approach that uses metamorphic relations, which are necessary properties of the software under test to help verify the correctness of a program. Prioritization of metamorphic relations helps to reduce the cost of metamorphic testing [1]. However, prioritizing metamorphic relations based on code coverage is often not effective for prioritizing MRs for machine learning programs, since the decision logic of a machine learning model is learned from training data, and 100% code coverage can be easily achieved with a single test input. To this end, in this work, we propose a cost-effective approach based on diversity in the source and follow-up data set to prioritize metamorphic relations for machine learning programs. We show that the proposed data diversity-based prioritization approach increase the fault detection effectiveness by up to 40% when compared to the code coverage-based approach and reduce the time taken to detect a fault by 29% when compared to random execution of MRs. Overall, our approach leads to saving time and cost during testing.

</details>

<details>

<summary>2022-09-01 01:50:04 - How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot</summary>

- *Naser Al Madi*

- `2208.14613v2` - [abs](http://arxiv.org/abs/2208.14613v2) - [pdf](http://arxiv.org/pdf/2208.14613v2)

> Background: Recent advancements in large language models have motivated the practical use of such models in code generation and program synthesis. However, little is known about the effects of such tools on code readability and visual attention in practice.   Objective: In this paper, we focus on GitHub Copilot to address the issues of readability and visual inspection of model generated code. Readability and low complexity are vital aspects of good source code, and visual inspection of generated code is important in light of automation bias.   Method: Through a human experiment (n=21) we compare model generated code to code written completely by human programmers. We use a combination of static code analysis and human annotators to assess code readability, and we use eye tracking to assess the visual inspection of code.   Results: Our results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers. At the same time, eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generated code.   Conclusion: Our findings highlight that reading code is more important than ever, and programmers should beware of complacency and automation bias with model generated code.

</details>

<details>

<summary>2022-09-01 07:31:19 - MODNet: Multi-offset Point Cloud Denoising Network Customized for Multi-scale Patches</summary>

- *Anyi Huang, Qian Xie, Zhoutao Wang, Dening Lu, Mingqiang Wei, Jun Wang*

- `2208.14160v2` - [abs](http://arxiv.org/abs/2208.14160v2) - [pdf](http://arxiv.org/pdf/2208.14160v2)

> The intricacy of 3D surfaces often results cutting-edge point cloud denoising (PCD) models in surface degradation including remnant noise, wrongly-removed geometric details. Although using multi-scale patches to encode the geometry of a point has become the common wisdom in PCD, we find that simple aggregation of extracted multi-scale features can not adaptively utilize the appropriate scale information according to the geometric information around noisy points. It leads to surface degradation, especially for points close to edges and points on complex curved surfaces. We raise an intriguing question -- if employing multi-scale geometric perception information to guide the network to utilize multi-scale information, can eliminate the severe surface degradation problem? To answer it, we propose a Multi-offset Denoising Network (MODNet) customized for multi-scale patches. First, we extract the low-level feature of three scales patches by patch feature encoders. Second, a multi-scale perception module is designed to embed multi-scale geometric information for each scale feature and regress multi-scale weights to guide a multi-offset denoising displacement. Third, a multi-offset decoder regresses three scale offsets, which are guided by the multi-scale weights to predict the final displacement by weighting them adaptively. Experiments demonstrate that our method achieves new state-of-the-art performance on both synthetic and real-scanned datasets.

</details>

<details>

<summary>2022-09-01 08:37:38 - Is this Change the Answer to that Problem? Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness</summary>

- *Haoye Tian, Xunzhu Tang, Andrew Habib, Shangwen Wang, Kui Liu, Xin Xia, Jacques Klein, TegawendÃ© F. BissyandÃ©*

- `2208.04125v2` - [abs](http://arxiv.org/abs/2208.04125v2) - [pdf](http://arxiv.org/pdf/2208.04125v2)

> In this work, we propose a novel perspective to the problem of patch correctness assessment: a correct patch implements changes that "answer" to a problem posed by buggy behaviour. Concretely, we turn the patch correctness assessment into a Question Answering problem. To tackle this problem, our intuition is that natural language processing can provide the necessary representations and models for assessing the semantic correlation between a bug (question) and a patch (answer). Specifically, we consider as inputs the bug reports as well as the natural language description of the generated patches. Our approach, Quatrain, first considers state of the art commit message generation models to produce the relevant inputs associated to each generated patch. Then we leverage a neural network architecture to learn the semantic correlation between bug reports and commit messages. Experiments on a large dataset of 9135 patches generated for three bug datasets (Defects4j, Bugs.jar and Bears) show that Quatrain can achieve an AUC of 0.886 on predicting patch correctness, and recalling 93% correct patches while filtering out 62% incorrect patches. Our experimental results further demonstrate the influence of inputs quality on prediction performance. We further perform experiments to highlight that the model indeed learns the relationship between bug reports and code change descriptions for the prediction. Finally, we compare against prior work and discuss the benefits of our approach.

</details>

<details>

<summary>2022-09-01 12:02:09 - Safe reinforcement learning for multi-energy management systems with known constraint functions</summary>

- *Glenn Ceusters, Luis Ramirez Camargo, RÃ¼diger Franke, Ann NowÃ©, Maarten Messagie*

- `2207.03830v4` - [abs](http://arxiv.org/abs/2207.03830v4) - [pdf](http://arxiv.org/pdf/2207.03830v4)

> Reinforcement learning (RL) is a promising optimal control technique for multi-energy management systems. It does not require a model a priori - reducing the upfront and ongoing project-specific engineering effort and is capable of learning better representations of the underlying system dynamics. However, vanilla RL does not provide constraint satisfaction guarantees - resulting in various potentially unsafe interactions within its safety-critical environment. In this paper, we present two novel safe RL methods, namely SafeFallback and GiveSafe, where the safety constraint formulation is decoupled from the RL formulation. These provide hard-constraint, rather than soft- and chance-constraint, satisfaction guarantees both during training a (near) optimal policy (which involves exploratory and exploitative, i.e. greedy, steps) as well as during deployment of any policy (e.g. random agents or offline trained RL agents). This without the need of solving a mathematical program, resulting in less computational power requirements and a more flexible constraint function formulation (no derivative information is required). In a simulated multi-energy systems case study we have shown that both methods start with a significantly higher utility (i.e. useful policy) compared to a vanilla RL benchmark and Optlayer benchmark (94,6% and 82,8% compared to 35,5% and 77,8%) and that the proposed SafeFallback method even can outperform the vanilla RL benchmark (102,9% to 100%). We conclude that both methods are viably safety constraint handling techniques applicable beyond RL, as demonstrated with random policies while still providing hard-constraint guarantees.

</details>

<details>

<summary>2022-09-01 12:12:50 - Has My Release Disobeyed Semantic Versioning? Static Detection Based on Semantic Differencing</summary>

- *Lyuye Zhang, Chengwei Liu, Zhengzi Xu, Sen Chen, Lingling Fan, Bihuan Chen, Yang Liu*

- `2209.00393v1` - [abs](http://arxiv.org/abs/2209.00393v1) - [pdf](http://arxiv.org/pdf/2209.00393v1)

> To enhance the compatibility in the version control of Java Third-party Libraries (TPLs), Maven adopts Semantic Versioning (SemVer) to standardize the underlying meaning of versions, but users could still confront abnormal execution and crash after upgrades even if compilation and linkage succeed. It is caused by semantic breaking (SemB) issues, such that APIs directly used by users have identical signatures but inconsistent semantics across upgrades. To strengthen compliance with SemVer rules, developers and users should be alerted of such issues. Unfortunately, it is challenging to detect them statically, because semantic changes in the internal methods of APIs are difficult to capture. Dynamic testing can confirmingly uncover some, but it is limited by inadequate coverage.   To detect SemB issues over compatible upgrades (Patch and Minor) by SemVer rules, we conduct an empirical study on 180 SemB issues to understand the root causes, inspired by which, we propose Sembid (Semantic Breaking Issue Detector) to statically detect such issues of TPLs for developers and users. Since APIs are directly used by users, Sembid detects and reports SemB issues based on APIs. For a pair of APIs, Sembid walks through the call chains originating from the API to locate breaking changes by measuring semantic diff. Then, Sembid checks if the breaking changes can affect API's output along call chains. The evaluation showed Sembid achieved 90.26% recall and 81.29% precision and outperformed other API checkers on SemB API detection. We also revealed Sembid detected over 3 times more SemB APIs with better coverage than unit tests, the commonly used solution. Furthermore, we carried out an empirical study on 1,629,589 APIs from 546 version pairs of top Java libraries and found there were 2-4 times more SemB APIs than those with signature-based issues.

</details>

<details>

<summary>2022-09-01 14:45:45 - Generalizability of Code Clone Detection on CodeBERT</summary>

- *Tim Sonnekalb, Bernd Gruner, Clemens-Alexander Brust, Patrick MÃ¤der*

- `2208.12588v2` - [abs](http://arxiv.org/abs/2208.12588v2) - [pdf](http://arxiv.org/pdf/2208.12588v2)

> Transformer networks such as CodeBERT already achieve outstanding results for code clone detection in benchmark datasets, so one could assume that this task has already been solved. However, code clone detection is not a trivial task. Semantic code clones, in particular, are challenging to detect. We show that the generalizability of CodeBERT decreases by evaluating two different subsets of Java code clones from BigCloneBench. We observe a significant drop in F1 score when we evaluate different code snippets and functionality IDs than those used for model building.

</details>

<details>

<summary>2022-09-01 15:31:55 - Predictive Semantics for Past-CTL Runtime Monitors</summary>

- *Giorgio Audrito, Volker Stolz, Gianluca Torta*

- `2209.00538v1` - [abs](http://arxiv.org/abs/2209.00538v1) - [pdf](http://arxiv.org/pdf/2209.00538v1)

> The distributed monitoring of swarms of devices cooperating to common global goals is becoming increasingly important, as such systems are employed for critical applications, e.g., in search and rescue missions during emergencies. In this paper, we target the distributed run-time verification of global properties of a swarm expressed as logical formulas in a temporal logic. In particular, for the implementation of decentralized monitors, we adopt the Field Calculus (FC) language, and exploit the results of previous works which have shown the possibility of automatically translating temporal logic formulas into FC programs. The main limitation of such works lies in the fact that the formulas are expressed in the past-CTL logic, which only features past modalities, and is therefore ineffective in predicting properties about the future evolution of a system. In this paper, we inject some limited prediction capability into the past-CTL logic by providing an extended semantics on a multi-valued logic, then assessing how this affects the automated translation into field calculus monitors.

</details>

<details>

<summary>2022-09-01 17:44:11 - EvolvingBehavior: Towards Co-Creative Evolution of Behavior Trees for Game NPCs</summary>

- *Nathan Partlan, Luis Soto, Jim Howe, Sarthak Shrivastava, Magy Seif El-Nasr, Stacy Marsella*

- `2209.01020v1` - [abs](http://arxiv.org/abs/2209.01020v1) - [pdf](http://arxiv.org/pdf/2209.01020v1)

> To assist game developers in crafting game NPCs, we present EvolvingBehavior, a novel tool for genetic programming to evolve behavior trees in Unreal Engine 4. In an initial evaluation, we compare evolved behavior to hand-crafted trees designed by our researchers, and to randomly-grown trees, in a 3D survival game. We find that EvolvingBehavior is capable of producing behavior approaching the designer's goals in this context. Finally, we discuss implications and future avenues of exploration for co-creative game AI design tools, as well as challenges and difficulties in behavior tree evolution.

</details>

<details>

<summary>2022-09-01 19:01:05 - Approximation Algorithms for Continuous Clustering and Facility Location Problems</summary>

- *Deeparnab Chakrabarty, Maryam Negahbani, Ankita Sarkar*

- `2206.15105v3` - [abs](http://arxiv.org/abs/2206.15105v3) - [pdf](http://arxiv.org/pdf/2206.15105v3)

> We consider the approximability of center-based clustering problems where the points to be clustered lie in a metric space, and no candidate centers are specified. We call such problems "continuous", to distinguish from "discrete" clustering where candidate centers are specified. For many objectives, one can reduce the continuous case to the discrete case, and use an $\alpha$-approximation algorithm for the discrete case to get a $\beta\alpha$-approximation for the continuous case, where $\beta$ depends on the objective: e.g. for $k$-median, $\beta = 2$, and for $k$-means, $\beta = 4$. Our motivating question is whether this gap of $\beta$ is inherent, or are there better algorithms for continuous clustering than simply reducing to the discrete case? In a recent SODA 2021 paper, Cohen-Addad, Karthik, and Lee prove a factor-$2$ and a factor-$4$ hardness, respectively, for continuous $k$-median and $k$-means, even when the number of centers $k$ is a constant. The discrete case for a constant $k$ is exactly solvable in polytime, so the $\beta$ loss seems unavoidable in some regimes.   In this paper, we approach continuous clustering via the round-or-cut framework. For four continuous clustering problems, we outperform the reduction to the discrete case. Notably, for the problem $\lambda$-UFL, where $\beta = 2$ and the discrete case has a hardness of $1.27$, we obtain an approximation ratio of $2.32 < 2 \times 1.27$ for the continuous case. Also, for continuous $k$-means, where the best known approximation ratio for the discrete case is $9$, we obtain an approximation ratio of $32 < 4 \times 9$. The key challenge is that most algorithms for discrete clustering, including the state of the art, depend on linear programs that become infinite-sized in the continuous case. To overcome this, we design new linear programs for the continuous case which are amenable to the round-or-cut framework.

</details>

<details>

<summary>2022-09-01 22:42:39 - Examining average and discounted reward optimality criteria in reinforcement learning</summary>

- *Vektor Dewanto, Marcus Gallagher*

- `2107.01348v2` - [abs](http://arxiv.org/abs/2107.01348v2) - [pdf](http://arxiv.org/pdf/2107.01348v2)

> In reinforcement learning (RL), the goal is to obtain an optimal policy, for which the optimality criterion is fundamentally important. Two major optimality criteria are average and discounted rewards. While the latter is more popular, it is problematic to apply in environments without an inherent notion of discounting. This motivates us to revisit a) the progression of optimality criteria in dynamic programming, b) justification for and complication of an artificial discount factor, and c) benefits of directly maximizing the average reward criterion, which is discounting-free. Our contributions include a thorough examination of the relationship between average and discounted rewards, as well as a discussion of their pros and cons in RL. We emphasize that average-reward RL methods possess the ingredient and mechanism for applying a family of discounting-free optimality criteria (Veinott, 1969) to RL.

</details>

<details>

<summary>2022-09-02 04:39:32 - Improving debris flow evacuation alerts in Taiwan using machine learning</summary>

- *Yi-Lin Tsai, Jeremy Irvin, Suhas Chundi, Andrew Y. Ng, Christopher B. Field, Peter K. Kitanidis*

- `2208.13027v2` - [abs](http://arxiv.org/abs/2208.13027v2) - [pdf](http://arxiv.org/pdf/2208.13027v2)

> Taiwan has the highest susceptibility to and fatalities from debris flows worldwide. The existing debris flow warning system in Taiwan, which uses a time-weighted measure of rainfall, leads to alerts when the measure exceeds a predefined threshold. However, this system generates many false alarms and misses a substantial fraction of the actual debris flows. Towards improving this system, we implemented five machine learning models that input historical rainfall data and predict whether a debris flow will occur within a selected time. We found that a random forest model performed the best among the five models and outperformed the existing system in Taiwan. Furthermore, we identified the rainfall trajectories strongly related to debris flow occurrences and explored trade-offs between the risks of missing debris flows versus frequent false alerts. These results suggest the potential for machine learning models trained on hourly rainfall data alone to save lives while reducing false alerts.

</details>

<details>

<summary>2022-09-02 07:01:16 - Developer Discussion Topics on the Adoption and Barriers of Low Code Software Development Platforms</summary>

- *Md Abdullah Al Alamin, Gias Uddin, Sanjay Malakar, Sadia Afroz, Tameem Bin Haider, Anindya Iqbal*

- `2209.00844v1` - [abs](http://arxiv.org/abs/2209.00844v1) - [pdf](http://arxiv.org/pdf/2209.00844v1)

> Low-code software development (LCSD) is an emerging approach to democratize application development for software practitioners from diverse backgrounds. LCSD platforms promote rapid application development with a drag-and-drop interface and minimal programming by hand. As it is a relatively new paradigm, it is vital to study developers' difficulties when adopting LCSD platforms. Software engineers frequently use the online developer forum Stack Overflow (SO) to seek assistance with technical issues. We observe a growing body of LCSD-related posts in SO. This paper presents an empirical study of around 33K SO posts containing discussions of 38 popular LCSD platforms. We use Topic Modeling to determine the topics discussed in those posts. Additionally, we examine how these topics are spread across the various phases of the agile software development life cycle (SDLC) and which part of LCSD is the most popular and challenging. Our study offers several interesting findings. First, we find 40 LCSD topics that we group into five categories: Application Customization, Database, and File Management, Platform Adoption, Platform Maintenance, and Third-party API Integration. Second, while the Application Customization (30\%) and Data Storage (25\%) \rev{topic} categories are the most common, inquiries relating to several other categories (e.g., the Platform Adoption \rev{topic} category) have gained considerable attention in recent years. Third, all topic categories are evolving rapidly, especially during the Covid-19 pandemic. The findings of this study have implications for all three LCSD stakeholders: LCSD platform vendors, LCSD developers/practitioners, Researchers, and Educators. Researchers and LCSD platform vendors can collaborate to improve different aspects of LCSD, such as better tutorial-based documentation, testing, and DevOps support.

</details>

<details>

<summary>2022-09-02 08:16:15 - Unsupervised Joint Image Transfer and Uncertainty Quantification Using Patch Invariant Networks</summary>

- *Christoph Angermann, Markus Haltmeier, Ahsan Raza Siyal*

- `2207.04325v2` - [abs](http://arxiv.org/abs/2207.04325v2) - [pdf](http://arxiv.org/pdf/2207.04325v2)

> Unsupervised image transfer enables intra- and inter-modality image translation in applications where a large amount of paired training data is not abundant. To ensure a structure-preserving mapping from the input to the target domain, existing methods for unpaired image transfer are commonly based on cycle-consistency, causing additional computational resources and instability due to the learning of an inverse mapping. This paper presents a novel method for uni-directional domain mapping that does not rely on any paired training data. A proper transfer is achieved by using a GAN architecture and a novel generator loss based on patch invariance. To be more specific, the generator outputs are evaluated and compared at different scales, also leading to an increased focus on high-frequency details as well as an implicit data augmentation. This novel patch loss also offers the possibility to accurately predict aleatoric uncertainty by modeling an input-dependent scale map for the patch residuals. The proposed method is comprehensively evaluated on three well-established medical databases. As compared to four state-of-the-art methods, we observe significantly higher accuracy on these datasets, indicating great potential of the proposed method for unpaired image transfer with uncertainty taken into account. Implementation of the proposed framework is released here: \url{https://github.com/anger-man/unsupervised-image-transfer-and-uq}.

</details>

<details>

<summary>2022-09-02 14:57:44 - How Developers Extract Functions: An Experiment</summary>

- *Alexey Braver, Dror G. Feitelson*

- `2209.01098v1` - [abs](http://arxiv.org/abs/2209.01098v1) - [pdf](http://arxiv.org/pdf/2209.01098v1)

> Creating functions is at the center of writing computer programs. But there has been little empirical research on how this is done and what are the considerations that developers use. We design an experiment in which we can compare the decisions made by multiple developers under exactly the same conditions. The experiment is based on taking existing production code, "flattening" it into a single monolithic function, and then charging developers with the task of refactoring it to achieve a better design by extracting functions. The results indicate that developers tend to extract functions based on structural cues, such as 'if' or 'try' blocks. And while there are significant correlations between the refactorings performed by different developers, there are also significant differences in the magnitude of refactoring done. For example, the number of functions that were extracted was between 3 and 10, and the amount of code extracted into functions ranged from 37% to 95%.

</details>

<details>

<summary>2022-09-02 17:20:59 - Automatic Detection of Speculative Execution Combinations</summary>

- *Xaver Fabian, Marco Guarnieri, Marco Patrignani*

- `2209.01179v1` - [abs](http://arxiv.org/abs/2209.01179v1) - [pdf](http://arxiv.org/pdf/2209.01179v1)

> Modern processors employ different prediction mechanisms to speculate over different kinds of instructions. Attackers can exploit these prediction mechanisms simultaneously in order to trigger leaks about speculatively-accessed data. Thus, sound reasoning about such speculative leaks requires accounting for all potential mechanisms of speculation. Unfortunately, existing formal models only support reasoning about fixed, hard-coded mechanisms of speculation, with no simple support to extend said reasoning to new mechanisms.   In this paper we develop a framework for reasoning about composed speculative semantics that capture speculation due to different mechanisms and implement it as part of the Spectector verification tool. We implement novel semantics for speculating over store and return instructions and combine them with the semantics for speculating over branches. Our framework yields speculative semantics for speculating over any combination of those instructions that are secure by construction, i.e., we obtain these security guarantees for free. The implementation of our novel semantics in Spectector let us verify existing codebases that are vulnerable to Spectre v1, Spectre v4, and Spectre v5 vulnerabilities as well as new snippets that are only vulnerable to their compositions.

</details>

<details>

<summary>2022-09-02 19:15:48 - FuzzerAid: Grouping Fuzzed Crashes Based On Fault Signatures</summary>

- *Ashwin Kallingal Joshy, Wei Le*

- `2209.01244v1` - [abs](http://arxiv.org/abs/2209.01244v1) - [pdf](http://arxiv.org/pdf/2209.01244v1)

> Fuzzing has been an important approach for finding bugs and vulnerabilities in programs. Many fuzzers deployed in industry run daily and can generate an overwhelming number of crashes. Diagnosing such crashes can be very challenging and time-consuming. Existing fuzzers typically employ heuristics such as code coverage or call stack hashes to weed out duplicate reporting of bugs. While these heuristics are cheap, they are often imprecise and end up still reporting many "unique" crashes corresponding to the same bug. In this paper, we present FuzzerAid that uses fault signatures to group crashes reported by the fuzzers. Fault signature is a small executable program and consists of a selection of necessary statements from the original program that can reproduce a bug. In our approach, we first generate a fault signature using a given crash. We then execute the fault signature with other crash inducing inputs. If the failure is reproduced, we classify the crashes into the group labeled with the fault signature; if not, we generate a new fault signature. After all the crash inducing inputs are classified, we further merge the fault signatures of the same root cause into a group. We implemented our approach in a tool called FuzzerAid and evaluated it on 3020 crashes generated from 15 real-world bugs and 4 large open source projects. Our evaluation shows that we are able to correctly group 99.1% of the crashes and reported only 17 (+2) "unique" bugs, outperforming the state-of-the-art fuzzers.

</details>

<details>

<summary>2022-09-02 20:45:28 - On the arboreal jump number of a poset</summary>

- *Evellyn S. Cavalcante, SebastiÃ¡n Urrutia, Vinicius F. dos Santos*

- `2209.01270v1` - [abs](http://arxiv.org/abs/2209.01270v1) - [pdf](http://arxiv.org/pdf/2209.01270v1)

> A jump is a pair of consecutive elements in an extension of a poset which are incomparable in the original poset. The arboreal jump number is an NP-hard problem that aims to find an arboreal extension of a given poset with minimum number of jumps. The contribution of this paper is twofold: (i)~a characterization that reveals a relation between the number of jumps of an arboreal order extension and the size of a partition of its elements that satisfy some structural properties of the covering graph; (ii)~a compact integer programming model and a heuristic to solve the arboreal jump number problem along with computational results comparing both strategies. The exact method provides an optimality certificate for 18 out of 41 instances with execution time limited to two hours. Furthermore, our heuristic was able to find good feasible solutions for all instances in less than three minutes.

</details>

<details>

<summary>2022-09-03 06:35:49 - Software Engineering Practices for Machine Learning</summary>

- *Peter Kriens, Tim Verbelen*

- `1906.10366v2` - [abs](http://arxiv.org/abs/1906.10366v2) - [pdf](http://arxiv.org/pdf/1906.10366v2)

> In the last couple of years we have witnessed an enormous increase of machine learning (ML) applications. More and more program functions are no longer written in code, but learnt from a huge amount of data samples using an ML algorithm. However, what is often overlooked is the complexity of managing the resulting ML models as well as bringing these into a real production system. In software engineering, we have spent decades on developing tools and methodologies to create, manage and assemble complex software modules. We present an overview of current techniques to manage complex software, and how this applies to ML models.

</details>

<details>

<summary>2022-09-03 11:52:21 - SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics</summary>

- *He Ye, Matias Martinez, Xiapu Luo, Tao Zhang, Martin Monperrus*

- `2203.12755v3` - [abs](http://arxiv.org/abs/2203.12755v3) - [pdf](http://arxiv.org/pdf/2203.12755v3)

> Learning-based program repair has achieved good results in a recent series of papers. Yet, we observe that the related work fails to repair some bugs because of a lack of knowledge about 1) the application domain of the program being repaired, and 2) the fault type being repaired. In this paper, we solve both problems by changing the learning paradigm from supervised training to self-supervised training in an approach called SelfAPR. First, SelfAPR generates training samples on disk by perturbing a previous version of the program being repaired, enforcing the neural model to capture projectspecific knowledge. This is different from the previous work based on mined past commits. Second, SelfAPR executes all training samples and extracts and encodes test execution diagnostics into the input representation, steering the neural model to fix the kind of fault. This is different from the existing studies that only consider static source code as input. We implement SelfAPR and evaluate it in a systematic manner. We generate 1 039 873 training samples obtained by perturbing 17 open-source projects. We evaluate SelfAPR on 818 bugs from Defects4J, SelfAPR correctly repairs 110 of them, outperforming all the supervised learning repair approaches.

</details>

<details>

<summary>2022-09-03 14:11:33 - BEiT: BERT Pre-Training of Image Transformers</summary>

- *Hangbo Bao, Li Dong, Songhao Piao, Furu Wei*

- `2106.08254v2` - [abs](http://arxiv.org/abs/2106.08254v2) - [pdf](http://arxiv.org/pdf/2106.08254v2)

> We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.

</details>

<details>

<summary>2022-09-03 23:08:50 - All You Need Is Logs: Improving Code Completion by Learning from Anonymous IDE Usage Logs</summary>

- *Vitaliy Bibaev, Alexey Kalina, Vadim Lomshakov, Yaroslav Golubev, Alexander Bezzubov, Nikita Povarov, Timofey Bryksin*

- `2205.10692v2` - [abs](http://arxiv.org/abs/2205.10692v2) - [pdf](http://arxiv.org/pdf/2205.10692v2)

> In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832.   The approach adheres to privacy requirements and legal constraints, since it does not require collecting personal information, performing all the necessary anonymization on the client's side. Importantly, it can be improved continuously: implementing new features, collecting new data, and evaluating new models - this way, we have been using it in production since the end of 2020.

</details>

<details>

<summary>2022-09-04 02:58:37 - An Empirical Study of Automation in Software Security Patch Management</summary>

- *Nesara Dissanayake, Asangi Jayatilaka, Mansooreh Zahedi, Muhammad Ali Babar*

- `2209.01518v1` - [abs](http://arxiv.org/abs/2209.01518v1) - [pdf](http://arxiv.org/pdf/2209.01518v1)

> Several studies have shown that automated support for different activities of the security patch management process has great potential for reducing delays in installing security patches. However, it is also important to understand how automation is used in practice, its limitations in meeting real-world needs and what practitioners really need, an area that has not been empirically investigated in the existing software engineering literature. This paper reports an empirical study aimed at investigating different aspects of automation for security patch management using semi-structured interviews with 17 practitioners from three different organisations in the healthcare domain. The findings are focused on the role of automation in security patch management for providing insights into the as-is state of automation in practice, the limitations of current automation, how automation support can be enhanced to effectively meet practitioners' needs, and the role of the human in an automated process. Based on the findings, we have derived a set of recommendations for directing future efforts aimed at developing automated support for security patch management.

</details>

<details>

<summary>2022-09-04 03:04:22 - Why, How and Where of Delays in Software Security Patch Management: An Empirical Investigation in the Healthcare Sector</summary>

- *Nesara Dissanayake, Mansooreh Zahedi, Asangi Jayatilaka, M. Ali Babar*

- `2202.09016v2` - [abs](http://arxiv.org/abs/2202.09016v2) - [pdf](http://arxiv.org/pdf/2202.09016v2)

> Numerous security attacks that resulted in devastating consequences can be traced back to a delay in applying a security patch. Despite the criticality of timely patch application, not much is known about why and how delays occur when applying security patches in practice, and how the delays can be mitigated. Based on longitudinal data collected from 132 delayed patching tasks over a period of four years and observations of patch meetings involving eight teams from two organisations in the healthcare domain, and using quantitative and qualitative data analysis approaches, we identify a set of reasons relating to technology, people and organisation as key explanations that cause delays in patching. Our findings also reveal that the most prominent cause of delays is attributable to coordination delays in the patch management process and a majority of delays occur during the patch deployment phase. Towards mitigating the delays, we describe a set of strategies employed by the studied practitioners. This research serves as the first step towards understanding the practical reasons for delays and possible mitigation strategies in vulnerability patch management. Our findings provide useful insights for practitioners to understand what and where improvement is needed in the patch management process and guide them towards taking timely actions against potential attacks. Also, our findings help researchers to invest effort into designing and developing computer-supported tools to better support a timely security patch management process.

</details>

<details>

<summary>2022-09-04 11:21:28 - Efficient Greybox Fuzzing to Detect Memory Errors</summary>

- *Jinsheng Ba, Gregory J. Duck, Abhik Roychoudhury*

- `2204.02773v2` - [abs](http://arxiv.org/abs/2204.02773v2) - [pdf](http://arxiv.org/pdf/2204.02773v2)

> Greybox fuzzing is a proven and effective testing method for the detection of security vulnerabilities and other bugs in modern software systems. Greybox fuzzing can also be used in combination with a sanitizer, such as AddressSanitizer (ASAN), to further enhance the detection of certain classes of bugs such as buffer overflow and use-after-free errors. However, sanitizers also introduce additional performance overheads, and this can degrade the performance of greybox mode fuzzing -- measured in the order of 2.36X for fuzzing with ASAN -- partially negating the benefit of using a sanitizer in the first place. Recent research attributes the extra overhead to program startup/teardown costs that can dominate fork-mode fuzzing.   In this paper, we present a new memory error sanitizer design that is specifically optimized for fork-mode fuzzing. The basic idea is to mark object boundaries using randomized tokens rather than disjoint metadata (as used by traditional sanitizer designs). All read/write operations are then instrumented to check for the token, and if present, a memory error will be detected. Since our design does not use a disjoint metadata, it is also very lightweight, meaning that program startup and teardown costs are minimized for the benefit of fork-mode fuzzing. We implement our design in the form of the ReZZan tool, and show an improved fuzzing performance overhead of 1.14-1.27X, depending on the configuration.

</details>

<details>

<summary>2022-09-04 14:46:27 - Why Are Some Online Educational Programs Successful? Student Cognition and Success</summary>

- *Marissa Keech, Ashok Goel*

- `2209.05462v1` - [abs](http://arxiv.org/abs/2209.05462v1) - [pdf](http://arxiv.org/pdf/2209.05462v1)

> Massive Open Online Courses (MOOCs) once offered the promise of accessibility and affordability. However, MOOCs typically lack expert feedback and social interaction, and have low student engagement and retention. Thus, alternative programs for online education have emerged including an online graduate program in computer science at a major public university in USA. This program is considered a success with over 9000 students now enrolled in the program. We adopt the perspective of cognitive science to answer the question why do only some online educational courses succeed? We measure learner motivation and self-regulation in one course in the program, specifically a course on artificial intelligence (AI). Surveys of students indicate that students self-reported assessments of self-efficacy, cognitive strategy use, and intrinsic value of the course are not only fairly high, but also generally increase over the course of learning. This data suggests that the online AI course might be a success because the students have high self-efficacy and the class fosters self-regulated learning.

</details>

<details>

<summary>2022-09-05 08:03:20 - An Exploratory Study on the Predominant Programming Paradigms in Python Code</summary>

- *Robert Dyer, Jigyasa Chauhan*

- `2209.01817v1` - [abs](http://arxiv.org/abs/2209.01817v1) - [pdf](http://arxiv.org/pdf/2209.01817v1)

> Python is a multi-paradigm programming language that fully supports object-oriented (OO) programming. The language allows writing code in a non-procedural imperative manner, using procedures, using classes, or in a functional style. To date, no one has studied what paradigm(s), if any, are predominant in Python code and projects. In this work, we first define a technique to classify Python files into predominant paradigm(s). We then automate our approach and evaluate it against human judgements, showing over 80% agreement. We then analyze over 100k open-source Python projects, automatically classifying each source file and investigating the paradigm distributions. The results indicate Python developers tend to heavily favor OO features. We also observed a positive correlation between OO and procedural paradigms and the size of the project. And despite few files or projects being predominantly functional, we still found many functional feature uses.

</details>

<details>

<summary>2022-09-05 09:12:07 - Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?</summary>

- *Sankini Rancha Godage, FrÃ¸y LÃ¸vÃ¥sdal, Sushma Venkatesh, Kiran Raja, Raghavendra Ramachandra, Christoph Busch*

- `2202.12426v4` - [abs](http://arxiv.org/abs/2202.12426v4) - [pdf](http://arxiv.org/pdf/2202.12426v4)

> Few studies have focused on examining how people recognize morphing attacks, even as several publications have examined the susceptibility of automated FRS and offered morphing attack detection (MAD) approaches. MAD approaches base their decisions either on a single image with no reference to compare against (S-MAD) or using a reference image (D-MAD). One prevalent misconception is that an examiner's or observer's capacity for facial morph detection depends on their subject expertise, experience, and familiarity with the issue and that no works have reported the specific results of observers who regularly verify identity (ID) documents for their jobs. As human observers are involved in checking the ID documents having facial images, a lapse in their competence can have significant societal challenges. To assess the observers' proficiency, this work first builds a new benchmark database of realistic morphing attacks from 48 different subjects, resulting in 400 morphed images. We also capture images from Automated Border Control (ABC) gates to mimic the realistic border-crossing scenarios in the D-MAD setting with 400 probe images to study the ability of human observers to detect morphed images. A new dataset of 180 morphing images is also produced to research human capacity in the S-MAD environment. In addition to creating a new evaluation platform to conduct S-MAD and D-MAD analysis, the study employs 469 observers for D-MAD and 410 observers for S-MAD who are primarily governmental employees from more than 40 countries, along with 103 subjects who are not examiners. The analysis offers intriguing insights and highlights the lack of expertise and failure to recognize a sizable number of morphing attacks by experts. The results of this study are intended to aid in the development of training programs to prevent security failures while determining whether an image is bona fide or altered.

</details>

<details>

<summary>2022-09-05 10:23:30 - Performance optimization and analysis of the unstructured Discontinuous Galerkin solver on multi-core and many-core architectures</summary>

- *Zhe Dai, Liang D, Yueqin Wang, Fang Wang, Li Ming, Jian Zhang*

- `2209.01877v1` - [abs](http://arxiv.org/abs/2209.01877v1) - [pdf](http://arxiv.org/pdf/2209.01877v1)

> The discontinuous Galerkin (DG) algorithm is a representative high order method in Computational Fluid Dynamics (CFD) area which possesses considerable mathematical advantages such as high resolution, low dissipation, and dispersion. However, DG is rather computationally intensive to demonstrate practical engineering problems. This paper discusses the implementation of our in-house practical DG application in three different programming models, as well as some optimization techniques, including grid renumbering and mixed precision to maximize the performance improvements in a single node system. The experiment on CPU and GPU shows that our CUDA, OpenACC, and OpenMP-based code obtains a maximum speedup of 42.9x, 35.3x, and 8.1x compared with serial execution by the original application, respectively. Besides, we systematically compare the programming models in two aspects: performance and productivity. Our empirical conclusions facilitate the programmers to select the right platform with a suitable programming model according to their target applications.

</details>

<details>

<summary>2022-09-05 12:05:05 - Newly Developed Flexible Grid Trading Model Combined ANN and SSO algorithm</summary>

- *Wei-Chang Yeh, Yu-Hsin Hsieh, Chia-Ling Huang*

- `2211.12839v1` - [abs](http://arxiv.org/abs/2211.12839v1) - [pdf](http://arxiv.org/pdf/2211.12839v1)

> In modern society, the trading methods and strategies used in financial market have gradually changed from traditional on-site trading to electronic remote trading, and even online automatic trading performed by a pre-programmed computer programs because the continuous development of network and computer computing technology. The quantitative trading, which the main purpose is to automatically formulate people's investment decisions into a fixed and quantifiable operation logic that eliminates all emotional interference and the influence of subjective thoughts and applies this logic to financial market activities in order to obtain excess profits above average returns, has led a lot of attentions in financial market. The development of self-adjustment programming algorithms for automatically trading in financial market has transformed a top priority for academic research and financial practice. Thus, a new flexible grid trading model combined with the Simplified Swarm Optimization (SSO) algorithm for optimizing parameters for various market situations as input values and the fully connected neural network (FNN) and Long Short-Term Memory (LSTM) model for training a quantitative trading model to automatically calculate and adjust the optimal trading parameters for trading after inputting the existing market situation is developed and studied in this work. The proposed model provides a self-adjust model to reduce investors' effort in the trading market, obtains outperformed investment return rate and model robustness, and can properly control the balance between risk and return.

</details>

<details>

<summary>2022-09-05 12:24:13 - HEAT: Hyperedge Attention Networks</summary>

- *Dobrik Georgiev, Marc Brockschmidt, Miltiadis Allamanis*

- `2201.12113v2` - [abs](http://arxiv.org/abs/2201.12113v2) - [pdf](http://arxiv.org/pdf/2201.12113v2)

> Learning from structured data is a core machine learning task. Commonly, such data is represented as graphs, which normally only consider (typed) binary relationships between pairs of nodes. This is a substantial limitation for many domains with highly-structured data. One important such domain is source code, where hypergraph-based representations can better capture the semantically rich and structured nature of code.   In this work, we present HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes contribute. It can be viewed as a generalization of both message passing neural networks and Transformers. We evaluate HEAT on knowledge base completion and on bug detection and repair using a novel hypergraph representation of programs. In both settings, it outperforms strong baselines, indicating its power and generality.

</details>

<details>

<summary>2022-09-05 13:46:14 - Can Language Understand Depth?</summary>

- *Renrui Zhang, Ziyao Zeng, Ziyu Guo, Yafeng Li*

- `2207.01077v3` - [abs](http://arxiv.org/abs/2207.01077v3) - [pdf](http://arxiv.org/pdf/2207.01077v3)

> Besides image classification, Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for a wide range of vision tasks, including object-level and 3D space understanding. However, it's still challenging to transfer semantic knowledge learned from CLIP into more intricate tasks of quantified targets, such as depth estimation with geometric information. In this paper, we propose to apply CLIP for zero-shot monocular depth estimation, named DepthCLIP. We found that the patches of the input image could respond to a certain semantic distance token and then be projected to a quantified depth bin for coarse estimation. Without any training, our DepthCLIP surpasses existing unsupervised methods and even approaches the early fully-supervised networks. To our best knowledge, we are the first to conduct zero-shot adaptation from the semantic language knowledge to quantified downstream tasks and perform zero-shot monocular depth estimation. We hope our work could cast a light on future research. The code is available at https://github.com/Adonis-galaxy/DepthCLIP.

</details>

<details>

<summary>2022-09-05 18:40:14 - PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch</summary>

- *Ke Xu, Yao Xiao, Zhaoheng Zheng, Kaijie Cai, Ram Nevatia*

- `2207.01795v3` - [abs](http://arxiv.org/abs/2207.01795v3) - [pdf](http://arxiv.org/pdf/2207.01795v3)

> Adversarial patch attacks mislead neural networks by injecting adversarial pixels within a local region. Patch attacks can be highly effective in a variety of tasks and physically realizable via attachment (e.g. a sticker) to the real-world objects. Despite the diversity in attack patterns, adversarial patches tend to be highly textured and different in appearance from natural images. We exploit this property and present PatchZero, a general defense pipeline against white-box adversarial patches without retraining the downstream classifier or detector. Specifically, our defense detects adversaries at the pixel-level and "zeros out" the patch region by repainting with mean pixel values. We further design a two-stage adversarial training scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA defense performance on the image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and video classification (UCF101) tasks with little degradation in benign performance. In addition, PatchZero transfers to different patch shapes and attack types.

</details>

<details>

<summary>2022-09-05 19:08:44 - Shape complexity in cluster analysis</summary>

- *Eduardo J. Aguilar, Valmir C. Barbosa*

- `2205.08046v3` - [abs](http://arxiv.org/abs/2205.08046v3) - [pdf](http://arxiv.org/pdf/2205.08046v3)

> In cluster analysis, a common first step is to scale the data aiming to better partition them into clusters. Even though many different techniques have throughout many years been introduced to this end, it is probably fair to say that the workhorse in this preprocessing phase has been to divide the data by the standard deviation along each dimension. Like division by the standard deviation, the great majority of scaling techniques can be said to have roots in some sort of statistical take on the data. Here we explore the use of multidimensional shapes of data, aiming to obtain scaling factors for use prior to clustering by some method, like k-means, that makes explicit use of distances between samples. We borrow from the field of cosmology and related areas the recently introduced notion of shape complexity, which in the variant we use is a relatively simple, data-dependent nonlinear function that we show can be used to help with the determination of appropriate scaling factors. Focusing on what might be called "midrange" distances, we formulate a constrained nonlinear programming problem and use it to produce candidate scaling-factor sets that can be sifted on the basis of further considerations of the data, say via expert knowledge. We give results on some iconic data sets, highlighting the strengths and potential weaknesses of the new approach. These results are generally positive across all the data sets used.

</details>

<details>

<summary>2022-09-05 22:38:53 - Linguistic Knowledge in Data Augmentation for Natural Language Processing: An Example on Chinese Question Matching</summary>

- *Zhengxiang Wang*

- `2111.14709v3` - [abs](http://arxiv.org/abs/2111.14709v3) - [pdf](http://arxiv.org/pdf/2111.14709v3)

> To investigate the role of linguistic knowledge in data augmentation (DA) for Natural Language Processing (NLP), we designed two adapted DA programs and applied them to LCQMC (a Large-scale Chinese Question Matching Corpus) for a binary Chinese question matching classification task. The two DA programs produce augmented texts by five simple text editing operations (or DA techniques), largely irrespective of language generation rules, but one is enhanced with a pre-trained n-gram language model to fuse it with prior linguistic knowledge. We then trained four neural network models (BOW, CNN, LSTM, and GRU) and a pre-trained model (ERNIE-Gram) on the LCQMCs train sets of varying size as well as the related augmented train sets produced by the two DA programs. The results show that there are no significant performance differences between the models trained on the two types of augmented train sets, both when the five DA techniques are applied together or separately. Moreover, due to the inability of the five DA techniques to make strictly paraphrastic augmented texts, the results indicate the need of sufficient amounts of training examples for the classification models trained on them to mediate the negative impact of false matching augmented text pairs and improve performance, a limitation of random text editing perturbations used as a DA approach. Similar results were also obtained for English.

</details>

<details>

<summary>2022-09-06 01:31:22 - Transformer-Based Language Models for Software Vulnerability Detection</summary>

- *Chandra Thapa, Seung Ick Jang, Muhammad Ejaz Ahmed, Seyit Camtepe, Josef Pieprzyk, Surya Nepal*

- `2204.03214v2` - [abs](http://arxiv.org/abs/2204.03214v2) - [pdf](http://arxiv.org/pdf/2204.03214v2)

> The large transformer-based language models demonstrate excellent performance in natural language processing. By considering the transferability of the knowledge gained by these models in one domain to other related domains, and the closeness of natural languages to high-level programming languages, such as C/C++, this work studies how to leverage (large) transformer-based language models in detecting software vulnerabilities and how good are these models for vulnerability detection tasks. In this regard, firstly, a systematic (cohesive) framework that details source code translation, model preparation, and inference is presented. Then, an empirical analysis is performed with software vulnerability datasets with C/C++ source codes having multiple vulnerabilities corresponding to the library function call, pointer usage, array usage, and arithmetic expression. Our empirical results demonstrate the good performance of the language models in vulnerability detection. Moreover, these language models have better performance metrics, such as F1-score, than the contemporary models, namely bidirectional long short-term memory and bidirectional gated recurrent unit. Experimenting with the language models is always challenging due to the requirement of computing resources, platforms, libraries, and dependencies. Thus, this paper also analyses the popular platforms to efficiently fine-tune these models and present recommendations while choosing the platforms.

</details>

<details>

<summary>2022-09-06 05:07:46 - Understanding Skills for OSS Communities on GitHub</summary>

- *Jenny T. Liang, Thomas Zimmermann, Denae Ford*

- `2209.02222v1` - [abs](http://arxiv.org/abs/2209.02222v1) - [pdf](http://arxiv.org/pdf/2209.02222v1)

> The development of open source software (OSS) is a broad field which requires diverse skill sets. For example, maintainers help lead the project and promote its longevity, technical writers assist with documentation, bug reporters identify defects in software, and developers program the software. However, it is unknown which skills are used in OSS development as well as OSS contributors' general attitudes towards skills in OSS. In this paper, we address this gap by administering a survey to a diverse set of 455 OSS contributors. Guided by these responses as well as prior literature on software development expertise and social factors of OSS, we develop a model of skills in OSS that considers the many contexts OSS contributors work in. This model has 45 skills in the following 9 categories: technical skills, working styles, problem solving, contribution types, project-specific skills, interpersonal skills, external relations, management, and characteristics. Through a mix of qualitative and quantitative analyses, we find that OSS contributors are actively motivated to improve skills and perceive many benefits in sharing their skills with others. We then use this analysis to derive a set of design implications and best practices for those who incorporate skills into OSS tools and platforms, such as GitHub.

</details>

<details>

<summary>2022-09-06 06:08:54 - On Collective Robustness of Bagging Against Data Poisoning</summary>

- *Ruoxin Chen, Zenan Li, Jie Li, Chentao Wu, Junchi Yan*

- `2205.13176v2` - [abs](http://arxiv.org/abs/2205.13176v2) - [pdf](http://arxiv.org/pdf/2205.13176v2)

> Bootstrap aggregating (bagging) is an effective ensemble protocol, which is believed can enhance robustness by its majority voting mechanism. Recent works further prove the sample-wise robustness certificates for certain forms of bagging (e.g. partition aggregation). Beyond these particular forms, in this paper, \emph{we propose the first collective certification for general bagging to compute the tight robustness against the global poisoning attack}. Specifically, we compute the maximum number of simultaneously changed predictions via solving a binary integer linear programming (BILP) problem. Then we analyze the robustness of vanilla bagging and give the upper bound of the tolerable poison budget. Based on this analysis, \emph{we propose hash bagging} to improve the robustness of vanilla bagging almost for free. This is achieved by modifying the random subsampling in vanilla bagging to a hash-based deterministic subsampling, as a way of controlling the influence scope for each poisoning sample universally. Our extensive experiments show the notable advantage in terms of applicability and robustness.

</details>

<details>

<summary>2022-09-06 06:10:03 - Automatic Code Documentation Generation Using GPT-3</summary>

- *Junaed Younus Khan, Gias Uddin*

- `2209.02235v1` - [abs](http://arxiv.org/abs/2209.02235v1) - [pdf](http://arxiv.org/pdf/2209.02235v1)

> Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.

</details>

<details>

<summary>2022-09-06 10:02:58 - EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models</summary>

- *Jiangsu Du, Ziming Liu, Jiarui Fang, Shenggui Li, Yongbin Li, Yutong Lu, Yang You*

- `2209.02341v1` - [abs](http://arxiv.org/abs/2209.02341v1) - [pdf](http://arxiv.org/pdf/2209.02341v1)

> Large transformer models display promising performance on a wide range of natural language processing (NLP) tasks. Although the AI community has expanded the model scale to the trillion parameter level, the practical deployment of 10-100 billion parameter models is still uncertain due to the latency, throughput, and memory constraints.   In this paper, we proposed EnergonAI to solve the challenges of the efficient deployment of 10-100 billion parameter transformer models on single- or multi-GPU systems. EnergonAI adopts a hierarchy-controller system architecture to coordinate multiple devices and efficiently support different parallel patterns. It delegates the execution of sub-models to multiple workers in the single-controller style and applies tensor parallelism and pipeline parallelism among the workers in a multi-controller style. Upon the novel architecture, we propose three techniques, i.e. non-blocking pipeline parallelism, distributed redundant computation elimination, and peer memory pooling. EnergonAI enables the users to program complex parallel code the same as a serial one. Compared with the FasterTransformer, we have proven that EnergonAI has superior performance on latency and throughput. In our experiments, EnergonAI can achieve 37% latency reduction in tensor parallelism, 10% scalability improvement in pipeline parallelism, and it improves the model scale inferred on a single GPU by using a larger heterogeneous memory space at cost of limited performance reduction.

</details>

<details>

<summary>2022-09-06 10:37:55 - Bisimulations Respecting Duration and Causality for the Non-interleaving Applied $Ï$-Calculus</summary>

- *ClÃ©ment Aubert, Ross Horne, Christian Johansen*

- `2209.05231v1` - [abs](http://arxiv.org/abs/2209.05231v1) - [pdf](http://arxiv.org/pdf/2209.05231v1)

> This paper shows how we can make use of an asynchronous transition system, whose transitions are labelled with events and which is equipped with a notion of independence of events, to define non-interleaving semantics for the applied $\pi$-calculus. The most important notions we define are: Start-Termination or ST-bisimilarity, preserving duration of events; and History-Preserving or HP- bisimilarity, preserving causality. We point out that corresponding similarity preorders expose clearly distinctions between these semantics. We draw particular attention to the distinguishing power of HP failure similarity, and discuss how it affects the attacker threat model against which we verify security and privacy properties. We also compare existing notions of located bisimilarity to the definitions we introduce.

</details>

<details>

<summary>2022-09-06 10:38:19 - From Legal Contracts to Legal Calculi: the code-driven normativity</summary>

- *Silvia Crafa*

- `2209.02353v1` - [abs](http://arxiv.org/abs/2209.02353v1) - [pdf](http://arxiv.org/pdf/2209.02353v1)

> Using dedicated software to represent or enact legislation or regulation has the advantage of solving the inherent ambiguity of legal texts and enabling the automation of compliance with legal norms. On the other hand, the so-called code-driven normativity is less flexible than the legal provisions it claims to implement, and transforms the nature of legal protection, potentially reducing the capability of individual human beings to invoke legal remedies.   In this article we focus on software-based legal contracts; we illustrate the design of a legal calculus whose primitives allow a direct formalisation of contracts' normative elements (i.e., permissions, prohibitions, obligations, asset transfer, judicial enforcement and openness to the external context). We show that interpreting legal contracts as interaction protocols between (untrusted) parties enables the generalisation of formal methods and tools for concurrent systems to the legal setting

</details>

<details>

<summary>2022-09-06 13:56:02 - Neural Termination Analysis</summary>

- *Mirco Giacobbe, Daniel Kroening, Julian Parsert*

- `2102.03824v4` - [abs](http://arxiv.org/abs/2102.03824v4) - [pdf](http://arxiv.org/pdf/2102.03824v4)

> We introduce a novel approach to the automated termination analysis of computer programs: we use neural networks to represent ranking functions. Ranking functions map program states to values that are bounded from below and decrease as a program runs; the existence of a ranking function proves that the program terminates. We train a neural network from sampled execution traces of a program so that the network's output decreases along the traces; then, we use symbolic reasoning to formally verify that it generalises to all possible executions. Upon the affirmative answer we obtain a formal certificate of termination for the program, which we call a neural ranking function. We demonstrate that thanks to the ability of neural networks to represent nonlinear functions our method succeeds over programs that are beyond the reach of state-of-the-art tools. This includes programs that use disjunctions in their loop conditions and programs that include nonlinear expressions.

</details>

<details>

<summary>2022-09-06 13:57:05 - Least-Squares Linear Dilation-Erosion Regressor Trained using a Convex-Concave Procedure</summary>

- *Angelica LourenÃ§o Oliveira, Marcos Eduardo Valle*

- `2107.05682v2` - [abs](http://arxiv.org/abs/2107.05682v2) - [pdf](http://arxiv.org/pdf/2107.05682v2)

> This paper presents a hybrid morphological neural network for regression tasks called linear dilation-erosion regressor ($\ell$-DER). An $\ell$-DER is given by a convex combination of the composition of linear and morphological operators. They yield continuous piecewise linear functions and, thus, are universal approximators. Besides introducing the $\ell$-DER model, we formulate their training as a difference of convex (DC) programming problem. Precisely, an $\ell$-DER is trained by minimizing the least-squares using the convex-concave procedure (CCP). Computational experiments using several regression tasks confirm the efficacy of the proposed regressor, outperforming other hybrid morphological models and state-of-the-art approaches such as the multilayer perceptron network and the radial-basis support vector regressor.

</details>

<details>

<summary>2022-09-06 15:57:51 - Quantum cryptography with classical communication: parallel remote state preparation for copy-protection, verification, and more</summary>

- *Alexandru Gheorghiu, Tony Metger, Alexander Poremba*

- `2201.13445v2` - [abs](http://arxiv.org/abs/2201.13445v2) - [pdf](http://arxiv.org/pdf/2201.13445v2)

> Quantum mechanical effects have enabled the construction of cryptographic primitives that are impossible classically. For example, quantum copy-protection allows for a program to be encoded in a quantum state in such a way that the program can be evaluated, but not copied. Many of these cryptographic primitives are two-party protocols, where one party, Bob, has full quantum computational capabilities, and the other party, Alice, is only required to send random BB84 states to Bob. In this work, we show how such protocols can generically be converted to ones where Alice is fully classical, assuming that Bob cannot efficiently solve the LWE problem. In particular, this means that all communication between (classical) Alice and (quantum) Bob is classical, yet they can still make use of cryptographic primitives that would be impossible if both parties were classical. We apply this conversion procedure to obtain quantum cryptographic protocols with classical communication for unclonable encryption, copy-protection, computing on encrypted data, and verifiable blind delegated computation. The key technical ingredient for our result is a protocol for classically-instructed parallel remote state preparation of BB84 states. This is a multi-round protocol between (classical) Alice and (quantum polynomial-time) Bob that allows Alice to certify that Bob must have prepared $n$ uniformly random BB84 states (up to a change of basis on his space). Furthermore, Alice knows which specific BB84 states Bob has prepared, while Bob himself does not. Hence, the situation at the end of this protocol is (almost) equivalent to one where Alice sent $n$ random BB84 states to Bob. This allows us to replace the step of preparing and sending BB84 states in existing protocols by our remote-state preparation protocol in a generic and modular way.

</details>

<details>

<summary>2022-09-06 16:48:28 - DFI: An Interprocedural Value-Flow Analysis Framework that Scales to Large Codebases</summary>

- *Min-Yih Hsu, Felicitas Hetzelt, Michael Franz*

- `2209.02638v1` - [abs](http://arxiv.org/abs/2209.02638v1) - [pdf](http://arxiv.org/pdf/2209.02638v1)

> Context- and flow-sensitive value-flow information is an important building block for many static analysis tools. Unfortunately, current approaches to compute value-flows do not scale to large codebases, due to high memory and runtime requirements. This paper proposes a new scalable approach to compute value-flows via graph reachability. To this end, we develop a new graph structure as an extension of LLVM IR that contains two additional operations which significantly simplify the modeling of pointer aliasing. Further, by processing nodes in the opposite direction of SSA def-use chains, we are able to minimize the tree width of the resulting graph. This allows us to employ efficient tree traversal algorithms in order to resolve graph reachability.   We present a value-flow analysis framework,DFI, implementing our approach. We compare DFI against two state-of-the-art value-flow analysis frameworks, Phasar and SVF, to extract value-flows from 4 real-world software projects. Given 32GB of memory, Phasar and SVF are unable to complete analysis of larger projects such as OpenSSL or FFmpeg, while DFI is able to complete all evaluations. For the subset of benchmarks that Phasar and SVF do handle, DFI requires significantly less memory (1.5% of Phasar's, 6.4% of SVF's memory footprint on average) and runs significantly faster (23x speedup over Phasar, 57x compared to SVF). Our analysis shows that, in contrast to previous approaches, DFI's memory and runtime requirements scale almost linearly with the number of analyzed instructions.

</details>

<details>

<summary>2022-09-07 01:37:06 - Fooling MOSS Detection with Pretrained Language Models</summary>

- *Stella Biderman, Edward Raff*

- `2201.07406v2` - [abs](http://arxiv.org/abs/2201.07406v2) - [pdf](http://arxiv.org/pdf/2201.07406v2)

> As artificial intelligence (AI) technologies become increasingly powerful and prominent in society, their misuse is a growing concern. In educational settings, AI technologies could be used by students to cheat on assignments and exams. In this paper we explore whether transformers can be used to solve introductory level programming assignments while bypassing commonly used AI tools to detect similarities between pieces of software. We find that a student using GPT-J [Wang and Komatsuzaki, 2021] can complete introductory level programming assignments without triggering suspicion from MOSS [Aiken, 2000], a widely used software similarity and plagiarism detection tool. This holds despite the fact that GPT-J was not trained on the problems in question and is not provided with any examples to work from. We further find that the code written by GPT-J is diverse in structure, lacking any particular tells that future plagiarism detection techniques may use to try to identify algorithmically generated code. We conclude with a discussion of the ethical and educational implications of large language models and directions for future research.

</details>

<details>

<summary>2022-09-07 11:06:07 - A Review on the Process of Automated Software Testing</summary>

- *Durga Shree N, Sree Dharinya S, Dasari Vijayasree, Nadendla Sai Roopa, Anugu Arun*

- `2209.03069v1` - [abs](http://arxiv.org/abs/2209.03069v1) - [pdf](http://arxiv.org/pdf/2209.03069v1)

> The requirements in automation, digitalization, and fast computations have loaded the IT sector with expectations of highly reliable, efficient, and cost-effective software. Given that the process of testing, verification, and validation of software products consumes 50-75% of the total revenue if the testing process is ineffective, "n" times the expenditure must be invested to mend the havoc caused. A delay in project completion is often attributed to the testing phase because of the numerous cycles of debugging process. The software testing process determines the face of the product released to the user. It sets the standard and reliability of a company's outputs. As the complexity increases, testing gets intense so as to examine all the outliers and various branches of the processing flow. The testing process is automated using software tools to avoid the tedious manual process of test input generation and validation criteria, which certifies the program only to a certain confidence level in the presence of outliers.

</details>

<details>

<summary>2022-09-07 12:27:32 - Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation</summary>

- *Perry Gibson, JosÃ© Cano*

- `2201.05587v2` - [abs](http://arxiv.org/abs/2201.05587v2) - [pdf](http://arxiv.org/pdf/2201.05587v2)

> Auto-scheduling for tensor programs is a process where a search algorithm automatically explores candidate schedules (program transformations) for a given program on a target hardware platform to improve its performance. However this can be a very time consuming process depending on the complexity of the tensor program and the capacity of the target device, with often many thousands of program variants being explored. To address this, in this paper we introduce the idea of transfer-tuning, a novel approach to identify and reuse auto-schedules between tensor programs. We demonstrate this concept using Deep Neural Networks (DNNs), taking sets of auto-schedules from pre-tuned DNNs and using them to reduce the inference time of a new DNN. We compare transfer-tuning against the state-of-the-art Ansor auto-scheduler, defining the maximum possible speedup for a given DNN model as what Ansor achieves using its recommended full tuning time. On a server-class CPU and across 11 widely used DNN models, we observe that transfer-tuning achieves up to $88.41\%$ ($49.13\%$ on average) of this maximum speedup, while Ansor requires $6.5\times$ more search time on average to match it. We also evaluate transfer-tuning on a constrained edge CPU and observe that the differences in search time are exacerbated, with Ansor requiring $10.8\times$ more time on average to match transfer-tuning's speedup, which further demonstrates its value. Our code is available at https://www.github.com/gicLAB/transfer-tuning

</details>

<details>

<summary>2022-09-07 12:48:57 - Open-Ended Evolution for Minecraft Building Generation</summary>

- *Matthew Barthet, Antonios Liapis, Georgios N. Yannakakis*

- `2209.03108v1` - [abs](http://arxiv.org/abs/2209.03108v1) - [pdf](http://arxiv.org/pdf/2209.03108v1)

> This paper proposes a procedural content generator which evolves Minecraft buildings according to an open-ended and intrinsic definition of novelty. To realize this goal we evaluate individuals' novelty in the latent space using a 3D autoencoder, and alternate between phases of exploration and transformation. During exploration the system evolves multiple populations of CPPNs through CPPN-NEAT and constrained novelty search in the latent space (defined by the current autoencoder). We apply a set of repair and constraint functions to ensure candidates adhere to basic structural rules and constraints during evolution. During transformation, we reshape the boundaries of the latent space to identify new interesting areas of the solution space by retraining the autoencoder with novel content. In this study we evaluate five different approaches for training the autoencoder during transformation and its impact on populations' quality and diversity during evolution. Our results show that by retraining the autoencoder we can achieve better open-ended complexity compared to a static model, which is further improved when retraining using larger datasets of individuals with diverse complexities.

</details>

<details>

<summary>2022-09-07 14:56:02 - When Are Names Similar Or the Same? Introducing the Code Names Matcher Library</summary>

- *Moshe Munk, Dror G. Feitelson*

- `2209.03198v1` - [abs](http://arxiv.org/abs/2209.03198v1) - [pdf](http://arxiv.org/pdf/2209.03198v1)

> Program code contains functions, variables, and data structures that are represented by names. To promote human understanding, these names should describe the role and use of the code elements they represent. But the names given by developers show high variability, reflecting the tastes of each developer, with different words used for the same meaning or the same words used for different meanings. This makes comparing names hard. A precise comparison should be based on matching identical words, but also take into account possible variations on the words (including spelling and typing errors), reordering of the words, matching between synonyms, and so on. To facilitate this we developed a library of comparison functions specifically targeted to comparing names in code. The different functions calculate the similarity between names in different ways, so a researcher can choose the one appropriate for his specific needs. All of them share an attempt to reflect human perceptions of similarity, at the possible expense of lexical matching.

</details>

<details>

<summary>2022-09-07 15:35:28 - AutoPruner: Transformer-Based Call Graph Pruning</summary>

- *Thanh Le-Cong, Hong Jin Kang, Truong Giang Nguyen, Stefanus Agus Haryono, David Lo, Xuan-Bach D. Le, Huynh Quyet Thang*

- `2209.03230v1` - [abs](http://arxiv.org/abs/2209.03230v1) - [pdf](http://arxiv.org/pdf/2209.03230v1)

> Constructing a static call graph requires trade-offs between soundness and precision. Program analysis techniques for constructing call graphs are unfortunately usually imprecise. To address this problem, researchers have recently proposed call graph pruning empowered by machine learning to post-process call graphs constructed by static analysis. A machine learning model is built to capture information from the call graph by extracting structural features for use in a random forest classifier. It then removes edges that are predicted to be false positives. Despite the improvements shown by machine learning models, they are still limited as they do not consider the source code semantics and thus often are not able to effectively distinguish true and false positives. In this paper, we present a novel call graph pruning technique, AutoPruner, for eliminating false positives in call graphs via both statistical semantic and structural analysis. Given a call graph constructed by traditional static analysis tools, AutoPruner takes a Transformer-based approach to capture the semantic relationships between the caller and callee functions associated with each edge in the call graph. To do so, AutoPruner fine-tunes a model of code that was pre-trained on a large corpus to represent source code based on descriptions of its semantics. Next, the model is used to extract semantic features from the functions related to each edge in the call graph. AutoPruner uses these semantic features together with the structural features extracted from the call graph to classify each edge via a feed-forward neural network. Our empirical evaluation on a benchmark dataset of real-world programs shows that AutoPruner outperforms the state-of-the-art baselines, improving on F-measure by up to 13% in identifying false-positive edges in a static call graph.

</details>

<details>

<summary>2022-09-07 19:57:50 - Sparse Coding with Multi-Layer Decoders using Variance Regularization</summary>

- *Katrina Evtimova, Yann LeCun*

- `2112.09214v2` - [abs](http://arxiv.org/abs/2112.09214v2) - [pdf](http://arxiv.org/pdf/2112.09214v2)

> Sparse representations of images are useful in many computer vision applications. Sparse coding with an $l_1$ penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the $l_1$ norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements. In this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries. In our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime.

</details>

<details>

<summary>2022-09-07 20:46:30 - The Science Gateway Community Institute's Consulting Services Program: Lessons for Research Software Engineering Organizations</summary>

- *Marlon Pierce, Michael Zentner, Maytal Dahan, Sandra Gesing, Claire Stirm, Linda Bailey Hayden*

- `2209.03958v1` - [abs](http://arxiv.org/abs/2209.03958v1) - [pdf](http://arxiv.org/pdf/2209.03958v1)

> The Science Gateways Community Institute (SGCI) is an NSF Software Infrastructure for Sustained Innovation (S2I2) funded project that leads and supports the science gateway community. Major activities for SGCI include a) sustainability training, including the Focus Week week-long course designed to help science gateway operators develop sustainability plans, and the Jumpstart virtual short-course; b) usability and user experience consulting; c) a community catalog of science gateways and science gateway software; d) workforce development activities, including a coding institute for students, internship opportunities, and hackathons; e) an annual conference; and f) in-depth technical support for client gateway projects. The goals of SGCI's Embedded Technical Support component are to help the institute's clients to create new science gateways or to significantly enhance existing science gateways. Examples of the latter include helping to implement major new capabilities and to implement significant usability improvements suggested by SGCI's usability consultants. The Embedded Technical Support component was managed by Indiana University and involved research software engineers at San Diego Supercomputer Center, Texas Advanced Computing Center, Indiana University, and Purdue University (through 2019). Since 2016, the component has involved 20 research software engineers as consultants and has conducted 59 client consultations. This short paper provides a summary of lessons learned from the Embedded Technical Support program that may be useful for the research software engineering community.

</details>

<details>

<summary>2022-09-08 00:59:56 - Relationship between Gender and Code Reading Speed in Software Development</summary>

- *Yuriko Takatsuka, Yukasa Murakami, Masateru Tsunoda, Masahide Nakamura*

- `2209.03516v1` - [abs](http://arxiv.org/abs/2209.03516v1) - [pdf](http://arxiv.org/pdf/2209.03516v1)

> Recently, workforce shortage has become a popular issue in information technology (IT). One solution to increasing the workforce supply is to increase the number of female IT professionals. This is because there is gender imbalance in information technology area. To accomplish this, it is important to suppress the influence of biases, such as the belief that men are more suited for careers in science and technology than women, and to increase the choice of careers available to female professionals. To help suppress the influence of gender bias, we analyzed the relationship between gender and code reading speed in the field of software development. Certain source codes require developers to use substantial memory to properly understand them, such as those with many variables that frequently change values. Several studies have indicated that the performance of memory differs in males and females. To test the veracity of this claim, we analyzed the influence of gender on code-reading speed through an experiment. Pursuant to this, we prepared four programs that required varied amounts of memory to properly understand them. Then, we measured the time required by each of the 17 male and 16 female subjects (33 subjects in total) to comprehend the different programs. The results suggest that there is no explicit difference between male and female subjects in this regard, even in the case of programs that require high memory capacities for proper understanding.

</details>

<details>

<summary>2022-09-08 03:18:41 - MalDetConv: Automated Behaviour-based Malware Detection Framework Based on Natural Language Processing and Deep Learning Techniques</summary>

- *Pascal Maniriho, Abdun Naser Mahmood, Mohammad Jabed Morshed Chowdhury*

- `2209.03547v1` - [abs](http://arxiv.org/abs/2209.03547v1) - [pdf](http://arxiv.org/pdf/2209.03547v1)

> The popularity of Windows attracts the attention of hackers/cyber-attackers, making Windows devices the primary target of malware attacks in recent years. Several sophisticated malware variants and anti-detection methods have been significantly enhanced and as a result, traditional malware detection techniques have become less effective. This work presents MalBehavD-V1, a new behavioural dataset of Windows Application Programming Interface (API) calls extracted from benign and malware executable files using the dynamic analysis approach. In addition, we present MalDetConV, a new automated behaviour-based framework for detecting both existing and zero-day malware attacks. MalDetConv uses a text processing-based encoder to transform features of API calls into a suitable format supported by deep learning models. It then uses a hybrid of convolutional neural network (CNN) and bidirectional gated recurrent unit (CNN-BiGRU) automatic feature extractor to select high-level features of the API Calls which are then fed to a fully connected neural network module for malware classification. MalDetConv also uses an explainable component that reveals features that contributed to the final classification outcome, helping the decision-making process for security analysts. The performance of the proposed framework is evaluated using our MalBehavD-V1 dataset and other benchmark datasets. The detection results demonstrate the effectiveness of MalDetConv over the state-of-the-art techniques with detection accuracy of 96.10%, 95.73%, 98.18%, and 99.93% achieved while detecting unseen malware from MalBehavD-V1, Allan and John, Brazilian, and Ki-D datasets, respectively. The experimental results show that MalDetConv is highly accurate in detecting both known and zero-day malware attacks on Windows devices.

</details>

<details>

<summary>2022-09-08 09:28:24 - Predict+Optimize for Packing and Covering LPs with Unknown Parameters in Constraints</summary>

- *Xinyi Hu, Jasper C. H. Lee, Jimmy H. M. Lee*

- `2209.03668v1` - [abs](http://arxiv.org/abs/2209.03668v1) - [pdf](http://arxiv.org/pdf/2209.03668v1)

> Predict+Optimize is a recently proposed framework which combines machine learning and constrained optimization, tackling optimization problems that contain parameters that are unknown at solving time. The goal is to predict the unknown parameters and use the estimates to solve for an estimated optimal solution to the optimization problem. However, all prior works have focused on the case where unknown parameters appear only in the optimization objective and not the constraints, for the simple reason that if the constraints were not known exactly, the estimated optimal solution might not even be feasible under the true parameters. The contributions of this paper are two-fold. First, we propose a novel and practically relevant framework for the Predict+Optimize setting, but with unknown parameters in both the objective and the constraints. We introduce the notion of a correction function, and an additional penalty term in the loss function, modelling practical scenarios where an estimated optimal solution can be modified into a feasible solution after the true parameters are revealed, but at an additional cost. Second, we propose a corresponding algorithmic approach for our framework, which handles all packing and covering linear programs. Our approach is inspired by the prior work of Mandi and Guns, though with crucial modifications and re-derivations for our very different setting. Experimentation demonstrates the superior empirical performance of our method over classical approaches.

</details>

<details>

<summary>2022-09-08 11:58:29 - Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives</summary>

- *Yifan He, Claus Aranha, Tetsuya Sakurai*

- `2209.03736v1` - [abs](http://arxiv.org/abs/2209.03736v1) - [pdf](http://arxiv.org/pdf/2209.03736v1)

> We introduce Knowledge-Driven Program Synthesis (KDPS) as a variant of the program synthesis task that requires the agent to solve a sequence of program synthesis problems. In KDPS, the agent should use knowledge from the earlier problems to solve the later ones. We propose a novel method based on PushGP to solve the KDPS problem, which takes subprograms as knowledge. The proposed method extracts subprograms from the solution of previously solved problems by the Even Partitioning (EP) method and uses these subprograms to solve the upcoming programming task using Adaptive Replacement Mutation (ARM). We call this method PushGP+EP+ARM. With PushGP+EP+ARM, no human effort is required in the knowledge extraction and utilization processes. We compare the proposed method with PushGP, as well as a method using subprograms manually extracted by a human. Our PushGP+EP+ARM achieves better train error, success count, and faster convergence than PushGP. Additionally, we demonstrate the superiority of PushGP+EP+ARM when consecutively solving a sequence of six program synthesis problems.

</details>

<details>

<summary>2022-09-08 12:48:24 - Smoothing Codes and Lattices: Systematic Study and New Bounds</summary>

- *Thomas Debris-Alazard, LÃ©o Ducas, Nicolas Resch, Jean-Pierre Tillich*

- `2205.10552v2` - [abs](http://arxiv.org/abs/2205.10552v2) - [pdf](http://arxiv.org/pdf/2205.10552v2)

> In this article we revisit smoothing bounds in parallel between lattices $and$ codes. Initially introduced by Micciancio and Regev, these bounds were instantiated with Gaussian distributions and were crucial for arguing the security of many lattice-based cryptosystems. Unencumbered by direct application concerns, we provide a systematic study of how these bounds are obtained for both lattices $and$ codes, transferring techniques between both areas. We also consider multiple choices of spherically symmetric noise distribution.   We found that the best strategy for a worst-case bound combines Parseval's Identity, the Cauchy-Schwarz inequality, and the second linear programming bound, and this holds for both codes and lattices and all noise distributions at hand. For an average-case analysis, the linear programming bound can be replaced by a tight average count.   This alone gives optimal results for spherically uniform noise over random codes and random lattices. This also improves previous Gaussian smoothing bound for worst-case lattices, but surprisingly this provides even better results with uniform ball noise than for Gaussian (or Bernoulli noise for codes).   This counter-intuitive situation can be resolved by adequate decomposition and truncation of Gaussian and Bernoulli distributions into a superposition of uniform noise, giving further improvement for those cases, and putting them on par with the uniform cases.

</details>

<details>

<summary>2022-09-08 13:34:36 - Presentation: SymDefFix -- Sound Automatic Repair Using Symbolic Execution</summary>

- *Tareq Mohammed Nazir, Martin Pinzger*

- `2209.03815v1` - [abs](http://arxiv.org/abs/2209.03815v1) - [pdf](http://arxiv.org/pdf/2209.03815v1)

> In this presentation, we introduce our constraint-based repair approach, called SymDefFix. SymDefFix is based on ExtractFix [3] and replaces the dynamic analysis steps of ExtractFix to detect the error and find the potential fix locations in an input program with symbolic execution. We first briefly motivate and introduce our modifications of ExtractFix, and then demonstrate it with an example.

</details>

<details>

<summary>2022-09-08 17:17:46 - NeuralFMU: Presenting a workflow for integrating hybrid NeuralODEs into real world applications</summary>

- *Tobias Thummerer, Johannes Stoljar, Lars Mikelsons*

- `2209.03933v1` - [abs](http://arxiv.org/abs/2209.03933v1) - [pdf](http://arxiv.org/pdf/2209.03933v1)

> The term NeuralODE describes the structural combination of an Artifical Neural Network (ANN) and a numerical solver for Ordinary Differential Equations (ODEs), the former acts as the right-hand side of the ODE to be solved. This concept was further extended by a black-box model in the form of a Functional Mock-up Unit (FMU) to obtain a subclass of NeuralODEs, named NeuralFMUs. The resulting structure features the advantages of first-principle and data-driven modeling approaches in one single simulation model: A higher prediction accuracy compared to conventional First Principle Models (FPMs), while also a lower training effort compared to purely data-driven models. We present an intuitive workflow to setup and use NeuralFMUs, enabling the encapsulation and reuse of existing conventional models exported from common modeling tools. Moreover, we exemplify this concept by deploying a NeuralFMU for a consumption simulation based on a Vehicle Longitudinal Dynamics Model (VLDM), which is a typical use case in automotive industry. Related challenges that are often neglected in scientific use cases, like real measurements (e.g. noise), an unknown system state or high-frequent discontinuities, are handled in this contribution. For the aim to build a hybrid model with a higher prediction quality than the original FPM, we briefly highlight two open-source libraries: FMI.jl for integrating FMUs into the Julia programming environment, as well as an extension to this library called FMIFlux.jl, that allows for the integration of FMUs into a neural network topology to finally obtain a NeuralFMU.

</details>

<details>

<summary>2022-09-08 20:41:52 - SPIDER: A Practical Fuzzing Framework to Uncover Stateful Performance Issues in SDN Controllers</summary>

- *Ao Li, Rohan Padhye, Vyas Sekar*

- `2209.04026v1` - [abs](http://arxiv.org/abs/2209.04026v1) - [pdf](http://arxiv.org/pdf/2209.04026v1)

> Performance issues in software-defined network (SDN) controllers can have serious impacts on the performance and availability of networks. We specifically consider stateful performance issues, where a sequence of initial input messages drives an SDN controller into a state such that its performance degrades pathologically when processing subsequent messages. We identify key challenges in applying canonical program analysis techniques: large input space of messages (e.g., stateful OpenFlow protocol), complex code base and software architecture (e.g., OSGi framework with dynamic launch), and the semantic dependencies between the internal state and external inputs. We design SPIDER, a practical fuzzing workflow that tackles these challenges and automatically uncovers such issues in SDN controllers. SPIDER's design entails a careful synthesis and extension of semantic fuzzing, performance fuzzing, and static analysis, taken together with domain-specific insights to tackle these challenges. We show that our design workflow is robust across two controllers -- ONOS and OpenDaylight -- with very different internal implementations. Using SPIDER, we were able to identify and confirm multiple stateful performance issues.

</details>

<details>

<summary>2022-09-08 20:51:41 - Evaluating the Security of Aircraft Systems</summary>

- *Edan Habler, Ron Bitton, Asaf Shabtai*

- `2209.04028v1` - [abs](http://arxiv.org/abs/2209.04028v1) - [pdf](http://arxiv.org/pdf/2209.04028v1)

> The sophistication and complexity of cyber attacks and the variety of targeted platforms have been growing in recent years. Various adversaries are abusing an increasing range of platforms, e.g., enterprise platforms, mobile phones, PCs, transportation systems, and industrial control systems. In recent years, we have witnessed various cyber attacks on transportation systems, including attacks on ports, airports, and trains. It is only a matter of time before transportation systems become a more common target of cyber attackers. Due to the enormous potential damage inherent in attacking vehicles carrying many passengers and the lack of security measures applied in traditional airborne systems, the vulnerability of aircraft systems is one of the most concerning topics in the vehicle security domain. This paper provides a comprehensive review of aircraft systems and components and their various networks, emphasizing the cyber threats they are exposed to and the impact of a cyber attack on these components and networks and the essential capabilities of the aircraft. In addition, we present a comprehensive and in-depth taxonomy that standardizes the knowledge and understanding of cyber security in the avionics field from an adversary's perspective. The taxonomy divides techniques into relevant categories (tactics) reflecting the various phases of the adversarial attack lifecycle and maps existing attacks according to the MITRE ATT&CK methodology. Furthermore, we analyze the security risks among the various systems according to the potential threat actors and categorize the threats based on STRIDE threat model. Future work directions are presented as guidelines for industry and academia.

</details>

<details>

<summary>2022-09-09 08:55:34 - Conversion of Acoustic Signal (Speech) Into Text By Digital Filter using Natural Language Processing</summary>

- *Abhiram Katuri, Sindhu Salugu, Gelli Tharuni, Challa Sri Gouri*

- `2209.04189v1` - [abs](http://arxiv.org/abs/2209.04189v1) - [pdf](http://arxiv.org/pdf/2209.04189v1)

> One of the most crucial aspects of communication in daily life is speech recognition. Speech recognition that is based on natural language processing is one of the essential elements in the conversion of one system to another. In this paper, we created an interface that transforms speech and other auditory inputs into text using a digital filter. Contrary to the many methods for this conversion, it is also possible for linguistic faults to appear occasionally, gender recognition, speech recognition that is unsuccessful (cannot recognize voice), and gender recognition to fail. Since technical problems are involved, we developed a program that acts as a mediator to prevent initiating software issues in order to eliminate even this little deviation. Its planned MFCC and HMM are in sync with its AI system. As a result, technical errors have been avoided.

</details>

<details>

<summary>2022-09-09 20:31:38 - Compiler Testing using Template Java Programs</summary>

- *Zhiqiang Zang, Nathan Wiatrek, Milos Gligoric, August Shi*

- `2209.04514v1` - [abs](http://arxiv.org/abs/2209.04514v1) - [pdf](http://arxiv.org/pdf/2209.04514v1)

> We present JAttack, a framework that enables template-based testing for compilers. Using JAttack, a developer writes a template program that describes a set of programs to be generated and given as test inputs to a compiler. Such a framework enables developers to incorporate their domain knowledge on testing compilers, giving a basic program structure that allows for exploring complex programs that can trigger sophisticated compiler optimizations. A developer writes a template program in the host language (Java) that contains holes to be filled by JAttack. Each hole, written using a domain-specific language, constructs a node within an extended abstract syntax tree (eAST). An eAST node defines the search space for the hole, i.e., a set of expressions and values. JAttack generates programs by executing templates and filling each hole by randomly choosing expressions and values (available within the search space defined by the hole). Additionally, we introduce several optimizations to reduce JAttack's generation cost. While JAttack could be used to test various compiler features, we demonstrate its capabilities in helping test just-in-time (JIT) Java compilers, whose optimizations occur at runtime after a sufficient number of executions. Using JAttack, we have found six critical bugs that were confirmed by Oracle developers. Four of them were previously unknown, including two unknown CVEs (Common Vulnerabilities and Exposures). JAttack shows the power of combining developers' domain knowledge (via templates) with random testing to detect bugs in JIT compilers.

</details>

<details>

<summary>2022-09-09 22:09:17 - PGAbB: A Block-Based Graph Processing Framework for Heterogeneous Platforms</summary>

- *Abdurrahman Yasar, Sivasankaran Rajamanickam, Jonathan W. Berry, Umit V. Catalyurek*

- `2209.04541v1` - [abs](http://arxiv.org/abs/2209.04541v1) - [pdf](http://arxiv.org/pdf/2209.04541v1)

> Designing flexible graph kernels that can run well on various platforms is a crucial research problem due to the frequent usage of graphs for modeling data and recent architectural advances and variety. In this work, we propose a novel graph processing framework, PGAbB (Parallel Graph Algorithms by Blocks), for modern shared-memory heterogeneous platforms. Our framework implements a block-based programming model. This allows a user to express a graph algorithm using kernels that operate on subgraphs. PGAbB support graph computations that fit in host DRAM but not in GPU device memory, and provides simple but effective scheduling techniques to schedule computations to all available resources in a heterogeneous architecture. We have demonstrated that one can easily implement a diverse set of graph algorithms in our framework by developing five algorithms. Our experimental results show that PGAbB implementations achieve better or competitive performance compared to hand-optimized implementations. Based on our experiments on five graph algorithms and forty-four graphs, in the median, PGAbB achieves 1.6, 1.6, 5.7, 3.4, 4.5, and 2.4 times better performance than GAPBS, Galois, Ligra, LAGraph Galois-GPU, and Gunrock graph processing systems, respectively.

</details>

<details>

<summary>2022-09-10 03:10:57 - Share the Tensor Tea: How Databases can Leverage the Machine Learning Ecosystem</summary>

- *Yuki Asada, Victor Fu, Apurva Gandhi, Advitya Gemawat, Lihao Zhang, Dong He, Vivek Gupta, Ehi Nosakhare, Dalitso Banda, Rathijit Sen, Matteo Interlandi*

- `2209.04579v1` - [abs](http://arxiv.org/abs/2209.04579v1) - [pdf](http://arxiv.org/pdf/2209.04579v1)

> We demonstrate Tensor Query Processor (TQP): a query processor that automatically compiles relational operators into tensor programs. By leveraging tensor runtimes such as PyTorch, TQP is able to: (1) integrate with ML tools (e.g., Pandas for data ingestion, Tensorboard for visualization); (2) target different hardware (e.g., CPU, GPU) and software (e.g., browser) backends; and (3) end-to-end accelerate queries containing both relational and ML operators. TQP is generic enough to support the TPC-H benchmark, and it provides performance that is comparable to, and often better than, that of specialized CPU and GPU query processors.

</details>

<details>

<summary>2022-09-10 03:27:35 - Explaining Results of Multi-Criteria Decision Making</summary>

- *Martin Erwig, Prashant Kumar*

- `2209.04582v1` - [abs](http://arxiv.org/abs/2209.04582v1) - [pdf](http://arxiv.org/pdf/2209.04582v1)

> We introduce a method for explaining the results of various linear and hierarchical multi-criteria decision-making (MCDM) techniques such as WSM and AHP. The two key ideas are (A) to maintain a fine-grained representation of the values manipulated by these techniques and (B) to derive explanations from these representations through merging, filtering, and aggregating operations. An explanation in our model presents a high-level comparison of two alternatives in an MCDM problem, presumably an optimal and a non-optimal one, illuminating why one alternative was preferred over the other one. We show the usefulness of our techniques by generating explanations for two well-known examples from the MCDM literature. Finally, we show their efficacy by performing computational experiments.

</details>

<details>

<summary>2022-09-10 12:11:57 - AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models</summary>

- *JosÃ© Antonio HernÃ¡ndez LÃ³pez, Martin Weyssow, JesÃºs SÃ¡nchez Cuadrado, Houari Sahraoui*

- `2206.11719v2` - [abs](http://arxiv.org/abs/2206.11719v2) - [pdf](http://arxiv.org/pdf/2206.11719v2)

> The objective of pre-trained language models is to learn contextual representations of textual data. Pre-trained language models have become mainstream in natural language processing and code modeling. Using probes, a technique to study the linguistic properties of hidden vector spaces, previous works have shown that these pre-trained language models encode simple linguistic properties in their hidden representations. However, none of the previous work assessed whether these models encode the whole grammatical structure of a programming language. In this paper, we prove the existence of a syntactic subspace, lying in the hidden representations of pre-trained language models, which contain the syntactic information of the programming language. We show that this subspace can be extracted from the models' representations and define a novel probing method, the AST-Probe, that enables recovering the whole abstract syntax tree (AST) of an input code snippet. In our experimentations, we show that this syntactic subspace exists in five state-of-the-art pre-trained language models. In addition, we highlight that the middle layers of the models are the ones that encode most of the AST information. Finally, we estimate the optimal size of this syntactic subspace and show that its dimension is substantially lower than those of the models' representation spaces. This suggests that pre-trained language models use a small part of their representation spaces to encode syntactic information of the programming languages.

</details>

<details>

<summary>2022-09-10 12:54:10 - An EPTAS for Budgeted Matroid Independent Set</summary>

- *Ilan Doron-Arad, Ariel Kulik, Hadas Shachnai*

- `2209.04654v1` - [abs](http://arxiv.org/abs/2209.04654v1) - [pdf](http://arxiv.org/pdf/2209.04654v1)

> We consider the budgeted matroid independent set problem. The input is a ground set, where each element has a cost and a non-negative profit, along with a matroid over the elements and a budget. The goal is to select a subset of elements which maximizes the total profit subject to the matroid and budget constraints. Several well known special cases, where we have, e.g., a uniform matroid and a budget, or no matroid constraint (i.e., the classic knapsack problem), admit a fully polynomial-time approximation scheme (FPTAS). In contrast, already a slight generalization to the multi-budgeted matroid independent set problem has a PTAS but does not admit an efficient polynomial-time approximation scheme (EPTAS). This implies a PTAS for our problem, which is the best known result prior to this work. Our main contribution is an EPTAS for the budgeted matroid independent set problem. A key idea of the scheme is to find a representative set for the instance, whose cardinality depends solely on $1/\varepsilon$, where $\varepsilon > 0$ is the accuracy parameter of the scheme. The representative set is identified via matroid basis minimization, which can be solved by a simple greedy algorithm. Our scheme enumerates over subsets of the representative set and extends each subset using a linear program. The notion of representative sets may be useful in solving other variants of the budgeted matroid independent set problem.

</details>

<details>

<summary>2022-09-11 13:28:26 - Using Quantum Computers to Speed Up Dynamic Testing of Software</summary>

- *Andriy Miranskyy*

- `2209.04860v1` - [abs](http://arxiv.org/abs/2209.04860v1) - [pdf](http://arxiv.org/pdf/2209.04860v1)

> Software under test can be analyzed dynamically, while it is being executed, to find defects. However, as the number and possible values of input parameters increase, the cost of dynamic testing rises.   This paper examines whether quantum computers (QCs) can help speed up the dynamic testing of programs written for classical computers (CCs). To accomplish this, an approach is devised involving the following three steps: (1) converting a classical program to a quantum program; (2) computing the number of inputs causing errors, denoted by $K$, using a quantum counting algorithm; and (3) obtaining the actual values of these inputs using Grover's search algorithm.   This approach can accelerate exhaustive and non-exhaustive dynamic testing techniques. On the CC, the computational complexity of these techniques is $O(N)$, where $N$ represents the count of combinations of input parameter values passed to the software under test. In contrast, on the QC, the complexity is $O(\varepsilon^{-1} \sqrt{N/K})$, where $\varepsilon$ is a relative error of measuring $K$.   The paper illustrates how the approach can be applied and discusses its limitations. Moreover, it provides a toy example executed on a simulator and an actual QC. This paper may be of interest to academics and practitioners as the approach presented in the paper may serve as a starting point for exploring the use of QC for dynamic testing of CC code.

</details>

<details>

<summary>2022-09-11 22:45:57 - Tutorial Recommendation for Livestream Videos using Discourse-Level Consistency and Ontology-Based Filtering</summary>

- *Amir Pouran Ben Veyseh, Franck Dernoncourt, Thien Huu Nguyen*

- `2209.04953v1` - [abs](http://arxiv.org/abs/2209.04953v1) - [pdf](http://arxiv.org/pdf/2209.04953v1)

> Streaming videos is one of the methods for creators to share their creative works with their audience. In these videos, the streamer share how they achieve their final objective by using various tools in one or several programs for creative projects. To this end, the steps required to achieve the final goal can be discussed. As such, these videos could provide substantial educational content that can be used to learn how to employ the tools used by the streamer. However, one of the drawbacks is that the streamer might not provide enough details for every step. Therefore, for the learners, it might be difficult to catch up with all the steps. In order to alleviate this issue, one solution is to link the streaming videos with the relevant tutorial available for the tools used in the streaming video. More specifically, a system can analyze the content of the live streaming video and recommend the most relevant tutorials. Since the existing document recommendation models cannot handle this situation, in this work, we present a novel dataset and model for the task of tutorial recommendation for live-streamed videos. We conduct extensive analyses on the proposed dataset and models, revealing the challenging nature of this task.

</details>

<details>

<summary>2022-09-12 04:38:12 - Memorization and Generalization in Neural Code Intelligence Models</summary>

- *Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, Vincent J. Hellendoorn*

- `2106.08704v3` - [abs](http://arxiv.org/abs/2106.08704v3) - [pdf](http://arxiv.org/pdf/2106.08704v3)

> Deep Neural Networks (DNNs) are increasingly being used in software engineering and code intelligence tasks. These are powerful tools that are capable of learning highly generalizable patterns from large datasets through millions of parameters. At the same time, their large capacity can render them prone to memorizing data points. Recent work suggests that the memorization risk manifests especially strongly when the training dataset is noisy, involving many ambiguous or questionable samples, and memorization is the only recourse. The goal of this paper is to evaluate and compare the extent of memorization and generalization in neural code intelligence models. It aims to provide insights on how memorization may impact the learning behavior of neural models in code intelligence systems. To observe the extent of memorization in models, we add random noise to the original training dataset and use various metrics to quantify the impact of noise on various aspects of training and testing. We evaluate several state-of-the-art neural code intelligence models and benchmarks based on Java, Python, and Ruby codebases. Our results highlight important risks: millions of trainable parameters allow the neural networks to memorize anything, including noisy data, and provide a false sense of generalization. We observed all models manifest some forms of memorization. This can be potentially troublesome in most code intelligence tasks where they rely on rather noise-prone and repetitive data sources, such as code from GitHub. To the best of our knowledge, we provide the first study to quantify memorization effects in the domain of software engineering and code intelligence systems. This work raises awareness and provides new insights into important issues of training neural models in code intelligence systems that are usually overlooked by software engineering researchers.

</details>

<details>

<summary>2022-09-12 05:56:35 - Vision Transformer with Convolutional Encoder-Decoder for Hand Gesture Recognition using 24 GHz Doppler Radar</summary>

- *Kavinda Kehelella, Gayangana Leelarathne, Dhanuka Marasinghe, Nisal Kariyawasam, Viduneth Ariyarathna, Arjuna Madanayake, Ranga Rodrigo, Chamira U. S. Edussooriya*

- `2209.05032v1` - [abs](http://arxiv.org/abs/2209.05032v1) - [pdf](http://arxiv.org/pdf/2209.05032v1)

> Transformers combined with convolutional encoders have been recently used for hand gesture recognition (HGR) using micro-Doppler signatures. We propose a vision-transformer-based architecture for HGR with multi-antenna continuous-wave Doppler radar receivers. The proposed architecture consists of three modules: a convolutional encoderdecoder, an attention module with three transformer layers, and a multi-layer perceptron. The novel convolutional decoder helps to feed patches with larger sizes to the attention module for improved feature extraction. Experimental results obtained with a dataset corresponding to a two-antenna continuous-wave Doppler radar receiver operating at 24 GHz (published by Skaria et al.) confirm that the proposed architecture achieves an accuracy of 98.3% which substantially surpasses the state-of-the-art on the used dataset.

</details>

<details>

<summary>2022-09-12 07:11:14 - Software Resurrection: Discovering Programming Pearls by Showing Modernity to Historical Software</summary>

- *Abhishek Dutta*

- `2209.05052v1` - [abs](http://arxiv.org/abs/2209.05052v1) - [pdf](http://arxiv.org/pdf/2209.05052v1)

> Reading computer program code and documentation written by others is, we are told, one of the best ways to learn the art of writing intelligible and maintainable code and documentation. The software resurrection exercise, introduced in this paper, requires a motivated learner to compile and test a historical release (e.g. 20 years old) version of a well maintained and widely adopted open source software on a modern hardware and software platform. This exercise concludes by writing a critique based on issues encountered while compiling and testing a historical software release on a hardware and software platform that could not have been foreseen at the time of release. The learner is also required to fix the issues as a part of the software resurrection exercise. The seemingly pointless exercise of resurrecting a historical software allows motivated learners to experience the pain and joy of software maintenance which is essential for understanding the factors that contribute to intelligibility and maintainability of program code and documentation. The concept of software resurrection exercise is illustrated using a version of the SQLite database engine that was released 20 years ago. This illustration shows that software engineering principles (or programming pearls) emerge when a historical software release is adapted to run successfully on a modern platform. The software resurrection exercise also has the potential to lay foundations for a lifelong willingness to explore and learn from existing program code.

</details>

<details>

<summary>2022-09-12 08:52:26 - Bilevel Optimization for Feature Selection in the Data-Driven Newsvendor Problem</summary>

- *Breno Serrano, Stefan Minner, Maximilian Schiffer, Thibaut Vidal*

- `2209.05093v1` - [abs](http://arxiv.org/abs/2209.05093v1) - [pdf](http://arxiv.org/pdf/2209.05093v1)

> We study the feature-based newsvendor problem, in which a decision-maker has access to historical data consisting of demand observations and exogenous features. In this setting, we investigate feature selection, aiming to derive sparse, explainable models with improved out-of-sample performance. Up to now, state-of-the-art methods utilize regularization, which penalizes the number of selected features or the norm of the solution vector. As an alternative, we introduce a novel bilevel programming formulation. The upper-level problem selects a subset of features that minimizes an estimate of the out-of-sample cost of ordering decisions based on a held-out validation set. The lower-level problem learns the optimal coefficients of the decision function on a training set, using only the features selected by the upper-level. We present a mixed integer linear program reformulation for the bilevel program, which can be solved to optimality with standard optimization solvers. Our computational experiments show that the method accurately recovers ground-truth features already for instances with a sample size of a few hundred observations. In contrast, regularization-based techniques often fail at feature recovery or require thousands of observations to obtain similar accuracy. Regarding out-of-sample generalization, we achieve improved or comparable cost performance.

</details>

<details>

<summary>2022-09-12 11:33:12 - Robust Uncertainty Bounds in Reproducing Kernel Hilbert Spaces: A Convex Optimization Approach</summary>

- *Paul Scharnhorst, Emilio T. Maddalena, Yuning Jiang, Colin N. Jones*

- `2104.09582v3` - [abs](http://arxiv.org/abs/2104.09582v3) - [pdf](http://arxiv.org/pdf/2104.09582v3)

> The problem of establishing out-of-sample bounds for the values of an unkonwn ground-truth function is considered. Kernels and their associated Hilbert spaces are the main formalism employed herein along with an observational model where outputs are corrupted by bounded measurement noise. The noise can originate from any compactly supported distribution and no independence assumptions are made on the available data. In this setting, we show how computing tight, finite-sample uncertainty bounds amounts to solving parametric quadratically constrained linear programs. Next, properties of our approach are established and its relationship with another methods is studied. Numerical experiments are presented to exemplify how the theory can be applied in a number of scenarios, and to contrast it with other closed-form alternatives.

</details>

<details>

<summary>2022-09-12 15:21:15 - Prototyping a Serial Number Based Authentication Model for a Computer in a Wireless Local Area Network</summary>

- *John C. Chebor, Simon M. Karume, Nelson B. Masese, Andrew Kipkebut*

- `2209.05319v1` - [abs](http://arxiv.org/abs/2209.05319v1) - [pdf](http://arxiv.org/pdf/2209.05319v1)

> With the increase of wireless LAN usage in homes and enterprises due to its numerous benefits, authenticating the ever increasing number of devices and their users has become a challenge to proprietors of such kind networks. A MAC address, a physical network address that is used as basis for this study, has a copy of its value in the system software that can be spoofed and altered rendering the address not unique, not secure and unreliable. On the contrary, a computers serial number is hard-coded in the system hardware only and therefore cannot be spoofed and altered making it unique, secure and reliable. The research, therefore, was aimed at designing a model that demonstrates how a computers serial number can be used for authenticating a computer in a wireless local area network. In order to achieve the research objective, the study examined the inbuilt access and use of a computers serial number prototype model as an alternative method of authenticating devices in a network. Design science research methodology that involved design and development, demonstration and model evaluation was employed. A Serial Number Based Authentication Prototype or SNAP was therefore designed using state chart and flow chart diagrams based on dynamic programming, developed over evolutionary prototyping and test run on a static experimental design using Java Development Kit and MySQL platforms to demonstrate, as proof of concept, that a computers serial number can be used to authenticate a computer in a wireless local area network. From the test runs whose outcomes were the binary values yes or no, it was found out that SNAP can actually allow or deny, enable or disable a computer in a network based on the computers serial number. The researcher therefore, recommends that the prototype be scaled up, then adopted as a network device authentication method.

</details>

<details>

<summary>2022-09-12 19:53:44 - Bao-Enclave: Virtualization-based Enclaves for Arm</summary>

- *Samuel Pereira, Joao Sousa, Sandro Pinto, JosÃ© Martins, David Cerdeira*

- `2209.05572v1` - [abs](http://arxiv.org/abs/2209.05572v1) - [pdf](http://arxiv.org/pdf/2209.05572v1)

> General-purpose operating systems (GPOS), such as Linux, encompass several million lines of code. Statistically, a larger code base inevitably leads to a higher number of potential vulnerabilities and inherently a more vulnerable system. To minimize the impact of vulnerabilities in GPOS, it has become common to implement security-sensitive programs outside the domain of the GPOS, i.e., in a Trusted Execution Environment (TEE). Arm TrustZone is the de-facto technology for implementing TEEs in Arm devices. However, over the last decade, TEEs have been successfully attacked hundreds of times. Unfortunately, these attacks have been possible due to the presence of several architectural and implementation flaws in TrustZone-based TEEs. In this paper, we propose Bao-Enclave, a virtualization-based solution that enables OEMs to remove security functionality from the TEE and move them into normal world isolated environments, protected from potentially malicious OSes, in the form of lightweight virtual machines (VMs). We evaluate Bao-Enclave on real hardware platforms and find out that Bao-Enclave may improve the performance of security-sensitive workloads by up to 4.8x, while significantly simplifying the TEE software TCB.

</details>

<details>

<summary>2022-09-12 20:10:02 - BayesLDM: A Domain-Specific Language for Probabilistic Modeling of Longitudinal Data</summary>

- *Karine Tung, Steven De La Torre, Mohamed El Mistiri, Rebecca Braga De Braganca, Eric Hekler, Misha Pavel, Daniel Rivera, Pedja Klasnja, Donna Spruijt-Metz, Benjamin M. Marlin*

- `2209.05581v1` - [abs](http://arxiv.org/abs/2209.05581v1) - [pdf](http://arxiv.org/pdf/2209.05581v1)

> In this paper we present BayesLDM, a system for Bayesian longitudinal data modeling consisting of a high-level modeling language with specific features for modeling complex multivariate time series data coupled with a compiler that can produce optimized probabilistic program code for performing inference in the specified model. BayesLDM supports modeling of Bayesian network models with a specific focus on the efficient, declarative specification of dynamic Bayesian Networks (DBNs). The BayesLDM compiler combines a model specification with inspection of available data and outputs code for performing Bayesian inference for unknown model parameters while simultaneously handling missing data. These capabilities have the potential to significantly accelerate iterative modeling workflows in domains that involve the analysis of complex longitudinal data by abstracting away the process of producing computationally efficient probabilistic inference code. We describe the BayesLDM system components, evaluate the efficiency of representation and inference optimizations and provide an illustrative example of the application of the system to analyzing heterogeneous and partially observed mobile health data.

</details>

<details>

<summary>2022-09-13 02:57:05 - Vision Transformers for Action Recognition: A Survey</summary>

- *Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna, Ajmal Mian*

- `2209.05700v1` - [abs](http://arxiv.org/abs/2209.05700v1) - [pdf](http://arxiv.org/pdf/2209.05700v1)

> Vision transformers are emerging as a powerful tool to solve computer vision problems. Recent techniques have also proven the efficacy of transformers beyond the image domain to solve numerous video-related tasks. Among those, human action recognition is receiving special attention from the research community due to its widespread applications. This article provides the first comprehensive survey of vision transformer techniques for action recognition. We analyze and summarize the existing and emerging literature in this direction while highlighting the popular trends in adapting transformers for action recognition. Due to their specialized application, we collectively refer to these methods as ``action transformers''. Our literature review provides suitable taxonomies for action transformers based on their architecture, modality, and intended objective. Within the context of action transformers, we explore the techniques to encode spatio-temporal data, dimensionality reduction, frame patch and spatio-temporal cube construction, and various representation methods. We also investigate the optimization of spatio-temporal attention in transformer layers to handle longer sequences, typically by reducing the number of tokens in a single attention operation. Moreover, we also investigate different network learning strategies, such as self-supervised and zero-shot learning, along with their associated losses for transformer-based action recognition. This survey also summarizes the progress towards gaining grounds on evaluation metric scores on important benchmarks with action transformers. Finally, it provides a discussion on the challenges, outlook, and future avenues for this research direction.

</details>

<details>

<summary>2022-09-13 04:58:27 - A Many-ported and Shared Memory Architecture for High-Performance ADAS SoCs</summary>

- *Hao Luan, Yu Yao, Chang Huang*

- `2209.05731v1` - [abs](http://arxiv.org/abs/2209.05731v1) - [pdf](http://arxiv.org/pdf/2209.05731v1)

> Increasing investment in computing technologies and the advancements in silicon technology has fueled rapid growth in advanced driver assistance systems (ADAS) and corresponding SoC developments. An ADAS SoC represents a heterogeneous architecture that consists of CPUs, GPUs and artificial intelligence (AI) accelerators. In order to guarantee its safety and reliability, it must process massive amount of raw data collected from multiple redundant sources such as high-definition video cameras, Radars, and Lidars to recognize objects correctly and to make the right decisions promptly. A domain specific memory architecture is essential to achieve the above goals. We present a shared memory architecture that enables high data throughput among multiple parallel accesses native to the ADAS applications. It also provides deterministic access latency with proper isolation under the stringent real-time QoS constraints. A prototype is built and analyzed. The results validate that the proposed architecture provides close to 100\% throughput for both read and write accesses generated simultaneously by many accessing masters with full injection rate. It can also provide consistent QoS to the domain specific payloads while enabling the scalability and modularity of the design.

</details>

<details>

<summary>2022-09-13 08:27:25 - Molecular Design Based on Integer Programming and Quadratic Descriptors in a Two-layered Model</summary>

- *Jianshen Zhu, Naveed Ahmed Azam, Shengjuan Cao, Ryota Ido, Kazuya Haraguchi, Liang Zhao, Hiroshi Nagamochi, Tatsuya Akutsu*

- `2209.13527v1` - [abs](http://arxiv.org/abs/2209.13527v1) - [pdf](http://arxiv.org/pdf/2209.13527v1)

> A novel framework has recently been proposed for designing the molecular structure of chemical compounds with a desired chemical property, where design of novel drugs is an important topic in bioinformatics and chemo-informatics. The framework infers a desired chemical graph by solving a mixed integer linear program (MILP) that simulates the computation process of a feature function defined by a two-layered model on chemical graphs and a prediction function constructed by a machine learning method. A set of graph theoretical descriptors in the feature function plays a key role to derive a compact formulation of such an MILP. To improve the learning performance of prediction functions in the framework maintaining the compactness of the MILP, this paper utilizes the product of two of those descriptors as a new descriptor and then designs a method of reducing the number of descriptors. The results of our computational experiments suggest that the proposed method improved the learning performance for many chemical properties and can infer a chemical structure with up to 50 non-hydrogen atoms.

</details>

<details>

<summary>2022-09-13 10:35:42 - Smart Contract Vulnerability Detection Technique: A Survey</summary>

- *Peng Qian, Zhenguang Liu, Qinming He, Butian Huang, Duanzheng Tian, Xun Wang*

- `2209.05872v1` - [abs](http://arxiv.org/abs/2209.05872v1) - [pdf](http://arxiv.org/pdf/2209.05872v1)

> Smart contract, one of the most successful applications of blockchain, is taking the world by storm, playing an essential role in the blockchain ecosystem. However, frequent smart contract security incidents not only result in tremendous economic losses but also destroy the blockchain-based credit system. The security and reliability of smart contracts thus gain extensive attention from researchers worldwide. In this survey, we first summarize the common types and typical cases of smart contract vulnerabilities from three levels, i.e., Solidity code layer, EVM execution layer, and Block dependency layer. Further, we review the research progress of smart contract vulnerability detection and classify existing counterparts into five categories, i.e., formal verification, symbolic execution, fuzzing detection, intermediate representation, and deep learning. Empirically, we take 300 real-world smart contracts deployed on Ethereum as the test samples and compare the representative methods in terms of accuracy, F1-Score, and average detection time. Finally, we discuss the challenges in the field of smart contract vulnerability detection and combine with the deep learning technology to look forward to future research directions.

</details>

<details>

<summary>2022-09-13 11:18:50 - Investigating Bias with a Synthetic Data Generator: Empirical Evidence and Philosophical Interpretation</summary>

- *Alessandro Castelnovo, Riccardo Crupi, Nicole Inverardi, Daniele Regoli, Andrea Cosentini*

- `2209.05889v1` - [abs](http://arxiv.org/abs/2209.05889v1) - [pdf](http://arxiv.org/pdf/2209.05889v1)

> Machine learning applications are becoming increasingly pervasive in our society. Since these decision-making systems rely on data-driven learning, risk is that they will systematically spread the bias embedded in data. In this paper, we propose to analyze biases by introducing a framework for generating synthetic data with specific types of bias and their combinations. We delve into the nature of these biases discussing their relationship to moral and justice frameworks. Finally, we exploit our proposed synthetic data generator to perform experiments on different scenarios, with various bias combinations. We thus analyze the impact of biases on performance and fairness metrics both in non-mitigated and mitigated machine learning models.

</details>

<details>

<summary>2022-09-13 15:07:42 - An Extensive Study of Residential Proxies in China</summary>

- *Mingshuo Yang, Yunnan Yu, Xianghang Mi, Shujun Tang, Shanqing Guo, Yilin Li, Xiaofeng Zheng, Haixin Duan*

- `2209.06056v1` - [abs](http://arxiv.org/abs/2209.06056v1) - [pdf](http://arxiv.org/pdf/2209.06056v1)

> We carry out the first in-depth characterization of residential proxies (RESIPs) in China, for which little is studied in previous works. Our study is made possible through a semantic-based classifier to automatically capture RESIP services. In addition to the classifier, new techniques have also been identified to capture RESIPs without interacting with and relaying traffic through RESIP services, which can significantly lower the cost and thus allow a continuous monitoring of RESIPs. Our RESIP service classifier has achieved a good performance with a recall of 99.7% and a precision of 97.6% in 10-fold cross validation. Applying the classifier has identified 399 RESIP services, a much larger set compared to 38 RESIP services collected in all previous works. Our effort of RESIP capturing lead to a collection of 9,077,278 RESIP IPs (51.36% are located in China), 96.70% of which are not covered in publicly available RESIP datasets. An extensive measurement on RESIPs and their services has uncovered a set of interesting findings as well as several security implications. Especially, 80.05% RESIP IPs located in China have sourced at least one malicious traffic flows during 2021, resulting in 52-million malicious traffic flows in total. And RESIPs have also been observed in corporation networks of 559 sensitive organizations including government agencies, education institutions and enterprises. Also, 3,232,698 China RESIP IPs have opened at least one TCP/UDP ports for accepting relaying requests, which incurs non-negligible security risks to the local network of RESIPs. Besides, 91% China RESIP IPs are of a lifetime less than 10 days while most China RESIP services show up a crest-trough pattern in terms of the daily active RESIPs across time.

</details>

<details>

<summary>2022-09-13 17:18:01 - Borch: A Deep Universal Probabilistic Programming Language</summary>

- *Lewis Belcher, Johan Gudmundsson, Michael Green*

- `2209.06168v1` - [abs](http://arxiv.org/abs/2209.06168v1) - [pdf](http://arxiv.org/pdf/2209.06168v1)

> Ever since the Multilayered Perceptron was first introduced the connectionist community has struggled with the concept of uncertainty and how this could be represented in these types of models. This past decade has seen a lot of effort in trying to join the principled approach of probabilistic modeling with the scalable nature of deep neural networks. While the theoretical benefits of this consolidation are clear, there are also several important practical aspects of these endeavors; namely to force the models we create to represent, learn, and report uncertainty in every prediction that is made. Many of these efforts have been based on extending existing frameworks with additional structures. We present Borch, a scalable deep universal probabilistic programming language, built on top of PyTorch. The code is available for download and use in our repository https://gitlab.com/desupervised/borch.

</details>

<details>

<summary>2022-09-13 18:23:16 - FedNest: Federated Bilevel, Minimax, and Compositional Optimization</summary>

- *Davoud Ataee Tarzanagh, Mingchen Li, Christos Thrampoulidis, Samet Oymak*

- `2205.02215v3` - [abs](http://arxiv.org/abs/2205.02215v3) - [pdf](http://arxiv.org/pdf/2205.02215v3)

> Standard federated optimization methods successfully apply to stochastic problems with single-level structure. However, many contemporary ML problems -- including adversarial robustness, hyperparameter tuning, and actor-critic -- fall under nested bilevel programming that subsumes minimax and compositional optimization. In this work, we propose \fedblo: A federated alternating stochastic gradient method to address general nested problems. We establish provable convergence rates for \fedblo in the presence of heterogeneous data and introduce variations for bilevel, minimax, and compositional optimization. \fedblo introduces multiple innovations including federated hypergradient computation and variance reduction to address inner-level heterogeneity. We complement our theory with experiments on hyperparameter \& hyper-representation learning and minimax optimization that demonstrate the benefits of our method in practice. Code is available at https://github.com/ucr-optml/FedNest.

</details>

<details>

<summary>2022-09-13 19:02:14 - $Ï$VAE: a stochastic process prior for Bayesian deep learning with MCMC</summary>

- *Swapnil Mishra, Seth Flaxman, Tresnia Berah, Harrison Zhu, Mikko Pakkanen, Samir Bhatt*

- `2002.06873v6` - [abs](http://arxiv.org/abs/2002.06873v6) - [pdf](http://arxiv.org/pdf/2002.06873v6)

> Stochastic processes provide a mathematically elegant way model complex data. In theory, they provide flexible priors over function classes that can encode a wide range of interesting assumptions. In practice, however, efficient inference by optimisation or marginalisation is difficult, a problem further exacerbated with big data and high dimensional input spaces. We propose a novel variational autoencoder (VAE) called the prior encoding variational autoencoder ($\pi$VAE). The $\pi$VAE is finitely exchangeable and Kolmogorov consistent, and thus is a continuous stochastic process. We use $\pi$VAE to learn low dimensional embeddings of function classes. We show that our framework can accurately learn expressive function classes such as Gaussian processes, but also properties of functions to enable statistical inference (such as the integral of a log Gaussian process). For popular tasks, such as spatial interpolation, $\pi$VAE achieves state-of-the-art performance both in terms of accuracy and computational efficiency. Perhaps most usefully, we demonstrate that the low dimensional independently distributed latent space representation learnt provides an elegant and scalable means of performing Bayesian inference for stochastic processes within probabilistic programming languages such as Stan.

</details>

<details>

<summary>2022-09-13 19:34:42 - Exploring Code Style Transfer with Neural Networks</summary>

- *Karl Munson, Anish Savla, Chih-Kai Ting, Serenity Wade, Kiran Kate, Kavitha Srinivas*

- `2209.06273v1` - [abs](http://arxiv.org/abs/2209.06273v1) - [pdf](http://arxiv.org/pdf/2209.06273v1)

> Style is a significant component of natural language text, reflecting a change in the tone of text while keeping the underlying information the same. Even though programming languages have strict syntax rules, they also have style. Code can be written with the same functionality but using different language features. However, programming style is difficult to quantify, and thus as part of this work, we define style attributes, specifically for Python. To build a definition of style, we utilized hierarchical clustering to capture a style definition without needing to specify transformations. In addition to defining style, we explore the capability of a pre-trained code language model to capture information about code style. To do this, we fine-tuned pre-trained code-language models and evaluated their performance in code style transfer tasks.

</details>

<details>

<summary>2022-09-13 21:46:50 - Inline Tests</summary>

- *Yu Liu, Pengyu Nie, Owolabi Legunsen, Milos Gligoric*

- `2209.06315v1` - [abs](http://arxiv.org/abs/2209.06315v1) - [pdf](http://arxiv.org/pdf/2209.06315v1)

> Unit tests are widely used to check source code quality, but they can be too coarse-grained or ill-suited for testing individual program statements. We introduce inline tests to make it easier to check for faults in statements. We motivate inline tests through several language features and a common testing scenario in which inline tests could be beneficial. For example, inline tests can allow a developer to test a regular expression in place. We also define language-agnostic requirements for inline testing frameworks. Lastly, we implement I-Test, the first inline testing framework. I-Test works for Python and Java, and it satisfies most of the requirements. We evaluate I-Test on open-source projects by using it to test 144 statements in 31 Python programs and 37 Java programs. We also perform a user study. All nine user study participants say that inline tests are easy to write and that inline testing is beneficial. The cost of running inline tests is negligible, at 0.007x--0.014x, and our inline tests helped find two faults that have been fixed by the developers.

</details>

<details>

<summary>2022-09-13 22:01:42 - Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance</summary>

- *Anna Gottardi, Osman Ipek, Giuseppe Castellucci, Shui Hu, Lavina Vaz, Yao Lu, Anju Khatri, Anjali Chadha, Desheng Zhang, Sattvik Sahai, Prerna Dwivedi, Hangjie Shi, Lucy Hu, Andy Huang, Luke Dai, Bofei Yang, Varun Somani, Pankaj Rajan, Ron Rezac, Michael Johnston, Savanna Stiff, Leslie Ball, David Carmel, Yang Liu, Dilek Hakkani-Tur, Oleg Rokhlenko, Kate Bland, Eugene Agichtein, Reza Ghanadan, Yoelle Maarek*

- `2209.06321v1` - [abs](http://arxiv.org/abs/2209.06321v1) - [pdf](http://arxiv.org/pdf/2209.06321v1)

> Since its inception in 2016, the Alexa Prize program has enabled hundreds of university students to explore and compete to develop conversational agents through the SocialBot Grand Challenge. The goal of the challenge is to build agents capable of conversing coherently and engagingly with humans on popular topics for 20 minutes, while achieving an average rating of at least 4.0/5.0. However, as conversational agents attempt to assist users with increasingly complex tasks, new conversational AI techniques and evaluation platforms are needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the success of the SocialBot challenge by introducing the requirements of interactively assisting humans with real-world Cooking and Do-It-Yourself tasks, while making use of both voice and visual modalities. This challenge requires the TaskBots to identify and understand the user's need, identify and integrate task and domain knowledge into the interaction, and develop new ways of engaging the user without distracting them from the task at hand, among other challenges. This paper provides an overview of the TaskBot challenge, describes the infrastructure support provided to the teams with the CoBot Toolkit, and summarizes the approaches the participating teams took to overcome the research challenges. Finally, it analyzes the performance of the competing TaskBots during the first year of the competition.

</details>

<details>

<summary>2022-09-14 02:33:44 - SORNet: Spatial Object-Centric Representations for Sequential Manipulation</summary>

- *Wentao Yuan, Chris Paxton, Karthik Desingh, Dieter Fox*

- `2109.03891v3` - [abs](http://arxiv.org/abs/2109.03891v3) - [pdf](http://arxiv.org/pdf/2109.03891v3)

> Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state. In such tasks, the ability to reason about spatial relations among object entities from raw sensor inputs is crucial in order to determine when a task has been completed and which actions can be executed. In this work, we propose SORNet (Spatial Object-Centric Representation Network), a framework for learning object-centric representations from RGB images conditioned on a set of object queries, represented as image patches called canonical object views. With only a single canonical view per object and no annotation, SORNet generalizes zero-shot to object entities whose shape and texture are both unseen during training. We evaluate SORNet on various spatial reasoning tasks such as spatial relation classification and relative direction regression in complex tabletop manipulation scenarios and show that SORNet significantly outperforms baselines including state-of-the-art representation learning techniques. We also demonstrate the application of the representation learned by SORNet on visual-servoing and task planning for sequential manipulation on a real robot.

</details>

<details>

<summary>2022-09-14 07:07:55 - Prediction Intervals and Confidence Regions for Symbolic Regression Models based on Likelihood Profiles</summary>

- *Fabricio Olivetti de Franca, Gabriel Kronberger*

- `2209.06454v1` - [abs](http://arxiv.org/abs/2209.06454v1) - [pdf](http://arxiv.org/pdf/2209.06454v1)

> Symbolic regression is a nonlinear regression method which is commonly performed by an evolutionary computation method such as genetic programming. Quantification of uncertainty of regression models is important for the interpretation of models and for decision making. The linear approximation and so-called likelihood profiles are well-known possibilities for the calculation of confidence and prediction intervals for nonlinear regression models. These simple and effective techniques have been completely ignored so far in the genetic programming literature. In this work we describe the calculation of likelihood profiles in details and also provide some illustrative examples with models created with three different symbolic regression algorithms on two different datasets. The examples highlight the importance of the likelihood profiles to understand the limitations of symbolic regression models and to help the user taking an informed post-prediction decision.

</details>

<details>

<summary>2022-09-14 09:30:03 - Revisiting the tree edit distance and its backtracing: A tutorial</summary>

- *Benjamin PaaÃen*

- `1805.06869v4` - [abs](http://arxiv.org/abs/1805.06869v4) - [pdf](http://arxiv.org/pdf/1805.06869v4)

> Almost 30 years ago, Zhang and Shasha (1989) published a seminal paper describing an efficient dynamic programming algorithm computing the tree edit distance, that is, the minimum number of node deletions, insertions, and replacements that are necessary to transform one tree into another. Since then, the tree edit distance has been widely applied, for example in biology and intelligent tutoring systems. However, the original paper of Zhang and Shasha can be challenging to read for newcomers and it does not describe how to efficiently infer the optimal edit script. In this contribution, we provide a comprehensive tutorial to the tree edit distance algorithm of Zhang and Shasha. We further prove metric properties of the tree edit distance, and describe efficient algorithms to infer the cheapest edit script, as well as a summary of all cheapest edit scripts between two trees.

</details>

<details>

<summary>2022-09-14 10:48:39 - The Role of Executable Abstract Programs in Software Development and Documentation</summary>

- *Egon Boerger*

- `2209.06546v1` - [abs](http://arxiv.org/abs/2209.06546v1) - [pdf](http://arxiv.org/pdf/2209.06546v1)

> We present Executable Abstract Programs and analyse their role for software development and documentation. The intuitive understanding of these programs fits the computational mindset of software system engineers and is supported by a simple but precise behavioural definition. Therefore, they can be smoothly integrated in the practitioner's daily work to rigorously formulate every design and implementation decision taken on the path from the Executable Abstract Program for the requirements to the targeted and efficiently runnable code.   The Executable Abstract Programs of the resulting system documentation represent definitions of implementation steps one can check and justify by testing (due to their executable character) or by reasoning (due to the mathematical definition of their behaviour). For complex systems the implementation involves multiple (orthogonal or successive) implementation steps which represent instances of a practical computational refinement concept. Such a system development process is driven by computational refinements and is strictly limited to explicitly formulate and justify -- besides the requirements -- only the necessary implementation steps. As a consequence, it produces as side-effect a corpus of documentation that facilitates the understandability of the final code and improves its reliability and resilience; it also enhances the maintenance process (including reuse and change of abstract programs and code) and reduces maintenance cost.

</details>

<details>

<summary>2022-09-14 11:56:30 - The Embeddings World and Artificial General Intelligence</summary>

- *Mostafa Haghir Chehreghani*

- `2209.06569v1` - [abs](http://arxiv.org/abs/2209.06569v1) - [pdf](http://arxiv.org/pdf/2209.06569v1)

> From early days, a key and controversial question inside the artificial intelligence community was whether Artificial General Intelligence (AGI) is achievable. AGI is the ability of machines and computer programs to achieve human-level intelligence and do all tasks that a human being can. While there exist a number of systems in the literature claiming they realize AGI, several other researchers argue that it is impossible to achieve it. In this paper, we take a different view to the problem. First, we discuss that in order to realize AGI, along with building intelligent machines and programs, an intelligent world should also be constructed which is on the one hand, an accurate approximation of our world and on the other hand, a significant part of reasoning of intelligent machines is already embedded in this world. Then we discuss that AGI is not a product or algorithm, rather it is a continuous process which will become more and more mature over time (like human civilization and wisdom). Then, we argue that pre-trained embeddings play a key role in building this intelligent world and as a result, realizing AGI. We discuss how pre-trained embeddings facilitate achieving several characteristics of human-level intelligence, such as embodiment, common sense knowledge, unconscious knowledge and continuality of learning, by machines.

</details>

<details>

<summary>2022-09-14 15:00:48 - Cornucopia: A Framework for Feedback Guided Generation of Binaries</summary>

- *Vidush Singhal, Akul Abhilash Pillai, Charitha Saumya, Milind Kulkarni, Aravind Machiry*

- `2209.06694v1` - [abs](http://arxiv.org/abs/2209.06694v1) - [pdf](http://arxiv.org/pdf/2209.06694v1)

> Binary analysis is an important capability required for many security and software engineering applications. Consequently, there are many binary analysis techniques and tools with varied capabilities. However, testing these tools requires a large, varied binary dataset with corresponding source-level information. In this paper, we present Cornucopia, an architecture agnostic automated framework that can generate a plethora of binaries from corresponding program source by exploiting compiler optimizations and feedback-guided learning. Our evaluation shows that Cornucopia was able to generate 309K binaries across four architectures (x86, x64, ARM, MIPS) with an average of 403 binaries for each program and outperforms Bintuner, a similar technique. Our experiments revealed issues with the LLVM optimization scheduler resulting in compiler crashes ($\sim$300). Our evaluation of four popular binary analysis tools Angr, Ghidra, Idapro, and Radare, using Cornucopia generated binaries, revealed various issues with these tools. Specifically, we found 263 crashes in Angr and one memory corruption issue in Idapro. Our differential testing on the analysis results revealed various semantic bugs in these tools. We also tested machine learning tools, Asmvec, Safe, and Debin, that claim to capture binary semantics and show that they perform poorly (For instance, Debin F1 score dropped to 12.9% from reported 63.1%) on Cornucopia generated binaries. In summary, our exhaustive evaluation shows that Cornucopia is an effective mechanism to generate binaries for testing binary analysis techniques effectively.

</details>

<details>

<summary>2022-09-14 18:58:57 - RASR: Risk-Averse Soft-Robust MDPs with EVaR and Entropic Risk</summary>

- *Jia Lin Hau, Marek Petrik, Mohammad Ghavamzadeh, Reazul Russel*

- `2209.04067v2` - [abs](http://arxiv.org/abs/2209.04067v2) - [pdf](http://arxiv.org/pdf/2209.04067v2)

> Prior work on safe Reinforcement Learning (RL) has studied risk-aversion to randomness in dynamics (aleatory) and to model uncertainty (epistemic) in isolation. We propose and analyze a new framework to jointly model the risk associated with epistemic and aleatory uncertainties in finite-horizon and discounted infinite-horizon MDPs. We call this framework that combines Risk-Averse and Soft-Robust methods RASR. We show that when the risk-aversion is defined using either EVaR or the entropic risk, the optimal policy in RASR can be computed efficiently using a new dynamic program formulation with a time-dependent risk level. As a result, the optimal risk-averse policies are deterministic but time-dependent, even in the infinite-horizon discounted setting. We also show that particular RASR objectives reduce to risk-averse RL with mean posterior transition probabilities. Our empirical results show that our new algorithms consistently mitigate uncertainty as measured by EVaR and other standard risk measures.

</details>

<details>

<summary>2022-09-14 22:02:32 - On the interplay of adversarial robustness and architecture components: patches, convolution and attention</summary>

- *Francesco Croce, Matthias Hein*

- `2209.06953v1` - [abs](http://arxiv.org/abs/2209.06953v1) - [pdf](http://arxiv.org/pdf/2209.06953v1)

> In recent years novel architecture components for image classification have been developed, starting with attention and patches used in transformers. While prior works have analyzed the influence of some aspects of architecture components on the robustness to adversarial attacks, in particular for vision transformers, the understanding of the main factors is still limited. We compare several (non)-robust classifiers with different architectures and study their properties, including the effect of adversarial training on the interpretability of the learnt features and robustness to unseen threat models. An ablation from ResNet to ConvNeXt reveals key architectural changes leading to almost $10\%$ higher $\ell_\infty$-robustness.

</details>

<details>

<summary>2022-09-14 23:47:32 - SQL and NoSQL Databases Software architectures performance analysis and assessments -- A Systematic Literature review</summary>

- *Wisal Khan, Teerath Kumar, Zhang Cheng, Kislay Raj, Arunabha M Roy, Bin Luo*

- `2209.06977v1` - [abs](http://arxiv.org/abs/2209.06977v1) - [pdf](http://arxiv.org/pdf/2209.06977v1)

> Context: The efficient processing of Big Data is a challenging task for SQL and NoSQL Databases, where competent software architecture plays a vital role. The SQL Databases are designed for structuring data and supporting vertical scalability. In contrast, horizontal scalability is backed by NoSQL Databases and can process sizeable unstructured Data efficiently. One can choose the right paradigm according to the organisation's needs; however, making the correct choice can often be challenging. The SQL and NoSQL Databases follow different architectures. Also, the mixed model is followed by each category of NoSQL Databases. Hence, data movement becomes difficult for cloud consumers across multiple cloud service providers (CSPs). In addition, each cloud platform IaaS, PaaS, SaaS, and DBaaS also monitors various paradigms. Objective: This systematic literature review (SLR) aims to study the related articles associated with SQL and NoSQL Database software architectures and tackle data portability and Interoperability among various cloud platforms. State of the art presented many performance comparison studies of SQL and NoSQL Databases by observing scaling, performance, availability, consistency and sharding characteristics. According to the research studies, NoSQL Database designed structures can be the right choice for big data analytics, while SQL Databases are suitable for OLTP Databases. The researcher proposes numerous approaches associated with data movement in the cloud. Platform-based APIs are developed, which makes users' data movement difficult. Therefore, data portability and Interoperability issues are noticed during data movement across multiple CSPs. To minimize developer efforts and Interoperability, Unified APIs are demanded to make data movement relatively more accessible among various cloud platforms.

</details>

<details>

<summary>2022-09-15 04:52:29 - Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal</summary>

- *Yucheng Shi, Yahong Han, Yu-an Tan, Xiaohui Kuang*

- `2112.03492v2` - [abs](http://arxiv.org/abs/2112.03492v2) - [pdf](http://arxiv.org/pdf/2112.03492v2)

> Vision transformers (ViTs) have demonstrated impressive performance and stronger adversarial robustness compared to Convolutional Neural Networks (CNNs). On the one hand, ViTs' focus on global interaction between individual patches reduces the local noise sensitivity of images. On the other hand, the neglect of noise sensitivity differences between image regions by existing decision-based attacks further compromises the efficiency of noise compression, especially for ViTs. Therefore, validating the black-box adversarial robustness of ViTs when the target model can only be queried still remains a challenging problem. In this paper, we theoretically analyze the limitations of existing decision-based attacks from the perspective of noise sensitivity difference between regions of the image, and propose a new decision-based black-box attack against ViTs, termed Patch-wise Adversarial Removal (PAR). PAR divides images into patches through a coarse-to-fine search process and compresses the noise on each patch separately. PAR records the noise magnitude and noise sensitivity of each patch and selects the patch with the highest query value for noise compression. In addition, PAR can be used as a noise initialization method for other decision-based attacks to improve the noise compression efficiency on both ViTs and CNNs without introducing additional calculations. Extensive experiments on three datasets demonstrate that PAR achieves a much lower noise magnitude with the same number of queries.

</details>

<details>

<summary>2022-09-15 05:02:01 - iFlipper: Label Flipping for Individual Fairness</summary>

- *Hantian Zhang, Ki Hyun Tae, Jaeyoung Park, Xu Chu, Steven Euijong Whang*

- `2209.07047v1` - [abs](http://arxiv.org/abs/2209.07047v1) - [pdf](http://arxiv.org/pdf/2209.07047v1)

> As machine learning becomes prevalent, mitigating any unfairness present in the training data becomes critical. Among the various notions of fairness, this paper focuses on the well-known individual fairness, which states that similar individuals should be treated similarly. While individual fairness can be improved when training a model (in-processing), we contend that fixing the data before model training (pre-processing) is a more fundamental solution. In particular, we show that label flipping is an effective pre-processing technique for improving individual fairness. Our system iFlipper solves the optimization problem of minimally flipping labels given a limit to the individual fairness violations, where a violation occurs when two similar examples in the training data have different labels. We first prove that the problem is NP-hard. We then propose an approximate linear programming algorithm and provide theoretical guarantees on how close its result is to the optimal solution in terms of the number of label flips. We also propose techniques for making the linear programming solution more optimal without exceeding the violations limit. Experiments on real datasets show that iFlipper significantly outperforms other pre-processing baselines in terms of individual fairness and accuracy on unseen test sets. In addition, iFlipper can be combined with in-processing techniques for even better results.

</details>

<details>

<summary>2022-09-15 07:28:38 - Concurrent Size</summary>

- *Gal Sela, Erez Petrank*

- `2209.07100v1` - [abs](http://arxiv.org/abs/2209.07100v1) - [pdf](http://arxiv.org/pdf/2209.07100v1)

> The size of a data structure (i.e., the number of elements in it) is a widely used property of a data set. However, for concurrent programs, obtaining a correct size efficiently is non-trivial. In fact, the literature does not offer a mechanism to obtain a correct (linearizable) size of a concurrent data set without resorting to inefficient solutions, such as taking a full snapshot of the data structure to count the elements, or acquiring one global lock in all update and size operations. This paper presents a methodology for adding a concurrent linearizable size operation to sets and dictionaries with a relatively low performance overhead. Theoretically, the proposed size operation is wait-free with asymptotic complexity linear in the number of threads (independently of data-structure size). Practically, we evaluated the performance overhead by adding size to various concurrent data structures in Java$-$a skip list, a hash table and a tree. The proposed linearizable size operation executes faster by orders of magnitude compared to the existing option of taking a snapshot, while incurring a throughput loss of $1\%-20\%$ on the original data structure's operations.

</details>

<details>

<summary>2022-09-15 11:26:44 - Number of Attention Heads vs Number of Transformer-Encoders in Computer Vision</summary>

- *Tomas Hrycej, Bernhard Bermeitinger, Siegfried Handschuh*

- `2209.07221v1` - [abs](http://arxiv.org/abs/2209.07221v1) - [pdf](http://arxiv.org/pdf/2209.07221v1)

> Determining an appropriate number of attention heads on one hand and the number of transformer-encoders, on the other hand, is an important choice for Computer Vision (CV) tasks using the Transformer architecture. Computing experiments confirmed the expectation that the total number of parameters has to satisfy the condition of overdetermination (i.e., number of constraints significantly exceeding the number of parameters). Then, good generalization performance can be expected. This sets the boundaries within which the number of heads and the number of transformers can be chosen. If the role of context in images to be classified can be assumed to be small, it is favorable to use multiple transformers with a low number of heads (such as one or two). In classifying objects whose class may heavily depend on the context within the image (i.e., the meaning of a patch being dependent on other patches), the number of heads is equally important as that of transformers.

</details>

<details>

<summary>2022-09-15 14:59:20 - Defending From Physically-Realizable Adversarial Attacks Through Internal Over-Activation Analysis</summary>

- *Giulio Rossolini, Federico Nesti, Fabio Brau, Alessandro Biondi, Giorgio Buttazzo*

- `2203.07341v2` - [abs](http://arxiv.org/abs/2203.07341v2) - [pdf](http://arxiv.org/pdf/2203.07341v2)

> This work presents Z-Mask, a robust and effective strategy to improve the adversarial robustness of convolutional networks against physically-realizable adversarial attacks. The presented defense relies on specific Z-score analysis performed on the internal network features to detect and mask the pixels corresponding to adversarial objects in the input image. To this end, spatially contiguous activations are examined in shallow and deep layers to suggest potential adversarial regions. Such proposals are then aggregated through a multi-thresholding mechanism. The effectiveness of Z-Mask is evaluated with an extensive set of experiments carried out on models for both semantic segmentation and object detection. The evaluation is performed with both digital patches added to the input images and printed patches positioned in the real world. The obtained results confirm that Z-Mask outperforms the state-of-the-art methods in terms of both detection accuracy and overall performance of the networks under attack. Additional experiments showed that Z-Mask is also robust against possible defense-aware attacks.

</details>

<details>

<summary>2022-09-15 15:28:38 - Do Cloud Developers Prefer CLIs or Web Consoles? CLIs Mostly, Though It Varies by Task</summary>

- *Cora Coleman, William G. Griswold, Nick Mitchell*

- `2209.07365v1` - [abs](http://arxiv.org/abs/2209.07365v1) - [pdf](http://arxiv.org/pdf/2209.07365v1)

> Despite the increased importance of Cloud tooling, and many large-scale studies of Cloud users, research has yet to answer what tool modalities (e.g. CLI or web console) developers prefer. In formulating our studies, we quickly found that preference varies heavily based on the programming task at hand. To address this gap, we conducted a two-part research study that quantifies modality preference as a function of programming task. Part one surveys how preference for three tool modalities (CLI, IDE, web console) varies across three classes of task (CRUD, debugging, monitoring). The survey shows, among 60 respondents, developers most prefer the CLI modality, especially for CRUD tasks. Monitoring tasks are the exception for which developers prefer the web console. Part two observes how four participants complete a task using the kubectl CLI and the OpenShift web console. All four participants prefer using the CLI to accomplish the task.

</details>

<details>

<summary>2022-09-15 15:37:09 - Complex systems science and urban science: towards applications to sustainability trade-offs in territorial systems</summary>

- *Juste Raimbault, Denise Pumain*

- `2209.07373v1` - [abs](http://arxiv.org/abs/2209.07373v1) - [pdf](http://arxiv.org/pdf/2209.07373v1)

> Urban systems are at the core of current sustainability concerns, and their study from a complexity perspective has a long history in several disciplines. We survey this literature and discuss future research directions relevant to sustainable planning, in particular the construction of integrative approaches. We finally illustrate this research program with the coupling of urban simulation models to explore trade-offs between sustainable development goals in systems of cities.

</details>

<details>

<summary>2022-09-15 15:47:25 - IoT-Aerial Base Station Task Offloading with Risk-Sensitive Reinforcement Learning for Smart Agriculture</summary>

- *Turgay Pamuklu, Anne Catherine Nguyen, Aisha Syed, W. Sean Kennedy, Melike Erol-Kantarci*

- `2209.07382v1` - [abs](http://arxiv.org/abs/2209.07382v1) - [pdf](http://arxiv.org/pdf/2209.07382v1)

> Aerial base stations (ABSs) allow smart farms to offload processing responsibility of complex tasks from internet of things (IoT) devices to ABSs. IoT devices have limited energy and computing resources, thus it is required to provide an advanced solution for a system that requires the support of ABSs. This paper introduces a novel multi-actor-based risk-sensitive reinforcement learning approach for ABS task scheduling for smart agriculture. The problem is defined as task offloading with a strict condition on completing the IoT tasks before their deadlines. Moreover, the algorithm must also consider the limited energy capacity of the ABSs. The results show that our proposed approach outperforms several heuristics and the classic Q-Learning approach. Furthermore, we provide a mixed integer linear programming solution to determine a lower bound on the performance, and clarify the gap between our risk-sensitive solution and the optimal solution, as well. The comparison proves our extensive simulation results demonstrate that our method is a promising approach for providing a guaranteed task processing services for the IoT tasks in a smart farm, while increasing the hovering time of the ABSs in this farm.

</details>

<details>

<summary>2022-09-15 19:28:05 - Exploring the Tradeoff between Competitive Ratio and Variance in Online-Matching Markets</summary>

- *Pan Xu*

- `2209.07580v1` - [abs](http://arxiv.org/abs/2209.07580v1) - [pdf](http://arxiv.org/pdf/2209.07580v1)

> In this paper, we propose an online-matching-based model to study the assignment problems arising in a wide range of online-matching markets, including online recommendations, ride-hailing platforms, and crowdsourcing markets. It features that each assignment can request a random set of resources and yield a random utility, and the two (cost and utility) can be arbitrarily correlated with each other. We present two linear-programming-based parameterized policies to study the tradeoff between the \emph{competitive ratio} (CR) on the total utilities and the \emph{variance} on the total number of matches (unweighted version). The first one (SAMP) is to sample an edge according to the distribution extracted from the clairvoyant optimal, while the second (ATT) features a time-adaptive attenuation framework that leads to an improvement over the state-of-the-art competitive-ratio result. We also consider the problem under a large-budget assumption and show that SAMP achieves asymptotically optimal performance in terms of competitive ratio.

</details>

<details>

<summary>2022-09-15 23:20:26 - Computing the optimal distributionally-robust strategy to commit to</summary>

- *Sai Mali Ananthanarayanan, Christian Kroer*

- `2209.07647v1` - [abs](http://arxiv.org/abs/2209.07647v1) - [pdf](http://arxiv.org/pdf/2209.07647v1)

> The Stackelberg game model, where a leader commits to a strategy and the follower best responds, has found widespread application, particularly to security problems. In the security setting, the goal is for the leader to compute an optimal strategy to commit to, in order to protect some asset. In many of these applications, the parameters of the follower utility model are not known with certainty. Distributionally-robust optimization addresses this issue by allowing a distribution over possible model parameters, where this distribution comes from a set of possible distributions. The goal is to maximize the expected utility with respect to the worst-case distribution. We initiate the study of distributionally-robust models for computing the optimal strategy to commit to. We consider the case of normal-form games with uncertainty about the follower utility model. Our main theoretical result is to show that a distributionally-robust Stackelberg equilibrium always exists across a wide array of uncertainty models. For the case of a finite set of possible follower utility functions we present two algorithms to compute a distributionally-robust strong Stackelberg equilibrium (DRSSE) using mathematical programs. Next, in the general case where there is an infinite number of possible follower utility functions and the uncertainty is represented by a Wasserstein ball around a finitely-supported nominal distribution, we give an incremental mixed-integer-programming-based algorithm for computing the optimal distributionally-robust strategy. Experiments substantiate the tractability of our algorithm on a classical Stackelberg game, showing that our approach scales to medium-sized games.

</details>

<details>

<summary>2022-09-16 04:10:29 - Multi-channel Nuclear Norm Minus Frobenius Norm Minimization for Color Image Denoising</summary>

- *Yiwen Shan, Dong Hu, Zhi Wang, Tao Jia*

- `2209.08094v1` - [abs](http://arxiv.org/abs/2209.08094v1) - [pdf](http://arxiv.org/pdf/2209.08094v1)

> Color image denoising is frequently encountered in various image processing and computer vision tasks. One traditional strategy is to convert the RGB image to a less correlated color space and denoise each channel of the new space separately. However, such a strategy can not fully exploit the correlated information between channels and is inadequate to obtain satisfactory results. To address this issue, this paper proposes a new multi-channel optimization model for color image denoising under the nuclear norm minus Frobenius norm minimization framework. Specifically, based on the block-matching, the color image is decomposed into overlapping RGB patches. For each patch, we stack its similar neighbors to form the corresponding patch matrix. The proposed model is performed on the patch matrix to recover its noise-free version. During the recovery process, a) a weight matrix is introduced to fully utilize the noise difference between channels; b) the singular values are shrunk adaptively without additionally assigning weights. With them, the proposed model can achieve promising results while keeping simplicity. To solve the proposed model, an accurate and effective algorithm is built based on the alternating direction method of multipliers framework. The solution of each updating step can be analytically expressed in closed-from. Rigorous theoretical analysis proves the solution sequences generated by the proposed algorithm converge to their respective stationary points. Experimental results on both synthetic and real noise datasets demonstrate the proposed model outperforms state-of-the-art models.

</details>

<details>

<summary>2022-09-16 08:11:21 - On the acceptance by code reviewers of candidate security patches suggested by Automated Program Repair tools</summary>

- *Aurora Papotti, Ranindya Paramitha, Fabio Massacci*

- `2209.07211v2` - [abs](http://arxiv.org/abs/2209.07211v2) - [pdf](http://arxiv.org/pdf/2209.07211v2)

> Background: Testing and validation of the semantic correctness of patches provided by tools for Automated Program Repairs (APR) has received a lot of attention. Yet, the eventual acceptance or rejection of suggested patches for real world projects by humans patch reviewers has received a limited attention. Objective: To address this issue, we plan to investigate whether (possibly incorrect) security patches suggested by APR tools are recognized by human reviewers. We also want to investigate whether knowing that a patch was produced by an allegedly specialized tool does change the decision of human reviewers. Method: In the first phase, using a balanced design, we propose to human reviewers a combination of patches proposed by APR tools for different vulnerabilities and ask reviewers to adopt or reject the proposed patches. In the second phase, we tell participants that some of the proposed patches were generated by security specialized tools (even if the tool was actually a `normal' APR tool) and measure whether the human reviewers would change their decision to adopt or reject a patch. Limitations: The experiment will be conducted in an academic setting, and to maintain power, it will focus on a limited sample of popular APR tools and popular vulnerability types.

</details>

<details>

<summary>2022-09-16 12:19:16 - Evaluating the Future Device Security Risk Indicator for Hundreds of IoT Devices</summary>

- *Pascal Oser, Felix Engelmann, Stefan LÃ¼ders, Frank Kargl*

- `2209.03826v2` - [abs](http://arxiv.org/abs/2209.03826v2) - [pdf](http://arxiv.org/pdf/2209.03826v2)

> IoT devices are present in many, especially corporate and sensitive, networks and regularly introduce security risks due to slow vendor responses to vulnerabilities and high difficulty of patching. In this paper, we want to evaluate to what extent the development of future risk of IoT devices due to new and unpatched vulnerabilities can be predicted based on historic information. For this analysis, we build on existing prediction algorithms available in the SAFER framework (prophet and ARIMA) which we evaluate by means of a large data-set of vulnerabilities and patches from 793 IoT devices. Our analysis shows that the SAFER framework can predict a correct future risk for 91% of the devices, demonstrating its applicability. We conclude that this approach is a reliable means for network operators to efficiently detect and act on risks emanating from IoT devices in their networks.

</details>

<details>

<summary>2022-09-16 14:06:12 - User Guided Abductive Proof Generation for Answer Set Programming Queries (Extended Version)</summary>

- *Avishkar Mahajan, Martin Strecker, Meng Weng Wong*

- `2209.07948v1` - [abs](http://arxiv.org/abs/2209.07948v1) - [pdf](http://arxiv.org/pdf/2209.07948v1)

> We present a method for generating possible proofs of a query with respect to a given Answer Set Programming (ASP) rule set using an abductive process where the space of abducibles is automatically constructed just from the input rules alone. Given a (possibly empty) set of user provided facts, our method infers any additional facts that may be needed for the entailment of a query and then outputs these extra facts, without the user needing to explicitly specify the space of all abducibles. We also present a method to generate a set of directed edges corresponding to the justification graph for the query. Furthermore, through different forms of implicit term substitution, our method can take user provided facts into account and suitably modify the abductive solutions. Past work on abduction has been primarily based on goal directed methods. However these methods can result in solvers that are not truly declarative. Much less work has been done on realizing abduction in a bottom up solver like the Clingo ASP solver. We describe novel ASP programs which can be run directly in Clingo to yield the abductive solutions and directed edge sets without needing to modify the underlying solving engine.

</details>

<details>

<summary>2022-09-16 17:23:24 - Web Application Weakness Ontology Based on Vulnerability Data</summary>

- *Onyeka Ezenwoye, Yi Liu*

- `2209.08067v1` - [abs](http://arxiv.org/abs/2209.08067v1) - [pdf](http://arxiv.org/pdf/2209.08067v1)

> Web applications are becoming more ubiquitous. All manner of physical devices are now connected and often have a variety of web applications and web-interfaces. This proliferation of web applications has been accompanied by an increase in reported software vulnerabilities. The objective of this analysis of vulnerability data is to understand the current landscape of reported web application flaws. Along those lines, this work reviews ten years (2011 - 2020) of vulnerability data in the National Vulnerability Database. Based on this data, most common web application weaknesses are identified and their profiles presented. A weakness ontology is developed to capture the attributes of these weaknesses. These include their attack method and attack vectors. Also described is the impact of the weaknesses to software quality attributes. Additionally, the technologies that are susceptible to each weakness are presented, they include programming languages, frameworks, communication protocols, and data formats.

</details>

<details>

<summary>2022-09-16 19:13:02 - Evolving Complexity is Hard</summary>

- *Alden H. Wright, Cheyenne L. Laue*

- `2209.13013v1` - [abs](http://arxiv.org/abs/2209.13013v1) - [pdf](http://arxiv.org/pdf/2209.13013v1)

> Understanding the evolution of complexity is an important topic in a wide variety of academic fields. Implications of better understanding complexity include increased knowledge of major evolutionary transitions and the properties of living and technological systems. Genotype-phenotype (G-P) maps are fundamental to evolution, and biologically-oriented G-P maps have been shown to have interesting and often-universal properties that enable evolution by following phenotype-preserving walks in genotype space. Here we use a digital logic gate circuit G-P map where genotypes are represented by circuits and phenotypes by the functions that the circuits compute. We compare two mathematical definitions of circuit and phenotype complexity and show how these definitions relate to other well-known properties of evolution such as redundancy, robustness, and evolvability. Using both Cartesian and Linear genetic programming implementations, we demonstrate that the logic gate circuit shares many universal properties of biologically derived G-P maps, with the exception of the relationship between one method of computing phenotypic evolvability, robustness, and complexity. Due to the inherent structure of the G-P map, including the predominance of rare phenotypes, large interconnected neutral networks, and the high mutational load of low robustness, complex phenotypes are difficult to discover using evolution. We suggest, based on this evidence, that evolving complexity is hard and we discuss computational strategies for genetic-programming-based evolution to successfully find genotypes that map to complex phenotypes in the search space.

</details>

<details>

<summary>2022-09-16 20:43:48 - A Decade of Code Comment Quality Assessment: A Systematic Literature Review</summary>

- *Pooja Rani, Arianna Blasi, Nataliia Stulova, Sebastiano Panichella, Alessandra Gorla, Oscar Nierstrasz*

- `2209.08165v1` - [abs](http://arxiv.org/abs/2209.08165v1) - [pdf](http://arxiv.org/pdf/2209.08165v1)

> Code comments are important artifacts in software systems and play a paramount role in many software engineering (SE) tasks related to maintenance and program comprehension. However, while it is widely accepted that high quality matters in code comments just as it matters in source code, assessing comment quality in practice is still an open problem. First and foremost, there is no unique definition of quality when it comes to evaluating code comments. The few existing studies on this topic rather focus on specific attributes of quality that can be easily quantified and measured. Existing techniques and corresponding tools may also focus on comments bound to a specific programming language, and may only deal with comments with specific scopes and clear goals (e.g., Javadoc comments at the method level, or in-body comments describing TODOs to be addressed). In this paper, we present a Systematic Literature Review (SLR) of the last decade of research in SE to answer the following research questions: (i) What types of comments do researchers focus on when assessing comment quality? (ii) What quality attributes (QAs) do they consider? (iii) Which tools and techniques do they use to assess comment quality?, and (iv) How do they evaluate their studies on comment quality assessment in general? Our evaluation, based on the analysis of 2353 papers and the actual review of 47 relevant ones, shows that (i) most studies and techniques focus on comments in Java code, thus may not be generalizable to other languages, and (ii) the analyzed studies focus on four main QAs of a total of 21 QAs identified in the literature, with a clear predominance of checking consistency between comments and the code. We observe that researchers rely on manual assessment and specific heuristics rather than the automated assessment of the comment quality attributes.

</details>

<details>

<summary>2022-09-17 12:13:04 - Calling for a feminist revolt to decolonise data and algorithms in the age of Datification</summary>

- *Genoveva Vargas-Solar*

- `2210.08965v1` - [abs](http://arxiv.org/abs/2210.08965v1) - [pdf](http://arxiv.org/pdf/2210.08965v1)

> Feminist and women groups, indigenous communities and scholars in the global south/north refusing to adhere to hegemonic datafication programs have started to organise and fight back from the inside. The first essential step is to show and problematise technological progress exhibiting the poverty, violence, exclusion, and cultural erase promoted by this "progress". The second step is to promote technology, algorithmic and artificial literacy. Education is critical to learn how to revert and revoke the datified digital twin already colonising all Earth's societies silently and with impunity. It is not the colonisation of body-territories; it goes beyond and occupies humanity's mind's essence, i.e., imagination and imaginary. Against the colonisation of the imaginary, militant groups are imagining and designing alternative algorithms, datasets collection strategies and appropriation methods. The paper discusses their actions and alternative thinking.

</details>

<details>

<summary>2022-09-17 16:46:07 - Long-Term Mentoring for Computer Science Researchers</summary>

- *Emily Ruppel, Sihang Liu, Elba Garza, Sukyoung Ryu, Alexandra Silva, Talia Ringer*

- `2208.04738v2` - [abs](http://arxiv.org/abs/2208.04738v2) - [pdf](http://arxiv.org/pdf/2208.04738v2)

> Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.   Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).   The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as "life changing" and "a career saver." And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.

</details>

<details>

<summary>2022-09-18 00:48:27 - Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?</summary>

- *Yi Wang, Zhiwen Fan, Tianlong Chen, Hehe Fan, Zhangyang Wang*

- `2209.07026v2` - [abs](http://arxiv.org/abs/2209.07026v2) - [pdf](http://arxiv.org/pdf/2209.07026v2)

> Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the "universal" modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we "inflate" the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant "minimalist" 3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance "for free".

</details>

<details>

<summary>2022-09-18 09:08:51 - Infrared: A Meta Bug Detector</summary>

- *Chi Zhang, Yu Wang, Linzhang Wang*

- `2209.08510v1` - [abs](http://arxiv.org/abs/2209.08510v1) - [pdf](http://arxiv.org/pdf/2209.08510v1)

> The recent breakthroughs in deep learning methods have sparked a wave of interest in learning-based bug detectors. Compared to the traditional static analysis tools, these bug detectors are directly learned from data, thus, easier to create. On the other hand, they are difficult to train, requiring a large amount of data which is not readily available. In this paper, we propose a new approach, called meta bug detection, which offers three crucial advantages over existing learning-based bug detectors: bug-type generic (i.e., capable of catching the types of bugs that are totally unobserved during training), self-explainable (i.e., capable of explaining its own prediction without any external interpretability methods) and sample efficient (i.e., requiring substantially less training data than standard bug detectors). Our extensive evaluation shows our meta bug detector (MBD) is effective in catching a variety of bugs including null pointer dereference, array index out-of-bound, file handle leak, and even data races in concurrent programs; in the process MBD also significantly outperforms several noteworthy baselines including Facebook Infer, a prominent static analysis tool, and FICS, the latest anomaly detection method.

</details>

<details>

<summary>2022-09-18 13:46:56 - ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding</summary>

- *Wenjin Wang, Zhengjie Huang, Bin Luo, Qianglong Chen, Qiming Peng, Yinxu Pan, Weichong Yin, Shikun Feng, Yu Sun, Dianhai Yu, Yin Zhang*

- `2209.08569v1` - [abs](http://arxiv.org/abs/2209.08569v1) - [pdf](http://arxiv.org/pdf/2209.08569v1)

> Recent efforts of multimodal Transformers have improved Visually Rich Document Understanding (VrDU) tasks via incorporating visual and textual information. However, existing approaches mainly focus on fine-grained elements such as words and document image patches, making it hard for them to learn from coarse-grained elements, including natural lexical units like phrases and salient visual regions like prominent image regions. In this paper, we attach more importance to coarse-grained elements containing high-density information and consistent semantics, which are valuable for document understanding. At first, a document graph is proposed to model complex relationships among multi-grained multimodal elements, in which salient visual regions are detected by a cluster-based method. Then, a multi-grained multimodal Transformer called mmLayout is proposed to incorporate coarse-grained information into existing pre-trained fine-grained multimodal Transformers based on the graph. In mmLayout, coarse-grained information is aggregated from fine-grained, and then, after further processing, is fused back into fine-grained for final prediction. Furthermore, common sense enhancement is introduced to exploit the semantic information of natural lexical units. Experimental results on four tasks, including information extraction and document question answering, show that our method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Qualitative analyses show that our method can capture consistent semantics in coarse-grained elements.

</details>

<details>

<summary>2022-09-19 02:36:28 - Detecting and Fixing Data Loss Issues in Android Apps</summary>

- *Wunan Guo, Zhen Dong, Liwei Shen, Wei Tian, Ting Su, Xin Peng*

- `2209.08719v1` - [abs](http://arxiv.org/abs/2209.08719v1) - [pdf](http://arxiv.org/pdf/2209.08719v1)

> Android apps are event-driven, and their execution is often interrupted by external events. This interruption can cause data loss issues that annoy users. For instance, when the screen is rotated, the current app page will be destroyed and recreated. If the app state is improperly preserved, user data will be lost. In this work, we present an approach and tool iFixDataloss that automatically detects and fixes data loss issues in Android apps. To achieve this, we identify scenarios in which data loss issues may occur, develop strategies to reveal data loss issues, and design patch templates to fix them. Our experiments on 66 Android apps show iFixDataloss detected 374 data loss issues (284 of them were previously unknown) and successfully generated patches for 188 of the 374 issues. Out of 20 submitted patches, 16 have been accepted by developers. In comparison with state-of-the-art techniques, iFixDataloss performed significantly better in terms of the number of detected data loss issues and the quality of generated patches.

</details>

<details>

<summary>2022-09-19 05:07:09 - An Optimal Level-synchronous Shared-memory Parallel BFS Algorithm with Optimal parallel Prefix-sum Algorithm and its Implications for Energy Consumption</summary>

- *Jesmin Jahan Tithi, Yonatan Fogel, Rezaul Chowdhury*

- `2209.08764v1` - [abs](http://arxiv.org/abs/2209.08764v1) - [pdf](http://arxiv.org/pdf/2209.08764v1)

> We present a work-efficient parallel level-synchronous Breadth First Search (BFS) algorithm for shared-memory architectures which achieves the theoretical lower bound on parallel running time. The optimality holds regardless of the shape of the graph. We also demonstrate the implication of this optimality for the energy consumption of the program empirically. The key idea is never to use more processing cores than necessary to complete the work in any computation step efficiently. We keep the rest of the cores idle to save energy and to reduce other resource contentions (e.g., bandwidth, shared caches, etc). Our BFS does not use locks and atomic instructions and is easily extendible to shared-memory coprocessors.

</details>

<details>

<summary>2022-09-19 08:14:42 - Hardening with Scapolite: a DevOps-based Approach for Improved Authoring and Testing of Security-Configuration Guides in Large-Scale Organizations</summary>

- *Patrick StÃ¶ckle, Ionut Pruteanu, Bernd Grobauer, Alexander Pretschner*

- `2209.08824v1` - [abs](http://arxiv.org/abs/2209.08824v1) - [pdf](http://arxiv.org/pdf/2209.08824v1)

> Security Hardening is the process of configuring IT systems to ensure the security of the systems' components and data they process or store. In many cases, so-called security-configuration guides are used as a basis for security hardening. These guides describe secure configuration settings for components such as operating systems and standard applications. Rigorous testing of security-configuration guides and automated mechanisms for their implementation and validation are necessary since erroneous implementations or checks of hardening guides may severely impact systems' security and functionality. At Siemens, centrally maintained security-configuration guides carry machine-readable information specifying both the implementation and validation of each required configuration step. The guides are maintained within git repositories; automated pipelines generate the artifacts for implementation and checking, e.g., PowerShell scripts for Windows, and carry out testing of these artifacts on AWS images. This paper describes our experiences with our DevOps-inspired approach for authoring, maintaining, and testing security-configuration guides. We want to share these experiences to help other organizations with their security hardening and, thus, increase their systems' security.

</details>

<details>

<summary>2022-09-19 08:16:09 - Rapid Recovery of Program Execution Under Power Failures for Embedded Systems with NVM</summary>

- *Min Jia, Edwin Hsing. -M. Sha, Qingfeng Zhuge, Rui Xu, Shouzhen Gu*

- `2209.08826v1` - [abs](http://arxiv.org/abs/2209.08826v1) - [pdf](http://arxiv.org/pdf/2209.08826v1)

> After power is switched on, recovering the interrupted program from the initial state can cause negative impact. Some programs are even unrecoverable. To rapid recovery of program execution under power failures, the execution states of checkpoints are backed up by NVM under power failures for embedded systems with NVM. However, frequent checkpoints will shorten the lifetime of the NVM and incur significant write overhead. In this paper, the technique of checkpoint setting triggered by function calls is proposed to reduce the write on NVM. The evaluation results show an average of 99.8% and 80.5$% reduction on NVM backup size for stack backup, compared to the log-based method and step-based method. In order to better achieve this, we also propose pseudo-function calls to increase backup points to reduce recovery costs, and exponential incremental call-based backup methods to reduce backup costs in the loop. To further avoid the content on NVM is cluttered and out of NVM, a method to clean the contents on the NVM that are useless for restoration is proposed. Based on aforementioned problems and techniques, the recovery technology is proposed, and the case is used to analyze how to recover rapidly under different power failures.

</details>

<details>

<summary>2022-09-19 08:37:57 - Mapping the Structure and Evolution of Software Testing Research Over the Past Three Decades</summary>

- *Alireza Salahirad, Gregory Gay, Ehsan Mohammadi*

- `2109.04086v4` - [abs](http://arxiv.org/abs/2109.04086v4) - [pdf](http://arxiv.org/pdf/2109.04086v4)

> Background: The field of software testing is growing and rapidly-evolving.   Aims: Based on keywords assigned to publications, we seek to identify predominant research topics and understand how they are connected and have evolved.   Method: We apply co-word analysis to map the topology of testing research as a network where author-assigned keywords are connected by edges indicating co-occurrence in publications. Keywords are clustered based on edge density and frequency of connection. We examine the most popular keywords, summarize clusters into high-level research topics, examine how topics connect, and examine how the field is changing.   Results: Testing research can be divided into 16 high-level topics and 18 subtopics. Creation guidance, automated test generation, evolution and maintenance, and test oracles have particularly strong connections to other topics, highlighting their multidisciplinary nature. Emerging keywords relate to web and mobile apps, machine learning, energy consumption, automated program repair and test generation, while emerging connections have formed between web apps, test oracles, and machine learning with many topics. Random and requirements-based testing show potential decline.   Conclusions: Our observations, advice, and map data offer a deeper understanding of the field and inspiration regarding challenges and connections to explore.

</details>

<details>

<summary>2022-09-19 10:03:37 - AnICA: Analyzing Inconsistencies in Microarchitectural Code Analyzers</summary>

- *Fabian Ritter, Sebastian Hack*

- `2209.05994v2` - [abs](http://arxiv.org/abs/2209.05994v2) - [pdf](http://arxiv.org/pdf/2209.05994v2)

> Microarchitectural code analyzers, i.e., tools that estimate the throughput of machine code basic blocks, are important utensils in the tool belt of performance engineers. Recent tools like llvm-mca, uiCA, and Ithemal use a variety of techniques and different models for their throughput predictions. When put to the test, it is common to see these state-of-the-art tools give very different results. These inconsistencies are either errors, or they point to different and rarely documented assumptions made by the tool designers.   In this paper, we present AnICA, a tool taking inspiration from differential testing and abstract interpretation to systematically analyze inconsistencies among these code analyzers. Our evaluation shows that AnICA can summarize thousands of inconsistencies in a few dozen descriptions that directly lead to high-level insights into the different behavior of the tools. In several case studies, we further demonstrate how AnICA automatically finds and characterizes known and unknown bugs in llvm-mca, as well as a quirk in AMD's Zen microarchitectures.

</details>

<details>

<summary>2022-09-19 10:21:56 - Gap-ETH-Tight Approximation Schemes for Red-Green-Blue Separation and Bicolored Noncrossing Euclidean Travelling Salesman Tours</summary>

- *FranÃ§ois Dross, Krzysztof Fleszar, Karol WÄgrzycki, Anna Zych-Pawlewicz*

- `2209.08904v1` - [abs](http://arxiv.org/abs/2209.08904v1) - [pdf](http://arxiv.org/pdf/2209.08904v1)

> In this paper, we study problems of connecting classes of points via noncrossing structures. Given a set of colored terminal points, we want to find a graph for each color that connects all terminals of its color with the restriction that no two graphs cross each other. We consider these problems both on the Euclidean plane and in planar graphs.   On the algorithmic side, we give a Gap-ETH-tight EPTAS for the two-colored traveling salesman problem as well as for the red-blue-green separation problem (in which we want to separate terminals of three colors with two noncrossing polygons of minimum length), both on the Euclidean plane. This improves the work of Arora and Chang (ICALP 2003) who gave a slower PTAS for the simpler red-blue separation problem. For the case of unweighted plane graphs, we also show a PTAS for the two-colored traveling salesman problem. All these results are based on our new patching procedure that might be of independent interest.   On the negative side, we show that the problem of connecting terminal pairs with noncrossing paths is NP-hard on the Euclidean plane, and that the problem of finding two noncrossing spanning trees is NP-hard in plane graphs.

</details>

<details>

<summary>2022-09-19 13:30:47 - Reflecting on Recurring Failures in IoT Development</summary>

- *Dharun Anandayuvaraj, James C. Davis*

- `2206.13560v2` - [abs](http://arxiv.org/abs/2206.13560v2) - [pdf](http://arxiv.org/pdf/2206.13560v2)

> As IoT systems are given more responsibility and autonomy, they offer greater benefits, but also carry greater risks. We believe this trend invigorates an old challenge of software engineering: how to develop high-risk software-intensive systems safely and securely under market pressures? As a first step, we conducted a systematic analysis of recent IoT failures to identify engineering challenges. We collected and analyzed 22 news reports and studied the sources, impacts, and repair strategies of failures in IoT systems. We observed failure trends both within and across application domains. We also observed that failure themes have persisted over time. To alleviate these trends, we outline a research agenda toward a Failure-Aware Software Development Life Cycle for IoT development. We propose an encyclopedia of failures and an empirical basis for system postmortems, complemented by appropriate automated tools.

</details>

<details>

<summary>2022-09-19 14:57:50 - Specifying and Exploiting Non-Monotonic Domain-Specific Declarative Heuristics in Answer Set Programming</summary>

- *Richard Comploi-Taupe, Gerhard Friedrich, Konstantin Schekotihin, Antonius Weinzierl*

- `2209.09066v1` - [abs](http://arxiv.org/abs/2209.09066v1) - [pdf](http://arxiv.org/pdf/2209.09066v1)

> Domain-specific heuristics are an essential technique for solving combinatorial problems efficiently. Current approaches to integrate domain-specific heuristics with Answer Set Programming (ASP) are unsatisfactory when dealing with heuristics that are specified non-monotonically on the basis of partial assignments. Such heuristics frequently occur in practice, for example, when picking an item that has not yet been placed in bin packing. Therefore, we present novel syntax and semantics for declarative specifications of domain-specific heuristics in ASP. Our approach supports heuristic statements that depend on the partial assignment maintained during solving, which has not been possible before. We provide an implementation in ALPHA that makes ALPHA the first lazy-grounding ASP system to support declaratively specified domain-specific heuristics. Two practical example domains are used to demonstrate the benefits of our proposal. Additionally, we use our approach to implement informed} search with A*, which is tackled within ASP for the first time. A* is applied to two further search problems. The experiments confirm that combining lazy-grounding ASP solving and our novel heuristics can be vital for solving industrial-size problems.

</details>

<details>

<summary>2022-09-19 16:04:49 - Dynamic Unicast-Multicast Scheduling for Age-Optimal Information Dissemination in Vehicular Networks</summary>

- *Ahmed Al-Habob, Hina Tabassum, Omer Waqar*

- `2209.13006v1` - [abs](http://arxiv.org/abs/2209.13006v1) - [pdf](http://arxiv.org/pdf/2209.13006v1)

> This paper investigates the problem of minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. Each vehicle is interested in maintaining the freshness of its information status about one or more physical processes. A framework is proposed to optimize the decisions to unicast, multicast, broadcast, or not transmit updates to vehicles as well as power allocations to minimize the AoI and the RSU's power consumption over a time horizon. The formulated problem is a mixed-integer nonlinear programming problem (MINLP), thus a global optimal solution is difficult to achieve. In this context, we first develop an ant colony optimization (ACO) solution which provides near-optimal performance and thus serves as an efficient benchmark. Then, for real-time implementation, we develop a deep reinforcement learning (DRL) framework that captures the vehicles' demands and channel conditions in the state space and assigns processes to vehicles through dynamic unicast-multicast scheduling actions. Complexity analysis of the proposed algorithms is presented. Simulation results depict interesting trade-offs between AoI and power consumption as a function of the network parameters.

</details>

<details>

<summary>2022-09-19 17:25:35 - Proceedings of the Sixth Working Formal Methods Symposium</summary>

- *Vlad Rusu*

- `2209.09208v1` - [abs](http://arxiv.org/abs/2209.09208v1) - [pdf](http://arxiv.org/pdf/2209.09208v1)

> It is our pleasure to present the papers of the sixth Working Formal Methods Symposium (FROM 2022) held at the "Al. I. Cuza" university of Iasi, Romania on September 19-20, 2022. FROM aims to bring together researchers and practitioners who work on formal methods by contributing new theoretical results, methods, techniques, and frameworks, and/or make the formal methods to work by creating or using software tools that apply theoretical contributions.   The program committee chose five contributions and four invited presentations, one of which was shared with the co-located 28th Workshop on Logic, Language, Information and Computation (WoLLIC 2022). Two of the contributions deal with Matching Logic, an expressive logical framework where the formal definitions of programming languages and other logics can be embedded. Other contributions deal with distributed systems, multi-agent systems, and teaching formal methods. The invited talks present work broadly consistent with these topics.

</details>

<details>

<summary>2022-09-19 20:07:54 - S2TD: a Separation Logic Verifier that Supports Reasoning of the Absence and Presence of Bugs</summary>

- *Quang Loc Le, Jun Sun, Long H. Pham, Shengchao Qin*

- `2209.09327v1` - [abs](http://arxiv.org/abs/2209.09327v1) - [pdf](http://arxiv.org/pdf/2209.09327v1)

> Heap-manipulating programs are known to be challenging to reason about. We present a novel verifier for heap-manipulating programs called S2TD, which encodes programs systematically in the form of Constrained Horn Clauses (CHC) using a novel extension of separation logic (SL) with recursive predicates and dangling predicates. S2TD actively explores cyclic proofs to address the path explosion problem. S2TD differentiates itself from existing CHC-based verifiers by focusing on heap-manipulating programs and employing cyclic proof to efficiently verify or falsify them with counterexamples. Compared with existing SL-based verifiers, S2TD precisely specifies the heaps of de-allocated pointers to avoid false positives in reasoning about the presence of bugs. S2TD has been evaluated using a comprehensive set of benchmark programs from the SV-COMP repository. The results show that S2TD is more effective than state-of-art program verifiers and is more efficient than most of them.

</details>

<details>

<summary>2022-09-20 15:49:35 - Assisted Specification of Code Using Search</summary>

- *Steven P. Reiss*

- `2209.09804v1` - [abs](http://arxiv.org/abs/2209.09804v1) - [pdf](http://arxiv.org/pdf/2209.09804v1)

> We describe an intelligent assistant based on mining existing software repositories to help the developer interactively create checkable specifications of code. To be most useful we apply this at the subsystem level, that is chunks of code of 1000-10000 lines that can be standalone or integrated into an existing application to provide additional functionality or capabilities. The resultant specifications include both a syntactic description of what should be written and a semantic specification of what it should do, initially in the form of test cases. The generated specification is designed to be used for automatic code generation using various technologies that have been proposed including machine learning, code search, and program synthesis. Our research goal is to enable these technologies to be used effectively for creating subsystems without requiring the developer to write detailed specifications from scratch.

</details>

<details>

<summary>2022-09-20 21:58:45 - SC2EGSet: StarCraft II Esport Replay and Game-state Dataset</summary>

- *Andrzej BiaÅecki, Natalia Jakubowska, PaweÅ Dobrowolski, Piotr BiaÅecki, Leszek KrupiÅski, Andrzej Szczap, Robert BiaÅecki, Jan Gajewski*

- `2207.03428v2` - [abs](http://arxiv.org/abs/2207.03428v2) - [pdf](http://arxiv.org/pdf/2207.03428v2)

> As a relatively new form of sport, esports offers unparalleled data availability. Despite the vast amounts of data that are generated by game engines, it can be challenging to extract them and verify their integrity for the purposes of practical and scientific use.   Our work aims to open esports to a broader scientific community by supplying raw and pre-processed files from StarCraft II esports tournaments. These files can be used in statistical and machine learning modeling tasks and related to various laboratory-based measurements (e.g., behavioral tests, brain imaging). We have gathered publicly available game-engine generated "replays" of tournament matches and performed data extraction and cleanup using a low-level application programming interface (API) parser library.   Additionally, we open-sourced and published all the custom tools that were developed in the process of creating our dataset. These tools include PyTorch and PyTorch Lightning API abstractions to load and model the data.   Our dataset contains replays from major and premiere StarCraft II tournaments since 2016. To prepare the dataset, we processed 55 tournament "replaypacks" that contained 17930 files with game-state information. Based on initial investigation of available StarCraft II datasets, we observed that our dataset is the largest publicly available source of StarCraft II esports data upon its publication.   Analysis of the extracted data holds promise for further Artificial Intelligence (AI), Machine Learning (ML), psychological, Human-Computer Interaction (HCI), and sports-related studies in a variety of supervised and self-supervised tasks.

</details>

<details>

<summary>2022-09-21 11:15:21 - Incremental Symbolic Bounded Model Checking of Software Using Interval Methods via Contractors</summary>

- *Mohannad Aldughaim, Kaled Alshmrany, Rafael Menezes, Lucas Cordeiro, Alexandru Stancu*

- `2012.11245v3` - [abs](http://arxiv.org/abs/2012.11245v3) - [pdf](http://arxiv.org/pdf/2012.11245v3)

> Bounded model checking (BMC) is vital for finding program property violations. For unsafe programs, BMC can quickly find an execution path from an initial state to the violated state that refutes a given safety property. However, BMC techniques struggle to falsify programs that contain loops. BMC needs to incrementally unfold the program loops up to the bound $k$, exposing the property violation, which can thus lead to exploring a considerable state space. Here, we describe and evaluate the first verification method based on interval methods via contractors to reduce the domains of variables representing the search space. This reduction is based on the specified property modeled as functions representing the contractor constraints. In particular, we exploit interval methods via contractors to incrementally analyze the program loop variables and contract the domain where the property is guaranteed to hold to prune the search exploration, thus reducing resource consumption aggressively. Experimental results demonstrate the efficiency and efficacy of our proposed approach over a large set of benchmarks, including $7044$ verification tasks, compared with state-of-the-art BMC tools. Our proposed method can reduce memory usage up to $75$\% while verifying $1$\% more verification tasks.

</details>

<details>

<summary>2022-09-21 11:46:21 - An Automatically Verified Prototype of the Android Permissions System</summary>

- *Maximiliano CristiÃ¡, Guido De Luca, Carlos Luna*

- `2209.10278v1` - [abs](http://arxiv.org/abs/2209.10278v1) - [pdf](http://arxiv.org/pdf/2209.10278v1)

> In a previous work De Luca and Luna presented formal specifications of idealized formulations of the permission model of Android in the Coq proof assistant. This formal development is about 23 KLOC of Coq code, including proofs. This work aims at showing that {log} (`setlog') -- a satisfiability solver and a constraint logic programming language -- can be used as an effective automated prover for the class of proofs that must be discharged in the formal verification of systems such as the one carried out by De Luca and Luna. We show how the Coq model is encoded in {log} and how automated proofs are performed. The resulting {log} model is an automatically verified executable prototype of the Android permissions system. Detailed data on the empirical evaluation resulting after executing all the proofs in {log} is provided. The integration of Coq and {log} as to provide a framework featuring automated proof and prototype generation is discussed.

</details>

<details>

<summary>2022-09-21 12:44:45 - Analyzing Robustness of Angluin's L* Algorithm in Presence of Noise</summary>

- *Igor Khmelnitsky, Serge Haddad, Lina Ye, BenoÃ®t Barbot, Benedikt Bollig, Martin Leucker, Daniel Neider, Rajarshi Roy*

- `2209.10315v1` - [abs](http://arxiv.org/abs/2209.10315v1) - [pdf](http://arxiv.org/pdf/2209.10315v1)

> Angluin's L* algorithm learns the minimal (complete) deterministic finite automaton (DFA) of a regular language using membership and equivalence queries. Its probabilistic approximatively correct (PAC) version substitutes an equivalence query by a large enough set of random membership queries to get a high level confidence to the answer. Thus it can be applied to any kind of (also non-regular) device and may be viewed as an algorithm for synthesizing an automaton abstracting the behavior of the device based on observations. Here we are interested on how Angluin's PAC learning algorithm behaves for devices which are obtained from a DFA by introducing some noise. More precisely we study whether Angluin's algorithm reduces the noise and produces a DFA closer to the original one than the noisy device. We propose several ways to introduce the noise: (1) the noisy device inverts the classification of words w.r.t. the DFA with a small probability, (2) the noisy device modifies with a small probability the letters of the word before asking its classification w.r.t. the DFA, and (3) the noisy device combines the classification of a word w.r.t. the DFA and its classification w.r.t. a counter automaton. Our experiments were performed on several hundred DFAs.   Our main contributions, bluntly stated, consist in showing that: (1) Angluin's algorithm behaves well whenever the noisy device is produced by a random process, (2) but poorly with a structured noise, and, that (3) almost surely randomness yields systems with non-recursively enumerable languages.

</details>

<details>

<summary>2022-09-21 12:58:45 - Neural Program Repair: Systems, Challenges and Solutions</summary>

- *Wenkang Zhong, Chuanyi Li, Jidong Ge, Bin Luo*

- `2202.10868v2` - [abs](http://arxiv.org/abs/2202.10868v2) - [pdf](http://arxiv.org/pdf/2202.10868v2)

> Automated Program Repair (APR) aims to automatically fix bugs in the source code. Recently, as advances in Deep Learning (DL) field, there is a rise of Neural Program Repair (NPR) studies, which formulate APR as a translation task from buggy code to correct code and adopt neural networks based on encoder-decoder architecture. Compared with other APR techniques, NPR approaches have a great advantage in applicability because they do not need any specification (i.e., a test suite). Although NPR has been a hot research direction, there isn't any overview on this field yet. In order to help interested readers understand architectures, challenges and corresponding solutions of existing NPR systems, we conduct a literature review on latest studies in this paper. We begin with introducing the background knowledge on this field. Next, to be understandable, we decompose the NPR procedure into a series of modules and explicate various design choices on each module. Furthermore, we identify several challenges and discuss the effect of existing solutions. Finally, we conclude and provide some promising directions for future research.

</details>

<details>

<summary>2022-09-21 17:20:20 - Improved Marginal Unbiased Score Expansion (MUSE) via Implicit Differentiation</summary>

- *Marius Millea*

- `2209.10512v1` - [abs](http://arxiv.org/abs/2209.10512v1) - [pdf](http://arxiv.org/pdf/2209.10512v1)

> We apply the technique of implicit differentiation to boost performance, reduce numerical error, and remove required user-tuning in the Marginal Unbiased Score Expansion (MUSE) algorithm for hierarchical Bayesian inference. We demonstrate these improvements on three representative inference problems: 1) an extended Neal's funnel 2) Bayesian neural networks, and 3) probabilistic principal component analysis. On our particular test cases, MUSE with implicit differentiation is faster than Hamiltonian Monte Carlo by factors of 155, 397, and 5, respectively, or factors of 65, 278, and 1 without implicit differentiation, and yields good approximate marginal posteriors. The Julia and Python MUSE packages have been updated to use implicit differentiation, and can solve problems defined by hand or with any of a number of popular probabilistic programming languages and automatic differentiation backends.

</details>

<details>

<summary>2022-09-21 21:08:24 - AutoMix: Unveiling the Power of Mixup for Stronger Classifiers</summary>

- *Zicheng Liu, Siyuan Li, Di Wu, Zihan Liu, Zhiyuan Chen, Lirong Wu, Stan Z. Li*

- `2103.13027v6` - [abs](http://arxiv.org/abs/2103.13027v6) - [pdf](http://arxiv.org/pdf/2103.13027v6)

> Data mixing augmentation have proved to be effective in improving the generalization ability of deep neural networks. While early methods mix samples by hand-crafted policies (e.g., linear interpolation), recent methods utilize saliency information to match the mixed samples and labels via complex offline optimization. However, there arises a trade-off between precise mixing policies and optimization complexity. To address this challenge, we propose a novel automatic mixup (AutoMix) framework, where the mixup policy is parameterized and serves the ultimate classification goal directly. Specifically, AutoMix reformulates the mixup classification into two sub-tasks (i.e., mixed sample generation and mixup classification) with corresponding sub-networks and solves them in a bi-level optimization framework. For the generation, a learnable lightweight mixup generator, Mix Block, is designed to generate mixed samples by modeling patch-wise relationships under the direct supervision of the corresponding mixed labels. To prevent the degradation and instability of bi-level optimization, we further introduce a momentum pipeline to train AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks prove the superiority of AutoMix compared with state-of-the-art in various classification scenarios and downstream tasks.

</details>

<details>

<summary>2022-09-22 00:53:04 - Talking Trojan: Analyzing an Industry-Wide Disclosure</summary>

- *Nicholas Boucher, Ross Anderson*

- `2209.10717v1` - [abs](http://arxiv.org/abs/2209.10717v1) - [pdf](http://arxiv.org/pdf/2209.10717v1)

> While vulnerability research often focuses on technical findings and post-public release industrial response, we provide an analysis of the rest of the story: the coordinated disclosure process from discovery through public release. The industry-wide 'Trojan Source' vulnerability which affected most compilers, interpreters, code editors, and code repositories provided an interesting natural experiment, enabling us to compare responses by firms versus nonprofits and by firms that managed their own response versus firms that outsourced it. We document the interaction with bug bounty programs, government disclosure assistance, academic peer review, and press coverage, among other topics. We compare the response to an attack on source code with the response to a comparable attack on NLP systems employing machine-learning techniques. We conclude with recommendations to improve the global coordinated disclosure system.

</details>

<details>

<summary>2022-09-22 03:02:43 - Reflections on Software Failure Analysis</summary>

- *Paschal C. Amusuo, Aishwarya Sharma, Siddharth R. Rao, Abbey Vincent, James C. Davis*

- `2209.02930v2` - [abs](http://arxiv.org/abs/2209.02930v2) - [pdf](http://arxiv.org/pdf/2209.02930v2)

> Failure studies are important in revealing the root causes, behaviors, and life cycle of defects in software systems. These studies either focus on understanding the characteristics of defects in specific classes of systems or the characteristics of a specific type of defect in the systems it manifests in. Failure studies have influenced various software engineering research directions, especially in the area of software evolution, defect detection, and program repair.   In this paper, we reflect on the conduct of failure studies in software engineering. We reviewed a sample of 52 failure study papers. We identified several recurring problems in these studies, some of which hinder the ability of the engineering community to trust or replicate the results. Based on our findings, we suggest future research directions, including identifying and analyzing failure causal chains, standardizing the conduct of failure studies, and tool support for faster defect analysis.

</details>

<details>

<summary>2022-09-22 11:23:42 - The Sample Complexity of One-Hidden-Layer Neural Networks</summary>

- *Gal Vardi, Ohad Shamir, Nathan Srebro*

- `2202.06233v2` - [abs](http://arxiv.org/abs/2202.06233v2) - [pdf](http://arxiv.org/pdf/2202.06233v2)

> We study norm-based uniform convergence bounds for neural networks, aiming at a tight understanding of how these are affected by the architecture and type of norm constraint, for the simple class of scalar-valued one-hidden-layer networks, and inputs bounded in Euclidean norm. We begin by proving that in general, controlling the spectral norm of the hidden layer weight matrix is insufficient to get uniform convergence guarantees (independent of the network width), while a stronger Frobenius norm control is sufficient, extending and improving on previous work. Motivated by the proof constructions, we identify and analyze two important settings where (perhaps surprisingly) a mere spectral norm control turns out to be sufficient: First, when the network's activation functions are sufficiently smooth (with the result extending to deeper networks); and second, for certain types of convolutional networks. In the latter setting, we study how the sample complexity is additionally affected by parameters such as the amount of overlap between patches and the overall number of patches.

</details>

<details>

<summary>2022-09-22 14:29:24 - Metamorphic Testing in Autonomous System Simulations</summary>

- *Jubril Gbolahan Adigun, Linus Eisele, Michael Felderer*

- `2209.11031v1` - [abs](http://arxiv.org/abs/2209.11031v1) - [pdf](http://arxiv.org/pdf/2209.11031v1)

> Metamorphic testing has proven to be effective for test case generation and fault detection in many domains. It is a software testing strategy that uses certain relations between input-output pairs of a program, referred to as metamorphic relations. This approach is relevant in the autonomous systems domain since it helps in cases where the outcome of a given test input may be difficult to determine. In this paper therefore, we provide an overview of metamorphic testing as well as an implementation in the autonomous systems domain. We implement an obstacle detection and avoidance task in autonomous drones utilising the GNC API alongside a simulation in Gazebo. Particularly, we describe properties and best practices that are crucial for the development of effective metamorphic relations. We also demonstrate two metamorphic relations for metamorphic testing of single and more than one drones, respectively. Our relations reveal several properties and some weak spots of both the implementation and the avoidance algorithm in the light of metamorphic testing. The results indicate that metamorphic testing has great potential in the autonomous systems domain and should be considered for quality assurance in this field.

</details>

<details>

<summary>2022-09-22 16:32:45 - Automatically Generating Test Cases for Safety-Critical Software via Symbolic Execution</summary>

- *Elson Kurian, Daniela Briola, Pietro Braione, Giovanni Denaro*

- `2209.11138v1` - [abs](http://arxiv.org/abs/2209.11138v1) - [pdf](http://arxiv.org/pdf/2209.11138v1)

> Automated test generation based on symbolic execution can be beneficial for systematically testing safety-critical software, to facilitate test engineers to pursue the strict testing requirements mandated by the certification standards, while controlling at the same time the costs of the testing process. At the same time, the development of safety-critical software is often constrained with programming languages or coding conventions that ban linguistic features which are believed to downgrade the safety of the programs, e.g., they do not allow dynamic memory allocation and variable-length arrays, limit the way in which loops are used, forbid recursion, and bound the complexity of control conditions. As a matter of facts, these linguistic features are also the main efficiency-blockers for the test generation approaches based on symbolic execution at the state of the art. This paper contributes new evidence of the effectiveness of generating test cases with symbolic execution for a significant class of industrial safety critical-systems. We specifically focus on Scade, a largely adopted model-based development language for safety-critical embedded software, and we report on a case study in which we exploited symbolic execution to automatically generate test cases for a set of safety-critical programs developed in Scade. To this end, we introduce a novel test generator that we developed in a recent industrial project on testing safety-critical railway software written in Scade, and we report on our experience of using this test generator for testing a set of Scade programs that belong to the development of an on-board signaling unit for high-speed rail. The results provide empirically evidence that symbolic execution is indeed a viable approach for generating high-quality test suites for the safety-critical programs considered in our case study.

</details>

<details>

<summary>2022-09-22 20:29:49 - ProgPrompt: Generating Situated Robot Task Plans using Large Language Models</summary>

- *Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg*

- `2209.11302v1` - [abs](http://arxiv.org/abs/2209.11302v1) - [pdf](http://arxiv.org/pdf/2209.11302v1)

> Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io

</details>

<details>

<summary>2022-09-23 15:05:55 - Faster Randomized Interior Point Methods for Tall/Wide Linear Programs</summary>

- *Agniva Chowdhury, Gregory Dexter, Palma London, Haim Avron, Petros Drineas*

- `2209.08722v2` - [abs](http://arxiv.org/abs/2209.08722v2) - [pdf](http://arxiv.org/pdf/2209.08722v2)

> Linear programming (LP) is an extremely useful tool which has been successfully applied to solve various problems in a wide range of areas, including operations research, engineering, economics, or even more abstract mathematical areas such as combinatorics. It is also used in many machine learning applications, such as $\ell_1$-regularized SVMs, basis pursuit, nonnegative matrix factorization, etc. Interior Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory and in practice. Their underlying complexity is dominated by the cost of solving a system of linear equations at each iteration. In this paper, we consider both feasible and infeasible IPMs for the special case where the number of variables is much larger than the number of constraints. Using tools from Randomized Linear Algebra, we present a preconditioning technique that, when combined with the iterative solvers such as Conjugate Gradient or Chebyshev Iteration, provably guarantees that IPM algorithms (suitably modified to account for the error incurred by the approximate solver), converge to a feasible, approximately optimal solution, without increasing their iteration complexity. Our empirical evaluations verify our theoretical results on both real-world and synthetic data.

</details>

<details>

<summary>2022-09-24 07:34:18 - Are Machine Programming Systems using Right Source-Code Measures to Select Code Repositories?</summary>

- *Niranjan Hasabnis*

- `2209.11946v1` - [abs](http://arxiv.org/abs/2209.11946v1) - [pdf](http://arxiv.org/pdf/2209.11946v1)

> Machine programming (MP) is an emerging field at the intersection of deterministic and probabilistic computing, and it aims to assist software and hardware engineers, among other applications. Along with powerful compute resources, MP systems often rely on vast amount of open-source code to learn interesting properties about code and programming and solve problems in the areas of debugging, code recommendation, auto-completion, etc. Unfortunately, several of the existing MP systems either do not consider quality of code repositories or use atypical quality measures than those typically used in software engineering community to select them. As such, impact of quality of code repositories on the performance of these systems needs to be studied.   In this preliminary paper, we evaluate impact of different quality repositories on the performance of a candidate MP system. Towards that objective, we develop a framework, named GitRank, to rank open-source repositories on quality, maintainability, and popularity by leveraging existing research on this topic. We then apply GitRank to evaluate correlation between the quality measures used by the candidate MP system and the quality measures used by our framework. Our preliminary results reveal some correlation between the quality measures used in GitRank and ControlFlag's performance, suggesting that some of the measures used in GitRank are applicable to ControlFlag. But it also raises questions around right quality measures for code repositories used in MP systems. We believe that our findings also generate interesting insights towards code quality measures that affect performance of MP systems.

</details>

<details>

<summary>2022-09-24 08:15:22 - NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries</summary>

- *Yiru Chen, Ryan Li, Austin Mac, Tianbao Xie, Tao Yu, Eugene Wu*

- `2209.08834v2` - [abs](http://arxiv.org/abs/2209.08834v2) - [pdf](http://arxiv.org/pdf/2209.08834v2)

> We develop NL2INTERFACE to explore the potential of generating usable interactive multi-visualization interfaces from natural language queries. With NL2INTERFACE, users can directly write natural language queries to automatically generate a fully interactive multi-visualization interface without any extra effort of learning a tool or programming language. Further, users can interact with the interfaces to easily transform the data and quickly see the results in the visualizations.

</details>

<details>

<summary>2022-09-24 14:02:05 - Non-monotonic Resource Utilization in the Bandits with Knapsacks Problem</summary>

- *Raunak Kumar, Robert Kleinberg*

- `2209.12013v1` - [abs](http://arxiv.org/abs/2209.12013v1) - [pdf](http://arxiv.org/pdf/2209.12013v1)

> Bandits with knapsacks (BwK) is an influential model of sequential decision-making under uncertainty that incorporates resource consumption constraints. In each round, the decision-maker observes an outcome consisting of a reward and a vector of nonnegative resource consumptions, and the budget of each resource is decremented by its consumption. In this paper we introduce a natural generalization of the stochastic BwK problem that allows non-monotonic resource utilization. In each round, the decision-maker observes an outcome consisting of a reward and a vector of resource drifts that can be positive, negative or zero, and the budget of each resource is incremented by its drift. Our main result is a Markov decision process (MDP) policy that has constant regret against a linear programming (LP) relaxation when the decision-maker knows the true outcome distributions. We build upon this to develop a learning algorithm that has logarithmic regret against the same LP relaxation when the decision-maker does not know the true outcome distributions. We also present a reduction from BwK to our model that shows our regret bound matches existing results.

</details>

<details>

<summary>2022-09-24 21:14:45 - AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time</summary>

- *Johannes Scheiermann, Wolfgang Konen*

- `2204.13307v3` - [abs](http://arxiv.org/abs/2204.13307v3) - [pdf](http://arxiv.org/pdf/2204.13307v3)

> Recently, the seminal algorithms AlphaGo and AlphaZero have started a new era in game learning and deep reinforcement learning. While the achievements of AlphaGo and AlphaZero - playing Go and other complex games at super human level - are truly impressive, these architectures have the drawback that they require high computational resources. Many researchers are looking for methods that are similar to AlphaZero, but have lower computational demands and are thus more easily reproducible.   In this paper, we pick an important element of AlphaZero - the Monte Carlo Tree Search (MCTS) planning stage - and combine it with temporal difference (TD) learning agents. We wrap MCTS for the first time around TD n-tuple networks and we use this wrapping only at test time to create versatile agents that keep at the same time the computational demands low. We apply this new architecture to several complex games (Othello, ConnectFour, Rubik's Cube) and show the advantages achieved with this AlphaZero-inspired MCTS wrapper. In particular, we present results that this agent is the first one trained on standard hardware (no GPU or TPU) to beat the very strong Othello program Edax up to and including level 7 (where most other learning-from-scratch algorithms could only defeat Edax up to level 2).

</details>

<details>

<summary>2022-09-25 00:34:39 - Answer-Set Programs for Repair Updates and Counterfactual Interventions</summary>

- *Leopoldo Bertossi*

- `2209.12110v1` - [abs](http://arxiv.org/abs/2209.12110v1) - [pdf](http://arxiv.org/pdf/2209.12110v1)

> We briefly describe -- mainly through very simple examples -- different kinds of answer-set programs with annotations that have been proposed for specifying: database repairs and consistent query answering; secrecy view and query evaluation with them; counterfactual interventions for causality in databases; and counterfactual-based explanations in machine learning.

</details>

<details>

<summary>2022-09-25 13:16:39 - GPatch: Patching Graph Neural Networks for Cold-Start Recommendations</summary>

- *Hao Chen, Zefan Wang, Yue Xu, Xiao Huang, Feiran Huang*

- `2209.12215v1` - [abs](http://arxiv.org/abs/2209.12215v1) - [pdf](http://arxiv.org/pdf/2209.12215v1)

> Cold start is an essential and persistent problem in recommender systems. State-of-the-art solutions rely on training hybrid models for both cold-start and existing users/items, based on the auxiliary information. Such a hybrid model would compromise the performance of existing users/items, which might make these solutions not applicable in real-worlds recommender systems where the experience of existing users/items must be guaranteed. Meanwhile, graph neural networks (GNNs) have been demonstrated to perform effectively warm (non-cold-start) recommendations. However, they have never been applied to handle the cold-start problem in a user-item bipartite graph. This is a challenging but rewarding task since cold-start users/items do not have links. Besides, it is nontrivial to design an appropriate GNN to conduct cold-start recommendations while maintaining the performance for existing users/items. To bridge the gap, we propose a tailored GNN-based framework (GPatch) that contains two separate but correlated components. First, an efficient GNN architecture -- GWarmer, is designed to model the warm users/items. Second, we construct correlated Patching Networks to simulate and patch GWarmer by conducting cold-start recommendations. Experiments on benchmark and large-scale commercial datasets demonstrate that GPatch is significantly superior in providing recommendations for both existing and cold-start users/items.

</details>

<details>

<summary>2022-09-25 14:37:45 - Residue-Based Natural Language Adversarial Attack Detection</summary>

- *Vyas Raina, Mark Gales*

- `2204.10192v2` - [abs](http://arxiv.org/abs/2204.10192v2) - [pdf](http://arxiv.org/pdf/2204.10192v2)

> Deep learning based systems are susceptible to adversarial attacks, where a small, imperceptible change at the input alters the model prediction. However, to date the majority of the approaches to detect these attacks have been designed for image processing systems. Many popular image adversarial detection approaches are able to identify adversarial examples from embedding feature spaces, whilst in the NLP domain existing state of the art detection approaches solely focus on input text features, without consideration of model embedding spaces. This work examines what differences result when porting these image designed strategies to Natural Language Processing (NLP) tasks - these detectors are found to not port over well. This is expected as NLP systems have a very different form of input: discrete and sequential in nature, rather than the continuous and fixed size inputs for images. As an equivalent model-focused NLP detection approach, this work proposes a simple sentence-embedding "residue" based detector to identify adversarial examples. On many tasks, it out-performs ported image domain detectors and recent state of the art NLP specific detectors.

</details>

<details>

<summary>2022-09-26 06:59:34 - Optimization problems in graphs with locational uncertainty</summary>

- *Marin Bougeret, JÃ©rÃ©my Omer, Michael Poss*

- `2109.00389v4` - [abs](http://arxiv.org/abs/2109.00389v4) - [pdf](http://arxiv.org/pdf/2109.00389v4)

> Many discrete optimization problems amount to selecting a feasible set of edges of least weight. We consider in this paper the context of spatial graphs where the positions of the vertices are uncertain and belong to known uncertainty sets. The objective is to minimize the sum of the distances of the chosen set of edges for the worst positions of the vertices in their uncertainty sets. We first prove that these problems are $\cal NP$-hard even when the feasible sets consist either of all spanning trees or of all $s-t$ paths. Given this hardness, we propose an exact solution algorithm combining integer programming formulations with a cutting plane algorithm, identifying the cases where the separation problem can be solved efficiently. We also propose a conservative approximation and show its equivalence to the affine decision rule approximation in the context of Euclidean distances. We compare our algorithms to three deterministic reformulations on instances inspired by the scientific literature for the Steiner tree problem and a facility location problem.

</details>

<details>

<summary>2022-09-26 14:34:26 - Machine Learning for Improved Gas Network Models in Coordinated Energy Systems</summary>

- *Adriano Arrigo, MihÃ¡ly DolÃ¡nyi, Kenneth Bruninx, Jean-FranÃ§ois Toubeau*

- `2209.12731v1` - [abs](http://arxiv.org/abs/2209.12731v1) - [pdf](http://arxiv.org/pdf/2209.12731v1)

> The current energy transition promotes the convergence of operation between the power and natural gas systems. In that direction, it becomes paramount to improve the modeling of non-convex natural gas flow dynamics within the coordinated power and gas dispatch. In this work, we propose a neural-network-constrained optimization method which includes a regression model of the Weymouth equation, based on supervised machine learning. The Weymouth equation links gas flow to inlet and outlet pressures for each pipeline via a quadratic equality, which is captured by a neural network. The latter is encoded via a tractable mixed-integer linear program into the set of constraints. In addition, our proposed framework is capable of considering bidirectionality without having recourse to complex and potentially inaccurate convexification approaches. We further enhance our model by introducing a reformulation of the activation function, which improves the computational efficiency. An extensive numerical study based on the real-life Belgian power and gas systems shows that the proposed methodology yields promising results in terms of accuracy and tractability.

</details>

<details>

<summary>2022-09-26 14:37:48 - Timeloops: Automatic System Call Policy Learning for Containerized Microservices</summary>

- *Meghna Pancholi, Andreas D. Kellas, Vasileios P. Kemerlis, Simha Sethumadhavan*

- `2204.06131v3` - [abs](http://arxiv.org/abs/2204.06131v3) - [pdf](http://arxiv.org/pdf/2204.06131v3)

> In this paper we introduce Timeloops a novel technique for automatically learning system call filtering policies for containerized microservices applications. At run-time, Timeloops automatically learns which system calls a program should be allowed to invoke while rejecting attempts to call spurious system calls. Further, Timeloops addresses many of the shortcomings of state-of-the-art static analysis-based techniques, such as the ability to generate tight filters for programs written in interpreted languages such as PHP, Python, and JavaScript. Timeloops has a simple and robust implementation because it is mainly built out of commodity, and proven, technologies such as seccomp-BPF, systemd, and Podman containers, with fewer than 500 lines of code. We demonstrate the utility of Timeloops by learning system calls for individual services and two microservices benchmark applications, which utilize popular technologies like Python Flask, Nginx (with PHP and Lua modules), Apache Thrift, Memcached, Redis, and MongoDB. Further, the amortized performance of Timeloops is similar to that of an unhardened system while producing a smaller system call filter than state-of-the-art static analysis-based techniques.

</details>

<details>

<summary>2022-09-26 15:07:26 - ASP-Based Declarative Process Mining (Extended Abstract)</summary>

- *Francesco Chiariello, Fabrizio Maria Maggi, Fabio Patrizi*

- `2205.01979v2` - [abs](http://arxiv.org/abs/2205.01979v2) - [pdf](http://arxiv.org/pdf/2205.01979v2)

> We propose Answer Set Programming (ASP) as an approach for modeling and solving problems from the area of Declarative Process Mining (DPM). We consider here three classical problems, namely, Log Generation, Conformance Checking, and Query Checking. These problems are addressed from both a control-flow and a data-aware perspective. The approach is based on the representation of process specifications as (finite-state) automata. Since these are strictly more expressive than the de facto DPM standard specification language DECLARE, more general specifications than those typical of DPM can be handled, such as formulas in linear-time temporal logic over finite traces. (Full version available in the Proceedings of the 36th AAAI Conference on Artificial Intelligence).

</details>

<details>

<summary>2022-09-26 16:50:30 - MSWasm: Soundly Enforcing Memory-Safe Execution of Unsafe Code</summary>

- *Alexandra E. Michael, Anitha Gollamudi, Jay Bosamiya, Craig Disselkoen, Aidan Denlinger, Conrad Watt, Bryan Parno, Marco Patrignani, Marco Vassena, Deian Stefan*

- `2208.13583v2` - [abs](http://arxiv.org/abs/2208.13583v2) - [pdf](http://arxiv.org/pdf/2208.13583v2)

> Most programs compiled to WebAssembly (Wasm) today are written in unsafe languages like C and C++. Unfortunately, memory-unsafe C code remains unsafe when compiled to Wasm -- and attackers can exploit buffer overflows and use-after-frees in Wasm almost as easily as they can on native platforms. Memory-Safe WebAssembly (MSWasm) proposes to extend Wasm with language-level memory-safety abstractions to precisely address this problem. In this paper, we build on the original MSWasm position paper to realize this vision. We give a precise and formal semantics of MSWasm, and prove that well-typed MSWasm programs are, by construction, robustly memory safe. To this end, we develop a novel, language-independent memory-safety property based on colored memory locations and pointers. This property also lets us reason about the security guarantees of a formal C-to-MSWasm compiler -- and prove that it always produces memory-safe programs (and preserves the semantics of safe programs). We use these formal results to then guide several implementations: Two compilers of MSWasm to native code, and a C-to-MSWasm compiler (that extends Clang). Our MSWasm compilers support different enforcement mechanisms, allowing developers to make security-performance trade-offs according to their needs. Our evaluation shows that the overhead of enforcing memory safety in software ranges from 22% (enforcing spatial safety alone) to 198% (enforcing full memory safety) on the PolyBenchC suite. More importantly, MSWasm's design makes it easy to swap between enforcement mechanisms; as fast (especially hardware-based) enforcement techniques become available, MSWasm will be able to take advantage of these advances almost for free.

</details>

<details>

<summary>2022-09-26 19:56:46 - Graph clustering with Boltzmann machines</summary>

- *Pierre Miasnikof, Mohammad Bagherbeik, Ali Sheikholeslami*

- `2203.02471v3` - [abs](http://arxiv.org/abs/2203.02471v3) - [pdf](http://arxiv.org/pdf/2203.02471v3)

> Graph clustering is the process of grouping vertices into densely connected sets called clusters. We tailor two mathematical programming formulations from the literature, to this problem. In doing so, we obtain a heuristic approximation to the intra-cluster density maximization problem. We use two variations of a Boltzmann machine heuristic to obtain numerical solutions. For benchmarking purposes, we compare solution quality and computational performances to those obtained using a commercial solver, Gurobi. We also compare clustering quality to the clusters obtained using the popular Louvain modularity maximization method. Our initial results clearly demonstrate the superiority of our problem formulations. They also establish the superiority of the Boltzmann machine over the traditional exact solver. In the case of smaller less complex graphs, Boltzmann machines provide the same solutions as Gurobi, but with solution times that are orders of magnitude lower. In the case of larger and more complex graphs, Gurobi fails to return meaningful results within a reasonable time frame. Finally, we also note that both our clustering formulations, the distance minimization and $K$-medoids, yield clusters of superior quality to those obtained with the Louvain algorithm.

</details>

<details>

<summary>2022-09-26 21:53:01 - An Application of a Runtime Epistemic Probabilistic Event Calculus to Decision-making in e-Health Systems</summary>

- *Fabio Aurelio D'Asaro, Luca Raggioli, Salim Malek, Marco Grazioso, Silvia Rossi*

- `2209.13043v1` - [abs](http://arxiv.org/abs/2209.13043v1) - [pdf](http://arxiv.org/pdf/2209.13043v1)

> We present and discuss a runtime architecture that integrates sensorial data and classifiers with a logic-based decision-making system in the context of an e-Health system for the rehabilitation of children with neuromotor disorders. In this application, children perform a rehabilitation task in the form of games. The main aim of the system is to derive a set of parameters the child's current level of cognitive and behavioral performance (e.g., engagement, attention, task accuracy) from the available sensors and classifiers (e.g., eye trackers, motion sensors, emotion recognition techniques) and take decisions accordingly. These decisions are typically aimed at improving the child's performance by triggering appropriate re-engagement stimuli when their attention is low, by changing the game or making it more difficult when the child is losing interest in the task as it is too easy. Alongside state-of-the-art techniques for emotion recognition and head pose estimation, we use a runtime variant of a probabilistic and epistemic logic programming dialect of the Event Calculus, known as the Epistemic Probabilistic Event Calculus. In particular, the probabilistic component of this symbolic framework allows for a natural interface with the machine learning techniques. We overview the architecture and its components, and show some of its characteristics through a discussion of a running example and experiments. Under consideration for publication in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2022-09-27 01:30:47 - Using Multiple Code Representations to Prioritize Static Analysis Warnings</summary>

- *Thanh Trong Vu, Hieu Dinh Vo*

- `2209.12181v2` - [abs](http://arxiv.org/abs/2209.12181v2) - [pdf](http://arxiv.org/pdf/2209.12181v2)

> In order to ensure the quality of software and prevent attacks from hackers on critical systems, static analysis tools are frequently utilized to detect vulnerabilities in the early development phase. However, these tools often report a large number of warnings with a high false-positive rate, which causes many difficulties for developers. In this paper, we introduce VulRG, a novel approach to address this problem. Specifically, VulRG predicts and ranks the warnings based on their likelihoods to be true positive. To predict that likelihood, VulRG combines two deep learning models CNN and BiGRU to capture the context of each warning in terms of program syntax, control flow, and program dependence. Our experimental results on a real-world dataset of 6,620 warnings show that VulRG's Recall at Top-50% is 90.9%. This means that using VulRG, 90% of the vulnerabilities can be found by examining only 50% of the warnings. Moreover, at Top-5%, VulRG can improve the state-of-the-art approach by +30% in both Precision and Recall.

</details>

<details>

<summary>2022-09-27 07:32:26 - Lossy compression of matrices by black-box optimisation of mixed integer nonlinear programming</summary>

- *Tadashi Kadowaki, Mitsuru Ambai*

- `2204.10579v2` - [abs](http://arxiv.org/abs/2204.10579v2) - [pdf](http://arxiv.org/pdf/2204.10579v2)

> In edge computing, suppressing data size is a challenge for machine learning models that perform complex tasks such as autonomous driving, in which computational resources (speed, memory size and power) are limited. Efficient lossy compression of matrix data has been introduced by decomposing it into the product of an integer and real matrices. However, its optimisation is difficult as it requires simultaneous optimisation of an integer and real variables. In this paper, we improve this optimisation by utilising recently developed black-box optimisation (BBO) algorithms with an Ising solver for integer variables. In addition, the algorithm can be used to solve mixed-integer programming problems that are linear and non-linear in terms of real and integer variables, respectively. The differences between the choice of Ising solvers (simulated annealing, quantum annealing and simulated quenching) and the strategies of the BBO algorithms (BOCS, FMQA and their variations) are discussed for further development of the BBO techniques.

</details>

<details>

<summary>2022-09-27 08:10:16 - Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient Image Classification</summary>

- *Ying Bi, Bing Xue, Mengjie Zhang*

- `2209.13233v1` - [abs](http://arxiv.org/abs/2209.13233v1) - [pdf](http://arxiv.org/pdf/2209.13233v1)

> Data-efficient image classification is a challenging task that aims to solve image classification using small training data. Neural network-based deep learning methods are effective for image classification, but they typically require large-scale training data and have major limitations such as requiring expertise to design network architectures and having poor interpretability. Evolutionary deep learning is a recent hot topic that combines evolutionary computation with deep learning. However, most evolutionary deep learning methods focus on evolving architectures of neural networks, which still suffer from limitations such as poor interpretability. To address this, this paper proposes a new genetic programming-based evolutionary deep learning approach to data-efficient image classification. The new approach can automatically evolve variable-length models using many important operators from both image and classification domains. It can learn different types of image features from colour or gray-scale images, and construct effective and diverse ensembles for image classification. A flexible multi-layer representation enables the new approach to automatically construct shallow or deep models/trees for different tasks and perform effective transformations on the input data via multiple internal nodes. The new approach is applied to solve five image classification tasks with different training set sizes. The results show that it achieves better performance in most cases than deep learning methods for data-efficient image classification. A deep analysis shows that the new approach has good convergence and evolves models with high interpretability, different lengths/sizes/shapes, and good transferability.

</details>

<details>

<summary>2022-09-27 12:14:33 - EqFix: Fixing LaTeX Equation Errors by Examples</summary>

- *Fengmin Zhu, Fei He*

- `2107.00613v2` - [abs](http://arxiv.org/abs/2107.00613v2) - [pdf](http://arxiv.org/pdf/2107.00613v2)

> LaTeX is a widely-used document preparation system. Its powerful ability in mathematical equation editing is perhaps the main reason for its popularity in academia. Sometimes, however, even an expert user may spend much time fixing an erroneous equation. In this paper, we present EqFix, a synthesis-based repairing system for LaTeX equations. It employs a set of fixing rules and can suggest possible repairs for common errors in LaTeX equations. A domain-specific language is proposed for formally expressing the fixing rules. The fixing rules can be automatically synthesized from a set of input-output examples. An extension of relaxers is also introduced to enhance the practicality of EqFix. We evaluate EqFix on real-world examples and find that it can synthesize rules with high generalization ability. Compared with a state-of-the-art string transformation synthesizer, EqFix solved 37% more cases and spent less than half of their synthesis time.

</details>

<details>

<summary>2022-09-27 14:08:19 - A Pathologist-Informed Workflow for Classification of Prostate Glands in Histopathology</summary>

- *Alessandro Ferrero, Beatrice Knudsen, Deepika Sirohi, Ross Whitaker*

- `2209.13408v1` - [abs](http://arxiv.org/abs/2209.13408v1) - [pdf](http://arxiv.org/pdf/2209.13408v1)

> Pathologists diagnose and grade prostate cancer by examining tissue from needle biopsies on glass slides. The cancer's severity and risk of metastasis are determined by the Gleason grade, a score based on the organization and morphology of prostate cancer glands. For diagnostic work-up, pathologists first locate glands in the whole biopsy core, and -- if they detect cancer -- they assign a Gleason grade. This time-consuming process is subject to errors and significant inter-observer variability, despite strict diagnostic criteria. This paper proposes an automated workflow that follows pathologists' \textit{modus operandi}, isolating and classifying multi-scale patches of individual glands in whole slide images (WSI) of biopsy tissues using distinct steps: (1) two fully convolutional networks segment epithelium versus stroma and gland boundaries, respectively; (2) a classifier network separates benign from cancer glands at high magnification; and (3) an additional classifier predicts the grade of each cancer gland at low magnification. Altogether, this process provides a gland-specific approach for prostate cancer grading that we compare against other machine-learning-based grading methods.

</details>

<details>

<summary>2022-09-27 20:01:12 - CEC-CNN: A Consecutive Expansion-Contraction Convolutional Network for Very Small Resolution Medical Image Classification</summary>

- *Ioannis Vezakis, Antonios Vezakis, Sofia Gourtsoyianni, Vassilis Koutoulidis, George K. Matsopoulos, Dimitrios Koutsouris*

- `2209.13661v1` - [abs](http://arxiv.org/abs/2209.13661v1) - [pdf](http://arxiv.org/pdf/2209.13661v1)

> Deep Convolutional Neural Networks (CNNs) for image classification successively alternate convolutions and downsampling operations, such as pooling layers or strided convolutions, resulting in lower resolution features the deeper the network gets. These downsampling operations save computational resources and provide some translational invariance as well as a bigger receptive field at the next layers. However, an inherent side-effect of this is that high-level features, produced at the deep end of the network, are always captured in low resolution feature maps. The inverse is also true, as shallow layers always contain small scale features. In biomedical image analysis engineers are often tasked with classifying very small image patches which carry only a limited amount of information. By their nature, these patches may not even contain objects, with the classification depending instead on the detection of subtle underlying patterns with an unknown scale in the image's texture. In these cases every bit of information is valuable; thus, it is important to extract the maximum number of informative features possible. Driven by these considerations, we introduce a new CNN architecture which preserves multi-scale features from deep, intermediate, and shallow layers by utilizing skip connections along with consecutive contractions and expansions of the feature maps. Using a dataset of very low resolution patches from Pancreatic Ductal Adenocarcinoma (PDAC) CT scans we demonstrate that our network can outperform current state of the art models.

</details>

<details>

<summary>2022-09-28 06:03:43 - Deep Learning based Automatic Quantification of Urethral Plate Quality using the Plate Objective Scoring Tool (POST)</summary>

- *Tariq O. Abbas, Mohamed AbdelMoniem, Ibrahim Khalil, Md Sakib Abrar Hossain, Muhammad E. H. Chowdhury*

- `2209.13848v1` - [abs](http://arxiv.org/abs/2209.13848v1) - [pdf](http://arxiv.org/pdf/2209.13848v1)

> Objectives: To explore the capacity of deep learning algorithm to further streamline and optimize urethral plate (UP) quality appraisal on 2D images using the plate objective scoring tool (POST), aiming to increase the objectivity and reproducibility of UP appraisal in hypospadias repair. Methods: The five key POST landmarks were marked by specialists in a 691-image dataset of prepubertal boys undergoing primary hypospadias repair. This dataset was then used to develop and validate a deep learning-based landmark detection model. The proposed framework begins with glans localization and detection, where the input image is cropped using the predicted bounding box. Next, a deep convolutional neural network (CNN) architecture is used to predict the coordinates of the five POST landmarks. These predicted landmarks are then used to assess UP quality in distal hypospadias. Results: The proposed model accurately localized the glans area, with a mean average precision (mAP) of 99.5% and an overall sensitivity of 99.1%. A normalized mean error (NME) of 0.07152 was achieved in predicting the coordinates of the landmarks, with a mean squared error (MSE) of 0.001 and a 20.2% failure rate at a threshold of 0.1 NME. Conclusions: This deep learning application shows robustness and high precision in using POST to appraise UP quality. Further assessment using international multi-centre image-based databases is ongoing. External validation could benefit deep learning algorithms and lead to better assessments, decision-making and predictions for surgical outcomes.

</details>

<details>

<summary>2022-09-28 10:55:25 - Revealing the Semantics of Data Wrangling Scripts With COMANTICS</summary>

- *Kai Xiong, Zhongsu Luo, Siwei Fu, Yongheng Wang, Mingliang Xu, Yingcai Wu*

- `2209.13995v1` - [abs](http://arxiv.org/abs/2209.13995v1) - [pdf](http://arxiv.org/pdf/2209.13995v1)

> Data workers usually seek to understand the semantics of data wrangling scripts in various scenarios, such as code debugging, reusing, and maintaining. However, the understanding is challenging for novice data workers due to the variety of programming languages, functions, and parameters. Based on the observation that differences between input and output tables highly relate to the type of data transformation, we outline a design space including 103 characteristics to describe table differences. Then, we develop COMANTICS, a three-step pipeline that automatically detects the semantics of data transformation scripts. The first step focuses on the detection of table differences for each line of wrangling code. Second, we incorporate a characteristic-based component and a Siamese convolutional neural network-based component for the detection of transformation types. Third, we derive the parameters of each data transformation by employing a "slot filling" strategy. We design experiments to evaluate the performance of COMANTICS. Further, we assess its flexibility using three example applications in different domains.

</details>

<details>

<summary>2022-09-28 12:55:26 - Topological Data Analysis in Time Series: Temporal Filtration and Application to Single-Cell Genomics</summary>

- *Baihan Lin*

- `2204.14048v2` - [abs](http://arxiv.org/abs/2204.14048v2) - [pdf](http://arxiv.org/pdf/2204.14048v2)

> The absence of a conventional association between the cell-cell cohabitation and its emergent dynamics into cliques during development has hindered our understanding of how cell populations proliferate, differentiate, and compete, i.e. the cell ecology. With the recent advancement of the single-cell RNA-sequencing (RNA-seq), we can potentially describe such a link by constructing network graphs that characterize the similarity of the gene expression profiles of the cell-specific transcriptional programs, and analyzing these graphs systematically using the summary statistics informed by the algebraic topology. We propose the single-cell topological simplicial analysis (scTSA). Applying this approach to the single-cell gene expression profiles from local networks of cells in different developmental stages with different outcomes reveals a previously unseen topology of cellular ecology. These networks contain an abundance of cliques of single-cell profiles bound into cavities that guide the emergence of more complicated habitation forms. We visualize these ecological patterns with topological simplicial architectures of these networks, compared with the null models. Benchmarked on the single-cell RNA-seq data of zebrafish embryogenesis spanning 38,731 cells, 25 cell types and 12 time steps, our approach highlights the gastrulation as the most critical stage, consistent with consensus in developmental biology. As a nonlinear, model-independent, and unsupervised framework, our approach can also be applied to tracing multi-scale cell lineage, identifying critical stages, or creating pseudo-time series.

</details>

<details>

<summary>2022-09-28 13:13:26 - Towards Auditable Distributed Systems</summary>

- *Lev Sorokin*

- `2209.14071v1` - [abs](http://arxiv.org/abs/2209.14071v1) - [pdf](http://arxiv.org/pdf/2209.14071v1)

> The emerging trend towards distributed (cloud) systems (DS) has widely arrived whether in the automotive, public or the financial sector, but the execution of services of heterogeneous service providers is exposed to several risks. Beside hardware/software faults or cyber attacks that can influence the correctness of the system, fraud is also an issue. In such case it is not only important to verify the correctness of the system, but also have evidence which component and participant behaves faulty. This makes it possible, e.g. to claim for compensation after systems execution but also to assure information for verification can be trusted. The main goal of our research is to assure the monitoring of DS based on auditable information. We follow a decentralized monitoring strategy and envision a distributed monitoring approach of system properties based on distributedlogic programs that consider auditability. The expected contribution of this work is to establish with the application of our framework the mutual trust of distributed parties, as well as trust of clients in the systems execution. We showcase our ideas on a DS for booking services with unmanned air vehicles.

</details>

<details>

<summary>2022-09-28 16:24:42 - Distance-based Positive and Unlabeled Learning for Ranking</summary>

- *Hayden S. Helm, Amitabh Basu, Avanti Athreya, Youngser Park, Joshua T. Vogelstein, Carey E. Priebe, Michael Winding, Marta Zlatic, Albert Cardona, Patrick Bourke, Jonathan Larson, Marah Abdin, Piali Choudhury, Weiwei Yang, Christopher W. White*

- `2005.10700v3` - [abs](http://arxiv.org/abs/2005.10700v3) - [pdf](http://arxiv.org/pdf/2005.10700v3)

> Learning to rank -- producing a ranked list of items specific to a query and with respect to a set of supervisory items -- is a problem of general interest. The setting we consider is one in which no analytic description of what constitutes a good ranking is available. Instead, we have a collection of representations and supervisory information consisting of a (target item, interesting items set) pair. We demonstrate analytically, in simulation, and in real data examples that learning to rank via combining representations using an integer linear program is effective when the supervision is as light as "these few items are similar to your item of interest." While this nomination task is quite general, for specificity we present our methodology from the perspective of vertex nomination in graphs. The methodology described herein is model agnostic.

</details>

<details>

<summary>2022-09-28 19:29:39 - Does Collaborative Editing Help Mitigate Security Vulnerabilities in Crowd-Shared IoT Code Examples?</summary>

- *Madhu Selvaraj, Gias Uddin*

- `2209.15011v1` - [abs](http://arxiv.org/abs/2209.15011v1) - [pdf](http://arxiv.org/pdf/2209.15011v1)

> Background: With the proliferation of crowd-sourced developer forums, software developers are increasingly sharing more coding solutions to programming problems with others in forums. The decentralized nature of knowledge sharing on sites has raised the concern of sharing security vulnerable code, which then can be reused into mission critical software systems - making those systems vulnerable in the process. Collaborative editing has been introduced in forums like Stack Overflow to improve the quality of the shared contents. Aim: In this paper, we investigate whether code editing can mitigate shared vulnerable code examples by analyzing IoT code snippets and their revisions in three Stack Exchange sites: Stack Overflow, Arduino, and Raspberry Pi. Method:We analyze the vulnerabilities present in shared IoT C/C++ code snippets, as C/C++ is one of the most widely used languages in mission-critical devices and low-powered IoT devices. We further analyse the revisions made to these code snippets, and their effects. Results: We find several vulnerabilities such as CWE 788 - Access of Memory Location After End of Buffer, in 740 code snippets . However, we find the vast majority of posts are not revised, or revisions are not made to the code snippets themselves (598 out of 740). We also find that revisions are most likely to result in no change to the number of vulnerabilities in a code snippet rather than deteriorating or improving the snippet. Conclusions: We conclude that the current collaborative editing system in the forums may be insufficient to help mitigate vulnerabilities in the shared code.

</details>

<details>

<summary>2022-09-29 00:29:43 - Temporal Analysis and Gender Bias in Computing</summary>

- *Thomas J. Misa*

- `2210.08983v1` - [abs](http://arxiv.org/abs/2210.08983v1) - [pdf](http://arxiv.org/pdf/2210.08983v1)

> Recent studies of gender bias in computing use large datasets involving automatic predictions of gender to analyze computing publications, conferences, and other key populations. Gender bias is partly defined by software-driven algorithmic analysis, but widely used gender prediction tools can result in unacknowledged gender bias when used for historical research. Many names change ascribed gender over decades: the "Leslie problem." Systematic analysis of the Social Security Administration dataset -- each year, all given names, identified by ascribed gender and frequency of use -- in 1900, 1925, 1950, 1975, and 2000 permits a rigorous assessment of the "Leslie problem." This article identifies 300 given names with measurable "gender shifts" across 1925-1975, spotlighting the 50 given names with the largest such shifts. This article demonstrates, quantitatively, there is net "female shift" that likely results in the overcounting of women (and undercounting of men) in earlier decades, just as computer science was professionalizing. Some aspects of the widely accepted 'making programming masculine' perspective may need revision.

</details>

<details>

<summary>2022-09-29 12:47:11 - FastPacket: Towards Pre-trained Packets Embedding based on FastText for next-generation NIDS</summary>

- *Khloud Al Jallad*

- `2209.14727v1` - [abs](http://arxiv.org/abs/2209.14727v1) - [pdf](http://arxiv.org/pdf/2209.14727v1)

> New Attacks are increasingly used by attackers everyday but many of them are not detected by Intrusion Detection Systems as most IDS ignore raw packet information and only care about some basic statistical information extracted from PCAP files. Using networking programs to extract fixed statistical features from packets is good, but may not enough to detect nowadays challenges. We think that it is time to utilize big data and deep learning for automatic dynamic feature extraction from packets. It is time to get inspired by deep learning pre-trained models in computer vision and natural language processing, so security deep learning solutions will have its pre-trained models on big datasets to be used in future researches. In this paper, we proposed a new approach for embedding packets based on character-level embeddings, inspired by FastText success on text data. We called this approach FastPacket. Results are measured on subsets of CIC-IDS-2017 dataset, but we expect promising results on big data pre-trained models. We suggest building pre-trained FastPacket on MAWI big dataset and make it available to community, similar to FastText. To be able to outperform currently used NIDS, to start a new era of packet-level NIDS that can better detect complex attacks.

</details>

<details>

<summary>2022-09-29 15:41:17 - Repairing Bugs in Python Assignments Using Large Language Models</summary>

- *Jialu Zhang, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, Gust Verbruggen*

- `2209.14876v1` - [abs](http://arxiv.org/abs/2209.14876v1) - [pdf](http://arxiv.org/pdf/2209.14876v1)

> Students often make mistakes on their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex, to build an APR system -- MMAPR -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate MMAPR on 286 real student programs and compare to a baseline built by combining a state-of-the-art Python syntax repair engine, BIFI, and state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that MMAPR can fix more programs and produce smaller patches on average.

</details>

<details>

<summary>2022-09-29 19:20:24 - Reasoning about Complex Networks: A Logic Programming Approach</summary>

- *Paulo Shakarian, Gerardo I. Simari, Devon Callahan*

- `2209.15067v1` - [abs](http://arxiv.org/abs/2209.15067v1) - [pdf](http://arxiv.org/pdf/2209.15067v1)

> Reasoning about complex networks has in recent years become an important topic of study due to its many applications: the adoption of commercial products, spread of disease, the diffusion of an idea, etc. In this paper, we present the MANCaLog language, a formalism based on logic programming that satisfies a set of desiderata proposed in previous work as recommendations for the development of approaches to reasoning in complex networks. To the best of our knowledge, this is the first formalism that satisfies all such criteria. We first focus on algorithms for finding minimal models (on which multi-attribute analysis can be done), and then on how this formalism can be applied in certain real world scenarios. Towards this end, we study the problem of deciding group membership in social networks: given a social network and a set of groups where group membership of only some of the individuals in the network is known, we wish to determine a degree of membership for the remaining group-individual pairs. We develop a prototype implementation that we use to obtain experimental results on two real world datasets, including a current social network of criminal gangs in a major U.S.\ city. We then show how the assignment of degree of membership to nodes in this case allows for a better understanding of the criminal gang problem when combined with other social network mining techniques -- including detection of sub-groups and identification of core group members -- which would not be possible without further identification of additional group members.

</details>

<details>

<summary>2022-09-29 20:11:56 - AutoMTL: A Programming Framework for Automating Efficient Multi-Task Learning</summary>

- *Lijun Zhang, Xiao Liu, Hui Guan*

- `2110.13076v3` - [abs](http://arxiv.org/abs/2110.13076v3) - [pdf](http://arxiv.org/pdf/2110.13076v3)

> Multi-task learning (MTL) jointly learns a set of tasks by sharing parameters among tasks. It is a promising approach for reducing storage costs while improving task accuracy for many computer vision tasks. The effective adoption of MTL faces two main challenges. The first challenge is to determine what parameters to share across tasks to optimize for both memory efficiency and task accuracy. The second challenge is to automatically apply MTL algorithms to an arbitrary CNN backbone without requiring time-consuming manual re-implementation and significant domain expertise. This paper addresses the challenges by developing the first programming framework AutoMTL that automates efficient MTL model development for vision tasks. AutoMTL takes as inputs an arbitrary backbone convolutional neural network (CNN) and a set of tasks to learn, and automatically produces a multi-task model that achieves high accuracy and small memory footprint simultaneously. Experiments on three popular MTL benchmarks (CityScapes, NYUv2, Tiny-Taskonomy) demonstrate the effectiveness of AutoMTL over state-of-the-art approaches as well as the generalizability of AutoMTL across CNNs. AutoMTL is open-sourced and available at https://github.com/zhanglijun95/AutoMTL.

</details>

<details>

<summary>2022-09-29 23:23:38 - Modeling driver's evasive behavior during safety-critical lane changes:Two-dimensional time-to-collision and deep reinforcement learning</summary>

- *Hongyu Guo, Kun Xie, Mehdi Keyvan-Ekbatani*

- `2209.15133v1` - [abs](http://arxiv.org/abs/2209.15133v1) - [pdf](http://arxiv.org/pdf/2209.15133v1)

> Lane changes are complex driving behaviors and frequently involve safety-critical situations. This study aims to develop a lane-change-related evasive behavior model, which can facilitate the development of safety-aware traffic simulations and predictive collision avoidance systems. Large-scale connected vehicle data from the Safety Pilot Model Deployment (SPMD) program were used for this study. A new surrogate safety measure, two-dimensional time-to-collision (2D-TTC), was proposed to identify the safety-critical situations during lane changes. The validity of 2D-TTC was confirmed by showing a high correlation between the detected conflict risks and the archived crashes. A deep deterministic policy gradient (DDPG) algorithm, which could learn the sequential decision-making process over continuous action spaces, was used to model the evasive behaviors in the identified safety-critical situations. The results showed the superiority of the proposed model in replicating both the longitudinal and lateral evasive behaviors.

</details>

<details>

<summary>2022-09-30 03:17:38 - A forensic analysis of the Google Home: repairing compressed data without error correction</summary>

- *Hadrien Barral, Georges-Axel Jaloyan, Fabien Thomas-Brans, Matthieu Regnery, RÃ©mi GÃ©raud-Stewart, Thibaut Heckmann, Thomas Souvignet, David Naccache*

- `2210.00856v1` - [abs](http://arxiv.org/abs/2210.00856v1) - [pdf](http://arxiv.org/pdf/2210.00856v1)

> This paper provides a detailed explanation of the steps taken to extract and repair a Google Home's internal data. Starting with reverse engineering the hardware of a commercial off-the-shelf Google Home, internal data is then extracted by desoldering and dumping the flash memory. As error correction is performed by the CPU using an undisclosed method, a new alternative method is shown to repair a corrupted SquashFS filesystem, under the assumption of a single or double bitflip per gzip-compressed fragment. Finally, a new method to handle multiple possible repairs using three-valued logic is presented.

</details>

<details>

<summary>2022-09-30 06:38:10 - A Multiple Criteria Decision Analysis based Approach to Remove Uncertainty in SMP Models</summary>

- *Gokul Yenduri, Thippa Reddy Gadekallu*

- `2209.15260v1` - [abs](http://arxiv.org/abs/2209.15260v1) - [pdf](http://arxiv.org/pdf/2209.15260v1)

> Advanced AI technologies are serving humankind in a number of ways, from healthcare to manufacturing. Advanced automated machines are quite expensive, but the end output is supposed to be of the highest possible quality. Depending on the agility of requirements, these automation technologies can change dramatically. The likelihood of making changes to automation software is extremely high, so it must be updated regularly. If maintainability is not taken into account, it will have an impact on the entire system and increase maintenance costs. Many companies use different programming paradigms in developing advanced automated machines based on client requirements. Therefore, it is essential to estimate the maintainability of heterogeneous software. As a result of the lack of widespread consensus on software maintainability prediction (SPM) methodologies, individuals and businesses are left perplexed when it comes to determining the appropriate model for estimating the maintainability of software, which serves as the inspiration for this research. A structured methodology was designed, and the datasets were preprocessed and maintainability index (MI) range was also found for all the datasets expect for UIMS and QUES, the metric CHANGE is used for UIMS and QUES. To remove the uncertainty among the aforementioned techniques, a popular multiple criteria decision-making model, namely the technique for order preference by similarity to ideal solution (TOPSIS), is used in this work. TOPSIS revealed that GARF outperforms the other considered techniques in predicting the maintainability of heterogeneous automated software.

</details>

<details>

<summary>2022-09-30 07:34:22 - Contextual Bandits with Knapsacks for a Conversion Model</summary>

- *Zhen Li, Gilles Stoltz*

- `2206.00314v2` - [abs](http://arxiv.org/abs/2206.00314v2) - [pdf](http://arxiv.org/pdf/2206.00314v2)

> We consider contextual bandits with knapsacks, with an underlying structure between rewards generated and cost vectors suffered. We do so motivated by sales with commercial discounts. At each round, given the stochastic i.i.d.\ context $\mathbf{x}_t$ and the arm picked $a_t$ (corresponding, e.g., to a discount level), a customer conversion may be obtained, in which case a reward $r(a,\mathbf{x}_t)$ is gained and vector costs $c(a_t,\mathbf{x}_t)$ are suffered (corresponding, e.g., to losses of earnings). Otherwise, in the absence of a conversion, the reward and costs are null. The reward and costs achieved are thus coupled through the binary variable measuring conversion or the absence thereof. This underlying structure between rewards and costs is different from the linear structures considered by Agrawal and Devanur [2016] (but we show that the techniques introduced in the present article may also be applied to the case of these linear structures). The adaptive policies exhibited solve at each round a linear program based on upper-confidence estimates of the probabilities of conversion given $a$ and $\mathbf{x}$. This kind of policy is most natural and achieves a regret bound of the typical order (OPT/$B$) $\sqrt{T}$, where $B$ is the total budget allowed, OPT is the optimal expected reward achievable by a static policy, and $T$ is the number of rounds.

</details>

<details>

<summary>2022-09-30 08:41:29 - Smooth Bilevel Programming for Sparse Regularization</summary>

- *Clarice Poon, Gabriel PeyrÃ©*

- `2106.01429v2` - [abs](http://arxiv.org/abs/2106.01429v2) - [pdf](http://arxiv.org/pdf/2106.01429v2)

> Iteratively reweighted least square (IRLS) is a popular approach to solve sparsity-enforcing regression problems in machine learning. State of the art approaches are more efficient but typically rely on specific coordinate pruning schemes. In this work, we show how a surprisingly simple reparametrization of IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is able to achieve top performances on a wide range of sparsity (such as Lasso, group Lasso and trace norm regularizations), regularization strength (including hard constraints), and design matrices (ranging from correlated designs to differential operators). Similarly to IRLS, our method only involves linear systems resolutions, but in sharp contrast, corresponds to the minimization of a smooth function. Despite being non-convex, we show that there is no spurious minima and that saddle points are "ridable", so that there always exists a descent direction. We thus advocate for the use of a BFGS quasi-Newton solver, which makes our approach simple, robust and efficient. We perform a numerical benchmark of the convergence speed of our algorithm against state of the art solvers for Lasso, group Lasso, trace norm and linearly constrained problems. These results highlight the versatility of our approach, removing the need to use different solvers depending on the specificity of the ML problem under study.

</details>

<details>

<summary>2022-09-30 12:42:18 - Empowering the trustworthiness of ML-based critical systems through engineering activities</summary>

- *Juliette Mattioli, Agnes Delaborde, Souhaiel Khalfaoui, Freddy Lecue, Henri Sohier, Frederic Jurie*

- `2209.15438v1` - [abs](http://arxiv.org/abs/2209.15438v1) - [pdf](http://arxiv.org/pdf/2209.15438v1)

> This paper reviews the entire engineering process of trustworthy Machine Learning (ML) algorithms designed to equip critical systems with advanced analytics and decision functions. We start from the fundamental principles of ML and describe the core elements conditioning its trust, particularly through its design: namely domain specification, data engineering, design of the ML algorithms, their implementation, evaluation and deployment. The latter components are organized in an unique framework for the design of trusted ML systems.

</details>

<details>

<summary>2022-09-30 13:05:03 - Road Network Deterioration Monitoring Using Aerial Images and Computer Vision</summary>

- *Nicolas Parra-A, Vladimir Vargas-CalderÃ³n, Herbert Vinck-Posada, Nicanor Vinck*

- `2209.15455v1` - [abs](http://arxiv.org/abs/2209.15455v1) - [pdf](http://arxiv.org/pdf/2209.15455v1)

> Road maintenance is an essential process for guaranteeing the quality of transportation in any city. A crucial step towards effective road maintenance is the ability to update the inventory of the road network. We present a proof of concept of a protocol for maintaining said inventory based on the use of unmanned aerial vehicles to quickly collect images which are processed by a computer vision program that automatically identifies potholes and their severity. Our protocol aims to provide information to local governments to prioritise the road network maintenance budget, and to be able to detect early stages of road deterioration so as to minimise maintenance expenditure.

</details>

<details>

<summary>2022-09-30 17:46:49 - Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction</summary>

- *Rahul Bera, Konstantinos Kanellopoulos, Shankar Balachandran, David Novo, Ataberk Olgun, Mohammad Sadrosadati, Onur Mutlu*

- `2209.00188v3` - [abs](http://arxiv.org/abs/2209.00188v3) - [pdf](http://arxiv.org/pdf/2209.00188v3)

> Long-latency load requests continue to limit the performance of high-performance processors. To increase the latency tolerance of a processor, architects have primarily relied on two key techniques: sophisticated data prefetchers and large on-chip caches. In this work, we show that: 1) even a sophisticated state-of-the-art prefetcher can only predict half of the off-chip load requests on average across a wide range of workloads, and 2) due to the increasing size and complexity of on-chip caches, a large fraction of the latency of an off-chip load request is spent accessing the on-chip cache hierarchy. The goal of this work is to accelerate off-chip load requests by removing the on-chip cache access latency from their critical path. To this end, we propose a new technique called Hermes, whose key idea is to: 1) accurately predict which load requests might go off-chip, and 2) speculatively fetch the data required by the predicted off-chip loads directly from the main memory, while also concurrently accessing the cache hierarchy for such loads. To enable Hermes, we develop a new lightweight, perceptron-based off-chip load prediction technique that learns to identify off-chip load requests using multiple program features (e.g., sequence of program counters). For every load request, the predictor observes a set of program features to predict whether or not the load would go off-chip. If the load is predicted to go off-chip, Hermes issues a speculative request directly to the memory controller once the load's physical address is generated. If the prediction is correct, the load eventually misses the cache hierarchy and waits for the ongoing speculative request to finish, thus hiding the on-chip cache hierarchy access latency from the critical path of the off-chip load. Our evaluation shows that Hermes significantly improves performance of a state-of-the-art baseline. We open-source Hermes.

</details>

<details>

<summary>2022-09-30 18:51:24 - Fair and Reliable Reconnections for Temporary Disruptions in Electric Distribution Networks using Submodularity</summary>

- *Cyrus Hettle, Swati Gupta, Daniel Molzahn*

- `2104.07631v3` - [abs](http://arxiv.org/abs/2104.07631v3) - [pdf](http://arxiv.org/pdf/2104.07631v3)

> Increasing reliability and reducing disruptions in supply networks are of increasing importance; for example, power outages in electricity distribution networks cost \$35-50 billion annually in the US. Motivated by the operational constraints of such networks and their rapid adoption of decentralized paradigms and self-healing components, we introduce the "minimum reconnection time" (MRT) problem. MRT seeks to reduce outage time after network disruptions by programming reconnection times of different edges (i.e., switches), while ensuring that the operating network is acyclic.   We show that MRT is NP-hard and is a special case of the well-known minimum linear ordering problem (MLOP) in the submodular optimization literature. MLOP is a special case of a broader class of ordering problems that often admit polynomial time approximation algorithms. We develop the theory of kernel-based randomized rounding approaches to give a tight polynomial-time approximation for MRT, improving the state-of-the-art approximation factor for a broad class of MLOP instances. Further, motivated by the reliability incentive structure for utility companies and operational energy losses in distribution networks, we propose local search over spanning trees to balance multiple objectives simultaneously. We computationally validate our reconfiguration methods on the NREL SMART-DS Greensboro synthetic network, and show that this improves service equity by a factor of four, across industrial and residential areas.

</details>


## 2022-10

<details>

<summary>2022-10-01 10:07:28 - Software system rationalisation: How to get better outcomes through stronger user engagement</summary>

- *Richard Shute, Nick Lynch*

- `2210.00236v1` - [abs](http://arxiv.org/abs/2210.00236v1) - [pdf](http://arxiv.org/pdf/2210.00236v1)

> As businesses get more sizable and more mature they now, inevitably accrete more and more software systems. This estate expansion leads not only to greater complexity and expense for the enterprise, but also to fragmentation, inconsistency and siloing of business processes. Because platform rationalisation and system decommissioning never happens spontaneously, a perennial problem for the enterprise then becomes how to simplify their corporate software platforms. Recently, Curlew Research personnel were involved in a software rationalisation program within a large global life sciences company and this paper describes an approach to decommissioning which we developed as part of that project, and which we feel could be of use more widely to help with objective more user-centric system rationalisation. The method derives from a model developed by Noriaki Kano et al to help with determining customer satisfaction and loyalty, and the prioritisation of new, additional functionality, features or "products", for example when looking to enhance software applications. Using a blueprint process for rationalisation, the Curlew-Kano method enables each application to be placed efficiently and objectively into one of four categories - Retain; Review; Remove; Research - thus allowing the enterprise to identify and prioritise quickly those systems which warrant further investigation as part of a decommissioning activity. The key difference of the Curlew-Kano method compared to other application rationalisation methodologies is the fundamental involvement of users in the identification of systems more suitable for rationalisation and possible decommissioning. In our view involving users more fully in system rationalisation leads to better outcomes for the enterprise.

</details>

<details>

<summary>2022-10-01 13:54:54 - Construction and Evaluation of a Self-Attention Model for Semantic Understanding of Sentence-Final Particles</summary>

- *Shuhei Mandokoro, Natsuki Oka, Akane Matsushima, Chie Fukada, Yuko Yoshimura, Koji Kawahara, Kazuaki Tanaka*

- `2210.00282v1` - [abs](http://arxiv.org/abs/2210.00282v1) - [pdf](http://arxiv.org/pdf/2210.00282v1)

> Sentence-final particles serve an essential role in spoken Japanese because they express the speaker's mental attitudes toward a proposition and/or an interlocutor. They are acquired at early ages and occur very frequently in everyday conversation. However, there has been little proposal for a computational model of acquiring sentence-final particles. This paper proposes Subjective BERT, a self-attention model that takes various subjective senses in addition to language and images as input and learns the relationship between words and subjective senses. An evaluation experiment revealed that the model understands the usage of "yo", which expresses the speaker's intention to communicate new information, and that of "ne", which denotes the speaker's desire to confirm that some information is shared.

</details>

<details>

<summary>2022-10-01 13:57:21 - Swift Markov Logic for Probabilistic Reasoning on Knowledge Graphs</summary>

- *Luigi Bellomarini, Eleonora Laurenza, Emanuel Sallinger, Evgeny Sherkhonov*

- `2210.00283v1` - [abs](http://arxiv.org/abs/2210.00283v1) - [pdf](http://arxiv.org/pdf/2210.00283v1)

> We provide a framework for probabilistic reasoning in Vadalog-based Knowledge Graphs (KGs), satisfying the requirements of ontological reasoning: full recursion, powerful existential quantification, expression of inductive definitions. Vadalog is a Knowledge Representation and Reasoning (KRR) language based on Warded Datalog+/-, a logical core language of existential rules, with a good balance between computational complexity and expressive power. Handling uncertainty is essential for reasoning with KGs. Yet Vadalog and Warded Datalog+/- are not covered by the existing probabilistic logic programming and statistical relational learning approaches for several reasons, including insufficient support for recursion with existential quantification, and the impossibility to express inductive definitions. In this work, we introduce Soft Vadalog, a probabilistic extension to Vadalog, satisfying these desiderata. A Soft Vadalog program induces what we call a Probabilistic Knowledge Graph (PKG), which consists of a probability distribution on a network of chase instances, structures obtained by grounding the rules over a database using the chase procedure. We exploit PKGs for probabilistic marginal inference. We discuss the theory and present MCMC-chase, a Monte Carlo method to use Soft Vadalog in practice. We apply our framework to solve data management and industrial problems, and experimentally evaluate it in the Vadalog system.   Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2022-10-01 19:35:44 - Learning programs with magic values</summary>

- *CÃ©line Hocquette, Andrew Cropper*

- `2208.03238v2` - [abs](http://arxiv.org/abs/2208.03238v2) - [pdf](http://arxiv.org/pdf/2208.03238v2)

> A magic value in a program is a constant symbol that is essential for the execution of the program but has no clear explanation for its choice. Learning programs with magic values is difficult for existing program synthesis approaches. To overcome this limitation, we introduce an inductive logic programming approach to efficiently learn programs with magic values. Our experiments on diverse domains, including program synthesis, drug design, and game playing, show that our approach can (i) outperform existing approaches in terms of predictive accuracies and learning times, (ii) learn magic values from infinite domains, such as the value of pi, and (iii) scale to domains with millions of constant symbols.

</details>

<details>

<summary>2022-10-02 21:27:45 - Online Regenerative Learning</summary>

- *Owen Shen*

- `2209.08657v2` - [abs](http://arxiv.org/abs/2209.08657v2) - [pdf](http://arxiv.org/pdf/2209.08657v2)

> We study a type of Online Linear Programming (OLP) problem that maximizes the objective function with stochastic inputs. The performance of various algorithms that analyze this type of OLP is well studied when the stochastic inputs follow some i.i.d distribution. The two central questions to ask are: (i) can the algorithms achieve the same efficiency if the stochastic inputs are not i.i.d but still stationary, and (ii) how can we modify our algorithms if we know the stochastic inputs are trendy, hence not stationary. We answer the first question by analyzing a regenerative type of input and show the regrets of two popular algorithms are bounded by the same orders as their i.i.d counterparts. We discuss the second question in the context of linearly growing inputs and propose a trend-adaptive algorithm. We provide numerical simulations to illustrate the performance of our algorithms under both regenerative and trendy inputs.

</details>

<details>

<summary>2022-10-03 03:53:09 - Towards Using Data-Influence Methods to Detect Noisy Samples in Source Code Corpora</summary>

- *Anh T. V. Dau, Thang Nguyen-Duc, Hoang Thanh-Tung, Nghi D. Q. Bui*

- `2205.13022v2` - [abs](http://arxiv.org/abs/2205.13022v2) - [pdf](http://arxiv.org/pdf/2205.13022v2)

> Despite the recent trend of developing and applying neural source code models to software engineering tasks, the quality of such models is insufficient for real-world use. This is because there could be noise in the source code corpora used to train such models. We adapt data-influence methods to detect such noises in this paper. Data-influence methods are used in machine learning to evaluate the similarity of a target sample to the correct samples in order to determine whether or not the target sample is noisy. Our evaluation results show that data-influence methods can identify noisy samples from neural code models in classification-based tasks. This approach will contribute to the larger vision of developing better neural source code models from a data-centric perspective, which is a key driver for developing useful source code models in practice.

</details>

<details>

<summary>2022-10-03 06:49:23 - Multi-Agent Chance-Constrained Stochastic Shortest Path with Application to Risk-Aware Intelligent Intersection</summary>

- *Majid Khonji, Rashid Alyassi, Wolfgang Merkt, Areg Karapetyan, Xin Huang, Sungkweon Hong, Jorge Dias, Brian Williams*

- `2210.01766v1` - [abs](http://arxiv.org/abs/2210.01766v1) - [pdf](http://arxiv.org/pdf/2210.01766v1)

> In transportation networks, where traffic lights have traditionally been used for vehicle coordination, intersections act as natural bottlenecks. A formidable challenge for existing automated intersections lies in detecting and reasoning about uncertainty from the operating environment and human-driven vehicles. In this paper, we propose a risk-aware intelligent intersection system for autonomous vehicles (AVs) as well as human-driven vehicles (HVs). We cast the problem as a novel class of Multi-agent Chance-Constrained Stochastic Shortest Path (MCC-SSP) problems and devise an exact Integer Linear Programming (ILP) formulation that is scalable in the number of agents' interaction points (e.g., potential collision points at the intersection). In particular, when the number of agents within an interaction point is small, which is often the case in intersections, the ILP has a polynomial number of variables and constraints. To further improve the running time performance, we show that the collision risk computation can be performed offline. Additionally, a trajectory optimization workflow is provided to generate risk-aware trajectories for any given intersection. The proposed framework is implemented in CARLA simulator and evaluated under a fully autonomous intersection with AVs only as well as in a hybrid setup with a signalized intersection for HVs and an intelligent scheme for AVs. As verified via simulations, the featured approach improves intersection's efficiency by up to $200\%$ while also conforming to the specified tunable risk threshold.

</details>

<details>

<summary>2022-10-03 16:04:22 - Testing by Dualization</summary>

- *Yishuai Li*

- `2210.01047v1` - [abs](http://arxiv.org/abs/2210.01047v1) - [pdf](http://arxiv.org/pdf/2210.01047v1)

> Software engineering requires rigorous testing to guarantee the product's quality. Semantic testing of functional correctness is challenged by nondeterminism in behavior, which makes testers difficult to write and reason about.   This thesis presents a language-based technique for testing interactive systems. I propose a theory for specifying and validating nondeterministic behaviors, with guaranteed soundness and correctness. I then apply the theory to testing practices, and show how to derive specifications into interactive tester programs. I also introduce a language design for producing test inputs that can effectively detect and reproduce invalid behaviors.   I evaluate the methodology by specifying and testing real-world systems such as web servers and file synchronizers, demonstrating the derived testers' ability to find disagreements between the specification and the implementation.

</details>

<details>

<summary>2022-10-03 16:57:10 - Path of Destruction: Learning an Iterative Level Generator Using a Small Dataset</summary>

- *Matthew Siper, Ahmed Khalifa, Julian Togelius*

- `2202.10184v2` - [abs](http://arxiv.org/abs/2202.10184v2) - [pdf](http://arxiv.org/pdf/2202.10184v2)

> We propose a new procedural content generation method which learns iterative level generators from a dataset of existing levels. The Path of Destruction method, as we call it, views level generation as repair; levels are created by iteratively repairing from a random starting level. The first step is to generate an artificial dataset from the original set of levels by introducing many different sequences of mutations to existing levels. In the generated dataset, features are observations of destroyed levels and targets are the specific actions that repair the mutated tile in the middle of the observations. Using this dataset, a convolutional network is trained to map from observations to their respective appropriate repair actions. The trained network is then used to iteratively produce levels from random starting maps. We demonstrate this method by applying it to generate unique and playable tile-based levels for several 2D games (Zelda, Danger Dave, and Sokoban) and vary key hyperparameters.

</details>

<details>

<summary>2022-10-03 17:58:35 - CacheQL: Quantifying and Localizing Cache Side-Channel Vulnerabilities in Production Software</summary>

- *Yuanyuan Yuan, Zhibo Liu, Shuai Wang*

- `2209.14952v2` - [abs](http://arxiv.org/abs/2209.14952v2) - [pdf](http://arxiv.org/pdf/2209.14952v2)

> Cache side-channel attacks extract secrets by examining how victim software accesses cache. To date, practical attacks on cryptosystems and media libraries are demonstrated under different scenarios, inferring secret keys and reconstructing private media data such as images.   This work first presents eight criteria for designing a full-fledged detector for cache side-channel vulnerabilities. Then, we propose CacheQL, a novel detector that meets all of these criteria. CacheQL precisely quantifies information leaks of binary code, by characterizing the distinguishability of logged side channel traces. Moreover, CacheQL models leakage as a cooperative game, allowing information leakage to be precisely distributed to program points vulnerable to cache side channels. CacheQL is meticulously optimized to analyze whole side channel traces logged from production software (where each trace can have millions of records), and it alleviates randomness introduced by cryptographic blinding, ORAM, or real-world noises.   Our evaluation quantifies side-channel leaks of production cryptographic and media software. We further localize vulnerabilities reported by previous detectors and also identify a few hundred new leakage sites in recent OpenSSL (ver. 3.0.0), MbedTLS (ver. 3.0.0), Libgcrypt (ver. 1.9.4). Many of our localized program points are within the pre-processing modules of cryptosystems, which are not analyzed by existing works due to scalability. We also localize vulnerabilities in Libjpeg (ver. 2.1.2) that leak privacy about input images.

</details>

<details>

<summary>2022-10-03 21:24:04 - CaiRL: A High-Performance Reinforcement Learning Environment Toolkit</summary>

- *Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo*

- `2210.01235v1` - [abs](http://arxiv.org/abs/2210.01235v1) - [pdf](http://arxiv.org/pdf/2210.01235v1)

> This paper addresses the dire need for a platform that efficiently provides a framework for running reinforcement learning (RL) experiments. We propose the CaiRL Environment Toolkit as an efficient, compatible, and more sustainable alternative for training learning agents and propose methods to develop more efficient environment simulations.   There is an increasing focus on developing sustainable artificial intelligence. However, little effort has been made to improve the efficiency of running environment simulations. The most popular development toolkit for reinforcement learning, OpenAI Gym, is built using Python, a powerful but slow programming language. We propose a toolkit written in C++ with the same flexibility level but works orders of magnitude faster to make up for Python's inefficiency. This would drastically cut climate emissions.   CaiRL also presents the first reinforcement learning toolkit with a built-in JVM and Flash support for running legacy flash games for reinforcement learning research. We demonstrate the effectiveness of CaiRL in the classic control benchmark, comparing the execution speed to OpenAI Gym. Furthermore, we illustrate that CaiRL can act as a drop-in replacement for OpenAI Gym to leverage significantly faster training speeds because of the reduced environment computation time.

</details>

<details>

<summary>2022-10-04 03:01:47 - One-shot Network Pruning at Initialization with Discriminative Image Patches</summary>

- *Yinan Yang, Yu Wang, Ying Ji, Heng Qi, Jien Kato*

- `2209.05683v2` - [abs](http://arxiv.org/abs/2209.05683v2) - [pdf](http://arxiv.org/pdf/2209.05683v2)

> One-shot Network Pruning at Initialization (OPaI) is an effective method to decrease network pruning costs. Recently, there is a growing belief that data is unnecessary in OPaI. However, we obtain an opposite conclusion by ablation experiments in two representative OPaI methods, SNIP and GraSP. Specifically, we find that informative data is crucial to enhancing pruning performance. In this paper, we propose two novel methods, Discriminative One-shot Network Pruning (DOP) and Super Stitching, to prune the network by high-level visual discriminative image patches. Our contributions are as follows. (1) Extensive experiments reveal that OPaI is data-dependent. (2) Super Stitching performs significantly better than the original OPaI method on benchmark ImageNet, especially in a highly compressed model.

</details>

<details>

<summary>2022-10-04 03:06:21 - MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting</summary>

- *Peiwang Tang, Xianchao Zhang*

- `2210.02199v1` - [abs](http://arxiv.org/abs/2210.02199v1) - [pdf](http://arxiv.org/pdf/2210.02199v1)

> Large-scale self-supervised pre-training Transformer architecture have significantly boosted the performance for various tasks in natural language processing (NLP) and computer vision (CV). However, there is a lack of researches on processing multivariate time-series by pre-trained Transformer, and especially, current study on masking time-series for self-supervised learning is still a gap. Different from language and image processing, the information density of time-series increases the difficulty of research. The challenge goes further with the invalidity of the previous patch embedding and mask methods. In this paper, according to the data characteristics of multivariate time-series, a patch embedding method is proposed, and we present an self-supervised pre-training approach based on Masked Autoencoders (MAE), called MTSMAE, which can improve the performance significantly over supervised learning without pre-training. Evaluating our method on several common multivariate time-series datasets from different fields and with different characteristics, experiment results demonstrate that the performance of our method is significantly better than the best method currently available.

</details>

<details>

<summary>2022-10-04 04:59:36 - NeuDep: Neural Binary Memory Dependence Analysis</summary>

- *Kexin Pei, Dongdong She, Michael Wang, Scott Geng, Zhou Xuan, Yaniv David, Junfeng Yang, Suman Jana, Baishakhi Ray*

- `2210.02853v1` - [abs](http://arxiv.org/abs/2210.02853v1) - [pdf](http://arxiv.org/pdf/2210.02853v1)

> Determining whether multiple instructions can access the same memory location is a critical task in binary analysis. It is challenging as statically computing precise alias information is undecidable in theory. The problem aggravates at the binary level due to the presence of compiler optimizations and the absence of symbols and types. Existing approaches either produce significant spurious dependencies due to conservative analysis or scale poorly to complex binaries.   We present a new machine-learning-based approach to predict memory dependencies by exploiting the model's learned knowledge about how binary programs execute. Our approach features (i) a self-supervised procedure that pretrains a neural net to reason over binary code and its dynamic value flows through memory addresses, followed by (ii) supervised finetuning to infer the memory dependencies statically. To facilitate efficient learning, we develop dedicated neural architectures to encode the heterogeneous inputs (i.e., code, data values, and memory addresses from traces) with specific modules and fuse them with a composition learning strategy.   We implement our approach in NeuDep and evaluate it on 41 popular software projects compiled by 2 compilers, 4 optimizations, and 4 obfuscation passes. We demonstrate that NeuDep is more precise (1.5x) and faster (3.5x) than the current state-of-the-art. Extensive probing studies on security-critical reverse engineering tasks suggest that NeuDep understands memory access patterns, learns function signatures, and is able to match indirect calls. All these tasks either assist or benefit from inferring memory dependencies. Notably, NeuDep also outperforms the current state-of-the-art on these tasks.

</details>

<details>

<summary>2022-10-04 06:22:32 - Physical Passive Patch Adversarial Attacks on Visual Odometry Systems</summary>

- *Yaniv Nemcovsky, Matan Jacoby, Alex M. Bronstein, Chaim Baskin*

- `2207.05729v2` - [abs](http://arxiv.org/abs/2207.05729v2) - [pdf](http://arxiv.org/pdf/2207.05729v2)

> Deep neural networks are known to be susceptible to adversarial perturbations -- small perturbations that alter the output of the network and exist under strict norm limitations. While such perturbations are usually discussed as tailored to a specific input, a universal perturbation can be constructed to alter the model's output on a set of inputs. Universal perturbations present a more realistic case of adversarial attacks, as awareness of the model's exact input is not required. In addition, the universal attack setting raises the subject of generalization to unseen data, where given a set of inputs, the universal perturbations aim to alter the model's output on out-of-sample data. In this work, we study physical passive patch adversarial attacks on visual odometry-based autonomous navigation systems. A visual odometry system aims to infer the relative camera motion between two corresponding viewpoints, and is frequently used by vision-based autonomous navigation systems to estimate their state. For such navigation systems, a patch adversarial perturbation poses a severe security issue, as it can be used to mislead a system onto some collision course. To the best of our knowledge, we show for the first time that the error margin of a visual odometry model can be significantly increased by deploying patch adversarial attacks in the scene. We provide evaluation on synthetic closed-loop drone navigation data and demonstrate that a comparable vulnerability exists in real data. A reference implementation of the proposed method and the reported experiments is provided at https://github.com/patchadversarialattacks/patchadversarialattacks.

</details>

<details>

<summary>2022-10-04 08:22:46 - Federated Reinforcement Learning for Real-Time Electric Vehicle Charging and Discharging Control</summary>

- *Zixuan Zhang, Yuning Jiang, Yuanming Shi, Ye Shi, Wei Chen*

- `2210.01452v1` - [abs](http://arxiv.org/abs/2210.01452v1) - [pdf](http://arxiv.org/pdf/2210.01452v1)

> With the recent advances in mobile energy storage technologies, electric vehicles (EVs) have become a crucial part of smart grids. When EVs participate in the demand response program, the charging cost can be significantly reduced by taking full advantage of the real-time pricing signals. However, many stochastic factors exist in the dynamic environment, bringing significant challenges to design an optimal charging/discharging control strategy. This paper develops an optimal EV charging/discharging control strategy for different EV users under dynamic environments to maximize EV users' benefits. We first formulate this problem as a Markov decision process (MDP). Then we consider EV users with different behaviors as agents in different environments. Furthermore, a horizontal federated reinforcement learning (HFRL)-based method is proposed to fit various users' behaviors and dynamic environments. This approach can learn an optimal charging/discharging control strategy without sharing users' profiles. Simulation results illustrate that the proposed real-time EV charging/discharging control strategy can perform well among various stochastic factors.

</details>

<details>

<summary>2022-10-04 08:40:14 - Relational program synthesis with numerical reasoning</summary>

- *CÃ©line Hocquette, Andrew Cropper*

- `2210.00764v2` - [abs](http://arxiv.org/abs/2210.00764v2) - [pdf](http://arxiv.org/pdf/2210.00764v2)

> Program synthesis approaches struggle to learn programs with numerical values. An especially difficult problem is learning continuous values over multiple examples, such as intervals. To overcome this limitation, we introduce an inductive logic programming approach which combines relational learning with numerical reasoning. Our approach, which we call NUMSYNTH, uses satisfiability modulo theories solvers to efficiently learn programs with numerical values. Our approach can identify numerical values in linear arithmetic fragments, such as real difference logic, and from infinite domains, such as real numbers or integers. Our experiments on four diverse domains, including game playing and program synthesis, show that our approach can (i) learn programs with numerical values from linear arithmetical reasoning, and (ii) outperform existing approaches in terms of predictive accuracies and learning times.

</details>

<details>

<summary>2022-10-04 11:02:31 - Scheduling with Many Shared Resources</summary>

- *Max A. Deppert, Klaus Jansen, Marten Maack, Simon Pukrop, Malin Rau*

- `2210.01523v1` - [abs](http://arxiv.org/abs/2210.01523v1) - [pdf](http://arxiv.org/pdf/2210.01523v1)

> Consider the many shared resource scheduling problem where jobs have to be scheduled on identical parallel machines with the goal of minimizing the makespan. However, each job needs exactly one additional shared resource in order to be executed and hence prevents the execution of jobs that need the same resource while being processed. Previously a $(2m/(m+1))$-approximation was the best known result for this problem. Furthermore, a $6/5$-approximation for the case with only two machines was known as well as a PTAS for the case with a constant number of machines. We present a simple and fast 5/3-approximation and a much more involved but still reasonable 1.5-approximation. Furthermore, we provide a PTAS for the case with only a constant number of machines, which is arguably simpler and faster than the previously known one, as well as a PTAS with resource augmentation for the general case. The approximation schemes make use of the N-fold integer programming machinery, which has found more and more applications in the field of scheduling recently. It is plausible that the latter results can be improved and extended to more general cases. Lastly, we give a $5/4 - \varepsilon$ inapproximability result for the natural problem extension where each job may need up to a constant number (in particular $3$) of different resources.

</details>

<details>

<summary>2022-10-04 11:45:23 - Decompiling x86 Deep Neural Network Executables</summary>

- *Zhibo Liu, Yuanyuan Yuan, Shuai Wang, Xiaofei Xie, Lei Ma*

- `2210.01075v2` - [abs](http://arxiv.org/abs/2210.01075v2) - [pdf](http://arxiv.org/pdf/2210.01075v2)

> Due to their widespread use on heterogeneous hardware devices, deep learning (DL) models are compiled into executables by DL compilers to fully leverage low-level hardware primitives. This approach allows DL computations to be undertaken at low cost across a variety of computing platforms, including CPUs, GPUs, and various hardware accelerators.   We present BTD (Bin to DNN), a decompiler for deep neural network (DNN) executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. BTD delivers a practical framework to process DNN executables compiled by different DL compilers and with full optimizations enabled on x86 platforms. It employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators.   Our evaluation reveals that BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recovered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable. We show that BTD can boost two representative attacks, adversarial example generation and knowledge stealing, against DNN executables. We also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching.

</details>

<details>

<summary>2022-10-04 12:18:24 - StateAFL: Greybox Fuzzing for Stateful Network Servers</summary>

- *Roberto Natella*

- `2110.06253v2` - [abs](http://arxiv.org/abs/2110.06253v2) - [pdf](http://arxiv.org/pdf/2110.06253v2)

> Fuzzing network servers is a technical challenge, since the behavior of the target server depends on its state over a sequence of multiple messages. Existing solutions are costly and difficult to use, as they rely on manually-customized artifacts such as protocol models, protocol parsers, and learning frameworks. The aim of this work is to develop a greybox fuzzer (StateaAFL) for network servers that only relies on lightweight analysis of the target program, with no manual customization, in a similar way to what the AFL fuzzer achieved for stateless programs. The proposed fuzzer instruments the target server at compile-time, to insert probes on memory allocations and network I/O operations. At run-time, it infers the current protocol state of the target server by taking snapshots of long-lived memory areas, and by applying a fuzzy hashing algorithm (Locality-Sensitive Hashing) to map memory contents to a unique state identifier. The fuzzer incrementally builds a protocol state machine for guiding fuzzing.   We implemented and released StateaAFL as open-source software. As a basis for reproducible experimentation, we integrated StateaAFL with a large set of network servers for popular protocols, with no manual customization to accomodate for the protocol. The experimental results show that the fuzzer can be applied with no manual customization on a large set of network servers for popular protocols, and that it can achieve comparable, or even better code coverage and bug detection than customized fuzzing. Moreover, our qualitative analysis shows that states inferred from memory better reflect the server behavior than only using response codes from messages.

</details>

<details>

<summary>2022-10-04 12:58:29 - PatchDropout: Economizing Vision Transformers Using Patch Dropout</summary>

- *Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Azizpour, Kevin Smith*

- `2208.07220v2` - [abs](http://arxiv.org/abs/2208.07220v2) - [pdf](http://arxiv.org/pdf/2208.07220v2)

> Vision transformers have demonstrated the potential to outperform CNNs in a variety of vision tasks. But the computational and memory requirements of these models prohibit their use in many applications, especially those that depend on high-resolution images, such as medical image classification. Efforts to train ViTs more efficiently are overly complicated, necessitating architectural changes or intricate training schemes. In this work, we show that standard ViT models can be efficiently trained at high resolution by randomly dropping input image patches. This simple approach, PatchDropout, reduces FLOPs and memory by at least 50% in standard natural image datasets such as ImageNet, and those savings only increase with image size. On CSAW, a high-resolution medical dataset, we observe a 5 times savings in computation and memory using PatchDropout, along with a boost in performance. For practitioners with a fixed computational or memory budget, PatchDropout makes it possible to choose image resolution, hyperparameters, or model size to get the most performance out of their model.

</details>

<details>

<summary>2022-10-04 14:27:42 - Backdoor Attacks in the Supply Chain of Masked Image Modeling</summary>

- *Xinyue Shen, Xinlei He, Zheng Li, Yun Shen, Michael Backes, Yang Zhang*

- `2210.01632v1` - [abs](http://arxiv.org/abs/2210.01632v1) - [pdf](http://arxiv.org/pdf/2210.01632v1)

> Masked image modeling (MIM) revolutionizes self-supervised learning (SSL) for image pre-training. In contrast to previous dominating self-supervised methods, i.e., contrastive learning, MIM attains state-of-the-art performance by masking and reconstructing random patches of the input image. However, the associated security and privacy risks of this novel generative method are unexplored. In this paper, we perform the first security risk quantification of MIM through the lens of backdoor attacks. Different from previous work, we are the first to systematically threat modeling on SSL in every phase of the model supply chain, i.e., pre-training, release, and downstream phases. Our evaluation shows that models built with MIM are vulnerable to existing backdoor attacks in release and downstream phases and are compromised by our proposed method in pre-training phase. For instance, on CIFAR10, the attack success rate can reach 99.62%, 96.48%, and 98.89% in the downstream phase, release phase, and pre-training phase, respectively. We also take the first step to investigate the success factors of backdoor attacks in the pre-training phase and find the trigger number and trigger pattern play key roles in the success of backdoor attacks while trigger location has only tiny effects. In the end, our empirical study of the defense mechanisms across three detection-level on model supply chain phases indicates that different defenses are suitable for backdoor attacks in different phases. However, backdoor attacks in the release phase cannot be detected by all three detection-level methods, calling for more effective defenses in future research.

</details>

<details>

<summary>2022-10-04 15:55:59 - Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks</summary>

- *Blake Bordelon, Cengiz Pehlevan*

- `2205.09653v3` - [abs](http://arxiv.org/abs/2205.09653v3) - [pdf](http://arxiv.org/pdf/2205.09653v3)

> We analyze feature learning in infinite-width neural networks trained with gradient flow through a self-consistent dynamical field theory. We construct a collection of deterministic dynamical order parameters which are inner-product kernels for hidden unit activations and gradients in each layer at pairs of time points, providing a reduced description of network activity through training. These kernel order parameters collectively define the hidden layer activation distribution, the evolution of the neural tangent kernel, and consequently output predictions. We show that the field theory derivation recovers the recursive stochastic process of infinite-width feature learning networks obtained from Yang and Hu (2021) with Tensor Programs . For deep linear networks, these kernels satisfy a set of algebraic matrix equations. For nonlinear networks, we provide an alternating sampling procedure to self-consistently solve for the kernel order parameters. We provide comparisons of the self-consistent solution to various approximation schemes including the static NTK approximation, gradient independence assumption, and leading order perturbation theory, showing that each of these approximations can break down in regimes where general self-consistent solutions still provide an accurate description. Lastly, we provide experiments in more realistic settings which demonstrate that the loss and kernel dynamics of CNNs at fixed feature learning strength is preserved across different widths on a CIFAR classification task.

</details>

<details>

<summary>2022-10-05 11:36:45 - Differentiable Mathematical Programming for Object-Centric Representation Learning</summary>

- *Adeel Pervez, Phillip Lippe, Efstratios Gavves*

- `2210.02159v1` - [abs](http://arxiv.org/abs/2210.02159v1) - [pdf](http://arxiv.org/pdf/2210.02159v1)

> We propose topology-aware feature partitioning into $k$ disjoint partitions for given scene features as a method for object-centric representation learning. To this end, we propose to use minimum $s$-$t$ graph cuts as a partitioning method which is represented as a linear program. The method is topologically aware since it explicitly encodes neighborhood relationships in the image graph. To solve the graph cuts our solution relies on an efficient, scalable, and differentiable quadratic programming approximation. Optimizations specific to cut problems allow us to solve the quadratic programs and compute their gradients significantly more efficiently compared with the general quadratic programming approach. Our results show that our approach is scalable and outperforms existing methods on object discovery tasks with textured scenes and objects.

</details>

<details>

<summary>2022-10-05 13:18:47 - On Neural Consolidation for Transfer in Reinforcement Learning</summary>

- *Valentin Guillet, Dennis G. Wilson, Carlos Aguilar-Melchor, Emmanuel Rachelson*

- `2210.02240v1` - [abs](http://arxiv.org/abs/2210.02240v1) - [pdf](http://arxiv.org/pdf/2210.02240v1)

> Although transfer learning is considered to be a milestone in deep reinforcement learning, the mechanisms behind it are still poorly understood. In particular, predicting if knowledge can be transferred between two given tasks is still an unresolved problem. In this work, we explore the use of network distillation as a feature extraction method to better understand the context in which transfer can occur. Notably, we show that distillation does not prevent knowledge transfer, including when transferring from multiple tasks to a new one, and we compare these results with transfer without prior distillation. We focus our work on the Atari benchmark due to the variability between different games, but also to their similarities in terms of visual features.

</details>

<details>

<summary>2022-10-05 13:33:25 - Hiding Images in Deep Probabilistic Models</summary>

- *Haoyu Chen, Linqi Song, Zhenxing Qian, Xinpeng Zhang, Kede Ma*

- `2210.02257v1` - [abs](http://arxiv.org/abs/2210.02257v1) - [pdf](http://arxiv.org/pdf/2210.02257v1)

> Data hiding with deep neural networks (DNNs) has experienced impressive successes in recent years. A prevailing scheme is to train an autoencoder, consisting of an encoding network to embed (or transform) secret messages in (or into) a carrier, and a decoding network to extract the hidden messages. This scheme may suffer from several limitations regarding practicability, security, and embedding capacity. In this work, we describe a different computational framework to hide images in deep probabilistic models. Specifically, we use a DNN to model the probability density of cover images, and hide a secret image in one particular location of the learned distribution. As an instantiation, we adopt a SinGAN, a pyramid of generative adversarial networks (GANs), to learn the patch distribution of one cover image. We hide the secret image by fitting a deterministic mapping from a fixed set of noise maps (generated by an embedding key) to the secret image during patch distribution learning. The stego SinGAN, behaving as the original SinGAN, is publicly communicated; only the receiver with the embedding key is able to extract the secret image. We demonstrate the feasibility of our SinGAN approach in terms of extraction accuracy and model security. Moreover, we show the flexibility of the proposed method in terms of hiding multiple images for different receivers and obfuscating the secret image.

</details>

<details>

<summary>2022-10-05 17:05:01 - SPEAR : Semi-supervised Data Programming in Python</summary>

- *Guttu Sai Abhishek, Harshad Ingole, Parth Laturia, Vineeth Dorna, Ayush Maheshwari, Rishabh Iyer, Ganesh Ramakrishnan*

- `2108.00373v3` - [abs](http://arxiv.org/abs/2108.00373v3) - [pdf](http://arxiv.org/pdf/2108.00373v3)

> We present SPEAR, an open-source python library for data programming with semi supervision. The package implements several recent data programming approaches including facility to programmatically label and build training data. SPEAR facilitates weak supervision in the form of heuristics (or rules) and association of noisy labels to the training dataset. These noisy labels are aggregated to assign labels to the unlabeled data for downstream tasks. We have implemented several label aggregation approaches that aggregate the noisy labels and then train using the noisily labeled set in a cascaded manner. Our implementation also includes other approaches that jointly aggregate and train the model for text classification tasks. Thus, in our python package, we integrate several cascade and joint data-programming approaches while also providing the facility of data programming by letting the user define labeling functions or rules. The code and tutorial notebooks are available at https://github.com/decile-team/spear. Further, extensive documentation can be found at https://spear-decile.readthedocs.io/. Video tutorials demonstrating the usage of our package are available here. We also present some real-world use cases of SPEAR.

</details>

<details>

<summary>2022-10-05 17:20:26 - Emotion Twenty Questions Dialog System for Lexical Emotional Intelligence</summary>

- *Abe Kazemzadeh, Adedamola Sanusi, Huihui, Nie*

- `2210.02400v1` - [abs](http://arxiv.org/abs/2210.02400v1) - [pdf](http://arxiv.org/pdf/2210.02400v1)

> This paper presents a web-based demonstration of Emotion Twenty Questions (EMO20Q), a dialog game whose purpose is to study how people describe emotions. EMO20Q can also be used to develop artificially intelligent dialog agents that can play the game. In previous work, an EMO20Q agent used a sequential Bayesian machine learning model and could play the question-asking role. Newer transformer-based neural machine learning models have made it possible to develop an agent for the question-answering role.   This demo paper describes the recent developments in the question-answering role of the EMO20Q game, which requires the agent to respond to more open-ended inputs. Furthermore, we also describe the design of the system, including the web-based front-end, agent architecture and programming, and updates to earlier software used.   The demo system will be available to collect pilot data during the ACII conference and this data will be used to inform future experiments and system design.

</details>

<details>

<summary>2022-10-05 18:10:01 - Analyzing historical diagnosis code data from NIH N3C and RECOVER Programs using deep learning to determine risk factors for Long Covid</summary>

- *Saurav Sengupta, Johanna Loomba, Suchetha Sharma, Donald E. Brown, Lorna Thorpe, Melissa A Haendel, Christopher G Chute, Stephanie Hong*

- `2210.02490v1` - [abs](http://arxiv.org/abs/2210.02490v1) - [pdf](http://arxiv.org/pdf/2210.02490v1)

> Post-acute sequelae of SARS-CoV-2 infection (PASC) or Long COVID is an emerging medical condition that has been observed in several patients with a positive diagnosis for COVID-19. Historical Electronic Health Records (EHR) like diagnosis codes, lab results and clinical notes have been analyzed using deep learning and have been used to predict future clinical events. In this paper, we propose an interpretable deep learning approach to analyze historical diagnosis code data from the National COVID Cohort Collective (N3C) to find the risk factors contributing to developing Long COVID. Using our deep learning approach, we are able to predict if a patient is suffering from Long COVID from a temporally ordered list of diagnosis codes up to 45 days post the first COVID positive test or diagnosis for each patient, with an accuracy of 70.48\%. We are then able to examine the trained model using Gradient-weighted Class Activation Mapping (GradCAM) to give each input diagnoses a score. The highest scored diagnosis were deemed to be the most important for making the correct prediction for a patient. We also propose a way to summarize these top diagnoses for each patient in our cohort and look at their temporal trends to determine which codes contribute towards a positive Long COVID diagnosis.

</details>

<details>

<summary>2022-10-05 22:03:22 - Functional Labeled Optimal Partitioning</summary>

- *Toby D. Hocking, Jacob M. Kaufman, Alyssa J. Stenberg*

- `2210.02580v1` - [abs](http://arxiv.org/abs/2210.02580v1) - [pdf](http://arxiv.org/pdf/2210.02580v1)

> Peak detection is a problem in sequential data analysis that involves differentiating regions with higher counts (peaks) from regions with lower counts (background noise).   It is crucial to correctly predict areas that deviate from the background noise, in both the train and test sets of labels.   Dynamic programming changepoint algorithms have been proposed to solve the peak detection problem by constraining the mean to alternatively increase and then decrease.   The current constrained changepoint algorithms only create predictions on the test set, while completely ignoring the train set.   Changepoint algorithms that are both accurate when fitting the train set, and make predictions on the test set, have been proposed but not in the context of peak detection models.   We propose to resolve these issues by creating a new dynamic programming algorithm, FLOPART, that has zero train label errors, and is able to provide highly accurate predictions on the test set.   We provide an empirical analysis that shows FLOPART has a similar time complexity while being more accurate than the existing algorithms in terms of train and test label errors.

</details>

<details>

<summary>2022-10-06 01:23:31 - Cooperative Coverage with a Leader and a Wingmate in Communication-Constrained Environments</summary>

- *Sai Krishna Kanth Hari, Sivakumar Rathinam, Swaroop Darbha, David W. Casbeer*

- `2210.02628v1` - [abs](http://arxiv.org/abs/2210.02628v1) - [pdf](http://arxiv.org/pdf/2210.02628v1)

> We consider a mission framework in which two unmanned vehicles (UVs), a leader and a wingmate, are required to provide cooperative coverage of an environment while being within a short communication range. This framework finds applications in underwater and/or military domains, where certain constraints are imposed on communication by either the application or the environment. An important objective of missions within this framework is to minimize the total travel and communication costs of the leader-wingmate duo. In this paper, we propose and formulate the problem of finding routes for the UVs that minimize the sum of their travel and communication costs as a network optimization problem of the form of a binary program (BP). The BP is computationally expensive, with the time required to compute optimal solutions increasing rapidly with the problem size. To address this challenge, here, we propose two algorithms, an approximation algorithm and a heuristic algorithm, to solve large-scale instances of the problem swiftly. We demonstrate the effectiveness and the scalability of these algorithms through an analysis of extensive numerical simulations performed over 500 instances, with the number of targets in the instances ranging from 6 to 100.

</details>

<details>

<summary>2022-10-06 07:00:40 - Pik-Fix: Restoring and Colorizing Old Photos</summary>

- *Runsheng Xu, Zhengzhong Tu, Yuanqi Du, Xiaoyu Dong, Jinlong Li, Zibo Meng, Jiaqi Ma, Alan Bovik, Hongkai Yu*

- `2205.01902v3` - [abs](http://arxiv.org/abs/2205.01902v3) - [pdf](http://arxiv.org/pdf/2205.01902v3)

> Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack of large-scale datasets of old photos makes addressing this restoration task very challenging. Here we present a novel reference-based end-to-end learning framework that is able to both repair and colorize old, degraded pictures. Our proposed framework consists of three modules: a restoration sub-network that conducts restoration from degradations, a similarity network that performs color histogram matching and color transfer, and a colorization subnet that learns to predict the chroma elements of images conditioned on chromatic reference signals. The overall system makes uses of color histogram priors from reference images, which greatly reduces the need for large-scale training data. We have also created a first-of-a-kind public dataset of real old photos that are paired with ground truth ''pristine'' photos that have been manually restored by PhotoShop experts. We conducted extensive experiments on this dataset and synthetic datasets, and found that our method significantly outperforms previous state-of-the-art models using both qualitative comparisons and quantitative measurements. The code is available at https://github.com/DerrickXuNu/Pik-Fix.

</details>

<details>

<summary>2022-10-06 07:09:28 - Motion and Appearance Adaptation for Cross-Domain Motion Transfer</summary>

- *Borun Xu, Biao Wang, Jinhong Deng, Jiale Tao, Tiezheng Ge, Yuning Jiang, Wen Li, Lixin Duan*

- `2209.14529v2` - [abs](http://arxiv.org/abs/2209.14529v2) - [pdf](http://arxiv.org/pdf/2209.14529v2)

> Motion transfer aims to transfer the motion of a driving video to a source image. When there are considerable differences between object in the driving video and that in the source image, traditional single domain motion transfer approaches often produce notable artifacts; for example, the synthesized image may fail to preserve the human shape of the source image (cf . Fig. 1 (a)). To address this issue, in this work, we propose a Motion and Appearance Adaptation (MAA) approach for cross-domain motion transfer, in which we regularize the object in the synthesized image to capture the motion of the object in the driving frame, while still preserving the shape and appearance of the object in the source image. On one hand, considering the object shapes of the synthesized image and the driving frame might be different, we design a shape-invariant motion adaptation module that enforces the consistency of the angles of object parts in two images to capture the motion information. On the other hand, we introduce a structure-guided appearance consistency module designed to regularize the similarity between the corresponding patches of the synthesized image and the source image without affecting the learned motion in the synthesized image. Our proposed MAA model can be trained in an end-to-end manner with a cyclic reconstruction loss, and ultimately produces a satisfactory motion transfer result (cf . Fig. 1 (b)). We conduct extensive experiments on human dancing dataset Mixamo-Video to Fashion-Video and human face dataset Vox-Celeb to Cufs; on both of these, our MAA model outperforms existing methods both quantitatively and qualitatively.

</details>

<details>

<summary>2022-10-06 09:05:42 - AutoQC: Automated Synthesis of Quantum Circuits Using Neural Network</summary>

- *Kentaro Murakami, Jianjun Zhao*

- `2210.02766v1` - [abs](http://arxiv.org/abs/2210.02766v1) - [pdf](http://arxiv.org/pdf/2210.02766v1)

> While the ability to build quantum computers is improving dramatically, developing quantum algorithms is limited and relies on human insight and ingenuity. Although a number of quantum programming languages have been developed, it is challenging for software developers who are not familiar with quantum computing to learn and use these languages. It is, therefore, necessary to develop tools to support developing new quantum algorithms and programs automatically. This paper proposes AutoQC, an approach to automatically synthesizing quantum circuits using the neural network from input and output pairs. We consider a quantum circuit a sequence of quantum gates and synthesize a quantum circuit probabilistically by prioritizing with a neural network at each step. The experimental results highlight the ability of AutoQC to synthesize some essential quantum circuits at a lower cost.

</details>

<details>

<summary>2022-10-06 11:33:31 - Single-Use Delegatable Signatures Based on Smart Contracts</summary>

- *Stephan Krenn, Thomas LorÃ¼nser*

- `2210.02826v1` - [abs](http://arxiv.org/abs/2210.02826v1) - [pdf](http://arxiv.org/pdf/2210.02826v1)

> Delegation of cryptographic signing rights has found many application in the literature and the real world. However, despite very advanced functionalities and specific use cases, existing solutions share the natural limitation that the number of usages of these signing rights cannot be efficiently limited, but users can at most be disincentivized to abuse their rights.   In this paper, we suggest a solution to this problem based on blockchains. We let a user define a smart contract defining delegated signing rights, which needs to be triggered to successfully sign a message. By leveraging the immutability of the blockchain, our construction can now guarantee that a user-defined threshold of signature invocations cannot be exceeded, thereby circumventing the need for dedicated hardware or similar assistance in existing constructions for one-time programs.   We discuss different constructions supporting different features, and provide concrete implementations in the Solidity language of the Ethereum blockchain, proving the real-world efficiency and feasibility of our construction.

</details>

<details>

<summary>2022-10-06 16:18:43 - CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture</summary>

- *Vikram Nitin, Shubhi Asthana, Baishakhi Ray, Rahul Krishna*

- `2207.11784v2` - [abs](http://arxiv.org/abs/2207.11784v2) - [pdf](http://arxiv.org/pdf/2207.11784v2)

> Microservices Architecture (MSA) has become a de-facto standard for designing cloud-native enterprise applications due to its efficient infrastructure setup, service availability, elastic scalability, dependability, and better security. Existing (monolithic) systems must be decomposed into microservices to harness these characteristics. Since manual decomposition of large scale applications can be laborious and error-prone, AI-based systems to detect decomposition strategies are gaining popularity. However, the usefulness of these approaches is limited by the expressiveness of the program representation and their inability to model the application's dependency on critical external resources such as databases. Consequently, partitioning recommendations offered by current tools result in architectures that result in (a) distributed monoliths, and/or (b) force the use of (often criticized) distributed transactions. This work attempts to overcome these challenges by introducing CARGO({short for [C]ontext-sensitive l[A]bel p[R]opa[G]ati[O]n})-a novel un-/semi-supervised partition refinement technique that uses a context- and flow-sensitive system dependency graph of the monolithic application to refine and thereby enrich the partitioning quality of the current state-of-the-art algorithms. CARGO was used to augment four state-of-the-art microservice partitioning techniques that were applied on five Java EE applications (including one industrial scale proprietary project). Experiments demostrate that CARGO can improve the partition quality of all modern microservice partitioning techniques. Further, CARGO substantially reduces distributed transactions and a real-world performance evaluation of a benchmark application (deployed under varying loads) shows that CARGO also lowers the overall the latency of the deployed microservice application by 11% and increases throughput by 120% on average.

</details>

<details>

<summary>2022-10-06 16:30:51 - Learning many-body Hamiltonians with Heisenberg-limited scaling</summary>

- *Hsin-Yuan Huang, Yu Tong, Di Fang, Yuan Su*

- `2210.03030v1` - [abs](http://arxiv.org/abs/2210.03030v1) - [pdf](http://arxiv.org/pdf/2210.03030v1)

> Learning a many-body Hamiltonian from its dynamics is a fundamental problem in physics. In this work, we propose the first algorithm to achieve the Heisenberg limit for learning an interacting $N$-qubit local Hamiltonian. After a total evolution time of $\mathcal{O}(\epsilon^{-1})$, the proposed algorithm can efficiently estimate any parameter in the $N$-qubit Hamiltonian to $\epsilon$-error with high probability. The proposed algorithm is robust against state preparation and measurement error, does not require eigenstates or thermal states, and only uses $\mathrm{polylog}(\epsilon^{-1})$ experiments. In contrast, the best previous algorithms, such as recent works using gradient-based optimization or polynomial interpolation, require a total evolution time of $\mathcal{O}(\epsilon^{-2})$ and $\mathcal{O}(\epsilon^{-2})$ experiments. Our algorithm uses ideas from quantum simulation to decouple the unknown $N$-qubit Hamiltonian $H$ into noninteracting patches, and learns $H$ using a quantum-enhanced divide-and-conquer approach. We prove a matching lower bound to establish the asymptotic optimality of our algorithm.

</details>

<details>

<summary>2022-10-06 20:58:48 - Threat Repair with Optimization Modulo Theories</summary>

- *Thorsten Tarrach, Masoud Ebrahimi, Sandra KÃ¶nig, Christoph Schmittner, Roderick Bloem, Dejan Nickovic*

- `2210.03207v1` - [abs](http://arxiv.org/abs/2210.03207v1) - [pdf](http://arxiv.org/pdf/2210.03207v1)

> We propose a model-based procedure for automatically preventing security threats using formal models. We encode system models and potential threats as satisfiability modulo theory (SMT) formulas. This model allows us to ask security questions as satisfiability queries. We formulate threat prevention as an optimization problem over the same formulas. The outcome of our threat prevention procedure is a suggestion of model attribute repair that eliminates threats. Whenever threat prevention fails, we automatically explain why the threat happens. We implement our approach using the state-of-the-art Z3 SMT solver and interface it with the threat analysis tool THREATGET. We demonstrate the value of our procedure in two case studies from automotive and smart home domains, including an industrial-strength example.

</details>

<details>

<summary>2022-10-07 01:59:45 - Automatic Prediction of Rejected Edits in Stack Overflow</summary>

- *Saikat Mondal, Gias Uddin, Chanchal Roy*

- `2210.03281v1` - [abs](http://arxiv.org/abs/2210.03281v1) - [pdf](http://arxiv.org/pdf/2210.03281v1)

> The content quality of shared knowledge in Stack Overflow (SO) is crucial in supporting software developers with their programming problems. Thus, SO allows its users to suggest edits to improve the quality of a post (i.e., question and answer). However, existing research shows that many suggested edits in SO are rejected due to undesired contents/formats or violating edit guidelines. Such a scenario frustrates or demotivates users who would like to conduct good-quality edits. Therefore, our research focuses on assisting SO users by offering them suggestions on how to improve their editing of posts. First, we manually investigate 764 (382 questions + 382 answers) rejected edits by rollbacks and produce a catalog of 19 rejection reasons. Second, we extract 15 texts and user-based features to capture those rejection reasons. Third, we develop four machine learning models using those features. Our best-performing model can predict rejected edits with 69.1% precision, 71.2% recall, 70.1% F1-score, and 69.8% overall accuracy. Fourth, we introduce an online tool named EditEx that works with the SO edit system. EditEx can assist users while editing posts by suggesting the potential causes of rejections. We recruit 20 participants to assess the effectiveness of EditEx. Half of the participants (i.e., treatment group) use EditEx and another half (i.e., control group) use the SO standard edit system to edit posts. According to our experiment, EditEx can support SO standard edit system to prevent 49% of rejected edits, including the commonly rejected ones. However, it can prevent 12% rejections even in free-form regular edits. The treatment group finds the potential rejection reasons identified by EditEx influential. Furthermore, the median workload suggesting edits using EditEx is half compared to the SO edit system.

</details>

<details>

<summary>2022-10-07 03:15:02 - GOLLIC: Learning Global Context beyond Patches for Lossless High-Resolution Image Compression</summary>

- *Yuan Lan, Liang Qin, Zhaoyi Sun, Yang Xiang, Jie Sun*

- `2210.03301v1` - [abs](http://arxiv.org/abs/2210.03301v1) - [pdf](http://arxiv.org/pdf/2210.03301v1)

> Neural-network-based approaches recently emerged in the field of data compression and have already led to significant progress in image compression, especially in achieving a higher compression ratio. In the lossless image compression scenario, however, existing methods often struggle to learn a probability model of full-size high-resolution images due to the limitation of the computation source. The current strategy is to crop high-resolution images into multiple non-overlapping patches and process them independently. This strategy ignores long-term dependencies beyond patches, thus limiting modeling performance. To address this problem, we propose a hierarchical latent variable model with a global context to capture the long-term dependencies of high-resolution images. Besides the latent variable unique to each patch, we introduce shared latent variables between patches to construct the global context. The shared latent variables are extracted by a self-supervised clustering module inside the model's encoder. This clustering module assigns each patch the confidence that it belongs to any cluster. Later, shared latent variables are learned according to latent variables of patches and their confidence, which reflects the similarity of patches in the same cluster and benefits the global context modeling. Experimental results show that our global context model improves compression ratio compared to the engineered codecs and deep learning models on three benchmark high-resolution image datasets, DIV2K, CLIC.pro, and CLIC.mobile.

</details>

<details>

<summary>2022-10-07 13:23:20 - Bayesian Persuasion for Algorithmic Recourse</summary>

- *Keegan Harris, Valerie Chen, Joon Sik Kim, Ameet Talwalkar, Hoda Heidari, Zhiwei Steven Wu*

- `2112.06283v3` - [abs](http://arxiv.org/abs/2112.06283v3) - [pdf](http://arxiv.org/pdf/2112.06283v3)

> When subjected to automated decision-making, decision subjects may strategically modify their observable features in ways they believe will maximize their chances of receiving a favorable decision. In many practical situations, the underlying assessment rule is deliberately kept secret to avoid gaming and maintain competitive advantage. The resulting opacity forces the decision subjects to rely on incomplete information when making strategic feature modifications. We capture such settings as a game of Bayesian persuasion, in which the decision maker offers a form of recourse to the decision subject by providing them with an action recommendation (or signal) to incentivize them to modify their features in desirable ways. We show that when using persuasion, the decision maker and decision subject are never worse off in expectation, while the decision maker can be significantly better off. While the decision maker's problem of finding the optimal Bayesian incentive-compatible (BIC) signaling policy takes the form of optimization over infinitely-many variables, we show that this optimization can be cast as a linear program over finitely-many regions of the space of possible assessment rules. While this reformulation simplifies the problem dramatically, solving the linear program requires reasoning about exponentially-many variables, even in relatively simple cases. Motivated by this observation, we provide a polynomial-time approximation scheme that recovers a near-optimal signaling policy. Finally, our numerical simulations on semi-synthetic data empirically demonstrate the benefits of using persuasion in the algorithmic recourse setting.

</details>

<details>

<summary>2022-10-07 13:40:07 - Treewidth-aware Reductions of Normal ASP to SAT -- Is Normal ASP Harder than SAT after All?</summary>

- *Markus Hecher*

- `2210.03553v1` - [abs](http://arxiv.org/abs/2210.03553v1) - [pdf](http://arxiv.org/pdf/2210.03553v1)

> Answer Set Programming (ASP) is a paradigm for modeling and solving problems for knowledge representation and reasoning. There are plenty of results dedicated to studying the hardness of (fragments of) ASP. So far, these studies resulted in characterizations in terms of computational complexity as well as in fine-grained insights presented in form of dichotomy-style results, lower bounds when translating to other formalisms like propositional satisfiability (SAT), and even detailed parameterized complexity landscapes. A generic parameter in parameterized complexity originating from graph theory is the so-called treewidth, which in a sense captures structural density of a program. Recently, there was an increase in the number of treewidth-based solvers related to SAT. While there are translations from (normal) ASP to SAT, no reduction that preserves treewidth or at least keeps track of the treewidth increase is known. In this paper we propose a novel reduction from normal ASP to SAT that is aware of the treewidth, and guarantees that a slight increase of treewidth is indeed sufficient. Further, we show a new result establishing that, when considering treewidth, already the fragment of normal ASP is slightly harder than SAT (under reasonable assumptions in computational complexity). This also confirms that our reduction probably cannot be significantly improved and that the slight increase of treewidth is unavoidable. Finally, we present an empirical study of our novel reduction from normal ASP to SAT, where we compare treewidth upper bounds that are obtained via known decomposition heuristics. Overall, our reduction works better with these heuristics than existing translations.

</details>

<details>

<summary>2022-10-07 15:18:32 - The $(1+(Î»,Î»))$ Global SEMO Algorithm</summary>

- *Benjamin Doerr, Omar El Hadri, Adrien Pinard*

- `2210.03618v1` - [abs](http://arxiv.org/abs/2210.03618v1) - [pdf](http://arxiv.org/pdf/2210.03618v1)

> The $(1+(\lambda,\lambda))$ genetic algorithm is a recently proposed single-objective evolutionary algorithm with several interesting properties. We show that its main working principle, mutation with a high rate and crossover as repair mechanism, can be transported also to multi-objective evolutionary computation. We define the $(1+(\lambda,\lambda))$ global SEMO algorithm, a variant of the classic global SEMO algorithm, and prove that it optimizes the OneMinMax benchmark asymptotically faster than the global SEMO. Following the single-objective example, we design a one-fifth rule inspired dynamic parameter setting (to the best of our knowledge for the first time in discrete multi-objective optimization) and prove that it further improves the runtime to $O(n^2)$, whereas the best runtime guarantee for the global SEMO is only $O(n^2 \log n)$.

</details>

<details>

<summary>2022-10-07 15:59:13 - Understanding the Covariance Structure of Convolutional Filters</summary>

- *Asher Trockman, Devin Willmott, J. Zico Kolter*

- `2210.03651v1` - [abs](http://arxiv.org/abs/2210.03651v1) - [pdf](http://arxiv.org/pdf/2210.03651v1)

> Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all.

</details>

<details>

<summary>2022-10-07 16:40:53 - Novice Type Error Diagnosis with Natural Language Models</summary>

- *Chuqin Geng, Haolin Ye, Yixuan Li, Tianyu Han, Brigitte Pientka, Xujie Si*

- `2210.03682v1` - [abs](http://arxiv.org/abs/2210.03682v1) - [pdf](http://arxiv.org/pdf/2210.03682v1)

> Strong static type systems help programmers eliminate many errors without much burden of supplying type annotations. However, this flexibility makes it highly non-trivial to diagnose ill-typed programs, especially for novice programmers. Compared to classic constraint solving and optimization-based approaches, the data-driven approach has shown great promise in identifying the root causes of type errors with higher accuracy. Instead of relying on hand-engineered features, this work explores natural language models for type error localization, which can be trained in an end-to-end fashion without requiring any features. We demonstrate that, for novice type error diagnosis, the language model-based approach significantly outperforms the previous state-of-the-art data-driven approach. Specifically, our model could predict type errors correctly 62% of the time, outperforming the state-of-the-art Nate's data-driven model by 11%, in a more rigorous accuracy metric. Furthermore, we also apply structural probes to explain the performance difference between different language models.

</details>

<details>

<summary>2022-10-07 17:46:46 - Automatic Discovery of Composite SPMD Partitioning Strategies in PartIR</summary>

- *Sami Alabed, Dominik Grewe, Juliana Franco, Bart Chrzaszcz, Tom Natan, Tamara Norman, Norman A. Rink, Dimitrios Vytiniotis, Michael Schaarschmidt*

- `2210.06352v1` - [abs](http://arxiv.org/abs/2210.06352v1) - [pdf](http://arxiv.org/pdf/2210.06352v1)

> Large neural network models are commonly trained through a combination of advanced parallelism strategies in a single program, multiple data (SPMD) paradigm. For example, training large transformer models requires combining data, model, and pipeline partitioning; and optimizer sharding techniques. However, identifying efficient combinations for many model architectures and accelerator systems requires significant manual analysis. In this work, we present an automatic partitioner that identifies these combinations through a goal-oriented search. Our key findings are that a Monte Carlo Tree Search-based partitioner leveraging partition-specific compiler analysis directly into the search and guided goals matches expert-level strategies for various models.

</details>

<details>

<summary>2022-10-07 19:27:05 - iMedBot: A Web-based Intelligent Agent for Healthcare Related Prediction and Deep Learning</summary>

- *Chuhan Xu, Xia Jiang*

- `2210.05671v1` - [abs](http://arxiv.org/abs/2210.05671v1) - [pdf](http://arxiv.org/pdf/2210.05671v1)

> Background: Breast cancer is a multifactorial disease, genetic and environmental factors will affect its incidence probability. Breast cancer metastasis is one of the main cause of breast cancer related deaths reported by the American Cancer Society (ACS). Method: the iMedBot is a web application that we developed using the python Flask web framework and deployed on Amazon Web Services. It contains a frontend and a backend. The backend is supported by a python program we developed using the python Keras and scikit-learn packages, which can be used to learn deep feedforward neural network (DFNN) models. Result: the iMedBot can provide two main services: 1. it can predict 5-, 10-, or 15-year breast cancer metastasis based on a set of clinical information provided by a user. The prediction is done by using a set of DFNN models that were pretrained, and 2. It can train DFNN models for a user using user-provided dataset. The model trained will be evaluated using AUC and both the AUC value and the AUC ROC curve will be provided. Conclusion: The iMedBot web application provides a user-friendly interface for user-agent interaction in conducting personalized prediction and model training. It is an initial attempt to convert results of deep learning research into an online tool that may stir further research interests in this direction. Keywords: Deep learning, Breast Cancer, Web application, Model training.

</details>

<details>

<summary>2022-10-07 19:42:09 - Can Artificial Intelligence Reconstruct Ancient Mosaics?</summary>

- *Fernando Moral-AndrÃ©s, Elena Merino-GÃ³mez, Pedro Reviriego, Fabrizio Lombardi*

- `2210.06145v1` - [abs](http://arxiv.org/abs/2210.06145v1) - [pdf](http://arxiv.org/pdf/2210.06145v1)

> A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, restoration and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward.

</details>

<details>

<summary>2022-10-08 09:55:59 - TransRepair: Context-aware Program Repair for Compilation Errors</summary>

- *Xueyang Li, Shangqing Liu, Ruitao Feng, Guozhu Meng, Xiaofei Xie, Kai Chen, Yang Liu*

- `2210.03986v1` - [abs](http://arxiv.org/abs/2210.03986v1) - [pdf](http://arxiv.org/pdf/2210.03986v1)

> Automatically fixing compilation errors can greatly raise the productivity of software development, by guiding the novice or AI programmers to write and debug code. Recently, learning-based program repair has gained extensive attention and became the state-of-the-art in practice. But it still leaves plenty of space for improvement. In this paper, we propose an end-to-end solution TransRepair to locate the error lines and create the correct substitute for a C program simultaneously. Superior to the counterpart, our approach takes into account the context of erroneous code and diagnostic compilation feedback. Then we devise a Transformer-based neural network to learn the ways of repair from the erroneous code as well as its context and the diagnostic feedback. To increase the effectiveness of TransRepair, we summarize 5 types and 74 fine-grained sub-types of compilations errors from two real-world program datasets and the Internet. Then a program corruption technique is developed to synthesize a large dataset with 1,821,275 erroneous C programs. Through the extensive experiments, we demonstrate that TransRepair outperforms the state-of-the-art in both single repair accuracy and full repair accuracy. Further analysis sheds light on the strengths and weaknesses in the contemporary solutions for future improvement.

</details>

<details>

<summary>2022-10-09 10:20:09 - Using Gradient to Boost the Generalization Performance of Deep Learning Models for Fluid Dynamics</summary>

- *Eduardo Vital Brasil*

- `2212.00716v1` - [abs](http://arxiv.org/abs/2212.00716v1) - [pdf](http://arxiv.org/pdf/2212.00716v1)

> Nowadays, Computational Fluid Dynamics (CFD) is a fundamental tool for industrial design. However, the computational cost of doing such simulations is expensive and can be detrimental for real-world use cases where many simulations are necessary, such as the task of shape optimization. Recently, Deep Learning (DL) has achieved a significant leap in a wide spectrum of applications and became a good candidate for physical systems, opening perspectives to CFD. To circumvent the computational bottleneck of CFD, DL models have been used to learn on Euclidean data, and more recently, on non-Euclidean data such as unstuctured grids and manifolds, allowing much faster and more efficient (memory, hardware) surrogate models. Nevertheless, DL presents the intrinsic limitation of extrapolating (generalizing) out of training data distribution (design space). In this study, we present a novel work to increase the generalization capabilities of Deep Learning. To do so, we incorporate the physical gradients (derivatives of the outputs w.r.t. the inputs) to the DL models. Our strategy has shown good results towards a better generalization of DL networks and our methodological/ theoretical study is corroborated with empirical validation, including an ablation study.

</details>

<details>

<summary>2022-10-09 16:56:45 - Are All Vision Models Created Equal? A Study of the Open-Loop to Closed-Loop Causality Gap</summary>

- *Mathias Lechner, Ramin Hasani, Alexander Amini, Tsun-Hsuan Wang, Thomas A. Henzinger, Daniela Rus*

- `2210.04303v1` - [abs](http://arxiv.org/abs/2210.04303v1) - [pdf](http://arxiv.org/pdf/2210.04303v1)

> There is an ever-growing zoo of modern neural network models that can efficiently learn end-to-end control from visual observations. These advanced deep models, ranging from convolutional to patch-based networks, have been extensively tested on offline image classification and regression tasks. In this paper, we study these vision architectures with respect to the open-loop to closed-loop causality gap, i.e., offline training followed by an online closed-loop deployment. This causality gap typically emerges in robotics applications such as autonomous driving, where a network is trained to imitate the control commands of a human. In this setting, two situations arise: 1) Closed-loop testing in-distribution, where the test environment shares properties with those of offline training data. 2) Closed-loop testing under distribution shifts and out-of-distribution. Contrary to recently reported results, we show that under proper training guidelines, all vision models perform indistinguishably well on in-distribution deployment, resolving the causality gap. In situation 2, We observe that the causality gap disrupts performance regardless of the choice of the model architecture. Our results imply that the causality gap can be solved in situation one with our proposed training guideline with any modern network architecture, whereas achieving out-of-distribution generalization (situation two) requires further investigations, for instance, on data diversity rather than the model architecture.

</details>

<details>

<summary>2022-10-09 19:22:55 - Tensor Program Optimization with Probabilistic Programs</summary>

- *Junru Shao, Xiyou Zhou, Siyuan Feng, Bohan Hou, Ruihang Lai, Hongyi Jin, Wuwei Lin, Masahiro Masuda, Cody Hao Yu, Tianqi Chen*

- `2205.13603v2` - [abs](http://arxiv.org/abs/2205.13603v2) - [pdf](http://arxiv.org/pdf/2205.13603v2)

> Automatic optimization for tensor programs becomes increasingly important as we deploy deep learning in various environments, and efficient optimization relies on a rich search space and effective search. Most existing efforts adopt a search space which lacks the ability to efficiently enable domain experts to grow the search space. This paper introduces MetaSchedule, a domain-specific probabilistic programming language abstraction to construct a rich search space of tensor programs. Our abstraction allows domain experts to analyze the program, and easily propose stochastic choices in a modular way to compose program transformation accordingly. We also build an end-to-end learning-driven framework to find an optimized program for a given search space. Experimental results show that MetaSchedule can cover the search space used in the state-of-the-art tensor program optimization frameworks in a modular way. Additionally, it empowers domain experts to conveniently grow the search space and modularly enhance the system, which brings 48% speedup on end-to-end deep learning workloads.

</details>

<details>

<summary>2022-10-10 02:28:53 - Subject-specific quantitative susceptibility mapping using patch based deep image priors</summary>

- *Arvind Balachandrasekaran, Davood Karimi, Camilo Jaimes, Ali Gholipour*

- `2210.06471v1` - [abs](http://arxiv.org/abs/2210.06471v1) - [pdf](http://arxiv.org/pdf/2210.06471v1)

> Quantitative Susceptibility Mapping is a parametric imaging technique to estimate the magnetic susceptibilities of biological tissues from MRI phase measurements. This problem of estimating the susceptibility map is ill posed. Regularized recovery approaches exploiting signal properties such as smoothness and sparsity improve reconstructions, but suffer from over-smoothing artifacts. Deep learning approaches have shown great potential and generate maps with reduced artifacts. However, for reasonable reconstructions and network generalization, they require numerous training datasets resulting in increased data acquisition time. To overcome this issue, we proposed a subject-specific, patch-based, unsupervised learning algorithm to estimate the susceptibility map. We make the problem well-posed by exploiting the redundancies across the patches of the map using a deep convolutional neural network. We formulated the recovery of the susceptibility map as a regularized optimization problem and adopted an alternating minimization strategy to solve it. We tested the algorithm on a 3D invivo dataset and, qualitatively and quantitatively, demonstrated improved reconstructions over competing methods.

</details>

<details>

<summary>2022-10-10 06:26:59 - Self-move and Other-move: Quantum Categorical Foundations of Japanese</summary>

- *Ryder Dale Walton*

- `2210.04451v1` - [abs](http://arxiv.org/abs/2210.04451v1) - [pdf](http://arxiv.org/pdf/2210.04451v1)

> The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological principles of these diagrams and many potential avenues for further research are proposed. Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic location. These languages are foundational to human survival, experience, flourishing, and living the good life. They are also, however, the strongest barrier between people groups. Over the last several decades, advancements in Natural Language Processing (NLP) have made it easier to bridge the gap between individuals who do not share a common language or culture. Tools like Google Translate and DeepL make it easier than ever before to share our experiences with people globally. Nevertheless, these tools are still inadequate as they fail to convey our ideas across the language barrier fluently, leaving people feeling anxious and embarrassed. This is particularly true of languages born out of substantially different cultures, such as English and Japanese. Quantum computers offer the best chance to achieve translation fluency in that they are better suited to simulating the natural world and natural phenomenon such as natural speech.   Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English grammar, translation, topology, Quantum Natural Language Processing, Natural Language Processing

</details>

<details>

<summary>2022-10-10 06:31:39 - Varying Coefficient Linear Discriminant Analysis for Dynamic Data</summary>

- *Yajie Bao, Yuyang Liu*

- `2203.06371v3` - [abs](http://arxiv.org/abs/2203.06371v3) - [pdf](http://arxiv.org/pdf/2203.06371v3)

> Linear discriminant analysis (LDA) is an important classification tool in statistics and machine learning. This paper investigates the varying coefficient LDA model for dynamic data, with Bayes' discriminant direction being a function of some exposure variable to address the heterogeneity. We propose a new least-square estimation method based on the B-spline approximation. The data-driven discriminant procedure is more computationally efficient than the dynamic linear programming rule \citep{jiang2020dynamic}. We also establish the convergence rates for the corresponding estimation error bound and the excess misclassification risk. The estimation error in $L_2$ distance is optimal for the low-dimensional regime and is near optimal for the high-dimensional regime. Numerical experiments on synthetic data and real data both corroborate the superiority of our proposed classification method.

</details>

<details>

<summary>2022-10-10 16:38:16 - Data types as a more ergonomic frontend for Grammar-Guided Genetic Programming</summary>

- *Guilherme Espada, Leon Ingelse, Paulo Canelas, Pedro Barbosa, Alcides Fonseca*

- `2210.04826v1` - [abs](http://arxiv.org/abs/2210.04826v1) - [pdf](http://arxiv.org/pdf/2210.04826v1)

> Genetic Programming (GP) is an heuristic method that can be applied to many Machine Learning, Optimization and Engineering problems. In particular, it has been widely used in Software Engineering for Test-case generation, Program Synthesis and Improvement of Software (GI).   Grammar-Guided Genetic Programming (GGGP) approaches allow the user to refine the domain of valid program solutions. Backus Normal Form is the most popular interface for describing Context-Free Grammars (CFG) for GGGP. BNF and its derivatives have the disadvantage of interleaving the grammar language and the target language of the program.   We propose to embed the grammar as an internal Domain-Specific Language in the host language of the framework. This approach has the same expressive power as BNF and EBNF while using the host language type-system to take advantage of all the existing tooling: linters, formatters, type-checkers, autocomplete, and legacy code support. These tools have a practical utility in designing software in general, and GP systems in particular.   We also present Meta-Handlers, user-defined overrides of the tree-generation system. This technique extends our object-oriented encoding with more practicability and expressive power than existing CFG approaches, achieving the same expressive power of Attribute Grammars, but without the grammar vs target language duality.   Furthermore, we evidence that this approach is feasible, showing an example Python implementation as proof. We also compare our approach against textual BNF-representations w.r.t. expressive power and ergonomics. These advantages do not come at the cost of performance, as shown by our empirical evaluation on 5 benchmarks of our example implementation against PonyGE2. We conclude that our approach has better ergonomics with the same expressive power and performance of textual BNF-based grammar encodings.

</details>

<details>

<summary>2022-10-10 18:11:05 - Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning</summary>

- *Zikang Xiong, Suresh Jagannathan*

- `2104.10219v3` - [abs](http://arxiv.org/abs/2104.10219v3) - [pdf](http://arxiv.org/pdf/2104.10219v3)

> There has been significant recent interest in devising verification techniques for learning-enabled controllers (LECs) that manage safety-critical systems. Given the opacity and lack of interpretability of the neural policies that govern the behavior of such controllers, many existing approaches enforce safety properties through shield, a dynamic monitoring-and-repairing mechanism that ensures a LEC does not emit actions that would violate desired safety conditions. These methods, however, have been shown to have significant scalability limitations because verification costs grow as problem dimensionality and objective complexity increase. In this paper, we propose a new automated verification pipeline capable of synthesizing high-quality safe controllers even when the problem domain involves hundreds of dimensions, or when the desired objective involves stochastic perturbations, liveness considerations, and other complex non-functional properties. Our key insight involves separating safety verification from neural controller training, and using pre-computed verified safety shields to constrain the training process. Experimental results over a range of high-dimensional benchmarks demonstrate the effectiveness of our approach in a range of stochastic linear time-invariant and time-variant systems.

</details>

<details>

<summary>2022-10-10 19:22:18 - Graph Neural Networks are Dynamic Programmers</summary>

- *Andrew Dudzik, Petar VeliÄkoviÄ*

- `2203.15544v3` - [abs](http://arxiv.org/abs/2203.15544v3) - [pdf](http://arxiv.org/pdf/2203.15544v3)

> Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, produce better-grounded GNN architectures for edge-centric tasks, and demonstrate empirical results on the CLRS algorithmic reasoning benchmark. We hope our exposition will serve as a foundation for building stronger algorithmically aligned GNNs.

</details>

<details>

<summary>2022-10-10 19:37:04 - Assortment Optimization Under the Multivariate MNL Model</summary>

- *Xin Chen, Jiachun Li, Menglong Li, Tiancheng Zhao, Yuan Zhou*

- `2209.15220v3` - [abs](http://arxiv.org/abs/2209.15220v3) - [pdf](http://arxiv.org/pdf/2209.15220v3)

> We study an assortment optimization problem under a multi-purchase choice model in which customers choose a bundle of up to one product from each of two product categories. Different bundles have different utilities and the bundle price is the summation of the prices of products in it. For the uncapacitated setting where any set of products can be offered, we prove that this problem is strongly NP-hard. We show that an adjusted-revenue-ordered assortment provides a 1/2-approximation. Furthermore, we develop an approximation framework based on a linear programming relaxation of the problem and obtain a 0.74-approximation algorithm. This approximation ratio almost matches the integrality gap of the linear program, which is proven to be at most 0.75. For the capacitated setting, we prove that there does not exist a constant-factor approximation algorithm assuming the Exponential Time Hypothesis. The same hardness result holds for settings with general bundle prices or more than two categories. Finally, we conduct numerical experiments on randomly generated problem instances. The average approximation ratios of our algorithms are over 99%.

</details>

<details>

<summary>2022-10-10 20:52:37 - A Quantitative Geometric Approach to Neural-Network Smoothness</summary>

- *Zi Wang, Gautam Prakriya, Somesh Jha*

- `2203.01212v2` - [abs](http://arxiv.org/abs/2203.01212v2) - [pdf](http://arxiv.org/pdf/2203.01212v2)

> Fast and precise Lipschitz constant estimation of neural networks is an important task for deep learning. Researchers have recently found an intrinsic trade-off between the accuracy and smoothness of neural networks, so training a network with a loose Lipschitz constant estimation imposes a strong regularization and can hurt the model accuracy significantly. In this work, we provide a unified theoretical framework, a quantitative geometric approach, to address the Lipschitz constant estimation. By adopting this framework, we can immediately obtain several theoretical results, including the computational hardness of Lipschitz constant estimation and its approximability. Furthermore, the quantitative geometric perspective can also provide some insights into recent empirical observations that techniques for one norm do not usually transfer to another one.   We also implement the algorithms induced from this quantitative geometric approach in a tool GeoLIP. These algorithms are based on semidefinite programming (SDP). Our empirical evaluation demonstrates that GeoLIP is more scalable and precise than existing tools on Lipschitz constant estimation for $\ell_\infty$-perturbations. Furthermore, we also show its intricate relations with other recent SDP-based techniques, both theoretically and empirically. We believe that this unified quantitative geometric perspective can bring new insights and theoretical tools to the investigation of neural-network smoothness and robustness.

</details>

<details>

<summary>2022-10-11 02:39:29 - Leveraging Artificial Intelligence on Binary Code Comprehension</summary>

- *Yifan Zhang*

- `2210.05103v1` - [abs](http://arxiv.org/abs/2210.05103v1) - [pdf](http://arxiv.org/pdf/2210.05103v1)

> Understanding binary code is an essential but complex software engineering task for reverse engineering, malware analysis, and compiler optimization. Unlike source code, binary code has limited semantic information, which makes it challenging for human comprehension. At the same time, compiling source to binary code, or transpiling among different programming languages (PLs) can provide a way to introduce external knowledge into binary comprehension. We propose to develop Artificial Intelligence (AI) models that aid human comprehension of binary code. Specifically, we propose to incorporate domain knowledge from large corpora of source code (e.g., variable names, comments) to build AI models that capture a generalizable representation of binary code. Lastly, we will investigate metrics to assess the performance of models that apply to binary code by using human studies of comprehension.

</details>

<details>

<summary>2022-10-11 07:32:56 - Abstract interpretation of Michelson smart-contracts</summary>

- *Guillaume Bau, Antoine MinÃ©, Vincent Botbol, Mehdi Bouaziz*

- `2210.05217v1` - [abs](http://arxiv.org/abs/2210.05217v1) - [pdf](http://arxiv.org/pdf/2210.05217v1)

> Static analysis of smart-contracts is becoming more widespread on blockchain platforms. Analyzers rely on techniques like symbolic execution or model checking, but few of them can provide strong soundness properties and guarantee the analysis termination at the same time. As smart-contracts often manipulate economic assets, proving numerical properties beyond the absence of runtime errors is also desirable. Smart-contract execution models differ considerably from mainstream programming languages and vary from one blockchain to another, making state-of-the-art analyses hard to adapt. For instance, smart-contract calls may modify a persistent storage impacting subsequent calls. This makes it difficult for tools to infer invariants required to formally ensure the absence of exploitable vulnerabilities. The Michelson smart-contract language, used in the Tezos blockchain, is strongly typed, stack-based, and has a strict execution model leaving few opportunities for implicit runtime errors. We present a work in progress static analyzer for Michelson based on Abstract Interpretation and implemented within MOPSA, a modular static analyzer. Our tool supports the Michelson semantic features, including inner calls to external contracts. It can prove the absence of runtime errors and infer invariants on the persistent storage over an unbounded number of calls. It is also being extended to prove high-level numerical and security properties. CCS Concepts: $\bullet$ Security and privacy $\rightarrow$ Logic and verification; $\bullet$ Software and its engineering $\rightarrow$ Automated static analysis.

</details>

<details>

<summary>2022-10-11 07:51:41 - Automating Code Review Activities by Large-Scale Pre-training</summary>

- *Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan*

- `2203.09095v2` - [abs](http://arxiv.org/abs/2203.09095v2) - [pdf](http://arxiv.org/pdf/2203.09095v2)

> Code review is an essential part to software development lifecycle since it aims at guaranteeing the quality of codes. Modern code review activities necessitate developers viewing, understanding and even running the programs to assess logic, functionality, latency, style and other factors. It turns out that developers have to spend far too much time reviewing the code of their peers. Accordingly, it is in significant demand to automate the code review process. In this research, we focus on utilizing pre-training techniques for the tasks in the code review scenario. We collect a large-scale dataset of real-world code changes and code reviews from open-source projects in nine of the most popular programming languages. To better understand code diffs and reviews, we propose CodeReviewer, a pre-trained model that utilizes four pre-training tasks tailored specifically for the code review scenario. To evaluate our model, we focus on three key tasks related to code review activities, including code change quality estimation, review comment generation and code refinement. Furthermore, we establish a high-quality benchmark dataset based on our collected data for these three tasks and conduct comprehensive experiments on it. The experimental results demonstrate that our model outperforms the previous state-of-the-art pre-training approaches in all tasks. Further analysis show that our proposed pre-training tasks and the multilingual pre-training dataset benefit the model on the understanding of code changes and reviews.

</details>

<details>

<summary>2022-10-11 08:40:40 - EOCSA: Predicting Prognosis of Epithelial Ovarian Cancer with Whole Slide Histopathological Images</summary>

- *Tianling Liu, Ran Su, Changming Sun, Xiuting Li, Leyi Wei*

- `2210.05258v1` - [abs](http://arxiv.org/abs/2210.05258v1) - [pdf](http://arxiv.org/pdf/2210.05258v1)

> Ovarian cancer is one of the most serious cancers that threaten women around the world. Epithelial ovarian cancer (EOC), as the most commonly seen subtype of ovarian cancer, has rather high mortality rate and poor prognosis among various gynecological cancers. Survival analysis outcome is able to provide treatment advices to doctors. In recent years, with the development of medical imaging technology, survival prediction approaches based on pathological images have been proposed. In this study, we designed a deep framework named EOCSA which analyzes the prognosis of EOC patients based on pathological whole slide images (WSIs). Specifically, we first randomly extracted patches from WSIs and grouped them into multiple clusters. Next, we developed a survival prediction model, named DeepConvAttentionSurv (DCAS), which was able to extract patch-level features, removed less discriminative clusters and predicted the EOC survival precisely. Particularly, channel attention, spatial attention, and neuron attention mechanisms were used to improve the performance of feature extraction. Then patient-level features were generated from our weight calculation method and the survival time was finally estimated using LASSO-Cox model. The proposed EOCSA is efficient and effective in predicting prognosis of EOC and the DCAS ensures more informative and discriminative features can be extracted. As far as we know, our work is the first to analyze the survival of EOC based on WSIs and deep neural network technologies. The experimental results demonstrate that our proposed framework has achieved state-of-the-art performance of 0.980 C-index. The implementation of the approach can be found at https://github.com/RanSuLab/EOCprognosis.

</details>

<details>

<summary>2022-10-11 11:00:22 - AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees</summary>

- *Rong Liang, Tiehua Zhang, Yujie Lu, Yuze Liu, Zhen Huang, Xin Chen*

- `2201.07984v4` - [abs](http://arxiv.org/abs/2201.07984v4) - [pdf](http://arxiv.org/pdf/2201.07984v4)

> Using the pre-trained language models to understand source codes has attracted increasing attention from financial institutions owing to the great potential to uncover financial risks. However, there are several challenges in applying these language models to solve programming language-related problems directly. For instance, the shift of domain knowledge between natural language (NL) and programming language (PL) requires understanding the semantic and syntactic information from the data from different perspectives. To this end, we propose the AstBERT model, a pre-trained PL model aiming to better understand the financial codes using the abstract syntax tree (AST). Specifically, we collect a sheer number of source codes (both Java and Python) from the Alipay code repository and incorporate both syntactic and semantic code knowledge into our model through the help of code parsers, in which AST information of the source codes can be interpreted and integrated. We evaluate the performance of the proposed model on three tasks, including code question answering, code clone detection and code refinement. Experiment results show that our AstBERT achieves promising performance on three different downstream tasks.

</details>

<details>

<summary>2022-10-11 15:20:35 - Sum-of-Max Partition under a Knapsack Constraint</summary>

- *Kai Jin, Danna Zhang, Canhui Zhang*

- `2207.00768v2` - [abs](http://arxiv.org/abs/2207.00768v2) - [pdf](http://arxiv.org/pdf/2207.00768v2)

> Sequence partition problems arise in many fields, such as sequential data analysis, information transmission, and parallel computing. In this paper, we study the following partition problem variant: given a sequence of $n$ items $1,\ldots,n$, where each item $i$ is associated with weight $w_i$ and another parameter $s_i$, partition the sequence into several consecutive subsequences, so that the total weight of each subsequence is no more than a threshold $w_0$, and the sum of the largest $s_i$ in each subsequence is minimized.   This problem admits a straightforward solution based on dynamic programming, which costs $O(n^2)$ time and can be improved to $O(n\log n)$ time easily. Our contribution is an $O(n)$ time algorithm, which is nontrivial yet easy to implement. We also study the corresponding tree partition problem. We prove that the problem on the tree is NP-complete and we present an $O(w_0 n^2)$ time ($O(w_0^2n^2)$ time, respectively) algorithm for the unit weight (integer weight, respectively) case.

</details>

<details>

<summary>2022-10-11 19:35:20 - SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics</summary>

- *Toufique Ahmed, Noah Rose Ledesma, Premkumar Devanbu*

- `2104.14671v2` - [abs](http://arxiv.org/abs/2104.14671v2) - [pdf](http://arxiv.org/pdf/2104.14671v2)

> Beginning programmers struggle with the complex grammar of modern programming languages like Java, and make lot of syntax errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and puzzling. Students could be helped, and instructors' time saved, by automated repair suggestions when dealing with syntax errors. Large samples of student errors and fixes are now available, offering the possibility of data-driven machine-learning approaches to help students fix syntax errors. Current machine-learning approaches do a reasonable job fixing syntax errors in shorter programs, but don't work as well even for moderately longer programs. We introduce SYNFIX, a machine-learning based tool that substantially improves on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised pre-training, and relying on multi-label classification rather than autoregressive synthesis to generate the (repaired) output. We describe SYNFIX's architecture in detail, and provide a detailed evaluation. We have built SYNFIX into a free, open-source version of Visual Studio Code; we make all our source code and models freely available.

</details>

<details>

<summary>2022-10-11 21:05:08 - REMS: Middleware for Robotics Education and Development</summary>

- *Yusuke Tanaka, Ankur Mehta*

- `2210.05784v1` - [abs](http://arxiv.org/abs/2210.05784v1) - [pdf](http://arxiv.org/pdf/2210.05784v1)

> This paper introduces REMS, a robotics middleware and control framework that is designed to introduce the Zen of Python to robotics and to improve robotics education and development flow. Although existing middleware can serve hardware abstraction and modularity, setting up environments and learning middleware-specific syntax and procedures are less viable in education. They can curb opportunities to understand robotics concepts, theories, and algorithms. Robotics is a field of integration; students and developers from various backgrounds will be involved in programming. Establishing Pythonic and object-oriented robotic framework in a natural way can enhance modular and abstracted programming for better readability, reusability, and simplicity, but also supports useful and practical skills generally in coding. REMS is to be a valuable robot educational medium not just as a tool and to be a platform from one robot to multi-agent across hardware, simulation, and analytical model implementations.

</details>

<details>

<summary>2022-10-11 21:12:00 - Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions</summary>

- *Rindranirina Ramamonjison, Haley Li, Timothy T. Yu, Shiqi He, Vishnu Rengan, Amin Banitalebi-Dehkordi, Zirui Zhou, Yong Zhang*

- `2209.15565v2` - [abs](http://arxiv.org/abs/2209.15565v2) - [pdf](http://arxiv.org/pdf/2209.15565v2)

> We describe an augmented intelligence system for simplifying and enhancing the modeling experience for operations research. Using this system, the user receives a suggested formulation of an optimization problem based on its description. To facilitate this process, we build an intuitive user interface system that enables the users to validate and edit the suggestions. We investigate controlled generation techniques to obtain an automatic suggestion of formulation. Then, we evaluate their effectiveness with a newly created dataset of linear programming problems drawn from various application domains.

</details>

<details>

<summary>2022-10-11 21:13:48 - Trading Off Resource Budgets for Improved Regret Bounds</summary>

- *Damon Falck, Thomas Orton*

- `2210.05789v1` - [abs](http://arxiv.org/abs/2210.05789v1) - [pdf](http://arxiv.org/pdf/2210.05789v1)

> In this work we consider a variant of adversarial online learning where in each round one picks $B$ out of $N$ arms and incurs cost equal to the $\textit{minimum}$ of the costs of each arm chosen. We propose an algorithm called Follow the Perturbed Multiple Leaders (FPML) for this problem, which we show (by adapting the techniques of Kalai and Vempala [2005]) achieves expected regret $\mathcal{O}(T^{\frac{1}{B+1}}\ln(N)^{\frac{B}{B+1}})$ over time horizon $T$ relative to the $\textit{single}$ best arm in hindsight. This introduces a trade-off between the budget $B$ and the single-best-arm regret, and we proceed to investigate several applications of this trade-off. First, we observe that algorithms which use standard regret minimizers as subroutines can sometimes be adapted by replacing these subroutines with FPML, and we use this to generalize existing algorithms for Online Submodular Function Maximization [Streeter and Golovin, 2008] in both the full feedback and semi-bandit feedback settings. Next, we empirically evaluate our new algorithms on an online black-box hyperparameter optimization problem. Finally, we show how FPML can lead to new algorithms for Linear Programming which require stronger oracles at the benefit of fewer oracle calls.

</details>

<details>

<summary>2022-10-11 22:33:14 - Patching open-vocabulary models by interpolating weights</summary>

- *Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, Ludwig Schmidt*

- `2208.05592v2` - [abs](http://arxiv.org/abs/2208.05592v2) - [pdf](http://arxiv.org/pdf/2208.05592v2)

> Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.

</details>

<details>

<summary>2022-10-11 23:05:32 - Short-term prediction of stream turbidity using surrogate data and a meta-model approach</summary>

- *Bhargav Rele, Caleb Hogan, Sevvandi Kandanaarachchi, Catherine Leigh*

- `2210.05821v1` - [abs](http://arxiv.org/abs/2210.05821v1) - [pdf](http://arxiv.org/pdf/2210.05821v1)

> Many water-quality monitoring programs aim to measure turbidity to help guide effective management of waterways and catchments, yet distributing turbidity sensors throughout networks is typically cost prohibitive. To this end, we built and compared the ability of dynamic regression (ARIMA), long short-term memory neural nets (LSTM), and generalized additive models (GAM) to forecast stream turbidity one step ahead, using surrogate data from relatively low-cost in-situ sensors and publicly available databases. We iteratively trialled combinations of four surrogate covariates (rainfall, water level, air temperature and total global solar exposure) selecting a final model for each type that minimised the corrected Akaike Information Criterion. Cross-validation using a rolling time-window indicated that ARIMA, which included the rainfall and water-level covariates only, produced the most accurate predictions, followed closely by GAM, which included all four covariates. We constructed a meta-model, trained on time-series features of turbidity, to take advantage of the strengths of each model over different time points and predict the best model (that with the lowest forecast error one-step prior) for each time step. The meta-model outperformed all other models, indicating that this methodology can yield high accuracy and may be a viable alternative to using measurements sourced directly from turbidity-sensors where costs prohibit their deployment and maintenance, and when predicting turbidity across the short term. Our findings also indicated that temperature and light-associated variables, for example underwater illuminance, may hold promise as cost-effective, high-frequency surrogates of turbidity, especially when combined with other covariates, like rainfall, that are typically measured at coarse levels of spatial resolution.

</details>

<details>

<summary>2022-10-12 00:36:01 - FasterRisk: Fast and Accurate Interpretable Risk Scores</summary>

- *Jiachang Liu, Chudi Zhong, Boxuan Li, Margo Seltzer, Cynthia Rudin*

- `2210.05846v1` - [abs](http://arxiv.org/abs/2210.05846v1) - [pdf](http://arxiv.org/pdf/2210.05846v1)

> Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a "star ray" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications.

</details>

<details>

<summary>2022-10-12 01:48:38 - A mixed-integer programming model for identifying intuitive ambulance dispatching policies</summary>

- *Laura A. Albert*

- `2202.09387v3` - [abs](http://arxiv.org/abs/2202.09387v3) - [pdf](http://arxiv.org/pdf/2202.09387v3)

> Markov decision process models and algorithms can be used to identify optimal policies for dispatching ambulances to spatially distributed customers, where the optimal policies indicate the ambulance to dispatch to each customer type in each state. Since the optimal solutions are dependent on Markov state variables, they may not always correspond to a simple set of rules when implementing the policies in practice. Restricted policies that conform to a priority list for each type of customer may be desirable for use in practice, since such policies are transparent, explainable, and easy to implement. A priority list policy is an ordered list of ambulances that indicates the preferred order to dispatch the ambulances to a customer type subject to ambulance availability. This paper proposes a constrained Markov decision process model for identifying optimal priority list policies that is formulated as a mixed integer programming model, does not extend the Markov state space, and can be solved using standard algorithms. A series of computational examples illustrate the benefit of intuitive policies. The optimal mixed integer programming solutions to the computational examples have objective function values that are close to those of the unrestricted model and are superior to those of heuristics.

</details>

<details>

<summary>2022-10-12 05:17:55 - Saliency Guided Experience Packing for Replay in Continual Learning</summary>

- *Gobinda Saha, Kaushik Roy*

- `2109.04954v2` - [abs](http://arxiv.org/abs/2109.04954v2) - [pdf](http://arxiv.org/pdf/2109.04954v2)

> Artificial learning systems aspire to mimic human intelligence by continually learning from a stream of tasks without forgetting past knowledge. One way to enable such learning is to store past experiences in the form of input examples in episodic memory and replay them when learning new tasks. However, performance of such method suffers as the size of the memory becomes smaller. In this paper, we propose a new approach for experience replay, where we select the past experiences by looking at the saliency maps which provide visual explanations for the model's decision. Guided by these saliency maps, we pack the memory with only the parts or patches of the input images important for the model's prediction. While learning a new task, we replay these memory patches with appropriate zero-padding to remind the model about its past decisions. We evaluate our algorithm on CIFAR-100, miniImageNet and CUB datasets and report better performance than the state-of-the-art approaches. With qualitative and quantitative analyses we show that our method captures richer summaries of past experiences without any memory increase, and hence performs well with small episodic memory.

</details>

<details>

<summary>2022-10-12 05:30:00 - Few-shot Backdoor Attacks via Neural Tangent Kernels</summary>

- *Jonathan Hayase, Sewoong Oh*

- `2210.05929v1` - [abs](http://arxiv.org/abs/2210.05929v1) - [pdf](http://arxiv.org/pdf/2210.05929v1)

> In a backdoor attack, an attacker injects corrupted examples into the training set. The goal of the attacker is to cause the final trained model to predict the attacker's desired target label when a predefined trigger is added to test inputs. Central to these attacks is the trade-off between the success rate of the attack and the number of corrupted training examples injected. We pose this attack as a novel bilevel optimization problem: construct strong poison examples that maximize the attack success rate of the trained model. We use neural tangent kernels to approximate the training dynamics of the model being attacked and automatically learn strong poison examples. We experiment on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt architectures on periodic and patch trigger attacks and show that NTBA-designed poisoned examples achieve, for example, an attack success rate of 90% with ten times smaller number of poison examples injected compared to the baseline. We provided an interpretation of the NTBA-designed attacks using the analysis of kernel linear regression. We further demonstrate a vulnerability in overparametrized deep neural networks, which is revealed by the shape of the neural tangent kernel.

</details>

<details>

<summary>2022-10-12 07:04:24 - Resolving the Approximability of Offline and Online Non-monotone DR-Submodular Maximization over General Convex Sets</summary>

- *Loay Mualem, Moran Feldman*

- `2210.05965v1` - [abs](http://arxiv.org/abs/2210.05965v1) - [pdf](http://arxiv.org/pdf/2210.05965v1)

> In recent years, maximization of DR-submodular continuous functions became an important research field, with many real-worlds applications in the domains of machine learning, communication systems, operation research and economics. Most of the works in this field study maximization subject to down-closed convex set constraints due to an inapproximability result by Vondr\'ak (2013). However, Durr et al. (2021) showed that one can bypass this inapproximability by proving approximation ratios that are functions of $m$, the minimum $\ell_{\infty}$-norm of any feasible vector. Given this observation, it is possible to get results for maximizing a DR-submodular function subject to general convex set constraints, which has led to multiple works on this problem. The most recent of which is a polynomial time $\tfrac{1}{4}(1 - m)$-approximation offline algorithm due to Du (2022). However, only a sub-exponential time $\tfrac{1}{3\sqrt{3}}(1 - m)$-approximation algorithm is known for the corresponding online problem. In this work, we present a polynomial time online algorithm matching the $\tfrac{1}{4}(1 - m)$-approximation of the state-of-the-art offline algorithm. We also present an inapproximability result showing that our online algorithm and Du's (2022) offline algorithm are both optimal in a strong sense. Finally, we study the empirical performance of our algorithm and the algorithm of Du (which was only theoretically studied previously), and show that they consistently outperform previously suggested algorithms on revenue maximization, location summarization and quadratic programming applications.

</details>

<details>

<summary>2022-10-12 10:30:31 - Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis</summary>

- *Giovanni De Toni, Bruno Lepri, Andrea Passerini*

- `2201.07135v2` - [abs](http://arxiv.org/abs/2201.07135v2) - [pdf](http://arxiv.org/pdf/2201.07135v2)

> Being able to provide counterfactual interventions - sequences of actions we would have had to take for a desirable outcome to happen - is essential to explain how to change an unfavourable decision by a black-box machine learning model (e.g., being denied a loan request). Existing solutions have mainly focused on generating feasible interventions without providing explanations on their rationale. Moreover, they need to solve a separate optimization problem for each user. In this paper, we take a different approach and learn a program that outputs a sequence of explainable counterfactual actions given a user description and a causal graph. We leverage program synthesis techniques, reinforcement learning coupled with Monte Carlo Tree Search for efficient exploration, and rule learning to extract explanations for each recommended action. An experimental evaluation on synthetic and real-world datasets shows how our approach generates effective interventions by making orders of magnitude fewer queries to the black-box classifier with respect to existing solutions, with the additional benefit of complementing them with interpretable explanations.

</details>

<details>

<summary>2022-10-12 12:44:33 - Integrating Accessibility in a Mobile App Development Course</summary>

- *Jaskaran Singh Bhatia, Parthasarathy P D, Snigdha Tiwari, Dhruv Nagpal, Swaroop Joshi*

- `2210.06132v1` - [abs](http://arxiv.org/abs/2210.06132v1) - [pdf](http://arxiv.org/pdf/2210.06132v1)

> The growing interest in accessible software reflects in computing educators' and education researchers' efforts to include accessibility in core computing education. We integrated accessibility in a junior/senior-level Android app development course at a large private university in India. The course introduced three accessibility-related topics using various interventions: Accessibility Awareness (a guest lecture by a legal expert), Technical Knowledge (lectures on Android accessibility guidelines and testing practices and graded components for implementing accessibility in programming assignments), and Empathy (an activity that required students to blindfold themselves and interact with their phones using a screen-reader). We evaluated their impact on student learning using three instruments: (A) A pre/post-course questionnaire, (B) Reflective questions on each of the four programming assignments, and (C) Midterm and Final exam questions. Our findings demonstrate that: (A) significantly more ($p<.05$) students considered disabilities when designing an app after taking this course, (B) many students developed empathy towards the challenges persons with disabilities face while using inaccessible apps, and (C) all students could correctly identify at least one accessibility issue in the user interface of a real-world app given its screenshot, and 90% of them could provide a correct solution to fix it.

</details>

<details>

<summary>2022-10-12 13:18:16 - Anomaly Detection using Generative Models and Sum-Product Networks in Mammography Scans</summary>

- *Marc Dietrichstein, David Major, Martin Trapp, Maria Wimmer, Dimitrios Lenis, Philip Winter, Astrid Berg, Theresa Neubauer, Katja BÃ¼hler*

- `2210.06188v1` - [abs](http://arxiv.org/abs/2210.06188v1) - [pdf](http://arxiv.org/pdf/2210.06188v1)

> Unsupervised anomaly detection models which are trained solely by healthy data, have gained importance in the recent years, as the annotation of medical data is a tedious task. Autoencoders and generative adversarial networks are the standard anomaly detection methods that are utilized to learn the data distribution. However, they fall short when it comes to inference and evaluation of the likelihood of test samples. We propose a novel combination of generative models and a probabilistic graphical model. After encoding image samples by autoencoders, the distribution of data is modeled by Random and Tensorized Sum-Product Networks ensuring exact and efficient inference at test time. We evaluate different autoencoder architectures in combination with Random and Tensorized Sum-Product Networks on mammography images using patch-wise processing and observe superior performance over utilizing the models standalone and state-of-the-art in anomaly detection for medical data.

</details>

<details>

<summary>2022-10-12 16:08:54 - Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games</summary>

- *Sihan Zeng, Thinh T. Doan, Justin Romberg*

- `2205.13746v3` - [abs](http://arxiv.org/abs/2205.13746v3) - [pdf](http://arxiv.org/pdf/2205.13746v3)

> We study the problem of finding the Nash equilibrium in a two-player zero-sum Markov game. Due to its formulation as a minimax optimization program, a natural approach to solve the problem is to perform gradient descent/ascent with respect to each player in an alternating fashion. However, due to the non-convexity/non-concavity of the underlying objective function, theoretical understandings of this method are limited. In our paper, we consider solving an entropy-regularized variant of the Markov game. The regularization introduces structure into the optimization landscape that make the solutions more identifiable and allow the problem to be solved more efficiently. Our main contribution is to show that under proper choices of the regularization parameter, the gradient descent ascent algorithm converges to the Nash equilibrium of the original unregularized problem. We explicitly characterize the finite-time performance of the last iterate of our algorithm, which vastly improves over the existing convergence bound of the gradient descent ascent algorithm without regularization. Finally, we complement the analysis with numerical simulations that illustrate the accelerated convergence of the algorithm.

</details>

<details>

<summary>2022-10-12 16:58:05 - Matching Pursuit Based Scheduling for Over-the-Air Federated Learning</summary>

- *Ali Bereyhi, Adela Vagollari, Saba Asaad, Ralf R. MÃ¼ller, Wolfgang Gerstacker, H. Vincent Poor*

- `2206.06679v2` - [abs](http://arxiv.org/abs/2206.06679v2) - [pdf](http://arxiv.org/pdf/2206.06679v2)

> This paper develops a class of low-complexity device scheduling algorithms for over-the-air federated learning via the method of matching pursuit. The proposed scheme tracks closely the close-to-optimal performance achieved by difference-of-convex programming, and outperforms significantly the well-known benchmark algorithms based on convex relaxation. Compared to the state-of-the-art, the proposed scheme poses a drastically lower computational load on the system: For $K$ devices and $N$ antennas at the parameter server, the benchmark complexity scales with $\left(N^2+K\right)^3 + N^6$ while the complexity of the proposed scheme scales with $K^p N^q$ for some $0 < p,q \leq 2$. The efficiency of the proposed scheme is confirmed via numerical experiments on the CIFAR-10 dataset.

</details>

<details>

<summary>2022-10-12 17:07:34 - Non-stationary Bandits with Knapsacks</summary>

- *Shang Liu, Jiashuo Jiang, Xiaocheng Li*

- `2205.12427v2` - [abs](http://arxiv.org/abs/2205.12427v2) - [pdf](http://arxiv.org/pdf/2205.12427v2)

> In this paper, we study the problem of bandits with knapsacks (BwK) in a non-stationary environment. The BwK problem generalizes the multi-arm bandit (MAB) problem to model the resource consumption associated with playing each arm. At each time, the decision maker/player chooses to play an arm, and s/he will receive a reward and consume certain amount of resource from each of the multiple resource types. The objective is to maximize the cumulative reward over a finite horizon subject to some knapsack constraints on the resources. Existing works study the BwK problem under either a stochastic or adversarial environment. Our paper considers a non-stationary environment which continuously interpolates between these two extremes. We first show that the traditional notion of variation budget is insufficient to characterize the non-stationarity of the BwK problem for a sublinear regret due to the presence of the constraints, and then we propose a new notion of global non-stationarity measure. We employ both non-stationarity measures to derive upper and lower bounds for the problem. Our results are based on a primal-dual analysis of the underlying linear programs and highlight the interplay between the constraints and the non-stationarity. Finally, we also extend the non-stationarity measure to the problem of online convex optimization with constraints and obtain new regret bounds accordingly.

</details>

<details>

<summary>2022-10-12 18:31:01 - AFIA: ATPG-Guided Fault Injection Attack on Secure Logic Locking</summary>

- *Yadi Zhong, Ayush Jain, M. Tanjidur Rahman, Navid Asadizanjani, Jiafeng Xie, Ujjwal Guin*

- `2206.04754v2` - [abs](http://arxiv.org/abs/2206.04754v2) - [pdf](http://arxiv.org/pdf/2206.04754v2)

> The outsourcing of the design and manufacturing of integrated circuits has raised severe concerns about the piracy of Intellectual Properties and illegal overproduction. Logic locking has emerged as an obfuscation technique to protect outsourced chip designs, where the circuit netlist is locked and can only be functional once a secure key is programmed. However, Boolean Satisfiability-based attacks have shown to break logic locking, simultaneously motivating researchers to develop more secure countermeasures. In this paper, we present a novel fault injection attack to break any locking technique that relies on a stored secret key, and denote this attack as AFIA, ATPG-guided Fault Injection Attack. The proposed attack is based on sensitizing a key bit to the primary output while injecting faults at a few other key lines that block the propagation of the targeted key bit. AIFA is very effective in determining a key bit as there exists a stuck-at fault pattern that detects a stuck-at 1 (or stuck-at 0) fault at any key line. The average complexity of number of injected faults for AFIA is linear with the key size and requires only |K| test patterns to determine a secret key, K. AFIA requires a fewer number of injected faults to sensitize a bit to the primary output, compared to 2|K|-1 faults for the differential fault analysis attack [26].

</details>

<details>

<summary>2022-10-12 19:11:51 - Blockchain for Unmanned Underwater Drones: Research Issues, Challenges, Trends and Future Directions</summary>

- *Neelu Jyoti Ahuja, Adarsh Kumar, Monika Thapliyal, Sarthika Dutt, Tanesh Kumar, Diego Augusto De Jesus Pacheco, Charalambos Konstantinou, Kim-Kwang Raymond Choo*

- `2210.06540v1` - [abs](http://arxiv.org/abs/2210.06540v1) - [pdf](http://arxiv.org/pdf/2210.06540v1)

> Underwater drones have found a place in oceanography, oceanic research, bathymetric surveys, military, surveillance, monitoring, undersea exploration, mining, commercial diving, photography and several other activities. Drones housed with several sensors and complex propulsion systems help oceanographic scientists and undersea explorers to map the seabed, study waves, view dead zones, analyze fish counts, predict tidal wave behaviors, aid in finding shipwrecks, building windfarms, examine oil platforms located in deep seas and inspect nuclear reactors in the ship vessels. While drones can be explicitly programmed for specific missions, data security and privacy are crucial issues of serious concern. Blockchain has emerged as a key enabling technology, amongst other disruptive technological enablers, to address security, data sharing, storage, process tracking, collaboration and resource management. This study presents a comprehensive review on the utilization of Blockchain in different underwater applications, discussing use cases and detailing benefits. Potential challenges of underwater applications addressed by Blockchain have been detailed. This work identifies knowledge gaps between theoretical research and real-time Blockchain integration in realistic underwater drone applications. The key limitations for effective integration of Blockchain in real-time integration in UUD applications, along with directions for future research have been presented.

</details>

<details>

<summary>2022-10-12 19:57:46 - Exact and approximation algorithms for sensor placement against DDoS attacks</summary>

- *Konstanty Junosza-Szaniawski, Dariusz Nogalski, PaweÅ RzÄÅ¼ewski*

- `2210.06559v1` - [abs](http://arxiv.org/abs/2210.06559v1) - [pdf](http://arxiv.org/pdf/2210.06559v1)

> In a DDoS attack (Distributed Denial of Service), an attacker gains control of many network users through a virus. Then the controlled users send many requests to a victim, leading to its resources being depleted. DDoS attacks are hard to defend because of their distributed nature, large scale and various attack techniques. One possible mode of defense is to place sensors in a network that can detect and stop an unwanted request. However, such sensors are expensive so there is a natural question as to the minimum number of sensors and the optimal placement required to get the necessary level of safety. Presented below are two mixed integer models for optimal sensor placement against DDoS attacks. Both models lead to a trade-off between the number of deployed sensors and the volume of uncontrolled flow. Since the above placement problems are NP-hard, two efficient heuristics are designed, implemented and compared experimentally with exact mixed integer linear programming solvers.

</details>

<details>

<summary>2022-10-12 21:17:22 - Wasserstein $K$-means for clustering probability distributions</summary>

- *Yubo Zhuang, Xiaohui Chen, Yun Yang*

- `2209.06975v2` - [abs](http://arxiv.org/abs/2209.06975v2) - [pdf](http://arxiv.org/pdf/2209.06975v2)

> Clustering is an important exploratory data analysis technique to group objects based on their similarity. The widely used $K$-means clustering method relies on some notion of distance to partition data into a fewer number of groups. In the Euclidean space, centroid-based and distance-based formulations of the $K$-means are equivalent. In modern machine learning applications, data often arise as probability distributions and a natural generalization to handle measure-valued data is to use the optimal transport metric. Due to non-negative Alexandrov curvature of the Wasserstein space, barycenters suffer from regularity and non-robustness issues. The peculiar behaviors of Wasserstein barycenters may make the centroid-based formulation fail to represent the within-cluster data points, while the more direct distance-based $K$-means approach and its semidefinite program (SDP) relaxation are capable of recovering the true cluster labels. In the special case of clustering Gaussian distributions, we show that the SDP relaxed Wasserstein $K$-means can achieve exact recovery given the clusters are well-separated under the $2$-Wasserstein metric. Our simulation and real data examples also demonstrate that distance-based $K$-means can achieve better classification performance over the standard centroid-based $K$-means for clustering probability distributions and images.

</details>

<details>

<summary>2022-10-12 21:22:01 - Neur2SP: Neural Two-Stage Stochastic Programming</summary>

- *Justin Dumouchelle, Rahul Patel, Elias B. Khalil, Merve Bodur*

- `2205.12006v2` - [abs](http://arxiv.org/abs/2205.12006v2) - [pdf](http://arxiv.org/pdf/2205.12006v2)

> Stochastic Programming is a powerful modeling framework for decision-making under uncertainty. In this work, we tackle two-stage stochastic programs (2SPs), the most widely used class of stochastic programming models. Solving 2SPs exactly requires optimizing over an expected value function that is computationally intractable. Having a mixed-integer linear program (MIP) or a nonlinear program (NLP) in the second stage further aggravates the intractability, even when specialized algorithms that exploit problem structure are employed. Finding high-quality (first-stage) solutions -- without leveraging problem structure -- can be crucial in such settings. We develop Neur2SP, a new method that approximates the expected value function via a neural network to obtain a surrogate model that can be solved more efficiently than the traditional extensive formulation approach. Neur2SP makes no assumptions about the problem structure, in particular about the second-stage problem, and can be implemented using an off-the-shelf MIP solver. Our extensive computational experiments on four benchmark 2SP problem classes with different structures (containing MIP and NLP second-stage problems) demonstrate the efficiency (time) and efficacy (solution quality) of Neur2SP. In under 1.66 seconds, Neur2SP finds high-quality solutions across all problems even as the number of scenarios increases, an ideal property that is difficult to have for traditional 2SP solution techniques. Namely, the most generic baseline method typically requires minutes to hours to find solutions of comparable quality.

</details>

<details>

<summary>2022-10-13 01:41:11 - Knowledge acquisition via interactive Distributed Cognitive skill Modules</summary>

- *Ahmet Orun*

- `2210.08007v1` - [abs](http://arxiv.org/abs/2210.08007v1) - [pdf](http://arxiv.org/pdf/2210.08007v1)

> The human's cognitive capacity for problem solving is always limited to his/her educational background, skills, experiences, etc. Hence, it is often insufficient to bring solution to extraordinary problems especially when there is a time restriction. Nowadays this sort of personal cognitive limitations are overcome at some extend by the computational utilities (e.g. program packages, internet, etc.) where each one provides a specific background skill to the individual to solve a particular problem. Nevertheless these models are all based on already available conventional tools or knowledge and unable to solve spontaneous unique problems, except human's procedural cognitive skills. But unfortunately such low-level skills can not be modelled and stored in a conventional way like classical models and knowledge. This work aims to introduce an early stage of a modular approach to procedural skill acquisition and storage via distributed cognitive skill modules which provide unique opportunity to extend the limits of its exploitation.

</details>

<details>

<summary>2022-10-13 02:35:24 - Augmenting Flight Training with AI to Efficiently Train Pilots</summary>

- *Michael Guevarra, Srijita Das, Christabel Wayllace, Carrie Demmans Epp, Matthew E. Taylor, Alan Tay*

- `2210.06683v1` - [abs](http://arxiv.org/abs/2210.06683v1) - [pdf](http://arxiv.org/pdf/2210.06683v1)

> We propose an AI-based pilot trainer to help students learn how to fly aircraft. First, an AI agent uses behavioral cloning to learn flying maneuvers from qualified flight instructors. Later, the system uses the agent's decisions to detect errors made by students and provide feedback to help students correct their errors. This paper presents an instantiation of the pilot trainer. We focus on teaching straight and level flying maneuvers by automatically providing formative feedback to the human student.

</details>

<details>

<summary>2022-10-13 09:28:18 - Fast genomic optical map assembly algorithm using binary representation</summary>

- *PrzemysÅaw Stawczyk, Robert Nowak*

- `2210.06865v1` - [abs](http://arxiv.org/abs/2210.06865v1) - [pdf](http://arxiv.org/pdf/2210.06865v1)

> Reducing the cost of sequencing genomes provided by next-generation sequencing technologies has greatly increased the number of genomic projects. As a result, there is a growing need for better assembly and assembly validation methods. One promising idea is to use heterogeneous data in assembly projects. Optical Mapping (OM) is beneficial in validating genomic assemblies, correction and scaffolding. Single raw OM read describes a DNA molecule's long fragment, up to 1Mbp. Raw OM data from the same genome could be assembled to create consensus maps that span an entire chromosome.   The assembly process is computationally hard because of the large number of errors in input data.   This work describes a new algorithm and computer program to assemble OM reads without a reference genome. In our algorithm, we explored binary representation for genome maps. We focused on the efficiency of data structures and algorithms and scale on parallel platforms. The algorithm consists of several steps, of which the most important are : (1) conversion of the restriction maps into binary strings, (2) detection of overlaps between restriction maps, (3) determining the layout of restriction maps set, (4) creation of consensus genomic maps. Our algorithm deals with optical mapping data with low error levels but fails with high-level error reads.   We developed a software library, console application and module for Python language. The approach presented in this paper proved to be faster than a dynamic programming approach and performed well on error-free data. It could be used as a step of \textit{de~novo} assembly pipelines or to detect misassemblies.The software is freely available in a public repository under GNU LGPL v3 license (https://sourceforge.net/p/binary-genome-maps/code).

</details>

<details>

<summary>2022-10-13 09:47:38 - On the Complexity of Scheduling Problems With a Fixed Number of Parallel Identical Machines</summary>

- *Klaus Jansen, Kai Kahler*

- `2202.07932v3` - [abs](http://arxiv.org/abs/2202.07932v3) - [pdf](http://arxiv.org/pdf/2202.07932v3)

> In parallel machine scheduling, we are given a set of jobs, together with a number of machines and our goal is to decide for each job, when and on which machine(s) it should be scheduled in order to minimize some objective function. Different machine models, job characteristics and objective functions result in a multitude of scheduling problems and many of them are NP-hard, even for a fixed number of identical machines. In this work, we give conditional running time lower bounds for a large number of scheduling problems, indicating the optimality of some classical algorithms. Most notably, we show that the algorithm by Lawler and Moore for $1||\sum w_jU_j$ and $Pm||C_{max}$, as well as the algorithm by Lee and Uzsoy for $P2||\sum w_jC_j$ are probably optimal. There is still small room for improvement for the $1|Rej\leq Q|\sum w_jU_j$ algorithm by Zhang et al., the algorithm for $1||\sum T_j$ by Lawler and the FPTAS for $1||\sum w_jU_j$ by Gens and Levner. We also give a lower bound for $P2|any|C_{max}$ and improve the dynamic program by Du and Leung from $\mathcal{O}(nP^2)$ to $\mathcal{O}(nP)$, matching this new lower bound. Here, $P$ is the sum of all processing times. The same idea also improves the algorithm for $P3|any|C_{max}$ by Du and Leung from $\mathcal{O}(nP^5)$ to $\mathcal{O}(nP^2)$. While our results suggest the optimality of some classical algorithms, they also motivate future research in cases where the best known algorithms do not quite match the lower bounds.

</details>

<details>

<summary>2022-10-13 10:11:41 - RaP: Redundancy-aware Video-language Pre-training for Text-Video Retrieval</summary>

- *Xing Wu, Chaochen Gao, Zijia Lin, Zhongyuan Wang, Jizhong Han, Songlin Hu*

- `2210.06881v1` - [abs](http://arxiv.org/abs/2210.06881v1) - [pdf](http://arxiv.org/pdf/2210.06881v1)

> Video language pre-training methods have mainly adopted sparse sampling techniques to alleviate the temporal redundancy of videos. Though effective, sparse sampling still suffers inter-modal redundancy: visual redundancy and textual redundancy. Compared with highly generalized text, sparsely sampled frames usually contain text-independent portions, called visual redundancy. Sparse sampling is also likely to miss important frames corresponding to some text portions, resulting in textual redundancy. Inter-modal redundancy leads to a mismatch of video and text information, hindering the model from better learning the shared semantics across modalities. To alleviate it, we propose Redundancy-aware Video-language Pre-training. We design a redundancy measurement of video patches and text tokens by calculating the cross-modal minimum dis-similarity. Then, we penalize the highredundant video patches and text tokens through a proposed redundancy-aware contrastive learning. We evaluate our method on four benchmark datasets, MSRVTT, MSVD, DiDeMo, and LSMDC, achieving a significant improvement over the previous stateof-the-art results. Our code are available at https://github.com/caskcsg/VLP/tree/main/RaP.

</details>

<details>

<summary>2022-10-13 13:37:42 - Learning to branch with Tree MDPs</summary>

- *Lara Scavuzzo, Feng Yang Chen, Didier ChÃ©telat, Maxime Gasse, Andrea Lodi, Neil Yorke-Smith, Karen Aardal*

- `2205.11107v3` - [abs](http://arxiv.org/abs/2205.11107v3) - [pdf](http://arxiv.org/pdf/2205.11107v3)

> State-of-the-art Mixed Integer Linear Program (MILP) solvers combine systematic tree search with a plethora of hard-coded heuristics, such as the branching rule. The idea of learning branching rules from data has received increasing attention recently, and promising results have been obtained by learning fast approximations of the strong branching expert. In this work, we instead propose to learn branching rules from scratch via Reinforcement Learning (RL). We revisit the work of Etheve et al. (2020) and propose tree Markov Decision Processes, or tree MDPs, a generalization of temporal MDPs that provides a more suitable framework for learning to branch. We derive a tree policy gradient theorem, which exhibits a better credit assignment compared to its temporal counterpart. We demonstrate through computational experiments that tree MDPs improve the learning convergence, and offer a promising framework for tackling the learning-to-branch problem in MILPs.

</details>

<details>

<summary>2022-10-13 14:11:34 - Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer</summary>

- *Guglielmo Camporese, Elena Izzo, Lamberto Ballan*

- `2206.00481v2` - [abs](http://arxiv.org/abs/2206.00481v2) - [pdf](http://arxiv.org/pdf/2206.00481v2)

> Vision Transformers (ViTs) enabled the use of the transformer architecture on vision tasks showing impressive performances when trained on big datasets. However, on relatively small datasets, ViTs are less accurate given their lack of inductive bias. To this end, we propose a simple but still effective Self-Supervised Learning (SSL) strategy to train ViTs, that without any external annotation or external data, can significantly improve the results. Specifically, we define a set of SSL tasks based on relations of image patches that the model has to solve before or jointly the supervised task. Differently from ViT, our RelViT model optimizes all the output tokens of the transformer encoder that are related to the image patches, thus exploiting more training signals at each training step. We investigated our methods on several image benchmarks finding that RelViT improves the SSL state-of-the-art methods by a large margin, especially on small datasets. Code is available at: https://github.com/guglielmocamporese/relvit.

</details>

<details>

<summary>2022-10-13 14:33:45 - Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors</summary>

- *Vignav Ramesh, Nathan Andrew Chi, Pranav Rajpurkar*

- `2210.06340v2` - [abs](http://arxiv.org/abs/2210.06340v2) - [pdf](http://arxiv.org/pdf/2210.06340v2)

> Current deep learning models trained to generate radiology reports from chest radiographs are capable of producing clinically accurate, clear, and actionable text that can advance patient care. However, such systems all succumb to the same problem: making hallucinated references to non-existent prior reports. Such hallucinations occur because these models are trained on datasets of real-world patient reports that inherently refer to priors. To this end, we propose two methods to remove references to priors in radiology reports: (1) a GPT-3-based few-shot approach to rewrite medical reports without references to priors; and (2) a BioBERT-based token classification approach to directly remove words referring to priors. We use the aforementioned approaches to modify MIMIC-CXR, a publicly available dataset of chest X-rays and their associated free-text radiology reports; we then retrain CXR-RePaiR, a radiology report generation system, on the adapted MIMIC-CXR dataset. We find that our re-trained model--which we call CXR-ReDonE--outperforms previous report generation methods on clinical metrics, achieving an average BERTScore of 0.2351 (2.57% absolute improvement). We expect our approach to be broadly valuable in enabling current radiology report generation systems to be more directly integrated into clinical pipelines.

</details>

<details>

<summary>2022-10-13 15:46:08 - POSE: Practical Off-chain Smart Contract Execution</summary>

- *Tommaso Frassetto, Patrick Jauernig, David Koisser, David Kretzler, Benjamin Schlosser, Sebastian Faust, Ahmad-Reza Sadeghi*

- `2210.07110v1` - [abs](http://arxiv.org/abs/2210.07110v1) - [pdf](http://arxiv.org/pdf/2210.07110v1)

> Smart contracts enable users to execute payments depending on complex program logic. Ethereum is the most notable example of a blockchain that supports smart contracts leveraged for countless applications including games, auctions and financial products. Unfortunately, the traditional method of running contract code on-chain is very expensive, for instance, on the Ethereum platform, fees have dramatically increased, rendering the system unsuitable for complex applications. A prominent solution to address this problem is to execute code off-chain and only use the blockchain as a trust anchor. While there has been significant progress in developing off-chain systems over the last years, current off-chain solutions suffer from various drawbacks including costly blockchain interactions, lack of data privacy, huge capital costs from locked collateral, or supporting only a restricted set of applications.   In this paper, we present POSE -- a practical off-chain protocol for smart contracts that addresses the aforementioned shortcomings of existing solutions. POSE leverages a pool of Trusted Execution Environments (TEEs) to execute the computation efficiently and to swiftly recover from accidental or malicious failures. We show that POSE provides strong security guarantees even if a large subset of parties is corrupted. We evaluate our proof-of-concept implementation with respect to its efficiency and effectiveness.

</details>

<details>

<summary>2022-10-13 18:18:09 - Harfang3D Dog-Fight Sandbox: A Reinforcement Learning Research Platform for the Customized Control Tasks of Fighter Aircrafts</summary>

- *Muhammed Murat Ãzbek, SÃ¼leyman YÄ±ldÄ±rÄ±m, Muhammet Aksoy, Eric Kernin, Emre Koyuncu*

- `2210.07282v1` - [abs](http://arxiv.org/abs/2210.07282v1) - [pdf](http://arxiv.org/pdf/2210.07282v1)

> The advent of deep learning (DL) gave rise to significant breakthroughs in Reinforcement Learning (RL) research. Deep Reinforcement Learning (DRL) algorithms have reached super-human level skills when applied to vision-based control problems as such in Atari 2600 games where environment states were extracted from pixel information. Unfortunately, these environments are far from being applicable to highly dynamic and complex real-world tasks as in autonomous control of a fighter aircraft since these environments only involve 2D representation of a visual world. Here, we present a semi-realistic flight simulation environment Harfang3D Dog-Fight Sandbox for fighter aircrafts. It is aimed to be a flexible toolbox for the investigation of main challenges in aviation studies using Reinforcement Learning. The program provides easy access to flight dynamics model, environment states, and aerodynamics of the plane enabling user to customize any specific task in order to build intelligent decision making (control) systems via RL. The software also allows deployment of bot aircrafts and development of multi-agent tasks. This way, multiple groups of aircrafts can be configured to be competitive or cooperative agents to perform complicated tasks including Dog Fight. During the experiments, we carried out training for two different scenarios: navigating to a designated location and within visual range (WVR) combat, shortly Dog Fight. Using Deep Reinforcement Learning techniques for both scenarios, we were able to train competent agents that exhibit human-like behaviours. Based on this results, it is confirmed that Harfang3D Dog-Fight Sandbox can be utilized as a 3D realistic RL research platform.

</details>

<details>

<summary>2022-10-13 19:53:56 - Vision Transformers provably learn spatial structure</summary>

- *Samy Jelassi, Michael E. Sander, Yuanzhi Li*

- `2210.09221v1` - [abs](http://arxiv.org/abs/2210.09221v1) - [pdf](http://arxiv.org/pdf/2210.09221v1)

> Vision Transformers (ViTs) have achieved comparable or superior performance than Convolutional Neural Networks (CNNs) in computer vision. This empirical breakthrough is even more remarkable since, in contrast to CNNs, ViTs do not embed any visual inductive bias of spatial locality. Yet, recent works have shown that while minimizing their training loss, ViTs specifically learn spatially localized patterns. This raises a central question: how do ViTs learn these patterns by solely minimizing their training loss using gradient-based methods from random initialization? In this paper, we provide some theoretical justification of this phenomenon. We propose a spatially structured dataset and a simplified ViT model. In this model, the attention matrix solely depends on the positional encodings. We call this mechanism the positional attention mechanism. On the theoretical side, we consider a binary classification task and show that while the learning problem admits multiple solutions that generalize, our model implicitly learns the spatial structure of the dataset while generalizing: we call this phenomenon patch association. We prove that patch association helps to sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but differ in the features. Lastly, we empirically verify that a ViT with positional attention performs similarly to the original one on CIFAR-10/100, SVHN and ImageNet.

</details>

<details>

<summary>2022-10-13 21:50:54 - Frustratingly Easy Sentiment Analysis of Text Streams: Generating High-Quality Emotion Arcs Using Emotion Lexicons</summary>

- *Daniela Teodorescu, Saif M. Mohammad*

- `2210.07381v1` - [abs](http://arxiv.org/abs/2210.07381v1) - [pdf](http://arxiv.org/pdf/2210.07381v1)

> Automatically generated emotion arcs -- that capture how an individual or a population feels over time -- are widely used in industry and research. However, there is little work on evaluating the generated arcs. This is in part due to the difficulty of establishing the true (gold) emotion arc. Our work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare two common ways of generating emotion arcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. Using a number of diverse datasets, we systematically study the relationship between the quality of an emotion lexicon and the quality of the emotion arc that can be generated with it. We also study the relationship between the quality of an instance-level emotion detection system (say from an ML model) and the quality of emotion arcs that can be generated with it. We show that despite being markedly poor at instance level, LexO methods are highly accurate at generating emotion arcs by aggregating information from hundreds of instances. This has wide-spread implications for commercial development, as well as research in psychology, public health, digital humanities, etc. that values simple interpretable methods and disprefers the need for domain-specific training data, programming expertise, and high-carbon-footprint models.

</details>

<details>

<summary>2022-10-14 02:30:54 - Learning Algorithms in Static Analysis of Web Applications</summary>

- *Akash Nagaraj, Bishesh Sinha, Mukund Sood, Yash Mathur, Sanchika Gupta, Dinkar Sitaram*

- `2210.07465v1` - [abs](http://arxiv.org/abs/2210.07465v1) - [pdf](http://arxiv.org/pdf/2210.07465v1)

> Web applications are distributed applications, they are programs that run on more than one computer and communicate through a network or server. This very distributed nature of web applications, combined with the scale and sheer complexity of modern software systems complicate manual security auditing, while also creating a huge attack surface of potential hackers. These factors are making automated analysis a necessity. Static Application Security Testing (SAST) is a method devised to automatically analyze application source code of large code bases without compiling it, and design conditions that are indicative of security vulnerabilities. However, the problem lies in the fact that the most widely used Static Application Security Testing Tools often yield unreliable results, owing to the false positive classification of vulnerabilities grossly outnumbering the classification of true positive vulnerabilities. This is one of the biggest hindrances to the proliferation of SAST testing, which leaves the user to review hundreds, if not thousands, of potential warnings, and classify them as either actionable or spurious. We try to minimize the problem of false positives by introducing a technique to filter the output of SAST tools. The aim of the project is to apply learning algorithms to the output by analyzing the true and false positives classified by OWASP Benchmark, and eliminate, or reduce the number of false positives presented to the user of the SAST Tool.

</details>

<details>

<summary>2022-10-14 03:20:20 - Cargo Ecosystem Dependency-Vulnerability Knowledge Graph Construction and Vulnerability Propagation Study</summary>

- *Peiyang Jia, Chengwei Liu, Hongyu Sun, Chengyi Sun, Mianxue Gu, Yang Liu, Yuqing Zhang*

- `2210.07482v1` - [abs](http://arxiv.org/abs/2210.07482v1) - [pdf](http://arxiv.org/pdf/2210.07482v1)

> Currently, little is known about the structure of the Cargo ecosystem and the potential for vulnerability propagation. Many empirical studies generalize third-party dependency governance strategies from a single software ecosystem to other ecosystems but ignore the differences in the technical structures of different software ecosystems, making it difficult to directly generalize security governance strategies from other ecosystems to the Cargo ecosystem. To fill the gap in this area, this paper constructs a knowledge graph of dependency vulnerabilities for the Cargo ecosystem using techniques related to knowledge graphs to address this challenge. This paper is the first large-scale empirical study in a related research area to address vulnerability propagation in the Cargo ecosystem. This paper proposes a dependency-vulnerability knowledge graph parsing algorithm to determine the vulnerability propagation path and propagation range and empirically studies the characteristics of vulnerabilities in the Cargo ecosystem, the propagation range, and the factors that cause vulnerability propagation. Our research has found that the Cargo ecosystem's security vulnerabilities are primarily memory-related. 18% of the libraries affected by the vulnerability is still affected by the vulnerability in the latest version of the library. The number of versions affected by the propagation of the vulnerabilities is 19.78% in the entire Cargo ecosystem. This paper looks at the characteristics and propagation factors triggering vulnerabilities in the Cargo ecosystem. It provides some practical resolution strategies for administrators of the Cargo community, developers who use Cargo to manage third-party libraries, and library owners. This paper provides new ideas for improving the overall security of the Cargo ecosystem.

</details>

<details>

<summary>2022-10-14 03:27:01 - AFETM: Adaptive function execution trace monitoring for fault diagnosis</summary>

- *Wei Zhang, Yuxi Hu, Bolong Tan, Xiaohai Shi, Jianhui Jiang*

- `2210.07486v1` - [abs](http://arxiv.org/abs/2210.07486v1) - [pdf](http://arxiv.org/pdf/2210.07486v1)

> The high tracking overhead, the amount of up-front effort required to selecting the trace points, and the lack of effective data analysis model are the significant barriers to the adoption of intra-component tracking for fault diagnosis today. This paper introduces a novel method for fault diagnosis by combining adaptive function level dynamic tracking, target fault injection, and graph convolutional network. In order to implement this method, we introduce techniques for (i) selecting function level trace points, (ii) constructing approximate function call tree of program when using adaptive tracking, and (iii) constructing graph convolutional network with fault injection campaign. We evaluate our method using a web service benchmark composed of Redis, Nginx, Httpd, and SQlite. The experimental results show that this method outperforms log based method, full tracking method, and Gaussian influence method in the accuracy of fault diagnosis, overhead, and performance impact on the diagnosis target.

</details>

<details>

<summary>2022-10-14 03:55:38 - S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces</summary>

- *Eric Nguyen, Karan Goel, Albert Gu, Gordon W. Downs, Preey Shah, Tri Dao, Stephen A. Baccus, Christopher RÃ©*

- `2210.06583v2` - [abs](http://arxiv.org/abs/2210.06583v2) - [pdf](http://arxiv.org/pdf/2210.06583v2)

> Visual data such as images and videos are typically modeled as discretizations of inherently continuous, multidimensional signals. Existing continuous-signal models attempt to exploit this fact by modeling the underlying signals of visual (e.g., image) data directly. However, these models have not yet been able to achieve competitive performance on practical vision tasks such as large-scale image and video classification. Building on a recent line of work on deep state space models (SSMs), we propose S4ND, a new multidimensional SSM layer that extends the continuous-signal modeling ability of SSMs to multidimensional data including images and videos. We show that S4ND can model large-scale visual data in $1$D, $2$D, and $3$D as continuous multidimensional signals and demonstrates strong performance by simply swapping Conv2D and self-attention layers with S4ND layers in existing state-of-the-art models. On ImageNet-1k, S4ND exceeds the performance of a Vision Transformer baseline by $1.5\%$ when training with a $1$D sequence of patches, and matches ConvNeXt when modeling images in $2$D. For videos, S4ND improves on an inflated $3$D ConvNeXt in activity classification on HMDB-51 by $4\%$. S4ND implicitly learns global, continuous convolutional kernels that are resolution invariant by construction, providing an inductive bias that enables generalization across multiple resolutions. By developing a simple bandlimiting modification to S4 to overcome aliasing, S4ND achieves strong zero-shot (unseen at training time) resolution performance, outperforming a baseline Conv2D by $40\%$ on CIFAR-10 when trained on $8 \times 8$ and tested on $32 \times 32$ images. When trained with progressive resizing, S4ND comes within $\sim 1\%$ of a high-resolution model while training $22\%$ faster.

</details>

<details>

<summary>2022-10-14 05:37:20 - When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture</summary>

- *Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, Yisen Wang*

- `2210.07540v1` - [abs](http://arxiv.org/abs/2210.07540v1) - [pdf](http://arxiv.org/pdf/2210.07540v1)

> Vision Transformers (ViTs) have recently achieved competitive performance in broad vision tasks. Unfortunately, on popular threat models, naturally trained ViTs are shown to provide no more adversarial robustness than convolutional neural networks (CNNs). Adversarial training is still required for ViTs to defend against such adversarial attacks. In this paper, we provide the first and comprehensive study on the adversarial training recipe of ViTs via extensive evaluation of various training techniques across benchmark datasets. We find that pre-training and SGD optimizer are necessary for ViTs' adversarial training. Further considering ViT as a new type of model architecture, we investigate its adversarial robustness from the perspective of its unique architectural components. We find, when randomly masking gradients from some attention blocks or masking perturbations on some patches during adversarial training, the adversarial robustness of ViTs can be remarkably improved, which may potentially open up a line of work to explore the architectural information inside the newly designed models like ViTs. Our code is available at https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.

</details>

<details>

<summary>2022-10-14 06:40:23 - Green Hierarchical Vision Transformer for Masked Image Modeling</summary>

- *Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Toshihiko Yamasaki*

- `2205.13515v2` - [abs](http://arxiv.org/abs/2205.13515v2) - [pdf](http://arxiv.org/pdf/2205.13515v2)

> We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of three key designs. First, for window attention, we propose a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. Third, as for the convolution layers, we convert them to the Sparse Convolution that works seamlessly with the sparse data, i.e., the visible patches in MIM. As a result, MIM can now work on most, if not all, hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs, e.g., Swin Transformer and Twins Transformer, about 2.7$\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks. Code and pre-trained models have been made publicly available at https://github.com/LayneH/GreenMIM.

</details>

<details>

<summary>2022-10-14 09:26:03 - Discovering and Certifying Lower Bounds for the Online Bin Stretching Problem</summary>

- *Martin BÃ¶hm, Bertrand Simon*

- `2001.01125v2` - [abs](http://arxiv.org/abs/2001.01125v2) - [pdf](http://arxiv.org/pdf/2001.01125v2)

> There are several problems in the theory of online computation where tight lower bounds on the competitive ratio are unknown and expected to be difficult to describe in a short form. A good example is the Online Bin Stretching problem, in which the task is to pack the incoming items online into bins while minimizing the load of the largest bin. Additionally, the optimal load of the entire instance is known in advance.   The contribution of this paper is twofold. We use the Coq proof assistant to formalize the Online Bin Stretching problem and provide a program certifying lower bounds of this problem. Because of the size of the certificates, previously claimed lower bounds were never formally proven. To the best of our knowledge, this is the first use of a formal verification toolkit to certify a lower bound for an online problem.   We also provide the first non-trivial lower bounds for Online Bin Stretching with 6, 7 and 8 bins, and increase the best known lower bound for 3 bins. We describe in detail the algorithmic improvements which were necessary for the discovery of the new lower bounds, which are several orders of magnitude more complex.

</details>

<details>

<summary>2022-10-14 12:23:13 - Discrete Optimal Transport with Independent Marginals is #P-Hard</summary>

- *Bahar TaÅkesen, Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, Karthik Natarajan*

- `2203.01161v2` - [abs](http://arxiv.org/abs/2203.01161v2) - [pdf](http://arxiv.org/pdf/2203.01161v2)

> We study the computational complexity of the optimal transport problem that evaluates the Wasserstein distance between the distributions of two K-dimensional discrete random vectors. The best known algorithms for this problem run in polynomial time in the maximum of the number of atoms of the two distributions. However, if the components of either random vector are independent, then this number can be exponential in K even though the size of the problem description scales linearly with K. We prove that the described optimal transport problem is #P-hard even if all components of the first random vector are independent uniform Bernoulli random variables, while the second random vector has merely two atoms, and even if only approximate solutions are sought. We also develop a dynamic programming-type algorithm that approximates the Wasserstein distance in pseudo-polynomial time when the components of the first random vector follow arbitrary independent discrete distributions, and we identify special problem instances that can be solved exactly in strongly polynomial time.

</details>

<details>

<summary>2022-10-14 14:01:59 - Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules</summary>

- *Kazuki Irie, Francesco Faccio, JÃ¼rgen Schmidhuber*

- `2206.01649v2` - [abs](http://arxiv.org/abs/2206.01649v2) - [pdf](http://arxiv.org/pdf/2206.01649v2)

> Neural ordinary differential equations (ODEs) have attracted much attention as continuous-time counterparts of deep residual neural networks (NNs), and numerous extensions for recurrent NNs have been proposed. Since the 1980s, ODEs have also been used to derive theoretical results for NN learning rules, e.g., the famous connection between Oja's rule and principal component analysis. Such rules are typically expressed as additive iterative update processes which have straightforward ODE counterparts. Here we introduce a novel combination of learning rules and Neural ODEs to build continuous-time sequence processing nets that learn to manipulate short-term memory in rapidly changing synaptic connections of other nets. This yields continuous-time counterparts of Fast Weight Programmers and linear Transformers. Our novel models outperform the best existing Neural Controlled Differential Equation based models on various time series classification tasks, while also addressing their fundamental scalability limitations. Our code is public.

</details>

<details>

<summary>2022-10-14 18:02:02 - Empirical Network Structure of Malicious Programs</summary>

- *John Musgrave, Alina Campan, Temesguen Messay-Kebede, David Kapp, Anca Ralescu*

- `2210.08034v1` - [abs](http://arxiv.org/abs/2210.08034v1) - [pdf](http://arxiv.org/pdf/2210.08034v1)

> A modern binary executable is a composition of various networks. Control flow graphs are commonly used to represent an executable program in labeled datasets used for classification tasks. Control flow and term representations are widely adopted, but provide only a partial view of program semantics. This study is an empirical analysis of the networks composing malicious binaries in order to provide a complete representation of the structural properties of a program. This is accomplished by the measurement of structural properties of program networks in a malicious binary executable dataset. We demonstrate the presence of Scale-Free properties of network structure for program data dependency and control flow graphs, and show that data dependency graphs also have Small-World structural properties. We show that program data dependency graphs have a degree correlation that is structurally disassortative, and that control flow graphs have a neutral degree assortativity, indicating the use of random graphs to model the structural properties of program control flow graphs would show increased accuracy. By providing an increase in feature resolution within labeled datasets of executable programs we provide a quantitative basis to interpret the results of classifiers trained on CFG graph features. An increase in feature resolution allows for the structural properties of program classes to be analyzed for patterns as well as their component parts. By capturing a complete picture of program graphs we can enable theoretical solutions for the mapping a program's operational semantics to its structure.

</details>

<details>

<summary>2022-10-14 19:31:39 - Zonotope Domains for Lagrangian Neural Network Verification</summary>

- *Matt Jordan, Jonathan Hayase, Alexandros G. Dimakis, Sewoong Oh*

- `2210.08069v1` - [abs](http://arxiv.org/abs/2210.08069v1) - [pdf](http://arxiv.org/pdf/2210.08069v1)

> Neural network verification aims to provide provable bounds for the output of a neural network for a given input range. Notable prior works in this domain have either generated bounds using abstract domains, which preserve some dependency between intermediate neurons in the network; or framed verification as an optimization problem and solved a relaxation using Lagrangian methods. A key drawback of the latter technique is that each neuron is treated independently, thereby ignoring important neuron interactions. We provide an approach that merges these two threads and uses zonotopes within a Lagrangian decomposition. Crucially, we can decompose the problem of verifying a deep neural network into the verification of many 2-layer neural networks. While each of these problems is provably hard, we provide efficient relaxation methods that are amenable to efficient dual ascent procedures. Our technique yields bounds that improve upon both linear programming and Lagrangian-based verification techniques in both time and bound tightness.

</details>

<details>

<summary>2022-10-15 10:17:52 - UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation</summary>

- *Yongwei Zhou, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He, Tiejun Zhao*

- `2210.08249v1` - [abs](http://arxiv.org/abs/2210.08249v1) - [pdf](http://arxiv.org/pdf/2210.08249v1)

> Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task. In this paper, we propose UniRPG, a semantic-parsing-based approach advanced in interpretability and scalability, to perform unified discrete reasoning over heterogeneous knowledge resources, i.e., table and text, as program generation. Concretely, UniRPG consists of a neural programmer and a symbolic program executor, where a program is the composition of a set of pre-defined general atomic and higher-order operations and arguments extracted from table and text. First, the programmer parses a question into a program by generating operations and copying arguments, and then the executor derives answers from table and text based on the program. To alleviate the costly program annotation issue, we design a distant supervision approach for programmer learning, where pseudo programs are automatically constructed without annotated derivations. Extensive experiments on the TAT-QA dataset show that UniRPG achieves tremendous improvements and enhances interpretability and scalability compared with state-of-the-art methods, even without derivation annotation. Moreover, it achieves promising performance on the textual dataset DROP without derivations.

</details>

<details>

<summary>2022-10-15 19:10:36 - Sampling with Riemannian Hamiltonian Monte Carlo in a Constrained Space</summary>

- *Yunbum Kook, Yin Tat Lee, Ruoqi Shen, Santosh S. Vempala*

- `2202.01908v2` - [abs](http://arxiv.org/abs/2202.01908v2) - [pdf](http://arxiv.org/pdf/2202.01908v2)

> We demonstrate for the first time that ill-conditioned, non-smooth, constrained distributions in very high dimension, upwards of 100,000, can be sampled efficiently $\textit{in practice}$. Our algorithm incorporates constraints into the Riemannian version of Hamiltonian Monte Carlo and maintains sparsity. This allows us to achieve a mixing rate independent of smoothness and condition numbers.   On benchmark data sets in systems biology and linear programming, our algorithm outperforms existing packages by orders of magnitude. In particular, we achieve a 1,000-fold speed-up for sampling from the largest published human metabolic network (RECON3D). Our package has been incorporated into the COBRA toolbox.

</details>

<details>

<summary>2022-10-16 00:37:52 - Using Answer Set Programming for HPC Dependency Solving</summary>

- *Todd Gamblin, Massimiliano Culpo, Gregory Becker, Sergei Shudler*

- `2210.08404v1` - [abs](http://arxiv.org/abs/2210.08404v1) - [pdf](http://arxiv.org/pdf/2210.08404v1)

> Modern scientific software stacks have become extremely complex, using many programming models and libraries to exploit a growing variety of GPUs and accelerators. Package managers can mitigate this complexity using dependency solvers, but they are reaching their limits. Finding compatible dependency versions is NP-complete, and modeling the semantics of package compatibility modulo build-time options, GPU runtimes, flags, and other parameters is extremely difficult. Within this enormous configuration space, defining a "good" configuration is daunting.   We tackle this problem using Answer Set Programming (ASP), a declarative model for combinatorial search problems. We show, using the Spack package manager, that ASP programs can concisely express the compatibility rules of HPC software stacks and provide strong quality-of-solution guarantees. Using ASP, we can mix new builds with preinstalled binaries, and solver performance is acceptable even when considering tens of thousands of packages.

</details>

<details>

<summary>2022-10-16 04:37:03 - Connection-Based Scheduling for Real-Time Intersection Control</summary>

- *Hsu-Chieh Hu, Joseph Zhou, Gregory J. Barlow, Stephen F. Smith*

- `2210.08445v1` - [abs](http://arxiv.org/abs/2210.08445v1) - [pdf](http://arxiv.org/pdf/2210.08445v1)

> We introduce a heuristic scheduling algorithm for real-time adaptive traffic signal control to reduce traffic congestion. This algorithm adopts a lane-based model that estimates the arrival time of all vehicles approaching an intersection through different lanes, and then computes a schedule (i.e., a signal timing plan) that minimizes the cumulative delay incurred by all approaching vehicles. State space, pruning checks and an admissible heuristic for A* search are described and shown to be capable of generating an intersection schedule in real-time (i.e., every second). Due to the effectiveness of the heuristics, the proposed approach outperforms a less expressive Dynamic Programming approach and previous A*-based approaches in run-time performance, both in simulated test environments and actual field tests.

</details>

<details>

<summary>2022-10-16 05:10:37 - A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization</summary>

- *Puyuan Liu, Xiang Zhang, Lili Mou*

- `2205.14522v2` - [abs](http://arxiv.org/abs/2205.14522v2) - [pdf](http://arxiv.org/pdf/2205.14522v2)

> Sentence summarization aims at compressing a long sentence into a short one that keeps the main gist, and has extensive real-world applications such as headline generation. In previous work, researchers have developed various approaches to improve the ROUGE score, which is the main evaluation metric for summarization, whereas controlling the summary length has not drawn much attention. In our work, we address a new problem of explicit character-level length control for summarization, and propose a dynamic programming algorithm based on the Connectionist Temporal Classification (CTC) model. Results show that our approach not only achieves higher ROUGE scores but also yields more complete sentences.

</details>

<details>

<summary>2022-10-16 10:15:46 - Time and Query Optimal Quantum Algorithms Based on Decision Trees</summary>

- *Salman Beigi, Leila Taghavi, Artin Tajdini*

- `2105.08309v2` - [abs](http://arxiv.org/abs/2105.08309v2) - [pdf](http://arxiv.org/pdf/2105.08309v2)

> It has recently been shown that starting with a classical query algorithm (decision tree) and a guessing algorithm that tries to predict the query answers, we can design a quantum algorithm with query complexity $O(\sqrt{GT})$ where $T$ is the query complexity of the classical algorithm (depth of the decision tree) and $G$ is the maximum number of wrong answers by the guessing algorithm [arXiv:1410.0932, arXiv:1905.13095]. In this paper we show that, given some constraints on the classical algorithms, this quantum algorithm can be implemented in time $\tilde O(\sqrt{GT})$. Our algorithm is based on non-binary span programs and their efficient implementation. We conclude that various graph theoretic problems including bipartiteness, cycle detection and topological sort can be solved in time $O(n^{3/2}\log n)$ and with $O(n^{3/2})$ quantum queries. Moreover, finding a maximal matching can be solved with $O(n^{3/2})$ quantum queries in time $O(n^{3/2}\log n)$, and maximum bipartite matching can be solved in time $O(n^2\log n)$.

</details>

<details>

<summary>2022-10-16 17:41:44 - Distinguishing Learning Rules with Brain Machine Interfaces</summary>

- *Jacob P. Portes, Christian Schmid, James M. Murray*

- `2206.13448v2` - [abs](http://arxiv.org/abs/2206.13448v2) - [pdf](http://arxiv.org/pdf/2206.13448v2)

> Despite extensive theoretical work on biologically plausible learning rules, clear evidence about whether and how such rules are implemented in the brain has been difficult to obtain. We consider biologically plausible supervised- and reinforcement-learning rules and ask whether changes in network activity during learning can be used to determine which learning rule is being used. Supervised learning requires a credit-assignment model estimating the mapping from neural activity to behavior, and, in a biological organism, this model will inevitably be an imperfect approximation of the ideal mapping, leading to a bias in the direction of the weight updates relative to the true gradient. Reinforcement learning, on the other hand, requires no credit-assignment model and tends to make weight updates following the true gradient direction. We derive a metric to distinguish between learning rules by observing changes in the network activity during learning, given that the mapping from brain to behavior is known by the experimenter. Because brain-machine interface (BMI) experiments allow for precise knowledge of this mapping, we model a cursor-control BMI task using recurrent neural networks, showing that learning rules can be distinguished in simulated experiments using only observations that a neuroscience experimenter would plausibly have access to.

</details>

<details>

<summary>2022-10-16 18:47:41 - Answer ranking in Community Question Answering: a deep learning approach</summary>

- *Lucas Valentin*

- `2212.01218v1` - [abs](http://arxiv.org/abs/2212.01218v1) - [pdf](http://arxiv.org/pdf/2212.01218v1)

> Community Question Answering is the field of computational linguistics that deals with problems derived from the questions and answers posted to websites such as Quora or Stack Overflow. Among some of these problems we find the issue of ranking the multiple answers posted in reply to each question by how informative they are in the attempt to solve the original question. This work tries to advance the state of the art on answer ranking for community Question Answering by proceeding with a deep learning approach. We started off by creating a large data set of questions and answers posted to the Stack Overflow website.   We then leveraged the natural language processing capabilities of dense embeddings and LSTM networks to produce a prediction for the accepted answer attribute, and present the answers in a ranked form ordered by how likely they are to be marked as accepted by the question asker. We also produced a set of numerical features to assist with the answer ranking task. These numerical features were either extracted from metadata found in the Stack Overflow posts or derived from the questions and answers texts. We compared the performance of our deep learning models against a set of forest and boosted trees ensemble methods and found that our models could not improve the best baseline results. We speculate that this lack of performance improvement versus the baseline models may be caused by the large number of out of vocabulary words present in the programming code snippets found in the questions and answers text. We conclude that while a deep learning approach may be helpful in answer ranking problems new methods should be developed to assist with the large number of out of vocabulary words present in the programming code snippets

</details>

<details>

<summary>2022-10-17 00:07:16 - From Function to Failure</summary>

- *Hamid Jahanian*

- `2210.08667v1` - [abs](http://arxiv.org/abs/2210.08667v1) - [pdf](http://arxiv.org/pdf/2210.08667v1)

> Failure Mode Reasoning (FMR) is a method for formal analysis of system-related faults. The method was originally developed for identifying failure modes of safety-critical systems based on an analysis of their programs. In this paper, we generalize the method and present a mathematical framework for its use in model-based system and safety analyses. We explain the concepts, formalize the method, formulate models for example systems, and discuss the practical application of the method.

</details>

<details>

<summary>2022-10-17 04:29:14 - Risk-Sensitive Markov Decision Processes with Long-Run CVaR Criterion</summary>

- *Li Xia, Peter W. Glynn*

- `2210.08740v1` - [abs](http://arxiv.org/abs/2210.08740v1) - [pdf](http://arxiv.org/pdf/2210.08740v1)

> CVaR (Conditional Value at Risk) is a risk metric widely used in finance. However, dynamically optimizing CVaR is difficult since it is not a standard Markov decision process (MDP) and the principle of dynamic programming fails. In this paper, we study the infinite-horizon discrete-time MDP with a long-run CVaR criterion, from the view of sensitivity-based optimization. By introducing a pseudo CVaR metric, we derive a CVaR difference formula which quantifies the difference of long-run CVaR under any two policies. The optimality of deterministic policies is derived. We obtain a so-called Bellman local optimality equation for CVaR, which is a necessary and sufficient condition for local optimal policies and only necessary for global optimal policies. A CVaR derivative formula is also derived for providing more sensitivity information. Then we develop a policy iteration type algorithm to efficiently optimize CVaR, which is shown to converge to local optima in the mixed policy space. We further discuss some extensions including the mean-CVaR optimization and the maximization of CVaR. Finally, we conduct numerical experiments relating to portfolio management to demonstrate the main results. Our work may shed light on dynamically optimizing CVaR from a sensitivity viewpoint.

</details>

<details>

<summary>2022-10-17 05:19:07 - Classification of animal sounds in a hyperdiverse rainforest using Convolutional Neural Networks</summary>

- *Yuren Sun, Tatiana Midori Maeda, Claudia Solis-Lemus, Daniel Pimentel-Alarcon, Zuzana Burivalova*

- `2111.14971v2` - [abs](http://arxiv.org/abs/2111.14971v2) - [pdf](http://arxiv.org/pdf/2111.14971v2)

> To protect tropical forest biodiversity, we need to be able to detect it reliably, cheaply, and at scale. Automated species detection from passively recorded soundscapes via machine-learning approaches is a promising technique towards this goal, but it is constrained by the necessity of large training data sets. Using soundscapes from a tropical forest in Borneo and a Convolutional Neural Network model (CNN) created with transfer learning, we investigate i) the minimum viable training data set size for accurate prediction of call types ('sonotypes'), and ii) the extent to which data augmentation can overcome the issue of small training data sets. We found that even relatively high sample sizes (> 80 per call type) lead to mediocre accuracy, which however improves significantly with data augmentation, including at extremely small sample sizes, regardless of taxonomic group or call characteristics. Our results suggest that transfer learning and data augmentation can make the use of CNNs to classify species' vocalizations feasible even for small soundscape-based projects with many rare species. Retraining our open-source model requires only basic programming skills which makes it possible for individual conservation initiatives to match their local context, in order to enable more evidence-informed management of biodiversity.

</details>

<details>

<summary>2022-10-17 07:14:08 - Enhancing Branch-and-Bound for Multi-Objective 0-1 Programming</summary>

- *Nicolas Forget, Sophie N. Parragh*

- `2210.05385v2` - [abs](http://arxiv.org/abs/2210.05385v2) - [pdf](http://arxiv.org/pdf/2210.05385v2)

> In the bi-objective branch-and-bound literature, a key ingredient is objective branching, i.e. to create smaller and disjoint sub-problems in the objective space, obtained from the partial dominance of the lower bound set by the upper bound set. When considering three or more objective functions, however, applying objective branching becomes more complex, and its benefit has so far been unclear. In this paper, we investigate several ingredients which allow to better exploit objective branching in a multi-objective setting. We extend the idea of probing to multiple objectives, enhance it in several ways, and show that when coupled with objective branching, it results in significant speed-ups in terms of CPU times. We also investigate cut generation based on the objective branching constraints. Besides, we generalize the best-bound idea for node selection to multiple objectives and we show that the proposed rules outperform the, in the multi-objective literature, commonly employed depth-first and breadth-first strategies. We also analyze problem specific branching rules. We test the proposed ideas on available benchmark instances for three problem classes with three and four objectives, namely the capacitated facility location problem, the uncapacitated facility location problem, and the knapsack problem. Our enhanced multi-objective branch-and-bound algorithm outperforms the best existing branch-and-bound based approach and is the first to obtain competitive and even slightly better results than a state-of-the-art objective space search method on a subset of the problem classes.

</details>

<details>

<summary>2022-10-17 07:26:44 - Cluster Explanation via Polyhedral Descriptions</summary>

- *Connor Lawless, Oktay Gunluk*

- `2210.08798v1` - [abs](http://arxiv.org/abs/2210.08798v1) - [pdf](http://arxiv.org/pdf/2210.08798v1)

> Clustering is an unsupervised learning problem that aims to partition unlabelled data points into groups with similar features. Traditional clustering algorithms provide limited insight into the groups they find as their main focus is accuracy and not the interpretability of the group assignments. This has spurred a recent line of work on explainable machine learning for clustering. In this paper we focus on the cluster description problem where, given a dataset and its partition into clusters, the task is to explain the clusters. We introduce a new approach to explain clusters by constructing polyhedra around each cluster while minimizing either the complexity of the resulting polyhedra or the number of features used in the description. We formulate the cluster description problem as an integer program and present a column generation approach to search over an exponential number of candidate half-spaces that can be used to build the polyhedra. To deal with large datasets, we introduce a novel grouping scheme that first forms smaller groups of data points and then builds the polyhedra around the grouped data, a strategy which out-performs simply sub-sampling data. Compared to state of the art cluster description algorithms, our approach is able to achieve competitive interpretability with improved description accuracy.

</details>

<details>

<summary>2022-10-17 13:54:00 - Learning to Find Proofs and Theorems by Learning to Refine Search Strategies: The Case of Loop Invariant Synthesis</summary>

- *Jonathan Laurent, AndrÃ© Platzer*

- `2205.14229v3` - [abs](http://arxiv.org/abs/2205.14229v3) - [pdf](http://arxiv.org/pdf/2205.14229v3)

> We propose a new approach to automated theorem proving where an AlphaZero-style agent is self-training to refine a generic high-level expert strategy expressed as a nondeterministic program. An analogous teacher agent is self-training to generate tasks of suitable relevance and difficulty for the learner. This allows leveraging minimal amounts of domain knowledge to tackle problems for which training data is unavailable or hard to synthesize. As a specific illustration, we consider loop invariant synthesis for imperative programs and use neural networks to refine both the teacher and solver strategies.

</details>

<details>

<summary>2022-10-17 13:57:07 - Detect and Classify IoT Camera Traffic</summary>

- *Priyanka Rushikesh Chaudhary, Rajib Ranjan Maiti*

- `2210.09108v1` - [abs](http://arxiv.org/abs/2210.09108v1) - [pdf](http://arxiv.org/pdf/2210.09108v1)

> Deployment of IoT cameras in an organization threatens security and privacy policies, and the classification of network traffic without using IP addresses and port numbers has been challenging. In this paper, we have designed, implemented and deployed a system called iCamInspector to classify network traffic arising from IoT camera in a mixed networking environment. We have collected a total of about 36GB of network traffic containing video data from three different types of applications (four online audio/video conferencing applications, two video sharing applications and six IoT camera from different manufacturers) in our IoT laboratory. We show that with the help of a limited number of flow-based features, iCamInspector achieves an average accuracy of more than 98% in a 10-fold cross-validation with a false rate of about 1.5% in testing phase of the system. A real deployment of our system in an unseen environment achieves a commendable performance of detecting IoT camera with an average detection probability higher than 0.9.

</details>

<details>

<summary>2022-10-17 14:36:41 - SA4U: Practical Static Analysis for Unit Type Error Detection</summary>

- *Max Taylor, Johnathon Aurand, Feng Qin, Xiaorui Wang, Brandon Henry, Xiangyu Zhang*

- `2210.09136v1` - [abs](http://arxiv.org/abs/2210.09136v1) - [pdf](http://arxiv.org/pdf/2210.09136v1)

> Unit type errors, where values with physical unit types (e.g., meters, hours) are used incorrectly in a computation, are common in today's unmanned aerial system (UAS) firmware. Recent studies show that unit type errors represent over 10% of bugs in UAS firmware. Moreover, the consequences of unit type errors are severe. Over 30% of unit type errors cause UAS crashes. This paper proposes SA4U: a practical system for detecting unit type errors in real-world UAS firmware. SA4U requires no modifications to firmware or developer annotations. It deduces the unit types of program variables by analyzing simulation traces and protocol definitions. SA4U uses the deduced unit types to identify when unit type errors occur. SA4U is effective: it identified 14 previously undetected bugs in two popular open-source firmware (ArduPilot & PX4.)

</details>

<details>

<summary>2022-10-17 15:05:02 - What is it like to program with artificial intelligence?</summary>

- *Advait Sarkar, Andrew D. Gordon, Carina Negreanu, Christian Poelitz, Sruti Srinivasa Ragavan, Ben Zorn*

- `2208.06213v2` - [abs](http://arxiv.org/abs/2208.06213v2) - [pdf](http://arxiv.org/pdf/2208.06213v2)

> Large language models, such as OpenAI's codex and Deepmind's AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot.   In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon publicly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We find that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges.   Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise.

</details>

<details>

<summary>2022-10-17 20:22:12 - Sufficient Exploration for Convex Q-learning</summary>

- *Fan Lu, Prashant Mehta, Sean Meyn, Gergely Neu*

- `2210.09409v1` - [abs](http://arxiv.org/abs/2210.09409v1) - [pdf](http://arxiv.org/pdf/2210.09409v1)

> In recent years there has been a collective research effort to find new formulations of reinforcement learning that are simultaneously more efficient and more amenable to analysis. This paper concerns one approach that builds on the linear programming (LP) formulation of optimal control of Manne. A primal version is called logistic Q-learning, and a dual variant is convex Q-learning. This paper focuses on the latter, while building bridges with the former. The main contributions follow: (i) The dual of convex Q-learning is not precisely Manne's LP or a version of logistic Q-learning, but has similar structure that reveals the need for regularization to avoid over-fitting. (ii) A sufficient condition is obtained for a bounded solution to the Q-learning LP. (iii) Simulation studies reveal numerical challenges when addressing sampled-data systems based on a continuous time model. The challenge is addressed using state-dependent sampling. The theory is illustrated with applications to examples from OpenAI gym. It is shown that convex Q-learning is successful in cases where standard Q-learning diverges, such as the LQR problem.

</details>

<details>

<summary>2022-10-17 20:59:43 - Jo Wilder and the Capitol Case: A taxonomy of uses for a historical inquiry game in 4th grade Classrooms in Wisconsin</summary>

- *Peter Wardrip, David Gagnon, James Mathews, Jen Scianna*

- `2210.09433v1` - [abs](http://arxiv.org/abs/2210.09433v1) - [pdf](http://arxiv.org/pdf/2210.09433v1)

> In this paper, we study the various ways 3rd-5th grade educators in Wisconsin utilized Jo Wilder and the Capitol Case, a historical inquiry game, as part of their classroom instruction. The 15 educators involved in the study were all grade school teachers in Wisconsin who took part in the "Doing History Fellowship" program, a professional development opportunity offered by the authors, designed to increase their understanding of historical inquiry instruction and game-based learning. As part of the program, the educators planned and implemented the game within their own classroom context and reported their results back to the authors and other educators. Through their reports, surveys and semi-structured interviews we discovered the educators were motivated by five distinct instructional purposes, which influenced how the game was integrated into their curriculum. In this paper, we name and describe these five purposes. We see these findings as useful insights into how educators think about games and how educational video games and corresponding professional development activities may be designed in the future.

</details>

<details>

<summary>2022-10-17 22:19:22 - System-Specific Interpreters Make Megasystems Friendlier</summary>

- *Matthew Sotoudeh*

- `2210.09460v1` - [abs](http://arxiv.org/abs/2210.09460v1) - [pdf](http://arxiv.org/pdf/2210.09460v1)

> Modern operating systems, browsers, and office suites have become megasystems built on millions of lines of code. Their sheer size can intimidate even experienced users and programmers away from attempting to understand and modify the software running on their machines. This paper introduces system-specific interpreters (SSIs) as a tool to help users regain knowledge of and control over megasystems. SSIs directly execute individual modules of a megasystem in a gdb-like environment without forcing the user to build, run, and trace the entire system. A prototype framework to help write SSIs is described in this paper and available for download at https://github.com/matthewsot/ssi-live22.

</details>

<details>

<summary>2022-10-18 01:30:25 - Agglomerative Hierarchical Clustering with Dynamic Time Warping for Household Load Curve Clustering</summary>

- *Fadi AlMahamid, Katarina Grolinger*

- `2210.09523v1` - [abs](http://arxiv.org/abs/2210.09523v1) - [pdf](http://arxiv.org/pdf/2210.09523v1)

> Energy companies often implement various demand response (DR) programs to better match electricity demand and supply by offering the consumers incentives to reduce their demand during critical periods. Classifying clients according to their consumption patterns enables targeting specific groups of consumers for DR. Traditional clustering algorithms use standard distance measurement to find the distance between two points. The results produced by clustering algorithms such as K-means, K-medoids, and Gaussian Mixture Models depend on the clustering parameters or initial clusters. In contrast, our methodology uses a shape-based approach that combines Agglomerative Hierarchical Clustering (AHC) with Dynamic Time Warping (DTW) to classify residential households' daily load curves based on their consumption patterns. While DTW seeks the optimal alignment between two load curves, AHC provides a realistic initial clusters center. In this paper, we compare the results with other clustering algorithms such as K-means, K-medoids, and GMM using different distance measures, and we show that AHC using DTW outperformed other clustering algorithms and needed fewer clusters.

</details>

<details>

<summary>2022-10-18 01:40:08 - Vision Transformer Visualization: What Neurons Tell and How Neurons Behave?</summary>

- *Van-Anh Nguyen, Khanh Pham Dinh, Long Tung Vuong, Thanh-Toan Do, Quan Hung Tran, Dinh Phung, Trung Le*

- `2210.07646v2` - [abs](http://arxiv.org/abs/2210.07646v2) - [pdf](http://arxiv.org/pdf/2210.07646v2)

> Recently vision transformers (ViT) have been applied successfully for various tasks in computer vision. However, important questions such as why they work or how they behave still remain largely unknown. In this paper, we propose an effective visualization technique, to assist us in exposing the information carried in neurons and feature embeddings across the ViT's layers. Our approach departs from the computational process of ViTs with a focus on visualizing the local and global information in input images and the latent feature embeddings at multiple levels. Visualizations at the input and embeddings at level 0 reveal interesting findings such as providing support as to why ViTs are rather generally robust to image occlusions and patch shuffling; or unlike CNNs, level 0 embeddings already carry rich semantic details. Next, we develop a rigorous framework to perform effective visualizations across layers, exposing the effects of ViTs filters and grouping/clustering behaviors to object patches. Finally, we provide comprehensive experiments on real datasets to qualitatively and quantitatively demonstrate the merit of our proposed methods as well as our findings. https://github.com/byM1902/ViT_visualization

</details>

<details>

<summary>2022-10-18 04:21:56 - A Novel Feature Representation for Malware Classification</summary>

- *John Musgrave, Temesguen Messay-Kebede, David Kapp, Anca Ralescu*

- `2210.09580v1` - [abs](http://arxiv.org/abs/2210.09580v1) - [pdf](http://arxiv.org/pdf/2210.09580v1)

> In this study we have presented a novel feature representation for malicious programs that can be used for malware classification. We have shown how to construct the features in a bottom-up approach, and analyzed the overlap of malicious and benign programs in terms of their components. We have shown that our method of analysis offers an increase in feature resolution that is descriptive of data movement in comparison to tf-idf features.

</details>

<details>

<summary>2022-10-18 18:27:47 - Auditing YouTube's Recommendation Algorithm for Misinformation Filter Bubbles</summary>

- *Ivan Srba, Robert Moro, Matus Tomlein, Branislav Pecher, Jakub Simko, Elena Stefancova, Michal Kompan, Andrea Hrckova, Juraj Podrouzek, Adrian Gavornik, Maria Bielikova*

- `2210.10085v1` - [abs](http://arxiv.org/abs/2210.10085v1) - [pdf](http://arxiv.org/pdf/2210.10085v1)

> In this paper, we present results of an auditing study performed over YouTube aimed at investigating how fast a user can get into a misinformation filter bubble, but also what it takes to "burst the bubble", i.e., revert the bubble enclosure. We employ a sock puppet audit methodology, in which pre-programmed agents (acting as YouTube users) delve into misinformation filter bubbles by watching misinformation promoting content. Then they try to burst the bubbles and reach more balanced recommendations by watching misinformation debunking content. We record search results, home page results, and recommendations for the watched videos. Overall, we recorded 17,405 unique videos, out of which we manually annotated 2,914 for the presence of misinformation. The labeled data was used to train a machine learning model classifying videos into three classes (promoting, debunking, neutral) with the accuracy of 0.82. We use the trained model to classify the remaining videos that would not be feasible to annotate manually.   Using both the manually and automatically annotated data, we observe the misinformation bubble dynamics for a range of audited topics. Our key finding is that even though filter bubbles do not appear in some situations, when they do, it is possible to burst them by watching misinformation debunking content (albeit it manifests differently from topic to topic). We also observe a sudden decrease of misinformation filter bubble effect when misinformation debunking videos are watched after misinformation promoting videos, suggesting a strong contextuality of recommendations. Finally, when comparing our results with a previous similar study, we do not observe significant improvements in the overall quantity of recommended misinformation content.

</details>

<details>

<summary>2022-10-19 09:52:03 - Convexity Certificates from Hessians</summary>

- *Julien Klaus, Niklas Merk, Konstantin Wiedom, SÃ¶ren Laue, Joachim Giesen*

- `2210.10430v1` - [abs](http://arxiv.org/abs/2210.10430v1) - [pdf](http://arxiv.org/pdf/2210.10430v1)

> The Hessian of a differentiable convex function is positive semidefinite. Therefore, checking the Hessian of a given function is a natural approach to certify convexity. However, implementing this approach is not straightforward since it requires a representation of the Hessian that allows its analysis. Here, we implement this approach for a class of functions that is rich enough to support classical machine learning. For this class of functions, it was recently shown how to compute computational graphs of their Hessians. We show how to check these graphs for positive semidefiniteness. We compare our implementation of the Hessian approach with the well-established disciplined convex programming (DCP) approach and prove that the Hessian approach is at least as powerful as the DCP approach for differentiable functions. Furthermore, we show for a state-of-the-art implementation of the DCP approach that, for differentiable functions, the Hessian approach is actually more powerful. That is, it can certify the convexity of a larger class of differentiable functions.

</details>

<details>

<summary>2022-10-19 10:57:33 - The Effectiveness of Social Media Engagement Strategy on Disaster Fundraising</summary>

- *Vivek Velivela, Chahat Raj, Muhammad Salman Tiwana, Raj Prasanna, Mahendra Samarawickrama, Mukesh Prasad*

- `2210.11322v1` - [abs](http://arxiv.org/abs/2210.11322v1) - [pdf](http://arxiv.org/pdf/2210.11322v1)

> Social media has been a powerful tool and an integral part of communication, especially during natural disasters. Social media platforms help nonprofits in effective disaster management by disseminating crucial information to various communities at the earliest. Besides spreading information to every corner of the world, various platforms incorporate many features that give access to host online fundraising events, process online donations, etc. The current literature lacks the theoretical structure investigating the correlation between social media engagement and crisis management. Large nonprofit organisations like the Australian Red Cross have upscaled their operations to help nearly 6,000 bushfire survivors through various grants and helped 21,563 people with psychological support and other assistance through their recovery program (Australian Red Cross, 2021). This paper considers the case of bushfires in Australia 2019-2020 to inspect the role of social media in escalating fundraising via analysing the donation data of the Australian Red Cross from October 2019 - March 2020 and analysing the level of public interaction with their Facebook page and its content in the same period.

</details>

<details>

<summary>2022-10-19 16:10:44 - BOAT: Bilateral Local Attention Vision Transformer</summary>

- *Tan Yu, Gangming Zhao, Ping Li, Yizhou Yu*

- `2201.13027v2` - [abs](http://arxiv.org/abs/2201.13027v2) - [pdf](http://arxiv.org/pdf/2201.13027v2)

> Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.

</details>

<details>

<summary>2022-10-19 16:56:11 - Dodona: learn to code with a virtual co-teacher that supports active learning</summary>

- *Charlotte Van Petegem, Rien Maertens, Niko Strijbol, Jorg Van Renterghem, Felix Van der Jeugt, Bram De Wever, Peter Dawyndt, Bart Mesuere*

- `2210.10719v1` - [abs](http://arxiv.org/abs/2210.10719v1) - [pdf](http://arxiv.org/pdf/2210.10719v1)

> Dodona (dodona.ugent.be) is an intelligent tutoring system for computer programming. It bridges the gap between assessment and learning by providing real-time data and feedback to help students learn better, teachers teach better and educational technology become more effective. We demonstrate how Dodona can be used as a virtual co-teacher to stimulate active learning and support challenge-based education in open and collaborative learning environments. We also highlight some of the opportunities (automated feedback, learning analytics, educational data mining) and challenges (scalable feedback, open internet exams, plagiarism) we faced in practice. Dodona is free for use and has more than 36 thousand registered users across many educational and research institutes, of which 15 thousand new users registered last year. Lowering the barriers for such a broad adoption was achieved by following best practices and extensible approaches for software development, authentication, content management, assessment, security and interoperability, and by adopting a holistic view on computer-assisted learning and teaching that spans all aspects of managing courses that involve programming assignments. The source code of Dodona is available on GitHub under the permissive MIT open-source license.

</details>

<details>

<summary>2022-10-19 17:56:03 - GraphCSPN: Geometry-Aware Depth Completion via Dynamic GCNs</summary>

- *Xin Liu, Xiaofei Shao, Bo Wang, Yali Li, Shengjin Wang*

- `2210.10758v1` - [abs](http://arxiv.org/abs/2210.10758v1) - [pdf](http://arxiv.org/pdf/2210.10758v1)

> Image guided depth completion aims to recover per-pixel dense depth maps from sparse depth measurements with the help of aligned color images, which has a wide range of applications from robotics to autonomous driving. However, the 3D nature of sparse-to-dense depth completion has not been fully explored by previous methods. In this work, we propose a Graph Convolution based Spatial Propagation Network (GraphCSPN) as a general approach for depth completion. First, unlike previous methods, we leverage convolution neural networks as well as graph neural networks in a complementary way for geometric representation learning. In addition, the proposed networks explicitly incorporate learnable geometric constraints to regularize the propagation process performed in three-dimensional space rather than in two-dimensional plane. Furthermore, we construct the graph utilizing sequences of feature patches, and update it dynamically with an edge attention module during propagation, so as to better capture both the local neighboring features and global relationships over long distance. Extensive experiments on both indoor NYU-Depth-v2 and outdoor KITTI datasets demonstrate that our method achieves the state-of-the-art performance, especially when compared in the case of using only a few propagation steps. Code and models are available at the project page.

</details>

<details>

<summary>2022-10-19 22:12:58 - Formal Specifications from Natural Language</summary>

- *Christopher Hahn, Frederik Schmitt, Julia J. Tillman, Niklas Metzger, Julian Siber, Bernd Finkbeiner*

- `2206.01962v2` - [abs](http://arxiv.org/abs/2206.01962v2) - [pdf](http://arxiv.org/pdf/2206.01962v2)

> We study the generalization abilities of language models when translating natural language into formal specifications with complex semantics. In particular, we fine-tune language models on three datasets consisting of English sentences and their corresponding formal representation: 1) regular expressions (regex), frequently used in programming and search; 2) First-order logic (FOL), commonly used in software verification and theorem proving; and 3) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages. Our experiments show that, in these diverse domains, the language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions. Additionally, they achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions, with the benefits of being easy to access, efficient to fine-tune, and without a particular need for domain-specific reasoning.

</details>

<details>

<summary>2022-10-20 08:13:22 - Sliced Gromov-Wasserstein</summary>

- *Titouan Vayer, RÃ©mi Flamary, Romain Tavenard, Laetitia Chapel, Nicolas Courty*

- `1905.10124v4` - [abs](http://arxiv.org/abs/1905.10124v4) - [pdf](http://arxiv.org/pdf/1905.10124v4)

> Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions whose supports do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties (e.g. duality) that permit large scale optimization. Among those, the solution of W on the real line, that only requires sorting discrete samples in 1D, allows defining the Sliced Wasserstein (SW) distance. This paper proposes a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with 1D distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being $O(n\log(n))$ to compute. We illustrate the behavior of this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute.

</details>

<details>

<summary>2022-10-20 09:33:17 - Developments in Performance and Portability for MadGraph5_aMC@NLO</summary>

- *Andrea Valassi, Taylor Childers, Laurence Field, Stefan HagebÃ¶ck, Walter Hopkins, Olivier Mattelaer, Nathan Nichols, Stefan Roiser, David Smith*

- `2210.11122v1` - [abs](http://arxiv.org/abs/2210.11122v1) - [pdf](http://arxiv.org/pdf/2210.11122v1)

> Event generators simulate particle interactions using Monte Carlo techniques, providing the primary connection between experiment and theory in experimental high energy physics. These software packages, which are the first step in the simulation worflow of collider experiments, represent approximately 5 to 20% of the annual WLCG usage for the ATLAS and CMS experiments. With computing architectures becoming more heterogeneous, it is important to ensure that these key software frameworks can be run on future systems, large and small. In this contribution, recent progress on porting and speeding up the Madgraph5_aMC@NLO event generator on hybrid architectures, i.e. CPU with GPU accelerators, is discussed. The main focus of this work has been in the calculation of scattering amplitudes and "matrix elements", which is the computational bottleneck of an event generation application. For physics processes limited to QCD leading order, the code generation toolkit has been expanded to produce matrix element calculations using C++ vector instructions on CPUs and using CUDA for NVidia GPUs, as well as using Alpaka, Kokkos and SYCL for multiple CPU and GPU architectures. Performance is reported in terms of matrix element calculations per time on NVidia, Intel, and AMD devices. The status and outlook for the integration of this work into a production release usable by the LHC experiments, with the same functionalities and very similar user interfaces as the current Fortran version, is also described.

</details>

<details>

<summary>2022-10-20 09:35:14 - Forest: Structural Code Editing with Multiple Cursors</summary>

- *Philippe Voinov, Manuel Rigger, Zhendong Su*

- `2210.11124v1` - [abs](http://arxiv.org/abs/2210.11124v1) - [pdf](http://arxiv.org/pdf/2210.11124v1)

> Software developers frequently refactor code. Often, a single logical refactoring change involves changing multiple related components in a source base such as renaming each occurrence of a variable or function. While many code editors can perform such common and generic refactorings, they do not support more complex refactorings or those that are specific to a given code base. For those, as a flexible - albeit less interactive - alternative, developers can write refactoring scripts that can implement arbitrarily complex logic by manipulating the program's tree representation. In this work, we present Forest, a structural code editor that aims to bridge the gap between the interactiveness of code editors and the expressiveness of refactoring scripts. While structural editors have occupied a niche as general code editors, the key insight of this work is that they enable a novel structural multi-cursor design that allows Forest to reach a similar expressiveness as refactoring scripts; Forest allows to perform a single action simultaneously in multiple program locations and thus support complex refactorings. To support interactivity, Forest provides features typical for text code editors such as writing and displaying the program through its textual representation. Our evaluation demonstrates that Forest allows performing edits similar to those from refactoring scripts, while still being interactive. We attempted to perform edits from 48 real-world refactoring scripts using Forest and found that 11 were possible, while another 17 would be possible with added features. We believe that a multi-cursor setting plays to the strengths of structural editing, since it benefits from reliable and expressive commands. Our results suggest that multi-cursor structural editors could be practical for performing small-scale specialized refactorings.

</details>

<details>

<summary>2022-10-20 09:49:24 - A general model-and-run solver for multistage robust discrete linear optimization</summary>

- *Michael Hartisch, Ulf Lorenz*

- `2210.11132v1` - [abs](http://arxiv.org/abs/2210.11132v1) - [pdf](http://arxiv.org/pdf/2210.11132v1)

> The necessity to deal with uncertain data is a major challenge in decision making. Robust optimization emerged as one of the predominant paradigms to produce solutions that hedge against uncertainty. In order to obtain an even more realistic description of the underlying problem where the decision maker can react to newly disclosed information, multistage models can be used. However, due to their computational difficulty, multistage problems beyond two stages have received less attention and are often only addressed using approximation rather than optimization schemes. Even less attention is paid to the consideration of decision-dependent uncertainty in a multistage setting. We explore multistage robust optimization via quantified linear programs, which are linear programs with ordered variables that are either existentially or universally quantified. Building upon a (mostly) discrete setting where the uncertain parameters -- the universally quantified variables -- are only restricted by their bounds, we present an augmented version that allows stating the discrete uncertainty set via a linear constraint system that also can be affected by decision variables. We present a general search-based solution approach and introduce our solver Yasol that is able to deal with multistage robust linear discrete optimization problems, with final mixed-integer recourse actions and a discrete uncertainty set, which even can be decision-dependent. In doing so, we provide a convenient model-and-run approach, that can serve as baseline for computational experiments in the field of multistage robust optimization, providing optimal solutions for problems with an arbitrary number of decision stages.

</details>

<details>

<summary>2022-10-20 11:38:07 - ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler</summary>

- *Jiaxin Zhang, Yashar Moshfeghi*

- `2210.10105v2` - [abs](http://arxiv.org/abs/2210.10105v2) - [pdf](http://arxiv.org/pdf/2210.10105v2)

> Numerical reasoning over text is a challenging task of Artificial Intelligence (AI), requiring reading comprehension and numerical reasoning abilities. Previous approaches use numerical reasoning programs to represent the reasoning process. However, most works do not separate the generation of operators and operands, which are key components of a numerical reasoning program, thus limiting their ability to generate such programs for complicated tasks. In this paper, we introduce the numEricaL reASoning with adapTive symbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the Encoder and a Compiler with four modules: Reasoning Manager, Operator Generator, Operands Generator, and Memory Register. ELASTIC is robust when conducting complicated reasoning. Also, it is domain agnostic by supporting the expansion of diverse operators without caring about the number of operands it contains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution accuracy and program accuracy on the FinQA dataset and 83.00 program accuracy on the MathQA dataset, outperforming previous state-of-the-art models significantly.

</details>

<details>

<summary>2022-10-20 14:18:28 - Combining BMC and Fuzzing Techniques for Finding Software Vulnerabilities in Concurrent Programs</summary>

- *Fatimah K. Aljaafari, Rafael Menezes, Edoardo Manino, Fedor Shmarov, Mustafa A. Mustafa, Lucas C. Cordeiro*

- `2206.06043v4` - [abs](http://arxiv.org/abs/2206.06043v4) - [pdf](http://arxiv.org/pdf/2206.06043v4)

> Finding software vulnerabilities in concurrent programs is a challenging task due to the size of the state-space exploration, as the number of interleavings grows exponentially with the number of program threads and statements. We propose and evaluate EBF (Ensembles of Bounded Model Checking with Fuzzing) -- a technique that combines Bounded Model Checking (BMC) and Gray-Box Fuzzing (GBF) to find software vulnerabilities in concurrent programs. Since there are no publicly-available GBF tools for concurrent code, we first propose OpenGBF -- a new open-source concurrency-aware gray-box fuzzer that explores different thread schedules by instrumenting the code under test with random delays. Then, we build an ensemble of a BMC tool and OpenGBF in the following way. On the one hand, when the BMC tool in the ensemble returns a counterexample, we use it as a seed for OpenGBF, thus increasing the likelihood of executing paths guarded by complex mathematical expressions. On the other hand, we aggregate the outcomes of the BMC and GBF tools in the ensemble using a decision matrix, thus improving the accuracy of EBF. We evaluate EBF against state-of-the-art pure BMC tools and show that it can generate up to 14.9% more correct verification witnesses than the corresponding BMC tools alone. Furthermore, we demonstrate the efficacy of OpenGBF, by showing that it can find 24.2% of the vulnerabilities in our evaluation suite, while non-concurrency-aware GBF tools can only find 0.55%. Finally, thanks to our concurrency-aware OpenGBF, EBF detects a data race in the open-source wolfMqtt library and reproduces known bugs in several other real-world programs, which demonstrates its effectiveness in finding vulnerabilities in real-world software.

</details>

<details>

<summary>2022-10-20 15:20:45 - Towards cryptographically-authenticated in-memory data structures</summary>

- *Setareh Ghorshi, Lachlan J. Gunn, Hans Liljestrand, N. Asokan*

- `2210.11340v1` - [abs](http://arxiv.org/abs/2210.11340v1) - [pdf](http://arxiv.org/pdf/2210.11340v1)

> Modern processors include high-performance cryptographic functionalities such as Intel's AES-NI and ARM's Pointer Authentication that allow programs to efficiently authenticate data held by the program. Pointer Authentication is already used to protect return addresses in recent Apple devices, but as yet these structures have seen little use for the protection of general program data.   In this paper, we show how cryptographically-authenticated data structures can be used to protect against attacks based on memory corruption, and show how they can be efficiently realized using widely available hardware-assisted cryptographic mechanisms. We present realizations of secure stacks and queues with minimal overall performance overhead (3.4%-6.4% slowdown of the OpenCV core performance tests), and provide proofs of correctness.

</details>

<details>

<summary>2022-10-20 20:20:25 - Sequential Submodular Maximization and Applications to Ranking an Assortment of Products</summary>

- *Arash Asadpour, Rad Niazadeh, Amin Saberi, Ali Shameli*

- `2002.09458v2` - [abs](http://arxiv.org/abs/2002.09458v2) - [pdf](http://arxiv.org/pdf/2002.09458v2)

> We study a submodular maximization problem motivated by applications in online retail. A platform displays a list of products to a user in response to a search query. The user inspects the first $k$ items in the list for a $k$ chosen at random from a given distribution, and decides whether to purchase an item from that set based on a choice model. The goal of the platform is to maximize the engagement of the shopper defined as the probability of purchase. This problem gives rise to a less-studied variation of submodular maximization in which we are asked to choose an $\textit{ordering}$ of a set of elements to maximize a linear combination of different submodular functions.   First, using a reduction to maximizing submodular functions over matroids, we give an optimal $\left(1-1/e\right)$-approximation for this problem. We then consider a variant in which the platform cares not only about user engagement, but also about diversification across various groups of users, that is, guaranteeing a certain probability of purchase in each group. We characterize the polytope of feasible solutions and give a bi-criteria $((1-1/e)^2,(1-1/e)^2)$-approximation for this problem by rounding an approximate solution of a linear programming relaxation. For rounding, we rely on our reduction and the particular rounding techniques for matroid polytopes. For the special case in which underlying submodular functions are coverage functions -- which is practically relevant in online retail -- we propose an alternative LP relaxation and a simpler randomized rounding for the problem. This approach yields to an optimal bi-criteria $(1-1/e,1-1/e)$-approximation algorithm for the special case of the problem with coverage functions.

</details>

<details>

<summary>2022-10-20 23:17:26 - Using Large Language Models to Enhance Programming Error Messages</summary>

- *Juho Leinonen, Arto Hellas, Sami Sarsa, Brent Reeves, Paul Denny, James Prather, Brett A. Becker*

- `2210.11630v1` - [abs](http://arxiv.org/abs/2210.11630v1) - [pdf](http://arxiv.org/pdf/2210.11630v1)

> A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix the error. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.

</details>

<details>

<summary>2022-10-21 00:34:05 - Two-stage Stochastic Matching and Pricing with Applications to Ride Hailing</summary>

- *Yiding Feng, Rad Niazadeh, Amin Saberi*

- `2210.11648v1` - [abs](http://arxiv.org/abs/2210.11648v1) - [pdf](http://arxiv.org/pdf/2210.11648v1)

> Matching and pricing are two critical levers in two-sided marketplaces to connect demand and supply. The platform can produce more efficient matching and pricing decisions by batching the demand requests. We initiate the study of the two-stage stochastic matching problem, with or without pricing, to enable the platform to make improved decisions in a batch with an eye toward the imminent future demand requests. This problem is motivated in part by applications in online marketplaces such as ride hailing platforms.   We design online competitive algorithms for vertex-weighted (or unweighted) two-stage stochastic matching for maximizing supply efficiency, and two-stage joint matching and pricing for maximizing market efficiency. In the former problem, using a randomized primal-dual algorithm applied to a family of ``balancing'' convex programs, we obtain the optimal $3/4$ competitive ratio against the optimum offline benchmark. Using a factor revealing program and connections to submodular optimization, we improve this ratio against the optimum online benchmark to $(1-1/e+1/e^2)\approx 0.767$ for the unweighted and $0.761$ for the weighted case. In the latter problem, we design optimal $1/2$-competitive joint pricing and matching algorithm by borrowing ideas from the ex-ante prophet inequality literature. We also show an improved $(1-1/e)$-competitive algorithm for the special case of demand efficiency objective using the correlation gap of submodular functions. Finally, we complement our theoretical study by using DiDi's ride-sharing dataset for Chengdu city and numerically evaluating the performance of our proposed algorithms in practical instances of this problem.

</details>

<details>

<summary>2022-10-21 01:02:10 - Evolutionary sparse data-driven discovery of complex multibody system dynamics</summary>

- *Ehsan Askari, Guillaume Crevecoeur*

- `2210.11656v1` - [abs](http://arxiv.org/abs/2210.11656v1) - [pdf](http://arxiv.org/pdf/2210.11656v1)

> The value of unknown parameters of multibody systems is crucial for prediction, monitoring, and control, sometimes estimated using a biased physics-based model leading to incorrect outcomes. Discovering motion equations of multibody systems from time-series data is challenging as they consist of complex rational functions, constants as function arguments, and diverse function terms, which are not trivial to guess. This study aims at developing an evolutionary symbolic sparse regression approach for the system identification of multibody systems. The procedure discovers equations of motion and system parameters appearing as either constant values in function arguments or coefficients of function expressions. A genetic programming algorithm is written to generate symbolic function expressions, in which a hard-thresholding regression method is embedded. In an evolutionary manner, the complex functional forms, constant arguments, and unknown coefficients are identified to eventually discover the governing equation of a given system. A fitness measure is presented to promote parsimony in distilled equations and reduction in fit-to-data error. Hybrid discrete-continuous dynamical systems are also investigated, for which an approach is suggested to determine both mode number and system submodels. The performance and efficiency of the suggested evolutionary symbolic sparse regression methodology are evaluated in a simulation environment. The capability of the developed approach is also demonstrated by studying several multibody systems. The procedure is efficient and gives the possibility to estimate system parameters and distill respective governing equations. This technique reduces the risk that the function dictionary does not cover all functionality required to unravel hidden physical laws and the need for prior knowledge of the mechanism of interest.

</details>

<details>

<summary>2022-10-21 01:32:49 - Doctors Handwritten Prescription Recognition System In Multi Language Using Deep Learning</summary>

- *Pavithiran G, Sharan Padmanabhan, Nuvvuru Divya, Aswathy V, Irene Jerusha P, Chandar B*

- `2210.11666v1` - [abs](http://arxiv.org/abs/2210.11666v1) - [pdf](http://arxiv.org/pdf/2210.11666v1)

> Doctors typically write in incomprehensible handwriting, making it difficult for both the general public and some pharmacists to understand the medications they have prescribed. It is not ideal for them to write the prescription quietly and methodically because they will be dealing with dozens of patients every day and will be swamped with work.As a result, their handwriting is illegible. This may result in reports or prescriptions consisting of short forms and cursive writing that a typical person or pharmacist won't be able to read properly, which will cause prescribed medications to be misspelled. However, some individuals are accustomed to writing prescriptions in regional languages because we all live in an area with a diversity of regional languages. It makes analyzing the content much more challenging. So, in this project, we'll use a recognition system to build a tool that can translate the handwriting of physicians in any language. This system will be made into an application which is fully autonomous in functioning. As the user uploads the prescription image the program will pre-process the image by performing image pre-processing, and word segmentations initially before processing the image for training. And it will be done for every language we require the model to detect. And as of the deduction model will be made using deep learning techniques including CNN, RNN, and LSTM, which are utilized to train the model. To match words from various languages that will be written in the system, Unicode will be used. Furthermore, fuzzy search and market basket analysis are employed to offer an end result that will be optimized from the pharmaceutical database and displayed to the user as a structured output.

</details>

<details>

<summary>2022-10-21 03:50:58 - Minimizing Trust with Exclusively-Used Physically-Isolated Hardware</summary>

- *Zhihao Yao, Seyed Mohammadjavad Seyed Talebi, Mingyi Chen, Ardalan Amiri Sani, Thomas Anderson*

- `2203.08284v2` - [abs](http://arxiv.org/abs/2203.08284v2) - [pdf](http://arxiv.org/pdf/2203.08284v2)

> Smartphone owners often need to run security-critical programs on the same device as other untrusted and potentially malicious programs. This requires users to trust hardware and system software to correctly sandbox malicious programs, trust that is often misplaced.   Our goal is to minimize the number and complexity of hardware and software components that a smartphone owner needs to trust to withstand adversarial inputs. We present a multi-domain hardware design composed of statically-partitioned, physically-isolated trust domains. We introduce a few simple, formally-verified hardware components to enable a program to gain provably exclusive and simultaneous access to both computation and I/O on a temporary basis. To manage this hardware, we present OctopOS, an OS composed of mutually distrustful subsystems.   We present a prototype of this machine (hardware and OS) on a CPU-FPGA board and show that it incurs a small hardware cost compared to modern SoCs. For security-critical programs, we show that this machine significantly reduces the required trust compared to mainstream TEEs while achieving decent performance. For normal programs, performance is similar to a legacy machine.

</details>

<details>

<summary>2022-10-21 07:38:12 - DARWIN: Survival of the Fittest Fuzzing Mutators</summary>

- *Patrick Jauernig, Domagoj Jakobovic, Stjepan Picek, Emmanuel Stapf, Ahmad-Reza Sadeghi*

- `2210.11783v1` - [abs](http://arxiv.org/abs/2210.11783v1) - [pdf](http://arxiv.org/pdf/2210.11783v1)

> Fuzzing is an automated software testing technique broadly adopted by the industry. A popular variant is mutation-based fuzzing, which discovers a large number of bugs in practice. While the research community has studied mutation-based fuzzing for years now, the algorithms' interactions within the fuzzer are highly complex and can, together with the randomness in every instance of a fuzzer, lead to unpredictable effects. Most efforts to improve this fragile interaction focused on optimizing seed scheduling. However, real-world results like Google's FuzzBench highlight that these approaches do not consistently show improvements in practice. Another approach to improve the fuzzing process algorithmically is optimizing mutation scheduling. Unfortunately, existing mutation scheduling approaches also failed to convince because of missing real-world improvements or too many user-controlled parameters whose configuration requires expert knowledge about the target program. This leaves the challenging problem of cleverly processing test cases and achieving a measurable improvement unsolved.   We present DARWIN, a novel mutation scheduler and the first to show fuzzing improvements in a realistic scenario without the need to introduce additional user-configurable parameters, opening this approach to the broad fuzzing community. DARWIN uses an Evolution Strategy to systematically optimize and adapt the probability distribution of the mutation operators during fuzzing. We implemented a prototype based on the popular general-purpose fuzzer AFL. DARWIN significantly outperforms the state-of-the-art mutation scheduler and the AFL baseline in our own coverage experiment, in FuzzBench, and by finding 15 out of 21 bugs the fastest in the MAGMA benchmark. Finally, DARWIN found 20 unique bugs (including one novel bug), 66% more than AFL, in widely-used real-world applications.

</details>

<details>

<summary>2022-10-21 07:40:54 - Few-shot Backdoor Defense Using Shapley Estimation</summary>

- *Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao*

- `2112.14889v2` - [abs](http://arxiv.org/abs/2112.14889v2) - [pdf](http://arxiv.org/pdf/2112.14889v2)

> Deep neural networks have achieved impressive performance in a variety of tasks over the last decade, such as autonomous driving, face recognition, and medical diagnosis. However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject malicious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Pruning (ShapPruning) that successfully mitigates backdoor attacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model's structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and $\epsilon$-greedy strategy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Experiments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to existing methods.

</details>

<details>

<summary>2022-10-21 08:49:30 - Valuing Vicinity: Memory attention framework for context-based semantic segmentation in histopathology</summary>

- *Oliver Ester, Fabian HÃ¶rst, Constantin Seibold, Julius Keyl, Saskia Ting, Nikolaos Vasileiadis, Jessica Schmitz, Philipp Ivanyi, Viktor GrÃ¼nwald, Jan Hinrich BrÃ¤sen, Jan Egger, Jens Kleesiek*

- `2210.11822v1` - [abs](http://arxiv.org/abs/2210.11822v1) - [pdf](http://arxiv.org/pdf/2210.11822v1)

> The segmentation of histopathological whole slide images into tumourous and non-tumourous types of tissue is a challenging task that requires the consideration of both local and global spatial contexts to classify tumourous regions precisely. The identification of subtypes of tumour tissue complicates the issue as the sharpness of separation decreases and the pathologist's reasoning is even more guided by spatial context. However, the identification of detailed types of tissue is crucial for providing personalized cancer therapies. Due to the high resolution of whole slide images, existing semantic segmentation methods, restricted to isolated image sections, are incapable of processing context information beyond. To take a step towards better context comprehension, we propose a patch neighbour attention mechanism to query the neighbouring tissue context from a patch embedding memory bank and infuse context embeddings into bottleneck hidden feature maps. Our memory attention framework (MAF) mimics a pathologist's annotation procedure -- zooming out and considering surrounding tissue context. The framework can be integrated into any encoder-decoder segmentation method. We evaluate the MAF on a public breast cancer and an internal kidney cancer data set using famous segmentation models (U-Net, DeeplabV3) and demonstrate the superiority over other context-integrating algorithms -- achieving a substantial improvement of up to $17\%$ on Dice score. The code is publicly available at: https://github.com/tio-ikim/valuing-vicinity

</details>

<details>

<summary>2022-10-21 09:16:43 - Masked Autoencoders As Spatiotemporal Learners</summary>

- *Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, Kaiming He*

- `2205.09113v2` - [abs](http://arxiv.org/abs/2205.09113v2) - [pdf](http://arxiv.org/pdf/2205.09113v2)

> This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.

</details>

<details>

<summary>2022-10-21 12:17:12 - Boosting vision transformers for image retrieval</summary>

- *Chull Hwan Song, Jooyoung Yoon, Shunghyun Choi, Yannis Avrithis*

- `2210.11909v1` - [abs](http://arxiv.org/abs/2210.11909v1) - [pdf](http://arxiv.org/pdf/2210.11909v1)

> Vision transformers have achieved remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image representation. (3) In each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation. Public code is available at https://github.com/dealicious-inc/DToP.

</details>

<details>

<summary>2022-10-21 14:50:15 - Assaying Out-Of-Distribution Generalization in Transfer Learning</summary>

- *Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, Bernhard SchÃ¶lkopf, Francesco Locatello*

- `2207.09239v2` - [abs](http://arxiv.org/abs/2207.09239v2) - [pdf](http://arxiv.org/pdf/2207.09239v2)

> Since out-of-distribution generalization is a generally ill-posed problem, various proxy targets (e.g., calibration, adversarial robustness, algorithmic corruptions, invariance across shifts) were studied across different research programs resulting in different recommendations. While sharing the same aspirational goal, these approaches have never been tested under the same experimental conditions on real data. In this paper, we take a unified view of previous work, highlighting message discrepancies that we address empirically, and providing recommendations on how to measure the robustness of a model and how to improve it. To this end, we collect 172 publicly available dataset pairs for training and out-of-distribution evaluation of accuracy, calibration error, adversarial attacks, environment invariance, and synthetic corruptions. We fine-tune over 31k networks, from nine different architectures in the many- and few-shot setting. Our findings confirm that in- and out-of-distribution accuracies tend to increase jointly, but show that their relation is largely dataset-dependent, and in general more nuanced and more complex than posited by previous, smaller scale studies.

</details>

<details>

<summary>2022-10-21 15:09:32 - Learning-Augmented Algorithms for Online Linear and Semidefinite Programming</summary>

- *Elena Grigorescu, Young-San Lin, Sandeep Silwal, Maoyuan Song, Samson Zhou*

- `2209.10614v2` - [abs](http://arxiv.org/abs/2209.10614v2) - [pdf](http://arxiv.org/pdf/2209.10614v2)

> Semidefinite programming (SDP) is a unifying framework that generalizes both linear programming and quadratically-constrained quadratic programming, while also yielding efficient solvers, both in theory and in practice. However, there exist known impossibility results for approximating the optimal solution when constraints for covering SDPs arrive in an online fashion. In this paper, we study online covering linear and semidefinite programs in which the algorithm is augmented with advice from a possibly erroneous predictor. We show that if the predictor is accurate, we can efficiently bypass these impossibility results and achieve a constant-factor approximation to the optimal solution, i.e., consistency. On the other hand, if the predictor is inaccurate, under some technical conditions, we achieve results that match both the classical optimal upper bounds and the tight lower bounds up to constant factors, i.e., robustness.   More broadly, we introduce a framework that extends both (1) the online set cover problem augmented with machine-learning predictors, studied by Bamas, Maggiori, and Svensson (NeurIPS 2020), and (2) the online covering SDP problem, initiated by Elad, Kale, and Naor (ICALP 2016). Specifically, we obtain general online learning-augmented algorithms for covering linear programs with fractional advice and constraints, and initiate the study of learning-augmented algorithms for covering SDP problems.   Our techniques are based on the primal-dual framework of Buchbinder and Naor (Mathematics of Operations Research, 34, 2009) and can be further adjusted to handle constraints where the variables lie in a bounded region, i.e., box constraints.

</details>

<details>

<summary>2022-10-21 20:51:40 - Time Series Synthesis via Multi-scale Patch-based Generation of Wavelet Scalogram</summary>

- *Amir Kazemi, Hadi Meidani*

- `2211.02620v1` - [abs](http://arxiv.org/abs/2211.02620v1) - [pdf](http://arxiv.org/pdf/2211.02620v1)

> A framework is proposed for the unconditional generation of synthetic time series based on learning from a single sample in low-data regime case. The framework aims at capturing the distribution of patches in wavelet scalogram of time series using single image generative models and producing realistic wavelet coefficients for the generation of synthetic time series. It is demonstrated that the framework is effective with respect to fidelity and diversity for time series with insignificant to no trends. Also, the performance is more promising for generating samples with the same duration (reshuffling) rather than longer ones (retargeting).

</details>

<details>

<summary>2022-10-21 21:44:33 - Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves</summary>

- *Rahul Singh, Liyuan Xu, Arthur Gretton*

- `2010.04855v7` - [abs](http://arxiv.org/abs/2010.04855v7) - [pdf](http://arxiv.org/pdf/2010.04855v7)

> We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves. Treatment and covariates may be discrete or continuous in general spaces. Due to a decomposition property specific to the RKHS, our estimators have simple closed form solutions. We prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression. We extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. We achieve state-of-the-art performance in nonlinear simulations with many covariates, and conduct a policy evaluation of the US Job Corps training program for disadvantaged youths.

</details>

<details>

<summary>2022-10-21 22:47:37 - Exploring Representation-Level Augmentation for Code Search</summary>

- *Haochen Li, Chunyan Miao, Cyril Leung, Yanxian Huang, Yuan Huang, Hongyu Zhang, Yanlin Wang*

- `2210.12285v1` - [abs](http://arxiv.org/abs/2210.12285v1) - [pdf](http://arxiv.org/pdf/2210.12285v1)

> Code search, which aims at retrieving the most relevant code fragment for a given natural language query, is a common activity in software development practice. Recently, contrastive learning is widely used in code search research, where many data augmentation approaches for source code (e.g., semantic-preserving program transformation) are proposed to learn better representations. However, these augmentations are at the raw-data level, which requires additional code analysis in the preprocessing stage and additional training costs in the training stage. In this paper, we explore augmentation methods that augment data (both code and query) at representation level which does not require additional data processing and training, and based on this we propose a general format of representation-level augmentation that unifies existing methods. Then, we propose three new augmentation methods (linear extrapolation, binary interpolation, and Gaussian scaling) based on the general format. Furthermore, we theoretically analyze the advantages of the proposed augmentation methods over traditional contrastive learning methods on code search. We experimentally evaluate the proposed representation-level augmentation methods with state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. The experimental results show that our approach can consistently boost the performance of the studied code search models. Our source code is available at https://github.com/Alex-HaochenLi/RACS.

</details>

<details>

<summary>2022-10-22 07:51:53 - Less is More: Summary of Long Instructions is Better for Program Synthesis</summary>

- *Kirby Kuznia, Swaroop Mishra, Mihir Parmar, Chitta Baral*

- `2203.08597v2` - [abs](http://arxiv.org/abs/2203.08597v2) - [pdf](http://arxiv.org/pdf/2203.08597v2)

> Despite the success of large pre-trained language models (LMs) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that LMs benefit from the summarized version of complicated questions. Our findings show that superfluous information often present in problem description such as human characters, background stories, and names (which are included to help humans in understanding a task) does not help models in understanding a task. To this extent, we create a meta-dataset from the frequently used APPS dataset and the newly created CodeContests dataset for the program synthesis task. Our meta-dataset consists of human and synthesized summaries of the long and complicated programming questions. Experimental results on Codex show that our proposed approach outperforms baseline by 8.13% on the APPS dataset and 11.88% on the CodeContests dataset on average in terms of strict accuracy. Our analysis shows that summaries significantly improve performance for introductory (9.86%) and interview (11.48%) programming questions. However, it shows improvement by a small margin (~ 2%) for competitive programming questions, implying scope for future research in this direction.

</details>

<details>

<summary>2022-10-22 13:46:24 - Reasoning Like Program Executors</summary>

- *Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, Weizhu Chen*

- `2201.11473v2` - [abs](http://arxiv.org/abs/2201.11473v2) - [pdf](http://arxiv.org/pdf/2201.11473v2)

> Reasoning over natural language is a long-standing goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pre-training language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoning-enhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.

</details>

<details>

<summary>2022-10-22 13:57:44 - Abstract Interpretation-Based Feature Importance for SVMs</summary>

- *Abhinandan Pal, Francesco Ranzato, Caterina Urban, Marco Zanella*

- `2210.12456v1` - [abs](http://arxiv.org/abs/2210.12456v1) - [pdf](http://arxiv.org/pdf/2210.12456v1)

> We propose a symbolic representation for support vector machines (SVMs) by means of abstract interpretation, a well-known and successful technique for designing and implementing static program analyses. We leverage this abstraction in two ways: (1) to enhance the interpretability of SVMs by deriving a novel feature importance measure, called abstract feature importance (AFI), that does not depend in any way on a given dataset of the accuracy of the SVM and is very fast to compute, and (2) for verifying stability, notably individual fairness, of SVMs and producing concrete counterexamples when the verification fails. We implemented our approach and we empirically demonstrated its effectiveness on SVMs based on linear and non-linear (polynomial and radial basis function) kernels. Our experimental results show that, independently of the accuracy of the SVM, our AFI measure correlates much more strongly with the stability of the SVM to feature perturbations than feature importance measures widely available in machine learning software such as permutation feature importance. It thus gives better insight into the trustworthiness of SVMs.

</details>

<details>

<summary>2022-10-22 14:24:28 - RACE: Retrieval-Augmented Commit Message Generation</summary>

- *Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun*

- `2203.02700v3` - [abs](http://arxiv.org/abs/2203.02700v3) - [pdf](http://arxiv.org/pdf/2203.02700v3)

> Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose RACE, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that RACE can outperform all baselines. Furthermore, RACE can boost the performance of existing Seq2Seq models in commit message generation.

</details>

<details>

<summary>2022-10-22 15:03:24 - Discrepancy Minimization in Input-Sparsity Time</summary>

- *Yichuan Deng, Zhao Song, Omri Weinstein*

- `2210.12468v1` - [abs](http://arxiv.org/abs/2210.12468v1) - [pdf](http://arxiv.org/pdf/2210.12468v1)

> A recent work of Larsen [Lar23] gave a faster combinatorial alternative to Bansal's SDP algorithm for finding a coloring $x\in\{-1,1\}^n$ that approximately minimizes the discrepancy $\mathrm{disc}(A,x) : = \| A x \|_{\infty}$ of a general real-valued $m\times n$ matrix $A$. Larsen's algorithm runs in $\widetilde{O}(mn^2)$ time compared to Bansal's $\widetilde{O}(mn^{4.5})$-time algorithm, at the price of a slightly weaker logarithmic approximation ratio in terms of the hereditary discrepancy of $A$ [Ban10].   In this work we present a combinatorial $\widetilde{O}(\mathrm{nnz}(A) + n^3)$ time algorithm with the same approximation guarantee as Larsen, which is optimal for tall matrices $m=\mathrm{poly}(n)$. Using a more intricate analysis and fast matrix-multiplication, we achieve $\widetilde{O}(\mathrm{nnz}(A) + n^{2.53})$ time, which breaks cubic runtime for square matrices, and bypasses the barrier of linear-programming approaches [ES14] for which input-sparsity time is currently out of reach.   Our algorithm relies on two main ideas: (i) A new sketching technique for finding a projection matrix with short $\ell_2$-basis using implicit leverage-score sampling; (ii) A data structure for faster implementation of the iterative Edge-Walk partial-coloring algorithm of Lovett-Meka, using an alternative analysis that enables ``lazy" batch-updates with low-rank corrections. Our result nearly closes the computational gap between real-valued and binary matrices (set-systems), for which input-sparsity time coloring was very recently obtained [JSS23].

</details>

<details>

<summary>2022-10-22 21:42:59 - B$^3$RTDP: A Belief Branch and Bound Real-Time Dynamic Programming Approach to Solving POMDPs</summary>

- *Sigurdur Orn Adalgeirsson, Cynthia Breazeal*

- `2210.12556v1` - [abs](http://arxiv.org/abs/2210.12556v1) - [pdf](http://arxiv.org/pdf/2210.12556v1)

> Partially Observable Markov Decision Processes (POMDPs) offer a promising world representation for autonomous agents, as they can model both transitional and perceptual uncertainties. Calculating the optimal solution to POMDP problems can be computationally expensive as they require reasoning over the (possibly infinite) space of beliefs. Several approaches have been proposed to overcome this difficulty, such as discretizing the belief space, point-based belief sampling, and Monte Carlo tree search. The Real-Time Dynamic Programming approach of the RTDP-Bel algorithm approximates the value function by storing it in a hashtable with discretized belief keys. We propose an extension to the RTDP-Bel algorithm which we call Belief Branch and Bound RTDP (B$^3$RTDP). Our algorithm uses a bounded value function representation and takes advantage of this in two novel ways: a search-bounding technique based on action selection convergence probabilities, and a method for leveraging early action convergence called the \textit{Convergence Frontier}. Lastly, we empirically demonstrate that B$^3$RTDP can achieve greater returns in less time than the state-of-the-art SARSOP solver on known POMDP problems.

</details>

<details>

<summary>2022-10-22 22:07:27 - Greedy Modality Selection via Approximate Submodular Maximization</summary>

- *Runxiang Cheng, Gargi Balasubramaniam, Yifei He, Yao-Hung Hubert Tsai, Han Zhao*

- `2210.12562v1` - [abs](http://arxiv.org/abs/2210.12562v1) - [pdf](http://arxiv.org/pdf/2210.12562v1)

> Multimodal learning considers learning from multi-modality data, aiming to fuse heterogeneous sources of information. However, it is not always feasible to leverage all available modalities due to memory constraints. Further, training on all the modalities may be inefficient when redundant information exists within data, such as different subsets of modalities providing similar performance. In light of these challenges, we study modality selection, intending to efficiently select the most informative and complementary modalities under certain computational constraints. We formulate a theoretical framework for optimizing modality selection in multimodal learning and introduce a utility measure to quantify the benefit of selecting a modality. For this optimization problem, we present efficient algorithms when the utility measure exhibits monotonicity and approximate submodularity. We also connect the utility measure with existing Shapley-value-based feature importance scores. Last, we demonstrate the efficacy of our algorithm on synthetic (Patch-MNIST) and two real-world (PEMS-SF, CMU-MOSI) datasets.

</details>

<details>

<summary>2022-10-24 01:13:33 - Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data Programming</summary>

- *Cheng-Yu Hsieh, Jieyu Zhang, Alexander Ratner*

- `2203.01382v3` - [abs](http://arxiv.org/abs/2203.01382v3) - [pdf](http://arxiv.org/pdf/2203.01382v3)

> Weak Supervision (WS) techniques allow users to efficiently create large training datasets by programmatically labeling data with heuristic sources of supervision. While the success of WS relies heavily on the provided labeling heuristics, the process of how these heuristics are created in practice has remained under-explored. In this work, we formalize the development process of labeling heuristics as an interactive procedure, built around the existing workflow where users draw ideas from a selected set of development data for designing the heuristic sources. With the formalism, we study two core problems of how to strategically select the development data to guide users in efficiently creating informative heuristics, and how to exploit the information within the development process to contextualize and better learn from the resultant heuristics. Building upon two novel methodologies that effectively tackle the respective problems considered, we present Nemo, an end-to-end interactive system that improves the overall productivity of WS learning pipeline by an average 20% (and up to 47% in one task) compared to the prevailing WS approach.

</details>

<details>

<summary>2022-10-24 05:30:19 - ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering</summary>

- *Yu Gu, Yu Su*

- `2204.08109v3` - [abs](http://arxiv.org/abs/2204.08109v3) - [pdf](http://arxiv.org/pdf/2204.08109v3)

> Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional ranking-based KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency.

</details>

<details>

<summary>2022-10-24 09:29:40 - Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs</summary>

- *Andrea Tirinzoni, Aymen Al-Marjani, Emilie Kaufmann*

- `2203.09251v3` - [abs](http://arxiv.org/abs/2203.09251v3) - [pdf](http://arxiv.org/pdf/2203.09251v3)

> In probably approximately correct (PAC) reinforcement learning (RL), an agent is required to identify an $\epsilon$-optimal policy with probability $1-\delta$. While minimax optimal algorithms exist for this problem, its instance-dependent complexity remains elusive in episodic Markov decision processes (MDPs). In this paper, we propose the first nearly matching (up to a horizon squared factor and logarithmic terms) upper and lower bounds on the sample complexity of PAC RL in deterministic episodic MDPs with finite state and action spaces. In particular, our bounds feature a new notion of sub-optimality gap for state-action pairs that we call the deterministic return gap. While our instance-dependent lower bound is written as a linear program, our algorithms are very simple and do not require solving such an optimization problem during learning. Their design and analyses employ novel ideas, including graph-theoretical concepts (minimum flows) and a new maximum-coverage exploration strategy.

</details>

<details>

<summary>2022-10-24 16:07:57 - Large Batch and Patch Size Training for Medical Image Segmentation</summary>

- *Junya Sato, Shoji Kido*

- `2210.13364v1` - [abs](http://arxiv.org/abs/2210.13364v1) - [pdf](http://arxiv.org/pdf/2210.13364v1)

> Multi-organ segmentation enables organ evaluation, accounts the relationship between multiple organs, and facilitates accurate diagnosis and treatment decisions. However, only few models can perform segmentation accurately because of the lack of datasets and computational resources. On AMOS2022 challenge, which is a large-scale, clinical, and diverse abdominal multiorgan segmentation benchmark, we trained a 3D-UNet model with large batch and patch sizes using multi-GPU distributed training. Segmentation performance tended to increase for models with large batch and patch sizes compared with the baseline settings. The accuracy was further improved by using ensemble models that were trained with different settings. These results provide a reference for parameter selection in organ segmentation.

</details>

<details>

<summary>2022-10-24 16:49:07 - Improved Bi-point Rounding Algorithms and a Golden Barrier for $k$-Median</summary>

- *Kishen N. Gowda, Thomas Pensyl, Aravind Srinivasan, Khoa Trinh*

- `2210.13395v1` - [abs](http://arxiv.org/abs/2210.13395v1) - [pdf](http://arxiv.org/pdf/2210.13395v1)

> The current best approximation algorithms for $k$-median rely on first obtaining a structured fractional solution known as a bi-point solution, and then rounding it to an integer solution. We improve this second step by unifying and refining previous approaches. We describe a hierarchy of increasingly-complex partitioning schemes for the facilities, along with corresponding sets of algorithms and factor-revealing non-linear programs. We prove that the third layer of this hierarchy is a $2.613$-approximation, improving upon the current best ratio of $2.675$, while no layer can be proved better than $2.588$ under the proposed analysis.   On the negative side, we give a family of bi-point solutions which cannot be approximated better than the square root of the golden ratio, even if allowed to open $k+o(k)$ facilities. This gives a barrier to current approaches for obtaining an approximation better than $2 \sqrt{\phi} \approx 2.544$. Altogether we reduce the approximation gap of bi-point solutions by two thirds.

</details>

<details>

<summary>2022-10-24 22:07:58 - Deep VULMAN: A Deep Reinforcement Learning-Enabled Cyber Vulnerability Management Framework</summary>

- *Soumyadeep Hore, Ankit Shah, Nathaniel D. Bastian*

- `2208.02369v2` - [abs](http://arxiv.org/abs/2208.02369v2) - [pdf](http://arxiv.org/pdf/2208.02369v2)

> Cyber vulnerability management is a critical function of a cybersecurity operations center (CSOC) that helps protect organizations against cyber-attacks on their computer and network systems. Adversaries hold an asymmetric advantage over the CSOC, as the number of deficiencies in these systems is increasing at a significantly higher rate compared to the expansion rate of the security teams to mitigate them in a resource-constrained environment. The current approaches are deterministic and one-time decision-making methods, which do not consider future uncertainties when prioritizing and selecting vulnerabilities for mitigation. These approaches are also constrained by the sub-optimal distribution of resources, providing no flexibility to adjust their response to fluctuations in vulnerability arrivals. We propose a novel framework, Deep VULMAN, consisting of a deep reinforcement learning agent and an integer programming method to fill this gap in the cyber vulnerability management process. Our sequential decision-making framework, first, determines the near-optimal amount of resources to be allocated for mitigation under uncertainty for a given system state and then determines the optimal set of prioritized vulnerability instances for mitigation. Our proposed framework outperforms the current methods in prioritizing the selection of important organization-specific vulnerabilities, on both simulated and real-world vulnerability data, observed over a one-year period.

</details>

<details>

<summary>2022-10-25 07:26:11 - Driving and charging an EV in Australia: A real-world analysis</summary>

- *Thara Philip, Kai Li Lim, Jake Whitehead*

- `2206.03277v2` - [abs](http://arxiv.org/abs/2206.03277v2) - [pdf](http://arxiv.org/pdf/2206.03277v2)

> As outlined by the Intergovernmental Panel on Climate Change, electric vehicles offer the greatest decarbonisation potential for land transport, in addition to other benefits, including reduced fuel and maintenance costs, improved air quality, reduced noise pollution, and improved national fuel security. Owing to these benefits, governments worldwide are planning and rolling out EV-favourable policies, and major car manufacturers are committing to fully electrifying their offerings over the coming decades. With the number of EVs on the roads expected to increase, it is imperative to understand the effect of EVs on transport and energy systems. While unmanaged charging of EVs could potentially add stress to the electricity grid, managed charging of EVs could be beneficial to the grid in terms of improved demand-supply management and improved integration of renewable energy sources into the grid, as well as offer other ancillary services. To assess the impact of EVs on the electricity grid and their potential use as batteries-on-wheels through smart charging capabilities, decision-makers need to understand how current EV owners drive and charge their vehicles. As such, an emerging area of research focuses on understanding these behaviours. Some studies have used stated preference surveys of non-EV owners or data collected from EV trials to estimate EV driving and charging patterns. Other studies have tried to decipher EV owners' behaviour based on data collected from national surveys or as reported by EV owners. This study aims to fill this gap in the literature by collecting data on real-world driving and charging patterns of 239 EVs across Australia. To this effect, data collection from current EV owners via an application programming interface platform began in November 2021 and is currently live.

</details>

<details>

<summary>2022-10-25 07:31:22 - Explainability via Short Formulas: the Case of Propositional Logic with Implementation</summary>

- *Reijo Jaakkola, Tomi Janhunen, Antti Kuusisto, Masood Feyzbakhsh Rankooh, Miikka Vilander*

- `2209.01403v2` - [abs](http://arxiv.org/abs/2209.01403v2) - [pdf](http://arxiv.org/pdf/2209.01403v2)

> We conceptualize explainability in terms of logic and formula size, giving a number of related definitions of explainability in a very general setting. Our main interest is the so-called special explanation problem which aims to explain the truth value of an input formula in an input model. The explanation is a formula of minimal size that (1) agrees with the input formula on the input model and (2) transmits the involved truth value to the input formula globally, i.e., on every model. As an important example case, we study propositional logic in this setting and show that the special explainability problem is complete for the second level of the polynomial hierarchy. We also provide an implementation of this problem in answer set programming and investigate its capacity in relation to explaining answers to the n-queens and dominating set problems.

</details>

<details>

<summary>2022-10-25 11:57:14 - Comparing neural network training performance between Elixir and Python</summary>

- *Lucas C. Tavano, Lucas K. Amin, Adolfo Gustavo Serra-Seca-Neto*

- `2210.13945v1` - [abs](http://arxiv.org/abs/2210.13945v1) - [pdf](http://arxiv.org/pdf/2210.13945v1)

> With a wide range of libraries focused on the machine learning market, such as TensorFlow, NumPy, Pandas, Keras, and others, Python has made a name for itself as one of the main programming languages. In February 2021, Jos\'e Valim and Sean Moriarity published the first version of the Numerical Elixir (Nx) library, a library for tensor operations written in Elixir. Nx aims to allow the language be a good choice for GPU-intensive operations. This work aims to compare the results of Python and Elixir on training convolutional neural networks (CNN) using MNIST and CIFAR-10 datasets, concluding that Python achieved overall better results, and that Elixir is already a viable alternative.

</details>

<details>

<summary>2022-10-25 15:03:58 - Behavior Trees in Robotics and AI: An Introduction</summary>

- *Michele Colledanchise, Petter Ãgren*

- `1709.00084v6` - [abs](http://arxiv.org/abs/1709.00084v6) - [pdf](http://arxiv.org/pdf/1709.00084v6)

> A Behavior Tree (BT) is a way to structure the switching between different tasks in an autonomous agent, such as a robot or a virtual entity in a computer game. BTs are a very efficient way of creating complex systems that are both modular and reactive. These properties are crucial in many applications, which has led to the spread of BT from computer game programming to many branches of AI and Robotics. In this book, we will first give an introduction to BTs, then we describe how BTs relate to, and in many cases generalize, earlier switching structures. These ideas are then used as a foundation for a set of efficient and easy to use design principles. Properties such as safety, robustness, and efficiency are important for an autonomous system, and we describe a set of tools for formally analyzing these using a state space description of BTs. With the new analysis tools, we can formalize the descriptions of how BTs generalize earlier approaches. We also show the use of BTs in automated planning and machine learning. Finally, we describe an extended set of tools to capture the behavior of Stochastic BTs, where the outcomes of actions are described by probabilities. These tools enable the computation of both success probabilities and time to completion.

</details>

<details>

<summary>2022-10-25 16:39:49 - Learning Explicit Object-Centric Representations with Vision Transformers</summary>

- *Oscar VikstrÃ¶m, Alexander Ilin*

- `2210.14139v1` - [abs](http://arxiv.org/abs/2210.14139v1) - [pdf](http://arxiv.org/pdf/2210.14139v1)

> With the recent successful adaptation of transformers to the vision domain, particularly when trained in a self-supervised fashion, it has been shown that vision transformers can learn impressive object-reasoning-like behaviour and features expressive for the task of object segmentation in images. In this paper, we build on the self-supervision task of masked autoencoding and explore its effectiveness for explicitly learning object-centric representations with transformers. To this end, we design an object-centric autoencoder using transformers only and train it end-to-end to reconstruct full images from unmasked patches. We show that the model efficiently learns to decompose simple scenes as measured by segmentation metrics on several multi-object benchmarks.

</details>

<details>

<summary>2022-10-25 21:07:37 - Modular Software for Real-Time Quantum Control Systems</summary>

- *Leon Riesebos, Brad Bondurant, Jacob Whitlow, Junki Kim, Mark Kuzyk, Tianyi Chen, Samuel Phiri, Ye Wang, Chao Fang, Andrew Van Horn, Jungsang Kim, Kenneth R. Brown*

- `2210.14341v1` - [abs](http://arxiv.org/abs/2210.14341v1) - [pdf](http://arxiv.org/pdf/2210.14341v1)

> Real-time control software and hardware is essential for operating quantum computers. In particular, the software plays a crucial role in bridging the gap between quantum programs and the quantum system. Unfortunately, current control software is often optimized for a specific system at the cost of flexibility and portability. We propose a systematic design strategy for modular real-time quantum control software and demonstrate that modular control software can reduce the execution time overhead of kernels by 63.3% on average while not increasing the binary size. Our analysis shows that modular control software for two distinctly different systems can share between 49.8% and 91.0% of covered code statements. To demonstrate the modularity and portability of our software architecture, we run a portable randomized benchmarking experiment on two different ion-trap quantum systems.

</details>

<details>

<summary>2022-10-25 22:11:32 - Functional Simulation of Real-Time Quantum Control Software</summary>

- *Leon Riesebos, Kenneth R. Brown*

- `2210.14364v1` - [abs](http://arxiv.org/abs/2210.14364v1) - [pdf](http://arxiv.org/pdf/2210.14364v1)

> Modern quantum computers rely heavily on real-time control systems for operation. Software for these systems is becoming increasingly more complex due to the demand for more features and more real-time devices to control. Unfortunately, testing real-time control software is often a complex process, and existing simulation software is not usable or practical for software testing. For this purpose, we implemented an interactive simulator that simulates signals at the application programming interface level. We show that our simulation infrastructure simulates kernels 6.9 times faster on average compared to execution on hardware, while the position of the timeline cursor is simulated with an average accuracy of 97.9% when choosing the appropriate configuration.

</details>

<details>

<summary>2022-10-26 03:06:58 - CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search</summary>

- *Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, Nan Duan*

- `2201.10866v3` - [abs](http://arxiv.org/abs/2201.10866v3) - [pdf](http://arxiv.org/pdf/2201.10866v3)

> In this paper, we propose the CodeRetriever model, which learns the function-level code semantic representations through large-scale code-text contrastive pre-training. We adopt two contrastive learning schemes in CodeRetriever: unimodal contrastive learning and bimodal contrastive learning. For unimodal contrastive learning, we design an unsupervised learning approach to build semantic-related code pairs based on the documentation and function name. For bimodal contrastive learning, we leverage the documentation and in-line comments of code to build code-text pairs. Both contrastive objectives can fully leverage large-scale code corpus for pre-training. Extensive experimental results show that CodeRetriever achieves new state-of-the-art with significant improvement over existing code pre-trained models, on eleven domain/language-specific code search tasks with six programming languages in different code granularity (function-level, snippet-level and statement-level). These results demonstrate the effectiveness and robustness of CodeRetriever.

</details>

<details>

<summary>2022-10-26 03:07:11 - Soft-Labeled Contrastive Pre-training for Function-level Code Representation</summary>

- *Xiaonan Li, Daya Guo, Yeyun Gong, Yun Lin, Yelong Shen, Xipeng Qiu, Daxin Jiang, Weizhu Chen, Nan Duan*

- `2210.09597v2` - [abs](http://arxiv.org/abs/2210.09597v2) - [pdf](http://arxiv.org/pdf/2210.09597v2)

> Code contrastive pre-training has recently achieved significant progress on code-related tasks. In this paper, we present \textbf{SCodeR}, a \textbf{S}oft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level \textbf{Code} \textbf{R}epresentation. Considering the relevance between codes in a large-scale code corpus, the soft-labeled contrastive pre-training can obtain fine-grained soft-labels through an iterative adversarial manner and use them to learn better code representation. The positive sample construction is another key for contrastive pre-training. Previous works use transformation-based methods like variable renaming to generate semantically equal positive codes. However, they usually result in the generated code with a highly similar surface form, and thus mislead the model to focus on superficial code structure instead of code semantics. To encourage SCodeR to capture semantic information from the code, we utilize code comments and abstract syntax sub-trees of the code to build positive samples. We conduct experiments on four code-related tasks over seven datasets. Extensive experimental results show that SCodeR achieves new state-of-the-art performance on all of them, which illustrates the effectiveness of the proposed pre-training method.

</details>

<details>

<summary>2022-10-26 03:55:39 - Short Paper: Static and Microarchitectural ML-Based Approaches For Detecting Spectre Vulnerabilities and Attacks</summary>

- *Chidera Biringa, Gaspard Baye, GÃ¶khan Kul*

- `2210.14452v1` - [abs](http://arxiv.org/abs/2210.14452v1) - [pdf](http://arxiv.org/pdf/2210.14452v1)

> Spectre intrusions exploit speculative execution design vulnerabilities in modern processors. The attacks violate the principles of isolation in programs to gain unauthorized private user information. Current state-of-the-art detection techniques utilize micro-architectural features or vulnerable speculative code to detect these threats. However, these techniques are insufficient as Spectre attacks have proven to be more stealthy with recently discovered variants that bypass current mitigation mechanisms. Side-channels generate distinct patterns in processor cache, and sensitive information leakage is dependent on source code vulnerable to Spectre attacks, where an adversary uses these vulnerabilities, such as branch prediction, which causes a data breach. Previous studies predominantly approach the detection of Spectre attacks using the microarchitectural analysis, a reactive approach. Hence, in this paper, we present the first comprehensive evaluation of static and microarchitectural analysis-assisted machine learning approaches to detect Spectre vulnerable code snippets (preventive) and Spectre attacks (reactive). We evaluate the performance trade-offs in employing classifiers for detecting Spectre vulnerabilities and attacks.

</details>

<details>

<summary>2022-10-26 03:59:43 - Self-supervision through Random Segments with Autoregressive Coding (RandSAC)</summary>

- *Tianyu Hua, Yonglong Tian, Sucheng Ren, Michalis Raptis, Hang Zhao, Leonid Sigal*

- `2203.12054v2` - [abs](http://arxiv.org/abs/2203.12054v2) - [pdf](http://arxiv.org/pdf/2203.12054v2)

> Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effect various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, CIFAR100, ImageNet). While our pre-training strategy works with a vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder$'$s feature layers, which further improves the performance.

</details>

<details>

<summary>2022-10-26 04:03:43 - Unsupervised Anomaly Detection for Auditing Data and Impact of Categorical Encodings</summary>

- *Ajay Chawda, Stefanie Grimm, Marius Kloft*

- `2210.14056v2` - [abs](http://arxiv.org/abs/2210.14056v2) - [pdf](http://arxiv.org/pdf/2210.14056v2)

> In this paper, we introduce the Vehicle Claims dataset, consisting of fraudulent insurance claims for automotive repairs. The data belongs to the more broad category of Auditing data, which includes also Journals and Network Intrusion data. Insurance claim data are distinctively different from other auditing data (such as network intrusion data) in their high number of categorical attributes. We tackle the common problem of missing benchmark datasets for anomaly detection: datasets are mostly confidential, and the public tabular datasets do not contain relevant and sufficient categorical attributes. Therefore, a large-sized dataset is created for this purpose and referred to as Vehicle Claims (VC) dataset. The dataset is evaluated on shallow and deep learning methods. Due to the introduction of categorical attributes, we encounter the challenge of encoding them for the large dataset. As One Hot encoding of high cardinal dataset invokes the "curse of dimensionality", we experiment with GEL encoding and embedding layer for representing categorical attributes. Our work compares competitive learning, reconstruction-error, density estimation and contrastive learning approaches for Label, One Hot, GEL encoding and embedding layer to handle categorical values.

</details>

<details>

<summary>2022-10-26 04:47:18 - Benchmarking Language Models for Code Syntax Understanding</summary>

- *Da Shen, Xinyun Chen, Chenguang Wang, Koushik Sen, Dawn Song*

- `2210.14473v1` - [abs](http://arxiv.org/abs/2210.14473v1) - [pdf](http://arxiv.org/pdf/2210.14473v1)

> Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the syntactic rules of natural languages without finetuning on syntax understanding tasks. However, there is limited understanding of how well pre-trained models understand the code structure so far. In this work, we perform the first thorough benchmarking of the state-of-the-art pre-trained models for identifying the syntactic structures of programs. Specifically, we introduce CodeSyntax, a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. Our key observation is that existing language models pretrained on code still lack the understanding of code syntax. In fact, these pre-trained programming language models fail to match the performance of simple baselines based on positional offsets and keywords. We also present a natural language benchmark to highlight the differences between natural languages and programming languages in terms of syntactic structure understanding. Our findings point out key limitations of existing pre-training methods for programming languages, and suggest the importance of modeling code syntactic structures.

</details>

<details>

<summary>2022-10-26 05:40:34 - CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course</summary>

- *Changyoon Lee, Yeon Seonwoo, Alice Oh*

- `2210.14494v1` - [abs](http://arxiv.org/abs/2210.14494v1) - [pdf](http://arxiv.org/pdf/2210.14494v1)

> We introduce CS1QA, a dataset for code-based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code. Each question is accompanied with the student's code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The tasks for CS1QA are to predict the question type, the relevant code snippet given the question and the code and retrieving an answer from the annotated corpus. Results for the experiments on several baseline models are reported and thoroughly analyzed. The tasks for CS1QA challenge models to understand both the code and natural language. This unique dataset can be used as a benchmark for source code comprehension and question answering in the educational setting.

</details>

<details>

<summary>2022-10-26 10:23:18 - Inapproximability of shortest paths on perfect matching polytopes</summary>

- *Jean Cardinal, Raphael Steiner*

- `2210.14608v1` - [abs](http://arxiv.org/abs/2210.14608v1) - [pdf](http://arxiv.org/pdf/2210.14608v1)

> We consider the computational problem of finding short paths in the skeleton of the perfect matching polytope of a bipartite graph. We prove that unless $P=NP$, there is no polynomial-time algorithm that computes a path of constant length between two vertices at distance two of the perfect matching polytope of a bipartite graph. Conditioned on $P\neq NP$, this disproves a conjecture by Ito, Kakimura, Kamiyama, Kobayashi and Okamoto [SIAM Journal on Discrete Mathematics, 36(2), pp. 1102-1123 (2022)]. Assuming the Exponential Time Hypothesis we prove the stronger result that there exists no polynomial-time algorithm computing a path of length at most $\left(\frac{1}{4}-o(1)\right)\frac{\log N}{\log \log N}$ between two vertices at distance two of the perfect matching polytope of an $N$-vertex bipartite graph. These results remain true if the bipartite graph is restricted to be of maximum degree three. The above has the following interesting implication for the performance of pivot rules for the simplex algorithm on simply-structured combinatorial polytopes: If $P\neq NP$, then for every simplex pivot rule executable in polynomial time and every constant $k \in \mathbb{N}$ there exists a linear program on a perfect matching polytope and a starting vertex of the polytope such that the optimal solution can be reached in two monotone steps from the starting vertex, yet the pivot rule will require at least $k$ steps to reach the optimal solution. This result remains true in the more general setting of pivot rules for so-called circuit-augmentation algorithms.

</details>

<details>

<summary>2022-10-26 11:41:57 - HSVI can solve zero-sum Partially Observable Stochastic Games</summary>

- *AurÃ©lien Delage, Olivier Buffet, Jilles S. Dibangoye, Abdallah Saffidine*

- `2210.14640v1` - [abs](http://arxiv.org/abs/2210.14640v1) - [pdf](http://arxiv.org/pdf/2210.14640v1)

> State-of-the-art methods for solving 2-player zero-sum imperfect information games rely on linear programming or regret minimization, though not on dynamic programming (DP) or heuristic search (HS), while the latter are often at the core of state-of-the-art solvers for other sequential decision-making problems. In partially observable or collaborative settings (e.g., POMDPs and Dec- POMDPs), DP and HS require introducing an appropriate statistic that induces a fully observable problem as well as bounding (convex) approximators of the optimal value function. This approach has succeeded in some subclasses of 2-player zero-sum partially observable stochastic games (zs- POSGs) as well, but how to apply it in the general case still remains an open question. We answer it by (i) rigorously defining an equivalent game to work with, (ii) proving mathematical properties of the optimal value function that allow deriving bounds that come with solution strategies, (iii) proposing for the first time an HSVI-like solver that provably converges to an $\epsilon$-optimal solution in finite time, and (iv) empirically analyzing it. This opens the door to a novel family of promising approaches complementing those relying on linear programming or iterative methods.

</details>

<details>

<summary>2022-10-27 00:59:21 - Natural Language Syntax Complies with the Free-Energy Principle</summary>

- *Elliot Murphy, Emma Holmes, Karl Friston*

- `2210.15098v1` - [abs](http://arxiv.org/abs/2210.15098v1) - [pdf](http://arxiv.org/pdf/2210.15098v1)

> Natural language syntax yields an unbounded array of hierarchically structured expressions. We claim that these are used in the service of active inference in accord with the free-energy principle (FEP). While conceptual advances alongside modelling and simulation work have attempted to connect speech segmentation and linguistic communication with the FEP, we extend this program to the underlying computations responsible for generating syntactic objects. We argue that recently proposed principles of economy in language design - such as "minimal search" criteria from theoretical syntax - adhere to the FEP. This affords a greater degree of explanatory power to the FEP - with respect to higher language functions - and offers linguistics a grounding in first principles with respect to computability. We show how both tree-geometric depth and a Kolmogorov complexity estimate (recruiting a Lempel-Ziv compression algorithm) can be used to accurately predict legal operations on syntactic workspaces, directly in line with formulations of variational free energy minimization. This is used to motivate a general principle of language design that we term Turing-Chomsky Compression (TCC). We use TCC to align concerns of linguists with the normative account of self-organization furnished by the FEP, by marshalling evidence from theoretical linguistics and psycholinguistics to ground core principles of efficient syntactic computation within active inference.

</details>

<details>

<summary>2022-10-27 03:48:24 - Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language</summary>

- *Paul Denny, Viraj Kumar, Nasser Giacaman*

- `2210.15157v1` - [abs](http://arxiv.org/abs/2210.15157v1) - [pdf](http://arxiv.org/pdf/2210.15157v1)

> GitHub Copilot is an artificial intelligence model for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about the impact it will have on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.

</details>

<details>

<summary>2022-10-27 04:20:55 - Large-scale Optimization of Partial AUC in a Range of False Positive Rates</summary>

- *Yao Yao, Qihang Lin, Tianbao Yang*

- `2203.01505v2` - [abs](http://arxiv.org/abs/2203.01505v2) - [pdf](http://arxiv.org/pdf/2203.01505v2)

> The area under the ROC curve (AUC) is one of the most widely used performance measures for classification models in machine learning. However, it summarizes the true positive rates (TPRs) over all false positive rates (FPRs) in the ROC space, which may include the FPRs with no practical relevance in some applications. The partial AUC, as a generalization of the AUC, summarizes only the TPRs over a specific range of the FPRs and is thus a more suitable performance measure in many real-world situations. Although partial AUC optimization in a range of FPRs had been studied, existing algorithms are not scalable to big data and not applicable to deep learning. To address this challenge, we cast the problem into a non-smooth difference-of-convex (DC) program for any smooth predictive functions (e.g., deep neural networks), which allowed us to develop an efficient approximated gradient descent method based on the Moreau envelope smoothing technique, inspired by recent advances in non-smooth DC optimization. To increase the efficiency of large data processing, we used an efficient stochastic block coordinate update in our algorithm. Our proposed algorithm can also be used to minimize the sum of ranked range loss, which also lacks efficient solvers. We established a complexity of $\tilde O(1/\epsilon^6)$ for finding a nearly $\epsilon$-critical solution. Finally, we numerically demonstrated the effectiveness of our proposed algorithms for both partial AUC maximization and sum of ranked range loss minimization.

</details>

<details>

<summary>2022-10-27 08:51:03 - Exploring the Verifiability of Code Generated by GitHub Copilot</summary>

- *Dakota Wong, Austin Kothig, Patrick Lam*

- `2209.01766v2` - [abs](http://arxiv.org/abs/2209.01766v2) - [pdf](http://arxiv.org/pdf/2209.01766v2)

> GitHub's Copilot generates code quickly. We investigate whether it generates good code. Our approach is to identify a set of problems, ask Copilot to generate solutions, and attempt to formally verify these solutions with Dafny. Our formal verification is with respect to hand-crafted specifications. We have carried out this process on 6 problems and succeeded in formally verifying 4 of the created solutions. We found evidence which corroborates the current consensus in the literature: Copilot is a powerful tool; however, it should not be "flying the plane" by itself.

</details>

<details>

<summary>2022-10-27 15:30:48 - Masked Transformer for image Anomaly Localization</summary>

- *Axel De Nardin, Pankaj Mishra, Gian Luca Foresti, Claudio Piciarelli*

- `2210.15540v1` - [abs](http://arxiv.org/abs/2210.15540v1) - [pdf](http://arxiv.org/pdf/2210.15540v1)

> Image anomaly detection consists in detecting images or image portions that are visually different from the majority of the samples in a dataset. The task is of practical importance for various real-life applications like biomedical image analysis, visual inspection in industrial production, banking, traffic management, etc. Most of the current deep learning approaches rely on image reconstruction: the input image is projected in some latent space and then reconstructed, assuming that the network (mostly trained on normal data) will not be able to reconstruct the anomalous portions. However, this assumption does not always hold. We thus propose a new model based on the Vision Transformer architecture with patch masking: the input image is split in several patches, and each patch is reconstructed only from the surrounding data, thus ignoring the potentially anomalous information contained in the patch itself. We then show that multi-resolution patches and their collective embeddings provide a large improvement in the model's performance compared to the exclusive use of the traditional square patches. The proposed model has been tested on popular anomaly detection datasets such as MVTec and head CT and achieved good results when compared to other state-of-the-art approaches.

</details>

<details>

<summary>2022-10-27 20:19:31 - Confident Approximate Policy Iteration for Efficient Local Planning in $q^Ï$-realizable MDPs</summary>

- *GellÃ©rt Weisz, AndrÃ¡s GyÃ¶rgy, Tadashi Kozuno, Csaba SzepesvÃ¡ri*

- `2210.15755v1` - [abs](http://arxiv.org/abs/2210.15755v1) - [pdf](http://arxiv.org/pdf/2210.15755v1)

> We consider approximate dynamic programming in $\gamma$-discounted Markov decision processes and apply it to approximate planning with linear value-function approximation. Our first contribution is a new variant of Approximate Policy Iteration (API), called Confident Approximate Policy Iteration (CAPI), which computes a deterministic stationary policy with an optimal error bound scaling linearly with the product of the effective horizon $H$ and the worst-case approximation error $\epsilon$ of the action-value functions of stationary policies. This improvement over API (whose error scales with $H^2$) comes at the price of an $H$-fold increase in memory cost. Unlike Scherrer and Lesner [2012], who recommended computing a non-stationary policy to achieve a similar improvement (with the same memory overhead), we are able to stick to stationary policies. This allows for our second contribution, the application of CAPI to planning with local access to a simulator and $d$-dimensional linear function approximation. As such, we design a planning algorithm that applies CAPI to obtain a sequence of policies with successively refined accuracies on a dynamically evolving set of states. The algorithm outputs an $\tilde O(\sqrt{d}H\epsilon)$-optimal policy after issuing $\tilde O(dH^4/\epsilon^2)$ queries to the simulator, simultaneously achieving the optimal accuracy bound and the best known query complexity bound, while earlier algorithms in the literature achieve only one of them. This query complexity is shown to be tight in all parameters except $H$. These improvements come at the expense of a mild (polynomial) increase in memory and computational costs of both the algorithm and its output policy.

</details>

<details>

<summary>2022-10-27 20:53:23 - TensorIR: An Abstraction for Automatic Tensorized Program Optimization</summary>

- *Siyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng, Cody Hao Yu, Yong Yu, Tianqi Chen*

- `2207.04296v2` - [abs](http://arxiv.org/abs/2207.04296v2) - [pdf](http://arxiv.org/pdf/2207.04296v2)

> Deploying deep learning models on various devices has become an important topic. The wave of hardware specialization brings a diverse set of acceleration primitives for multi-dimensional tensor computations. These new acceleration primitives, along with the emerging machine learning models, bring tremendous engineering challenges. In this paper, we present TensorIR, a compiler abstraction for optimizing programs with these tensor computation primitives. TensorIR generalizes the loop nest representation used in existing machine learning compilers to bring tensor computation as the first-class citizen. Finally, we build an end-to-end framework on top of our abstraction to automatically optimize deep learning models for given tensor computation primitives. Experimental results show that TensorIR compilation automatically uses the tensor computation primitives for given hardware backends and delivers performance that is competitive to state-of-art hand-optimized systems across platforms.

</details>

<details>

<summary>2022-10-27 22:31:58 - MAGE: Nearly Zero-Cost Virtual Memory for Secure Computation</summary>

- *Sam Kumar, David E. Culler, Raluca Ada Popa*

- `2106.14651v2` - [abs](http://arxiv.org/abs/2106.14651v2) - [pdf](http://arxiv.org/pdf/2106.14651v2)

> Secure Computation (SC) is a family of cryptographic primitives for computing on encrypted data in single-party and multi-party settings. SC is being increasingly adopted by industry for a variety of applications. A significant obstacle to using SC for practical applications is the memory overhead of the underlying cryptography. We develop MAGE, an execution engine for SC that efficiently runs SC computations that do not fit in memory. We observe that, due to their intended security guarantees, SC schemes are inherently oblivious -- their memory access patterns are independent of the input data. Using this property, MAGE calculates the memory access pattern ahead of time and uses it to produce a memory management plan. This formulation of memory management, which we call memory programming, is a generalization of paging that allows MAGE to provide a highly efficient virtual memory abstraction for SC. MAGE outperforms the OS virtual memory system by up to an order of magnitude, and in many cases, runs SC computations that do not fit in memory at nearly the same speed as if the underlying machines had unbounded physical memory to fit the entire computation.

</details>

<details>

<summary>2022-10-28 02:34:48 - I Know What You Are Searching For: Code Snippet Recommendation from Stack Overflow Posts</summary>

- *Zhipeng Gao, Xin Xia, David Lo, John Grundy, Xindong Zhang, Zhenchang Xing*

- `2210.15845v1` - [abs](http://arxiv.org/abs/2210.15845v1) - [pdf](http://arxiv.org/pdf/2210.15845v1)

> Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically-equivalent question retrieval and (ii) best code snippet recommendation. To evaluate the performance of our proposed model, we conduct a large scale experiment to evaluate the effectiveness of the semantically-equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers.

</details>

<details>

<summary>2022-10-28 03:39:55 - Improved Prediction of Beta-Amyloid and Tau Burden Using Hippocampal Surface Multivariate Morphometry Statistics and Sparse Coding</summary>

- *Jianfeng Wu, Yi Su, Wenhui Zhu, Negar Jalili Mallak, Natasha Lepore, Eric M. Reiman, Richard J. Caselli, Paul M. Thompson, Kewei Chen, Yalin Wang*

- `2211.05235v1` - [abs](http://arxiv.org/abs/2211.05235v1) - [pdf](http://arxiv.org/pdf/2211.05235v1)

> Background: Beta-amyloid (A$\beta$) plaques and tau protein tangles in the brain are the defining 'A' and 'T' hallmarks of Alzheimer's disease (AD), and together with structural atrophy detectable on brain magnetic resonance imaging (MRI) scans as one of the neurodegenerative ('N') biomarkers comprise the ''ATN framework'' of AD. Current methods to detect A$\beta$/tau pathology include cerebrospinal fluid (CSF; invasive), positron emission tomography (PET; costly and not widely available), and blood-based biomarkers (BBBM; promising but mainly still in development).   Objective: To develop a non-invasive and widely available structural MRI-based framework to quantitatively predict the amyloid and tau measurements.   Methods: With MRI-based hippocampal multivariate morphometry statistics (MMS) features, we apply our Patch Analysis-based Surface Correntropy-induced Sparse coding and max-pooling (PASCS-MP) method combined with the ridge regression model to individual amyloid/tau measure prediction.   Results: We evaluate our framework on amyloid PET/MRI and tau PET/MRI datasets from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Each subject has one pair consisting of a PET image and MRI scan, collected at about the same time. Experimental results suggest that amyloid/tau measurements predicted with our PASCP-MP representations are closer to the real values than the measures derived from other approaches, such as hippocampal surface area, volume, and shape morphometry features based on spherical harmonics (SPHARM).   Conclusion: The MMS-based PASCP-MP is an efficient tool that can bridge hippocampal atrophy with amyloid and tau pathology and thus help assess disease burden, progression, and treatment effects.

</details>

<details>

<summary>2022-10-28 08:33:29 - Differentially Private CutMix for Split Learning with Vision Transformer</summary>

- *Seungeun Oh, Jihong Park, Sihun Baek, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, Seong-Lyun Kim*

- `2210.15986v1` - [abs](http://arxiv.org/abs/2210.15986v1) - [pdf](http://arxiv.org/pdf/2210.15986v1)

> Recently, vision transformer (ViT) has started to outpace the conventional CNN in computer vision tasks. Considering privacy-preserving distributed learning with ViT, federated learning (FL) communicates models, which becomes ill-suited due to ViT' s large model size and computing costs. Split learning (SL) detours this by communicating smashed data at a cut-layer, yet suffers from data privacy leakage and large communication costs caused by high similarity between ViT' s smashed data and input data. Motivated by this problem, we propose DP-CutMixSL, a differentially private (DP) SL framework by developing DP patch-level randomized CutMix (DP-CutMix), a novel privacy-preserving inter-client interpolation scheme that replaces randomly selected patches in smashed data. By experiment, we show that DP-CutMixSL not only boosts privacy guarantees and communication efficiency, but also achieves higher accuracy than its Vanilla SL counterpart. Theoretically, we analyze that DP-CutMix amplifies R\'enyi DP (RDP), which is upper-bounded by its Vanilla Mixup counterpart.

</details>

<details>

<summary>2022-10-28 08:39:36 - Spectrograms Are Sequences of Patches</summary>

- *Leyi Zhao, Yi Li*

- `2210.15988v1` - [abs](http://arxiv.org/abs/2210.15988v1) - [pdf](http://arxiv.org/pdf/2210.15988v1)

> Self-supervised pre-training models have been used successfully in several machine learning domains. However, only a tiny amount of work is related to music. In our work, we treat a spectrogram of music as a series of patches and design a self-supervised model that captures the features of these sequential patches: Patchifier, which makes good use of self-supervised learning methods from both NLP and CV domains. We do not use labeled data for the pre-training process, only a subset of the MTAT dataset containing 16k music clips. After pre-training, we apply the model to several downstream tasks. Our model achieves a considerably acceptable result compared to other audio representation models. Meanwhile, our work demonstrates that it makes sense to consider audio as a series of patch segments.

</details>

<details>

<summary>2022-10-28 09:44:19 - Code4ML: a Large-scale Dataset of annotated Machine Learning Code</summary>

- *Anastasia Drozdova, Polina Guseva, Ekaterina Trofimova, Anna Scherbakova, Andrey Ustyuzhanin*

- `2210.16018v1` - [abs](http://arxiv.org/abs/2210.16018v1) - [pdf](http://arxiv.org/pdf/2210.16018v1)

> Program code as a data source is gaining popularity in the data science community. Possible applications for models trained on such assets range from classification for data dimensionality reduction to automatic code generation. However, without annotation number of methods that could be applied is somewhat limited. To address the lack of annotated datasets, we present the Code4ML corpus. It contains code snippets, task summaries, competitions and dataset descriptions publicly available from Kaggle - the leading platform for hosting data science competitions. The corpus consists of ~2.5 million snippets of ML code collected from ~100 thousand Jupyter notebooks. A representative fraction of the snippets is annotated by human assessors through a user-friendly interface specially designed for that purpose. Code4ML dataset can potentially help address a number of software engineering or data science challenges through a data-driven approach. For example, it can be helpful for semantic code classification, code auto-completion, and code generation for an ML task specified in natural language.

</details>

<details>

<summary>2022-10-28 12:04:35 - Benchopt: Reproducible, efficient and collaborative optimization benchmarks</summary>

- *Thomas Moreau, Mathurin Massias, Alexandre Gramfort, Pierre Ablin, Pierre-Antoine Bannier, Benjamin Charlier, Mathieu DagrÃ©ou, Tom DuprÃ© la Tour, Ghislain Durif, Cassio F. Dantas, Quentin Klopfenstein, Johan Larsson, En Lai, Tanguy Lefort, Benoit MalÃ©zieux, Badr Moufad, Binh T. Nguyen, Alain Rakotomamonjy, Zaccharie Ramzi, Joseph Salmon, Samuel Vaiter*

- `2206.13424v3` - [abs](http://arxiv.org/abs/2206.13424v3) - [pdf](http://arxiv.org/pdf/2206.13424v3)

> Numerical validation is at the core of machine learning research as it allows to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automate, reproduce and publish optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard learning tasks: $\ell_2$-regularized logistic regression, Lasso, and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of the state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details. We hope that Benchopt will foster collaborative work in the community hence improving the reproducibility of research findings.

</details>

<details>

<summary>2022-10-28 14:45:32 - Multimodal Transformer for Parallel Concatenated Variational Autoencoders</summary>

- *Stephen D. Liang, Jerry M. Mendel*

- `2210.16174v1` - [abs](http://arxiv.org/abs/2210.16174v1) - [pdf](http://arxiv.org/pdf/2210.16174v1)

> In this paper, we propose a multimodal transformer using parallel concatenated architecture. Instead of using patches, we use column stripes for images in R, G, B channels as the transformer input. The column stripes keep the spatial relations of original image. We incorporate the multimodal transformer with variational autoencoder for synthetic cross-modal data generation. The multimodal transformer is designed using multiple compression matrices, and it serves as encoders for Parallel Concatenated Variational AutoEncoders (PC-VAE). The PC-VAE consists of multiple encoders, one latent space, and two decoders. The encoders are based on random Gaussian matrices and don't need any training. We propose a new loss function based on the interaction information from partial information decomposition. The interaction information evaluates the input cross-modal information and decoder output. The PC-VAE are trained via minimizing the loss function. Experiments are performed to validate the proposed multimodal transformer for PC-VAE.

</details>

<details>

<summary>2022-10-28 16:23:29 - Comprehensively identifying Long Covid articles with human-in-the-loop machine learning</summary>

- *Robert Leaman, Rezarta Islamaj, Alexis Allot, Qingyu Chen, W. John Wilbur, Zhiyong Lu*

- `2209.08124v2` - [abs](http://arxiv.org/abs/2209.08124v2) - [pdf](http://arxiv.org/pdf/2209.08124v2)

> A significant percentage of COVID-19 survivors experience ongoing multisystemic symptoms that often affect daily living, a condition known as Long Covid or post-acute-sequelae of SARS-CoV-2 infection. However, identifying scientific articles relevant to Long Covid is challenging since there is no standardized or consensus terminology. We developed an iterative human-in-the-loop machine learning framework combining data programming with active learning into a robust ensemble model, demonstrating higher specificity and considerably higher sensitivity than other methods. Analysis of the Long Covid collection shows that (1) most Long Covid articles do not refer to Long Covid by any name (2) when the condition is named, the name used most frequently in the literature is Long Covid, and (3) Long Covid is associated with disorders in a wide variety of body systems. The Long Covid collection is updated weekly and is searchable online at the LitCovid portal: https://www.ncbi.nlm.nih.gov/research/coronavirus/docsum?filters=e_condition.LongCovid

</details>

<details>

<summary>2022-10-29 00:10:25 - Gender Bias in Computing</summary>

- *Thomas J. Misa*

- `2210.16449v1` - [abs](http://arxiv.org/abs/2210.16449v1) - [pdf](http://arxiv.org/pdf/2210.16449v1)

> This paper examines the historical dimension of gender bias in the US computing workforce. It offers new quantitative data on the computing workforce prior to the availability of US Census data in the 1970s. Computer user groups (including SHARE, Inc., and the Mark IV software user group) are taken as a cross-section of the computing workforce. A novel method of gender analysis is developed to estimate women's and men's participation in computing beginning in the 1950s. The data presented here are consistent with well-known NSF statistics that show computer science undergraduate programs enrolling increasing numbers of women students during 1965-1985. These findings challenge the 'making programming masculine' thesis, and serve to correct the unrealistically high figures often cited for women's participation in early computer programming. Gender bias in computing today is traced not to 1960s professionalization but to cultural changes in the 1980s and beyond.

</details>

<details>

<summary>2022-10-29 01:12:08 - Region of Interest Detection in Melanocytic Skin Tumor Whole Slide Images</summary>

- *Yi Cui, Yao Li, Jayson R. Miedema, Sherif Farag, J. S. Marron, Nancy E. Thomas*

- `2210.16457v1` - [abs](http://arxiv.org/abs/2210.16457v1) - [pdf](http://arxiv.org/pdf/2210.16457v1)

> Automated region of interest detection in histopathological image analysis is a challenging and important topic with tremendous potential impact on clinical practice. The deep-learning methods used in computational pathology help us to reduce costs and increase the speed and accuracy of regions of interest detection and cancer diagnosis. In this work, we propose a patch-based region of interest detection method for melanocytic skin tumor whole-slide images. We work with a dataset that contains 165 primary melanomas and nevi Hematoxylin and Eosin whole-slide images and build a deep-learning method. The proposed method performs well on a hold-out test data set including five TCGA-SKCM slides (accuracy of 93.94\% in slide classification task and intersection over union rate of 41.27\% in the region of interest detection task), showing the outstanding performance of our model on melanocytic skin tumor. Even though we test the experiments on the skin tumor dataset, our work could also be extended to other medical image detection problems, such as various tumors' classification and prediction, to help and benefit the clinical evaluation and diagnosis of different tumors.

</details>

<details>

<summary>2022-10-29 11:03:01 - Ice Core Dating using Probabilistic Programming</summary>

- *Aditya Ravuri, Tom R. Andersson, Ieva Kazlauskaite, Will Tebbutt, Richard E. Turner, J. Scott Hosking, Neil D. Lawrence, Markus Kaiser*

- `2210.16568v1` - [abs](http://arxiv.org/abs/2210.16568v1) - [pdf](http://arxiv.org/pdf/2210.16568v1)

> Ice cores record crucial information about past climate. However, before ice core data can have scientific value, the chronology must be inferred by estimating the age as a function of depth. Under certain conditions, chemicals locked in the ice display quasi-periodic cycles that delineate annual layers. Manually counting these noisy seasonal patterns to infer the chronology can be an imperfect and time-consuming process, and does not capture uncertainty in a principled fashion. In addition, several ice cores may be collected from a region, introducing an aspect of spatial correlation between them. We present an exploration of the use of probabilistic models for automatic dating of ice cores, using probabilistic programming to showcase its use for prototyping, automatic inference and maintainability, and demonstrate common failure modes of these tools.

</details>

<details>

<summary>2022-10-29 13:13:38 - Understanding Performance Problems in Deep Learning Systems</summary>

- *Junming Cao, Bihuan Chen, Chao Sun, Longjie Hu, Shuaihong Wu, Xin Peng*

- `2112.01771v2` - [abs](http://arxiv.org/abs/2112.01771v2) - [pdf](http://arxiv.org/pdf/2112.01771v2)

> Deep learning (DL) has been widely applied to many domains. Unique challenges in engineering DL systems are posed by the programming paradigm shift from traditional systems to DL systems, and performance is one of the challenges. Performance problems (PPs) in DL systems can cause severe consequences such as excessive resource consumption and financial loss. While bugs in DL systems have been extensively investigated, PPs in DL systems have hardly been explored. To bridge this gap, we present the first comprehensive study to i) characterize symptoms, root causes, and introducing and exposing stages of PPs in DL systems developed in TensorFLow and Keras, with 224 PPs collected from 210 StackOverflow posts, and to ii) assess the capability of existing performance analysis approaches in tackling PPs, with a constructed benchmark of 58 PPs in DL systems. Our findings shed light on the implications on developing high-performance DL systems, and detecting and localizing PPs in DL systems. To demonstrate the usefulness of our findings, we develop a static checker Deep-Perf to detect three types of PPs. It has detected 488 new PPs in 130 GitHub projects. 105 and 27 PPs have been confirmed and fixed.

</details>

<details>

<summary>2022-10-30 13:17:39 - End-to-End Rubbing Restoration Using Generative Adversarial Networks</summary>

- *Gongbo Sun, Zijie Zheng, Ming Zhang*

- `2205.03743v3` - [abs](http://arxiv.org/abs/2205.03743v3) - [pdf](http://arxiv.org/pdf/2205.03743v3)

> Rubbing restorations are significant for preserving world cultural history. In this paper, we propose the RubbingGAN model for restoring incomplete rubbing characters. Specifically, we collect characters from the Zhang Menglong Bei and build up the first rubbing restoration dataset. We design the first generative adversarial network for rubbing restoration. Based on the dataset we collect, we apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore the characters. The results of experiments show that RubbingGAN can repair both slightly and severely incomplete rubbing characters fast and effectively.

</details>

<details>

<summary>2022-10-30 16:21:22 - A simple, efficient and scalable contrastive masked autoencoder for learning visual representations</summary>

- *Shlok Mishra, Joshua Robinson, Huiwen Chang, David Jacobs, Aaron Sarna, Aaron Maschinot, Dilip Krishnan*

- `2210.16870v1` - [abs](http://arxiv.org/abs/2210.16870v1) - [pdf](http://arxiv.org/pdf/2210.16870v1)

> We introduce CAN, a simple, efficient and scalable method for self-supervised learning of visual representations. Our framework is a minimal and conceptually clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the noise prediction approach used in diffusion models. The learning mechanisms are complementary to one another: contrastive learning shapes the embedding space across a batch of image samples; masked autoencoders focus on reconstruction of the low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image. The combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with 50% of patches in both views being masked at random, yielding a considerable efficiency improvement over prior contrastive learning methods. Extensive empirical studies demonstrate that CAN achieves strong downstream performance under both linear and finetuning evaluations on transfer learning and robustness tasks. CAN outperforms MAE and SimCLR when pre-training on ImageNet, but is especially useful for pre-training on larger uncurated datasets such as JFT-300M: for linear probe on ImageNet, CAN achieves 75.4% compared to 73.4% for SimCLR and 64.1% for MAE. The finetuned performance on ImageNet of our ViT-L model is 86.1%, compared to 85.5% for SimCLR, and 85.4% for MAE. The overall FLOPs load of SimCLR is 70% higher than CAN for ViT-L models.

</details>

<details>

<summary>2022-10-30 19:38:23 - Learning to Compare Nodes in Branch and Bound with Graph Neural Networks</summary>

- *Abdel Ghani Labassi, Didier ChÃ©telat, Andrea Lodi*

- `2210.16934v1` - [abs](http://arxiv.org/abs/2210.16934v1) - [pdf](http://arxiv.org/pdf/2210.16934v1)

> Branch-and-bound approaches in integer programming require ordering portions of the space to explore next, a problem known as node comparison. We propose a new siamese graph neural network model to tackle this problem, where the nodes are represented as bipartite graphs with attributes. Similar to prior work, we train our model to imitate a diving oracle that plunges towards the optimal solution. We evaluate our method by solving the instances in a plain framework where the nodes are explored according to their rank. On three NP-hard benchmarks chosen to be particularly primal-difficult, our approach leads to faster solving and smaller branch- and-bound trees than the default ranking function of the open-source solver SCIP, as well as competing machine learning methods. Moreover, these results generalize to instances larger than used for training. Code for reproducing the experiments can be found at https://github.com/ds4dm/learn2comparenodes.

</details>

<details>

<summary>2022-10-31 00:55:01 - TPGen: A Self-Stabilizing GPU-Based Method for Prime and Test Paths Generation</summary>

- *Ebrahim Fazli, Ali Ebnenasir*

- `2210.16998v1` - [abs](http://arxiv.org/abs/2210.16998v1) - [pdf](http://arxiv.org/pdf/2210.16998v1)

> This paper presents a novel scalable GPU-based method for Test Paths (TPs) and Prime Paths (PPs) Generation, called TPGen, used in structural testing and in test data generation. TPGen outperforms existing methods for PPs and TPs generation in several orders of magnitude, both in time and space efficiency. Improving both time and space efficiency is made possible through devising a new non-contiguous and hierarchical memory allocation method, called Three-level Path Access Method (TPAM), that enables efficient storage of maximal simple paths in memory. In addition to its high time and space efficiency, a major significance of TPGen includes its self-stabilizing design where threads execute in a fully asynchronous and order-oblivious way without using any atomic instructions. TPGen can generate PPs and TPs of structurally complex programs that have an extremely high cyclomatic and Npath complexity.

</details>

<details>

<summary>2022-10-31 02:04:23 - LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks</summary>

- *Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, Kangwook Lee*

- `2206.06565v4` - [abs](http://arxiv.org/abs/2206.06565v4) - [pdf](http://arxiv.org/pdf/2206.06565v4)

> Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling "no-code machine learning with LMs." We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.

</details>

<details>

<summary>2022-10-31 02:06:39 - Embedding Space Augmentation for Weakly Supervised Learning in Whole-Slide Images</summary>

- *Imaad Zaffar, Guillaume Jaume, Nasir Rajpoot, Faisal Mahmood*

- `2210.17013v1` - [abs](http://arxiv.org/abs/2210.17013v1) - [pdf](http://arxiv.org/pdf/2210.17013v1)

> Multiple Instance Learning (MIL) is a widely employed framework for learning on gigapixel whole-slide images (WSIs) from WSI-level annotations. In most MIL based analytical pipelines for WSI-level analysis, the WSIs are often divided into patches and deep features for patches (i.e., patch embeddings) are extracted prior to training to reduce the overall computational cost and cope with the GPUs' limited RAM. To overcome this limitation, we present EmbAugmenter, a data augmentation generative adversarial network (DA-GAN) that can synthesize data augmentations in the embedding space rather than in the pixel space, thereby significantly reducing the computational requirements. Experiments on the SICAPv2 dataset show that our approach outperforms MIL without augmentation and is on par with traditional patch-level augmentation for MIL training while being substantially faster.

</details>

<details>

<summary>2022-10-31 03:06:40 - Poison Attack and Defense on Deep Source Code Processing Models</summary>

- *Jia Li, Zhuo Li, Huangzhao Zhang, Ge Li, Zhi Jin, Xing Hu, Xin Xia*

- `2210.17029v1` - [abs](http://arxiv.org/abs/2210.17029v1) - [pdf](http://arxiv.org/pdf/2210.17029v1)

> In the software engineering community, deep learning (DL) has recently been applied to many source code processing tasks. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat, namely poison attack. The attackers aim to inject insidious backdoors into models by poisoning the training data with poison samples. Poisoned models work normally with clean inputs but produce targeted erroneous results with poisoned inputs embedded with triggers. By activating backdoors, attackers can manipulate the poisoned models in security-related scenarios.   To verify the vulnerability of existing deep source code processing models to the poison attack, we present a poison attack framework for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable even human-imperceptible poison samples and attack models by poisoning the training data with poison samples. To defend against the poison attack, we further propose an effective defense approach named CodeDetector to detect poison samples in the training data. CodeDetector can be applied to many model architectures and effectively defend against multiple poison attack approaches. We apply our CodePoisoner and CodeDetector to three tasks, including defect detection, clone detection, and code repair. The results show that (1) CodePoisoner achieves a high attack success rate (max: 100%) in misleading models to targeted erroneous behaviors. It validates that existing deep source code processing models have a strong vulnerability to the poison attack. (2) CodeDetector effectively defends against multiple poison attack approaches by detecting (max: 100%) poison samples in the training data. We hope this work can help practitioners notice the poison attack and inspire the design of more advanced defense techniques.

</details>

<details>

<summary>2022-10-31 06:29:08 - DanZero: Mastering GuanDan Game with Reinforcement Learning</summary>

- *Yudong Lu, Jian Zhao, Youpeng Zhao, Wengang Zhou, Houqiang Li*

- `2210.17087v1` - [abs](http://arxiv.org/abs/2210.17087v1) - [pdf](http://arxiv.org/pdf/2210.17087v1)

> Card game AI has always been a hot topic in the research of artificial intelligence. In recent years, complex card games such as Mahjong, DouDizhu and Texas Hold'em have been solved and the corresponding AI programs have reached the level of human experts. In this paper, we are devoted to developing an AI program for a more complex card game, GuanDan, whose rules are similar to DouDizhu but much more complicated. To be specific, the characteristics of large state and action space, long length of one episode and the unsure number of players in the GuanDan pose great challenges for the development of the AI program. To address these issues, we propose the first AI program DanZero for GuanDan using reinforcement learning technique. Specifically, we utilize a distributed framework to train our AI system. In the actor processes, we carefully design the state features and agents generate samples by self-play. In the learner process, the model is updated by Deep Monte-Carlo Method. After training for 30 days using 160 CPUs and 1 GPU, we get our DanZero bot. We compare it with 8 baseline AI programs which are based on heuristic rules and the results reveal the outstanding performance of DanZero. We also test DanZero with human players and demonstrate its human-level performance.

</details>

<details>

<summary>2022-10-31 09:36:43 - DiffSearch: A Scalable and Precise Search Engine for Code Changes</summary>

- *Luca Di Grazia, Paul Bredl, Michael Pradel*

- `2204.02787v2` - [abs](http://arxiv.org/abs/2204.02787v2) - [pdf](http://arxiv.org/pdf/2204.02787v2)

> The source code of successful projects is evolving all the time, resulting in hundreds of thousands of code changes stored in source code repositories. This wealth of data can be useful, e.g., to find changes similar to a planned code change or examples of recurring code improvements. This paper presents DiffSearch, a search engine that, given a query that describes a code change, returns a set of changes that match the query. The approach is enabled by three key contributions. First, we present a query language that extends the underlying programming language with wildcards and placeholders, providing an intuitive way of formulating queries that is easy to adapt to different programming languages. Second, to ensure scalability, the approach indexes code changes in a one-time preprocessing step, mapping them into a feature space, and then performs an efficient search in the feature space for each query. Third, to guarantee precision, i.e., that any returned code change indeed matches the given query, we present a tree-based matching algorithm that checks whether a query can be expanded to a concrete code change. We present implementations for Java, JavaScript, and Python, and show that the approach responds within seconds to queries across one million code changes, has a recall of 80.7% for Java, 89.6% for Python, and 90.4% for JavaScript, enables users to find relevant code changes more effectively than a regular expression-based search, and is helpful for gathering a large-scale dataset of real-world bug fixes.

</details>

<details>

<summary>2022-10-31 09:52:23 - Cloud Native Robotic Applications with GPU Sharing on Kubernetes</summary>

- *Giovanni Toffetti, Leonardo Militano, SeÃ¡n Murphy, Remo Maurer, Mark Straub*

- `2210.03936v2` - [abs](http://arxiv.org/abs/2210.03936v2) - [pdf](http://arxiv.org/pdf/2210.03936v2)

> In this paper we discuss our experience in teaching the Robotic Applications Programming course at ZHAW combining the use of a Kubernetes (k8s) cluster and real, heterogeneous, robotic hardware. We discuss the main advantages of our solutions in terms of seamless simulation-to-real experience for students and the main shortcomings we encountered with networking and sharing GPUs to support deep learning workloads. We describe the current and foreseen alternatives to avoid these drawbacks in future course editions and propose a more cloud-native approach to deploying multiple robotics applications on a k8s cluster.

</details>

<details>

<summary>2022-10-31 11:42:06 - When Language Model Meets Private Library</summary>

- *Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, Jian-Guang Lou*

- `2210.17236v1` - [abs](http://arxiv.org/abs/2210.17236v1) - [pdf](http://arxiv.org/pdf/2210.17236v1)

> With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries. In practice, it is common for programmers to write code using private libraries. However, this is a challenge for language models since they have never seen private APIs during training. Motivated by the fact that private libraries usually come with elaborate API documentation, we propose a novel framework with two modules: the APIRetriever finds useful APIs, and then the APICoder generates code using these APIs. For APIRetriever, we present a dense retrieval system and also design a friendly interaction to involve uses. For APICoder, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing API information. Both modules are trained with data from public libraries and can be generalized to private ones. Furthermore, we craft three benchmarks for private libraries, named TorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate the impressive performance of our framework.

</details>

<details>

<summary>2022-10-31 14:31:58 - SBI: A Simulation-Based Test of Identifiability for Bayesian Causal Inference</summary>

- *Sam Witty, David Jensen, Vikash Mansinghka*

- `2102.11761v2` - [abs](http://arxiv.org/abs/2102.11761v2) - [pdf](http://arxiv.org/pdf/2102.11761v2)

> A growing family of approaches to causal inference rely on Bayesian formulations of assumptions that go beyond causal graph structure. For example, Bayesian approaches have been developed for analyzing instrumental variable designs, regression discontinuity designs, and within-subjects designs. This paper introduces simulation-based identifiability (SBI), a procedure for testing the identifiability of queries in Bayesian causal inference approaches that are implemented as probabilistic programs. SBI complements analytical approaches to identifiability, leveraging a particle-based optimization scheme on simulated data to determine identifiability for analytically intractable models. We analyze SBI's soundness for a broad class of differentiable, finite-dimensional probabilistic programs with bounded effects. Finally, we provide an implementation of SBI using stochastic gradient descent, and show empirically that it agrees with known identification results on a suite of graph-based and quasi-experimental design benchmarks, including those using Gaussian processes.

</details>

<details>

<summary>2022-10-31 15:16:28 - Latent Semantic Structure in Malicious Programs</summary>

- *John Musgrave, Temesguen Messay-Kebede, David Kapp, Anca Ralescu*

- `2210.17390v1` - [abs](http://arxiv.org/abs/2210.17390v1) - [pdf](http://arxiv.org/pdf/2210.17390v1)

> Latent Semantic Analysis is a method of matrix decomposition used for discovering topics and topic weights in natural language documents. This study uses Latent Semantic Analysis to analyze the composition of binaries of malicious programs. The semantic representation of the term frequency vector representation yields a set of topics, each topic being a composition of terms. The vectors and topics were evaluated quantitatively using a spatial representation. This semantic analysis provides a more abstract representation of the program derived from its term frequency analysis. We use a metric space to represent a program as a collection of vectors, and a distance metric to evaluate their similarity within a topic. The segmentation of the vectors in this dataset provides increased resolution into the program structure.

</details>

<details>

<summary>2022-10-31 16:54:14 - Blind Asynchronous Over-the-Air Federated Edge Learning</summary>

- *Saeed Razavikia, Jaume Anguera Peris, Jose Mairton B. da Silva Jr, Carlo Fischione*

- `2210.17469v1` - [abs](http://arxiv.org/abs/2210.17469v1) - [pdf](http://arxiv.org/pdf/2210.17469v1)

> Federated Edge Learning (FEEL) is a distributed machine learning technique where each device contributes to training a global inference model by independently performing local computations with their data. More recently, FEEL has been merged with over-the-air computation (OAC), where the global model is calculated over the air by leveraging the superposition of analog signals. However, when implementing FEEL with OAC, there is the challenge on how to precode the analog signals to overcome any time misalignment at the receiver. In this work, we propose a novel synchronization-free method to recover the parameters of the global model over the air without requiring any prior information about the time misalignments. For that, we construct a convex optimization based on the norm minimization problem to directly recover the global model by solving a convex semi-definite program. The performance of the proposed method is evaluated in terms of accuracy and convergence via numerical experiments. We show that our proposed algorithm is close to the ideal synchronized scenario by $10\%$, and performs $4\times$ better than the simple case where no recovering method is used.

</details>

<details>

<summary>2022-10-31 18:09:51 - Generating Sequences by Learning to Self-Correct</summary>

- *Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi*

- `2211.00053v1` - [abs](http://arxiv.org/abs/2211.00053v1) - [pdf](http://arxiv.org/pdf/2211.00053v1)

> Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.

</details>

<details>

<summary>2022-10-31 21:05:42 - TaTa: A Multilingual Table-to-Text Dataset for African Languages</summary>

- *Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan A. Botha, Michael Chavinda, Ankur Parikh, Clara Rivera*

- `2211.00142v1` - [abs](http://arxiv.org/abs/2211.00142v1) - [pdf](http://arxiv.org/pdf/2211.00142v1)

> Existing data-to-text generation datasets are mostly limited to English. To address this lack of data, we create Table-to-Text in African languages (TaTa), the first large multilingual table-to-text dataset with a focus on African languages. We created TaTa by transcribing figures and accompanying text in bilingual reports by the Demographic and Health Surveys Program, followed by professional translation to make the dataset fully parallel. TaTa includes 8,700 examples in nine languages including four African languages (Hausa, Igbo, Swahili, and Yor\`ub\'a) and a zero-shot test language (Russian). We additionally release screenshots of the original figures for future research on multilingual multi-modal approaches. Through an in-depth human evaluation, we show that TaTa is challenging for current models and that less than half the outputs from an mT5-XXL-based model are understandable and attributable to the source data. We further demonstrate that existing metrics perform poorly for TaTa and introduce learned metrics that achieve a high correlation with human judgments. We release all data and annotations at https://github.com/google-research/url-nlp.

</details>


## 2022-11

<details>

<summary>2022-11-01 02:45:52 - gMeta: Template-based Regular Expression Generation over Noisy Examples</summary>

- *Shujun Wang, Yongqiang Tian andDengcheng He*

- `2210.16744v2` - [abs](http://arxiv.org/abs/2210.16744v2) - [pdf](http://arxiv.org/pdf/2210.16744v2)

> Regular expressions (regexes) are widely used in different fields of computer science, such as programming languages, string processing, and databases. However, existing tools for synthesizing or repairing regexes always assume that the input examples are faultless. In real industrial scenarios, this assumption does not entirely hold. Thus, this paper presents a simple but effective templated-based approach to generate regular expressions over noisy examples. Specifically, we present a data model (i.e., MetaParam) to extract features of strings for clustering all examples. Then, we propose a practical dynamic thresholding scheme to filter out anomalous examples via detecting knee points on CDF graphs. Finally, we design a template-based algorithm to translate a finite of positve examples to regular expression, which is efficient, interpretable, and extensible. We performed an experimental evaluation on four different extraction tasks applied to real-world datasets and obtained promising results in terms of F-measure. Moreover, gMeta achieves excellent results in real industrial scenarios.

</details>

<details>

<summary>2022-11-01 02:59:30 - Contrastive Learning for Robust Android Malware Familial Classification</summary>

- *Yueming Wu, Shihan Dou, Deqing Zou, Wei Yang, Weizhong Qiang, Hai Jin*

- `2107.03799v2` - [abs](http://arxiv.org/abs/2107.03799v2) - [pdf](http://arxiv.org/pdf/2107.03799v2)

> Due to its open-source nature, Android operating system has been the main target of attackers to exploit. Malware creators always perform different code obfuscations on their apps to hide malicious activities. Features extracted from these obfuscated samples through program analysis contain many useless and disguised features, which leads to many false negatives. To address the issue, in this paper, we demonstrate that obfuscation-resilient malware family analysis can be achieved through contrastive learning. The key insight behind our analysis is that contrastive learning can be used to reduce the difference introduced by obfuscation while amplifying the difference between malware and other types of malware. Based on the proposed analysis, we design a system that can achieve robust and interpretable classification of Android malware. To achieve robust classification, we perform contrastive learning on malware samples to learn an encoder that can automatically extract robust features from malware samples. To achieve interpretable classification, we transform the function call graph of a sample into an image by centrality analysis. Then the corresponding heatmaps can be obtained by visualization techniques. These heatmaps can help users understand why the malware is classified as this family. We implement \emph{IFDroid} and perform extensive evaluations on two datasets. Experimental results show that \emph{IFDroid} is superior to state-of-the-art Android malware familial classification systems. Moreover, \emph{IFDroid} is capable of maintaining a 98.4\% F1 on classifying 69,421 obfuscated malware samples.

</details>

<details>

<summary>2022-11-01 04:22:16 - Natural Language to Code Translation with Execution</summary>

- *Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I. Wang*

- `2204.11454v2` - [abs](http://arxiv.org/abs/2204.11454v2) - [pdf](http://arxiv.org/pdf/2204.11454v2)

> Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results) during training, they are able to generate correct solutions for many problems. However, choosing a single correct program from a generated set for each problem remains challenging. In this work, we introduce execution result--based minimum Bayes risk decoding (MBR-EXEC) for program selection and show that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks. We select output programs from a generated candidate set by marginalizing over program implementations that share the same semantics. Because exact equivalence is intractable, we execute each program on a small number of test inputs to approximate semantic equivalence. Across datasets, execution or simulated execution significantly outperforms the methods that do not involve program semantics. We find that MBR-EXEC consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation. We open-source our code at github.com/facebookresearch/mbr-exec and data at dl.fbaipublicfiles.com/mbr-exec/mbr-exec-release.zip

</details>

<details>

<summary>2022-11-01 13:00:44 - Neural Copula: A unified framework for estimating generic high-dimensional Copula functions</summary>

- *Zhi Zeng, Ting Wang*

- `2205.15031v3` - [abs](http://arxiv.org/abs/2205.15031v3) - [pdf](http://arxiv.org/pdf/2205.15031v3)

> The Copula is widely used to describe the relationship between the marginal distribution and joint distribution of random variables. The estimation of high-dimensional Copula is difficult, and most existing solutions rely either on simplified assumptions or on complicating recursive decompositions. Therefore, people still hope to obtain a generic Copula estimation method with both universality and simplicity. To reach this goal, a novel neural network-based method (named Neural Copula) is proposed in this paper. In this method, a hierarchical unsupervised neural network is constructed to estimate the marginal distribution function and the Copula function by solving differential equations. In the training program, various constraints are imposed on both the neural network and its derivatives. The Copula estimated by the proposed method is smooth and has an analytic expression. The effectiveness of the proposed method is evaluated on both real-world datasets and complex numerical simulations. Experimental results show that Neural Copula's fitting quality for complex distributions is much better than classical methods. The relevant code for the experiments is available on GitHub. (We encourage the reader to run the program for a better understanding of the proposed method).

</details>

<details>

<summary>2022-11-01 15:01:20 - E2E Refined Dataset</summary>

- *Keisuke Toyama, Katsuhito Sudoh, Satoshi Nakamura*

- `2211.00513v1` - [abs](http://arxiv.org/abs/2211.00513v1) - [pdf](http://arxiv.org/pdf/2211.00513v1)

> Although the well-known MR-to-text E2E dataset has been used by many researchers, its MR-text pairs include many deletion/insertion/substitution errors. Since such errors affect the quality of MR-to-text systems, they must be fixed as much as possible. Therefore, we developed a refined dataset and some python programs that convert the original E2E dataset into a refined dataset.

</details>

<details>

<summary>2022-11-01 15:20:44 - Supervised Robustness-preserving Data-free Neural Network Pruning</summary>

- *Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Jin Song Dong*

- `2204.00783v2` - [abs](http://arxiv.org/abs/2204.00783v2) - [pdf](http://arxiv.org/pdf/2204.00783v2)

> When deploying pre-trained neural network models in real-world applications, model consumers often encounter resource-constraint platforms such as mobile and smart devices. They typically use the pruning technique to reduce the size and complexity of the model, generating a lighter one with less resource consumption. Nonetheless, most existing pruning methods are proposed with the premise that the model after being pruned has a chance to be fine-tuned or even retrained based on the original training data. This may be unrealistic in practice, as the data controllers are often reluctant to provide their model consumers with the original data. In this work, we study the neural network pruning in the data-free context, aiming to yield lightweight models that are not only accurate in prediction but also robust against undesired inputs in open-world deployments. Considering the absence of the fine-tuning and retraining that can fix the mis-pruned units, we replace the traditional aggressive one-shot strategy with a conservative one that treats the pruning as a progressive process. We propose a pruning method based on stochastic optimization that uses robustness-related metrics to guide the pruning process. Our method is implemented as a Python program and evaluated with a series of experiments on diverse neural network models. The experimental results show that it significantly outperforms existing one-shot data-free pruning approaches in terms of robustness preservation and accuracy.

</details>

<details>

<summary>2022-11-01 15:32:09 - When Bioprocess Engineering Meets Machine Learning: A Survey from the Perspective of Automated Bioprocess Development</summary>

- *Nghia Duong-Trung, Stefan Born, Jong Woo Kim, Marie-Therese Schermeyer, Katharina Paulick, Maxim Borisyak, Mariano Nicolas Cruz-Bournazou, Thorben Werner, Randolf Scholz, Lars Schmidt-Thieme, Peter Neubauer, Ernesto Martinez*

- `2209.01083v2` - [abs](http://arxiv.org/abs/2209.01083v2) - [pdf](http://arxiv.org/pdf/2209.01083v2)

> Machine learning (ML) is becoming increasingly crucial in many fields of engineering but has not yet played out its full potential in bioprocess engineering. While experimentation has been accelerated by increasing levels of lab automation, experimental planning and data modeling are still largerly depend on human intervention. ML can be seen as a set of tools that contribute to the automation of the whole experimental cycle, including model building and practical planning, thus allowing human experts to focus on the more demanding and overarching cognitive tasks. First, probabilistic programming is used for the autonomous building of predictive models. Second, machine learning automatically assesses alternative decisions by planning experiments to test hypotheses and conducting investigations to gather informative data that focus on model selection based on the uncertainty of model predictions. This review provides a comprehensive overview of ML-based automation in bioprocess development. On the one hand, the biotech and bioengineering community should be aware of the potential and, most importantly, the limitation of existing ML solutions for their application in biotechnology and biopharma. On the other hand, it is essential to identify the missing links to enable the easy implementation of ML and Artificial Intelligence (AI) tools in valuable solutions for the bio-community.

</details>

<details>

<summary>2022-11-01 15:59:28 - Data-driven generation of 4D velocity profiles in the aneurysmal ascending aorta</summary>

- *Simone Saitta, Ludovica Maga, Chloe Armour, Emiliano Votta, Declan P. O'Regan, M. Yousuf Salmasi, Thanos Athanasiou, Jonathan W. Weinsaft, Xiao Yun Xu, Selene Pirola, Alberto Redaelli*

- `2211.00551v1` - [abs](http://arxiv.org/abs/2211.00551v1) - [pdf](http://arxiv.org/pdf/2211.00551v1)

> Numerical simulations of blood flow are a valuable tool to investigate the pathophysiology of ascending thoracic aortic aneurysms (ATAA). To accurately reproduce hemodynamics, computational fluid dynamics (CFD) models must employ realistic inflow boundary conditions (BCs). However, the limited availability of in vivo velocity measurements still makes researchers resort to idealized BCs. In this study we generated and thoroughly characterized a large dataset of synthetic 4D aortic velocity profiles suitable to be used as BCs for CFD simulations. 4D flow MRI scans of 30 subjects with ATAA were processed to extract cross-sectional planes along the ascending aorta, ensuring spatial alignment among all planes and interpolating all velocity fields to a reference configuration. Velocity profiles of the clinical cohort were extensively characterized by computing flow morphology descriptors of both spatial and temporal features. By exploiting principal component analysis (PCA), a statistical shape model (SSM) of 4D aortic velocity profiles was built and a dataset of 437 synthetic cases with realistic properties was generated. Comparison between clinical and synthetic datasets showed that the synthetic data presented similar characteristics as the clinical population in terms of key morphological parameters. The average velocity profile qualitatively resembled a parabolic-shaped profile, but was quantitatively characterized by more complex flow patterns which an idealized profile would not replicate. Statistically significant correlations were found between PCA principal modes of variation and flow descriptors. We built a data-driven generative model of 4D aortic velocity profiles, suitable to be used in computational studies of blood flow. The proposed software system also allows to map any of the generated velocity profiles to the inlet plane of any virtual subject given its coordinate set.

</details>

<details>

<summary>2022-11-01 18:50:54 - Nonparametric Hamiltonian Monte Carlo</summary>

- *Carol Mak, Fabian Zaiser, Luke Ong*

- `2106.10238v2` - [abs](http://arxiv.org/abs/2106.10238v2) - [pdf](http://arxiv.org/pdf/2106.10238v2)

> Probabilistic programming uses programs to express generative models whose posterior probability is then computed by built-in inference engines. A challenging goal is to develop general purpose inference algorithms that work out-of-the-box for arbitrary programs in a universal probabilistic programming language (PPL). The densities defined by such programs, which may use stochastic branching and recursion, are (in general) nonparametric, in the sense that they correspond to models on an infinite-dimensional parameter space. However standard inference algorithms, such as the Hamiltonian Monte Carlo (HMC) algorithm, target distributions with a fixed number of parameters. This paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC) algorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a new class of measurable functions called "tree representable", which serve as a language-independent representation of the density functions of probabilistic programs in a universal PPL. We provide a correctness proof of NP-HMC, and empirically demonstrate significant performance improvements over existing approaches on several nonparametric examples.

</details>

<details>

<summary>2022-11-01 19:05:04 - Simplified Prophet Inequalities for Combinatorial Auctions</summary>

- *Alexander Braun, Thomas Kesselheim*

- `2211.00707v1` - [abs](http://arxiv.org/abs/2211.00707v1) - [pdf](http://arxiv.org/pdf/2211.00707v1)

> We consider prophet inequalities for XOS and MPH-$k$ combinatorial auctions and give a simplified proof for the existence of static and anonymous item prices which recover the state-of-the-art competitive ratios.   Our proofs make use of a linear programming formulation which has a non-negative objective value if there are prices which admit a given competitive ratio $\alpha \geq 1$. Changing our perspective to dual space by an application of strong LP duality, we use an interpretation of the dual variables as probabilities to directly obtain our result. In contrast to previous work, our proofs do not require to argue about specific values of buyers for bundles, but only about the presence or absence of items.   As a side remark, for any $k \geq 2$, this simplification also leads to a tiny improvement in the best competitive ratio for MPH-$k$ combinatorial auctions from $4k-2$ to $2k + 2 \sqrt{k(k-1)} -1$.

</details>

<details>

<summary>2022-11-01 20:35:13 - Excel Spreadsheet Analyzer</summary>

- *Amir Nassereldine, Patrick Chen, Jinjun Xiong*

- `2211.06333v1` - [abs](http://arxiv.org/abs/2211.06333v1) - [pdf](http://arxiv.org/pdf/2211.06333v1)

> Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.

</details>

<details>

<summary>2022-11-02 01:28:27 - OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology</summary>

- *Cheng Jiang, Asadur Chowdury, Xinhai Hou, Akhil Kondepudi, Christian W. Freudiger, Kyle Conway, Sandra Camelo-Piragua, Daniel A. Orringer, Honglak Lee, Todd C. Hollon*

- `2206.08439v2` - [abs](http://arxiv.org/abs/2206.08439v2) - [pdf](http://arxiv.org/pdf/2206.08439v2)

> Accurate intraoperative diagnosis is essential for providing safe and effective care during brain tumor surgery. Our standard-of-care diagnostic methods are time, resource, and labor intensive, which restricts access to optimal surgical treatments. To address these limitations, we propose an alternative workflow that combines stimulated Raman histology (SRH), a rapid optical imaging method, with deep learning-based automated interpretation of SRH images for intraoperative brain tumor diagnosis and real-time surgical decision support. Here, we present OpenSRH, the first public dataset of clinical SRH images from 300+ brain tumors patients and 1300+ unique whole slide optical images. OpenSRH contains data from the most common brain tumors diagnoses, full pathologic annotations, whole slide tumor segmentations, raw and processed optical imaging data for end-to-end model development and validation. We provide a framework for patch-based whole slide SRH classification and inference using weak (i.e. patient-level) diagnostic labels. Finally, we benchmark two computer vision tasks: multiclass histologic brain tumor classification and patch-based contrastive representation learning. We hope OpenSRH will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support in order to improve the access, safety, and efficacy of cancer surgery in the era of precision medicine. Dataset access, code, and benchmarks are available at opensrh.mlins.org.

</details>

<details>

<summary>2022-11-02 04:42:21 - ADPTriage: Approximate Dynamic Programming for Bug Triage</summary>

- *Hadi Jahanshahi, Mucahit Cevik, Kianoush Mousavi, AyÅe BaÅar*

- `2211.00872v1` - [abs](http://arxiv.org/abs/2211.00872v1) - [pdf](http://arxiv.org/pdf/2211.00872v1)

> Bug triaging is a critical task in any software development project. It entails triagers going over a list of open bugs, deciding whether each is required to be addressed, and, if so, which developer should fix it. However, the manual bug assignment in issue tracking systems (ITS) offers only a limited solution and might easily fail when triagers must handle a large number of bug reports. During the automated assignment, there are multiple sources of uncertainties in the ITS, which should be addressed meticulously. In this study, we develop a Markov decision process (MDP) model for an online bug triage task. In addition to an optimization-based myopic technique, we provide an ADP-based bug triage solution, called ADPTriage, which has the ability to reflect the downstream uncertainty in the bug arrivals and developers' timetables. Specifically, without placing any limits on the underlying stochastic process, this technique enables real-time decision-making on bug assignments while taking into consideration developers' expertise, bug type, and bug fixing time. Our result shows a significant improvement over the myopic approach in terms of assignment accuracy and fixing time. We also demonstrate the empirical convergence of the model and conduct sensitivity analysis with various model parameters. Accordingly, this work constitutes a significant step forward in addressing the uncertainty in bug triage solutions

</details>

<details>

<summary>2022-11-02 13:21:52 - Nonparametric Involutive Markov Chain Monte Carlo</summary>

- *Carol Mak, Fabian Zaiser, Luke Ong*

- `2211.01100v1` - [abs](http://arxiv.org/abs/2211.01100v1) - [pdf](http://arxiv.org/pdf/2211.01100v1)

> A challenging problem in probabilistic programming is to develop inference algorithms that work for arbitrary programs in a universal probabilistic programming language (PPL). We present the nonparametric involutive Markov chain Monte Carlo (NP-iMCMC) algorithm as a method for constructing MCMC inference algorithms for nonparametric models expressible in universal PPLs. Building on the unifying involutive MCMC framework, and by providing a general procedure for driving state movement between dimensions, we show that NP-iMCMC can generalise numerous existing iMCMC algorithms to work on nonparametric models. We prove the correctness of the NP-iMCMC sampler. Our empirical study shows that the existing strengths of several iMCMC algorithms carry over to their nonparametric extensions. Applying our method to the recently proposed Nonparametric HMC, an instance of (Multiple Step) NP-iMCMC, we have constructed several nonparametric extensions (all of which new) that exhibit significant performance improvements.

</details>

<details>

<summary>2022-11-02 14:38:01 - Lifted Inference with Linear Order Axiom</summary>

- *Jan TÃ³th, OndÅej KuÅ¾elka*

- `2211.01164v1` - [abs](http://arxiv.org/abs/2211.01164v1) - [pdf](http://arxiv.org/pdf/2211.01164v1)

> We consider the task of weighted first-order model counting (WFOMC) used for probabilistic inference in the area of statistical relational learning. Given a formula $\phi$, domain size $n$ and a pair of weight functions, what is the weighted sum of all models of $\phi$ over a domain of size $n$? It was shown that computing WFOMC of any logical sentence with at most two logical variables can be done in time polynomial in $n$. However, it was also shown that the task is $\texttt{#}P_1$-complete once we add the third variable, which inspired the search for extensions of the two-variable fragment that would still permit a running time polynomial in $n$. One of such extension is the two-variable fragment with counting quantifiers. In this paper, we prove that adding a linear order axiom (which forces one of the predicates in $\phi$ to introduce a linear ordering of the domain elements in each model of $\phi$) on top of the counting quantifiers still permits a computation time polynomial in the domain size. We present a new dynamic programming-based algorithm which can compute WFOMC with linear order in time polynomial in $n$, thus proving our primary claim.

</details>

<details>

<summary>2022-11-02 15:30:40 - AI Ethics in Smart Healthcare</summary>

- *Sudeep Pasricha*

- `2211.06346v1` - [abs](http://arxiv.org/abs/2211.06346v1) - [pdf](http://arxiv.org/pdf/2211.06346v1)

> This article reviews the landscape of ethical challenges of integrating artificial intelligence (AI) into smart healthcare products, including medical electronic devices. Differences between traditional ethics in the medical domain and emerging ethical challenges with AI-driven healthcare are presented, particularly as they relate to transparency, bias, privacy, safety, responsibility, justice, and autonomy. Open challenges and recommendations are outlined to enable the integration of ethical principles into the design, validation, clinical trials, deployment, monitoring, repair, and retirement of AI-based smart healthcare products.

</details>

<details>

<summary>2022-11-02 15:58:13 - Design and implementation of a Framework for remote experiments in education</summary>

- *Pavel KuriÅ¡ÄÃ¡k, Pedro Rossa, HorÃ¡cio Fernandes, JoÃ£o Nuno Silva*

- `2211.01217v1` - [abs](http://arxiv.org/abs/2211.01217v1) - [pdf](http://arxiv.org/pdf/2211.01217v1)

> Remote Controlled laboratories is a teaching and learning tool that increasingly becomes fundamental in the teaching and learning processes at all the levels. A study of available systems highlights a series of limitations on the used programming languages, overall architecture and network communication patterns that, that hinder these systems to be further adopted. Current technologies and modern WEB architectures allow the resolution of such limitations.   Here we present the FREE (Framework for Remote Experiments in Education) platform, a novel system, that, using modern technologies, architectures, and programming practices, will be easier to integrate with external tool and services and new experiments.   FREE was developed in Python, Django programming framework, HTML, JavaScript, and web services to easy the development of new functionalities. The designed architecture provides a louse coupling between the infrastructure and the remote experiments facilitating further developments and allow new experiment integrations.   Currently FREE is already running in various countries providing access to about five types of experiments in the area of physics), integration with various Learning Management Systems and external Authentication mechanisms. Using FREE the development and integration of new experiments (independently of the supporting Hardware and programming language) is now easier to be made available to remote users.

</details>

<details>

<summary>2022-11-02 16:16:28 - Deep Model Reassembly</summary>

- *Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, Xinchao Wang*

- `2210.17409v2` - [abs](http://arxiv.org/abs/2210.17409v2) - [pdf](http://arxiv.org/pdf/2210.17409v2)

> In this paper, we explore a novel knowledge-transfer task, termed as Deep Model Reassembly (DeRy), for general-purpose model reuse. Given a collection of heterogeneous models pre-trained from distinct sources and with diverse architectures, the goal of DeRy, as its name implies, is to first dissect each model into distinctive building blocks, and then selectively reassemble the derived blocks to produce customized networks under both the hardware resource and performance constraints. Such ambitious nature of DeRy inevitably imposes significant challenges, including, in the first place, the feasibility of its solution. We strive to showcase that, through a dedicated paradigm proposed in this paper, DeRy can be made not only possibly but practically efficiently. Specifically, we conduct the partitions of all pre-trained networks jointly via a cover set optimization, and derive a number of equivalence set, within each of which the network blocks are treated as functionally equivalent and hence interchangeable. The equivalence sets learned in this way, in turn, enable picking and assembling blocks to customize networks subject to certain constraints, which is achieved via solving an integer program backed up with a training-free proxy to estimate the task performance. The reassembled models, give rise to gratifying performances with the user-specified constraints satisfied. We demonstrate that on ImageNet, the best reassemble model achieves 78.6% top-1 accuracy without fine-tuning, which could be further elevated to 83.2% with end-to-end training. Our code is available at https://github.com/Adamdad/DeRy

</details>

<details>

<summary>2022-11-02 16:19:04 - Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals</summary>

- *Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, Zac Kenton*

- `2210.01790v2` - [abs](http://arxiv.org/abs/2210.01790v2) - [pdf](http://arxiv.org/pdf/2210.01790v2)

> The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.

</details>

<details>

<summary>2022-11-02 16:48:00 - TVLT: Textless Vision-Language Transformer</summary>

- *Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal*

- `2209.14156v2` - [abs](http://arxiv.org/abs/2209.14156v2) - [pdf](http://arxiv.org/pdf/2209.14156v2)

> In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text. Our code and checkpoints are available at: https://github.com/zinengtang/TVLT

</details>

<details>

<summary>2022-11-02 16:49:58 - Multi-Vector Retrieval as Sparse Alignment</summary>

- *Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu, Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim, Tao Lei, Vincent Y. Zhao*

- `2211.01267v1` - [abs](http://arxiv.org/abs/2211.01267v1) - [pdf](http://arxiv.org/pdf/2211.01267v1)

> Multi-vector retrieval models improve over single-vector dual encoders on many information retrieval tasks. In this paper, we cast the multi-vector retrieval problem as sparse alignment between query and document tokens. We propose AligneR, a novel multi-vector retrieval model that learns sparsified pairwise alignments between query and document tokens (e.g. `dog' vs. `puppy') and per-token unary saliences reflecting their relative importance for retrieval. We show that controlling the sparsity of pairwise token alignments often brings significant performance gains. While most factoid questions focusing on a specific part of a document require a smaller number of alignments, others requiring a broader understanding of a document favor a larger number of alignments. Unary saliences, on the other hand, decide whether a token ever needs to be aligned with others for retrieval (e.g. `kind' from `kind of currency is used in new zealand}'). With sparsified unary saliences, we are able to prune a large number of query and document token vectors and improve the efficiency of multi-vector retrieval. We learn the sparse unary saliences with entropy-regularized linear programming, which outperforms other methods to achieve sparsity. In a zero-shot setting, AligneR scores 51.1 points nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the BEIR benchmark. In addition, adapting pairwise alignments with a few examples (<= 8) further improves the performance up to 15.7 points nDCG@10 for argument retrieval tasks. The unary saliences of AligneR helps us to keep only 20% of the document token representations with minimal performance loss. We further show that our model often produces interpretable alignments and significantly improves its performance when initialized from larger language models.

</details>

<details>

<summary>2022-11-02 19:22:24 - OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks</summary>

- *Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, James Hegarty*

- `2210.12924v2` - [abs](http://arxiv.org/abs/2210.12924v2) - [pdf](http://arxiv.org/pdf/2210.12924v2)

> The size of deep neural networks has grown exponentially in recent years. Unfortunately, hardware devices have not kept pace with the rapidly increasing memory requirements. To cope with this, researchers have turned to techniques such as spilling and recomputation, which increase training time, or reduced precision and model pruning, which can affect model accuracy. We present OLLA, an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks. Our method reduces the memory usage of existing neural networks, without needing any modification to the models or their training procedures. We formulate the problem as a joint integer linear program (ILP). We present several techniques to simplify the encoding of the problem, and enable our approach to scale to the size of state-of-the-art neural networks using an off-the-shelf ILP solver. We experimentally demonstrate that OLLA only takes minutes if not seconds to allow the training of neural networks using one-third less memory on average.

</details>

<details>

<summary>2022-11-02 22:44:31 - Partially-Observable Security Games for Automating Attack-Defense Analysis</summary>

- *Narges Khakpour, David Parker*

- `2211.01508v1` - [abs](http://arxiv.org/abs/2211.01508v1) - [pdf](http://arxiv.org/pdf/2211.01508v1)

> Network systems often contain vulnerabilities that remain unfixed in a network for various reasons, such as the lack of a patch or knowledge to fix them. With the presence of such residual vulnerabilities, the network administrator should properly react to the malicious activities or proactively prevent them, by applying suitable countermeasures that minimize the likelihood of an attack by the attacker. In this paper, we propose a stochastic game-theoretic approach for analyzing network security and synthesizing defense strategies to protect a network. To support analysis under partial observation, where some of the attacker's activities are unobservable or undetectable by the defender, we construct a one-sided partially observable security game and transform it into a perfect game for further analysis. We prove that this transformation is sound for a sub-class of security games and a subset of properties specified in the logic rPATL. We implement a prototype that fully automates our approach, and evaluate it by conducting experiments on a real-life network.

</details>

<details>

<summary>2022-11-02 22:59:29 - Complete the Missing Half: Augmenting Aggregation Filtering with Diversification for Graph Convolutional Networks</summary>

- *Sitao Luan, Mingde Zhao, Chenqing Hua, Xiao-Wen Chang, Doina Precup*

- `2008.08844v4` - [abs](http://arxiv.org/abs/2008.08844v4) - [pdf](http://arxiv.org/pdf/2008.08844v4)

> The core operation of current Graph Neural Networks (GNNs) is the aggregation enabled by the graph Laplacian or message passing, which filters the neighborhood node information. Though effective for various tasks, in this paper, we show that they are potentially a problematic factor underlying all GNN methods for learning on certain datasets, as they force the node representations similar, making the nodes gradually lose their identity and become indistinguishable. Hence, we augment the aggregation operations with their dual, i.e. diversification operators that make the node more distinct and preserve the identity. Such augmentation replaces the aggregation with a two-channel filtering process that, in theory, is beneficial for enriching the node representations. In practice, the proposed two-channel filters can be easily patched on existing GNN methods with diverse training strategies, including spectral and spatial (message passing) methods. In the experiments, we observe desired characteristics of the models and significant performance boost upon the baselines on 9 node classification tasks.

</details>

<details>

<summary>2022-11-02 23:52:02 - SCL: A Secure Concurrency Layer For Paranoid Stateful Lambdas</summary>

- *Kaiyuan Chen, Alexander Thomas, Hanming Lu, William Mullen, Jeffery Ichnowski, Rahul Arya, Nivedha Krishnakumar, Ryan Teoh, Willis Wang, Anthony Joseph, John Kubiatowicz*

- `2210.11703v2` - [abs](http://arxiv.org/abs/2210.11703v2) - [pdf](http://arxiv.org/pdf/2210.11703v2)

> We propose a federated Function-as-a-Service (FaaS) execution model that provides secure and stateful execution in both Cloud and Edge environments. The FaaS workers, called Paranoid Stateful Lambdas (PSLs), collaborate with one another to perform large parallel computations. We exploit cryptographically hardened and mobile bundles of data, called DataCapsules, to provide persistent state for our PSLs, whose execution is protected using hardware-secured TEEs. To make PSLs easy to program and performant, we build the familiar Key-Value Store interface on top of DataCapsules in a way that allows amortization of cryptographic operations. We demonstrate PSLs functioning in an edge environment running on a group of Intel NUCs with SGXv2.   As described, our Secure Concurrency Layer (SCL), provides eventually-consistent semantics over written values using untrusted and unordered multicast. All SCL communication is encrypted, unforgeable, and private. For durability, updates are recorded in replicated DataCapsules, which are append-only cryptographically-hardened blockchain with confidentiality, integrity, and provenance guarantees. Values for inactive keys are stored in a log-structured merge-tree (LSM) in the same DataCapsule. SCL features a variety of communication optimizations, such as an efficient message passing framework that reduces the latency up to 44x from the Intel SGX SDK, and an actor-based cryptographic processing architecture that batches cryptographic operations and increases throughput by 81x.

</details>

<details>

<summary>2022-11-03 08:32:59 - CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</summary>

- *Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C. H. Hoi*

- `2207.01780v3` - [abs](http://arxiv.org/abs/2207.01780v3) - [pdf](http://arxiv.org/pdf/2207.01780v3)

> Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose "CodeRL", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.

</details>

<details>

<summary>2022-11-03 14:06:03 - Learners' Languages</summary>

- *David I. Spivak*

- `2103.01189v2` - [abs](http://arxiv.org/abs/2103.01189v2) - [pdf](http://arxiv.org/pdf/2103.01189v2)

> In "Backprop as functor", the authors show that the fundamental elements of deep learning -- gradient descent and backpropagation -- can be conceptualized as a strong monoidal functor Para(Euc)$\to$Learn from the category of parameterized Euclidean spaces to that of learners, a category developed explicitly to capture parameter update and backpropagation. It was soon realized that there is an isomorphism Learn$\cong$Para(Slens), where Slens is the symmetric monoidal category of simple lenses as used in functional programming.   In this note, we observe that Slens is a full subcategory of Poly, the category of polynomial functors in one variable, via the functor $A\mapsto Ay^A$. Using the fact that (Poly,$\otimes$) is monoidal closed, we show that a map $A\to B$ in Para(Slens) has a natural interpretation in terms of dynamical systems (more precisely, generalized Moore machines) whose interface is the internal-hom type $[Ay^A,By^B]$.   Finally, we review the fact that the category p-Coalg of dynamical systems on any $p \in$ Poly forms a topos, and consider the logical propositions that can be stated in its internal language. We give gradient descent as an example, and we conclude by discussing some directions for future work.

</details>

<details>

<summary>2022-11-03 17:10:25 - Driving innovation through project based learning: A pre-university STEAM for Social Good initiative</summary>

- *Gayathri Manikutty, Sreejith Sasidharan, Bhavani Rao*

- `2211.01998v1` - [abs](http://arxiv.org/abs/2211.01998v1) - [pdf](http://arxiv.org/pdf/2211.01998v1)

> The Covid pandemic is a clarion call for increased sensitivity to the interconnected nature of social problems facing our world today. A future-oriented education on critical issues, such as those outlined in the United Nations Sustainable Development Goals (UN SDGs) and designing potential solutions for such problems is an imperative skill that must be imparted to children to help them navigate their future in today's unpredictable world. Towards this goal, we have been conducting 3.5 month-long mentoring programs for pre-university students in India to participate in a STEAM for Social Good innovation challenge conducted annually by the Government of India. Using digital and physical computing skills, we helped children explore creative solutions for social problems through a constructionist approach to learning, wherein they ideated and reflected upon the problems in their communities. The children learnt the Engineering Design Thinking process and worked in online groups of two or three, from concept to completion. Despite the constraints posed by the pandemic, they explored creative ways to think about design and innovation. They completed a variety of tasks by making, tinkering, engineering, assembling, and programming to grasp the intricate relationship between software and hardware. Subsequently, the children showcased their creative abilities through video storytelling to a panel of domain experts. In this paper, we present the children's perspective of their experiences through this journey, the evaluation metrics based on IEEE design principles, and our learnings from conducting this initiative as a university-school partnership model for 84 middle and high school students. The aspirational intent of this initiative is to make the children better social problem solvers and help them perceive social problems as opportunities to enhance life for themselves and their communities.

</details>

<details>

<summary>2022-11-03 17:16:52 - Truthful Matching with Online Items and Offline Agents</summary>

- *Michal Feldman, Federico Fusco, Stefano Leonardi, Simon Mauras, Rebecca ReiffenhÃ¤user*

- `2211.02004v1` - [abs](http://arxiv.org/abs/2211.02004v1) - [pdf](http://arxiv.org/pdf/2211.02004v1)

> We study truthful mechanisms for welfare maximization in online bipartite matching. In our (multi-parameter) setting, every buyer is associated with a (possibly private) desired set of items, and has a private value for being assigned an item in her desired set. Unlike most online matching settings, where agents arrive online, in our setting the items arrive online in an adversarial order while the buyers are present for the entire duration of the process. This poses a significant challenge to the design of truthful mechanisms, due to the ability of buyers to strategize over future rounds. We provide an almost full picture of the competitive ratios in different scenarios, including myopic vs. non-myopic agents, tardy vs. prompt payments, and private vs. public desired sets. Among other results, we identify the frontier for which the celebrated $e/(e-1)$ competitive ratio for the vertex-weighted online matching of Karp, Vazirani and Vazirani extends to truthful agents and online items.

</details>

<details>

<summary>2022-11-03 18:45:46 - Making Machine Learning Datasets and Models FAIR for HPC: A Methodology and Case Study</summary>

- *Pei-Hung Lin, Chunhua Liao, Winson Chen, Tristan Vanderbruggen, Murali Emani, Hailu Xu*

- `2211.02092v1` - [abs](http://arxiv.org/abs/2211.02092v1) - [pdf](http://arxiv.org/pdf/2211.02092v1)

> The FAIR Guiding Principles aim to improve the findability, accessibility, interoperability, and reusability of digital content by making them both human and machine actionable. However, these principles have not yet been broadly adopted in the domain of machine learning-based program analyses and optimizations for High-Performance Computing (HPC). In this paper, we design a methodology to make HPC datasets and machine learning models FAIR after investigating existing FAIRness assessment and improvement techniques. Our methodology includes a comprehensive, quantitative assessment for elected data, followed by concrete, actionable suggestions to improve FAIRness with respect to common issues related to persistent identifiers, rich metadata descriptions, license and provenance information. Moreover, we select a representative training dataset to evaluate our methodology. The experiment shows the methodology can effectively improve the dataset and model's FAIRness from an initial score of 19.1% to the final score of 83.0%.

</details>

<details>

<summary>2022-11-03 19:38:06 - Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts</summary>

- *Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal*

- `2205.12496v2` - [abs](http://arxiv.org/abs/2205.12496v2) - [pdf](http://arxiv.org/pdf/2205.12496v2)

> Question-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns. We show that pretraining standard language models (LMs) on TeaBReaC before fine-tuning them on target datasets improves their performance by up to 13 F1 points across 4 multi-step QA datasets, with up to 21 point gain on more complex questions. The resulting models also demonstrate higher robustness, with a 5-8 F1 point improvement on two contrast sets. Furthermore, TeaBReaC pretraining substantially improves model performance and robustness even when starting with numerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work thus shows how to effectively use decomposition-guided contexts to robustly teach multi-step reasoning.

</details>

<details>

<summary>2022-11-03 19:50:12 - DetAIL : A Tool to Automatically Detect and Analyze Drift In Language</summary>

- *Nishtha Madaan, Adithya Manjunatha, Hrithik Nambiar, Aviral Kumar Goel, Harivansh Kumar, Diptikalyan Saha, Srikanta Bedathur*

- `2211.04250v1` - [abs](http://arxiv.org/abs/2211.04250v1) - [pdf](http://arxiv.org/pdf/2211.04250v1)

> Machine learning and deep learning-based decision making has become part of today's software. The goal of this work is to ensure that machine learning and deep learning-based systems are as trusted as traditional software. Traditional software is made dependable by following rigorous practice like static analysis, testing, debugging, verifying, and repairing throughout the development and maintenance life-cycle. Similarly for machine learning systems, we need to keep these models up to date so that their performance is not compromised. For this, current systems rely on scheduled re-training of these models as new data kicks in. In this work, we propose to measure the data drift that takes place when new data kicks in so that one can adaptively re-train the models whenever re-training is actually required irrespective of schedules. In addition to that, we generate various explanations at sentence level and dataset level to capture why a given payload text has drifted.

</details>

<details>

<summary>2022-11-04 02:21:48 - A Multi-Head Convolutional Neural Network With Multi-path Attention improves Image Denoising</summary>

- *Jiahong Zhang, Meijun Qu, Ye Wang, Lihong Cao*

- `2204.12736v2` - [abs](http://arxiv.org/abs/2204.12736v2) - [pdf](http://arxiv.org/pdf/2204.12736v2)

> Recently, convolutional neural networks (CNNs) and attention mechanisms have been widely used in image denoising and achieved satisfactory performance. However, the previous works mostly use a single head to receive the noisy image, limiting the richness of extracted features. Therefore, a novel CNN with multiple heads (MH) named MHCNN is proposed in this paper, whose heads will receive the input images rotated by different rotation angles. MH makes MHCNN simultaneously utilize features of rotated images to remove noise. To integrate these features effectively, we present a novel multi-path attention mechanism (MPA). Unlike previous attention mechanisms that handle pixel-level, channel-level, or patch-level features, MPA focuses on features at the image level. Experiments show MHCNN surpasses other state-of-the-art CNN models on additive white Gaussian noise (AWGN) denoising and real-world image denoising. Its peak signal-to-noise ratio (PSNR) results are higher than other networks, such as BRDNet, RIDNet, PAN-Net, and CSANN. The code is accessible at https://github.com/JiaHongZ/MHCNN.

</details>

<details>

<summary>2022-11-04 04:56:35 - Impact Learning: A Learning Method from Features Impact and Competition</summary>

- *Nusrat Jahan Prottasha, Saydul Akbar Murad, Abu Jafar Md Muzahid, Masud Rana, Md Kowsher, Apurba Adhikary, Sujit Biswas, Anupam Kumar Bairagi*

- `2211.02263v1` - [abs](http://arxiv.org/abs/2211.02263v1) - [pdf](http://arxiv.org/pdf/2211.02263v1)

> Machine learning is the study of computer algorithms that can automatically improve based on data and experience. Machine learning algorithms build a model from sample data, called training data, to make predictions or judgments without being explicitly programmed to do so. A variety of wellknown machine learning algorithms have been developed for use in the field of computer science to analyze data. This paper introduced a new machine learning algorithm called impact learning. Impact learning is a supervised learning algorithm that can be consolidated in both classification and regression problems. It can furthermore manifest its superiority in analyzing competitive data. This algorithm is remarkable for learning from the competitive situation and the competition comes from the effects of autonomous features. It is prepared by the impacts of the highlights from the intrinsic rate of natural increase (RNI). We, moreover, manifest the prevalence of the impact learning over the conventional machine learning algorithm.

</details>

<details>

<summary>2022-11-04 10:30:12 - An approach to standardize, automate omni-channel and AI transactional digital service creation</summary>

- *Antoine Aamarcha, Martin Caussanel, Hadrien Lanneau, Kevin Mege, Florian Peyron*

- `2211.03543v1` - [abs](http://arxiv.org/abs/2211.03543v1) - [pdf](http://arxiv.org/pdf/2211.03543v1)

> Our work is at the crossroads of two categories of technologies. On the one hand, omnichannel digit services, to address the needs of users in the most seamless way. On the other hand, low code approaches, to build simply even complex software applications. In this twofold context, we propose DSUL (Digital Service Universal Language). It allows to build omnichannel services with minimal work from their designers. We describe precisely how DSUL operates, and its innovation in regard to the state of the art. We also consider the various methods to evaluate this framework.

</details>

<details>

<summary>2022-11-04 11:32:45 - Lightweight Transformer in Federated Setting for Human Activity Recognition</summary>

- *Ali Raza, Kim Phuc Tran, Ludovic Koehl, Shujun Li, Xianyi Zeng, Khaled Benzaidi*

- `2110.00244v3` - [abs](http://arxiv.org/abs/2110.00244v3) - [pdf](http://arxiv.org/pdf/2110.00244v3)

> Human activity recognition (HAR) is a machine learning task with important applications in healthcare especially in the context of home care of patients and older adults. HAR is often based on data collected from smart sensors, particularly smart home IoT devices such as smartphones, wearables and other body sensors. Deep learning techniques like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been used for HAR, both in centralized and federated settings. However, these techniques have certain limitations: RNNs cannot be easily parallelized, CNNs have the limitation of sequence length, and both are computationally expensive. Moreover, in home healthcare applications the centralized approach can raise serious privacy concerns since the sensors used by a HAR classifier collect a lot of highly personal and sensitive data about people in the home. In this paper, to address some of such challenges facing HAR, we propose a novel lightweight (one-patch) transformer, which can combine the advantages of RNNs and CNNs without their major limitations, and also TransFed, a more privacy-friendly, federated learning-based HAR classifier using our proposed lightweight transformer. We designed a testbed to construct a new HAR dataset from five recruited human participants, and used the new dataset to evaluate the performance of the proposed HAR classifier in both federated and centralized settings. Additionally, we use another public dataset to evaluate the performance of the proposed HAR classifier in centralized setting to compare it with existing HAR classifiers. The experimental results showed that our proposed new solution outperformed state-of-the-art HAR classifiers based on CNNs and RNNs, whiling being more computationally efficient.

</details>

<details>

<summary>2022-11-04 12:57:40 - Time and Memory Efficient Parallel Algorithm for Structural Graph Summaries and two Extensions to Incremental Summarization and $k$-Bisimulation for Long $k$-Chaining</summary>

- *Till Blume, Jannik Rau, David Richerby, Ansgar Scherp*

- `2111.12493v3` - [abs](http://arxiv.org/abs/2111.12493v3) - [pdf](http://arxiv.org/pdf/2111.12493v3)

> We developed a flexible parallel algorithm for graph summarization based on vertex-centric programming and parameterized message passing. The base algorithm supports infinitely many structural graph summary models defined in a formal language. An extension of the parallel base algorithm allows incremental graph summarization. In this paper, we prove that the incremental algorithm is correct and show that updates are performed in time $\mathcal{O}(\Delta \cdot d^k)$, where $\Delta$ is the number of additions, deletions, and modifications to the input graph, $d$ the maximum degree, and $k$ is the maximum distance in the subgraphs considered. Although the iterative algorithm supports values of $k>1$, it requires nested data structures for the message passing that are memory-inefficient. Thus, we extended the base summarization algorithm by a hash-based messaging mechanism to support a scalable iterative computation of graph summarizations based on $k$-bisimulation for arbitrary $k$. We empirically evaluate the performance of our algorithms using benchmark and real-world datasets. The incremental algorithm almost always outperforms the batch computation. We observe in our experiments that the incremental algorithm is faster even in cases when $50\%$ of the graph database changes from one version to the next. The incremental computation requires a three-layered hash index, which has a low memory overhead of only $8\%$ ($\pm 1\%$). Finally, the incremental summarization algorithm outperforms the batch algorithm even with fewer cores. The iterative parallel $k$-bisimulation algorithm computes summaries on graphs with over $10$M edges within seconds. We show that the algorithm processes graphs of $100+\,$M edges within a few minutes while having a moderate memory consumption of $<150$ GB. For the largest BSBM1B dataset with 1 billion edges, it computes $k=10$ bisimulation in under an hour.

</details>

<details>

<summary>2022-11-04 13:20:19 - Advanced Automatic Code Generation for Multiple Relaxation-Time Lattice Boltzmann Methods</summary>

- *Frederik Hennig, Markus Holzer, Ulrich RÃ¼de*

- `2211.02435v1` - [abs](http://arxiv.org/abs/2211.02435v1) - [pdf](http://arxiv.org/pdf/2211.02435v1)

> The scientific code generation package lbmpy supports the automated design and the efficient implementation of lattice Boltzmann methods (LBMs) through metaprogramming. It is based on a new, concise calculus for describing multiple relaxation-time LBMs, including techniques that enable the numerically advantageous subtraction of the constant background component from the populations. These techniques are generalized to a wide range of collision spaces and equilibrium distributions. The article contains an overview of lbmpy's front-end and its code generation pipeline, which implements the new LBM calculus by means of symbolic formula manipulation tools and object-oriented programming. The generated codes have only a minimal number of arithmetic operations. Their automatic derivation rests on two novel Chimera transforms that have been specifically developed for efficiently computing raw and central moments. Information contained in the symbolic representation of the methods is further exploited in a customized sequence of algebraic simplifications, further reducing computational cost. When combined, these algebraic transformations lead to concise and compact numerical kernels. Specifically, with these optimizations, the advanced central moment- and cumulant-based methods can be realized with only little additional cost as when compared with the simple BGK method. The effectiveness and flexibility of the new lbmpy code generation system is demonstrated in simulating Taylor-Green vortex decay and the automatic derivation of an LBM algorithm to solve the shallow water equations.

</details>

<details>

<summary>2022-11-04 14:08:43 - The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning</summary>

- *Ahmed Izzidien, David Stillwell*

- `2111.00107v4` - [abs](http://arxiv.org/abs/2111.00107v4) - [pdf](http://arxiv.org/pdf/2111.00107v4)

> In this paper we present a natural language programming framework to consider how the fairness of acts can be measured. For the purposes of the paper, a fair act is defined as one that one would be accepting of if it were done to oneself. The approach is based on an implementation of the golden rule (GR) in the digital domain. Despite the GRs prevalence as an axiom throughout history, no transfer of this moral philosophy into computational systems exists. In this paper we consider how to algorithmically operationalise this rule so that it may be used to measure sentences such as: the boy harmed the girl, and categorise them as fair or unfair. A review and reply to criticisms of the GR is made. A suggestion of how the technology may be implemented to avoid unfair biases in word embeddings is made - given that individuals would typically not wish to be on the receiving end of an unfair act, such as racism, irrespective of whether the corpus being used deems such discrimination as praiseworthy.

</details>

<details>

<summary>2022-11-04 15:03:05 - GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion Estimates</summary>

- *Jerin Geo James, Devansh Jain, Ajit Rajwade*

- `2210.13769v3` - [abs](http://arxiv.org/abs/2210.13769v3) - [pdf](http://arxiv.org/pdf/2210.13769v3)

> Videos shot by laymen using hand-held cameras contain undesirable shaky motion. Estimating the global motion between successive frames, in a manner not influenced by moving objects, is central to many video stabilization techniques, but poses significant challenges. A large body of work uses 2D affine transformations or homography for the global motion. However, in this work, we introduce a more general representation scheme, which adapts any existing optical flow network to ignore the moving objects and obtain a spatially smooth approximation of the global motion between video frames. We achieve this by a knowledge distillation approach, where we first introduce a low pass filter module into the optical flow network to constrain the predicted optical flow to be spatially smooth. This becomes our student network, named as \textsc{GlobalFlowNet}. Then, using the original optical flow network as the teacher network, we train the student network using a robust loss function. Given a trained \textsc{GlobalFlowNet}, we stabilize videos using a two stage process. In the first stage, we correct the instability in affine parameters using a quadratic programming approach constrained by a user-specified cropping limit to control loss of field of view. In the second stage, we stabilize the video further by smoothing global motion parameters, expressed using a small number of discrete cosine transform coefficients. In extensive experiments on a variety of different videos, our technique outperforms state of the art techniques in terms of subjective quality and different quantitative measures of video stability. The source code is publicly available at \href{https://github.com/GlobalFlowNet/GlobalFlowNet}{https://github.com/GlobalFlowNet/GlobalFlowNet}

</details>

<details>

<summary>2022-11-04 18:56:44 - A Prompt-based Few-shot Learning Approach to Software Conflict Detection</summary>

- *Robert K. Helmeczi, Mucahit Cevik, Savas YÄ±ldÄ±rÄ±m*

- `2211.02709v1` - [abs](http://arxiv.org/abs/2211.02709v1) - [pdf](http://arxiv.org/pdf/2211.02709v1)

> A software requirement specification (SRS) document is an essential part of the software development life cycle which outlines the requirements that a software program in development must satisfy. This document is often specified by a diverse group of stakeholders and is subject to continual change, making the process of maintaining the document and detecting conflicts between requirements an essential task in software development. Notably, projects that do not address conflicts in the SRS document early on face considerable problems later in the development life cycle. These problems incur substantial costs in terms of time and money, and these costs often become insurmountable barriers that ultimately result in the termination of a software project altogether. As a result, early detection of SRS conflicts is critical to project sustainability. The conflict detection task is approached in numerous ways, many of which require a significant amount of manual intervention from developers, or require access to a large amount of labeled, task-specific training data. In this work, we propose using a prompt-based learning approach to perform few-shot learning for conflict detection. We compare our results to supervised learning approaches that use pretrained language models, such as BERT and its variants. Our results show that prompting with just 32 labeled examples can achieve a similar level of performance in many key metrics to that of supervised learning on training sets that are magnitudes larger in size. In contrast to many other conflict detection approaches, we make no assumptions about the type of underlying requirements, allowing us to analyze pairings of both functional and non-functional requirements. This allows us to omit the potentially expensive task of filtering out non-functional requirements from our dataset.

</details>

<details>

<summary>2022-11-05 01:26:59 - UltraFuzz: Towards Resource-saving in Distributed Fuzzing</summary>

- *Xu Zhou, Pengfei Wang, Chenyifan Liu, Tai Yue, Yingying Liu, Congxi Song, Kai Lu, Qidi Yin, Xu Han*

- `2009.06124v2` - [abs](http://arxiv.org/abs/2009.06124v2) - [pdf](http://arxiv.org/pdf/2009.06124v2)

> Recent research has sought to improve fuzzing performance via parallel computing. However, researchers focus on improving efficiency while ignoring the increasing cost of testing resources. Parallel fuzzing in the distributed environment amplifies the resource-wasting problem caused by the random nature of fuzzing. In the parallel mode, owing to the lack of an appropriate task dispatching scheme and timely fuzzing status synchronization among different fuzzing instances, task conflicts and workload imbalance occur, making the resource-wasting problem severe. In this paper, we design UltraFuzz, a fuzzer for resource-saving in distributed fuzzing. Based on centralized dynamic scheduling, UltraFuzz can dispatch tasks and schedule power globally and reasonably to avoid resource-wasting. Besides, UltraFuzz can elastically allocate computing power for fuzzing and seed evaluation, thereby avoiding the potential bottleneck of seed evaluation that blocks the fuzzing process. UltraFuzz was evaluated using real-world programs, and the results show that with the same testing resource, UltraFuzz outperforms state-of-the-art tools, such as AFL, AFL-P, PAFL, and EnFuzz. Most importantly, the experiment reveals certain results that seem counter-intuitive, namely that parallel fuzzing can achieve ``super-linear acceleration'' when compared with single-core fuzzing. We conduct additional experiments to reveal the deep reasons behind this phenomenon and dig deep into the inherent advantages of parallel fuzzing over serial fuzzing, including the global optimization of seed energy scheduling and the escape of local optimal seed. Additionally, 24 real-world vulnerabilities were discovered using UltraFuzz.

</details>

<details>

<summary>2022-11-05 16:24:55 - Efficient Cavity Searching for Gene Network of Influenza A Virus</summary>

- *Junjie Li, Jietong Zhao, Yanqing Su, Jiahao Shen, Yaohua Liu, Xinyue Fan, Zheng Kou*

- `2211.02935v1` - [abs](http://arxiv.org/abs/2211.02935v1) - [pdf](http://arxiv.org/pdf/2211.02935v1)

> High order structures (cavities and cliques) of the gene network of influenza A virus reveal tight associations among viruses during evolution and are key signals that indicate viral cross-species infection and cause pandemics. As indicators for sensing the dynamic changes of viral genes, these higher order structures have been the focus of attention in the field of virology. However, the size of the viral gene network is usually huge, and searching these structures in the networks introduces unacceptable delay. To mitigate this issue, in this paper, we propose a simple-yet-effective model named HyperSearch based on deep learning to search cavities in a computable complex network for influenza virus genetics. Extensive experiments conducted on a public influenza virus dataset demonstrate the effectiveness of HyperSearch over other advanced deep-learning methods without any elaborated model crafting. Moreover, HyperSearch can finish the search works in minutes while 0-1 programming takes days. Since the proposed method is simple and easy to be transferred to other complex networks, HyperSearch has the potential to facilitate the monitoring of dynamic changes in viral genes and help humans keep up with the pace of virus mutations.

</details>

<details>

<summary>2022-11-05 17:06:29 - Automatic Assessment of the Design Quality of Student Python and Java Programs</summary>

- *J. Walker Orr*

- `2208.12654v2` - [abs](http://arxiv.org/abs/2208.12654v2) - [pdf](http://arxiv.org/pdf/2208.12654v2)

> Programs are a kind of communication to both computers and people, hence as students are trained to write programs they need to learn to write well-designed, readable code rather than code that simply functions correctly. The difficulty in teaching good design practices that promote readability is the labor intensiveness of assessing student programs. Typically assessing design quality involves a careful reading of student programs in order to give personalized feedback which naturally is time consuming for instructors. We propose a rule-based system that assesses student programs for quality of design of and provides personalized, precise feedback on how to improve their work. To study its effectiveness, we made the system available to students by deploying it online, allowing students to receive feedback and make corrections before turning in their assignments. The students benefited from the system and the rate of design quality flaws dropped 47.84\% on average over 4 different assignments, 2 in Python and 2 in Java, in comparison to the previous 2 to 3 years of student submissions.

</details>

<details>

<summary>2022-11-05 20:29:51 - A Constant-Factor Approximation for Quasi-bipartite Directed Steiner Tree on Minor-Free Graphs</summary>

- *Zachary Friggstad, Ramin Mousavi*

- `2111.02572v3` - [abs](http://arxiv.org/abs/2111.02572v3) - [pdf](http://arxiv.org/pdf/2111.02572v3)

> We give the first constant-factor approximation algorithm for quasi-bipartite instances of Directed Steiner Tree on graphs that exclude fixed minors. In particular, for $K_r$-minor-free graphs our approximation guarantee is $O(r\cdot\sqrt{\log r})$ and, further, for planar graphs our approximation guarantee is 20.   Our algorithm uses the primal-dual scheme. We employ a more involved method of determining when to buy an edge while raising dual variables since, as we show, the natural primal-dual scheme fails to raise enough dual value to pay for the purchased solution. As a consequence, we also demonstrate integrality gap upper bounds on the standard cut-based linear programming relaxation for the Directed Steiner Tree instances we consider.

</details>

<details>

<summary>2022-11-06 02:23:35 - What can the millions of random treatments in nonexperimental data reveal about causes?</summary>

- *Andre F. Ribeiro, Frank Neffke, Ricardo Hausmann*

- `2105.01152v2` - [abs](http://arxiv.org/abs/2105.01152v2) - [pdf](http://arxiv.org/pdf/2105.01152v2)

> We propose a new method to estimate causal effects from nonexperimental data. Each pair of sample units is first associated with a stochastic 'treatment' - differences in factors between units - and an effect - a resultant outcome difference. It is then proposed that all such pairs can be combined to provide more accurate estimates of causal effects in observational data, provided a statistical model connecting combinatorial properties of treatments to the accuracy and unbiasedness of their effects. The article introduces one such model and a Bayesian approach to combine the $O(n^2)$ pairwise observations typically available in nonexperimnetal data. This also leads to an interpretation of nonexperimental datasets as incomplete, or noisy, versions of ideal factorial experimental designs.   This approach to causal effect estimation has several advantages: (1) it expands the number of observations, converting thousands of individuals into millions of observational treatments; (2) starting with treatments closest to the experimental ideal, it identifies noncausal variables that can be ignored in the future, making estimation easier in each subsequent iteration while departing minimally from experiment-like conditions; (3) it recovers individual causal effects in heterogeneous populations. We evaluate the method in simulations and the National Supported Work (NSW) program, an intensively studied program whose effects are known from randomized field experiments. We demonstrate that the proposed approach recovers causal effects in common NSW samples, as well as in arbitrary subpopulations and an order-of-magnitude larger supersample with the entire national program data, outperforming Statistical, Econometrics and Machine Learning estimators in all cases...

</details>

<details>

<summary>2022-11-06 09:48:06 - MAIL: Malware Analysis Intermediate Language</summary>

- *Shahid Alam*

- `2211.03068v1` - [abs](http://arxiv.org/abs/2211.03068v1) - [pdf](http://arxiv.org/pdf/2211.03068v1)

> This paper introduces and presents a new language named MAIL (Malware Analysis Intermediate Language). MAIL is basically used for building malware analysis and detection tools. MAIL provides an abstract representation of an assembly program and hence the ability of a tool to automate malware analysis and detection. By translating binaries compiled for different platforms to MAIL, a tool can achieve platform independence. Each MAIL statement is annotated with patterns that can be used by a tool to optimize malware analysis and detection.

</details>

<details>

<summary>2022-11-06 14:01:59 - Random Test Generation of Application Programming Interfaces</summary>

- *Eitan Farchi, Krithika Prakash, Vitali Sokhin*

- `2207.13143v2` - [abs](http://arxiv.org/abs/2207.13143v2) - [pdf](http://arxiv.org/pdf/2207.13143v2)

> Cloud high quality API (Application Programming Interface) testing is essential for supporting the API economy. Autotest is a random test generator that addresses this need. It reads the API specification and deduces a model used in the test generation. This paper describes Autotest. It also address the topic of API specification pitfalls which Autotest may reveal when reading the specification. A best practice is to add an appropriate test to the regression once a problem is revealed and solved. How to do that in the context of Autotest's random test generation is covered.

</details>

<details>

<summary>2022-11-06 19:01:02 - "Seeing Sound": Audio Classification with the Wigner-Wille Distribution and Convolutional Neural Networks</summary>

- *Antonios Marios Christonasis, Stef van Eijndhoven, Peter Duin*

- `2211.03202v1` - [abs](http://arxiv.org/abs/2211.03202v1) - [pdf](http://arxiv.org/pdf/2211.03202v1)

> With big data becoming increasingly available, IoT hardware becoming widely adopted, and AI capabilities becoming more powerful, organizations are continuously investing in sensing. Data coming from sensor networks are currently combined with sensor fusion and AI algorithms to drive innovation in fields such as self-driving cars. Data from these sensors can be utilized in numerous use cases, including alerts in safety systems of urban settings, for events such as gun shots and explosions. Moreover, diverse types of sensors, such as sound sensors, can be utilized in low-light conditions or at locations where a camera is not available. This paper investigates the potential of the utilization of sound-sensor data in an urban context. Technically, we propose a novel approach of classifying sound data using the Wigner-Ville distribution and Convolutional Neural Networks. In this paper, we report on the performance of the approach on open-source datasets. The concept and work presented is based on my doctoral thesis, which was performed as part of the Engineering Doctorate program in Data Science at the University of Eindhoven, in collaboration with the Dutch National Police. Additional work on real-world datasets was performed during the thesis, which are not presented here due to confidentiality.

</details>

<details>

<summary>2022-11-07 03:39:00 - SLOPT: Bandit Optimization Framework for Mutation-Based Fuzzing</summary>

- *Yuki Koike, Hiroyuki Katsura, Hiromu Yakura, Yuma Kurogome*

- `2211.03285v1` - [abs](http://arxiv.org/abs/2211.03285v1) - [pdf](http://arxiv.org/pdf/2211.03285v1)

> Mutation-based fuzzing has become one of the most common vulnerability discovery solutions over the last decade. Fuzzing can be optimized when targeting specific programs, and given that, some studies have employed online optimization methods to do it automatically, i.e., tuning fuzzers for any given program in a program-agnostic manner. However, previous studies have neither fully explored mutation schemes suitable for online optimization methods, nor online optimization methods suitable for mutation schemes. In this study, we propose an optimization framework called SLOPT that encompasses both a bandit-friendly mutation scheme and mutation-scheme-friendly bandit algorithms. The advantage of SLOPT is that it can generally be incorporated into existing fuzzers, such as AFL and Honggfuzz. As a proof of concept, we implemented SLOPT-AFL++ by integrating SLOPT into AFL++ and showed that the program-agnostic optimization delivered by SLOPT enabled SLOPT-AFL++ to achieve higher code coverage than AFL++ in all of ten real-world FuzzBench programs. Moreover, we ran SLOPT-AFL++ against several real-world programs from OSS-Fuzz and successfully identified three previously unknown vulnerabilities, even though these programs have been fuzzed by AFL++ for a considerable number of CPU days on OSS-Fuzz.

</details>

<details>

<summary>2022-11-07 10:43:36 - Automatic Creativity Measurement in Scratch Programs Across Modalities</summary>

- *Anastasia Kovalkov, Benjamin PaaÃen, Avi Segal, Niels Pinkwart, Kobi Gal*

- `2211.05227v1` - [abs](http://arxiv.org/abs/2211.05227v1) - [pdf](http://arxiv.org/pdf/2211.05227v1)

> Promoting creativity is considered an important goal of education, but creativity is notoriously hard to measure.In this paper, we make the journey fromdefining a formal measure of creativity that is efficientlycomputable to applying the measure in a practical domain. The measure is general and relies on coretheoretical concepts in creativity theory, namely fluency, flexibility, and originality, integratingwith prior cognitive science literature. We adapted the general measure for projects in the popular visual programming language Scratch.We designed a machine learning model for predicting the creativity of Scratch projects, trained and evaluated on human expert creativity assessments in an extensive user study. Our results show that opinions about creativity in Scratch varied widely across experts. The automatic creativity assessment aligned with the assessment of the human experts more than the experts agreed with each other. This is a first step in providing computational models for measuring creativity that can be applied to educational technologies, and to scale up the benefit of creativity education in schools.

</details>

<details>

<summary>2022-11-07 11:05:44 - Symbolic Abstract Heaps for Polymorphic Information-flow Guard Inference (Extended Version)</summary>

- *Nicolas Berthier, Narges Khakpour*

- `2211.03450v1` - [abs](http://arxiv.org/abs/2211.03450v1) - [pdf](http://arxiv.org/pdf/2211.03450v1)

> In the realm of sound object-oriented program analyses for information-flow control, very few approaches adopt flow-sensitive abstractions of the heap that enable a precise modeling of implicit flows. To tackle this challenge, we advance a new symbolic abstraction approach for modeling the heap in Java-like programs. We use a store-less representation that is parameterized with a family of relations among references to offer various levels of precision based on user preferences. This enables us to automatically infer polymorphic information-flow guards for methods via a co-reachability analysis of a symbolic finite-state system. We instantiate the heap abstraction with three different families of relations. We prove the soundness of our approach and compare the precision and scalability obtained with each instantiated heap domain by using the IFSpec benchmarks and real-life applications.

</details>

<details>

<summary>2022-11-07 15:21:32 - Neurosymbolic Programming for Science</summary>

- *Jennifer J. Sun, Megan Tjandrasuwita, Atharva Sehgal, Armando Solar-Lezama, Swarat Chaudhuri, Yisong Yue, Omar Costilla-Reyes*

- `2210.05050v2` - [abs](http://arxiv.org/abs/2210.05050v2) - [pdf](http://arxiv.org/pdf/2210.05050v2)

> Neurosymbolic Programming (NP) techniques have the potential to accelerate scientific discovery. These models combine neural and symbolic components to learn complex patterns and representations from data, using high-level concepts or known constraints. NP techniques can interface with symbolic domain knowledge from scientists, such as prior knowledge and experimental context, to produce interpretable outputs. We identify opportunities and challenges between current NP models and scientific workflows, with real-world examples from behavior analysis in science: to enable the use of NP broadly for workflows across the natural and social sciences.

</details>

<details>

<summary>2022-11-07 16:27:01 - PAEDID: Patch Autoencoder Based Deep Image Decomposition For Pixel-level Defective Region Segmentation</summary>

- *Shancong Mou, Meng Cao, Haoping Bai, Ping Huang, Jianjun Shi, Jiulong Shan*

- `2203.14457v3` - [abs](http://arxiv.org/abs/2203.14457v3) - [pdf](http://arxiv.org/pdf/2203.14457v3)

> Unsupervised pixel-level defective region segmentation is an important task in image-based anomaly detection for various industrial applications. The state-of-the-art methods have their own advantages and limitations: matrix-decomposition-based methods are robust to noise but lack complex background image modeling capability; representation-based methods are good at defective region localization but lack accuracy in defective region shape contour extraction; reconstruction-based methods detected defective region match well with the ground truth defective region shape contour but are noisy. To combine the best of both worlds, we present an unsupervised patch autoencoder based deep image decomposition (PAEDID) method for defective region segmentation. In the training stage, we learn the common background as a deep image prior by a patch autoencoder (PAE) network. In the inference stage, we formulate anomaly detection as an image decomposition problem with the deep image prior and domain-specific regularizations. By adopting the proposed approach, the defective regions in the image can be accurately extracted in an unsupervised fashion. We demonstrate the effectiveness of the PAEDID method in simulation studies and an industrial dataset in the case study.

</details>

<details>

<summary>2022-11-07 23:44:04 - Towards Extending the Range of Bugs That Automated Program Repair Can Handle</summary>

- *Omar I. Al-Bataineh, Leon Moonen*

- `2211.03911v1` - [abs](http://arxiv.org/abs/2211.03911v1) - [pdf](http://arxiv.org/pdf/2211.03911v1)

> Modern automated program repair (APR) is well-tuned to finding and repairing bugs that introduce observable erroneous behavior to a program. However, a significant class of bugs does not lead to such observable behavior (e.g., liveness/termination bugs, non-functional bugs, and information flow bugs). Such bugs can generally not be handled with current APR approaches, so, as a community, we need to develop complementary techniques.   To stimulate the systematic study of alternative APR approaches and hybrid APR combinations, we devise a novel bug classification system that enables methodical analysis of their bug detection power and bug repair capabilities. To demonstrate the benefits, we analyze the repair of termination bugs in sequential and concurrent programs. The study shows that integrating dynamic APR with formal analysis techniques, such as termination provers and software model checkers, reduces complexity and improves the overall reliability of these repairs.

</details>

<details>

<summary>2022-11-08 01:37:07 - Final Report on MITRE Evaluations for the DARPA Big Mechanism Program</summary>

- *Matthew Peterson, Tonia Korves, Christopher Garay, Robyn Kozierok, Lynette Hirschman*

- `2211.03943v1` - [abs](http://arxiv.org/abs/2211.03943v1) - [pdf](http://arxiv.org/pdf/2211.03943v1)

> This report presents the evaluation approach developed for the DARPA Big Mechanism program, which aimed at developing computer systems that will read research papers, integrate the information into a computer model of cancer mechanisms, and frame new hypotheses. We employed an iterative, incremental approach to the evaluation of the three phases of the program. In Phase I, we evaluated the ability of system and human teams ability to read-with-a-model to capture mechanistic information from the biomedical literature, integrated with information from expert curated biological databases. In Phase II we evaluated the ability of systems to assemble fragments of information into a mechanistic model. The Phase III evaluation focused on the ability of systems to provide explanations of experimental observations based on models assembled (largely automatically) by the Big Mechanism process. The evaluation for each phase built on earlier evaluations and guided developers towards creating capabilities for the new phase. The report describes our approach, including innovations such as a reference set (a curated data set limited to major findings of each paper) to assess the accuracy of systems in extracting mechanistic findings in the absence of a gold standard, and a method to evaluate model-based explanations of experimental data. Results of the evaluation and supporting materials are included in the appendices.

</details>

<details>

<summary>2022-11-08 09:27:53 - Computing better approximate pure Nash equilibria in cut games via semidefinite programming</summary>

- *Ioannis Caragiannis, Zhile Jiang*

- `2211.04117v1` - [abs](http://arxiv.org/abs/2211.04117v1) - [pdf](http://arxiv.org/pdf/2211.04117v1)

> Cut games are among the most fundamental strategic games in algorithmic game theory. It is well-known that computing an exact pure Nash equilibrium in these games is PLS-hard, so research has focused on computing approximate equilibria. We present a polynomial-time algorithm that computes $2.7371$-approximate pure Nash equilibria in cut games. This is the first improvement to the previously best-known bound of $3$, due to the work of Bhalgat, Chakraborty, and Khanna from EC 2010. Our algorithm is based on a general recipe proposed by Caragiannis, Fanelli, Gravin, and Skopalik from FOCS 2011 and applied on several potential games since then. The first novelty of our work is the introduction of a phase that can identify subsets of players who can simultaneously improve their utilities considerably. This is done via semidefinite programming and randomized rounding. In particular, a negative objective value to the semidefinite program guarantees that no such considerable improvement is possible for a given set of players. Otherwise, randomized rounding of the SDP solution is used to identify a set of players who can simultaneously improve their strategies considerably and allows the algorithm to make progress. The way rounding is performed is another important novelty of our work. Here, we exploit an idea that dates back to a paper by Feige and Goemans from 1995, but we take it to an extreme that has not been analyzed before.

</details>

<details>

<summary>2022-11-08 12:18:02 - Submission-Aware Reviewer Profiling for Reviewer Recommender System</summary>

- *Omer Anjum, Alok Kamatar, Toby Liang, Jinjun Xiong, Wen-mei Hwu*

- `2211.04194v1` - [abs](http://arxiv.org/abs/2211.04194v1) - [pdf](http://arxiv.org/pdf/2211.04194v1)

> Assigning qualified, unbiased and interested reviewers to paper submissions is vital for maintaining the integrity and quality of the academic publishing system and providing valuable reviews to authors. However, matching thousands of submissions with thousands of potential reviewers within a limited time is a daunting challenge for a conference program committee. Prior efforts based on topic modeling have suffered from losing the specific context that help define the topics in a publication or submission abstract. Moreover, in some cases, topics identified are difficult to interpret. We propose an approach that learns from each abstract published by a potential reviewer the topics studied and the explicit context in which the reviewer studied the topics. Furthermore, we contribute a new dataset for evaluating reviewer matching systems. Our experiments show a significant, consistent improvement in precision when compared with the existing methods. We also use examples to demonstrate why our recommendations are more explainable. The new approach has been deployed successfully at top-tier conferences in the last two years.

</details>

<details>

<summary>2022-11-08 16:24:19 - Improved Analysis of RANKING for Online Vertex-Weighted Bipartite Matching in the Random Order Model</summary>

- *Billy Jin, David P. Williamson*

- `2007.12823v2` - [abs](http://arxiv.org/abs/2007.12823v2) - [pdf](http://arxiv.org/pdf/2007.12823v2)

> In this paper, we consider the online vertex-weighted bipartite matching problem in the random arrival model. We consider the generalization of the RANKING algorithm for this problem introduced by Huang, Tang, Wu, and Zhang (TALG 2019), who show that their algorithm has a competitive ratio of 0.6534. We show that assumptions in their analysis can be weakened, allowing us to replace their derivation of a crucial function $g$ on the unit square with a linear program that computes the values of a best possible $g$ under these assumptions on a discretized unit square. We show that the discretization does not incur much error, and show computationally that we can obtain a competitive ratio of 0.6629. To compute the bound over our discretized unit square we use parallelization, and still needed two days of computing on a 64-core machine. Furthermore, by modifying our linear program somewhat, we can show computationally an upper bound on our approach of 0.6688; any further progress beyond this bound will require either further weakening in the assumptions of $g$ or a stronger analysis than that of Huang et al.

</details>

<details>

<summary>2022-11-08 18:17:56 - Taking the Intentional Stance Seriously, or "Intending" to Improve Cognitive Systems</summary>

- *Will Bridewell*

- `2209.11764v3` - [abs](http://arxiv.org/abs/2209.11764v3) - [pdf](http://arxiv.org/pdf/2209.11764v3)

> Finding claims that researchers have made considerable progress in artificial intelligence over the last several decades is easy. However, our everyday interactions with cognitive systems (e.g., Siri, Alexa, DALL-E) quickly move from intriguing to frustrating. One cause of those frustrations rests in a mismatch between the expectations we have due to our inherent, folk-psychological theories and the real limitations we experience with existing computer programs. The software does not understand that people have goals, beliefs about how to achieve those goals, and intentions to act accordingly. One way to align cognitive systems with our expectations is to imbue them with mental states that mirror those we use to predict and explain human behavior. This paper discusses these concerns and illustrates the challenge of following this route by analyzing the mental state 'intention.' That analysis is joined with high-level methodological suggestions that support progress in this endeavor.

</details>

<details>

<summary>2022-11-08 19:07:06 - Differentiable Quantum Programming with Unbounded Loops</summary>

- *Wang Fang, Mingsheng Ying, Xiaodi Wu*

- `2211.04507v1` - [abs](http://arxiv.org/abs/2211.04507v1) - [pdf](http://arxiv.org/pdf/2211.04507v1)

> The emergence of variational quantum applications has led to the development of automatic differentiation techniques in quantum computing. Recently, Zhu et al. (PLDI 2020) have formulated differentiable quantum programming with bounded loops, providing a framework for scalable gradient calculation by quantum means for training quantum variational applications. However, promising parameterized quantum applications, e.g., quantum walk and unitary implementation, cannot be trained in the existing framework due to the natural involvement of unbounded loops. To fill in the gap, we provide the first differentiable quantum programming framework with unbounded loops, including a newly designed differentiation rule, code transformation, and their correctness proof. Technically, we introduce a randomized estimator for derivatives to deal with the infinite sum in the differentiation of unbounded loops, whose applicability in classical and probabilistic programming is also discussed. We implement our framework with Python and Q#, and demonstrate a reasonable sample efficiency. Through extensive case studies, we showcase an exciting application of our framework in automatically identifying close-to-optimal parameters for several parameterized quantum applications.

</details>

<details>

<summary>2022-11-08 19:56:59 - Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks</summary>

- *Marvin Schmitt, Paul-Christian BÃ¼rkner, Ullrich KÃ¶the, Stefan T. Radev*

- `2112.08866v5` - [abs](http://arxiv.org/abs/2112.08866v5) - [pdf](http://arxiv.org/pdf/2112.08866v5)

> Neural density estimators have proven remarkably powerful in performing efficient simulation-based Bayesian inference in various research domains. In particular, the BayesFlow framework uses a two-step approach to enable amortized parameter estimation in settings where the likelihood function is implicitly defined by a simulation program. But how faithful is such inference when simulations are poor representations of reality? In this paper, we conceptualize the types of model misspecification arising in simulation-based inference and systematically investigate the performance of the BayesFlow framework under these misspecifications. We propose an augmented optimization objective which imposes a probabilistic structure on the latent data space and utilize maximum mean discrepancy (MMD) to detect potentially catastrophic misspecifications during inference undermining the validity of the obtained results. We verify our detection criterion on a number of artificial and realistic misspecifications, ranging from toy conjugate models to complex models of decision making and disease outbreak dynamics applied to real data. Further, we show that posterior inference errors increase as a function of the distance between the true data-generating distribution and the typical set of simulations in the latent summary space. Thus, we demonstrate the dual utility of MMD as a method for detecting model misspecification and as a proxy for verifying the faithfulness of amortized Bayesian inference.

</details>

<details>

<summary>2022-11-08 20:43:51 - OutlierDetection.jl: A modular outlier detection ecosystem for the Julia programming language</summary>

- *David Muhr, Michael Affenzeller, Anthony D. Blaom*

- `2211.04550v1` - [abs](http://arxiv.org/abs/2211.04550v1) - [pdf](http://arxiv.org/pdf/2211.04550v1)

> OutlierDetection.jl is an open-source ecosystem for outlier detection in Julia. It provides a range of high-performance outlier detection algorithms implemented directly in Julia. In contrast to previous packages, our ecosystem enables the development highly-scalable outlier detection algorithms using a high-level programming language. Additionally, it provides a standardized, yet flexible, interface for future outlier detection algorithms and allows for model composition unseen in previous packages. Best practices such as unit testing, continuous integration, and code coverage reporting are enforced across the ecosystem. The most recent version of OutlierDetection.jl is available at https://github.com/OutlierDetectionJL/OutlierDetection.jl.

</details>

<details>

<summary>2022-11-08 21:12:19 - Focused Dynamic Slicing for Large Applications using an Abstract Memory-Model</summary>

- *Alexis Soifer, Diego Garbervetsky, Victor Braberman, Sebastian Uchitel*

- `2211.04560v1` - [abs](http://arxiv.org/abs/2211.04560v1) - [pdf](http://arxiv.org/pdf/2211.04560v1)

> Dynamic slicing techniques compute program dependencies to find all statements that affect the value of a variable at a program point for a specific execution. Despite their many potential uses, applicability is limited by the fact that they typically cannot scale beyond small-sized applications. We believe that at the heart of this limitation is the use of memory references to identify data-dependencies. Particularly, working with memory references hinders distinct treatment of the code-to-be-sliced (e.g., classes the user has an interest in) from the rest of the code (including libraries and frameworks). The ability to perform a coarser-grained analysis for the code that is not under focus may provide performance gains and could become one avenue toward scalability. In this paper, we propose a novel approach that completely replaces memory reference registering and processing with a memory analysis model that works with program symbols (i.e., terms). In fact, this approach enables the alternative of not instrumenting -- thus, not generating any trace -- for code that is not part of the code-to-be-sliced. We report on an implementation of an abstract dynamic slicer for C\#, \textit{DynAbs}, and an evaluation that shows how large and relevant parts of Roslyn and Powershell -- two of the largest and modern C\# applications that can be found in GitHub -- can be sliced for their test cases assertions in at most a few minutes. We also show how reducing the code-to-be-sliced focus can bring important speedups with marginal relative precision loss.

</details>

<details>

<summary>2022-11-08 22:58:58 - System Safety Engineering for Social and Ethical ML Risks: A Case Study</summary>

- *Edgar W. Jatho III, Logan O. Mailloux, Shalaleh Rismani, Eugene D. Williams, Joshua A. Kroll*

- `2211.04602v1` - [abs](http://arxiv.org/abs/2211.04602v1) - [pdf](http://arxiv.org/pdf/2211.04602v1)

> Governments, industry, and academia have undertaken efforts to identify and mitigate harms in ML-driven systems, with a particular focus on social and ethical risks of ML components in complex sociotechnical systems. However, existing approaches are largely disjointed, ad-hoc and of unknown effectiveness. Systems safety engineering is a well established discipline with a track record of identifying and managing risks in many complex sociotechnical domains. We adopt the natural hypothesis that tools from this domain could serve to enhance risk analyses of ML in its context of use. To test this hypothesis, we apply a "best of breed" systems safety analysis, Systems Theoretic Process Analysis (STPA), to a specific high-consequence system with an important ML-driven component, namely the Prescription Drug Monitoring Programs (PDMPs) operated by many US States, several of which rely on an ML-derived risk score. We focus in particular on how this analysis can extend to identifying social and ethical risks and developing concrete design-level controls to mitigate them.

</details>

<details>

<summary>2022-11-09 02:25:48 - RIGID: Robust Linear Regression with Missing Data</summary>

- *Alireza Aghasi, MohammadJavad Feizollahi, Saeed Ghadimi*

- `2205.13635v3` - [abs](http://arxiv.org/abs/2205.13635v3) - [pdf](http://arxiv.org/pdf/2205.13635v3)

> We present a robust framework to perform linear regression with missing entries in the features. By considering an elliptical data distribution, and specifically a multivariate normal model, we are able to conditionally formulate a distribution for the missing entries and present a robust framework, which minimizes the worst case error caused by the uncertainty about the missing data. We show that the proposed formulation, which naturally takes into account the dependency between different variables, ultimately reduces to a convex program, for which a customized and scalable solver can be delivered. In addition to a detailed analysis to deliver such solver, we also asymptoticly analyze the behavior of the proposed framework, and present technical discussions to estimate the required input parameters. We complement our analysis with experiments performed on synthetic, semi-synthetic, and real data, and show how the proposed formulation improves the prediction accuracy and robustness, and outperforms the competing techniques.   Missing data is a common problem associated with many datasets in machine learning. With the significant increase in using robust optimization techniques to train machine learning models, this paper presents a novel robust regression framework that operates by minimizing the uncertainty associated with missing data. The proposed approach allows training models with incomplete data, while minimizing the impact of uncertainty associated with the unavailable data. The ideas developed in this paper can be generalized beyond linear models and elliptical data distributions.

</details>

<details>

<summary>2022-11-09 04:53:25 - Dynamic Slicing by On-demand Re-execution</summary>

- *Ivan Postolski, Victor Braberman, Diego Garbervetsky, Sebastian Uchitel*

- `2211.04683v1` - [abs](http://arxiv.org/abs/2211.04683v1) - [pdf](http://arxiv.org/pdf/2211.04683v1)

> In this paper, we propose a novel approach that aims to offer an alternative to the prevalent paradigm to dynamic slicing construction. Dynamic slicing requires dynamic data and control dependencies that arise in an execution. During a single execution, memory reference information is recorded and then traversed to extract dependencies. Execute-once approaches and tools are challenged even by executions of moderate size of simple and short programs. We propose to shift practical time complexity from execution size to slice size. In particular, our approach executes the program multiple times while tracking targeted information at each execution. We present a concrete algorithm that follows an on-demand re-execution paradigm that uses a novel concept of frontier dependency to incrementally build a dynamic slice. To focus dependency tracking, the algorithm relies on static analysis. We show results of an evaluation on the SV-COMP benchmark and Antrl4 unit tests that provide evidence that on-demand re-execution can provide performance gains particularly when slice size is small and execution size is large.

</details>

<details>

<summary>2022-11-09 07:09:06 - Improve Model Testing by Integrating Bounded Model Checking and Coverage Guided Fuzzing</summary>

- *Yixiao Yang*

- `2211.04712v1` - [abs](http://arxiv.org/abs/2211.04712v1) - [pdf](http://arxiv.org/pdf/2211.04712v1)

> The control logic models built by Simulink or Ptolemy have been widely used in industry scenes. It is an urgent need to ensure the safety and security of the control logic models. Test case generation technologies are widely used to ensure the safety and security. State-of-the-art model testing tools employ model checking techniques or search-based methods to generate test cases. Traditional search based techniques based on Simulink simulation are plagued by problems such as low speed and high overhead. Traditional model checking techniques such as symbolic execution have limited performance when dealing with nonlinear elements and complex loops. Recently, coverage guided fuzzing technologies are known to be effective for test case generation, due to their high efficiency and impressive effects over complex branches of loops.   In this paper, we apply fuzzing methods to improve model testing and demonstrate the effectiveness. The fuzzing methods aim to cover more program branches by mutating valuable seeds. Inspired by this feature, we propose a novel integration technology SPsCGF, which leverages bounded model checking for symbolic execution to generate test cases as initial seeds and then conduct fuzzing based upon these worthy seeds. In this manner, our work combines the advantages of the model checking methods and fuzzing techniques in a novel way. Since the control logic models always receive signal inputs, we specifically design novel mutation operators for signals to improve the existing fuzzing method in model testing. Over the evaluated benchmarks which consist of industrial cases, SPsCGF could achieve 8% to 38% higher model coverage and 3x-10x time efficiency compared with the state-of-the-art works.

</details>

<details>

<summary>2022-11-09 11:41:33 - An Exploratory Analysis of Feedback Types Used in Online Coding Exercises</summary>

- *Natalie Kiesler*

- `2206.03077v2` - [abs](http://arxiv.org/abs/2206.03077v2) - [pdf](http://arxiv.org/pdf/2206.03077v2)

> Online coding environments can help support computing students gain programming practice at their own pace. Especially informative feedback can be beneficial during such self-guided, independent study phases. This research aims at the identification of feedback types applied by CodingBat, Scratch and Blockly. Tutoring feedback as coined by Susanne Narciss along with the specification of subtypes by Keuning, Jeuring and Heeren constitute the theoretical basis. Accordingly, the five categories of elaborated feedback (knowledge about task requirements, knowledge about concepts, knowledge about mistakes, knowledge about how to proceed, and knowledge about meta-cognition) and their subtypes were utilized for the analysis of available feedback options. The study revealed difficulties in identifying clear-cut boundaries between feedback types, as the offered feedback usually integrates more than one type or subtype. Moreover, currently defined feedback types do not rigorously distinguish individualized and generic feedback. The lack of granularity is also evident in the absence of subtypes relating to the knowledge type of the task. The analysis thus has implications for the future design and investigation of applied tutoring feedback. It encourages future research on feedback types and their implementation in the context of programming exercises to define feedback types that match the demands of novice programmers.

</details>

<details>

<summary>2022-11-09 16:03:11 - Fast and Scalable Channels in Kotlin Coroutines</summary>

- *Nikita Koval, Dan Alistarh, Roman Elizarov*

- `2211.04986v1` - [abs](http://arxiv.org/abs/2211.04986v1) - [pdf](http://arxiv.org/pdf/2211.04986v1)

> Asynchronous programming has gained significant popularity over the last decade: support for this programming pattern is available in many popular languages via libraries and native language implementations, typically in the form of coroutines or the async/await construct. Instead of programming via shared memory, this concept assumes implicit synchronization through message passing. The key data structure enabling such communication is the rendezvous channel. Roughly, a rendezvous channel is a blocking queue of size zero, so both send(e) and receive() operations wait for each other, performing a rendezvous when they meet. To optimize the message passing pattern, channels are usually equipped with a fixed-size buffer, so send(e)-s do not suspend and put elements into the buffer until its capacity is exceeded. This primitive is known as a buffered channel.   This paper presents a fast and scalable algorithm for both rendezvous and buffered channels. Similarly to modern queues, our solution is based on an infinite array with two positional counters for send(e) and receive() operations, leveraging the unconditional Fetch-And-Add instruction to update them. Yet, the algorithm requires non-trivial modifications of this classic pattern, in order to support the full channel semantics, such as buffering and cancellation of waiting requests. We compare the performance of our solution to that of the Kotlin implementation, as well as against other academic proposals, showing up to 9.8x speedup. To showcase its expressiveness and performance, we also integrated the proposed algorithm into the standard Kotlin Coroutines library, replacing the previous channel implementations.

</details>

<details>

<summary>2022-11-09 18:08:44 - Comparative analysis of machine learning methods for active flow control</summary>

- *Fabio Pino, Lorenzo Schena, Jean Rabault, Miguel A. Mendez*

- `2202.11664v3` - [abs](http://arxiv.org/abs/2202.11664v3) - [pdf](http://arxiv.org/pdf/2202.11664v3)

> Machine learning frameworks such as Genetic Programming (GP) and Reinforcement Learning (RL) are gaining popularity in flow control. This work presents a comparative analysis of the two, bench-marking some of their most representative algorithms against global optimization techniques such as Bayesian Optimization (BO) and Lipschitz global optimization (LIPO). First, we review the general framework of the model-free control problem, bringing together all methods as black-box optimization problems. Then, we test the control algorithms on three test cases. These are (1) the stabilization of a nonlinear dynamical system featuring frequency cross-talk, (2) the wave cancellation from a Burgers' flow and (3) the drag reduction in a cylinder wake flow. We present a comprehensive comparison to illustrate their differences in exploration versus exploitation and their balance between `model capacity' in the control law definition versus `required complexity'. We believe that such a comparison paves the way toward the hybridization of the various methods, and we offer some perspective on their future development in the literature on flow control problems.

</details>

<details>

<summary>2022-11-09 18:58:21 - ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention</summary>

- *Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng Wang, Yingyan Lin*

- `2211.05109v1` - [abs](http://arxiv.org/abs/2211.05109v1) - [pdf](http://arxiv.org/pdf/2211.05109v1)

> Vision Transformer (ViT) has emerged as a competitive alternative to convolutional neural networks for various computer vision applications. Specifically, ViT multi-head attention layers make it possible to embed information globally across the overall image. Nevertheless, computing and storing such attention matrices incurs a quadratic cost dependency on the number of patches, limiting its achievable efficiency and scalability and prohibiting more extensive real-world ViT applications on resource-constrained devices. Sparse attention has been shown to be a promising direction for improving hardware acceleration efficiency for NLP models. However, a systematic counterpart approach is still missing for accelerating ViT models. To close the above gap, we propose a first-of-its-kind algorithm-hardware codesigned framework, dubbed ViTALiTy, for boosting the inference efficiency of ViTs. Unlike sparsity-based Transformer accelerators for NLP, ViTALiTy unifies both low-rank and sparse components of the attention in ViTs. At the algorithm level, we approximate the dot-product softmax operation via first-order Taylor attention with row-mean centering as the low-rank component to linearize the cost of attention blocks and further boost the accuracy by incorporating a sparsity-based regularization. At the hardware level, we develop a dedicated accelerator to better leverage the resulting workload and pipeline from ViTALiTy's linear Taylor attention which requires the execution of only the low-rank component, to further boost the hardware efficiency. Extensive experiments and ablation studies validate that ViTALiTy offers boosted end-to-end efficiency (e.g., $3\times$ faster and $3\times$ energy-efficient) under comparable accuracy, with respect to the state-of-the-art solution.

</details>

<details>

<summary>2022-11-09 19:33:27 - Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database</summary>

- *Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, Yingbo Zhou*

- `2211.05165v1` - [abs](http://arxiv.org/abs/2211.05165v1) - [pdf](http://arxiv.org/pdf/2211.05165v1)

> Parsing natural language questions into executable logical forms is a useful and interpretable way to perform question answering on structured data such as knowledge bases (KB) or databases (DB). However, existing approaches on semantic parsing cannot adapt to both modalities, as they suffer from the exponential growth of the logical form candidates and can hardly generalize to unseen data. In this work, we propose Uni-Parser, a unified semantic parser for question answering (QA) on both KB and DB. We introduce the primitive (relation and entity in KB, and table name, column name and cell value in DB) as an essential element in our framework. The number of primitives grows linearly with the number of retrieved relations in KB and DB, preventing us from dealing with exponential logic form candidates. We leverage the generator to predict final logical forms by altering and composing topranked primitives with different operations (e.g. select, where, count). With sufficiently pruned search space by a contrastive primitive ranker, the generator is empowered to capture the composition of primitives enhancing its generalization ability. We achieve competitive results on multiple KB and DB QA benchmarks more efficiently, especially in the compositional and zero-shot settings.

</details>

<details>

<summary>2022-11-09 20:25:33 - Modeling Motivational Interviewing Strategies On An Online Peer-to-Peer Counseling Platform</summary>

- *Raj Sanjay Shah, Faye Holt, Shirley Anugrah Hayati, Aastha Agarwal, Yi-Chia Wang, Robert E. Kraut, Diyi Yang*

- `2211.05182v1` - [abs](http://arxiv.org/abs/2211.05182v1) - [pdf](http://arxiv.org/pdf/2211.05182v1)

> Millions of people participate in online peer-to-peer support sessions, yet there has been little prior research on systematic psychology-based evaluations of fine-grained peer-counselor behavior in relation to client satisfaction. This paper seeks to bridge this gap by mapping peer-counselor chat-messages to motivational interviewing (MI) techniques. We annotate 14,797 utterances from 734 chat conversations using 17 MI techniques and introduce four new interviewing codes such as chit-chat and inappropriate to account for the unique conversational patterns observed on online platforms. We automate the process of labeling peer-counselor responses to MI techniques by fine-tuning large domain-specific language models and then use these automated measures to investigate the behavior of the peer counselors via correlational studies. Specifically, we study the impact of MI techniques on the conversation ratings to investigate the techniques that predict clients' satisfaction with their counseling sessions. When counselors use techniques such as reflection and affirmation, clients are more satisfied. Examining volunteer counselors' change in usage of techniques suggest that counselors learn to use more introduction and open questions as they gain experience. This work provides a deeper understanding of the use of motivational interviewing techniques on peer-to-peer counselor platforms and sheds light on how to build better training programs for volunteer counselors on online platforms.

</details>

<details>

<summary>2022-11-09 20:32:25 - Using Deception in Markov Game to Understand Adversarial Behaviors through a Capture-The-Flag Environment</summary>

- *Siddhant Bhambri, Purv Chauhan, Frederico Araujo, Adam DoupÃ©, Subbarao Kambhampati*

- `2210.15011v2` - [abs](http://arxiv.org/abs/2210.15011v2) - [pdf](http://arxiv.org/pdf/2210.15011v2)

> Identifying the actual adversarial threat against a system vulnerability has been a long-standing challenge for cybersecurity research. To determine an optimal strategy for the defender, game-theoretic based decision models have been widely used to simulate the real-world attacker-defender scenarios while taking the defender's constraints into consideration. In this work, we focus on understanding human attacker behaviors in order to optimize the defender's strategy. To achieve this goal, we model attacker-defender engagements as Markov Games and search for their Bayesian Stackelberg Equilibrium. We validate our modeling approach and report our empirical findings using a Capture-The-Flag (CTF) setup, and we conduct user studies on adversaries with varying skill-levels. Our studies show that application-level deceptions are an optimal mitigation strategy against targeted attacks -- outperforming classic cyber-defensive maneuvers, such as patching or blocking network requests. We use this result to further hypothesize over the attacker's behaviors when trapped in an embedded honeypot environment and present a detailed analysis of the same.

</details>

<details>

<summary>2022-11-09 21:04:59 - Repairing Neural Networks by Leaving the Right Past Behind</summary>

- *Ryutaro Tanno, Melanie F. Pradier, Aditya Nori, Yingzhen Li*

- `2207.04806v2` - [abs](http://arxiv.org/abs/2207.04806v2) - [pdf](http://arxiv.org/pdf/2207.04806v2)

> Prediction failures of machine learning models often arise from deficiencies in training data, such as incorrect labels, outliers, and selection biases. However, such data points that are responsible for a given failure mode are generally not known a priori, let alone a mechanism for repairing the failure. This work draws on the Bayesian view of continual learning, and develops a generic framework for both, identifying training examples that have given rise to the target failure, and fixing the model through erasing information about them. This framework naturally allows leveraging recent advances in continual learning to this new problem of model repairment, while subsuming the existing works on influence functions and data deletion as specific instances. Experimentally, the proposed approach outperforms the baselines for both identification of detrimental training data and fixing model failures in a generalisable manner.

</details>

<details>

<summary>2022-11-09 21:50:41 - Flaky Performances when Pretraining on Relational Databases</summary>

- *Shengchao Liu, David Vazquez, Jian Tang, Pierre-AndrÃ© NoÃ«l*

- `2211.05213v1` - [abs](http://arxiv.org/abs/2211.05213v1) - [pdf](http://arxiv.org/pdf/2211.05213v1)

> We explore the downstream task performances for graph neural network (GNN) self-supervised learning (SSL) methods trained on subgraphs extracted from relational databases (RDBs). Intuitively, this joint use of SSL and GNNs should allow to leverage more of the available data, which could translate to better results. However, we found that naively porting contrastive SSL techniques can cause ``negative transfer'': linear evaluation on fixed representations from a pretrained model performs worse than on representations from the randomly-initialized model. Based on the conjecture that contrastive SSL conflicts with the message passing layers of the GNN, we propose InfoNode: a contrastive loss aiming to maximize the mutual information between a node's initial- and final-layer representation. The primary empirical results support our conjecture and the effectiveness of InfoNode.

</details>

<details>

<summary>2022-11-10 09:31:51 - Semantic Learning and Emulation Based Cross-platform Binary Vulnerability Seeker</summary>

- *Jian Gao, Yu Jiang, Zhe Liu, Xin Yang, Cong Wang, Xun Jiao, Zijiang Yang, Jiaguang Sun*

- `2211.05441v1` - [abs](http://arxiv.org/abs/2211.05441v1) - [pdf](http://arxiv.org/pdf/2211.05441v1)

> Clone detection is widely exploited for software vulnerability search. The approaches based on source code analysis cannot be applied to binary clone detection because the same source code can produce significantly different binaries. In this paper, we present BinSeeker, a cross-platform binary seeker that integrates semantic learning and emulation. With the help of the labeled semantic flow graph, BinSeeker can quickly identify M candidate functions that are most similar to the vulnerability from the target binary. The value of M is relatively large so this semantic learning procedure essentially eliminates those functions that are very unlikely to have the vulnerability. Then, semantic emulation is conducted on these M candidates to obtain their dynamic signature sequences. By comparing signature sequences, BinSeeker produces top-N functions that exhibit most similar behavior to that of the vulnerability. With fast filtering of semantic learning and accurate comparison of semantic emulation, BinSeeker seeks vulnerability precisely with little overhead. The experiments on six widely used programs with fifteen known CVE vulnerabilities demonstrate that BinSeeker outperforms three state-of-the-art tools Genius, Gemini and CACompare. Regarding search accuracy, BinSeeker achieves an MRR value of 0.65 in the target programs, whereas the MRR values by Genius, Gemini and CACompare are 0.17, 0.07 and 0.42, respectively. If we consider ranking a function with the targeted vulnerability in the top-5 as accurate, BinSeeker achieves the accuracy of 93.33 percent, while the accuracy of the other three tools is merely 33.33, 13.33 and 53.33 percent, respectively. Such accuracy is achieved with 0.27s on average to determine whether the target binary function contains a known vulnerability, and the time for the other three tools are 1.57s, 0.15s and 0.98s, respectively.

</details>

<details>

<summary>2022-11-10 10:50:55 - The Complexity Landscape of Fixed-Parameter Directed Steiner Network Problems</summary>

- *Andreas Emil Feldmann, Daniel Marx*

- `1707.06808v5` - [abs](http://arxiv.org/abs/1707.06808v5) - [pdf](http://arxiv.org/pdf/1707.06808v5)

> Given a directed graph $G$ and a list $(s_1,t_1),\dots,(s_d,t_d)$ of terminal pairs, the Directed Steiner Network problem asks for a minimum-cost subgraph of $G$ that contains a directed $s_i\to t_i$ path for every $1\le i \le k$. The special case Directed Steiner Tree (when we ask for paths from a root $r$ to terminals $t_1,\dots,t_d$) is known to be fixed-parameter tractable parameterized by the number of terminals, while the special case Strongly Connected Steiner Subgraph (when we ask for a path from every $t_i$ to every other $t_j$) is known to be W[1]-hard. We systematically explore the complexity landscape of directed Steiner problems to fully understand which other special cases are FPT or W[1]-hard. Formally, if $\mathcal{H}$ is a class of directed graphs, then we look at the special case of Directed Steiner Network where the list $(s_1,t_1),\dots,(s_d,t_d)$ of requests form a directed graph that is a member of $\mathcal{H}$. Our main result is a complete characterization of the classes $\mathcal{H}$ resulting in fixed-parameter tractable special cases: we show that if every pattern in $\mathcal{H}$ has the combinatorial property of being "transitively equivalent to a bounded-length caterpillar with a bounded number of extra edges," then the problem is FPT, and it is W[1]-hard for every recursively enumerable $\mathcal{H}$ not having this property. This complete dichotomy unifies and generalizes the known results showing that Directed Steiner Tree is FPT [Dreyfus and Wagner, Networks 1971], $q$-Root Steiner Tree is FPT for constant $q$ [Such\'y, WG 2016], Strongly Connected Steiner Subgraph is W[1]-hard [Guo et al., SIAM J. Discrete Math. 2011], and Directed Steiner Network is solvable in polynomial-time for constant number of terminals [Feldman and Ruhl, SIAM J. Comput. 2006], and moreover reveals a large continent of tractable cases that were not known before.

</details>

<details>

<summary>2022-11-10 15:40:44 - The Sherali-Adams Hierarchy for Promise CSPs through Tensors</summary>

- *Lorenzo Ciardo, Stanislav Å½ivnÃ½*

- `2203.02478v3` - [abs](http://arxiv.org/abs/2203.02478v3) - [pdf](http://arxiv.org/pdf/2203.02478v3)

> We study the Sherali-Adams linear programming hierarchy in the context of promise constraint satisfaction problems (PCSPs). We characterise when a level of the hierarchy accepts an instance in terms of a homomorphism problem for an appropriate multilinear structure obtained through a tensor power of the constraint language. The geometry of this structure, which consists in a space of tensors satisfying certain symmetries, allows then to establish non-solvability of the approximate graph colouring problem via constantly many levels of Sherali-Adams.   Besides this primary application, our tensorisation construction introduces a new tool to the study of hierarchies of algorithmic relaxations for computational problems within (and, possibly, beyond) the context of constraint satisfaction. In particular, we see it as a key step towards the algebraic characterisation of the power of Sherali-Adams for PCSPs.

</details>

<details>

<summary>2022-11-10 18:38:09 - On Minimizing Tardy Processing Time, Max-Min Skewed Convolution, and Triangular Structured ILPs</summary>

- *Kim-Manuel Klein, Adam Polak, Lars Rohwedder*

- `2211.05053v2` - [abs](http://arxiv.org/abs/2211.05053v2) - [pdf](http://arxiv.org/pdf/2211.05053v2)

> The starting point of this paper is the problem of scheduling $n$ jobs with processing times and due dates on a single machine so as to minimize the total processing time of tardy jobs, i.e., $1||\sum p_j U_j$. This problem was identified by Bringmann et al. (Algorithmica 2022) as a natural subquadratic-time special case of the classic $1||\sum w_j U_j$ problem, which likely requires time quadratic in the total processing time $P$, because of a fine-grained lower bound. Bringmann et al.~obtain their $\tilde{O}(P^{7/4})$ time scheduling algorithm through a new variant of convolution, dubbed Max-Min Skewed Convolution, which they solve in $\tilde{O}(n^{7/4})$ time. Our main technical contribution is a faster and simpler convolution algorithm running in $\tilde{O}(n^{5/3})$ time. It implies an $\tilde{O}(P^{5/3})$ time algorithm for $1||\sum p_j U_j$, but may also be of independent interest.   Inspired by recent developments for the Subset Sum and Knapsack problems, we study $1||\sum p_j U_j$ parameterized by the maximum job processing time $p_{\max}$. With proximity techniques borrowed from integer linear programming (ILP), we show structural properties of the problem that, coupled with a new dynamic programming formulation, lead to an $\tilde{O}(n+p_{\max}^3)$ time algorithm. Moreover, in the setting with multiple machines, we use similar techniques to get an $n \cdot p_{\max}^{O(m)}$ time algorithm for $Pm||\sum p_j U_j$.   Finally, we point out that the considered problems exhibit a particular triangular block structure in the constraint matrices of their ILP formulations. In light of recent ILP research, a question that arises is whether one can devise a generic algorithm for such a class of ILPs. We give a negative answer to this question: we show that already a slight generalization of the structure of the scheduling ILP leads to a strongly NP-hard problem.

</details>

<details>

<summary>2022-11-10 19:27:21 - No Privacy in the Electronics Repair Industry</summary>

- *Jason Ceci, Jonah Stegman, Hassan Khan*

- `2211.05824v1` - [abs](http://arxiv.org/abs/2211.05824v1) - [pdf](http://arxiv.org/pdf/2211.05824v1)

> Electronics repair and service providers offer a range of services to computing device owners across North America -- from software installation to hardware repair. Device owners obtain these services and leave their device along with their access credentials at the mercy of technicians, which leads to privacy concerns for owners' personal data. We conduct a comprehensive four-part study to measure the state of privacy in the electronics repair industry. First, through a field study with 18 service providers, we uncover that most service providers do not have any privacy policy or controls to safeguard device owners' personal data from snooping by technicians. Second, we drop rigged devices for repair at 16 service providers and collect data on widespread privacy violations by technicians, including snooping on personal data, copying data off the device, and removing tracks of snooping activities. Third, we conduct an online survey (n=112) to collect data on customers' experiences when getting devices repaired. Fourth, we invite a subset of survey respondents (n=30) for semi-structured interviews to establish a deeper understanding of their experiences and identify potential solutions to curtail privacy violations by technicians. We apply our findings to discuss possible controls and actions different stakeholders and regulatory agencies should take to improve the state of privacy in the repair industry.

</details>

<details>

<summary>2022-11-10 21:04:36 - Silent Spring: Prototype Pollution Leads to Remote Code Execution in Node.js</summary>

- *Mikhail Shcherbakov, Musard Balliu, Cristian-Alexandru Staicu*

- `2207.11171v2` - [abs](http://arxiv.org/abs/2207.11171v2) - [pdf](http://arxiv.org/pdf/2207.11171v2)

> Prototype pollution is a dangerous vulnerability affecting prototype-based languages like JavaScript and the Node.js platform. It refers to the ability of an attacker to inject properties into an object's root prototype at runtime and subsequently trigger the execution of legitimate code gadgets that access these properties on the object's prototype, leading to attacks such as Denial of Service (DoS), privilege escalation, and Remote Code Execution (RCE). While there is anecdotal evidence that prototype pollution leads to RCE, current research does not tackle the challenge of gadget detection, thus only showing feasibility of DoS attacks, mainly against Node.js libraries.   In this paper, we set out to study the problem in a holistic way, from the detection of prototype pollution to detection of gadgets, with the ambitious goal of finding end-to-end exploits beyond DoS, in full-fledged Node.js applications. We build the first multi-staged framework that uses multi-label static taint analysis to identify prototype pollution in Node.js libraries and applications, as well as a hybrid approach to detect universal gadgets, notably, by analyzing the Node.js source code. We implement our framework on top of GitHub's static analysis framework CodeQL to find 11 universal gadgets in core Node.js APIs, leading to code execution. Furthermore, we use our methodology in a study of 15 popular Node.js applications to identify prototype pollutions and gadgets. We manually exploit eight RCE vulnerabilities in three high-profile applications such as NPM CLI, Parse Server, and Rocket.Chat. Our results provide alarming evidence that prototype pollution in combination with powerful universal gadgets lead to RCE in Node.js.

</details>

<details>

<summary>2022-11-10 21:49:29 - Neural Architecture Search using Property Guided Synthesis</summary>

- *Charles Jin, Phitchaya Mangpo Phothilimthana, Sudip Roy*

- `2205.03960v3` - [abs](http://arxiv.org/abs/2205.03960v3) - [pdf](http://arxiv.org/pdf/2205.03960v3)

> In the past few years, neural architecture search (NAS) has become an increasingly important tool within the deep learning community. Despite the many recent successes of NAS, however, most existing approaches operate within highly structured design spaces, and hence explore only a small fraction of the full search space of neural architectures while also requiring significant manual effort from domain experts. In this work, we develop techniques that enable efficient NAS in a significantly larger design space. To accomplish this, we propose to perform NAS in an abstract search space of program properties. Our key insights are as follows: (1) the abstract search space is significantly smaller than the original search space, and (2) architectures with similar program properties also have similar performance; thus, we can search more efficiently in the abstract search space. To enable this approach, we also propose a novel efficient synthesis procedure, which accepts a set of promising program properties, and returns a satisfying neural architecture. We implement our approach, $\alpha$NAS, within an evolutionary framework, where the mutations are guided by the program properties. Starting with a ResNet-34 model, $\alpha$NAS produces a model with slightly improved accuracy on CIFAR-10 but 96% fewer parameters. On ImageNet, $\alpha$NAS is able to improve over Vision Transformer (30% fewer FLOPS and parameters), ResNet-50 (23% fewer FLOPS, 14% fewer parameters), and EfficientNet (7% fewer FLOPS and parameters) without any degradation in accuracy.

</details>

<details>

<summary>2022-11-11 01:16:06 - Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning Rates</summary>

- *Jacob Portes, Davis Blalock, Cory Stephenson, Jonathan Frankle*

- `2206.00832v2` - [abs](http://arxiv.org/abs/2206.00832v2) - [pdf](http://arxiv.org/pdf/2206.00832v2)

> Benchmarking the tradeoff between neural network accuracy and training time is computationally expensive. Here we show how a multiplicative cyclic learning rate schedule can be used to construct a tradeoff curve in a single training run. We generate cyclic tradeoff curves for combinations of training methods such as Blurpool, Channels Last, Label Smoothing and MixUp, and highlight how these cyclic tradeoff curves can be used to evaluate the effects of algorithmic choices on network training efficiency.

</details>

<details>

<summary>2022-11-11 02:35:48 - Mapping Out the HPC Dependency Chaos</summary>

- *Farid Zakaria, Thomas R. W. Scogland, Todd Gamblin, Carlos Maltzahn*

- `2211.05118v2` - [abs](http://arxiv.org/abs/2211.05118v2) - [pdf](http://arxiv.org/pdf/2211.05118v2)

> High Performance Computing~(HPC) software stacks have become complex, with the dependencies of some applications numbering in the hundreds. Packaging, distributing, and administering software stacks of that scale is a complex undertaking anywhere. HPC systems deal with esoteric compilers, hardware, and a panoply of uncommon combinations. In this paper, we explore the mechanisms available for packaging software to find its own dependencies in the context of a taxonomy of software distribution, and discuss their benefits and pitfalls. We discuss workarounds for some common problems caused by using these composed stacks and introduce Shrinkwrap: A solution to producing binaries that directly load their dependencies from precise locations and in a precise order. Beyond simplifying the use of the binaries, this approach also speeds up loading as much as 7x for a large dynamically-linked MPI application in our evaluation.

</details>

<details>

<summary>2022-11-11 07:09:31 - A Faster Small Treewidth SDP Solver</summary>

- *Yuzhou Gu, Zhao Song*

- `2211.06033v1` - [abs](http://arxiv.org/abs/2211.06033v1) - [pdf](http://arxiv.org/pdf/2211.06033v1)

> Semidefinite programming is a fundamental tool in optimization and theoretical computer science. It has been extensively used as a black-box for solving many problems, such as embedding, complexity, learning, and discrepancy.   One natural setting of semidefinite programming is the small treewidth setting. The best previous SDP solver under small treewidth setting is due to Zhang-Lavaei '18, which takes $n^{1.5} \tau^{6.5}$ time. In this work, we show how to solve a semidefinite programming with $n \times n$ variables, $m$ constraints and $\tau$ treewidth in $n \tau^{2\omega+0.5}$ time, where $\omega < 2.373$ denotes the exponent of matrix multiplication. We give the first SDP solver that runs in time in linear in number of variables under this setting.   In addition, we improve the running time that solves a linear programming with tau treewidth from $n \tau^2$ (Dong-Lee-Ye '21) to $n \tau^{(\omega+1)/2}$.

</details>

<details>

<summary>2022-11-11 10:15:55 - Anonymization of Whole Slide Images in Histopathology for Research and Education</summary>

- *Tom Bisson, Michael Franz, Isil Dogan O, Daniel Romberg, Christoph Jansen, Peter Hufnagl, Norman Zerbe*

- `2211.06103v1` - [abs](http://arxiv.org/abs/2211.06103v1) - [pdf](http://arxiv.org/pdf/2211.06103v1)

> Objective: The exchange of health-related data is subject to regional laws and regulations, such as the General Data Protection Regulation (GDPR) in the EU or the Health Insurance Portability and Accountability Act (HIPAA) in the United States, resulting in non-trivial challenges for researchers and educators when working with these data. In pathology, the digitization of diagnostic tissue samples inevitably generates identifying data that can consist of sensitive but also acquisition-related information stored in vendor-specific file formats. Distribution and off-clinical use of these Whole Slide Images (WSI) is usually done in these formats, as an industry-wide standardization such as DICOM is yet only tentatively adopted and slide scanner vendors currently do not provide anonymization functionality.   Methods: We developed a guideline for the proper handling of histopathological image data particularly for research and education with regard to the GDPR. In this context, we evaluated existing anonymization methods and examined proprietary format specifications to identify all sensitive information for the most common WSI formats. This work results in a software library that enables GDPR-compliant anonymization of WSIs while preserving the native formats.   Results: Based on the analysis of proprietary formats, all occurrences of sensitive information were identified for file formats frequently used in clinical routine, and finally, an open-source programming library with an executable CLI-tool and wrappers for different programming languages was developed.   Conclusions: Our analysis showed that there is no straightforward software solution to anonymize WSIs in a GDPR-compliant way while maintaining the data format. We closed this gap with our extensible open-source library that works instantaneously and offline.

</details>

<details>

<summary>2022-11-11 11:20:35 - Physically Consistent Neural ODEs for Learning Multi-Physics Systems</summary>

- *Muhammad Zakwan, Loris Di Natale, Bratislav Svetozarevic, Philipp Heer, Colin N. Jones, Giancarlo Ferrari Trecate*

- `2211.06130v1` - [abs](http://arxiv.org/abs/2211.06130v1) - [pdf](http://arxiv.org/pdf/2211.06130v1)

> Despite the immense success of neural networks in modeling system dynamics from data, they often remain physics-agnostic black boxes. In the particular case of physical systems, they might consequently make physically inconsistent predictions, which makes them unreliable in practice. In this paper, we leverage the framework of Irreversible port-Hamiltonian Systems (IPHS), which can describe most multi-physics systems, and rely on Neural Ordinary Differential Equations (NODEs) to learn their parameters from data. Since IPHS models are consistent with the first and second principles of thermodynamics by design, so are the proposed Physically Consistent NODEs (PC-NODEs). Furthermore, the NODE training procedure allows us to seamlessly incorporate prior knowledge of the system properties in the learned dynamics. We demonstrate the effectiveness of the proposed method by learning the thermodynamics of a building from the real-world measurements and the dynamics of a simulated gas-piston system. Thanks to the modularity and flexibility of the IPHS framework, PC-NODEs can be extended to learn physically consistent models of multi-physics distributed systems.

</details>

<details>

<summary>2022-11-11 14:42:34 - An Integrity-Focused Threat Model for Software Development Pipelines</summary>

- *B. M. Reichert, R. R. Obelheiro*

- `2211.06249v1` - [abs](http://arxiv.org/abs/2211.06249v1) - [pdf](http://arxiv.org/pdf/2211.06249v1)

> In recent years, there has been a growing concern with software integrity, that is, the assurance that software has not been tampered with on the path between developers and users. This path is represented by a software development pipeline and plays a pivotal role in software supply chain security. While there have been efforts to improve the security of development pipelines, there is a lack of a comprehensive view of the threats affecting them. We develop a systematic threat model for a generic software development pipeline using the STRIDE framework and identify possible mitigations for each threat. The pipeline adopted as a reference comprises five stages (integration, continuous integration, infrastructure-as-code, deployment, and release), and we review vulnerabilities and attacks in all stages reported in the literature. We present a case study applying this threat model to a specific pipeline, showing that the adaptation is straightforward and produces a list of relevant threats.

</details>

<details>

<summary>2022-11-11 15:20:23 - Approximate Max-Flow Min-Multicut Theorem for Graphs of Bounded Treewidth</summary>

- *Tobias Friedrich, Davis Issac, Nikhil Kumar, Nadym Mallek, Ziena Zeif*

- `2211.06267v1` - [abs](http://arxiv.org/abs/2211.06267v1) - [pdf](http://arxiv.org/pdf/2211.06267v1)

> We prove an approximate max-multiflow min-multicut theorem for bounded treewidth graphs. In particular, we show the following: Given a treewidth-$r$ graph, there exists a (fractional) multicommodity flow of value $f$, and a multicut of capacity $c$ such that $ f \leq c \leq \mathcal{O}(\ln (r+1)) \cdot f$. It is well known that the multiflow-multicut gap on an $r$-vertex (constant degree) expander graph can be $\Omega(\ln r)$, and hence our result is tight up to constant factors. Our proof is constructive, and we also obtain a polynomial time $\mathcal{O}(\ln (r+1))$-approximation algorithm for the minimum multicut problem on treewidth-$r$ graphs. Our algorithm proceeds by rounding the optimal fractional solution to the natural linear programming relaxation of the multicut problem. We introduce novel modifications to the well-known region growing algorithm to facilitate the rounding while guaranteeing at most a logarithmic factor loss in the treewidth.

</details>

<details>

<summary>2022-11-11 19:56:11 - NeuroCERIL: Robotic Imitation Learning via Hierarchical Cause-Effect Reasoning in Programmable Attractor Neural Networks</summary>

- *Gregory P. Davis, Garrett E. Katz, Rodolphe J. Gentili, James A. Reggia*

- `2211.06462v1` - [abs](http://arxiv.org/abs/2211.06462v1) - [pdf](http://arxiv.org/pdf/2211.06462v1)

> Imitation learning allows social robots to learn new skills from human teachers without substantial manual programming, but it is difficult for robotic imitation learning systems to generalize demonstrated skills as well as human learners do. Contemporary neurocomputational approaches to imitation learning achieve limited generalization at the cost of data-intensive training, and often produce opaque models that are difficult to understand and debug. In this study, we explore the viability of developing purely-neural controllers for social robots that learn to imitate by reasoning about the underlying intentions of demonstrated behaviors. We present NeuroCERIL, a brain-inspired neurocognitive architecture that uses a novel hypothetico-deductive reasoning procedure to produce generalizable and human-readable explanations for demonstrated behavior. This approach combines bottom-up abductive inference with top-down predictive verification, and captures important aspects of human causal reasoning that are relevant to a broad range of cognitive domains. Our empirical results demonstrate that NeuroCERIL can learn various procedural skills in a simulated robotic imitation learning domain. We also show that its causal reasoning procedure is computationally efficient, and that its memory use is dominated by highly transient short-term memories, much like human working memory. We conclude that NeuroCERIL is a viable neural model of human-like imitation learning that can improve human-robot collaboration and contribute to investigations of the neurocomputational basis of human cognition.

</details>

<details>

<summary>2022-11-12 14:19:26 - Approximation algorithms for Node-weighted Steiner Problems: Digraphs with Additive Prizes and Graphs with Submodular Prizes</summary>

- *Gianlorenzo D'Angelo, Esmaeil Delfaraz*

- `2211.03653v2` - [abs](http://arxiv.org/abs/2211.03653v2) - [pdf](http://arxiv.org/pdf/2211.03653v2)

> In the \emph{budgeted rooted node-weighted Steiner tree} problem, we are given a graph $G$ with $n$ nodes, a predefined node $r$, two weights associated to each node modelling costs and prizes. The aim is to find a tree in $G$ rooted at $r$ such that the total cost of its nodes is at most a given budget $B$ and the total prize is maximized. In the \emph{quota rooted node-weighted Steiner tree} problem, we are given a real-valued quota $Q$, instead of the budget, and we aim at minimizing the cost of a tree rooted at $r$ whose overall prize is at least $Q$.   For the case of directed graphs with additive prize function, we develop a technique resorting on a standard flow-based linear programming relaxation to compute a tree with good trade-off between prize and cost, which allows us to provide very simple polynomial time approximation algorithms for both the budgeted and the quota problems. For the \emph{budgeted} problem, our algorithm achieves a bicriteria $(1+\epsilon, O(\frac{1}{\epsilon^2}n^{2/3}\ln{n}))$-approximation, for any $\epsilon \in (0, 1]$. For the \emph{quota} problem, our algorithm guarantees a bicriteria approximation factor of $(2, O(n^{2/3}\ln{n}))$. Next, by using the flow-based LP, we provide a surprisingly simple polynomial time $O((1+\epsilon)\sqrt{n} \ln {n})$-approximation algorithm for the node-weighted version of the directed Steiner tree problem, for any $\epsilon>0$.   For the case of undirected graphs with monotone submodular prize functions over subsets of nodes, we provide a polynomial time $O(\frac{1}{\epsilon^3}\sqrt{n}\log{n})$-approximation algorithm for the budgeted problem that violates the budget constraint by a factor of at most $1+\epsilon$, for any $\epsilon \in (0, 1]$. Our technique allows us to provide a good approximation also for the quota problem.

</details>

<details>

<summary>2022-11-12 20:16:13 - The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches</summary>

- *Haoye Tian, Kui Liu, Yinghua Li, Abdoul Kader KaborÃ©, Anil Koyuncu, Andrew Habib, Li Li, Junhao Wen, Jacques Klein, TegawendÃ© F. BissyandÃ©*

- `2203.08912v2` - [abs](http://arxiv.org/abs/2203.08912v2) - [pdf](http://arxiv.org/pdf/2203.08912v2)

> A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.

</details>

<details>

<summary>2022-11-12 21:36:58 - Smoothness Analysis for Probabilistic Programs with Application to Optimised Variational Inference</summary>

- *Wonyeol Lee, Xavier Rival, Hongseok Yang*

- `2208.10530v3` - [abs](http://arxiv.org/abs/2208.10530v3) - [pdf](http://arxiv.org/pdf/2208.10530v3)

> We present a static analysis for discovering differentiable or more generally smooth parts of a given probabilistic program, and show how the analysis can be used to improve the pathwise gradient estimator, one of the most popular methods for posterior inference and model learning. Our improvement increases the scope of the estimator from differentiable models to non-differentiable ones without requiring manual intervention of the user; the improved estimator automatically identifies differentiable parts of a given probabilistic program using our static analysis, and applies the pathwise gradient estimator to the identified parts while using a more general but less efficient estimator, called score estimator, for the rest of the program. Our analysis has a surprisingly subtle soundness argument, partly due to the misbehaviours of some target smoothness properties when viewed from the perspective of program analysis designers. For instance, some smoothness properties are not preserved by function composition, and this makes it difficult to analyse sequential composition soundly without heavily sacrificing precision. We formulate five assumptions on a target smoothness property, prove the soundness of our analysis under those assumptions, and show that our leading examples satisfy these assumptions. We also show that by using information from our analysis instantiated for differentiability, our improved gradient estimator satisfies an important differentiability requirement and thus computes the correct estimate on average (i.e., returns an unbiased estimate) under a regularity condition. Our experiments with representative probabilistic programs in the Pyro language show that our static analysis is capable of identifying smooth parts of those programs accurately, and making our improved pathwise gradient estimator exploit all the opportunities for high performance in those programs.

</details>

<details>

<summary>2022-11-13 15:18:31 - Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application</summary>

- *Leijie Wu, Song Guo, Yaohong Ding, Junxiao Wang, Wenchao Xu, Richard Yida Xu, Jie Zhang*

- `2211.08543v1` - [abs](http://arxiv.org/abs/2211.08543v1) - [pdf](http://arxiv.org/pdf/2211.08543v1)

> Self-attention mechanisms, especially multi-head self-attention (MSA), have achieved great success in many fields such as computer vision and natural language processing. However, many existing vision transformer (ViT) works simply inherent transformer designs from NLP to adapt vision tasks, while ignoring the fundamental difference between ``how MSA works in image and language settings''. Language naturally contains highly semantic structures that are directly interpretable by humans. Its basic unit (word) is discrete without redundant information, which readily supports interpretable studies on MSA mechanisms of language transformer. In contrast, visual data exhibits a fundamentally different structure: Its basic unit (pixel) is a natural low-level representation with significant redundancies in the neighbourhood, which poses obvious challenges to the interpretability of MSA mechanism in ViT. In this paper, we introduce a typical image processing technique, i.e., scale-invariant feature transforms (SIFTs), which maps low-level representations into mid-level spaces, and annotates extensive discrete keypoints with semantically rich information. Next, we construct a weighted patch interrelation analysis based on SIFT keypoints to capture the attention patterns hidden in patches with different semantic concentrations Interestingly, we find this quantitative analysis is not only an effective complement to the interpretability of MSA mechanisms in ViT, but can also be applied to 1) spurious correlation discovery and ``prompting'' during model inference, 2) and guided model pre-training acceleration. Experimental results on both applications show significant advantages over baselines, demonstrating the efficacy of our method.

</details>

<details>

<summary>2022-11-13 15:59:17 - TorchOpt: An Efficient Library for Differentiable Optimization</summary>

- *Jie Ren, Xidong Feng, Bo Liu, Xuehai Pan, Yao Fu, Luo Mai, Yaodong Yang*

- `2211.06934v1` - [abs](http://arxiv.org/abs/2211.06934v1) - [pdf](http://arxiv.org/pdf/2211.06934v1)

> Recent years have witnessed the booming of various differentiable optimization algorithms. These algorithms exhibit different execution patterns, and their execution needs massive computational resources that go beyond a single CPU and GPU. Existing differentiable optimization libraries, however, cannot support efficient algorithm development and multi-CPU/GPU execution, making the development of differentiable optimization algorithms often cumbersome and expensive. This paper introduces TorchOpt, a PyTorch-based efficient library for differentiable optimization. TorchOpt provides a unified and expressive differentiable optimization programming abstraction. This abstraction allows users to efficiently declare and analyze various differentiable optimization programs with explicit gradients, implicit gradients, and zero-order gradients. TorchOpt further provides a high-performance distributed execution runtime. This runtime can fully parallelize computation-intensive differentiation operations (e.g. tensor tree flattening) on CPUs / GPUs and automatically distribute computation to distributed devices. Experimental results show that TorchOpt achieves $5.2\times$ training time speedup on an 8-GPU server. TorchOpt is available at: https://github.com/metaopt/torchopt/.

</details>

<details>

<summary>2022-11-13 17:57:07 - Ground Truth Inference for Weakly Supervised Entity Matching</summary>

- *Renzhi Wu, Alexander Bendeck, Xu Chu, Yeye He*

- `2211.06975v1` - [abs](http://arxiv.org/abs/2211.06975v1) - [pdf](http://arxiv.org/pdf/2211.06975v1)

> Entity matching (EM) refers to the problem of identifying pairs of data records in one or more relational tables that refer to the same entity in the real world. Supervised machine learning (ML) models currently achieve state-of-the-art matching performance; however, they require many labeled examples, which are often expensive or infeasible to obtain. This has inspired us to approach data labeling for EM using weak supervision. In particular, we use the labeling function abstraction popularized by Snorkel, where each labeling function (LF) is a user-provided program that can generate many noisy match/non-match labels quickly and cheaply. Given a set of user-written LFs, the quality of data labeling depends on a labeling model to accurately infer the ground-truth labels. In this work, we first propose a simple but powerful labeling model for general weak supervision tasks. Then, we tailor the labeling model specifically to the task of entity matching by considering the EM-specific transitivity property.   The general form of our labeling model is simple while substantially outperforming the best existing method across ten general weak supervision datasets. To tailor the labeling model for EM, we formulate an approach to ensure that the final predictions of the labeling model satisfy the transitivity property required in EM, utilizing an exact solution where possible and an ML-based approximation in remaining cases. On two single-table and nine two-table real-world EM datasets, we show that our labeling model results in a 9% higher F1 score on average than the best existing method. We also show that a deep learning EM end model (DeepMatcher) trained on labels generated from our weak supervision approach is comparable to an end model trained using tens of thousands of ground-truth labels, demonstrating that our approach can significantly reduce the labeling efforts required in EM.

</details>

<details>

<summary>2022-11-13 19:08:28 - Application of Explainable Machine Learning in Detecting and Classifying Ransomware Families Based on API Call Analysis</summary>

- *Rawshan Ara Mowri, Madhuri Siddula, Kaushik Roy*

- `2210.11235v3` - [abs](http://arxiv.org/abs/2210.11235v3) - [pdf](http://arxiv.org/pdf/2210.11235v3)

> Ransomware has appeared as one of the major global threats in recent days. The alarming increasing rate of ransomware attacks and new ransomware variants intrigue the researchers to constantly examine the distinguishing traits of ransomware and refine their detection strategies. Application Programming Interface (API) is a way for one program to collaborate with another; API calls are the medium by which they communicate. Ransomware uses this strategy to interact with the OS and makes a significantly higher number of calls in different sequences to ask for taking action. This research work utilizes the frequencies of different API calls to detect and classify ransomware families. First, a Web-Crawler is developed to automate collecting the Windows Portable Executable (PE) files of 15 different ransomware families. By extracting different frequencies of 68 API calls, we develop our dataset in the first phase of the two-phase feature engineering process. After selecting the most significant features in the second phase of the feature engineering process, we deploy six Supervised Machine Learning models: Na"ive Bayes, Logistic Regression, Random Forest, Stochastic Gradient Descent, K-Nearest Neighbor, and Support Vector Machine. Then, the performances of all the classifiers are compared to select the best model. The results reveal that Logistic Regression can efficiently classify ransomware into their corresponding families securing 99.15% overall accuracy. Finally, instead of relying on the 'Black box' characteristic of the Machine Learning models, we present the post-hoc analysis of our best-performing model using 'SHapley Additive exPlanations' or SHAP values to ascertain the transparency and trustworthiness of the model's prediction.

</details>

<details>

<summary>2022-11-14 02:51:04 - Learning predictive checklists from continuous medical data</summary>

- *Yukti Makhija, Edward De Brouwer, Rahul G. Krishnan*

- `2211.07076v1` - [abs](http://arxiv.org/abs/2211.07076v1) - [pdf](http://arxiv.org/pdf/2211.07076v1)

> Checklists, while being only recently introduced in the medical domain, have become highly popular in daily clinical practice due to their combined effectiveness and great interpretability. Checklists are usually designed by expert clinicians that manually collect and analyze available evidence. However, the increasing quantity of available medical data is calling for a partially automated checklist design. Recent works have taken a step in that direction by learning predictive checklists from categorical data. In this work, we propose to extend this approach to accomodate learning checklists from continuous medical data using mixed-integer programming approach. We show that this extension outperforms a range of explainable machine learning baselines on the prediction of sepsis from intensive care clinical trajectories.

</details>

<details>

<summary>2022-11-14 06:27:29 - Cerberus: A Formal Approach to Secure and Efficient Enclave Memory Sharing</summary>

- *Dayeol Lee, Kevin Cheang, Alexander Thomas, Catherine Lu, Pranav Gaddamadugu, Anjo Vahldiek-Oberwagner, Mona Vij, Dawn Song, Sanjit A. Seshia, Krste AsanoviÄ*

- `2209.15253v2` - [abs](http://arxiv.org/abs/2209.15253v2) - [pdf](http://arxiv.org/pdf/2209.15253v2)

> Hardware enclaves rely on a disjoint memory model, which maps each physical address to an enclave to achieve strong memory isolation. However, this severely limits the performance and programmability of enclave programs. While some prior work proposes enclave memory sharing, it does not provide a formal model or verification of their designs. This paper presents Cerberus, a formal approach to secure and efficient enclave memory sharing. To reduce the burden of formal verification, we compare different sharing models and choose a simple yet powerful sharing model. Based on the sharing model, Cerberus extends an enclave platform such that enclave memory can be made immutable and shareable across multiple enclaves via additional operations. We use incremental verification starting with an existing formal model called the Trusted Abstract Platform (TAP). Using our extended TAP model, we formally verify that Cerberus does not break or weaken the security guarantees of the enclaves despite allowing memory sharing. More specifically, we prove the Secure Remote Execution (SRE) property on our formal model. Finally, the paper shows the feasibility of Cerberus by implementing it in an existing enclave platform, RISC-V Keystone.

</details>

<details>

<summary>2022-11-14 07:02:50 - Multi-modal Masked Autoencoders Learn Compositional Histopathological Representations</summary>

- *Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Linda Shapiro*

- `2209.01534v2` - [abs](http://arxiv.org/abs/2209.01534v2) - [pdf](http://arxiv.org/pdf/2209.01534v2)

> Self-supervised learning (SSL) enables learning useful inductive biases through utilizing pretext tasks that require no labels. The unlabeled nature of SSL makes it especially important for whole slide histopathological images (WSIs), where patch-level human annotation is difficult. Masked Autoencoders (MAE) is a recent SSL method suitable for digital pathology as it does not require negative sampling and requires little to no data augmentations. However, the domain shift between natural images and digital pathology images requires further research in designing MAE for patch-level WSIs. In this paper, we investigate several design choices for MAE in histopathology. Furthermore, we introduce a multi-modal MAE (MMAE) that leverages the specific compositionality of Hematoxylin & Eosin (H&E) stained WSIs. We performed our experiments on the public patch-level dataset NCT-CRC-HE-100K. The results show that the MMAE architecture outperforms supervised baselines and other state-of-the-art SSL techniques for an eight-class tissue phenotyping task, utilizing only 100 labeled samples for fine-tuning. Our code is available at https://github.com/wisdomikezogwo/MMAE_Pathology

</details>

<details>

<summary>2022-11-14 08:49:24 - SUNDEW: An Ensemble of Predictors for Case-Sensitive Detection of Malware</summary>

- *Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, Kamakoti V*

- `2211.06153v2` - [abs](http://arxiv.org/abs/2211.06153v2) - [pdf](http://arxiv.org/pdf/2211.06153v2)

> Malware programs are diverse, with varying objectives, functionalities, and threat levels ranging from mere pop-ups to financial losses. Consequently, their run-time footprints across the system differ, impacting the optimal data source (Network, Operating system (OS), Hardware) and features that are instrumental to malware detection. Further, the variations in threat levels of malware classes affect the user requirements for detection. Thus, the optimal tuple of <data-source, features, user-requirements> is different for each malware class, impacting the state-of-the-art detection solutions that are agnostic to these subtle differences.   This paper presents SUNDEW, a framework to detect malware classes using their optimal tuple of <data-source, features, user-requirements>. SUNDEW uses an ensemble of specialized predictors, each trained with a particular data source (network, OS, and hardware) and tuned for features and requirements of a specific class. While the specialized ensemble with a holistic view across the system improves detection, aggregating the independent conflicting inferences from the different predictors is challenging. SUNDEW resolves such conflicts with a hierarchical aggregation considering the threat-level, noise in the data sources, and prior domain knowledge. We evaluate SUNDEW on a real-world dataset of over 10,000 malware samples from 8 classes. It achieves an F1-Score of one for most classes, with an average of 0.93 and a limited performance overhead of 1.5%.

</details>

<details>

<summary>2022-11-14 08:50:40 - Unique in the Smart Grid -The Privacy Cost of Fine-Grained Electrical Consumption Data</summary>

- *Antonin Voyez, Tristan Allard, Gildas Avoine, Pierre Cauchois, Elisa Fromont, Matthieu Simonin*

- `2211.07205v1` - [abs](http://arxiv.org/abs/2211.07205v1) - [pdf](http://arxiv.org/pdf/2211.07205v1)

> The collection of electrical consumption time series through smart meters grows with ambitious nationwide smart grid programs. This data is both highly sensitive and highly valuable: strong laws about personal data protect it while laws about open data aim at making it public after a privacy-preserving data publishing process. In this work, we study the uniqueness of large scale real-life fine-grained electrical consumption time-series and show its link to privacy threats. Our results show a worryingly high uniqueness rate in such datasets. In particular, we show that knowing 5 consecutive electric measures allows to re-identify on average more than 90% of households in our 2.5M half-hourly electric time series dataset. Moreover, uniqueness remains high even when data is severely degraded. For example, when data is rounded to the nearest 100 watts, knowing 7 consecutive electric measures allows to re-identify on average more than 40% of the households (same dataset). We also study the relationship between uniqueness and entropy, uniqueness and electric consumption, and electric consumption and temperatures, showing their strong correlation.

</details>

<details>

<summary>2022-11-14 11:17:46 - Robustifying Deep Vision Models Through Shape Sensitization</summary>

- *Aditay Tripathi, Rishubh Singh, Anirban Chakraborty, Pradeep Shenoy*

- `2211.07277v1` - [abs](http://arxiv.org/abs/2211.07277v1) - [pdf](http://arxiv.org/pdf/2211.07277v1)

> Recent work has shown that deep vision models tend to be overly dependent on low-level or "texture" features, leading to poor generalization. Various data augmentation strategies have been proposed to overcome this so-called texture bias in DNNs. We propose a simple, lightweight adversarial augmentation technique that explicitly incentivizes the network to learn holistic shapes for accurate prediction in an object classification setting. Our augmentations superpose edgemaps from one image onto another image with shuffled patches, using a randomly determined mixing proportion, with the image label of the edgemap image. To classify these augmented images, the model needs to not only detect and focus on edges but distinguish between relevant and spurious edges. We show that our augmentations significantly improve classification accuracy and robustness measures on a range of datasets and neural architectures. As an example, for ViT-S, We obtain absolute gains on classification accuracy gains up to 6%. We also obtain gains of up to 28% and 8.5% on natural adversarial and out-of-distribution datasets like ImageNet-A (for ViT-B) and ImageNet-R (for ViT-S), respectively. Analysis using a range of probe datasets shows substantially increased shape sensitivity in our trained models, explaining the observed improvement in robustness and classification accuracy.

</details>

<details>

<summary>2022-11-14 14:56:27 - Seeded iterative clustering for histology region identification</summary>

- *Eduard Chelebian, Francesco Ciompi, Carolina WÃ¤hlby*

- `2211.07425v1` - [abs](http://arxiv.org/abs/2211.07425v1) - [pdf](http://arxiv.org/pdf/2211.07425v1)

> Annotations are necessary to develop computer vision algorithms for histopathology, but dense annotations at a high resolution are often time-consuming to make. Deep learning models for segmentation are a way to alleviate the process, but require large amounts of training data, training times and computing power. To address these issues, we present seeded iterative clustering to produce a coarse segmentation densely and at the whole slide level. The algorithm uses precomputed representations as the clustering space and a limited amount of sparse interactive annotations as seeds to iteratively classify image patches. We obtain a fast and effective way of generating dense annotations for whole slide images and a framework that allows the comparison of neural network latent representations in the context of transfer learning.

</details>

<details>

<summary>2022-11-14 16:49:54 - Neural Set Function Extensions: Learning with Discrete Functions in High Dimensions</summary>

- *Nikolaos Karalias, Joshua Robinson, Andreas Loukas, Stefanie Jegelka*

- `2208.04055v2` - [abs](http://arxiv.org/abs/2208.04055v2) - [pdf](http://arxiv.org/pdf/2208.04055v2)

> Integrating functions on discrete domains into neural networks is key to developing their capability to reason about discrete objects. But, discrete domains are (1) not naturally amenable to gradient-based optimization, and (2) incompatible with deep learning architectures that rely on representations in high-dimensional vector spaces. In this work, we address both difficulties for set functions, which capture many important discrete problems. First, we develop a framework for extending set functions onto low-dimensional continuous domains, where many extensions are naturally defined. Our framework subsumes many well-known extensions as special cases. Second, to avoid undesirable low-dimensional neural network bottlenecks, we convert low-dimensional extensions into representations in high-dimensional spaces, taking inspiration from the success of semidefinite programs for combinatorial optimization. Empirically, we observe benefits of our extensions for unsupervised neural combinatorial optimization, in particular with high-dimensional representations.

</details>

<details>

<summary>2022-11-14 16:52:32 - Towards a Mathematics Formalisation Assistant using Large Language Models</summary>

- *Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni Narayanan, Anand Tadipatri*

- `2211.07524v1` - [abs](http://arxiv.org/abs/2211.07524v1) - [pdf](http://arxiv.org/pdf/2211.07524v1)

> Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful input-dependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75\% accuracy for $120$ theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of $13$ theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.

</details>

<details>

<summary>2022-11-14 17:42:28 - Quantum Computing for Software Engineering: Prospects</summary>

- *Andriy Miranskyy, Mushahid Khan, Jean Paul Latyr Faye, Udson C. Mendes*

- `2203.03575v3` - [abs](http://arxiv.org/abs/2203.03575v3) - [pdf](http://arxiv.org/pdf/2203.03575v3)

> Quantum computers (QCs) are maturing. When QCs are powerful enough, they may be able to handle problems in chemistry, physics, and finance that are not classically solvable. However, the applicability of quantum algorithms to speed up Software Engineering (SE) tasks has not been explored. We examine eight groups of quantum algorithms that may accelerate SE tasks across the different phases of SE and sketch potential opportunities and challenges.

</details>

<details>

<summary>2022-11-14 20:18:47 - (De-)Randomized Smoothing for Decision Stump Ensembles</summary>

- *MiklÃ³s Z. HorvÃ¡th, Mark Niklas MÃ¼ller, Marc Fischer, Martin Vechev*

- `2205.13909v2` - [abs](http://arxiv.org/abs/2205.13909v2) - [pdf](http://arxiv.org/pdf/2205.13909v2)

> Tree-based models are used in many high-stakes application domains such as finance and medicine, where robustness and interpretability are of utmost importance. Yet, methods for improving and certifying their robustness are severely under-explored, in contrast to those focusing on neural networks. Targeting this important challenge, we propose deterministic smoothing for decision stump ensembles. Whereas most prior work on randomized smoothing focuses on evaluating arbitrary base models approximately under input randomization, the key insight of our work is that decision stump ensembles enable exact yet efficient evaluation via dynamic programming. Importantly, we obtain deterministic robustness certificates, even jointly over numerical and categorical features, a setting ubiquitous in the real world. Further, we derive an MLE-optimal training method for smoothed decision stumps under randomization and propose two boosting approaches to improve their provable robustness. An extensive experimental evaluation on computer vision and tabular data tasks shows that our approach yields significantly higher certified accuracies than the state-of-the-art for tree-based models. We release all code and trained models at https://github.com/eth-sri/drs.

</details>

<details>

<summary>2022-11-14 22:34:14 - An approach for Test Impact Analysis on the Integration Level in Java programs</summary>

- *Muzammil Shahbaz*

- `2211.07782v1` - [abs](http://arxiv.org/abs/2211.07782v1) - [pdf](http://arxiv.org/pdf/2211.07782v1)

> Test Impact Analysis is an approach to obtain a subset of tests impacted by code changes. This approach is mainly applied to unit testing where the link between the code and its associated tests is easy to obtain. On the integration level, however, it is not straightforward to find such a link programmatically, especially when the integration tests are held into separate repositories. We propose an approach for selecting integration tests based on the runtime analysis of code changes to reduce the test execution overhead. We provide a set of tools and a framework that can be plugged into existing CI/CD pipelines. We have evaluated the approach on a range of open-source Java programs and found $\approx$50\% reduction in tests on average, and above 80\% in a few cases. We have also applied the approach to a large-scale commercial system in production and found similar results.

</details>

<details>

<summary>2022-11-15 01:53:20 - Evaluating How Fine-tuning on Bimodal Data Effects Code Generation</summary>

- *Gabriel Orlanski, Seonhye Yang, Michael Healy*

- `2211.07842v1` - [abs](http://arxiv.org/abs/2211.07842v1) - [pdf](http://arxiv.org/pdf/2211.07842v1)

> Despite the increase in popularity of language models for code generation, it is still unknown how training on bimodal coding forums affects a model's code generation performance and reliability. We, therefore, collect a dataset of over 2.2M StackOverflow questions with answers for finetuning. These fine-tuned models have average $pass@k$ improvements of 54.64% and 85.35% on the HumanEval (Chen et al., 2021) and Mostly Basic Program Problems (Austin et al., 2021) tasks, respectively. This regime further decreases the number of generated programs with both syntax and runtime errors. However, we find that at higher temperatures, there are significant decreases to the model's ability to generate runnable programs despite higher $pass@k$ scores, underscoring the need for better methods of incorporating such data that mitigate these side effects. The code can be found https://github.com/gabeorlanski/bimodalcode-generation

</details>

<details>

<summary>2022-11-15 03:42:52 - Giving RSEs a Larger Stage through the Better Scientific Software Fellowship</summary>

- *William F. Godoy, Ritu Arora, Keith Beattie, David E. Bernholdt, Sarah E. Bratt, Daniel S. Katz, Ignacio Laguna, Amiya K. Maji, Addi Malviya Thakur, Rafael M. Mudafort, Nitin Sukhija, Damian Rouson, Cindy Rubio-GonzÃ¡lez, Karan Vahi*

- `2211.07436v2` - [abs](http://arxiv.org/abs/2211.07436v2) - [pdf](http://arxiv.org/pdf/2211.07436v2)

> The Better Scientific Software Fellowship (BSSwF) was launched in 2018 to foster and promote practices, processes, and tools to improve developer productivity and software sustainability of scientific codes. BSSwF's vision is to grow the community with practitioners, leaders, mentors, and consultants to increase the visibility of scientific software production and sustainability. Over the last five years, many fellowship recipients and honorable mentions have identified as research software engineers (RSEs). This paper provides case studies from several of the program's participants to illustrate some of the diverse ways BSSwF has benefited both the RSE and scientific communities. In an environment where the contributions of RSEs are too often undervalued, we believe that programs such as BSSwF can be a valuable means to recognize and encourage community members to step outside of their regular commitments and expand on their work, collaborations and ideas for a larger audience.

</details>

<details>

<summary>2022-11-15 14:23:47 - HSVI for zs-POSGs using Concavity, Convexity and Lipschitz Properties</summary>

- *AurÃ©lien Delage, Olivier Buffet, Jilles Dibangoye*

- `2110.14529v2` - [abs](http://arxiv.org/abs/2110.14529v2) - [pdf](http://arxiv.org/pdf/2110.14529v2)

> Dynamic programming and heuristic search are at the core of state-of-the-art solvers for sequential decision-making problems. In partially observable or collaborative settings (\eg, POMDPs and Dec-POMDPs), this requires introducing an appropriate statistic that induces a fully observable problem as well as bounding (convex) approximators of the optimal value function. This approach has succeeded in some subclasses of 2-player zero-sum partially observable stochastic games (zs-POSGs) as well, but failed in the general case despite known concavity and convexity properties, which only led to heuristic algorithms with poor convergence guarantees. We overcome this issue, leveraging on these properties to derive bounding approximators and efficient update and selection operators, before deriving a prototypical solver inspired by HSVI that provably converges to an $\epsilon$-optimal solution in finite time, and which we empirically evaluate. This opens the door to a novel family of promising approaches complementing those relying on linear programming or iterative methods.

</details>

<details>

<summary>2022-11-15 15:58:34 - Scene-to-Patch Earth Observation: Multiple Instance Learning for Land Cover Classification</summary>

- *Joseph Early, Ying-Jung Deweese, Christine Evers, Sarvapali Ramchurn*

- `2211.08247v1` - [abs](http://arxiv.org/abs/2211.08247v1) - [pdf](http://arxiv.org/pdf/2211.08247v1)

> Land cover classification (LCC), and monitoring how land use changes over time, is an important process in climate change mitigation and adaptation. Existing approaches that use machine learning with Earth observation data for LCC rely on fully-annotated and segmented datasets. Creating these datasets requires a large amount of effort, and a lack of suitable datasets has become an obstacle in scaling the use of LCC. In this study, we propose Scene-to-Patch models: an alternative LCC approach utilising Multiple Instance Learning (MIL) that requires only high-level scene labels. This enables much faster development of new datasets whilst still providing segmentation through patch-level predictions, ultimately increasing the accessibility of using LCC for different scenarios. On the DeepGlobe-LCC dataset, our approach outperforms non-MIL baselines on both scene- and patch-level prediction. This work provides the foundation for expanding the use of LCC in climate change mitigation methods for technology, government, and academia.

</details>

<details>

<summary>2022-11-15 17:50:17 - FOON Creation and Traversal for Recipe Generation</summary>

- *Raj Patel*

- `2210.07335v2` - [abs](http://arxiv.org/abs/2210.07335v2) - [pdf](http://arxiv.org/pdf/2210.07335v2)

> Task competition by robots is still off from being completely dependable and usable. One way a robot may decipher information given to it and accomplish tasks is by utilizing FOON, which stands for functional object-oriented network. The network first needs to be created by having a human creates action nodes as well as input and output nodes in a .txt file. After the network is sizeable, utilization of this network allows for traversal of the network in a variety of ways such as choosing steps via iterative deepening searching by using the first seen valid option. Another mechanism is heuristics, such as choosing steps based on the highest success rate or lowest amount of input ingredients. Via any of these methods, a program can traverse the network given an output product, and derive the series of steps that need to be taken to produce the output.

</details>

<details>

<summary>2022-11-15 17:57:12 - Probabilistic Deep Metric Learning for Hyperspectral Image Classification</summary>

- *Chengkun Wang, Wenzhao Zheng, Xian Sun, Jiwen Lu, Jie Zhou*

- `2211.08349v1` - [abs](http://arxiv.org/abs/2211.08349v1) - [pdf](http://arxiv.org/pdf/2211.08349v1)

> This paper proposes a probabilistic deep metric learning (PDML) framework for hyperspectral image classification, which aims to predict the category of each pixel for an image captured by hyperspectral sensors. The core problem for hyperspectral image classification is the spectral variability between intraclass materials and the spectral similarity between interclass materials, motivating the further incorporation of spatial information to differentiate a pixel based on its surrounding patch. However, different pixels and even the same pixel in one patch might not encode the same material due to the low spatial resolution of most hyperspectral sensors, leading to an inconsistent judgment of a specific pixel. To address this issue, we propose a probabilistic deep metric learning framework to model the categorical uncertainty of the spectral distribution of an observed pixel. We propose to learn a global probabilistic distribution for each pixel in the patch and a probabilistic metric to model the distance between distributions. We treat each pixel in a patch as a training sample, enabling us to exploit more information from the patch compared with conventional methods. Our framework can be readily applied to existing hyperspectral image classification methods with various network architectures and loss functions. Extensive experiments on four widely used datasets including IN, UP, KSC, and Houston 2013 datasets demonstrate that our framework improves the performance of existing methods and further achieves the state of the art. Code is available at: https://github.com/wzzheng/PDML.

</details>

<details>

<summary>2022-11-15 18:05:48 - Improving AFL++ CmpLog: Tackling the bottlenecks</summary>

- *Sander Wiebing, Thomas Rooijakkers, Sebastiaan Tesink*

- `2211.08357v1` - [abs](http://arxiv.org/abs/2211.08357v1) - [pdf](http://arxiv.org/pdf/2211.08357v1)

> The performance of the AFL++ CmpLog feature varies considerably for specific programs under test (PUTs). In this paper it is demonstrated that the main cause of the poor performance is low seed entropy, and a lack of deduplication of magic bytes candidates. An improvement is proposed by mapping comparisons to input bytes, in order to track which comparisons are controlled by what input bytes. This mapping is then used to fuzz only the comparison values that are magic byte candidates for that input part. Second, a caching mechanism is introduced to reduce the number of redundant executions. The evaluation of the improved versions shows a significant coverage gain compared to the original AFL++ implementation of CmpLog for all PUTs, without breaking functionality. The proposed solution in this paper provides a solid basis for a redesign of CmpLog.

</details>

<details>

<summary>2022-11-15 18:27:15 - Optimizing Polymatroid Functions</summary>

- *Sungjin Im, Benjamin Moseley, Hung Q. Ngo, Kirk Pruhs, Alireza Samadian*

- `2211.08381v1` - [abs](http://arxiv.org/abs/2211.08381v1) - [pdf](http://arxiv.org/pdf/2211.08381v1)

> We consider a class of optimization problems that involve determining the maximum value that a function in a particular class can attain subject to a collection of difference constraints. We show that a particular linear programming technique, based on duality and projections, can be used to rederive some structural results that were previously established using more ad hoc methods. We then show that this technique can be used to obtain a polynomial-time algorithm for a certain type of simple difference constraints. Finally we give lower bound results that show that certain possible extensions of these results are probably not feasible.

</details>

<details>

<summary>2022-11-15 21:21:27 - A Hierarchical Deep Neural Network for Detecting Lines of Codes with Vulnerabilities</summary>

- *Arash Mahyari*

- `2211.08517v1` - [abs](http://arxiv.org/abs/2211.08517v1) - [pdf](http://arxiv.org/pdf/2211.08517v1)

> Software vulnerabilities, caused by unintentional flaws in source codes, are the main root cause of cyberattacks. Source code static analysis has been used extensively to detect the unintentional defects, i.e. vulnerabilities, introduced into the source codes by software developers. In this paper, we propose a deep learning approach to detect vulnerabilities from their LLVM IR representations based on the techniques that have been used in natural language processing. The proposed approach uses a hierarchical process to first identify source codes with vulnerabilities, and then it identifies the lines of codes that contribute to the vulnerability within the detected source codes. This proposed two-step approach reduces the false alarm of detecting vulnerable lines. Our extensive experiment on real-world and synthetic codes collected in NVD and SARD shows high accuracy (about 98\%) in detecting source code vulnerabilities.

</details>

<details>

<summary>2022-11-15 22:19:50 - Towards a Catalog of Composite Refactorings</summary>

- *Aline Brito, Andre Hora, Marco Tulio Valente*

- `2201.04599v3` - [abs](http://arxiv.org/abs/2201.04599v3) - [pdf](http://arxiv.org/pdf/2201.04599v3)

> Catalogs of refactoring have key importance in software maintenance and evolution, since developers rely on such documents to understand and perform refactoring operations. Furthermore, these catalogs constitute a reference guide for communication between practitioners since they standardize a common refactoring vocabulary. Fowler's book describes the most popular catalog of refactorings, which documents single and well-known refactoring operations. However, sometimes refactorings are composite transformations, i.e., a sequence of refactorings is performed over a given program element. For example, a sequence of Extract Method operations (a single refactoring) can be performed over the same method, in one or in multiple commits, to simplify its implementation, therefore, leading to a Method Decomposition operation (a composite refactoring). In this paper, we propose and document a catalog with eight composite refactorings. We also implement a set of scripts to mine composite refactorings by preprocessing the results of refactoring detection tools. Using such scripts, we search for composites in a representative refactoring oracle with hundreds of confirmed single refactoring operations. Next, to complement this first study, we also search for composites in the full history of ten well-known open-source projects. We characterize the detected composite refactorings, under dimensions such as size and location. We conclude by addressing the applications and implications of the proposed catalog.

</details>

<details>

<summary>2022-11-15 23:21:07 - Detection and Prediction of Nutrient Deficiency Stress using Longitudinal Aerial Imagery</summary>

- *Saba Dadsetan, Gisele Rose, Naira Hovakimyan, Jennifer Hobbs*

- `2012.09654v2` - [abs](http://arxiv.org/abs/2012.09654v2) - [pdf](http://arxiv.org/pdf/2012.09654v2)

> Early, precise detection of nutrient deficiency stress (NDS) has key economic as well as environmental impact; precision application of chemicals in place of blanket application reduces operational costs for the growers while reducing the amount of chemicals which may enter the environment unnecessarily. Furthermore, earlier treatment reduces the amount of loss and therefore boosts crop production during a given season. With this in mind, we collect sequences of high-resolution aerial imagery and construct semantic segmentation models to detect and predict NDS across the field. Our work sits at the intersection of agriculture, remote sensing, and modern computer vision and deep learning. First, we establish a baseline for full-field detection of NDS and quantify the impact of pretraining, backbone architecture, input representation, and sampling strategy. We then quantify the amount of information available at different points in the season by building a single-timestamp model based on a UNet. Next, we construct our proposed spatiotemporal architecture, which combines a UNet with a convolutional LSTM layer, to accurately detect regions of the field showing NDS; this approach has an impressive IOU score of 0.53. Finally, we show that this architecture can be trained to predict regions of the field which are expected to show NDS in a later flight -- potentially more than three weeks in the future -- maintaining an IOU score of 0.47-0.51 depending on how far in advance the prediction is made. We will also release a dataset which we believe will benefit the computer vision, remote sensing, as well as agriculture fields. This work contributes to the recent developments in deep learning for remote sensing and agriculture, while addressing a key social challenge with implications for economics and sustainability.

</details>

<details>

<summary>2022-11-15 23:27:59 - Superpixels and Graph Convolutional Neural Networks for Efficient Detection of Nutrient Deficiency Stress from Aerial Imagery</summary>

- *Saba Dadsetan, David Pichler, David Wilson, Naira Hovakimyan, Jennifer Hobbs*

- `2104.10249v3` - [abs](http://arxiv.org/abs/2104.10249v3) - [pdf](http://arxiv.org/pdf/2104.10249v3)

> Advances in remote sensing technology have led to the capture of massive amounts of data. Increased image resolution, more frequent revisit times, and additional spectral channels have created an explosion in the amount of data that is available to provide analyses and intelligence across domains, including agriculture. However, the processing of this data comes with a cost in terms of computation time and money, both of which must be considered when the goal of an algorithm is to provide real-time intelligence to improve efficiencies. Specifically, we seek to identify nutrient deficient areas from remotely sensed data to alert farmers to regions that require attention; detection of nutrient deficient areas is a key task in precision agriculture as farmers must quickly respond to struggling areas to protect their harvests. Past methods have focused on pixel-level classification (i.e. semantic segmentation) of the field to achieve these tasks, often using deep learning models with tens-of-millions of parameters. In contrast, we propose a much lighter graph-based method to perform node-based classification. We first use Simple Linear Iterative Cluster (SLIC) to produce superpixels across the field. Then, to perform segmentation across the non-Euclidean domain of superpixels, we leverage a Graph Convolutional Neural Network (GCN). This model has 4-orders-of-magnitude fewer parameters than a CNN model and trains in a matter of minutes.

</details>

<details>

<summary>2022-11-16 00:02:10 - Conflict-Aware Pseudo Labeling via Optimal Transport for Entity Alignment</summary>

- *Qijie Ding, Daokun Zhang, Jie Yin*

- `2209.01847v2` - [abs](http://arxiv.org/abs/2209.01847v2) - [pdf](http://arxiv.org/pdf/2209.01847v2)

> Entity alignment aims to discover unique equivalent entity pairs with the same meaning across different knowledge graphs (KGs). Existing models have focused on projecting KGs into a latent embedding space so that inherent semantics between entities can be captured for entity alignment. However, the adverse impacts of alignment conflicts have been largely overlooked during training, thereby limiting the entity alignment performance. To address this issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport model (CPL-OT) for entity alignment. The key idea is to iteratively pseudo-label alignment pairs empowered with conflict-aware optimal transport (OT) modeling to boost the precision of entity alignment. CPL-OT is composed of two key components -- entity embedding learning with global-local aggregation and iterative conflict-aware pseudo labeling -- that mutually reinforce each other. To mitigate alignment conflicts during pseudo labeling, we propose to use optimal transport as an effective means to warrant one-to-one entity alignment between two KGs with the minimal overall transport cost. Extensive experiments on benchmark datasets validate the superiority of CPL-OT over state-of-the-art baselines under both settings with and without prior alignment seeds.

</details>

<details>

<summary>2022-11-16 03:08:15 - Dwelling Type Classification for Disaster Risk Assessment Using Satellite Imagery</summary>

- *Md Nasir, Tina Sederholm, Anshu Sharma, Sundeep Reddy Mallu, Sumedh Ranjan Ghatage, Rahul Dodhia, Juan Lavista Ferres*

- `2211.11636v1` - [abs](http://arxiv.org/abs/2211.11636v1) - [pdf](http://arxiv.org/pdf/2211.11636v1)

> Vulnerability and risk assessment of neighborhoods is essential for effective disaster preparedness. Existing traditional systems, due to dependency on time-consuming and cost-intensive field surveying, do not provide a scalable way to decipher warnings and assess the precise extent of the risk at a hyper-local level. In this work, machine learning was used to automate the process of identifying dwellings and their type to build a potentially more effective disaster vulnerability assessment system. First, satellite imageries of low-income settlements and vulnerable areas in India were used to identify 7 different dwelling types. Specifically, we formulated the dwelling type classification as a semantic segmentation task and trained a U-net based neural network model, namely TernausNet, with the data we collected. Then a risk score assessment model was employed, using the determined dwelling type along with an inundation model of the regions. The entire pipeline was deployed to multiple locations prior to natural hazards in India in 2020. Post hoc ground-truth data from those regions was collected to validate the efficacy of this model which showed promising performance. This work can aid disaster response organizations and communities at risk by providing household-level risk information that can inform preemptive actions.

</details>

<details>

<summary>2022-11-16 05:18:42 - Nano-Resolution Visual Identifiers Enable Secure Monitoring in Next-Generation Cyber-Physical Systems</summary>

- *Hao Wang, Xiwen Chen, Abolfazl Razi, Michael Kozicki, Rahul Amin, Mark Manfredo*

- `2211.08678v1` - [abs](http://arxiv.org/abs/2211.08678v1) - [pdf](http://arxiv.org/pdf/2211.08678v1)

> Today's supply chains heavily rely on cyber-physical systems such as intelligent transportation, online shopping, and E-commerce. It is advantageous to track goods in real-time by web-based registration and authentication of products after any substantial change or relocation. Despite recent advantages in technology-based tracking systems, most supply chains still rely on plainly printed tags such as barcodes and Quick Response (QR) codes for tracking purposes. Although affordable and efficient, these tags convey no security against counterfeit and cloning attacks, raising privacy concerns. It is a critical matter since a few security breaches in merchandise databases in recent years has caused crucial social and economic impacts such as identity loss, social panic, and loss of trust in the community. This paper considers an end-to-end system using dendrites as nano-resolution visual identifiers to secure supply chains. Dendrites are formed by generating fractal metallic patterns on transparent substrates through an electrochemical process, which can be used as secure identifiers due to their natural randomness, high entropy, and unclonable features. The proposed framework compromises the back-end program for identification and authentication, a web-based application for mobile devices, and a cloud database. We review architectural design, dendrite operational phases (personalization, registration, inspection), a lightweight identification method based on 2D graph-matching, and a deep 3D image authentication method based on Digital Holography (DH). A two-step search is proposed to make the system scalable by limiting the search space to samples with high similarity scores in a lower-dimensional space. We conclude by presenting our solution to make dendrites secure against adversarial attacks.

</details>

<details>

<summary>2022-11-16 06:28:20 - Interpretable Self-Aware Neural Networks for Robust Trajectory Prediction</summary>

- *Masha Itkina, Mykel J. Kochenderfer*

- `2211.08701v1` - [abs](http://arxiv.org/abs/2211.08701v1) - [pdf](http://arxiv.org/pdf/2211.08701v1)

> Although neural networks have seen tremendous success as predictive models in a variety of domains, they can be overly confident in their predictions on out-of-distribution (OOD) data. To be viable for safety-critical applications, like autonomous vehicles, neural networks must accurately estimate their epistemic or model uncertainty, achieving a level of system self-awareness. Techniques for epistemic uncertainty quantification often require OOD data during training or multiple neural network forward passes during inference. These approaches may not be suitable for real-time performance on high-dimensional inputs. Furthermore, existing methods lack interpretability of the estimated uncertainty, which limits their usefulness both to engineers for further system development and to downstream modules in the autonomy stack. We propose the use of evidential deep learning to estimate the epistemic uncertainty over a low-dimensional, interpretable latent space in a trajectory prediction setting. We introduce an interpretable paradigm for trajectory prediction that distributes the uncertainty among the semantic concepts: past agent behavior, road structure, and social context. We validate our approach on real-world autonomous driving data, demonstrating superior performance over state-of-the-art baselines. Our code is available at: https://github.com/sisl/InterpretableSelfAwarePrediction.

</details>

<details>

<summary>2022-11-16 08:30:05 - Advanced Situational Graphs for Robot Navigation in Structured Indoor Environments</summary>

- *Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos*

- `2211.08754v1` - [abs](http://arxiv.org/abs/2211.08754v1) - [pdf](http://arxiv.org/pdf/2211.08754v1)

> Mobile robots extract information from its environment to understand their current situation to enable intelligent decision making and autonomous task execution. In our previous work, we introduced the concept of Situation Graphs (S-Graphs) which combines in a single optimizable graph, the robot keyframes and the representation of the environment with geometric, semantic and topological abstractions. Although S-Graphs were built and optimized in real-time and demonstrated state-of-the-art results, they are limited to specific structured environments with specific hand-tuned dimensions of rooms and corridors.   In this work, we present an advanced version of the Situational Graphs (S-Graphs+), consisting of the five layered optimizable graph that includes (1) metric layer along with the graph of free-space clusters (2) keyframe layer where the robot poses are registered (3) metric-semantic layer consisting of the extracted planar walls (4) novel rooms layer constraining the extracted planar walls (5) novel floors layer encompassing the rooms within a given floor level. S-Graphs+ demonstrates improved performance over S-Graphs efficiently extracting the room information while simultaneously improving the pose estimate of the robot, thus extending the robots situational awareness in the form of a five layered environmental model.

</details>

<details>

<summary>2022-11-16 08:57:55 - RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models</summary>

- *Shitao Xiao, Zheng Liu*

- `2211.08769v1` - [abs](http://arxiv.org/abs/2211.08769v1) - [pdf](http://arxiv.org/pdf/2211.08769v1)

> To better support retrieval applications such as web search and question answering, growing effort is made to develop retrieval-oriented language models. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which helps to produce a better representation effect. As such, it's necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks.   With this motivation, we propose a new pre-training method: duplex masked auto-encoder, a.k.a. DupMAE, which targets on improving the semantic representation capacity for the contextualized embeddings of both [CLS] and ordinary tokens. It introduces two decoding tasks: one is to reconstruct the original input sentence based on the [CLS] embedding, the other one is to minimize the bag-of-words loss (BoW) about the input sentence based on the entire ordinary tokens' embeddings. The two decoding losses are added up to train a unified encoding model. The embeddings from [CLS] and ordinary tokens, after dimension reduction and aggregation, are concatenated as one unified semantic representation for the input. DupMAE is simple but empirically competitive: with a small decoding cost, it substantially contributes to the model's representation capability and transferability, where remarkable improvements are achieved on MS MARCO and BEIR benchmarks.

</details>

<details>

<summary>2022-11-16 10:00:23 - Giving Feedback on Interactive Student Programs with Meta-Exploration</summary>

- *Evan Zheran Liu, Moritz Stephan, Allen Nie, Chris Piech, Emma Brunskill, Chelsea Finn*

- `2211.08802v1` - [abs](http://arxiv.org/abs/2211.08802v1) - [pdf](http://arxiv.org/pdf/2211.08802v1)

> Developing interactive software, such as websites or games, is a particularly engaging way to learn computer science. However, teaching and giving feedback on such software is time-consuming -- standard approaches require instructors to manually grade student-implemented interactive programs. As a result, online platforms that serve millions, like Code.org, are unable to provide any feedback on assignments for implementing interactive programs, which critically hinders students' ability to learn. One approach toward automatic grading is to learn an agent that interacts with a student's program and explores states indicative of errors via reinforcement learning. However, existing work on this approach only provides binary feedback of whether a program is correct or not, while students require finer-grained feedback on the specific errors in their programs to understand their mistakes. In this work, we show that exploring to discover errors can be cast as a meta-exploration problem. This enables us to construct a principled objective for discovering errors and an algorithm for optimizing this objective, which provides fine-grained feedback. We evaluate our approach on a set of over 700K real anonymized student programs from a Code.org interactive assignment. Our approach provides feedback with 94.3% accuracy, improving over existing approaches by 17.7% and coming within 1.5% of human-level accuracy. Project web page: https://ezliu.github.io/dreamgrader.

</details>

<details>

<summary>2022-11-16 10:27:06 - T-SEA: Transfer-based Self-Ensemble Attack on Object Detection</summary>

- *Hao Huang, Ziyan Chen, Huanran Chen, Yongtao Wang, Kevin Zhang*

- `2211.09773v1` - [abs](http://arxiv.org/abs/2211.09773v1) - [pdf](http://arxiv.org/pdf/2211.09773v1)

> Compared to query-based black-box attacks, transfer-based black-box attacks do not require any information of the attacked models, which ensures their secrecy. However, most existing transfer-based approaches rely on ensembling multiple models to boost the attack transferability, which is time- and resource-intensive, not to mention the difficulty of obtaining diverse models on the same task. To address this limitation, in this work, we focus on the single-model transfer-based black-box attack on object detection, utilizing only one model to achieve a high-transferability adversarial attack on multiple black-box detectors. Specifically, we first make observations on the patch optimization process of the existing method and propose an enhanced attack framework by slightly adjusting its training strategies. Then, we analogize patch optimization with regular model optimization, proposing a series of self-ensemble approaches on the input data, the attacked model, and the adversarial patch to efficiently make use of the limited information and prevent the patch from overfitting. The experimental results show that the proposed framework can be applied with multiple classical base attack methods (e.g., PGD and MIM) to greatly improve the black-box transferability of the well-optimized patch on multiple mainstream detectors, meanwhile boosting white-box performance. Our code is available at https://github.com/VDIGPKU/T-SEA.

</details>

<details>

<summary>2022-11-16 12:08:58 - Attacking Object Detector Using A Universal Targeted Label-Switch Patch</summary>

- *Avishag Shapira, Ron Bitton, Dan Avraham, Alon Zolfi, Yuval Elovici, Asaf Shabtai*

- `2211.08859v1` - [abs](http://arxiv.org/abs/2211.08859v1) - [pdf](http://arxiv.org/pdf/2211.08859v1)

> Adversarial attacks against deep learning-based object detectors (ODs) have been studied extensively in the past few years. These attacks cause the model to make incorrect predictions by placing a patch containing an adversarial pattern on the target object or anywhere within the frame. However, none of prior research proposed a misclassification attack on ODs, in which the patch is applied on the target object. In this study, we propose a novel, universal, targeted, label-switch attack against the state-of-the-art object detector, YOLO. In our attack, we use (i) a tailored projection function to enable the placement of the adversarial patch on multiple target objects in the image (e.g., cars), each of which may be located a different distance away from the camera or have a different view angle relative to the camera, and (ii) a unique loss function capable of changing the label of the attacked objects. The proposed universal patch, which is trained in the digital domain, is transferable to the physical domain. We performed an extensive evaluation using different types of object detectors, different video streams captured by different cameras, and various target classes, and evaluated different configurations of the adversarial patch in the physical domain.

</details>

<details>

<summary>2022-11-16 15:31:45 - A framework for online, stabilizing reinforcement learning</summary>

- *Grigory Yaremenko, Georgiy Malaniya, Pavel Osinenko*

- `2207.08730v9` - [abs](http://arxiv.org/abs/2207.08730v9) - [pdf](http://arxiv.org/pdf/2207.08730v9)

> Online reinforcement learning is concerned with training an agent on-the-fly via dynamic interaction with the environment. Here, due to the specifics of the application, it is not generally possible to perform long pre-training, as it is commonly done in off-line, model-free approaches, which are akin to dynamic programming. Such applications may be found more frequently in industry, rather than in pure digital fields, such as cloud services, video games, database management, etc., where reinforcement learning has been demonstrating success. Online reinforcement learning, in contrast, is more akin to classical control, which utilizes some model knowledge about the environment. Stability of the closed-loop (agent plus the environment) is a major challenge for such online approaches. In this paper, we tackle this problem by a special fusion of online reinforcement learning with elements of classical control, namely, based on the Lyapunov theory of stability. The idea is to start the agent at once, without pre-training, and learn approximately optimal policy under specially designed constraints, which guarantee stability. The resulting approach was tested in an extensive experimental study with a mobile robot. A nominal parking controller was used as a baseline. It was observed that the suggested agent could always successfully park the robot, while significantly improving the cost. While many approaches may be exploited for mobile robot control, we suggest that the experiments showed the promising potential of online reinforcement learning agents based on Lyapunov-like constraints. The presented methodology may be utilized in safety-critical, industrial applications where stability is necessary.

</details>

<details>

<summary>2022-11-16 17:35:52 - Towards Computationally Verifiable Semantic Grounding for Language Models</summary>

- *Chris Alberti, Kuzman Ganchev, Michael Collins, Sebastian Gehrmann, Ciprian Chelba*

- `2211.09070v1` - [abs](http://arxiv.org/abs/2211.09070v1) - [pdf](http://arxiv.org/pdf/2211.09070v1)

> The paper presents an approach to semantic grounding of language models (LMs) that conceptualizes the LM as a conditional model generating text given a desired semantic message formalized as a set of entity-relationship triples. It embeds the LM in an auto-encoder by feeding its output to a semantic parser whose output is in the same representation domain as the input message. Compared to a baseline that generates text using greedy search, we demonstrate two techniques that improve the fluency and semantic accuracy of the generated text: The first technique samples multiple candidate text sequences from which the semantic parser chooses. The second trains the language model while keeping the semantic parser frozen to improve the semantic accuracy of the auto-encoder. We carry out experiments on the English WebNLG 3.0 data set, using BLEU to measure the fluency of generated text and standard parsing metrics to measure semantic accuracy. We show that our proposed approaches significantly improve on the greedy search baseline. Human evaluation corroborates the results of the automatic evaluation experiments.

</details>

<details>

<summary>2022-11-16 18:59:48 - AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders</summary>

- *Wele Gedara Chaminda Bandara, Naman Patel, Ali Gholami, Mehdi Nikkhah, Motilal Agrawal, Vishal M. Patel*

- `2211.09120v1` - [abs](http://arxiv.org/abs/2211.09120v1) - [pdf](http://arxiv.org/pdf/2211.09120v1)

> Masked Autoencoders (MAEs) learn generalizable representations for image, text, audio, video, etc., by reconstructing masked input data from tokens of the visible data. Current MAE approaches for videos rely on random patch, tube, or frame-based masking strategies to select these tokens. This paper proposes AdaMAE, an adaptive masking strategy for MAEs that is end-to-end trainable. Our adaptive masking strategy samples visible tokens based on the semantic context using an auxiliary sampling network. This network estimates a categorical distribution over spacetime-patch tokens. The tokens that increase the expected reconstruction error are rewarded and selected as visible tokens, motivated by the policy gradient algorithm in reinforcement learning. We show that AdaMAE samples more tokens from the high spatiotemporal information regions, thereby allowing us to mask 95% of tokens, resulting in lower memory requirements and faster pre-training. We conduct ablation studies on the Something-Something v2 (SSv2) dataset to demonstrate the efficacy of our adaptive sampling approach and report state-of-the-art results of 70.0% and 81.7% in top-1 accuracy on SSv2 and Kinetics-400 action classification datasets with a ViT-Base backbone and 800 pre-training epochs.

</details>

<details>

<summary>2022-11-17 06:48:04 - Lesion Guided Explainable Few Weak-shot Medical Report Generation</summary>

- *Jinghan Sun, Dong Wei, Liansheng Wang, Yefeng Zheng*

- `2211.08732v2` - [abs](http://arxiv.org/abs/2211.08732v2) - [pdf](http://arxiv.org/pdf/2211.08732v2)

> Medical images are widely used in clinical practice for diagnosis. Automatically generating interpretable medical reports can reduce radiologists' burden and facilitate timely care. However, most existing approaches to automatic report generation require sufficient labeled data for training. In addition, the learned model can only generate reports for the training classes, lacking the ability to adapt to previously unseen novel diseases. To this end, we propose a lesion guided explainable few weak-shot medical report generation framework that learns correlation between seen and novel classes through visual and semantic feature alignment, aiming to generate medical reports for diseases not observed in training. It integrates a lesion-centric feature extractor and a Transformer-based report generation module. Concretely, the lesion-centric feature extractor detects the abnormal regions and learns correlations between seen and novel classes with multi-view (visual and lexical) embeddings. Then, features of the detected regions and corresponding embeddings are concatenated as multi-view input to the report generation module for explainable report generation, including text descriptions and corresponding abnormal regions detected in the images. We conduct experiments on FFA-IR, a dataset providing explainable annotations, showing that our framework outperforms others on report generation for novel diseases.

</details>

<details>

<summary>2022-11-17 07:04:11 - Execution-based Evaluation for Data Science Code Generation Models</summary>

- *Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, Nan Duan, Jianfeng Gao*

- `2211.09374v1` - [abs](http://arxiv.org/abs/2211.09374v1) - [pdf](http://arxiv.org/pdf/2211.09374v1)

> Code generation models can benefit data scientists' productivity by automatically generating code from context and text descriptions. An important measure of the modeling progress is whether a model can generate code that can correctly execute to solve the task. However, due to the lack of an evaluation dataset that directly supports execution-based model evaluation, existing work relies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model selection, which can be inaccurate.   To remedy this, we introduce ExeDS, an evaluation dataset for execution evaluation for data science code generation tasks. ExeDS contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output. With ExeDS, we evaluate the execution performance of five state-of-the-art code generation models that have achieved high surface-form evaluation scores. Our experiments show that models with high surface-form scores do not necessarily perform well on execution metrics, and execution-based metrics can better capture model code generation errors. Source code and data can be found at https://github.com/Jun-jie-Huang/ExeDS

</details>

<details>

<summary>2022-11-17 08:39:55 - Temporal Word Meaning Disambiguation using TimeLMs</summary>

- *Mihir Godbole, Parth Dandavate, Aditya Kane*

- `2210.08207v2` - [abs](http://arxiv.org/abs/2210.08207v2) - [pdf](http://arxiv.org/pdf/2210.08207v2)

> Meaning of words constantly changes given the events in modern civilization. Large Language Models use word embeddings, which are often static and thus cannot cope with this semantic change. Thus,it is important to resolve ambiguity in word meanings. This paper is an effort in this direction, where we explore methods for word sense disambiguation for the EvoNLP shared task. We conduct rigorous ablations for two solutions to this problem. We see that an approach using time-aware language models helps this task. Furthermore, we explore possible future directions to this problem.

</details>

<details>

<summary>2022-11-17 10:01:46 - Siamese based Neural Network for Offline Writer Identification on word level data</summary>

- *Vineet Kumar, Suresh Sundaram*

- `2211.14443v1` - [abs](http://arxiv.org/abs/2211.14443v1) - [pdf](http://arxiv.org/pdf/2211.14443v1)

> Handwriting recognition is one of the desirable attributes of document comprehension and analysis. It is concerned with the documents writing style and characteristics that distinguish the authors. The diversity of text images, notably in images with varying handwriting, makes the process of learning good features difficult in cases where little data is available. In this paper, we propose a novel scheme to identify the author of a document based on the input word image. Our method is text independent and does not impose any constraint on the size of the input image under examination. To begin with, we detect crucial components in handwriting and extract regions surrounding them using Scale Invariant Feature Transform (SIFT). These patches are designed to capture individual writing features (including allographs, characters, or combinations of characters) that are likely to be unique for an individual writer. These features are then passed through a deep Convolutional Neural Network (CNN) in which the weights are learned by applying the concept of Similarity learning using Siamese network. Siamese network enhances the discrimination power of CNN by mapping similarity between different pairs of input image. Features learned at different scales of the extracted SIFT key-points are encoded using Sparse PCA, each components of the Sparse PCA is assigned a saliency score signifying its level of significance in discriminating different writers effectively. Finally, the weighted Sparse PCA corresponding to each SIFT key-points is combined to arrive at a final classification score for each writer. The proposed algorithm was evaluated on two publicly available databases (namely IAM and CVL) and is able to achieve promising result, when compared with other deep learning based algorithm.

</details>

<details>

<summary>2022-11-17 14:32:01 - Convolutional neural networks for medical image segmentation</summary>

- *Jeroen Bertels, David Robben, Robin Lemmens, Dirk Vandermeulen*

- `2211.09562v1` - [abs](http://arxiv.org/abs/2211.09562v1) - [pdf](http://arxiv.org/pdf/2211.09562v1)

> In this article, we look into some essential aspects of convolutional neural networks (CNNs) with the focus on medical image segmentation. First, we discuss the CNN architecture, thereby highlighting the spatial origin of the data, voxel-wise classification and the receptive field. Second, we discuss the sampling of input-output pairs, thereby highlighting the interaction between voxel-wise classification, patch size and the receptive field. Finally, we give a historical overview of crucial changes to CNN architectures for classification and segmentation, giving insights in the relation between three pivotal CNN architectures: FCN, U-Net and DeepMedic.

</details>

<details>

<summary>2022-11-17 14:54:08 - Where Did My Variable Go? Poking Holes in Incomplete Debug Information</summary>

- *Cristian Assaiante, Daniele Cono D'Elia, Giuseppe Antonio Di Luna, Leonardo Querzoni*

- `2211.09568v1` - [abs](http://arxiv.org/abs/2211.09568v1) - [pdf](http://arxiv.org/pdf/2211.09568v1)

> The availability of debug information for optimized executables can largely ease crucial tasks such as crash analysis. Source-level debuggers use this information to display program state in terms of source code, allowing users to reason on it even when optimizations alter program structure extensively. A few recent endeavors have proposed effective methodologies for identifying incorrect instances of debug information, which can mislead users by presenting them with an inconsistent program state.   In this work, we identify and study a related important problem: the completeness of debug information. Unlike correctness issues for which an unoptimized executable can serve as reference, we find there is no analogous oracle to deem when the cause behind an unreported part of program state is an unavoidable effect of optimization or a compiler implementation defect. In this scenario, we argue that empirically derived conjectures on the expected availability of debug information can serve as an effective means to expose classes of these defects.   We propose three conjectures involving variable values and study how often synthetic programs compiled with different configurations of the popular gcc and LLVM compilers deviate from them. We then discuss techniques to pinpoint the optimizations behind such violations and minimize bug reports accordingly. Our experiments revealed, among others, 24 bugs already confirmed by the developers of the gcc-gdb and clang-lldb ecosystems.

</details>

<details>

<summary>2022-11-17 16:17:22 - Predicting Human Mobility via Self-supervised Disentanglement Learning</summary>

- *Qiang Gao, Jinyu Hong, Xovee Xu, Ping Kuang, Fan Zhou, Goce Trajcevski*

- `2211.09625v1` - [abs](http://arxiv.org/abs/2211.09625v1) - [pdf](http://arxiv.org/pdf/2211.09625v1)

> Deep neural networks have recently achieved considerable improvements in learning human behavioral patterns and individual preferences from massive spatial-temporal trajectories data. However, most of the existing research concentrates on fusing different semantics underlying sequential trajectories for mobility pattern learning which, in turn, yields a narrow perspective on comprehending human intrinsic motions. In addition, the inherent sparsity and under-explored heterogeneous collaborative items pertaining to human check-ins hinder the potential exploitation of human diverse periodic regularities as well as common interests. Motivated by recent advances in disentanglement learning, in this study we propose a novel disentangled solution called SSDL for tackling the next POI prediction problem. SSDL primarily seeks to disentangle the potential time-invariant and time-varying factors into different latent spaces from massive trajectories data, providing an interpretable view to understand the intricate semantics underlying human diverse mobility representations. To address the data sparsity issue, we present two realistic trajectory augmentation approaches to enhance the understanding of both the human intrinsic periodicity and constantly-changing intents. In addition, we devise a POI-centric graph structure to explore heterogeneous collaborative signals underlying historical check-ins. Extensive experiments conducted on four real-world datasets demonstrate that our proposed SSDL significantly outperforms the state-of-the-art approaches -- for example, it yields up to 8.57% improvements on ACC@1.

</details>

<details>

<summary>2022-11-17 20:13:04 - Gender Bias in Big Data Analysis</summary>

- *Thomas J. Misa*

- `2211.09865v1` - [abs](http://arxiv.org/abs/2211.09865v1) - [pdf](http://arxiv.org/pdf/2211.09865v1)

> This article combines humanistic "data critique" with informed inspection of big data analysis. It measures gender bias when gender prediction software tools (Gender API, Namsor, and Genderize.io) are used in historical big data research. Gender bias is measured by contrasting personally identified computer science authors in the well-regarded DBLP dataset (1950-1980) with exactly comparable results from the software tools. Implications for public understanding of gender bias in computing and the nature of the computing profession are outlined. Preliminary assessment of the Semantic Scholar dataset is presented. The conclusion combines humanistic approaches with selective use of big data methods.

</details>

<details>

<summary>2022-11-17 22:50:56 - Proceedings of the 2nd Workshop on Logic and Practice of Programming (LPOP)</summary>

- *David S. Warren, Peter Van Roy, Yanhong A. Liu*

- `2211.09923v1` - [abs](http://arxiv.org/abs/2211.09923v1) - [pdf](http://arxiv.org/pdf/2211.09923v1)

> This proceedings contains abstracts and position papers for the work presented at the second Logic and Practice of Programming (LPOP) Workshop. The workshop was held online, virtually in place of Chicago, USA, on November 15, 2010, in conjunction with the ACM SIGPLAN Conference on Systems, Programming, Languages, and Applications: Software for Humanity (SPLASH) 2020. The purpose of this workshop is to be a bridge between different areas of computer science that use logic as a practical tool. We take advantage of the common language of formal logic to exchange ideas between these different areas.

</details>

<details>

<summary>2022-11-17 23:17:01 - Explainability Via Causal Self-Talk</summary>

- *Nicholas A. Roy, Junkyung Kim, Neil Rabinowitz*

- `2211.09937v1` - [abs](http://arxiv.org/abs/2211.09937v1) - [pdf](http://arxiv.org/pdf/2211.09937v1)

> Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.

</details>

<details>

<summary>2022-11-18 00:35:05 - Potential Auto-driving Threat: Universal Rain-removal Attack</summary>

- *Jinchegn Hu, Jihao Li, Zhuoran Hou, Jingjing Jiang, Cunjia Liu, Yuanjian Zhang*

- `2211.09959v1` - [abs](http://arxiv.org/abs/2211.09959v1) - [pdf](http://arxiv.org/pdf/2211.09959v1)

> The problem of robustness in adverse weather conditions is considered a significant challenge for computer vision algorithms in the applicants of autonomous driving. Image rain removal algorithms are a general solution to this problem. They find a deep connection between raindrops/rain-streaks and images by mining the hidden features and restoring information about the rain-free environment based on the powerful representation capabilities of neural networks. However, previous research has focused on architecture innovations and has yet to consider the vulnerability issues that already exist in neural networks. This research gap hints at a potential security threat geared toward the intelligent perception of autonomous driving in the rain. In this paper, we propose a universal rain-removal attack (URA) on the vulnerability of image rain-removal algorithms by generating a non-additive spatial perturbation that significantly reduces the similarity and image quality of scene restoration. Notably, this perturbation is difficult to recognise by humans and is also the same for different target images. Thus, URA could be considered a critical tool for the vulnerability detection of image rain-removal algorithms. It also could be developed as a real-world artificial intelligence attack method. Experimental results show that URA can reduce the scene repair capability by 39.5% and the image generation quality by 26.4%, targeting the state-of-the-art (SOTA) single-image rain-removal algorithms currently available.

</details>

<details>

<summary>2022-11-18 04:15:13 - Autonomous Platoon Control with Integrated Deep Reinforcement Learning and Dynamic Programming</summary>

- *Tong Liu, Lei Lei, Kan Zheng, Kuan Zhang*

- `2206.07536v2` - [abs](http://arxiv.org/abs/2206.07536v2) - [pdf](http://arxiv.org/pdf/2206.07536v2)

> Deep Reinforcement Learning (DRL) is regarded as a potential method for car-following control and has been mostly studied to support a single following vehicle. However, it is more challenging to learn a stable and efficient car-following policy when there are multiple following vehicles in a platoon, especially with unpredictable leading vehicle behavior. In this context, we adopt an integrated DRL and Dynamic Programming (DP) approach to learn autonomous platoon control policies, which embeds the Deep Deterministic Policy Gradient (DDPG) algorithm into a finite-horizon value iteration framework. Although the DP framework can improve the stability and performance of DDPG, it has the limitations of lower sampling and training efficiency. In this paper, we propose an algorithm, namely Finite-Horizon-DDPG with Sweeping through reduced state space using Stationary approximation (FH-DDPG-SS), which uses three key ideas to overcome the above limitations, i.e., transferring network weights backward in time, stationary policy approximation for earlier time steps, and sweeping through reduced state space. In order to verify the effectiveness of FH-DDPG-SS, simulation using real driving data is performed, where the performance of FH-DDPG-SS is compared with those of the benchmark algorithms. Finally, platoon safety and string stability for FH-DDPG-SS are demonstrated.

</details>

<details>

<summary>2022-11-18 05:01:19 - Contrastive Knowledge Graph Error Detection</summary>

- *Qinggang Zhang, Junnan Dong, Keyu Duan, Xiao Huang, Yezi Liu, Linchuan Xu*

- `2211.10030v1` - [abs](http://arxiv.org/abs/2211.10030v1) - [pdf](http://arxiv.org/pdf/2211.10030v1)

> Knowledge Graph (KG) errors introduce non-negligible noise, severely affecting KG-related downstream tasks. Detecting errors in KGs is challenging since the patterns of errors are unknown and diverse, while ground-truth labels are rare or even unavailable. A traditional solution is to construct logical rules to verify triples, but it is not generalizable since different KGs have distinct rules with domain knowledge involved. Recent studies focus on designing tailored detectors or ranking triples based on KG embedding loss. However, they all rely on negative samples for training, which are generated by randomly replacing the head or tail entity of existing triples. Such a negative sampling strategy is not enough for prototyping practical KG errors, e.g., (Bruce_Lee, place_of_birth, China), in which the three elements are often relevant, although mismatched. We desire a more effective unsupervised learning mechanism tailored for KG error detection. To this end, we propose a novel framework - ContrAstive knowledge Graph Error Detection (CAGED). It introduces contrastive learning into KG learning and provides a novel way of modeling KG. Instead of following the traditional setting, i.e., considering entities as nodes and relations as semantic edges, CAGED augments a KG into different hyper-views, by regarding each relational triple as a node. After joint training with KG embedding and contrastive learning loss, CAGED assesses the trustworthiness of each triple based on two learning signals, i.e., the consistency of triple representations across multi-views and the self-consistency within the triple. Extensive experiments on three real-world KGs show that CAGED outperforms state-of-the-art methods in KG error detection. Our codes and datasets are available at https://github.com/Qing145/CAGED.git.

</details>

<details>

<summary>2022-11-18 09:33:05 - Algorithms for Weighted Pushdown Automata</summary>

- *Alexandra Butoi, Brian DuSell, Tim Vieira, Ryan Cotterell, David Chiang*

- `2210.06884v3` - [abs](http://arxiv.org/abs/2210.06884v3) - [pdf](http://arxiv.org/pdf/2210.06884v3)

> Weighted pushdown automata (WPDAs) are at the core of many natural language processing tasks, like syntax-based statistical machine translation and transition-based dependency parsing. As most existing dynamic programming algorithms are designed for context-free grammars (CFGs), algorithms for PDAs often resort to a PDA-to-CFG conversion. In this paper, we develop novel algorithms that operate directly on WPDAs. Our algorithms are inspired by Lang's algorithm, but use a more general definition of pushdown automaton and either reduce the space requirements by a factor of $|\Gamma|$ (the size of the stack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the number of states). When run on the same class of PDAs as Lang's algorithm, our algorithm is both more space-efficient by a factor of $|\Gamma|$ and more time-efficient by a factor of $|Q| \cdot |\Gamma|$.

</details>

<details>

<summary>2022-11-18 11:29:04 - Language-Conditioned Reinforcement Learning to Solve Misunderstandings with Action Corrections</summary>

- *Frank RÃ¶der, Manfred Eppe*

- `2211.10168v1` - [abs](http://arxiv.org/abs/2211.10168v1) - [pdf](http://arxiv.org/pdf/2211.10168v1)

> Human-to-human conversation is not just talking and listening. It is an incremental process where participants continually establish a common understanding to rule out misunderstandings. Current language understanding methods for intelligent robots do not consider this. There exist numerous approaches considering non-understandings, but they ignore the incremental process of resolving misunderstandings. In this article, we present a first formalization and experimental validation of incremental action-repair for robotic instruction-following based on reinforcement learning. To evaluate our approach, we propose a collection of benchmark environments for action correction in language-conditioned reinforcement learning, utilizing a synthetic instructor to generate language goals and their corresponding corrections. We show that a reinforcement learning agent can successfully learn to understand incremental corrections of misunderstood instructions.

</details>

<details>

<summary>2022-11-18 12:56:01 - Adaptive Constraint Partition based Optimization Framework for Large-scale Integer Linear Programming(Student Abstract)</summary>

- *Huigen Ye, Hongyan Wang, Hua Xu, Chengming Wang, Yu Jiang*

- `2211.11564v1` - [abs](http://arxiv.org/abs/2211.11564v1) - [pdf](http://arxiv.org/pdf/2211.11564v1)

> Integer programming problems (IPs) are challenging to be solved efficiently due to the NP-hardness, especially for large-scale IPs. To solve this type of IPs, Large neighborhood search (LNS) uses an initial feasible solution and iteratively improves it by searching a large neighborhood around the current solution. However, LNS easily steps into local optima and ignores the correlation between variables to be optimized, leading to compromised performance. This paper presents a general adaptive constraint partition-based optimization framework (ACP) for large-scale IPs that can efficiently use any existing optimization solver as a subroutine. Specifically, ACP first randomly partitions the constraints into blocks, where the number of blocks is adaptively adjusted to avoid local optima. Then, ACP uses a subroutine solver to optimize the decision variables in a randomly selected block of constraints to enhance the variable correlation. ACP is compared with LNS framework with different subroutine solvers on four IPs and a real-world IP. The experimental results demonstrate that in specified wall-clock time ACP shows better performance than SCIP and Gurobi.

</details>

<details>

<summary>2022-11-18 13:17:25 - Geometric Multimodal Contrastive Representation Learning</summary>

- *Petra Poklukar, Miguel Vasco, Hang Yin, Francisco S. Melo, Ana Paiva, Danica Kragic*

- `2202.03390v4` - [abs](http://arxiv.org/abs/2202.03390v4) - [pdf](http://arxiv.org/pdf/2202.03390v4)

> Learning representations of multimodal data that are both informative and robust to missing modalities at test time remains a challenging problem due to the inherent heterogeneity of data obtained from different channels. To address it, we present a novel Geometric Multimodal Contrastive (GMC) representation learning method consisting of two main components: i) a two-level architecture consisting of modality-specific base encoders, allowing to process an arbitrary number of modalities to an intermediate representation of fixed dimensionality, and a shared projection head, mapping the intermediate representations to a latent representation space; ii) a multimodal contrastive loss function that encourages the geometric alignment of the learned representations. We experimentally demonstrate that GMC representations are semantically rich and achieve state-of-the-art performance with missing modality information on three different learning problems including prediction and reinforcement learning tasks.

</details>

<details>

<summary>2022-11-18 16:33:19 - Modeling chronic pain experiences from online reports using the Reddit Reports of Chronic Pain dataset</summary>

- *Diogo A. P. Nunes, Joana Ferreira-Gomes, Fani Neto, David Martins de Matos*

- `2108.10218v4` - [abs](http://arxiv.org/abs/2108.10218v4) - [pdf](http://arxiv.org/pdf/2108.10218v4)

> Objective: Reveal and quantify qualities of reported experiences of chronic pain on social media, from multiple pathological backgrounds, by means of the novel Reddit Reports of Chronic Pain (RRCP) dataset, using Natural Language Processing techniques. Materials and Methods: Define and validate the RRCP dataset for a set of subreddits related to chronic pain. Identify the main concerns discussed in each subreddit. Model each subreddit according to their main concerns. Compare subreddit models. Results: The RRCP dataset comprises 86,537 Reddit submissions from 12 subreddits related to chronic pain (each related to one pathological background). Each RRCP subreddit has various main concerns. Some of these concerns are shared between multiple subreddits (e.g., the subreddit Sciatica semantically entails the subreddit backpain in their various concerns, but not the other way around), whilst some concerns are exclusive to specific subreddits (e.g., Interstitialcystitis and CrohnsDisease). Discussion: These results suggest that the reported experience of chronic pain, from multiple pathologies (i.e., subreddits), has concerns relevant to all, and concerns exclusive to certain pathologies. Our analysis details each of these concerns and their similarity relations. Conclusion: Although limited by intrinsic qualities of the Reddit platform, to the best of our knowledge, this is the first research work attempting to model the linguistic expression of various chronic pain-inducing pathologies and comparing these models to identify and quantify the similarities and differences between the corresponding emergent chronic pain experiences.

</details>

<details>

<summary>2022-11-18 17:35:36 - Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction</summary>

- *Wenjun Xia, Wenxiang Cong, Ge Wang*

- `2211.10388v1` - [abs](http://arxiv.org/abs/2211.10388v1) - [pdf](http://arxiv.org/pdf/2211.10388v1)

> Sparse-view computed tomography (CT) can be used to reduce radiation dose greatly but is suffers from severe image artifacts. Recently, the deep learning based method for sparse-view CT reconstruction has attracted a major attention. However, neural networks often have a limited ability to remove the artifacts when they only work in the image domain. Deep learning-based sinogram processing can achieve a better anti-artifact performance, but it inevitably requires feature maps of the whole image in a video memory, which makes handling large-scale or three-dimensional (3D) images rather challenging. In this paper, we propose a patch-based denoising diffusion probabilistic model (DDPM) for sparse-view CT reconstruction. A DDPM network based on patches extracted from fully sampled projection data is trained and then used to inpaint down-sampled projection data. The network does not require paired full-sampled and down-sampled data, enabling unsupervised learning. Since the data processing is patch-based, the deep learning workflow can be distributed in parallel, overcoming the memory problem of large-scale data. Our experiments show that the proposed method can effectively suppress few-view artifacts while faithfully preserving textural details.

</details>

<details>

<summary>2022-11-18 17:43:00 - Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation</summary>

- *Danni Liu, Jan Niehues*

- `2211.01292v2` - [abs](http://arxiv.org/abs/2211.01292v2) - [pdf](http://arxiv.org/pdf/2211.01292v2)

> The cornerstone of multilingual neural translation is shared representations across languages. Given the theoretically infinite representation power of neural networks, semantically identical sentences are likely represented differently. While representing sentences in the continuous latent space ensures expressiveness, it introduces the risk of capturing of irrelevant features which hinders the learning of a common representation. In this work, we discretize the encoder output latent space of multilingual models by assigning encoder states to entries in a codebook, which in effect represents source sentences in a new artificial language. This discretization process not only offers a new way to interpret the otherwise black-box model representations, but, more importantly, gives potential for increasing robustness in unseen testing conditions. We validate our approach on large-scale experiments with realistic data volumes and domains. When tested in zero-shot conditions, our approach is competitive with two strong alternatives from the literature. We also use the learned artificial language to analyze model behavior, and discover that using a similar bridge language increases knowledge-sharing among the remaining languages.

</details>

<details>

<summary>2022-11-18 18:50:09 - Visual Programming: Compositional visual reasoning without training</summary>

- *Tanmay Gupta, Aniruddha Kembhavi*

- `2211.11559v1` - [abs](http://arxiv.org/abs/2211.11559v1) - [pdf](http://arxiv.org/pdf/2211.11559v1)

> We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.

</details>

<details>

<summary>2022-11-18 19:01:21 - Knowledge Graph Refinement based on Triplet BERT-Networks</summary>

- *Armita Khajeh Nassiri, Nathalie Pernelle, Fatiha Sais, Gianluca Quercini*

- `2211.10460v1` - [abs](http://arxiv.org/abs/2211.10460v1) - [pdf](http://arxiv.org/pdf/2211.10460v1)

> Knowledge graph embedding techniques are widely used for knowledge graph refinement tasks such as graph completion and triple classification. These techniques aim at embedding the entities and relations of a Knowledge Graph (KG) in a low dimensional continuous feature space. This paper adopts a transformer-based triplet network creating an embedding space that clusters the information about an entity or relation in the KG. It creates textual sequences from facts and fine-tunes a triplet network of pre-trained transformer-based language models. It adheres to an evaluation paradigm that relies on an efficient spatial semantic search technique. We show that this evaluation protocol is more adapted to a few-shot setting for the relation prediction task. Our proposed GilBERT method is evaluated on triplet classification and relation prediction tasks on multiple well-known benchmark knowledge graphs such as FB13, WN11, and FB15K. We show that GilBERT achieves better or comparable results to the state-of-the-art performance on these two refinement tasks.

</details>

<details>

<summary>2022-11-18 20:48:22 - Efficient Determinant Maximization for All Matroids</summary>

- *Adam Brown, Aditi Laddha, Madhusudhan Pittu, Mohit Singh*

- `2211.10507v1` - [abs](http://arxiv.org/abs/2211.10507v1) - [pdf](http://arxiv.org/pdf/2211.10507v1)

> Determinant maximization provides an elegant generalization of problems in many areas, including convex geometry, statistics, machine learning, fair allocation of goods, and network design. In an instance of the determinant maximization problem, we are given a collection of vectors $v_1,\ldots, v_n \in \mathbb{R}^d$, and the goal is to pick a subset $S\subseteq [n]$ of given vectors to maximize the determinant of the matrix $\sum_{i \in S} v_iv_i^\top$, where the picked set of vectors $S$ must satisfy some combinatorial constraint such as cardinality constraint ($|S| \leq k$) or matroid constraint ($S$ is a basis of a matroid defined on $[n]$).   In this work, we give a combinatorial algorithm for the determinant maximization problem under a matroid constraint that achieves $O(d^{O(d)})$-approximation for any matroid of rank $r\geq d$. This complements the recent result of~\cite{BrownLPST22} that achieves a similar bound for matroids of rank $r\leq d$, relying on a geometric interpretation of the determinant. Our result matches the best-known estimation algorithms~\cite{madan2020maximizing} for the problem, which could estimate the objective value but could not give an approximate solution with a similar guarantee. Our work follows the framework developed by~\cite{BrownLPST22} of using matroid intersection based algorithms for determinant maximization. To overcome the lack of a simple geometric interpretation of the objective when $r \geq d$, our approach combines ideas from combinatorial optimization with algebraic properties of the determinant. We also critically use the properties of a convex programming relaxation of the problem introduced by~\cite{madan2020maximizing}.

</details>

<details>

<summary>2022-11-18 23:25:17 - Toward a Flexible Metadata Pipeline for Fish Specimen Images</summary>

- *Dom Jebbia, Xiaojun Wang, Yasin Bakis, Henry L. Bart Jr., Jane Greenberg*

- `2211.15472v1` - [abs](http://arxiv.org/abs/2211.15472v1) - [pdf](http://arxiv.org/pdf/2211.15472v1)

> Flexible metadata pipelines are crucial for supporting the FAIR data principles. Despite this need, researchers seldom report their approaches for identifying metadata standards and protocols that support optimal flexibility. This paper reports on an initiative targeting the development of a flexible metadata pipeline for a collection containing over 300,000 digital fish specimen images, harvested from multiple data repositories and fish collections. The images and their associated metadata are being used for AI-related scientific research involving automated species identification, segmentation and trait extraction. The paper provides contextual background, followed by the presentation of a four-phased approach involving: 1. Assessment of the Problem, 2. Investigation of Solutions, 3. Implementation, and 4. Refinement. The work is part of the NSF Harnessing the Data Revolution, Biology Guided Neural Networks (NSF/HDR-BGNN) project and the HDR Imageomics Institute. An RDF graph prototype pipeline is presented, followed by a discussion of research implications and conclusion summarizing the results.

</details>

<details>

<summary>2022-11-19 00:20:31 - PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming</summary>

- *Alexander K. Lew, Monica Agrawal, David Sontag, Vikash K. Mansinghka*

- `2007.11838v5` - [abs](http://arxiv.org/abs/2007.11838v5) - [pdf](http://arxiv.org/pdf/2007.11838v5)

> Data cleaning is naturally framed as probabilistic inference in a generative model of ground-truth data and likely errors, but the diversity of real-world error patterns and the hardness of inference make Bayesian approaches difficult to automate. We present PClean, a probabilistic programming language (PPL) for leveraging dataset-specific knowledge to automate Bayesian cleaning. Compared to general-purpose PPLs, PClean tackles a restricted problem domain, enabling three modeling and inference innovations: (1) a non-parametric model of relational database instances, which users' programs customize; (2) a novel sequential Monte Carlo inference algorithm that exploits the structure of PClean's model class; and (3) a compiler that generates near-optimal SMC proposals and blocked-Gibbs rejuvenation kernels based on the user's model and data. We show empirically that short (< 50-line) PClean programs can: be faster and more accurate than generic PPL inference on data-cleaning benchmarks; match state-of-the-art data-cleaning systems in terms of accuracy and runtime (unlike generic PPL inference in the same runtime); and scale to real-world datasets with millions of records.

</details>

<details>

<summary>2022-11-19 02:12:56 - Fictitious Play with Maximin Initialization</summary>

- *Sam Ganzfried*

- `2203.10774v5` - [abs](http://arxiv.org/abs/2203.10774v5) - [pdf](http://arxiv.org/pdf/2203.10774v5)

> Fictitious play has recently emerged as the most accurate scalable algorithm for approximating Nash equilibrium strategies in multiplayer games. We show that the degree of equilibrium approximation error of fictitious play can be significantly reduced by carefully selecting the initial strategies. We present several new procedures for strategy initialization and compare them to the classic approach, which initializes all pure strategies to have equal probability. The best-performing approach, called maximin, solves a nonconvex quadratic program to compute initial strategies and results in a nearly 75% reduction in approximation error compared to the classic approach when 5 initializations are used.

</details>

<details>

<summary>2022-11-19 03:55:55 - Person Text-Image Matching via Text-Feature Interpretability Embedding and External Attack Node Implantation</summary>

- *Fan Li, Hang Zhou, Huafeng Li, Yafei Zhang, Zhengtao Yu*

- `2211.08657v2` - [abs](http://arxiv.org/abs/2211.08657v2) - [pdf](http://arxiv.org/pdf/2211.08657v2)

> Person text-image matching, also known as text based person search, aims to retrieve images of specific pedestrians using text descriptions. Although person text-image matching has made great research progress, existing methods still face two challenges. First, the lack of interpretability of text features makes it challenging to effectively align them with their corresponding image features. Second, the same pedestrian image often corresponds to multiple different text descriptions, and a single text description can correspond to multiple different images of the same identity. The diversity of text descriptions and images makes it difficult for a network to extract robust features that match the two modalities. To address these problems, we propose a person text-image matching method by embedding text-feature interpretability and an external attack node. Specifically, we improve the interpretability of text features by providing them with consistent semantic information with image features to achieve the alignment of text and describe image region features.To address the challenges posed by the diversity of text and the corresponding person images, we treat the variation caused by diversity to features as caused by perturbation information and propose a novel adversarial attack and defense method to solve it. In the model design, graph convolution is used as the basic framework for feature representation and the adversarial attacks caused by text and image diversity on feature extraction is simulated by implanting an additional attack node in the graph convolution layer to improve the robustness of the model against text and image diversity. Extensive experiments demonstrate the effectiveness and superiority of text-pedestrian image matching over existing methods. The source code of the method is published at

</details>

<details>

<summary>2022-11-19 05:13:45 - Behind the Machine's Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention</summary>

- *Leo Schwinn, Doina Precup, BjÃ¶rn Eskofier, Dario Zanca*

- `2204.09093v2` - [abs](http://arxiv.org/abs/2204.09093v2) - [pdf](http://arxiv.org/pdf/2204.09093v2)

> By and large, existing computational models of visual attention tacitly assume perfect vision and full access to the stimulus and thereby deviate from foveated biological vision. Moreover, modeling top-down attention is generally reduced to the integration of semantic features without incorporating the signal of a high-level visual tasks that have been shown to partially guide human attention. We propose the Neural Visual Attention (NeVA) algorithm to generate visual scanpaths in a top-down manner. With our method, we explore the ability of neural networks on which we impose a biologically-inspired foveated vision constraint to generate human-like scanpaths without directly training for this objective. The loss of a neural network performing a downstream visual task (i.e., classification or reconstruction) flexibly provides top-down guidance to the scanpath. Extensive experiments show that our method outperforms state-of-the-art unsupervised human attention models in terms of similarity to human scanpaths. Additionally, the flexibility of the framework allows to quantitatively investigate the role of different tasks in the generated visual behaviors. Finally, we demonstrate the superiority of the approach in a novel experiment that investigates the utility of scanpaths in real-world applications, where imperfect viewing conditions are given.

</details>

<details>

<summary>2022-11-19 11:59:45 - Proceedings 9th Workshop on Horn Clauses for Verification and Synthesis and 10th International Workshop on Verification and Program Transformation</summary>

- *Geoffrey W. Hamilton, Temesghen Kahsai, Maurizio Proietti*

- `2211.10675v1` - [abs](http://arxiv.org/abs/2211.10675v1) - [pdf](http://arxiv.org/pdf/2211.10675v1)

> These proceedings include selected papers presented at the 9th Workshop on Horn Clauses for Verification and Synthesis and the Tenth International Workshop on Verification and Program Transformation, both affiliated with ETAPS 2022.   Many Program Verification and Synthesis problems of interest can be modeled directly using Horn clauses and many recent advances in the CLP and CAV communities have centered around efficiently solving problems presented as Horn clauses.   The HCVS series of workshops aims to bring together researchers working in the communities of Constraint/Logic Programming (e.g., ICLP and CP), Program Verification (e.g., CAV, TACAS, and VMCAI), and Automated Deduction (e.g., CADE, IJCAR), on the topic of Horn clause based analysis, verification, and synthesis.   Horn clauses for verification and synthesis have been advocated by these communities in different times and from different perspectives and HCVS is organized to stimulate interaction and a fruitful exchange and integration of experiences.   The aim of the VPT workshop is to bring together researchers working in the fields of Program Verification and Program Transformation.   There is a great potential for beneficial interactions between these two fields because:   1) On one hand, methods and tools developed in the field of Program Transformation such as partial evaluation, fold/unfold transformations, and supercompilation, have all been applied with success for the verification of infinite state and parameterized systems.   2) On the other hand, model checking, abstract interpretation, SAT and SMT solving and automated theorem proving have been used to enhance program transformation techniques. Moreover, the formal certification of program transformation tools, such as automated refactoring tools and compilers, has recently attracted considerable interest, posed major challenges.

</details>

<details>

<summary>2022-11-19 12:03:30 - Entity-Assisted Language Models for Identifying Check-worthy Sentences</summary>

- *Ting Su, Craig Macdonald, Iadh Ounis*

- `2211.10678v1` - [abs](http://arxiv.org/abs/2211.10678v1) - [pdf](http://arxiv.org/pdf/2211.10678v1)

> We propose a new uniform framework for text classification and ranking that can automate the process of identifying check-worthy sentences in political debates and speech transcripts. Our framework combines the semantic analysis of the sentences, with additional entity embeddings obtained through the identified entities within the sentences. In particular, we analyse the semantic meaning of each sentence using state-of-the-art neural language models such as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained from knowledge graph (KG) embedding models. Specifically, we instantiate our framework using five different language models, entity embeddings obtained from six different KG embedding models, as well as two combination methods leading to several Entity-Assisted neural language models. We extensively evaluate the effectiveness of our framework using two publicly available datasets from the CLEF' 2019 & 2020 CheckThat! Labs. Our results show that the neural language models significantly outperform traditional TF.IDF and LSTM methods. In addition, we show that the ALBERT model is consistently the most effective model among all the tested neural language models. Our entity embeddings significantly outperform other existing approaches from the literature that are based on similarity and relatedness scores between the entities in a sentence, when used alongside a KG embedding.

</details>

<details>

<summary>2022-11-19 12:18:18 - Make an Omelette with Breaking Eggs: Zero-Shot Learning for Novel Attribute Synthesis</summary>

- *Yu-Hsuan Li, Tzu-Yin Chao, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu*

- `2111.14182v6` - [abs](http://arxiv.org/abs/2111.14182v6) - [pdf](http://arxiv.org/pdf/2111.14182v6)

> Most of the existing algorithms for zero-shot classification problems typically rely on the attribute-based semantic relations among categories to realize the classification of novel categories without observing any of their instances. However, training the zero-shot classification models still requires attribute labeling for each class (or even instance) in the training dataset, which is also expensive. To this end, in this paper, we bring up a new problem scenario: "Can we derive zero-shot learning for novel attribute detectors/classifiers and use them to automatically annotate the dataset for labeling efficiency?". Basically, given only a small set of detectors that are learned to recognize some manually annotated attributes (i.e., the seen attributes), we aim to synthesize the detectors of novel attributes in a zero-shot learning manner. Our proposed method, Zero-Shot Learning for Attributes (ZSLA), which is the first of its kind to the best of our knowledge, tackles this new research problem by applying the set operations to first decompose the seen attributes into their basic attributes and then recombine these basic attributes into the novel ones. Extensive experiments are conducted to verify the capacity of our synthesized detectors for accurately capturing the semantics of the novel attributes and show their superior performance in terms of detection and localization compared to other baseline approaches. Moreover, we demonstrate the application of automatic annotation using our synthesized detectors on Caltech-UCSD Birds-200-2011 dataset. Various generalized zero-shot classification algorithms trained upon the dataset re-annotated by ZSLA show comparable performance with those trained with the manual ground-truth annotations. Please refer to our project page for source code: https://yuhsuanli.github.io/ZSLA/

</details>

<details>

<summary>2022-11-19 12:42:39 - Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling</summary>

- *Iain Carmichael, Andrew H. Song, Richard J. Chen, Drew F. K. Williamson, Tiffany Y. Chen, Faisal Mahmood*

- `2206.08885v2` - [abs](http://arxiv.org/abs/2206.08885v2) - [pdf](http://arxiv.org/pdf/2206.08885v2)

> Supervised learning tasks such as cancer survival prediction from gigapixel whole slide images (WSIs) are a critical challenge in computational pathology that requires modeling complex features of the tumor microenvironment. These learning tasks are often solved with deep multi-instance learning (MIL) models that do not explicitly capture intratumoral heterogeneity. We develop a novel variance pooling architecture that enables a MIL model to incorporate intratumoral heterogeneity into its predictions. Two interpretability tools based on representative patches are illustrated to probe the biological signals captured by these models. An empirical study with 4,479 gigapixel WSIs from the Cancer Genome Atlas shows that adding variance pooling onto MIL frameworks improves survival prediction performance for five cancer types.

</details>

<details>

<summary>2022-11-19 16:11:13 - Towards Ontology-Based Requirements Engineering for IoT-Supported Well-Being, Aging and Health</summary>

- *Hrvoje Belani, Petar Solic, Toni Perkovic*

- `2211.10735v1` - [abs](http://arxiv.org/abs/2211.10735v1) - [pdf](http://arxiv.org/pdf/2211.10735v1)

> Ontologies serve as a one of the formal means to represent and model knowledge in computer science, electrical engineering, system engineering and other related disciplines. Ontologies within requirements engineering may be used for formal representation of system requirements. In the Internet of Things, ontologies may be used to represent sensor knowledge and describe acquired data semantics. Designing an ontology comprehensive enough with an appropriate level of knowledge expressiveness, serving multiple purposes, from system requirements specifications to modeling knowledge based on data from IoT sensors, is one of the great challenges. This paper proposes an approach towards ontology-based requirements engineering for well-being, aging and health supported by the Internet of Things. Such an ontology design does not aim at creating a new ontology, but extending the appropriate one already existing, SAREF4EHAW, in order align with the well-being, aging and health concepts and structure the knowledge within the domain. Other contributions include a conceptual formulation for Well-Being, Aging and Health and a related taxonomy, as well as a concept of One Well-Being, Aging and Health. New attributes and relations have been proposed for the new ontology extension, along with the updated list of use cases and particular ontological requirements not covered by the original ontology. Future work envisions full specification of the new ontology extension, as well as structuring system requirements and sensor measurement parameters to follow description logic.

</details>

<details>

<summary>2022-11-19 21:15:47 - Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training</summary>

- *Zhenglun Kong, Haoyu Ma, Geng Yuan, Mengshu Sun, Yanyue Xie, Peiyan Dong, Xin Meng, Xuan Shen, Hao Tang, Minghai Qin, Tianlong Chen, Xiaolong Ma, Xiaohui Xie, Zhangyang Wang, Yanzhi Wang*

- `2211.10801v1` - [abs](http://arxiv.org/abs/2211.10801v1) - [pdf](http://arxiv.org/pdf/2211.10801v1)

> Vision transformers (ViTs) have recently obtained success in many applications, but their intensive computation and heavy memory usage at both training and inference time limit their generalization. Previous compression algorithms usually start from the pre-trained dense models and only focus on efficient inference, while time-consuming training is still unavoidable. In contrast, this paper points out that the million-scale training data is redundant, which is the fundamental reason for the tedious training. To address the issue, this paper aims to introduce sparsity into data and proposes an end-to-end efficient training framework from three sparse perspectives, dubbed Tri-Level E-ViT. Specifically, we leverage a hierarchical data redundancy reduction scheme, by exploring the sparsity under three levels: number of training examples in the dataset, number of patches (tokens) in each example, and number of connections between tokens that lie in attention weights. With extensive experiments, we demonstrate that our proposed technique can noticeably accelerate training for various ViT architectures while maintaining accuracy. Remarkably, under certain ratios, we are able to improve the ViT accuracy rather than compromising it. For example, we can achieve 15.2% speedup with 72.6% (+0.4) Top-1 accuracy on Deit-T, and 15.7% speedup with 79.9% (+0.1) Top-1 accuracy on Deit-S. This proves the existence of data redundancy in ViT.

</details>

<details>

<summary>2022-11-19 21:42:55 - Concept-based Explanations using Non-negative Concept Activation Vectors and Decision Tree for CNN Models</summary>

- *Gayda Mutahar, Tim Miller*

- `2211.10807v1` - [abs](http://arxiv.org/abs/2211.10807v1) - [pdf](http://arxiv.org/pdf/2211.10807v1)

> This paper evaluates whether training a decision tree based on concepts extracted from a concept-based explainer can increase interpretability for Convolutional Neural Networks (CNNs) models and boost the fidelity and performance of the used explainer. CNNs for computer vision have shown exceptional performance in critical industries. However, it is a significant barrier when deploying CNNs due to their complexity and lack of interpretability. Recent studies to explain computer vision models have shifted from extracting low-level features (pixel-based explanations) to mid-or high-level features (concept-based explanations). The current research direction tends to use extracted features in developing approximation algorithms such as linear or decision tree models to interpret an original model. In this work, we modify one of the state-of-the-art concept-based explanations and propose an alternative framework named TreeICE. We design a systematic evaluation based on the requirements of fidelity (approximate models to original model's labels), performance (approximate models to ground-truth labels), and interpretability (meaningful of approximate models to humans). We conduct computational evaluation (for fidelity and performance) and human subject experiments (for interpretability) We find that Tree-ICE outperforms the baseline in interpretability and generates more human readable explanations in the form of a semantic tree structure. This work features how important to have more understandable explanations when interpretability is crucial.

</details>

<details>

<summary>2022-11-20 01:28:44 - SeDR: Segment Representation Learning for Long Documents Dense Retrieval</summary>

- *Junying Chen, Qingcai Chen, Dongfang Li, Yutao Huang*

- `2211.10841v1` - [abs](http://arxiv.org/abs/2211.10841v1) - [pdf](http://arxiv.org/pdf/2211.10841v1)

> Recently, Dense Retrieval (DR) has become a promising solution to document retrieval, where document representations are used to perform effective and efficient semantic search. However, DR remains challenging on long documents, due to the quadratic complexity of its Transformer-based encoder and the finite capacity of a low-dimension embedding. Current DR models use suboptimal strategies such as truncating or splitting-and-pooling to long documents leading to poor utilization of whole document information. In this work, to tackle this problem, we propose Segment representation learning for long documents Dense Retrieval (SeDR). In SeDR, Segment-Interaction Transformer is proposed to encode long documents into document-aware and segment-sensitive representations, while it holds the complexity of splitting-and-pooling and outperforms other segment-interaction patterns on DR. Since GPU memory requirements for long document encoding causes insufficient negatives for DR training, Late-Cache Negative is further proposed to provide additional cache negatives for optimizing representation learning. Experiments on MS MARCO and TREC-DL datasets show that SeDR achieves superior performance among DR models, and confirm the effectiveness of SeDR on long document retrieval.

</details>

<details>

<summary>2022-11-20 04:59:33 - Detecting Conspiracy Theory Against COVID-19 Vaccines</summary>

- *Md Hasibul Amin, Harika Madanu, Sahithi Lavu, Hadi Mansourifar, Dana Alsagheer, Weidong Shi*

- `2211.13003v1` - [abs](http://arxiv.org/abs/2211.13003v1) - [pdf](http://arxiv.org/pdf/2211.13003v1)

> Since the beginning of the vaccination trial, social media has been flooded with anti-vaccination comments and conspiracy beliefs. As the day passes, the number of COVID- 19 cases increases, and online platforms and a few news portals entertain sharing different conspiracy theories. The most popular conspiracy belief was the link between the 5G network spreading COVID-19 and the Chinese government spreading the virus as a bioweapon, which initially created racial hatred. Although some disbelief has less impact on society, others create massive destruction. For example, the 5G conspiracy led to the burn of the 5G Tower, and belief in the Chinese bioweapon story promoted an attack on the Asian-Americans. Another popular conspiracy belief was that Bill Gates spread this Coronavirus disease (COVID-19) by launching a mass vaccination program to track everyone. This Conspiracy belief creates distrust issues among laypeople and creates vaccine hesitancy. This study aims to discover the conspiracy theory against the vaccine on social platforms. We performed a sentiment analysis on the 598 unique sample comments related to COVID-19 vaccines. We used two different models, BERT and Perspective API, to find out the sentiment and toxicity of the sentence toward the COVID-19 vaccine.

</details>

<details>

<summary>2022-11-20 05:30:16 - Neural Embeddings for Text</summary>

- *Oleg Vasilyev, John Bohannon*

- `2208.08386v2` - [abs](http://arxiv.org/abs/2208.08386v2) - [pdf](http://arxiv.org/pdf/2208.08386v2)

> We propose a new kind of embedding for natural language text that deeply represents semantic meaning. Standard text embeddings use the outputs from hidden layers of a pretrained language model. In our method, we let a language model learn from the text and then literally pick its brain, taking the actual weights of the model's neurons to generate a vector. We call this representation of the text a neural embedding. We confirm the ability of this representation to reflect semantics of the text by an analysis of its behavior on several datasets, and by a comparison of neural embedding with state of the art sentence embeddings.

</details>

<details>

<summary>2022-11-20 15:25:19 - Embracing Ambiguity: Improving Similarity-oriented Tasks with Contextual Synonym Knowledge</summary>

- *Yangning Li, Jiaoyan Chen, Yinghui Li, Tianyu Yu, Xi Chen, Hai-Tao Zheng*

- `2211.10997v1` - [abs](http://arxiv.org/abs/2211.10997v1) - [pdf](http://arxiv.org/pdf/2211.10997v1)

> Contextual synonym knowledge is crucial for those similarity-oriented tasks whose core challenge lies in capturing semantic similarity between entities in their contexts, such as entity linking and entity matching. However, most Pre-trained Language Models (PLMs) lack synonym knowledge due to inherent limitations of their pre-training objectives such as masked language modeling (MLM). Existing works which inject synonym knowledge into PLMs often suffer from two severe problems: (i) Neglecting the ambiguity of synonyms, and (ii) Undermining semantic understanding of original PLMs, which is caused by inconsistency between the exact semantic similarity of the synonyms and the broad conceptual relevance learned from the original corpus. To address these issues, we propose PICSO, a flexible framework that supports the injection of contextual synonym knowledge from multiple domains into PLMs via a novel entity-aware Adapter which focuses on the semantics of the entities (synonyms) in the contexts. Meanwhile, PICSO stores the synonym knowledge in additional parameters of the Adapter structure, which prevents it from corrupting the semantic understanding of the original PLM. Extensive experiments demonstrate that PICSO can dramatically outperform the original PLMs and the other knowledge and synonym injection models on four different similarity-oriented tasks. In addition, experiments on GLUE prove that PICSO also benefits general natural language understanding tasks. Codes and data will be public.

</details>

<details>

<summary>2022-11-20 17:51:06 - Pragmatic Constraint on Distributional Semantics</summary>

- *Elizaveta Zhemchuzhina, Nikolai Filippov, Ivan P. Yamshchikov*

- `2211.11041v1` - [abs](http://arxiv.org/abs/2211.11041v1) - [pdf](http://arxiv.org/pdf/2211.11041v1)

> This paper studies the limits of language models' statistical learning in the context of Zipf's law. First, we demonstrate that Zipf-law token distribution emerges irrespective of the chosen tokenization. Second, we show that Zipf distribution is characterized by two distinct groups of tokens that differ both in terms of their frequency and their semantics. Namely, the tokens that have a one-to-one correspondence with one semantic concept have different statistical properties than those with semantic ambiguity. Finally, we demonstrate how these properties interfere with statistical learning procedures motivated by distributional semantics.

</details>

<details>

<summary>2022-11-20 18:15:30 - The Stack: 3 TB of permissively licensed source code</summary>

- *Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos MuÃ±oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, Harm de Vries*

- `2211.15533v1` - [abs](http://arxiv.org/abs/2211.15533v1) - [pdf](http://arxiv.org/pdf/2211.15533v1)

> Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.

</details>

<details>

<summary>2022-11-20 19:03:19 - Semantic Similarity-Based Clustering of Findings From Security Testing Tools</summary>

- *Phillip Schneider, Markus Voggenreiter, Abdullah Gulraiz, Florian Matthes*

- `2211.11057v1` - [abs](http://arxiv.org/abs/2211.11057v1) - [pdf](http://arxiv.org/pdf/2211.11057v1)

> Over the last years, software development in domains with high security demands transitioned from traditional methodologies to uniting modern approaches from software development and operations (DevOps). Key principles of DevOps gained more importance and are now applied to security aspects of software development, resulting in the automation of security-enhancing activities. In particular, it is common practice to use automated security testing tools that generate reports after inspecting a software artifact from multiple perspectives. However, this raises the challenge of generating duplicate security findings. To identify these duplicate findings manually, a security expert has to invest resources like time, effort, and knowledge. A partial automation of this process could reduce the analysis effort, encourage DevOps principles, and diminish the chance of human error. In this study, we investigated the potential of applying Natural Language Processing for clustering semantically similar security findings to support the identification of problem-specific duplicate findings. Towards this goal, we developed a web application for annotating and assessing security testing tool reports and published a human-annotated corpus of clustered security findings. In addition, we performed a comparison of different semantic similarity techniques for automatically grouping security findings. Finally, we assess the resulting clusters using both quantitative and qualitative evaluation methods.

</details>

<details>

<summary>2022-11-20 19:03:33 - Energy Storage Price Arbitrage via Opportunity Value Function Prediction</summary>

- *Ningkun Zheng, Xiaoxiang Liu, Bolun Xu, Yuanyuan Shi*

- `2211.07797v2` - [abs](http://arxiv.org/abs/2211.07797v2) - [pdf](http://arxiv.org/pdf/2211.07797v2)

> This paper proposes a novel energy storage price arbitrage algorithm combining supervised learning with dynamic programming. The proposed approach uses a neural network to directly predicts the opportunity cost at different energy storage state-of-charge levels, and then input the predicted opportunity cost into a model-based arbitrage control algorithm for optimal decisions. We generate the historical optimal opportunity value function using price data and a dynamic programming algorithm, then use it as the ground truth and historical price as predictors to train the opportunity value function prediction model. Our method achieves 65% to 90% profit compared to perfect foresight in case studies using different energy storage models and price data from New York State, which significantly outperforms existing model-based and learning-based methods. While guaranteeing high profitability, the algorithm is also light-weighted and can be trained and implemented with minimal computational cost. Our results also show that the learned prediction model has excellent transferability. The prediction model trained using price data from one region also provides good arbitrage results when tested over other regions.

</details>

<details>

<summary>2022-11-20 23:00:23 - Fixing Model Bugs with Natural Language Patches</summary>

- *Shikhar Murty, Christopher D. Manning, Scott Lundberg, Marco Tulio Ribeiro*

- `2211.03318v2` - [abs](http://arxiv.org/abs/2211.03318v2) - [pdf](http://arxiv.org/pdf/2211.03318v2)

> Current approaches for fixing systematic problems in NLP models (e.g. regex patches, finetuning on more data) are either brittle, or labor-intensive and liable to shortcuts. In contrast, humans often provide corrections to each other through natural language. Taking inspiration from this, we explore natural language patches -- declarative statements that allow developers to provide corrective feedback at the right level of abstraction, either overriding the model (``if a review gives 2 stars, the sentiment is negative'') or providing additional information the model may lack (``if something is described as the bomb, then it is good''). We model the task of determining if a patch applies separately from the task of integrating patch information, and show that with a small amount of synthetic data, we can teach models to effectively use real patches on real data -- 1 to 7 patches improve accuracy by ~1-4 accuracy points on different slices of a sentiment analysis dataset, and F1 by 7 points on a relation extraction dataset. Finally, we show that finetuning on as many as 100 labeled examples may be needed to match the performance of a small set of language patches.

</details>

<details>

<summary>2022-11-20 23:34:24 - Beyond Deterministic Translation for Unsupervised Domain Adaptation</summary>

- *Eleni Chiou, Eleftheria Panagiotaki, Iasonas Kokkinos*

- `2202.07778v3` - [abs](http://arxiv.org/abs/2202.07778v3) - [pdf](http://arxiv.org/pdf/2202.07778v3)

> In this work we challenge the common approach of using a one-to-one mapping ('translation') between the source and target domains in unsupervised domain adaptation (UDA). Instead, we rely on stochastic translation to capture inherent translation ambiguities. This allows us to (i) train more accurate target networks by generating multiple outputs conditioned on the same source image, leveraging both accurate translation and data augmentation for appearance variability, (ii) impute robust pseudo-labels for the target data by averaging the predictions of a source network on multiple translated versions of a single target image and (iii) train and ensemble diverse networks in the target domain by modulating the degree of stochasticity in the translations. We report improvements over strong recent baselines, leading to state-of-the-art UDA results on two challenging semantic segmentation benchmarks. Our code is available at https://github.com/elchiou/Beyond-deterministic-translation-for-UDA.

</details>

<details>

<summary>2022-11-21 02:51:16 - A Continuous $hp-$Mesh Model for Discontinuous Petrov-Galerkin Finite Element Schemes with Optimal Test Functions</summary>

- *Ankit Chakraborty, Georg May*

- `2211.11156v1` - [abs](http://arxiv.org/abs/2211.11156v1) - [pdf](http://arxiv.org/pdf/2211.11156v1)

> We present an anisotropic $hp-$mesh adaptation strategy using a continuous mesh model for discontinuous Petrov-Galerkin (DPG) finite element schemes with optimal test functions, extending our previous work on $h-$adaptation. The proposed strategy utilizes the inbuilt residual-based error estimator of the DPG discretization to compute both the polynomial distribution and the anisotropy of the mesh elements. In order to predict the optimal order of approximation, we solve local problems on element patches, thus making these computations highly parallelizable. The continuous mesh model is formulated either with respect to the error in the solution, measured in a suitable norm, or with respect to certain admissible target functionals. We demonstrate the performance of the proposed strategy using several numerical examples on triangular grids.   Keywords: Discontinuous Petrov-Galerkin, Continuous mesh models, $hp-$ adaptations, Anisotropy

</details>

<details>

<summary>2022-11-21 04:15:27 - HARL: Hierarchical Adaptive Reinforcement Learning Based Auto Scheduler for Neural Networks</summary>

- *Zining Zhang, Bingsheng He, Zhenjie Zhang*

- `2211.11172v1` - [abs](http://arxiv.org/abs/2211.11172v1) - [pdf](http://arxiv.org/pdf/2211.11172v1)

> To efficiently perform inference with neural networks, the underlying tensor programs require sufficient tuning efforts before being deployed into production environments. Usually, enormous tensor program candidates need to be sufficiently explored to find the one with the best performance. This is necessary to make the neural network products meet the high demand of real-world applications such as natural language processing, auto-driving, etc. Auto-schedulers are being developed to avoid the need for human intervention. However, due to the gigantic search space and lack of intelligent search guidance, current auto-schedulers require hours to days of tuning time to find the best-performing tensor program for the entire neural network.   In this paper, we propose HARL, a reinforcement learning (RL) based auto-scheduler specifically designed for efficient tensor program exploration. HARL uses a hierarchical RL architecture in which learning-based decisions are made at all different levels of search granularity. It also automatically adjusts exploration configurations in real-time for faster performance convergence. As a result, HARL improves the tensor operator performance by 22% and the search speed by 4.3x compared to the state-of-the-art auto-scheduler. Inference performance and search speed are also significantly improved on end-to-end neural networks.

</details>

<details>

<summary>2022-11-21 05:09:34 - DualApp: Tight Over-Approximation for Neural Network Robustness Verification via Under-Approximation</summary>

- *Yiting Wu, Zhaodi Zhang, Zhiyi Xue, Si Liu, Min Zhang*

- `2211.11186v1` - [abs](http://arxiv.org/abs/2211.11186v1) - [pdf](http://arxiv.org/pdf/2211.11186v1)

> The robustness of neural networks is fundamental to the hosting system's reliability and security. Formal verification has been proven to be effective in providing provable robustness guarantees. To improve the verification scalability, over-approximating the non-linear activation functions in neural networks by linear constraints is widely adopted, which transforms the verification problem into an efficiently solvable linear programming problem. As over-approximations inevitably introduce overestimation, many efforts have been dedicated to defining the tightest possible approximations. Recent studies have however showed that the existing so-called tightest approximations are superior to each other. In this paper we identify and report an crucial factor in defining tight approximations, namely the approximation domains of activation functions. We observe that existing approaches only rely on overestimated domains, while the corresponding tight approximation may not necessarily be tight on its actual domain. We propose a novel under-approximation-guided approach, called dual-approximation, to define tight over-approximations and two complementary under-approximation algorithms based on sampling and gradient descent. The overestimated domain guarantees the soundness while the underestimated one guides the tightness. We implement our approach into a tool called DualApp and extensively evaluate it on a comprehensive benchmark of 84 collected and trained neural networks with different architectures. The experimental results show that DualApp outperforms the state-of-the-art approximation-based approaches, with up to 71.22% improvement to the verification result.

</details>

<details>

<summary>2022-11-21 07:07:19 - Investigating Prompt Engineering in Diffusion Models</summary>

- *Sam Witteveen, Martin Andrews*

- `2211.15462v1` - [abs](http://arxiv.org/abs/2211.15462v1) - [pdf](http://arxiv.org/pdf/2211.15462v1)

> With the spread of the use of Text2Img diffusion models such as DALL-E 2, Imagen, Mid Journey and Stable Diffusion, one challenge that artists face is selecting the right prompts to achieve the desired artistic output. We present techniques for measuring the effect that specific words and phrases in prompts have, and (in the Appendix) present guidance on the selection of prompts to produce desired effects.

</details>

<details>

<summary>2022-11-21 07:20:55 - SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition</summary>

- *Jianing Wang, Chengcheng Han, Chengyu Wang, Chuanqi Tan, Minghui Qiu, Songfang Huang, Jun Huang, Ming Gao*

- `2210.09049v2` - [abs](http://arxiv.org/abs/2210.09049v2) - [pdf](http://arxiv.org/pdf/2210.09049v2)

> Few-shot Named Entity Recognition (NER) aims to identify named entities with very little annotated data. Previous methods solve this problem based on token-wise classification, which ignores the information of entity boundaries, and inevitably the performance is affected by the massive non-entity tokens. To this end, we propose a seminal span-based prototypical network (SpanProto) that tackles few-shot NER via a two-stage approach, including span extraction and mention classification. In the span extraction stage, we transform the sequential tags into a global boundary matrix, enabling the model to focus on the explicit boundary information. For mention classification, we leverage prototypical learning to capture the semantic representations for each labeled span and make the model better adapt to novel-class entities. To further improve the model performance, we split out the false positives generated by the span extractor but not labeled in the current episode set, and then present a margin-based loss to separate them from each prototype region. Experiments over multiple benchmarks demonstrate that our model outperforms strong baselines by a large margin.

</details>

<details>

<summary>2022-11-21 07:24:12 - GUDN: A novel guide network with label reinforcement strategy for extreme multi-label text classification</summary>

- *Qing Wang, Jia Zhu, Hongji Shu, Kwame Omono Asamoah, Jianyang Shi, Cong Zhou*

- `2201.11582v2` - [abs](http://arxiv.org/abs/2201.11582v2) - [pdf](http://arxiv.org/pdf/2201.11582v2)

> In natural language processing, extreme multi-label text classification is an emerging but essential task. The problem of extreme multi-label text classification (XMTC) is to recall some of the most relevant labels for a text from an extremely large label set. Large-scale pre-trained models have brought a new trend to this problem. Though the large-scale pre-trained models have made significant achievements on this problem, the valuable fine-tuned methods have yet to be studied. Though label semantics have been introduced in XMTC, the vast semantic gap between texts and labels has yet to gain enough attention. This paper builds a new guide network (GUDN) to help fine-tune the pre-trained model to instruct classification later. Furthermore, GUDN uses raw label semantics combined with a helpful label reinforcement strategy to effectively explore the latent space between texts and labels, narrowing the semantic gap, which can further improve predicted accuracy. Experimental results demonstrate that GUDN outperforms state-of-the-art methods on Eurlex-4k and has competitive results on other popular datasets. In an additional experiment, we investigated the input lengths' influence on the Transformer-based model's accuracy. Our source code is released at https://t.hk.uy/aFSH.

</details>

<details>

<summary>2022-11-21 07:40:01 - TimbreCLIP: Connecting Timbre to Text and Images</summary>

- *Nicolas Jonason, Bob L. T. Sturm*

- `2211.11225v1` - [abs](http://arxiv.org/abs/2211.11225v1) - [pdf](http://arxiv.org/pdf/2211.11225v1)

> We present work in progress on TimbreCLIP, an audio-text cross modal embedding trained on single instrument notes. We evaluate the models with a cross-modal retrieval task on synth patches. Finally, we demonstrate the application of TimbreCLIP on two tasks: text-driven audio equalization and timbre to image generation.

</details>

<details>

<summary>2022-11-21 08:21:27 - Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models</summary>

- *Ozan Ãzdenizci, Robert Legenstein*

- `2207.14626v2` - [abs](http://arxiv.org/abs/2207.14626v2) - [pdf](http://arxiv.org/pdf/2207.14626v2)

> Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and experimentally show strong generalization to real-world test images.

</details>

<details>

<summary>2022-11-21 08:46:01 - UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition</summary>

- *Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, Yongbin Li*

- `2211.11256v1` - [abs](http://arxiv.org/abs/2211.11256v1) - [pdf](http://arxiv.org/pdf/2211.11256v1)

> Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods.

</details>

<details>

<summary>2022-11-21 08:51:52 - The applicability of transperceptual and deep learning approaches to the study and mimicry of complex cartilaginous tissues</summary>

- *J. Waghorne, C. Howard, H. Hu, J. Pang, W. J. Peveler, L. Harris, O. Barrera*

- `2211.14314v1` - [abs](http://arxiv.org/abs/2211.14314v1) - [pdf](http://arxiv.org/pdf/2211.14314v1)

> Complex soft tissues, for example the knee meniscus, play a crucial role in mobility and joint health, but when damaged are incredibly difficult to repair and replace. This is due to their highly hierarchical and porous nature which in turn leads to their unique mechanical properties. In order to design tissue substitutes, the internal architecture of the native tissue needs to be understood and replicated. Here we explore a combined audio-visual approach - so called transperceptual - to generate artificial architectures mimicking the native ones. The proposed method uses both traditional imagery, and sound generated from each image as a method of rapidly comparing and contrasting the porosity and pore size within the samples. We have trained and tested a generative adversarial network (GAN) on the 2D image stacks. The impact of the training set of images on the similarity of the artificial to the original dataset was assessed by analyzing two samples. The first consisting of n=478 pairs of audio and image files for which the images were downsampled to 64 $\times$ 64 pixels, the second one consisting of n=7640 pairs of audio and image files for which the full resolution 256 $\times$ 256 pixels is retained but each image is divided into 16 squares to maintain the limit of 64 $\times$ 64 pixels required by the GAN. We reconstruct the 2D stacks of artificially generated datasets into 3D objects and run image analysis algorithms to characterize statistically the architectural parameters - pore size, tortuosity and pore connectivity - and compare them with the original dataset. Results show that the artificially generated dataset that undergoes downsampling performs better in terms of parameter matching. Our audiovisual approach has the potential to be extended to larger data sets to explore both how similarities and differences can be audibly recognized across multiple samples.

</details>

<details>

<summary>2022-11-21 10:04:27 - VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</summary>

- *Ajay Jain, Amber Xie, Pieter Abbeel*

- `2211.11319v1` - [abs](http://arxiv.org/abs/2211.11319v1) - [pdf](http://arxiv.org/pdf/2211.11319v1)

> Diffusion models have shown impressive results in text-to-image synthesis. Using massive datasets of captioned images, diffusion models learn to generate raster images of highly diverse objects and scenes. However, designers frequently use vector representations of images like Scalable Vector Graphics (SVGs) for digital icons or art. Vector graphics can be scaled to any size, and are compact. We show that a text-conditioned diffusion model trained on pixel representations of images can be used to generate SVG-exportable vector graphics. We do so without access to large datasets of captioned SVGs. By optimizing a differentiable vector graphics rasterizer, our method, VectorFusion, distills abstract semantic knowledge out of a pretrained diffusion model. Inspired by recent text-to-3D work, we learn an SVG consistent with a caption using Score Distillation Sampling. To accelerate generation and improve fidelity, VectorFusion also initializes from an image sample. Experiments show greater quality than prior work, and demonstrate a range of styles including pixel art and sketches. See our project webpage at https://ajayj.com/vectorfusion .

</details>

<details>

<summary>2022-11-21 10:14:19 - Geometric Model Checking of Continuous Space</summary>

- *Nick Bezhanishvili, Vincenzo Ciancia, David Gabelaia, Gianluca Grilletti, Diego Latella, Mieke Massink*

- `2105.06194v5` - [abs](http://arxiv.org/abs/2105.06194v5) - [pdf](http://arxiv.org/pdf/2105.06194v5)

> Topological Spatial Model Checking is a recent paradigm where model checking techniques are developed for the topological interpretation of Modal Logic. The Spatial Logic of Closure Spaces, SLCS, extends Modal Logic with reachability connectives that, in turn, can be used for expressing interesting spatial properties, such as "being near to" or "being surrounded by". SLCS constitutes the kernel of a solid logical framework for reasoning about discrete space, such as graphs and digital images, interpreted as quasi discrete closure spaces. Following a recently developed geometric semantics of Modal Logic, we propose an interpretation of SLCS in continuous space, admitting a geometric spatial model checking procedure, by resorting to models based on polyhedra. Such representations of space are increasingly relevant in many domains of application, due to recent developments of 3D scanning and visualisation techniques that exploit mesh processing. We introduce PolyLogicA, a geometric spatial model checker for SLCS formulas on polyhedra and demonstrate feasibility of our approach on two 3D polyhedral models of realistic size. Finally, we introduce a geometric definition of bisimilarity, proving that it characterises logical equivalence.

</details>

<details>

<summary>2022-11-21 10:34:43 - OPTION: OPTImization Algorithm Benchmarking ONtology</summary>

- *Ana Kostovska, Diederick Vermetten, Carola Doerr, Saso DÅ¾eroski, PanÄe Panov, Tome Eftimov*

- `2211.11332v1` - [abs](http://arxiv.org/abs/2211.11332v1) - [pdf](http://arxiv.org/pdf/2211.11332v1)

> Many optimization algorithm benchmarking platforms allow users to share their experimental data to promote reproducible and reusable research. However, different platforms use different data models and formats, which drastically complicates the identification of relevant datasets, their interpretation, and their interoperability. Therefore, a semantically rich, ontology-based, machine-readable data model that can be used by different platforms is highly desirable. In this paper, we report on the development of such an ontology, which we call OPTION (OPTImization algorithm benchmarking ONtology). Our ontology provides the vocabulary needed for semantic annotation of the core entities involved in the benchmarking process, such as algorithms, problems, and evaluation measures. It also provides means for automatic data integration, improved interoperability, and powerful querying capabilities, thereby increasing the value of the benchmarking data. We demonstrate the utility of OPTION, by annotating and querying a corpus of benchmark performance data from the BBOB collection of the COCO framework and from the Yet Another Black-Box Optimization Benchmark (YABBOB) family of the Nevergrad environment. In addition, we integrate features of the BBOB functional performance landscape into the OPTION knowledge base using publicly available datasets with exploratory landscape analysis. Finally, we integrate the OPTION knowledge base into the IOHprofiler environment and provide users with the ability to perform meta-analysis of performance data.

</details>

<details>

<summary>2022-11-21 10:54:10 - Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy</summary>

- *Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli*

- `2106.01100v6` - [abs](http://arxiv.org/abs/2106.01100v6) - [pdf](http://arxiv.org/pdf/2106.01100v6)

> During lung radiotherapy, the position of infrared reflective objects on the chest can be recorded to estimate the tumor location. However, radiotherapy systems have a latency inherent to robot control limitations that impedes the radiation delivery precision. Prediction with online learning of recurrent neural networks (RNN) allows for adaptation to non-stationary respiratory signals, but classical methods such as RTRL and truncated BPTT are respectively slow and biased. This study investigates the capabilities of unbiased online recurrent optimization (UORO) to forecast respiratory motion and enhance safety in lung radiotherapy.   We used 9 observation records of the 3D position of 3 external markers on the chest and abdomen of healthy individuals breathing during intervals from 73s to 222s. The sampling frequency was 10Hz, and the amplitudes of the recorded trajectories range from 6mm to 40mm in the superior-inferior direction. We forecast the 3D location of each marker simultaneously with a horizon value between 0.1s and 2.0s, using an RNN trained with UORO. We compare its performance with an RNN trained with RTRL, LMS, and offline linear regression. We provide closed-form expressions for quantities involved in the loss gradient calculation in UORO, thereby making its implementation efficient. Training and cross-validation were performed during the first minute of each sequence.   On average over the horizon values considered and the 9 sequences, UORO achieves the lowest root-mean-square (RMS) error and maximum error among the compared algorithms. These errors are respectively equal to 1.3mm and 8.8mm, and the prediction time per time step was lower than 2.8ms (Dell Intel core i9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s, and UORO for horizon values greater than 0.6s.

</details>

<details>

<summary>2022-11-21 11:57:46 - Towards Trace-based Deductive Verification (Tech Report)</summary>

- *Richard Bubel, Dilian Gurov, Reiner HÃ¤hnle, Marco Scaletta*

- `2211.09487v2` - [abs](http://arxiv.org/abs/2211.09487v2) - [pdf](http://arxiv.org/pdf/2211.09487v2)

> Contracts specifying a procedure's behavior in terms of pre- and postconditions are essential for scalable software verification, but cannot express any constraints on the events occurring during execution of the procedure. This necessitates to annotate code with intermediate assertions, preventing full specification abstraction.   We propose a logic over symbolic traces able to specify recursive procedures in a modular manner that refers to specified programs only in terms of events. We also provide a deduction system based on symbolic execution and induction that we prove to be sound relative to a trace semantics.   Our work generalizes contract-based to trace-based deductive verification.

</details>

<details>

<summary>2022-11-21 13:24:55 - PatchNR: Learning from Very Few Images by Patch Normalizing Flow Regularization</summary>

- *Fabian AltekrÃ¼ger, Alexander Denker, Paul Hagemann, Johannes Hertrich, Peter Maass, Gabriele Steidl*

- `2205.12021v3` - [abs](http://arxiv.org/abs/2205.12021v3) - [pdf](http://arxiv.org/pdf/2205.12021v3)

> Learning neural networks using only few available information is an important ongoing research topic with tremendous potential for applications. In this paper, we introduce a powerful regularizer for the variational modeling of inverse problems in imaging. Our regularizer, called patch normalizing flow regularizer (patchNR), involves a normalizing flow learned on small patches of very few images. In particular, the training is independent of the considered inverse problem such that the same regularizer can be applied for different forward operators acting on the same class of images. By investigating the distribution of patches versus those of the whole image class, we prove that our model is indeed a MAP approach. Numerical examples for low-dose and limited-angle computed tomography (CT) as well as superresolution of material images demonstrate that our method provides very high quality results. The training set consists of just six images for CT and one image for superresolution. Finally, we combine our patchNR with ideas from internal learning for performing superresolution of natural images directly from the low-resolution observation without knowledge of any high-resolution image.

</details>

<details>

<summary>2022-11-21 13:31:39 - Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code</summary>

- *Zhaowei Zhang, Hongyu Zhang, Beijun Shen, Xiaodong Gu*

- `2206.14390v5` - [abs](http://arxiv.org/abs/2206.14390v5) - [pdf](http://arxiv.org/pdf/2206.14390v5)

> Pre-trained code representation models such as CodeBERT have demonstrated superior performance in a variety of software engineering tasks, yet they are often heavy in complexity, quadratically with the length of the input sequence. Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more attention to certain types of tokens and statements such as keywords and data-relevant statements. Based on these findings, we propose DietCode, which aims at lightweight leverage of large pre-trained models for source code. DietCode simplifies the input program of CodeBERT with three strategies, namely, word dropout, frequency filtering, and an attention-based strategy which selects statements and tokens that receive the most attention weights during pre-training. Hence, it gives a substantial reduction in the computational cost without hampering the model performance. Experimental results on two downstream tasks show that DietCodeBERT provides comparable results to CodeBERT with 40% less computational cost in fine-tuning and testing.

</details>

<details>

<summary>2022-11-21 13:33:05 - (B)LOCKBOX -- Secure Software Architecture with Blockchain Verification</summary>

- *Erik Heiland, Peter Hillmann*

- `2211.11444v1` - [abs](http://arxiv.org/abs/2211.11444v1) - [pdf](http://arxiv.org/pdf/2211.11444v1)

> According to experts, one third of all IT vulnerabilities today are due to inadequate software verification. Internal program processes are not sufficiently secured against manipulation by attackers, especially if access has been gained. There is a lack of internal control instances that can monitor and control program flows. Especially when a software vulnerability becomes known, quick action is required, whereby the consequences for an individual application are often not foreseeable. With our approach (B)LOCKBOX, software building blocks act as verified entities within a transaction-based blockchain network. Source Code, binaries and application execution become supervised. Unwanted interference and manipulation are prevented by the integrity of the distributed system.

</details>

<details>

<summary>2022-11-21 14:59:21 - Variable-Based Fault Localization via Enhanced Decision Tree</summary>

- *Jiajun Jiang, Yumeng Wang, Junjie Chen, Delin Lv, Mengjiao Liu*

- `2211.11526v1` - [abs](http://arxiv.org/abs/2211.11526v1) - [pdf](http://arxiv.org/pdf/2211.11526v1)

> Fault localization, aiming at localizing the root cause of the bug under repair, has been a longstanding research topic. Although many approaches have been proposed in the last decades, most of the existing studies work at coarse-grained statement or method levels with very limited insights about how to repair the bug (granularity problem), but few studies target the finer-grained fault localization. In this paper, we target the granularity problem and propose a novel finer-grained variable-level fault localization technique. Specifically, we design a program-dependency-enhanced decision tree model to boost the identification of fault-relevant variables via discriminating failed and passed test cases based on the variable values. To evaluate the effectiveness of our approach, we have implemented it in a tool called VARDT and conducted an extensive study over the Defects4J benchmark. The results show that VARDT outperforms the state-of-the-art fault localization approaches with at least 247.8% improvements in terms of bugs located at Top-1, and the average improvements are 330.5%.   Besides, to investigate whether our finer-grained fault localization result can further improve the effectiveness of downstream APR techniques, we have adapted VARDT to the application of patch filtering, where VARDT outperforms the state-of-the-art PATCH-SIM by filtering 26.0% more incorrect patches. The results demonstrate the effectiveness of our approach and it also provides a new way of thinking for improving automatic program repair techniques.

</details>

<details>

<summary>2022-11-21 16:03:16 - Semantic Segmentation for Fully Automated Macrofouling Analysis on Coatings after Field Exposure</summary>

- *Lutz M. K. Krause, Emily Manderfeld, Patricia Gnutt, Louisa Vogler, Ann Wassick, Kailey Richard, Marco Rudolph, Kelli Z. Hunsucker, Geoffrey W. Swain, Bodo Rosenhahn, Axel Rosenhahn*

- `2211.11607v1` - [abs](http://arxiv.org/abs/2211.11607v1) - [pdf](http://arxiv.org/pdf/2211.11607v1)

> Biofouling is a major challenge for sustainable shipping, filter membranes, heat exchangers, and medical devices. The development of fouling-resistant coatings requires the evaluation of their effectiveness. Such an evaluation is usually based on the assessment of fouling progression after different exposure times to the target medium (e.g., salt water). The manual assessment of macrofouling requires expert knowledge about local fouling communities due to high variances in phenotypical appearance, has single-image sampling inaccuracies for certain species, and lacks spatial information. Here we present an approach for automatic image-based macrofouling analysis. We created a dataset with dense labels prepared from field panel images and propose a convolutional network (adapted U-Net) for the semantic segmentation of different macrofouling classes. The establishment of macrofouling localization allows for the generation of a successional model which enables the determination of direct surface attachment and in-depth epibiotic studies.

</details>

<details>

<summary>2022-11-21 16:53:11 - Backward Reachability Analysis for Neural Feedback Loops</summary>

- *Nicholas Rober, Michael Everett, Jonathan P. How*

- `2204.08319v2` - [abs](http://arxiv.org/abs/2204.08319v2) - [pdf](http://arxiv.org/pdf/2204.08319v2)

> The increasing prevalence of neural networks (NNs) in safety-critical applications calls for methods to certify their behavior and guarantee safety. This paper presents a backward reachability approach for safety verification of neural feedback loops (NFLs), i.e., closed-loop systems with NN control policies. While recent works have focused on forward reachability as a strategy for safety certification of NFLs, backward reachability offers advantages over the forward strategy, particularly in obstacle avoidance scenarios. Prior works have developed techniques for backward reachability analysis for systems without NNs, but the presence of NNs in the feedback loop presents a unique set of problems due to the nonlinearities in their activation functions and because NN models are generally not invertible. To overcome these challenges, we use existing forward NN analysis tools to find affine bounds on the control inputs and solve a series of linear programs (LPs) to efficiently find an approximation of the backprojection (BP) set, i.e., the set of states for which the NN control policy will drive the system to a given target set. We present an algorithm to iteratively find BP set estimates over a given time horizon and demonstrate the ability to reduce conservativeness in the BP set estimates by up to 88% with low additional computational cost. We use numerical results from a double integrator model to verify the efficacy of these algorithms and demonstrate the ability to certify safety for a linearized ground robot model in a collision avoidance scenario where forward reachability fails.

</details>

<details>

<summary>2022-11-21 19:05:06 - Unsupervised extraction, labelling and clustering of segments from clinical notes</summary>

- *Petr Zelina, Jana HalÃ¡mkovÃ¡, VÃ­t NovÃ¡Äek*

- `2211.11799v1` - [abs](http://arxiv.org/abs/2211.11799v1) - [pdf](http://arxiv.org/pdf/2211.11799v1)

> This work is motivated by the scarcity of tools for accurate, unsupervised information extraction from unstructured clinical notes in computationally underrepresented languages, such as Czech. We introduce a stepping stone to a broad array of downstream tasks such as summarisation or integration of individual patient records, extraction of structured information for national cancer registry reporting or building of semi-structured semantic patient representations for computing patient embeddings. More specifically, we present a method for unsupervised extraction of semantically-labelled textual segments from clinical notes and test it out on a dataset of Czech breast cancer patients, provided by Masaryk Memorial Cancer Institute (the largest Czech hospital specialising in oncology). Our goal was to extract, classify (i.e. label) and cluster segments of the free-text notes that correspond to specific clinical features (e.g., family background, comorbidities or toxicities). The presented results demonstrate the practical relevance of the proposed approach for building more sophisticated extraction and analytical pipelines deployed on Czech clinical notes.

</details>

<details>

<summary>2022-11-21 22:01:36 - Addressing Mistake Severity in Neural Networks with Semantic Knowledge</summary>

- *Natalie Abreu, Nathan Vaska, Victoria Helus*

- `2211.11880v1` - [abs](http://arxiv.org/abs/2211.11880v1) - [pdf](http://arxiv.org/pdf/2211.11880v1)

> Robustness in deep neural networks and machine learning algorithms in general is an open research challenge. In particular, it is difficult to ensure algorithmic performance is maintained on out-of-distribution inputs or anomalous instances that cannot be anticipated at training time. Embodied agents will be deployed in these conditions, and are likely to make incorrect predictions. An agent will be viewed as untrustworthy unless it can maintain its performance in dynamic environments. Most robust training techniques aim to improve model accuracy on perturbed inputs; as an alternate form of robustness, we aim to reduce the severity of mistakes made by neural networks in challenging conditions. We leverage current adversarial training methods to generate targeted adversarial attacks during the training process in order to increase the semantic similarity between a model's predictions and true labels of misclassified instances. Results demonstrate that our approach performs better with respect to mistake severity compared to standard and adversarially trained models. We also find an intriguing role that non-robust features play with regards to semantic similarity.

</details>

<details>

<summary>2022-11-21 22:16:52 - CodEval: Improving Student Success In Programming Assignments</summary>

- *Aditi Agrawal, Archit Jain, Benjamin Reed*

- `2211.11883v1` - [abs](http://arxiv.org/abs/2211.11883v1) - [pdf](http://arxiv.org/pdf/2211.11883v1)

> CodEval is a code evaluation tool that integrates with the Canvas Learning Management System to automatically evaluates students' work within a few minutes of the submission. This early feedback allows students to catch and correct problems in their submissions before their submission is graded and gives them a clear idea of the quality of their submission. CodEval handles the tedious aspects of grading, such as compiling and running tests, leaving graders more time to spend on the qualitative aspect of grading.   Before using CodEval, instructors would not have a clear view of the student's comprehension of the concept evaluated by the assignment until after the due date. CodeEval helps instructors identify and address the gaps in students' understanding and thus helps more students successfully complete the assignment.   We implemented CodEval using Python using the public Canvas API. Any instructor or grader for a Canvas course can use CodEval to automatically evaluate submissions for programming assignments. We developed a syntax to express requirements of submissions such as compilation parameters, inputs, outputs, command-line arguments, timeouts, exit codes, functions used, files generated, output validators, and more. We have made CodEval open source.   CodEval is an easy tool for students, graders, and instructors and seamlessly integrates with Canvas. We share our experience with using CodEval in two classes with a total of 90 students and multiple coding assignments.

</details>

<details>

<summary>2022-11-21 23:25:41 - Contract-Based Specification Refinement and Repair for Mission Planning</summary>

- *Piergiuseppe Mallozzi, Inigo Incer, Pierluigi Nuzzo, Alberto Sangiovanni-Vincentelli*

- `2211.11908v1` - [abs](http://arxiv.org/abs/2211.11908v1) - [pdf](http://arxiv.org/pdf/2211.11908v1)

> We address the problem of modeling, refining, and repairing formal specifications for robotic missions using assume-guarantee contracts. We show how to model mission specifications at various levels of abstraction and implement them using a library of pre-implemented specifications. Suppose the specification cannot be met using components from the library. In that case, we compute a proxy for the best approximation to the specification that can be generated using elements from the library. Afterward, we propose a systematic way to either 1) search for and refine the `missing part' of the specification that the library cannot meet or 2) repair the current specification such that the existing library can refine it. Our methodology for searching and repairing mission requirements leverages the quotient, separation, composition, and merging operations between contracts.

</details>

<details>

<summary>2022-11-22 01:30:40 - Synthetic Data for Semantic Image Segmentation of Imagery of Unmanned Spacecraft</summary>

- *William S. Armstrong, Spencer Drakontaidis, Nicholas Lui*

- `2211.11941v1` - [abs](http://arxiv.org/abs/2211.11941v1) - [pdf](http://arxiv.org/pdf/2211.11941v1)

> Images of spacecraft photographed from other spacecraft operating in outer space are difficult to come by, especially at a scale typically required for deep learning tasks. Semantic image segmentation, object detection and localization, and pose estimation are well researched areas with powerful results for many applications, and would be very useful in autonomous spacecraft operation and rendezvous. However, recent studies show that these strong results in broad and common domains may generalize poorly even to specific industrial applications on earth. To address this, we propose a method for generating synthetic image data that are labelled for semantic segmentation, generalizable to other tasks, and provide a prototype synthetic image dataset consisting of 2D monocular images of unmanned spacecraft, in order to enable further research in the area of autonomous spacecraft rendezvous. We also present a strong benchmark result (S{\o}rensen-Dice coefficient 0.8723) on these synthetic data, suggesting that it is feasible to train well-performing image segmentation models for this task, especially if the target spacecraft and its configuration are known.

</details>

<details>

<summary>2022-11-22 02:09:38 - Re-Imagen: Retrieval-Augmented Text-to-Image Generator</summary>

- *Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen*

- `2209.14491v3` - [abs](http://arxiv.org/abs/2209.14491v3) - [pdf](http://arxiv.org/pdf/2209.14491v3)

> Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as `Chortai (dog)' or `Picarones (food)'. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs and uses them as references to generate the image. With this retrieval step, Re-Imagen is augmented with the knowledge of high-level semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities' visual appearances. We train Re-Imagen on a constructed dataset containing (image, text, retrieval) triples to teach the model to ground on both text prompt and retrieval. Furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval conditions to balance the text and retrieval alignment. Re-Imagen achieves significant gain on FID score over COCO and WikiImage. To further evaluate the capabilities of the model, we introduce EntityDrawBench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple object categories including dogs, foods, landmarks, birds, and characters. Human evaluation on EntityDrawBench shows that Re-Imagen can significantly improve the fidelity of generated images, especially on less frequent entities.

</details>

<details>

<summary>2022-11-22 04:55:58 - Deep-Learning-Based Computer Vision Approach For The Segmentation Of Ball Deliveries And Tracking In Cricket</summary>

- *Kumail Abbas, Muhammad Saeed, M. Imad Khan, Khandakar Ahmed, Hua Wang*

- `2211.12009v1` - [abs](http://arxiv.org/abs/2211.12009v1) - [pdf](http://arxiv.org/pdf/2211.12009v1)

> There has been a significant increase in the adoption of technology in cricket recently. This trend has created the problem of duplicate work being done in similar computer vision-based research works. Our research tries to solve one of these problems by segmenting ball deliveries in a cricket broadcast using deep learning models, MobileNet and YOLO, thus enabling researchers to use our work as a dataset for their research. The output from our research can be used by cricket coaches and players to analyze ball deliveries which are played during the match. This paper presents an approach to segment and extract video shots in which only the ball is being delivered. The video shots are a series of continuous frames that make up the whole scene of the video. Object detection models are applied to reach a high level of accuracy in terms of correctly extracting video shots. The proof of concept for building large datasets of video shots for ball deliveries is proposed which paves the way for further processing on those shots for the extraction of semantics. Ball tracking in these video shots is also done using a separate RetinaNet model as a sample of the usefulness of the proposed dataset. The position on the cricket pitch where the ball lands is also extracted by tracking the ball along the y-axis. The video shot is then classified as a full-pitched, good-length or short-pitched delivery.

</details>

<details>

<summary>2022-11-22 06:08:57 - BASM: A Bottom-up Adaptive Spatiotemporal Model for Online Food Ordering Service</summary>

- *Boya Du, Shaochuan Lin, Jiong Gao, Xiyu Ji, Mengya Wang, Taotao Zhou, Hengxu He, Jia Jia, Ning Hu*

- `2211.12033v1` - [abs](http://arxiv.org/abs/2211.12033v1) - [pdf](http://arxiv.org/pdf/2211.12033v1)

> Online Food Ordering Service (OFOS) is a popular location-based service that helps people to order what you want. Compared with traditional e-commerce recommendation systems, users' interests may be diverse under different spatiotemporal contexts, leading to various spatiotemporal data distribution, which limits the fitting capacity of the model. However, numerous current works simply mix all samples to train a set of model parameters, which makes it difficult to capture the diversity in different spatiotemporal contexts. Therefore, we address this challenge by proposing a Bottom-up Adaptive Spatiotemporal Model(BASM) to adaptively fit the spatiotemporal data distribution, which further improve the fitting capability of the model. Specifically, a spatiotemporal-aware embedding layer performs weight adaptation on field granularity in feature embedding, to achieve the purpose of dynamically perceiving spatiotemporal contexts. Meanwhile, we propose a spatiotemporal semantic transformation layer to explicitly convert the concatenated input of the raw semantic to spatiotemporal semantic, which can further enhance the semantic representation under different spatiotemporal contexts. Furthermore, we introduce a novel spatiotemporal adaptive bias tower to capture diverse spatiotemporal bias, reducing the difficulty to model spatiotemporal distinction. To further verify the effectiveness of BASM, we also novelly propose two new metrics, Time-period-wise AUC (TAUC) and City-wise AUC (CAUC). Extensive offline evaluations on public and industrial datasets are conducted to demonstrate the effectiveness of our proposed modle. The online A/B experiment also further illustrates the practicability of the model online service. This proposed method has now been implemented on the Ele.me, a major online food ordering platform in China, serving more than 100 million online users.

</details>

<details>

<summary>2022-11-22 08:14:07 - Benchmarking Evaluation Metrics for Code-Switching Automatic Speech Recognition</summary>

- *Injy Hamed, Amir Hussein, Oumnia Chellah, Shammur Chowdhury, Hamdy Mubarak, Sunayana Sitaram, Nizar Habash, Ahmed Ali*

- `2211.16319v1` - [abs](http://arxiv.org/abs/2211.16319v1) - [pdf](http://arxiv.org/pdf/2211.16319v1)

> Code-switching poses a number of challenges and opportunities for multilingual automatic speech recognition. In this paper, we focus on the question of robust and fair evaluation metrics. To that end, we develop a reference benchmark data set of code-switching speech recognition hypotheses with human judgments. We define clear guidelines for minimal editing of automatic hypotheses. We validate the guidelines using 4-way inter-annotator agreement. We evaluate a large number of metrics in terms of correlation with human judgments. The metrics we consider vary in terms of representation (orthographic, phonological, semantic), directness (intrinsic vs extrinsic), granularity (e.g. word, character), and similarity computation method. The highest correlation to human judgment is achieved using transliteration followed by text normalization. We release the first corpus for human acceptance of code-switching speech recognition results in dialectal Arabic/English conversation speech.

</details>

<details>

<summary>2022-11-22 10:42:07 - OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type</summary>

- *Suresh Kumar, P Sreenivasa Kumar*

- `2211.12164v1` - [abs](http://arxiv.org/abs/2211.12164v1) - [pdf](http://arxiv.org/pdf/2211.12164v1)

> Machine generation of Arithmetic Word Problems (AWPs) is challenging as they express quantities and mathematical relationships and need to be consistent. ML-solvers require a large annotated training set of consistent problems with language variations. Exploiting domain-knowledge is needed for consistency checking whereas LSTM-based approaches are good for producing text with language variations. Combining these we propose a system, OLGA, to generate consistent word problems of TC (Transfer-Case) type, involving object transfers among agents. Though we provide a dataset of consistent 2-agent TC-problems for training, only about 36% of the outputs of an LSTM-based generator are found consistent. We use an extension of TC-Ontology, proposed by us previously, to determine the consistency of problems. Among the remaining 64%, about 40% have minor errors which we repair using the same ontology. To check consistency and for the repair process, we construct an instance-specific representation (ABox) of an auto-generated problem. We use a sentence classifier and BERT models for this task. The training set for these LMs is problem-texts where sentence-parts are annotated with ontology class-names. As three-agent problems are longer, the percentage of consistent problems generated by an LSTM-based approach drops further. Hence, we propose an ontology-based method that extends consistent 2-agent problems into consistent 3-agent problems. Overall, our approach generates a large number of consistent TC-type AWPs involving 2 or 3 agents. As ABox has all the information of a problem, any annotations can also be generated. Adopting the proposed approach to generate other types of AWPs is interesting future work.

</details>

<details>

<summary>2022-11-22 11:32:05 - A Combined Approach of Process Mining and Rule-based AI for Study Planning and Monitoring in Higher Education</summary>

- *Miriam Wagner, Hayyan Helal, Rene Roepke, Sven Judel, Jens Doveren, Sergej Goerzen, Pouya Soudmand, Gerhard Lakemeyer, Ulrik Schroeder, Wil van der Aalst*

- `2211.12190v1` - [abs](http://arxiv.org/abs/2211.12190v1) - [pdf](http://arxiv.org/pdf/2211.12190v1)

> This paper presents an approach of using methods of process mining and rule-based artificial intelligence to analyze and understand study paths of students based on campus management system data and study program models. Process mining techniques are used to characterize successful study paths, as well as to detect and visualize deviations from expected plans. These insights are combined with recommendations and requirements of the corresponding study programs extracted from examination regulations. Here, event calculus and answer set programming are used to provide models of the study programs which support planning and conformance checking while providing feedback on possible study plan violations. In its combination, process mining and rule-based artificial intelligence are used to support study planning and monitoring by deriving rules and recommendations for guiding students to more suitable study paths with higher success rates. Two applications will be implemented, one for students and one for study program designers.

</details>

<details>

<summary>2022-11-22 12:24:22 - A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding</summary>

- *Lizhi Cheng, Wenmian Yang, Weijia Jia*

- `2211.12220v1` - [abs](http://arxiv.org/abs/2211.12220v1) - [pdf](http://arxiv.org/pdf/2211.12220v1)

> Multi-Intent Spoken Language Understanding (SLU), a novel and more complex scenario of SLU, is attracting increasing attention. Unlike traditional SLU, each intent in this scenario has its specific scope. Semantic information outside the scope even hinders the prediction, which tremendously increases the difficulty of intent detection. More seriously, guiding slot filling with these inaccurate intent labels suffers error propagation problems, resulting in unsatisfied overall performance. To solve these challenges, in this paper, we propose a novel Scope-Sensitive Result Attention Network (SSRAN) based on Transformer, which contains a Scope Recognizer (SR) and a Result Attention Network (RAN). Scope Recognizer assignments scope information to each token, reducing the distraction of out-of-scope tokens. Result Attention Network effectively utilizes the bidirectional interaction between results of slot filling and intent detection, mitigating the error propagation problem. Experiments on two public datasets indicate that our model significantly improves SLU performance (5.4\% and 2.1\% on Overall accuracy) over the state-of-the-art baseline.

</details>

<details>

<summary>2022-11-22 12:32:38 - Reversible Programming: A Case Study of Two String-Matching Algorithms</summary>

- *Robert GlÃ¼ck, Tetsuo Yokoyama*

- `2211.12225v1` - [abs](http://arxiv.org/abs/2211.12225v1) - [pdf](http://arxiv.org/pdf/2211.12225v1)

> String matching is a fundamental problem in algorithm. This study examines the development and construction of two reversible string-matching algorithms: a naive string-matching algorithm and the Rabin-Karp algorithm. The algorithms are used to introduce reversible computing concepts, beginning from basic reversible programming techniques to more advanced considerations about the injectivization of the polynomial hash-update function employed by the Rabin-Karp algorithm. The results are two clean input-preserving reversible algorithms that require no additional space and have the same asymptotic time complexity as their classic irreversible originals. This study aims to contribute to the body of reversible algorithms and to the discipline of reversible programming.

</details>

<details>

<summary>2022-11-22 12:35:38 - OptiRica: Towards an Efficient Optimizing Horn Solver</summary>

- *Hossein Hojjat, Philipp RÃ¼mmer*

- `2211.12229v1` - [abs](http://arxiv.org/abs/2211.12229v1) - [pdf](http://arxiv.org/pdf/2211.12229v1)

> This paper describes an ongoing effort to develop an optimizing version of the Eldarica Horn solver. The work starts from the observation that many kinds of optimization problems, and in particular the MaxSAT/SMT problem, can be seen as search problems on lattices. The paper presents a Scala library providing a domain-specific language (DSL) to uniformly model optimization problems of this kind, by defining, manipulating, and systematically exploring lattices with associated objective functions. The framework can be instantiated to obtain an optimizing Horn solver. As an illustration, the application of an optimizing solver for repairing software-defined networks is described.

</details>

<details>

<summary>2022-11-22 12:39:37 - Toward a Fairness-Aware Scoring System for Algorithmic Decision-Making</summary>

- *Yi Yang, Ying Wu, Mei Li, Xiangyu Chang, Yong Tan*

- `2109.10053v4` - [abs](http://arxiv.org/abs/2109.10053v4) - [pdf](http://arxiv.org/pdf/2109.10053v4)

> Scoring systems, as a type of predictive model, have significant advantages in interpretability and transparency and facilitate quick decision-making. As such, scoring systems have been extensively used in a wide variety of industries such as healthcare and criminal justice. However, the fairness issues in these models have long been criticized, and the use of big data and machine learning algorithms in the construction of scoring systems heightens this concern. In this paper, we propose a general framework to create fairness-aware, data-driven scoring systems. First, we develop a social welfare function that incorporates both efficiency and group fairness. Then, we transform the social welfare maximization problem into the risk minimization task in machine learning, and derive a fairness-aware scoring system with the help of mixed integer programming. Lastly, several theoretical bounds are derived for providing parameter selection suggestions. Our proposed framework provides a suitable solution to address group fairness concerns in the development of scoring systems. It enables policymakers to set and customize their desired fairness requirements as well as other application-specific constraints. We test the proposed algorithm with several empirical data sets. Experimental evidence supports the effectiveness of the proposed scoring system in achieving the optimal welfare of stakeholders and in balancing the needs for interpretability, fairness, and efficiency.

</details>

<details>

<summary>2022-11-22 15:00:56 - TLP: A Deep Learning-based Cost Model for Tensor Program Tuning</summary>

- *Yi Zhai, Yu Zhang, Shuo Liu, Xiaomeng Chu, Jie Peng, Jianmin Ji, Yanyong Zhang*

- `2211.03578v2` - [abs](http://arxiv.org/abs/2211.03578v2) - [pdf](http://arxiv.org/pdf/2211.03578v2)

> Tensor program tuning is a non-convex objective optimization problem, to which search-based approaches have proven to be effective. At the core of the search-based approaches lies the design of the cost model. Though deep learning-based cost models perform significantly better than other methods, they still fall short and suffer from the following problems. First, their feature extraction heavily relies on expert-level domain knowledge in hardware architectures. Even so, the extracted features are often unsatisfactory and require separate considerations for CPUs and GPUs. Second, a cost model trained on one hardware platform usually performs poorly on another, a problem we call cross-hardware unavailability.   In order to address these problems, we propose TLP and MTLTLP. TLP is a deep learning-based cost model that facilitates tensor program tuning. Instead of extracting features from the tensor program itself, TLP extracts features from the schedule primitives. We treat schedule primitives as tensor languages. TLP is thus a Tensor Language Processing task. In this way, the task of predicting the tensor program latency through the cost model is transformed into a natural language processing (NLP) regression task. MTL-TLP combines Multi-Task Learning and TLP to cope with the cross-hardware unavailability problem.   We incorporate these techniques into the Ansor framework and conduct detailed experiments. Results show that TLP can speed up the average search time by 9.1X and 3.0X on CPU and GPU workloads, respectively, compared to the state-of-the-art implementation. MTL-TLP can achieve a speed-up of 4.7X and 2.9X on CPU and GPU workloads, respectively, using only 7% of the target hardware data.

</details>

<details>

<summary>2022-11-22 15:03:48 - Interpreting Neural Networks through the Polytope Lens</summary>

- *Sid Black, Lee Sharkey, Leo Grinsztajn, Eric Winsor, Dan Braun, Jacob Merizian, Kip Parker, Carlos RamÃ³n Guevara, Beren Millidge, Gabriel Alfour, Connor Leahy*

- `2211.12312v1` - [abs](http://arxiv.org/abs/2211.12312v1) - [pdf](http://arxiv.org/pdf/2211.12312v1)

> Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.

</details>

<details>

<summary>2022-11-22 16:34:14 - The Burer-Monteiro SDP method can fail even above the Barvinok-Pataki bound</summary>

- *Liam O'Carroll, Vaidehi Srinivas, Aravindan Vijayaraghavan*

- `2211.12389v1` - [abs](http://arxiv.org/abs/2211.12389v1) - [pdf](http://arxiv.org/pdf/2211.12389v1)

> The most widely used technique for solving large-scale semidefinite programs (SDPs) in practice is the non-convex Burer-Monteiro method, which explicitly maintains a low-rank SDP solution for memory efficiency. There has been much recent interest in obtaining a better theoretical understanding of the Burer-Monteiro method. When the maximum allowed rank $p$ of the SDP solution is above the Barvinok-Pataki bound (where a globally optimal solution of rank at most $p$ is guaranteed to exist), a recent line of work established convergence to a global optimum for generic or smoothed instances of the problem. However, it was open whether there even exists an instance in this regime where the Burer-Monteiro method fails. We prove that the Burer-Monteiro method can fail for the Max-Cut SDP on $n$ vertices when the rank is above the Barvinok-Pataki bound ($p \ge \sqrt{2n}$). We provide a family of instances that have spurious local minima even when the rank $p = n/2$. Combined with existing guarantees, this settles the question of the existence of spurious local minima for the Max-Cut formulation in all ranges of the rank and justifies the use of beyond worst-case paradigms like smoothed analysis to obtain guarantees for the Burer-Monteiro method.

</details>

<details>

<summary>2022-11-22 17:52:36 - Learning context-aware adaptive solvers to accelerate quadratic programming</summary>

- *Haewon Jung, Junyoung Park, Jinkyoo Park*

- `2211.12443v1` - [abs](http://arxiv.org/abs/2211.12443v1) - [pdf](http://arxiv.org/pdf/2211.12443v1)

> Convex quadratic programming (QP) is an important sub-field of mathematical optimization. The alternating direction method of multipliers (ADMM) is a successful method to solve QP. Even though ADMM shows promising results in solving various types of QP, its convergence speed is known to be highly dependent on the step-size parameter $\rho$. Due to the absence of a general rule for setting $\rho$, it is often tuned manually or heuristically. In this paper, we propose CA-ADMM (Context-aware Adaptive ADMM)) which learns to adaptively adjust $\rho$ to accelerate ADMM. CA-ADMM extracts the spatio-temporal context, which captures the dependency of the primal and dual variables of QP and their temporal evolution during the ADMM iterations. CA-ADMM chooses $\rho$ based on the extracted context. Through extensive numerical experiments, we validated that CA-ADMM effectively generalizes to unseen QP problems with different sizes and classes (i.e., having different QP parameter structures). Furthermore, we verified that CA-ADMM could dynamically adjust $\rho$ considering the stage of the optimization process to accelerate the convergence speed further.

</details>

<details>

<summary>2022-11-22 18:12:53 - KnowGL: Knowledge Generation and Linking from Text</summary>

- *Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, Owen Cornec, Alfio Massimiliano Gliozzo*

- `2210.13952v5` - [abs](http://arxiv.org/abs/2210.13952v5) - [pdf](http://arxiv.org/pdf/2210.13952v5)

> We propose KnowGL, a tool that allows converting text into structured relational data represented as a set of ABox assertions compliant with the TBox of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a sequence generation task by leveraging pre-trained sequence-to-sequence language models, e.g. BART. Given a sentence, we fine-tune such models to detect pairs of entity mentions and jointly generate a set of facts consisting of the full set of semantic annotations for a KG, such as entity labels, entity types, and their relationships. To showcase the capabilities of our tool, we build a web application consisting of a set of UI widgets that help users to navigate through the semantic data extracted from a given input text. We make the KnowGL model available at https://huggingface.co/ibm/knowgl-large.

</details>

<details>

<summary>2022-11-22 20:39:18 - Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</summary>

- *Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel*

- `2211.12572v1` - [abs](http://arxiv.org/abs/2211.12572v1) - [pdf](http://arxiv.org/pdf/2211.12572v1)

> Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, allowing us to synthesize diverse images that convey highly complex visual concepts. However, a pivotal challenge in leveraging such models for real-world content creation tasks is providing users with control over the generated content. In this paper, we present a new framework that takes text-to-image synthesis to the realm of image-to-image translation -- given a guidance image and a target text prompt, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that complies with the target text, while preserving the semantic layout of the source image. Specifically, we observe and empirically demonstrate that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the target image, requiring no training or fine-tuning and applicable for both real or generated guidance images. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing of the class and appearance of objects in a given image, and modifications of global qualities such as lighting and color.

</details>

<details>

<summary>2022-11-22 20:42:49 - BERN-NN: Tight Bound Propagation For Neural Networks Using Bernstein Polynomial Interval Arithmetic</summary>

- *Wael Fatnassi, Haitham Khedr, Valen Yamamoto, Yasser Shoukry*

- `2211.14438v1` - [abs](http://arxiv.org/abs/2211.14438v1) - [pdf](http://arxiv.org/pdf/2211.14438v1)

> In this paper, we present BERN-NN as an efficient tool to perform bound propagation of Neural Networks (NNs). Bound propagation is a critical step in wide range of NN model checkers and reachability analysis tools. Given a bounded input set, bound propagation algorithms aim to compute tight bounds on the output of the NN. So far, linear and convex optimizations have been used to perform bound propagation. Since neural networks are highly non-convex, state-of-the-art bound propagation techniques suffer from introducing large errors. To circumvent such drawback, BERN-NN approximates the bounds of each neuron using a class of polynomials called Bernstein polynomials. Bernstein polynomials enjoy several interesting properties that allow BERN-NN to obtain tighter bounds compared to those relying on linear and convex approximations. BERN-NN is efficiently parallelized on graphic processing units (GPUs). Extensive numerical results show that bounds obtained by BERN-NN are orders of magnitude tighter than those obtained by state-of-the-art verifiers such as linear programming and linear interval arithmetic. Moreoveer, BERN-NN is both faster and produces tighter outputs compared to convex programming approaches like alpha-CROWN.

</details>

<details>

<summary>2022-11-22 20:58:54 - Big Earth Data and Machine Learning for Sustainable and Resilient Agriculture</summary>

- *Vasileios Sitokonstantinou*

- `2211.12584v1` - [abs](http://arxiv.org/abs/2211.12584v1) - [pdf](http://arxiv.org/pdf/2211.12584v1)

> Big streams of Earth images from satellites or other platforms (e.g., drones and mobile phones) are becoming increasingly available at low or no cost and with enhanced spatial and temporal resolution. This thesis recognizes the unprecedented opportunities offered by the high quality and open access Earth observation data of our times and introduces novel machine learning and big data methods to properly exploit them towards developing applications for sustainable and resilient agriculture. The thesis addresses three distinct thematic areas, i.e., the monitoring of the Common Agricultural Policy (CAP), the monitoring of food security and applications for smart and resilient agriculture. The methodological innovations of the developments related to the three thematic areas address the following issues: i) the processing of big Earth Observation (EO) data, ii) the scarcity of annotated data for machine learning model training and iii) the gap between machine learning outputs and actionable advice.   This thesis demonstrated how big data technologies such as data cubes, distributed learning, linked open data and semantic enrichment can be used to exploit the data deluge and extract knowledge to address real user needs. Furthermore, this thesis argues for the importance of semi-supervised and unsupervised machine learning models that circumvent the ever-present challenge of scarce annotations and thus allow for model generalization in space and time. Specifically, it is shown how merely few ground truth data are needed to generate high quality crop type maps and crop phenology estimations. Finally, this thesis argues there is considerable distance in value between model inferences and decision making in real-world scenarios and thereby showcases the power of causal and interpretable machine learning in bridging this gap.

</details>

<details>

<summary>2022-11-22 21:02:49 - Modeling System Events and Negative Events Using Thinging Machines Based on Lupascian Logic</summary>

- *Sabah Al-Fedaghi*

- `2211.12586v1` - [abs](http://arxiv.org/abs/2211.12586v1) - [pdf](http://arxiv.org/pdf/2211.12586v1)

> This paper is an exploration of the ontological foundations of conceptual modeling that addresses the concept of events and related notions. Development models that convey how things change over space and time demand continued attention in systems and software engineering. In this context, foundational matters in modeling systems include the definition of an event, the types of events, and the kinds of relationships that can be recognized among events. Although a broad spectrum of research of such issues exists in various fields of study, events have extensive applicability in computing (e.g., event-driven programming, architecture, data modeling, automation, and surveillance). While these computing notions are diverse, their event-based nature lets us apply many of the same software engineering techniques to all of them. In this paper, the focus is on addressing the dynamic concepts of system events and negative events. Specifically, we concentrate on what computer scientists would refer to as an event grammar and event calculus. Analyzing the concept of event would further the understanding of the event notion and provide a sound foundation for improving the theory and practice of conceptual modeling. An event in computer science has many definitions (e.g., anything that happens, changes in the properties of objects, and the occurrence of and transition between states). This paper is based upon a different conceptualization using thinging machines and Lupascian logic to define negative events. An event is defined as a time penetrated domain s region, which is described in terms of things and five-action machines. Accordingly, samples from event grammar and event calculus are remodeled and analyzed in terms of this definition. The results point to an enriched modeling technique with an enhanced conceptualization of events that can benefit behavior modeling in systems.

</details>

<details>

<summary>2022-11-22 22:27:39 - Phylogeny-Inspired Adaptation of Multilingual Models to New Languages</summary>

- *Fahim Faisal, Antonios Anastasopoulos*

- `2205.09634v2` - [abs](http://arxiv.org/abs/2205.09634v2) - [pdf](http://arxiv.org/pdf/2205.09634v2)

> Large pretrained multilingual models, trained on dozens of languages, have delivered promising results due to cross-lingual learning capabilities on variety of language tasks. Further adapting these models to specific languages, especially ones unseen during pre-training, is an important goal towards expanding the coverage of language technologies. In this study, we show how we can use language phylogenetic information to improve cross-lingual transfer leveraging closely related languages in a structured, linguistically-informed manner. We perform adapter-based training on languages from diverse language families (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic and semantic tasks, obtaining more than 20% relative performance improvements over strong commonly used baselines, especially on languages unseen during pre-training.

</details>

<details>

<summary>2022-11-23 00:04:57 - Leveraging Data Recasting to Enhance Tabular Reasoning</summary>

- *Aashna Jena, Vivek Gupta, Manish Shrivastava, Julian Martin Eisenschlos*

- `2211.12641v1` - [abs](http://arxiv.org/abs/2211.12641v1) - [pdf](http://arxiv.org/pdf/2211.12641v1)

> Creating challenging tabular inference data is essential for learning complex reasoning. Prior work has mostly relied on two data generation strategies. The first is human annotation, which yields linguistically diverse data but is difficult to scale. The second category for creation is synthetic generation, which is scalable and cost effective but lacks inventiveness. In this research, we present a framework for semi-automatically recasting existing tabular data to make use of the benefits of both approaches. We utilize our framework to build tabular NLI instances from five datasets that were initially intended for tasks like table2text creation, tabular Q/A, and semantic parsing. We demonstrate that recasted data could be used as evaluation benchmarks as well as augmentation data to enhance performance on tabular NLI tasks. Furthermore, we investigate the effectiveness of models trained on recasted data in the zero-shot scenario, and analyse trends in performance across different recasted datasets types.

</details>

<details>

<summary>2022-11-23 00:18:32 - STN: a new tensor network method to identify stimulus category from brain activity pattern</summary>

- *Chunyu Liu, Jiacai Zhang*

- `2210.16993v3` - [abs](http://arxiv.org/abs/2210.16993v3) - [pdf](http://arxiv.org/pdf/2210.16993v3)

> Neural decoding is still a challenge and hot topic in neurocomputing science. Recently, many studies have shown that brain network patterns containing rich spatial and temporal structure information, which represents the activation information of brain under external stimuli. %Therefore, the research of decoding stimuli from brain network received extensive more attention. The traditional method extracts brain network features directly from the common machine learning method, then puts these features into the classifier, and realizes to decode external stimuli. However, this method cannot effectively extract the multi-dimensional structural information, which is hidden in the brain network. The tensor researchers show that the tensor decomposition model can fully mine unique spatio-temporal structure characteristics in multi-dimensional structure data. This research proposed a stimulus constrained tensor brain model(STN)which involves the tensor decomposition idea and stimulus category constraint information. The model was verified on the real neuroimaging data sets (MEG and fMRI). The experimental results show that the STN model achieves more than 11.06% and 18.46% on accuracy matrix compared with others methods on two modal data sets. These results imply the superiority of extracting discriminative characteristics about STN model, especially for decoding object stimuli with semantic information.

</details>

<details>

<summary>2022-11-23 02:41:50 - DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data</summary>

- *Xiao Li, Yin Zhu, Sichen Liu, Jiangzhou Ju, Yuzhong Qu, Gong Cheng*

- `2211.12668v1` - [abs](http://arxiv.org/abs/2211.12668v1) - [pdf](http://arxiv.org/pdf/2211.12668v1)

> Numerical reasoning over hybrid data containing tables and long texts has recently received research attention from the AI community. To generate an executable reasoning program consisting of math and table operations to answer a question, state-of-the-art methods use a retriever-generator pipeline. However, their retrieval results are static, while different generation steps may rely on different sentences. To attend to the retrieved information that is relevant to each generation step, in this paper, we propose DyRRen, an extended retriever-reranker-generator framework where each generation step is enhanced by a dynamic reranking of retrieved sentences. It outperforms existing baselines on the FinQA dataset.

</details>

<details>

<summary>2022-11-23 05:15:32 - SciAI4Industry -- Solving PDEs for industry-scale problems with deep learning</summary>

- *Philipp A. Witte, Russell J. Hewett, Kumar Saurabh, AmirHossein Sojoodi, Ranveer Chandra*

- `2211.12709v1` - [abs](http://arxiv.org/abs/2211.12709v1) - [pdf](http://arxiv.org/pdf/2211.12709v1)

> Solving partial differential equations with deep learning makes it possible to reduce simulation times by multiple orders of magnitude and unlock scientific methods that typically rely on large numbers of sequential simulations, such as optimization and uncertainty quantification. Two of the largest challenges of adopting scientific AI for industrial problem settings is that training datasets must be simulated in advance and that neural networks for solving large-scale PDEs exceed the memory capabilities of current GPUs. We introduce a distributed programming API in the Julia language for simulating training data in parallel on the cloud and without requiring users to manage the underlying HPC infrastructure. In addition, we show that model-parallel deep learning based on domain decomposition allows us to scale neural networks for solving PDEs to commercial-scale problem settings and achieve above 90% parallel efficiency. Combining our cloud API for training data generation and model-parallel deep learning, we train large-scale neural networks for solving the 3D Navier-Stokes equation and simulating 3D CO2 flow in porous media. For the CO2 example, we simulate a training dataset based on a commercial carbon capture and storage (CCS) project and train a neural network for CO2 flow simulation on a 3D grid with over 2 million cells that is 5 orders of magnitudes faster than a conventional numerical simulator and 3,200 times cheaper.

</details>

<details>

<summary>2022-11-23 06:51:02 - Effectiveness of an Online Course in Programming in a State University in the Philippines</summary>

- *Aaron Paul M. Dela Rosa*

- `2211.14430v1` - [abs](http://arxiv.org/abs/2211.14430v1) - [pdf](http://arxiv.org/pdf/2211.14430v1)

> Online courses, as a pedagogical approach to teaching, boomed during this Coronavirus Disease 2019 pandemic era. Universities shifted from traditional face to face classes to online distance learning due to the cause of the pandemic. This study aimed to determine how effective an online course is in learning a programming course. The study utilized mixed method research applied through a validated survey questionnaire consisting of closed and open ended questions. Python programming was the course selected to undergo the study and underwent an evaluation to determine the students' responses. Student respondents are from Bulacan State University, a state university in the Philippines, under the Bachelor of Science in Information Technology program. Based on their responses, the students found that the online Python programming was Very Effective, with an overall mean of 4.49. This result shows that students found the online course effective, provided the proper course design and content, allowed them to spend enough time finishing tasks, and provided communication and interaction with their instructor and fellow students. Additionally, students gave overwhelmingly positive responses when asked what their instructors had done well on the course delivery and provided insightful and constructive comments for further enhancement and delivery of the course. This study found that most students strongly agreed and believed in the effectiveness of delivering the Python Programming course asynchronously. With such positive results from the student's perspective and evaluation, the course can be enhanced to continue providing quality education at Bulacan State University.

</details>

<details>

<summary>2022-11-23 06:58:09 - RoentGen: Vision-Language Foundation Model for Chest X-ray Generation</summary>

- *Pierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier Van der Sluijs, MaÅgorzata PoÅacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P. Langlotz, Akshay Chaudhari*

- `2211.12737v1` - [abs](http://arxiv.org/abs/2211.12737v1) - [pdf](http://arxiv.org/pdf/2211.12737v1)

> Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5% improvement of a classifier trained jointly on synthetic and real images, and a 3% improvement when trained on a larger but purely synthetic training set. Finally, we observe that this fine-tuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25%.

</details>

<details>

<summary>2022-11-23 07:42:10 - CodeT: Code Generation with Generated Tests</summary>

- *Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen*

- `2207.10397v2` - [abs](http://arxiv.org/abs/2207.10397v2) - [pdf](http://arxiv.org/pdf/2207.10397v2)

> The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.

</details>

<details>

<summary>2022-11-23 07:53:17 - FAIRification of MLC data</summary>

- *Ana Kostovska, Jasmin Bogatinovski, Andrej Treven, SaÅ¡o DÅ¾eroski, Dragi Kocev, PanÄe Panov*

- `2211.12757v1` - [abs](http://arxiv.org/abs/2211.12757v1) - [pdf](http://arxiv.org/pdf/2211.12757v1)

> The multi-label classification (MLC) task has increasingly been receiving interest from the machine learning (ML) community, as evidenced by the growing number of papers and methods that appear in the literature. Hence, ensuring proper, correct, robust, and trustworthy benchmarking is of utmost importance for the further development of the field. We believe that this can be achieved by adhering to the recently emerged data management standards, such as the FAIR (Findable, Accessible, Interoperable, and Reusable) and TRUST (Transparency, Responsibility, User focus, Sustainability, and Technology) principles. To FAIRify the MLC datasets, we introduce an ontology-based online catalogue of MLC datasets that follow these principles. The catalogue extensively describes many MLC datasets with comprehensible meta-features, MLC-specific semantic descriptions, and different data provenance information. The MLC data catalogue is extensively described in our recent publication in Nature Scientific Reports, Kostovska & Bogatinovski et al., and available at: http://semantichub.ijs.si/MLCdatasets. In addition, we provide an ontology-based system for easy access and querying of performance/benchmark data obtained from a comprehensive MLC benchmark study. The system is available at: http://semantichub.ijs.si/MLCbenchmark.

</details>

<details>

<summary>2022-11-23 09:04:45 - Program Repair</summary>

- *Xiang Gao, Yannic Noller, Abhik Roychoudhury*

- `2211.12787v1` - [abs](http://arxiv.org/abs/2211.12787v1) - [pdf](http://arxiv.org/pdf/2211.12787v1)

> Automated program repair is an emerging technology which consists of a suite of techniques to automatically fix bugs or vulnerabilities in programs. In this paper, we present a comprehensive survey of the state of the art in program repair. We first study the different suite of techniques used including search based repair, constraint based repair and learning based repair. We then discuss one of the main challenges in program repair namely patch overfitting, by distilling a class of techniques which can alleviate patch overfitting. We then discuss classes of program repair tools, applications of program repair as well as uses of program repair in industry. We conclude the survey with a forward looking outlook on future usages of program repair, as well as research opportunities arising from work on code from large language models.

</details>

<details>

<summary>2022-11-23 11:33:43 - Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning</summary>

- *Xiaobao Wu, Anh Tuan Luu, Xinshuai Dong*

- `2211.12878v1` - [abs](http://arxiv.org/abs/2211.12878v1) - [pdf](http://arxiv.org/pdf/2211.12878v1)

> To overcome the data sparsity issue in short text topic modeling, existing methods commonly rely on data augmentation or the data characteristic of short texts to introduce more word co-occurrence information. However, most of them do not make full use of the augmented data or the data characteristic: they insufficiently learn the relations among samples in data, leading to dissimilar topic distributions of semantically similar text pairs. To better address data sparsity, in this paper we propose a novel short text topic modeling framework, Topic-Semantic Contrastive Topic Model (TSCTM). To sufficiently model the relations among samples, we employ a new contrastive learning method with efficient positive and negative sampling strategies based on topic semantics. This contrastive learning method refines the representations, enriches the learning signals, and thus mitigates the sparsity issue. Extensive experimental results show that our TSCTM outperforms state-of-the-art baselines regardless of the data augmentation availability, producing high-quality topics and topic distributions.

</details>

<details>

<summary>2022-11-23 12:14:28 - OMPQ: Orthogonal Mixed Precision Quantization</summary>

- *Yuexiao Ma, Taisong Jin, Xiawu Zheng, Yan Wang, Huixia Li, Yongjian Wu, Guannan Jiang, Wei Zhang, Rongrong Ji*

- `2109.07865v3` - [abs](http://arxiv.org/abs/2109.07865v3) - [pdf](http://arxiv.org/pdf/2109.07865v3)

> To bridge the ever increasing gap between deep neural networks' complexity and hardware capability, network quantization has attracted more and more research attention. The latest trend of mixed precision quantization takes advantage of hardware's multiple bit-width arithmetic operations to unleash the full potential of network quantization. However, this also results in a difficult integer programming formulation, and forces most existing approaches to use an extremely time-consuming search process even with various relaxations. Instead of solving a problem of the original integer programming, we propose to optimize a proxy metric, the concept of network orthogonality, which is highly correlated with the loss of the integer programming but also easy to optimize with linear programming. This approach reduces the search time and required data amount by orders of magnitude, with little compromise on quantization accuracy. Specifically, we achieve 72.08% Top-1 accuracy on ResNet-18 with 6.7Mb, which does not require any searching iterations. Given the high efficiency and low data dependency of our algorithm, we used it for the post-training quantization, which achieve 71.27% Top-1 accuracy on MobileNetV2 with only 1.5Mb. Our code is available at https://github.com/MAC-AutoML/OMPQ.

</details>

<details>

<summary>2022-11-23 13:24:36 - EurNet: Efficient Multi-Range Relational Modeling of Spatial Multi-Relational Data</summary>

- *Minghao Xu, Yuanfan Guo, Yi Xu, Jian Tang, Xinlei Chen, Yuandong Tian*

- `2211.12941v1` - [abs](http://arxiv.org/abs/2211.12941v1) - [pdf](http://arxiv.org/pdf/2211.12941v1)

> Modeling spatial relationship in the data remains critical across many different tasks, such as image classification, semantic segmentation and protein structure understanding. Previous works often use a unified solution like relative positional encoding. However, there exists different kinds of spatial relations, including short-range, medium-range and long-range relations, and modeling them separately can better capture the focus of different tasks on the multi-range relations (e.g., short-range relations can be important in instance segmentation, while long-range relations should be upweighted for semantic segmentation). In this work, we introduce the EurNet for Efficient multi-range relational modeling. EurNet constructs the multi-relational graph, where each type of edge corresponds to short-, medium- or long-range spatial interactions. In the constructed graph, EurNet adopts a novel modeling layer, called gated relational message passing (GRMP), to propagate multi-relational information across the data. GRMP captures multiple relations within the data with little extra computational cost. We study EurNets in two important domains for image and protein structure modeling. Extensive experiments on ImageNet classification, COCO object detection and ADE20K semantic segmentation verify the gains of EurNet over the previous SoTA FocalNet. On the EC and GO protein function prediction benchmarks, EurNet consistently surpasses the previous SoTA GearNet. Our results demonstrate the strength of EurNets on modeling spatial multi-relational data from various domains. The implementations of EurNet for image modeling are available at https://github.com/hirl-team/EurNet-Image . The implementations for other applied domains/tasks will be released soon.

</details>

<details>

<summary>2022-11-23 13:43:05 - Superoptimization of WebAssembly Bytecode</summary>

- *Javier Cabrera-Arteaga, Shrinish Donde, Jian Gu, Orestis Floros, Lucas Satabin, Benoit Baudry, Martin Monperrus*

- `2002.10213v2` - [abs](http://arxiv.org/abs/2002.10213v2) - [pdf](http://arxiv.org/pdf/2002.10213v2)

> Motivated by the fast adoption of WebAssembly, we propose the first functional pipeline to support the superoptimization of WebAssembly bytecode. Our pipeline works over LLVM and Souper. We evaluate our superoptimization pipeline with 12 programs from the Rosetta code project. Our pipeline improves the code section size of 8 out of 12 programs. We discuss the challenges faced in superoptimization of WebAssembly with two case studies.

</details>

<details>

<summary>2022-11-23 14:35:54 - Verified Reversible Programming for Verified Lossless Compression</summary>

- *James Townsend, Jan-Willem van de Meent*

- `2211.09676v2` - [abs](http://arxiv.org/abs/2211.09676v2) - [pdf](http://arxiv.org/pdf/2211.09676v2)

> Lossless compression implementations typically contain two programs, an encoder and a decoder, which are required to be inverse to one another. We observe that a significant class of compression methods, based on asymmetric numeral systems (ANS), have shared structure between the encoder and decoder -- the decoder program is the 'reverse' of the encoder program -- allowing both to be simultaneously specified by a single, reversible function. To exploit this, we have implemented a small reversible language, embedded in Agda, which we call 'Flipper' (available at https://github.com/j-towns/flipper). Agda supports formal verification of program properties, and the compiler for our reversible language (which is implemented as an Agda macro), produces not just an encoder/decoder pair of functions but also a proof that they are inverse to one another. Thus users of the language get formal verification 'for free'. We give a small example use-case of Flipper in this paper, and plan to publish a full compression implementation soon.

</details>

<details>

<summary>2022-11-23 14:37:11 - Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis</summary>

- *Kai Zhang, Kun Zhang, Mengdi Zhang, Hongke Zhao, Qi Liu, Wei Wu, Enhong Chen*

- `2203.16369v2` - [abs](http://arxiv.org/abs/2203.16369v2) - [pdf](http://arxiv.org/pdf/2203.16369v2)

> Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.

</details>

<details>

<summary>2022-11-23 15:04:55 - Contrastive Masked Autoencoders for Self-Supervised Video Hashing</summary>

- *Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shutao Xia*

- `2211.11210v2` - [abs](http://arxiv.org/abs/2211.11210v2) - [pdf](http://arxiv.org/pdf/2211.11210v2)

> Self-Supervised Video Hashing (SSVH) models learn to generate short binary representations for videos without ground-truth supervision, facilitating large-scale video retrieval efficiency and attracting increasing research attention. The success of SSVH lies in the understanding of video content and the ability to capture the semantic relation among unlabeled videos. Typically, state-of-the-art SSVH methods consider these two points in a two-stage training pipeline, where they firstly train an auxiliary network by instance-wise mask-and-predict tasks and secondly train a hashing model to preserve the pseudo-neighborhood structure transferred from the auxiliary network. This consecutive training strategy is inflexible and also unnecessary. In this paper, we propose a simple yet effective one-stage SSVH method called ConMH, which incorporates video semantic information and video similarity relationship understanding in a single stage. To capture video semantic information for better hashing learning, we adopt an encoder-decoder structure to reconstruct the video from its temporal-masked frames. Particularly, we find that a higher masking ratio helps video understanding. Besides, we fully exploit the similarity relationship between videos by maximizing agreement between two augmented views of a video, which contributes to more discriminative and robust hash codes. Extensive experiments on three large-scale video datasets (i.e., FCVID, ActivityNet and YFCC) indicate that ConMH achieves state-of-the-art results. Code is available at https://github.com/huangmozhi9527/ConMH.

</details>

<details>

<summary>2022-11-23 16:31:03 - DeepVulSeeker: A Novel Vulnerability Identification Framework via Code Graph Structure and Pre-training Mechanism</summary>

- *Jin Wang, Hui Xiao, Shuwen Zhong, Yinhao Xiao*

- `2211.13097v1` - [abs](http://arxiv.org/abs/2211.13097v1) - [pdf](http://arxiv.org/pdf/2211.13097v1)

> Software vulnerabilities can pose severe harms to a computing system. They can lead to system crash, privacy leakage, or even physical damage. Correctly identifying vulnerabilities among enormous software codes in a timely manner is so far the essential prerequisite to patch them. Unfortantely, the current vulnerability identification methods, either the classic ones or the deep-learning-based ones, have several critical drawbacks, making them unable to meet the present-day demands put forward by the software industry. To overcome the drawbacks, in this paper, we propose DeepVulSeeker, a novel fully automated vulnerability identification framework, which leverages both code graph structures and the semantic features with the help of the recently advanced Graph Representation Self-Attention and pre-training mechanisms. Our experiments show that DeepVulSeeker not only reaches an accuracy as high as 0.99 on traditional CWE datasets, but also outperforms all other exisiting methods on two highly-complicated datasets. We also testified DeepVulSeeker based on three case studies, and found that DeepVulSeeker is able to understand the implications of the vulnerbilities. We have fully implemented DeepVulSeeker and open-sourced it for future follow-up research.

</details>

<details>

<summary>2022-11-23 21:42:43 - A framework for structural shape optimization based on automatic differentiation, the adjoint method and accelerated linear algebra</summary>

- *Gaoyuan Wu*

- `2211.15409v1` - [abs](http://arxiv.org/abs/2211.15409v1) - [pdf](http://arxiv.org/pdf/2211.15409v1)

> Shape optimization is of great significance in structural engineering, as an efficient geometry leads to better performance of structures. However, the application of gradient-based shape optimization for structural and architectural design is limited, which is partly due to the difficulty and the complexity in gradient evaluation. In this work, an efficient framework based on automatic differentiation (AD), the adjoint method and accelerated linear algebra (XLA) is proposed to promote the implementation of gradient-based shape optimization. The framework is realized by the implementation of the high-performance computing (HPC) library JAX. We leverage AD for gradient evaluation in the sensitivity analysis stage. Compared to numerical differentiation, AD is more accurate; compared to analytical and symbolic differentiation, AD is more efficient and easier to apply. In addition, the adjoint method is used to reduce the complexity of computation of the sensitivity. The XLA feature is exploited by an efficient programming architecture that we proposed, which can boost gradient evaluation. The proposed framework also supports hardware acceleration such as GPUs. The framework is applied to the form finding of arches and different free-form gridshells: gridshell inspired by Mannheim Multihalle, four-point supported gridshell, and canopy-like structures. Two geometric descriptive methods are used: non-parametric and parametric description via B\'ezier surface. Non-constrained and constrained shape optimization problems are considered, where the former is solved by gradient descent and the latter is solved by sequential quadratic programming (SQP). Through these examples, the proposed framework is shown to be able to provide structural engineers with a more efficient tool for shape optimization, enabling better design for the built environment.

</details>

<details>

<summary>2022-11-24 02:00:03 - Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring</summary>

- *Xinying Qiu, Shuxuan Liao, Jiajun Xie, Jian-Yun Nie*

- `2211.13373v1` - [abs](http://arxiv.org/abs/2211.13373v1) - [pdf](http://arxiv.org/pdf/2211.13373v1)

> In the prompt-specific holistic score prediction task for Automatic Essay Scoring, the general approaches include pre-trained neural model, coherence model, and hybrid model that incorporate syntactic features with neural model. In this paper, we propose a novel approach to extract and represent essay coherence features with prompt-learning NSP that shows to match the state-of-the-art AES coherence model, and achieves the best performance for long essays. We apply syntactic feature dense embedding to augment BERT-based model and achieve the best performance for hybrid methodology for AES. In addition, we explore various ideas to combine coherence, syntactic information and semantic embeddings, which no previous study has done before. Our combined model also performs better than the SOTA available for combined model, even though it does not outperform our syntactic enhanced neural model. We further offer analyses that can be useful for future study.

</details>

<details>

<summary>2022-11-24 02:38:28 - ifMixup: Interpolating Graph Pair to Regularize Graph Classification</summary>

- *Hongyu Guo, Yongyi Mao*

- `2110.09344v3` - [abs](http://arxiv.org/abs/2110.09344v3) - [pdf](http://arxiv.org/pdf/2110.09344v3)

> We present a simple and yet effective interpolation-based regularization technique, aiming to improve the generalization of Graph Neural Networks (GNNs) on supervised graph classification. We leverage Mixup, an effective regularizer for vision, where random sample pairs and their labels are interpolated to create synthetic images for training. Unlike images with grid-like coordinates, graphs have arbitrary structure and topology, which can be very sensitive to any modification that alters the graph's semantic meanings. This posts two unanswered questions for Mixup-like regularization schemes: Can we directly mix up a pair of graph inputs? If so, how well does such mixing strategy regularize the learning of GNNs? To answer these two questions, we propose ifMixup, which first adds dummy nodes to make two graphs have the same input size and then simultaneously performs linear interpolation between the aligned node feature vectors and the aligned edge representations of the two graphs. We empirically show that such simple mixing schema can effectively regularize the classification learning, resulting in superior predictive accuracy to popular graph augmentation and GNN methods.

</details>

<details>

<summary>2022-11-24 04:22:25 - Segmentation with mixed supervision: Confidence maximization helps knowledge distillation</summary>

- *Bingyuan Liu, Christian Desrosiers, Ismail Ben Ayed, Jose Dolz*

- `2109.10902v5` - [abs](http://arxiv.org/abs/2109.10902v5) - [pdf](http://arxiv.org/pdf/2109.10902v5)

> Despite achieving promising results in a breadth of medical image segmentation tasks, deep neural networks require large training datasets with pixel-wise annotations. Obtaining these curated datasets is a cumbersome process which limits the applicability in scenarios. Mixed supervision is an appealing alternative for mitigating this obstacle. In this work, we propose a dual-branch architecture, where the upper branch (teacher) receives strong annotations, while the bottom one (student) is driven by limited supervision and guided by the upper branch. Combined with a standard cross-entropy loss over the labeled pixels, our novel formulation integrates two important terms: (i) a Shannon entropy loss defined over the less-supervised images, which encourages confident student predictions in the bottom branch; and (ii) a KL divergence term, which transfers the knowledge (i.e., predictions) of the strongly supervised branch to the less-supervised branch and guides the entropy (student-confidence) term to avoid trivial solutions. We show that the synergy between the entropy and KL divergence yields substantial improvements in performance. We also discuss an interesting link between Shannon-entropy minimization and standard pseudo-mask generation, and argue that the former should be preferred over the latter for leveraging information from unlabeled pixels. We evaluate the effectiveness of the proposed formulation through a series of quantitative and qualitative experiments using two publicly available datasets. Results demonstrate that our method significantly outperforms other strategies for semantic segmentation within a mixed-supervision framework, as well as recent semi-supervised approaches. Our code is publicly available: https://github.com/by-liu/ConfKD.

</details>

<details>

<summary>2022-11-24 07:12:51 - Delving into Out-of-Distribution Detection with Vision-Language Representations</summary>

- *Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, Yixuan Li*

- `2211.13445v1` - [abs](http://arxiv.org/abs/2211.13445v1) - [pdf](http://arxiv.org/pdf/2211.13445v1)

> Recognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (e.g., either vision or language), leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts. We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM. Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1% (AUROC). Code is available at https://github.com/deeplearning-wisc/MCM.

</details>

<details>

<summary>2022-11-24 08:02:52 - On the Importance of Image Encoding in Automated Chest X-Ray Report Generation</summary>

- *Otabek Nazarov, Mohammad Yaqub, Karthik Nandakumar*

- `2211.13465v1` - [abs](http://arxiv.org/abs/2211.13465v1) - [pdf](http://arxiv.org/pdf/2211.13465v1)

> Chest X-ray is one of the most popular medical imaging modalities due to its accessibility and effectiveness. However, there is a chronic shortage of well-trained radiologists who can interpret these images and diagnose the patient's condition. Therefore, automated radiology report generation can be a very helpful tool in clinical practice. A typical report generation workflow consists of two main steps: (i) encoding the image into a latent space and (ii) generating the text of the report based on the latent image embedding. Many existing report generation techniques use a standard convolutional neural network (CNN) architecture for image encoding followed by a Transformer-based decoder for medical text generation. In most cases, CNN and the decoder are trained jointly in an end-to-end fashion. In this work, we primarily focus on understanding the relative importance of encoder and decoder components. Towards this end, we analyze four different image encoding approaches: direct, fine-grained, CLIP-based, and Cluster-CLIP-based encodings in conjunction with three different decoders on the large-scale MIMIC-CXR dataset. Among these encoders, the cluster CLIP visual encoder is a novel approach that aims to generate more discriminative and explainable representations. CLIP-based encoders produce comparable results to traditional CNN-based encoders in terms of NLP metrics, while fine-grained encoding outperforms all other encoders both in terms of NLP and clinical accuracy metrics, thereby validating the importance of image encoder to effectively extract semantic information. GitHub repository: https://github.com/mudabek/encoding-cxr-report-gen

</details>

<details>

<summary>2022-11-24 08:27:47 - Efficient Zero-shot Visual Search via Target and Context-aware Transformer</summary>

- *Zhiwei Ding, Xuezhe Ren, Erwan David, Melissa Vo, Gabriel Kreiman, Mengmi Zhang*

- `2211.13470v1` - [abs](http://arxiv.org/abs/2211.13470v1) - [pdf](http://arxiv.org/pdf/2211.13470v1)

> Visual search is a ubiquitous challenge in natural vision, including daily tasks such as finding a friend in a crowd or searching for a car in a parking lot. Human rely heavily on relevant target features to perform goal-directed visual search. Meanwhile, context is of critical importance for locating a target object in complex scenes as it helps narrow down the search area and makes the search process more efficient. However, few works have combined both target and context information in visual search computational models. Here we propose a zero-shot deep learning architecture, TCT (Target and Context-aware Transformer), that modulates self attention in the Vision Transformer with target and contextual relevant information to enable human-like zero-shot visual search performance. Target modulation is computed as patch-wise local relevance between the target and search images, whereas contextual modulation is applied in a global fashion. We conduct visual search experiments on TCT and other competitive visual search models on three natural scene datasets with varying levels of difficulty. TCT demonstrates human-like performance in terms of search efficiency and beats the SOTA models in challenging visual search tasks. Importantly, TCT generalizes well across datasets with novel objects without retraining or fine-tuning. Furthermore, we also introduce a new dataset to benchmark models for invariant visual search under incongruent contexts. TCT manages to search flexibly via target and context modulation, even under incongruent contexts.

</details>

<details>

<summary>2022-11-24 09:55:57 - Transition-based Semantic Role Labeling with Pointer Networks</summary>

- *Daniel FernÃ¡ndez-GonzÃ¡lez*

- `2205.10023v2` - [abs](http://arxiv.org/abs/2205.10023v2) - [pdf](http://arxiv.org/pdf/2205.10023v2)

> Semantic role labeling (SRL) focuses on recognizing the predicate-argument structure of a sentence and plays a critical role in many natural language processing tasks such as machine translation and question answering. Practically all available methods do not perform full SRL, since they rely on pre-identified predicates, and most of them follow a pipeline strategy, using specific models for undertaking one or several SRL subtasks. In addition, previous approaches have a strong dependence on syntactic information to achieve state-of-the-art performance, despite being syntactic trees equally hard to produce. These simplifications and requirements make the majority of SRL systems impractical for real-world applications. In this article, we propose the first transition-based SRL approach that is capable of completely processing an input sentence in a single left-to-right pass, with neither leveraging syntactic information nor resorting to additional modules. Thanks to our implementation based on Pointer Networks, full SRL can be accurately and efficiently done in $O(n^2)$, achieving the best performance to date on the majority of languages from the CoNLL-2009 shared task.

</details>

<details>

<summary>2022-11-24 10:46:23 - Specognitor: Identifying Spectre Vulnerabilities via Prediction-Aware Symbolic Execution</summary>

- *Ali Sahraee*

- `2211.13526v1` - [abs](http://arxiv.org/abs/2211.13526v1) - [pdf](http://arxiv.org/pdf/2211.13526v1)

> Spectre attacks exploit speculative execution to leak sensitive information. In the last few years, a number of static side-channel detectors have been proposed to detect cache leakage in the presence of speculative execution. However, these techniques either ignore branch prediction mechanism, detect static pre-defined patterns which is not suitable for detecting new patterns, or lead to false negatives.   In this paper, we illustrate the weakness of prediction-agnostic state-of-the-art approaches. We propose Specognitor, a novel prediction-aware symbolic execution engine to soundly explore program paths and detect subtle spectre variant 1 and variant 2 vulnerabilities. We propose a dynamic pattern detection mechanism to account for both existing and future vulnerabilities. Our experimental results show the effectiveness and efficiency of Specognitor in analyzing real-world cryptographic programs w.r.t. different processor families.

</details>

<details>

<summary>2022-11-24 12:07:50 - A posteriori learning for quasi-geostrophic turbulence parametrization</summary>

- *Hugo Frezat, Julien Le Sommer, Ronan Fablet, Guillaume Balarac, Redouane Lguensat*

- `2204.03911v2` - [abs](http://arxiv.org/abs/2204.03911v2) - [pdf](http://arxiv.org/pdf/2204.03911v2)

> The use of machine learning to build subgrid parametrizations for climate models is receiving growing attention. State-of-the-art strategies address the problem as a supervised learning task and optimize algorithms that predict subgrid fluxes based on information from coarse resolution models. In practice, training data are generated from higher resolution numerical simulations transformed in order to mimic coarse resolution simulations. By essence, these strategies optimize subgrid parametrizations to meet so-called $\textit{a priori}$ criteria. But the actual purpose of a subgrid parametrization is to obtain good performance in terms of $\textit{a posteriori}$ metrics which imply computing entire model trajectories. In this paper, we focus on the representation of energy backscatter in two dimensional quasi-geostrophic turbulence and compare parametrizations obtained with different learning strategies at fixed computational complexity. We show that strategies based on $\textit{a priori}$ criteria yield parametrizations that tend to be unstable in direct simulations and describe how subgrid parametrizations can alternatively be trained end-to-end in order to meet $\textit{a posteriori}$ criteria. We illustrate that end-to-end learning strategies yield parametrizations that outperform known empirical and data-driven schemes in terms of performance, stability and ability to apply to different flow configurations. These results support the relevance of differentiable programming paradigms for climate models in the future.

</details>

<details>

<summary>2022-11-24 13:04:08 - Relation-based Motion Prediction using Traffic Scene Graphs</summary>

- *Maximilian Zipfl, Felix Hertlein, Achim Rettinger, Steffen Thoma, Lavdim Halilaj, Juergen Luettin, Stefan Schmid, Cory Henson*

- `2212.02503v1` - [abs](http://arxiv.org/abs/2212.02503v1) - [pdf](http://arxiv.org/pdf/2212.02503v1)

> Representing relevant information of a traffic scene and understanding its environment is crucial for the success of autonomous driving. Modeling the surrounding of an autonomous car using semantic relations, i.e., how different traffic participants relate in the context of traffic rule based behaviors, is hardly been considered in previous work. This stems from the fact that these relations are hard to extract from real-world traffic scenes. In this work, we model traffic scenes in a form of spatial semantic scene graphs for various different predictions about the traffic participants, e.g., acceleration and deceleration. Our learning and inference approach uses Graph Neural Networks (GNNs) and shows that incorporating explicit information about the spatial semantic relations between traffic participants improves the predicdtion results. Specifically, the acceleration prediction of traffic participants is improved by up to 12% compared to the baselines, which do not exploit this explicit information. Furthermore, by including additional information about previous scenes, we achieve 73% improvements.

</details>

<details>

<summary>2022-11-24 17:28:29 - Question-type Identification for Academic Questions in Online Learning Platform</summary>

- *Azam Rabiee, Alok Goel, Johnson D'Souza, Saurabh Khanwalkar*

- `2211.13727v1` - [abs](http://arxiv.org/abs/2211.13727v1) - [pdf](http://arxiv.org/pdf/2211.13727v1)

> Online learning platforms provide learning materials and answers to students' academic questions by experts, peers, or systems. This paper explores question-type identification as a step in content understanding for an online learning platform. The aim of the question-type identifier is to categorize question types based on their structure and complexity, using the question text, subject, and structural features. We have defined twelve question-type classes, including Multiple-Choice Question (MCQ), essay, and others. We have compiled an internal dataset of students' questions and used a combination of weak-supervision techniques and manual annotation. We then trained a BERT-based ensemble model on this dataset and evaluated this model on a separate human-labeled test set. Our experiments yielded an F1-score of 0.94 for MCQ binary classification and promising results for 12-class multilabel classification. We deployed the model in our online learning platform as a crucial enabler for content understanding to enhance the student learning experience.

</details>

<details>

<summary>2022-11-24 18:03:02 - Cutting Medusa's Path -- Tackling Kill-Chains with Quantum Computing</summary>

- *Mark Carney*

- `2211.13740v1` - [abs](http://arxiv.org/abs/2211.13740v1) - [pdf](http://arxiv.org/pdf/2211.13740v1)

> This paper embarks upon exploration of quantum vulnerability analysis. By introducing vulnerability graphs, related to attack graphs, this paper provides background theory and a subsequent method for solving significant cybersecurity problems with quantum computing. The example given is to prioritize patches by expressing the connectivity of various vulnerabilities on a network with a QUBO and then solving this with quantum annealing. Such a solution is then proved to remove all kill-chains (paths to security compromise) on a network. The results demonstrate that the quantum computer's solve time is almost constant compared to the exponential increase in classical solve time for vulnerability graphs of expected real world density. As such, this paper presents a novel example of advantageous quantum vulnerability analysis.

</details>

<details>

<summary>2022-11-24 19:14:27 - Generative Joint Source-Channel Coding for Semantic Image Transmission</summary>

- *Ecenaz Erdemir, Tze-Yang Tung, Pier Luigi Dragotti, Deniz Gunduz*

- `2211.13772v1` - [abs](http://arxiv.org/abs/2211.13772v1) - [pdf](http://arxiv.org/pdf/2211.13772v1)

> Recent works have shown that joint source-channel coding (JSCC) schemes using deep neural networks (DNNs), called DeepJSCC, provide promising results in wireless image transmission. However, these methods mostly focus on the distortion of the reconstructed signals with respect to the input image, rather than their perception by humans. However, focusing on traditional distortion metrics alone does not necessarily result in high perceptual quality, especially in extreme physical conditions, such as very low bandwidth compression ratio (BCR) and low signal-to-noise ratio (SNR) regimes. In this work, we propose two novel JSCC schemes that leverage the perceptual quality of deep generative models (DGMs) for wireless image transmission, namely InverseJSCC and GenerativeJSCC. While the former is an inverse problem approach to DeepJSCC, the latter is an end-to-end optimized JSCC scheme. In both, we optimize a weighted sum of mean squared error (MSE) and learned perceptual image patch similarity (LPIPS) losses, which capture more semantic similarities than other distortion metrics. InverseJSCC performs denoising on the distorted reconstructions of a DeepJSCC model by solving an inverse optimization problem using style-based generative adversarial network (StyleGAN). Our simulation results show that InverseJSCC significantly improves the state-of-the-art (SotA) DeepJSCC in terms of perceptual quality in edge cases. In GenerativeJSCC, we carry out end-to-end training of an encoder and a StyleGAN-based decoder, and show that GenerativeJSCC significantly outperforms DeepJSCC both in terms of distortion and perceptual quality.

</details>

<details>

<summary>2022-11-24 22:52:55 - Self Supervised Clustering of Traffic Scenes using Graph Representations</summary>

- *Maximilian Zipfl, Moritz Jarosch, J. Marius ZÃ¶llner*

- `2211.15508v1` - [abs](http://arxiv.org/abs/2211.15508v1) - [pdf](http://arxiv.org/pdf/2211.15508v1)

> Examining graphs for similarity is a well-known challenge, but one that is mandatory for grouping graphs together. We present a data-driven method to cluster traffic scenes that is self-supervised, i.e. without manual labelling. We leverage the semantic scene graph model to create a generic graph embedding of the traffic scene, which is then mapped to a low-dimensional embedding space using a Siamese network, in which clustering is performed. In the training process of our novel approach, we augment existing traffic scenes in the Cartesian space to generate positive similarity samples. This allows us to overcome the challenge of reconstructing a graph and at the same time obtain a representation to describe the similarity of traffic scenes. We could show, that the resulting clusters possess common semantic characteristics. The approach was evaluated on the INTERACTION dataset.

</details>

<details>

<summary>2022-11-25 02:39:41 - Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?</summary>

- *Pei Zhang, Baosong Yang, Haoran Wei, Dayiheng Liu, Kai Fan, Luo Si, Jun Xie*

- `2211.13865v1` - [abs](http://arxiv.org/abs/2211.13865v1) - [pdf](http://arxiv.org/pdf/2211.13865v1)

> Neural machine translation (NMT) is often criticized for failures that happen without awareness. The lack of competency awareness makes NMT untrustworthy. This is in sharp contrast to human translators who give feedback or conduct further investigations whenever they are in doubt about predictions. To fill this gap, we propose a novel competency-aware NMT by extending conventional NMT with a self-estimator, offering abilities to translate a source sentence and estimate its competency. The self-estimator encodes the information of the decoding procedure and then examines whether it can reconstruct the original semantics of the source sentence. Experimental results on four translation tasks demonstrate that the proposed method not only carries out translation tasks intact but also delivers outstanding performance on quality estimation. Without depending on any reference or annotated data typically required by state-of-the-art metric and quality estimation methods, our model yields an even higher correlation with human quality judgments than a variety of aforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and qualitative analyses show better robustness of competency awareness in our model.

</details>

<details>

<summary>2022-11-25 03:45:31 - Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism</summary>

- *Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, Bin Cui*

- `2211.13878v1` - [abs](http://arxiv.org/abs/2211.13878v1) - [pdf](http://arxiv.org/pdf/2211.13878v1)

> Transformer models have achieved state-of-the-art performance on various domains of applications and gradually becomes the foundations of the advanced large deep learning (DL) models. However, how to train these models over multiple GPUs efficiently is still challenging due to a large number of parallelism choices. Existing DL systems either rely on manual efforts to make distributed training plans or apply parallelism combinations within a very limited search space. In this approach, we propose Galvatron, a new system framework that incorporates multiple popular parallelism dimensions and automatically finds the most efficient hybrid parallelism strategy. To better explore such a rarely huge search space, we 1) involve a decision tree to make decomposition and pruning based on some reasonable intuitions, and then 2) design a dynamic programming search algorithm to generate the optimal plan. Evaluations on four representative Transformer workloads show that Galvatron could perform automatically distributed training with different GPU memory budgets. Among all evluated scenarios, Galvatron always achieves superior system throughput compared to previous work with limited parallelism.

</details>

<details>

<summary>2022-11-25 05:11:00 - Synthesis Cost-Optimal Targeted Mutant Protein Libraries</summary>

- *Dimitris Papamichail, Madeline Febinger, Shm Almeda, Georgios Papamichail*

- `2211.13898v1` - [abs](http://arxiv.org/abs/2211.13898v1) - [pdf](http://arxiv.org/pdf/2211.13898v1)

> Protein variant libraries produced by site-directed mutagenesis are a useful tool utilized by protein engineers to explore variants with potentially improved properties, such as activity and stability. These libraries are commonly built by selecting residue positions and alternative beneficial mutations for each position. All possible combinations are then constructed and screened, by incorporating degenerate codons at mutation sites. These degenerate codons often encode additional unwanted amino acids or even STOP codons. Our study aims to take advantage of annealing based recombination of oligonucleotides during synthesis and utilize multiple degenerate codons per mutation site to produce targeted protein libraries devoid of unwanted variants. Toward this goal we created an algorithm to calculate the minimum number of degenerate codons necessary to specify any given amino acid set, and a dynamic programming method that uses this algorithm to optimally partition a DNA target sequence with degeneracies into overlapping oligonucleotides, such that the total cost of synthesis of the target mutant protein library is minimized. Computational experiments show that, for a modest increase in DNA synthesis costs, beneficial variant yields in produced mutant libraries are increased by orders of magnitude, an effect particularly pronounced in large combinatorial libraries.

</details>

<details>

<summary>2022-11-25 08:05:52 - Interactive Image Manipulation with Complex Text Instructions</summary>

- *Ryugo Morita, Zhiqiang Zhang, Man M. Ho, Jinjia Zhou*

- `2211.15352v1` - [abs](http://arxiv.org/abs/2211.15352v1) - [pdf](http://arxiv.org/pdf/2211.15352v1)

> Recently, text-guided image manipulation has received increasing attention in the research field of multimedia processing and computer vision due to its high flexibility and controllability. Its goal is to semantically manipulate parts of an input reference image according to the text descriptions. However, most of the existing works have the following problems: (1) text-irrelevant content cannot always be maintained but randomly changed, (2) the performance of image manipulation still needs to be further improved, (3) only can manipulate descriptive attributes. To solve these problems, we propose a novel image manipulation method that interactively edits an image using complex text instructions. It allows users to not only improve the accuracy of image manipulation but also achieve complex tasks such as enlarging, dwindling, or removing objects and replacing the background with the input image. To make these tasks possible, we apply three strategies. First, the given image is divided into text-relevant content and text-irrelevant content. Only the text-relevant content is manipulated and the text-irrelevant content can be maintained. Second, a super-resolution method is used to enlarge the manipulation region to further improve the operability and to help manipulate the object itself. Third, a user interface is introduced for editing the segmentation map interactively to re-modify the generated image according to the user's desires. Extensive experiments on the Caltech-UCSD Birds-200-2011 (CUB) dataset and Microsoft Common Objects in Context (MS COCO) datasets demonstrate our proposed method can enable interactive, flexible, and accurate image manipulation in real-time. Through qualitative and quantitative evaluations, we show that the proposed model outperforms other state-of-the-art methods.

</details>

<details>

<summary>2022-11-25 09:56:00 - Quantum Software Engineering: A New Genre of Computing</summary>

- *Muhammad Azeem Akbar, Arif Ali Khan, Sajjad Mahmood, Saima Rafi*

- `2211.13990v1` - [abs](http://arxiv.org/abs/2211.13990v1) - [pdf](http://arxiv.org/pdf/2211.13990v1)

> Quantum computing (QC) is no longer only a scientific interest but is rapidly becoming an industrially available technology that can potentially tackle the limitations of classical computing. Over the last few years, major technology giants have invested in developing hardware and programming frameworks to develop quantum-specific applications. QC hardware technologies are gaining momentum, however, operationalizing the QC technologies trigger the need for software-intensive methodologies, techniques, processes, tools, roles, and responsibilities for developing industrial-centric quantum software applications. This paper presents the vision of the quantum software engineering (QSE) life cycle consisting of quantum requirements engineering, quantum software design, quantum software implementation, quantum software testing, and quantum software maintenance. This paper particularly calls for joint contributions of software engineering research and industrial community to present real-world solutions to support the entire quantum software development activities. The proposed vision facilitates the researchers and practitioners to propose new processes, reference architectures, novel tools, and practices to leverage quantum computers and develop emerging and next generations of quantum software.

</details>

<details>

<summary>2022-11-25 11:27:55 - A comparison of latent semantic analysis and correspondence analysis of document-term matrices</summary>

- *Qianqian Qi, David J. Hessen, Tejaswini Deoskar, Peter G. M. van der Heijden*

- `2108.06197v4` - [abs](http://arxiv.org/abs/2108.06197v4) - [pdf](http://arxiv.org/pdf/2108.06197v4)

> Latent semantic analysis (LSA) and correspondence analysis (CA) are two techniques that use a singular value decomposition (SVD) for dimensionality reduction. LSA has been extensively used to obtain low-dimensional representations that capture relationships among documents and terms. In this article, we present a theoretical analysis and comparison of the two techniques in the context of document-term matrices. We show that CA has some attractive properties as compared to LSA, for instance that effects of margins, i.e. sums of row elements and column elements, arising from differing document-lengths and term-frequencies are effectively eliminated, so that the CA solution is optimally suited to focus on relationships among documents and terms. A unifying framework is proposed that includes both CA and LSA as special cases. We empirically compare CA to various LSA based methods on text categorization in English and authorship attribution on historical Dutch texts, and find that CA performs significantly better. We also apply CA to a long-standing question regarding the authorship of the Dutch national anthem Wilhelmus and provide further support that it can be attributed to the author Datheen, amongst several contenders.

</details>

<details>

<summary>2022-11-25 12:32:36 - Cross-Domain Ensemble Distillation for Domain Generalization</summary>

- *Kyungmoon Lee, Sungyeon Kim, Suha Kwak*

- `2211.14058v1` - [abs](http://arxiv.org/abs/2211.14058v1) - [pdf](http://arxiv.org/pdf/2211.14058v1)

> Domain generalization is the task of learning models that generalize to unseen target domains. We propose a simple yet effective method for domain generalization, named cross-domain ensemble distillation (XDED), that learns domain-invariant features while encouraging the model to converge to flat minima, which recently turned out to be a sufficient condition for domain generalization. To this end, our method generates an ensemble of the output logits from training data with the same label but from different domains and then penalizes each output for the mismatch with the ensemble. Also, we present a de-stylization technique that standardizes features to encourage the model to produce style-consistent predictions even in an arbitrary target domain. Our method greatly improves generalization capability in public benchmarks for cross-domain image classification, cross-dataset person re-ID, and cross-dataset semantic segmentation. Moreover, we show that models learned by our method are robust against adversarial attacks and image corruptions.

</details>

<details>

<summary>2022-11-25 12:56:07 - Semantic Table Detection with LayoutLMv3</summary>

- *Ivan Silajev, Niels Victor, Phillip Mortimer*

- `2211.15504v1` - [abs](http://arxiv.org/abs/2211.15504v1) - [pdf](http://arxiv.org/pdf/2211.15504v1)

> This paper presents an application of the LayoutLMv3 model for semantic table detection on financial documents from the IIIT-AR-13K dataset. The motivation behind this paper's experiment was that LayoutLMv3's official paper had no results for table detection using semantic information. We concluded that our approach did not improve the model's table detection capabilities, for which we can give several possible reasons. Either the model's weights were unsuitable for our purpose, or we needed to invest more time in optimising the model's hyperparameters. It is also possible that semantic information does not improve a model's table detection accuracy.

</details>

<details>

<summary>2022-11-25 16:47:45 - strategFTO: Untimed control for timed opacity</summary>

- *Ãtienne AndrÃ©, Shapagat Bolat, Engel Lefaucheux, Dylan Marinho*

- `2211.14233v1` - [abs](http://arxiv.org/abs/2211.14233v1) - [pdf](http://arxiv.org/pdf/2211.14233v1)

> We introduce a prototype tool strategFTO addressing the verification of a security property in critical software. We consider a recent definition of timed opacity where an attacker aims to deduce some secret while having access only to the total execution time. The system, here modeled by timed automata, is deemed opaque if for any execution time, there are either no corresponding runs, or both public and private corresponding runs. We focus on the untimed control problem: exhibiting a controller, i.e., a set of allowed actions, such that the system restricted to those actions is fully timed-opaque. We first show that this problem is not more complex than the full timed opacity problem, and then we propose an algorithm, implemented and evaluated in practice.

</details>

<details>

<summary>2022-11-25 18:05:44 - CodeExp: Explanatory Code Document Generation</summary>

- *Haotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang, Jianfeng Gao, Nan Duan*

- `2211.15395v1` - [abs](http://arxiv.org/abs/2211.15395v1) - [pdf](http://arxiv.org/pdf/2211.15395v1)

> Developing models that can automatically generate detailed code explanation can greatly benefit software maintenance and programming education. However, existing code-to-text generation models often produce only high-level summaries of code that do not capture implementation-level choices essential for these scenarios. To fill in this gap, we propose the code explanation generation task. We first conducted a human study to identify the criteria for high-quality explanatory docstring for code. Based on that, we collected and refined a large-scale code docstring corpus and formulated automatic evaluation metrics that best match human assessments. Finally, we present a multi-stage fine-tuning strategy and baseline models for the task. Our experiments show that (1) our refined training dataset lets models achieve better performance in the explanation generation tasks compared to larger unrefined data (15x larger), and (2) fine-tuned models can generate well-structured long docstrings comparable to human-written ones. We envision our training dataset, human-evaluation protocol, recommended metrics, and fine-tuning strategy can boost future code explanation research. The code and annotated data are available at https://github.com/subercui/CodeExp.

</details>

<details>

<summary>2022-11-25 19:03:25 - Less Data, More Knowledge: Building Next Generation Semantic Communication Networks</summary>

- *Christina Chaccour, Walid Saad, Merouane Debbah, Zhu Han, H. Vincent Poor*

- `2211.14343v1` - [abs](http://arxiv.org/abs/2211.14343v1) - [pdf](http://arxiv.org/pdf/2211.14343v1)

> Semantic communication is viewed as a revolutionary paradigm that can potentially transform how we design and operate wireless communication systems. However, despite a recent surge of research activities in this area, the research landscape remains limited. In this tutorial, we present the first rigorous vision of a scalable end-to-end semantic communication network that is founded on novel concepts from artificial intelligence (AI), causal reasoning, and communication theory. We first discuss how the design of semantic communication networks requires a move from data-driven networks towards knowledge-driven ones. Subsequently, we highlight the necessity of creating semantic representations of data that satisfy the key properties of minimalism, generalizability, and efficiency so as to do more with less. We then explain how those representations can form the basis a so-called semantic language. By using semantic representation and languages, we show that the traditional transmitter and receiver now become a teacher and apprentice. Then, we define the concept of reasoning by investigating the fundamentals of causal representation learning and their role in designing semantic communication networks. We demonstrate that reasoning faculties are majorly characterized by the ability to capture causal and associational relationships in datastreams. For such reasoning-driven networks, we propose novel and essential semantic communication metrics that include new "reasoning capacity" measures that could go beyond Shannon's bound to capture the convergence of computing and communication. Finally, we explain how semantic communications can be scaled to large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to provide a comprehensive reference on how to properly build, analyze, and deploy future semantic communication networks.

</details>

<details>

<summary>2022-11-25 20:26:39 - Deep Learning and Linear Programming for Automated Ensemble Forecasting and Interpretation</summary>

- *Lars Lien Ankile, Kjartan Krange*

- `2201.00426v2` - [abs](http://arxiv.org/abs/2201.00426v2) - [pdf](http://arxiv.org/pdf/2201.00426v2)

> This paper presents an ensemble forecasting method that shows strong results on the M4 Competition dataset by decreasing feature and model selection assumptions, termed DONUT (DO Not UTilize human beliefs). Our assumption reductions, primarily consisting of auto-generated features and a more diverse model pool for the ensemble, significantly outperform the statistical, feature-based ensemble method FFORMA by Montero-Manso et al. (2020). We also investigate feature extraction with a Long Short-term Memory Network (LSTM) Autoencoder and find that such features contain crucial information not captured by standard statistical feature approaches. The ensemble weighting model uses LSTM and statistical features to combine the models accurately. The analysis of feature importance and interaction shows a slight superiority for LSTM features over the statistical ones alone. Clustering analysis shows that essential LSTM features differ from most statistical features and each other. We also find that increasing the solution space of the weighting model by augmenting the ensemble with new models is something the weighting model learns to use, thus explaining part of the accuracy gains. Moreover, we present a formal ex-post-facto analysis of an optimal combination and selection for ensembles, quantifying differences through linear optimization on the M4 dataset. Our findings indicate that classical statistical time series features, such as trend and seasonality, alone do not capture all relevant information for forecasting a time series. On the contrary, our novel LSTM features contain significantly more predictive power than the statistical ones alone, but combining the two feature sets proved the best in practice.

</details>

<details>

<summary>2022-11-25 21:59:30 - Pac-Man Pete: An extensible framework for building AI in VEX Robotics</summary>

- *Jacob Zietek, Nicholas Wade, Cole Roberts, Aref Malek, Manish Pylla, Will Xu, Sagar Patil*

- `2211.14385v1` - [abs](http://arxiv.org/abs/2211.14385v1) - [pdf](http://arxiv.org/pdf/2211.14385v1)

> This technical report details VEX Robotics team BLRSAI's development of a fully autonomous robot for VEX Robotics' Tipping Point AI Competition. We identify and develop three separate critical components. This includes a Unity simulation and reinforcement learning model training pipeline, a malleable computer vision pipeline, and a data transfer pipeline to offload large computations from the VEX V5 Brain/micro-controller to an external computer. We give the community access to all of these components in hopes they can reuse and improve upon them in the future, and that it'll spark new ideas for autonomy as well as the necessary infrastructure and programs for AI in educational robotics.

</details>

<details>

<summary>2022-11-26 07:50:01 - Predictive linguistic cues for fake news: a societal artificial intelligence problem</summary>

- *Sandhya Aneja, Nagender Aneja, Ponnurangam Kumaraguru*

- `2211.14505v1` - [abs](http://arxiv.org/abs/2211.14505v1) - [pdf](http://arxiv.org/pdf/2211.14505v1)

> Media news are making a large part of public opinion and, therefore, must not be fake. News on web sites, blogs, and social media must be analyzed before being published. In this paper, we present linguistic characteristics of media news items to differentiate between fake news and real news using machine learning algorithms. Neural fake news generation, headlines created by machines, semantic incongruities in text and image captions generated by machine are other types of fake news problems. These problems use neural networks which mainly control distributional features rather than evidence. We propose applying correlation between features set and class, and correlation among the features to compute correlation attribute evaluation metric and covariance metric to compute variance of attributes over the news items. Features unique, negative, positive, and cardinal numbers with high values on the metrics are observed to provide a high area under the curve (AUC) and F1-score.

</details>

<details>

<summary>2022-11-26 07:59:20 - Lexicon-injected Semantic Parsing for Task-Oriented Dialog</summary>

- *Xiaojun Meng, Wenlin Dai, Yasheng Wang, Baojun Wang, Zhiyong Wu, Xin Jiang, Qun Liu*

- `2211.14508v1` - [abs](http://arxiv.org/abs/2211.14508v1) - [pdf](http://arxiv.org/pdf/2211.14508v1)

> Recently, semantic parsing using hierarchical representations for dialog systems has captured substantial attention. Task-Oriented Parse (TOP), a tree representation with intents and slots as labels of nested tree nodes, has been proposed for parsing user utterances. Previous TOP parsing methods are limited on tackling unseen dynamic slot values (e.g., new songs and locations added), which is an urgent matter for real dialog systems. To mitigate this issue, we first propose a novel span-splitting representation for span-based parser that outperforms existing methods. Then we present a novel lexicon-injected semantic parser, which collects slot labels of tree representation as a lexicon, and injects lexical features to the span representation of parser. An additional slot disambiguation technique is involved to remove inappropriate span match occurrences from the lexicon. Our best parser produces a new state-of-the-art result (87.62%) on the TOP dataset, and demonstrates its adaptability to frequently updated slot lexicon entries in real task-oriented dialog, with no need of retraining.

</details>

<details>

<summary>2022-11-26 10:06:00 - Equity Promotion in Public Transportation</summary>

- *Anik Pramanik, Pan Xu, Yifan Xu*

- `2211.14531v1` - [abs](http://arxiv.org/abs/2211.14531v1) - [pdf](http://arxiv.org/pdf/2211.14531v1)

> There are many news articles reporting the obstacles confronting poverty-stricken households in access to public transits. These barriers create a great deal of inconveniences for these impoverished families and more importantly, they contribute a lot of social inequalities. A typical approach addressing the issue is to build more transport infrastructure to offer more opportunities to access the public transits especially for those deprived communities. Examples include adding more bus lines connecting needy residents to railways systems and extending existing bus lines to areas with low socioeconomic status. Recently, a new strategy is proposed, which is to harness the ubiquitous ride-hailing services to connect disadvantaged households with the nearest public transportations. Compared with the former infrastructure-based solution, the ride-hailing-based strategy enjoys a few exclusive benefits such as higher effectiveness and more flexibility.   In this paper, we propose an optimization model to study how to integrate the two approaches together for equity-promotion purposes. Specifically, we aim to design a strategy of allocating a given limited budget to different candidate programs such that the overall social equity is maximized, which is defined as the minimum covering ratio among all pre-specified protected groups of households (based on race, income, etc.). We have designed a linear-programming (LP) based rounding algorithm, which proves to achieve an optimal approximation ratio of 1-1/e. Additionally, we test our algorithm against a few baselines on real data assembled by outsourcing multiple public datasets collected in the city of Chicago. Experimental results confirm our theoretical predictions and demonstrate the effectiveness of our LP-based strategy in promoting social equity, especially when the budget is insufficient.

</details>

<details>

<summary>2022-11-26 10:18:30 - Control-Flow Integrity at RISC: Attacking RISC-V by Jump-Oriented Programming</summary>

- *Olivier Gilles, Franck Viguier, Nikolai Kosmatov, Daniel Gracia PÃ©rez*

- `2211.16212v1` - [abs](http://arxiv.org/abs/2211.16212v1) - [pdf](http://arxiv.org/pdf/2211.16212v1)

> RISC-V is an open instruction set architecture recently developed for embedded real-time systems. To achieve a lasting security on these systems and design efficient countermeasures, a better understanding of vulnerabilities to novel and potential future attacks is mandatory. This paper demonstrates that RISC-V is sensible to Jump-Oriented Programming, a class of complex code-reuse attacks, able to bypass existing protections. We provide a first analysis of RISC-V systems' attack surface exploitable by such attacks, and show how they can be chained together in order to build a full-fledged attack. We use a conservative hypothesis on exploited registers and instruction patterns, in an approach we called reserved registers. This approach is implemented on a vulnerable RISC-V application, and successfully applied to expose an AES256 secret.

</details>

<details>

<summary>2022-11-26 14:56:22 - LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation</summary>

- *Florent Bartoccioni, Ãloi Zablocki, Andrei Bursuc, Patrick PÃ©rez, Matthieu Cord, Karteek Alahari*

- `2206.13294v2` - [abs](http://arxiv.org/abs/2206.13294v2) - [pdf](http://arxiv.org/pdf/2206.13294v2)

> Recent works in autonomous driving have widely adopted the bird's-eye-view (BEV) semantic map as an intermediate representation of the world. Online prediction of these BEV maps involves non-trivial operations such as multi-camera data extraction as well as fusion and projection into a common topview grid. This is usually done with error-prone geometric operations (e.g., homography or back-projection from monocular depth estimation) or expensive direct dense mapping between image pixels and pixels in BEV (e.g., with MLP or attention). In this work, we present 'LaRa', an efficient encoder-decoder, transformer-based model for vehicle semantic segmentation from multiple cameras. Our approach uses a system of cross-attention to aggregate information over multiple sensors into a compact, yet rich, collection of latent representations. These latent representations, after being processed by a series of self-attention blocks, are then reprojected with a second cross-attention in the BEV space. We demonstrate that our model outperforms the best previous works using transformers on nuScenes. The code and trained models are available at https://github.com/valeoai/LaRa

</details>

<details>

<summary>2022-11-26 15:45:31 - 1st Place Solution to NeurIPS 2022 Challenge on Visual Domain Adaptation</summary>

- *Daehan Kim, Minseok Seo, YoungJin Jeon, Dong-Geol Choi*

- `2211.14596v1` - [abs](http://arxiv.org/abs/2211.14596v1) - [pdf](http://arxiv.org/pdf/2211.14596v1)

> The Visual Domain Adaptation(VisDA) 2022 Challenge calls for an unsupervised domain adaptive model in semantic segmentation tasks for industrial waste sorting. In this paper, we introduce the SIA_Adapt method, which incorporates several methods for domain adaptive models. The core of our method in the transferable representation from large-scale pre-training. In this process, we choose a network architecture that differs from the state-of-the-art for domain adaptation. After that, self-training using pseudo-labels helps to make the initial adaptation model more adaptable to the target domain. Finally, the model soup scheme helped to improve the generalization performance in the target domain. Our method SIA_Adapt achieves 1st place in the VisDA2022 challenge. The code is available on https: //github.com/DaehanKim-Korea/VisDA2022_Winner_Solution.

</details>

<details>

<summary>2022-11-26 17:24:52 - Data-free Backdoor Removal based on Channel Lipschitzness</summary>

- *Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu*

- `2208.03111v2` - [abs](http://arxiv.org/abs/2208.03111v2) - [pdf](http://arxiv.org/pdf/2208.03111v2)

> Recent studies have shown that Deep Neural Networks (DNNs) are vulnerable to the backdoor attacks, which leads to malicious behaviors of DNNs when specific triggers are attached to the input images. It was further demonstrated that the infected DNNs possess a collection of channels, which are more sensitive to the backdoor triggers compared with normal channels. Pruning these channels was then shown to be effective in mitigating the backdoor behaviors. To locate those channels, it is natural to consider their Lipschitzness, which measures their sensitivity against worst-case perturbations on the inputs. In this work, we introduce a novel concept called Channel Lipschitz Constant (CLC), which is defined as the Lipschitz constant of the mapping from the input images to the output of each channel. Then we provide empirical evidences to show the strong correlation between an Upper bound of the CLC (UCLC) and the trigger-activated change on the channel activation. Since UCLC can be directly calculated from the weight matrices, we can detect the potential backdoor channels in a data-free manner, and do simple pruning on the infected DNN to repair the model. The proposed Channel Lipschitzness based Pruning (CLP) method is super fast, simple, data-free and robust to the choice of the pruning threshold. Extensive experiments are conducted to evaluate the efficiency and effectiveness of CLP, which achieves state-of-the-art results among the mainstream defense methods even without any data. Source codes are available at https://github.com/rkteddy/channel-Lipschitzness-based-pruning.

</details>

<details>

<summary>2022-11-27 03:25:48 - Neural Network Verification as Piecewise Linear Optimization: Formulations for the Composition of Staircase Functions</summary>

- *Tu Anh-Nguyen, Joey Huchette*

- `2211.14706v1` - [abs](http://arxiv.org/abs/2211.14706v1) - [pdf](http://arxiv.org/pdf/2211.14706v1)

> We present a technique for neural network verification using mixed-integer programming (MIP) formulations. We derive a \emph{strong formulation} for each neuron in a network using piecewise linear activation functions. Additionally, as in general, these formulations may require an exponential number of inequalities, we also derive a separation procedure that runs in super-linear time in the input dimension. We first introduce and develop our technique on the class of \emph{staircase} functions, which generalizes the ReLU, binarized, and quantized activation functions. We then use results for staircase activation functions to obtain a separation method for general piecewise linear activation functions. Empirically, using our strong formulation and separation technique, we can reduce the computational time in exact verification settings based on MIP and improve the false negative rate for inexact verifiers relying on the relaxation of the MIP formulation.

</details>

<details>

<summary>2022-11-27 05:46:46 - X-PuDu at SemEval-2022 Task 7: A Replaced Token Detection Task Pre-trained Model with Pattern-aware Ensembling for Identifying Plausible Clarifications</summary>

- *Junyuan Shang, Shuohuan Wang, Yu Sun, Yanjun Yu, Yue Zhou, Li Xiang, Guixiu Yang*

- `2211.14734v1` - [abs](http://arxiv.org/abs/2211.14734v1) - [pdf](http://arxiv.org/pdf/2211.14734v1)

> This paper describes our winning system on SemEval 2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. A replaced token detection pre-trained model is utilized with minorly different task-specific heads for SubTask-A: Multi-class Classification and SubTask-B: Ranking. Incorporating a pattern-aware ensemble method, our system achieves a 68.90% accuracy score and 0.8070 spearman's rank correlation score surpassing the 2nd place with a large margin by 2.7 and 2.2 percent points for SubTask-A and SubTask-B, respectively. Our approach is simple and easy to implement, and we conducted ablation studies and qualitative and quantitative analyses for the working strategies used in our system.

</details>

<details>

<summary>2022-11-27 07:20:27 - Simulation Intelligence: Towards a New Generation of Scientific Methods</summary>

- *Alexander Lavin, David Krakauer, Hector Zenil, Justin Gottschlich, Tim Mattson, Johann Brehmer, Anima Anandkumar, Sanjay Choudry, Kamil Rocki, AtÄ±lÄ±m GÃ¼neÅ Baydin, Carina Prunkl, Brooks Paige, Olexandr Isayev, Erik Peterson, Peter L. McMahon, Jakob Macke, Kyle Cranmer, Jiaxin Zhang, Haruko Wainwright, Adi Hanuka, Manuela Veloso, Samuel Assefa, Stephan Zheng, Avi Pfeffer*

- `2112.03235v2` - [abs](http://arxiv.org/abs/2112.03235v2) - [pdf](http://arxiv.org/pdf/2112.03235v2)

> The original "Seven Motifs" set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the "Nine Motifs of Simulation Intelligence", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.

</details>

<details>

<summary>2022-11-27 07:35:42 - Differentiable Meta Multigraph Search with Partial Message Propagation on Heterogeneous Information Networks</summary>

- *Chao Li, Hao Xu, Kun He*

- `2211.14752v1` - [abs](http://arxiv.org/abs/2211.14752v1) - [pdf](http://arxiv.org/pdf/2211.14752v1)

> Heterogeneous information networks (HINs) are widely employed for describing real-world data with intricate entities and relationships. To automatically utilize their semantic information, graph neural architecture search has recently been developed on various tasks of HINs. Existing works, on the other hand, show weaknesses in instability and inflexibility. To address these issues, we propose a novel method called Partial Message Meta Multigraph search (PMMM) to automatically optimize the neural architecture design on HINs. Specifically, to learn how graph neural networks (GNNs) propagate messages along various types of edges, PMMM adopts an efficient differentiable framework to search for a meaningful meta multigraph, which can capture more flexible and complex semantic relations than a meta graph. The differentiable search typically suffers from performance instability, so we further propose a stable algorithm called partial message search to ensure that the searched meta multigraph consistently surpasses the manually designed meta-structures, i.e., meta-paths. Extensive experiments on six benchmark datasets over two representative tasks, including node classification and recommendation, demonstrate the effectiveness of the proposed method. Our approach outperforms the state-of-the-art heterogeneous GNNs, finds out meaningful meta multigraphs, and is significantly more stable.

</details>

<details>

<summary>2022-11-27 09:27:50 - Class-aware Information for Logit-based Knowledge Distillation</summary>

- *Shuoxi Zhang, Hanpeng Liu, John E. Hopcroft, Kun He*

- `2211.14773v1` - [abs](http://arxiv.org/abs/2211.14773v1) - [pdf](http://arxiv.org/pdf/2211.14773v1)

> Knowledge distillation aims to transfer knowledge to the student model by utilizing the predictions/features of the teacher model, and feature-based distillation has recently shown its superiority over logit-based distillation. However, due to the cumbersome computation and storage of extra feature transformation, the training overhead of feature-based methods is much higher than that of logit-based distillation. In this work, we revisit the logit-based knowledge distillation, and observe that the existing logit-based distillation methods treat the prediction logits only in the instance level, while many other useful semantic information is overlooked. To address this issue, we propose a Class-aware Logit Knowledge Distillation (CLKD) method, that extents the logit distillation in both instance-level and class-level. CLKD enables the student model mimic higher semantic information from the teacher model, hence improving the distillation performance. We further introduce a novel loss called Class Correlation Loss to force the student learn the inherent class-level correlation of the teacher. Empirical comparisons demonstrate the superiority of the proposed method over several prevailing logit-based methods and feature-based methods, in which CLKD achieves compelling results on various visual classification tasks and outperforms the state-of-the-art baselines.

</details>

<details>

<summary>2022-11-27 11:07:37 - Devils in the Clouds: An Evolutionary Study of Telnet Bot Loaders</summary>

- *Yuhui Zhu, Zhenxiang Chen, Qiben Yan, Shanshan Wang, Alberto Giaretta, Enlong Li, Lizhi Peng, Chuan Zhao, Mauro Conti*

- `2211.14790v1` - [abs](http://arxiv.org/abs/2211.14790v1) - [pdf](http://arxiv.org/pdf/2211.14790v1)

> One of the innovations brought by Mirai and its derived malware is the adoption of self-contained loaders for infecting IoT devices and recruiting them in botnets. Functionally decoupled from other botnet components and not embedded in the payload, loaders cannot be analysed using conventional approaches that rely on honeypots for capturing samples. Different approaches are necessary for studying the loaders evolution and defining a genealogy. To address the insufficient knowledge about loaders' lineage in existing studies, in this paper, we propose a semantic-aware method to measure, categorize, and compare different loader servers, with the goal of highlighting their evolution, independent from the payload evolution. Leveraging behavior-based metrics, we cluster the discovered loaders and define eight families to determine the genealogy and draw a homology map. Our study shows that the source code of Mirai is evolving and spawning new botnets with new capabilities, both on the client side and the server side. In turn, shedding light on the infection loaders can help the cybersecurity community to improve detection and prevention tools.

</details>

<details>

<summary>2022-11-27 12:16:40 - Doubly-Asynchronous Value Iteration: Making Value Iteration Asynchronous in Actions</summary>

- *Tian Tian, Kenny Young, Richard S. Sutton*

- `2207.01613v2` - [abs](http://arxiv.org/abs/2207.01613v2) - [pdf](http://arxiv.org/pdf/2207.01613v2)

> Value iteration (VI) is a foundational dynamic programming method, important for learning and planning in optimal control and reinforcement learning. VI proceeds in batches, where the update to the value of each state must be completed before the next batch of updates can begin. Completing a single batch is prohibitively expensive if the state space is large, rendering VI impractical for many applications. Asynchronous VI helps to address the large state space problem by updating one state at a time, in-place and in an arbitrary order. However, Asynchronous VI still requires a maximization over the entire action space, making it impractical for domains with large action space. To address this issue, we propose doubly-asynchronous value iteration (DAVI), a new algorithm that generalizes the idea of asynchrony from states to states and actions. More concretely, DAVI maximizes over a sampled subset of actions that can be of any user-defined size. This simple approach of using sampling to reduce computation maintains similarly appealing theoretical properties to VI without the need to wait for a full sweep through the entire action space in each update. In this paper, we show DAVI converges to the optimal value function with probability one, converges at a near-geometric rate with probability 1-delta, and returns a near-optimal policy in computation time that nearly matches a previously established bound for VI. We also empirically demonstrate DAVI's effectiveness in several experiments.

</details>

<details>

<summary>2022-11-27 13:53:52 - Combined Peak Reduction and Self-Consumption Using Proximal Policy Optimization</summary>

- *Thijs Peirelinck, Chris Hermans, Fred Spiessens, Geert Deconinck*

- `2211.14831v1` - [abs](http://arxiv.org/abs/2211.14831v1) - [pdf](http://arxiv.org/pdf/2211.14831v1)

> Residential demand response programs aim to activate demand flexibility at the household level. In recent years, reinforcement learning (RL) has gained significant attention for these type of applications. A major challenge of RL algorithms is data efficiency. New RL algorithms, such as proximal policy optimisation (PPO), have tried to increase data efficiency. Additionally, combining RL with transfer learning has been proposed in an effort to mitigate this challenge. In this work, we further improve upon state-of-the-art transfer learning performance by incorporating demand response domain knowledge into the learning pipeline. We evaluate our approach on a demand response use case where peak shaving and self-consumption is incentivised by means of a capacity tariff. We show our adapted version of PPO, combined with transfer learning, reduces cost by 14.51% compared to a regular hysteresis controller and by 6.68% compared to traditional PPO.

</details>

<details>

<summary>2022-11-27 16:54:24 - Managing Controlled Unclassified Information in Research Institutions</summary>

- *Baijian Yang, Carolyn Ellis, Preston Smith, Huyunting Huang*

- `2211.14886v1` - [abs](http://arxiv.org/abs/2211.14886v1) - [pdf](http://arxiv.org/pdf/2211.14886v1)

> In order to operate in a regulated world, researchers need to ensure compliance with ever-evolving landscape of information security regulations and best practices. This work explains the concept of Controlled Unclassified Information (CUI) and the challenges it brings to the research institutions. Survey from the user perceptions showed that most researchers and IT administrators lack a good understanding of CUI and how it is related to other regulations, such as HIPAA, ITAR, GLBA, and FERPA. A managed research ecosystem is introduced in this work. The workflow of this efficient and cost effective framework is elaborated to demonstrate how controlled research data are processed to be compliant with one of the highest level of cybersecurity in a campus environment. Issues beyond the framework itself is also discussed. The framework serves as a reference model for other institutions to support CUI research. The awareness and training program developed from this work will be shared with other institutions to build a bigger CUI ecosystem.

</details>

<details>

<summary>2022-11-27 19:45:38 - Unsupervised Opinion Summarisation in the Wasserstein Space</summary>

- *Jiayu Song, Iman Munire Bilal, Adam Tsakalidis, Rob Procter, Maria Liakata*

- `2211.14923v1` - [abs](http://arxiv.org/abs/2211.14923v1) - [pdf](http://arxiv.org/pdf/2211.14923v1)

> Opinion summarisation synthesises opinions expressed in a group of documents discussing the same topic to produce a single summary. Recent work has looked at opinion summarisation of clusters of social media posts. Such posts are noisy and have unpredictable structure, posing additional challenges for the construction of the summary distribution and the preservation of meaning compared to online reviews, which has been so far the focus of opinion summarisation. To address these challenges we present \textit{WassOS}, an unsupervised abstractive summarization model which makes use of the Wasserstein distance. A Variational Autoencoder is used to get the distribution of documents/posts, and the distributions are disentangled into separate semantic and syntactic spaces. The summary distribution is obtained using the Wasserstein barycenter of the semantic and syntactic distributions. A latent variable sampled from the summary distribution is fed into a GRU decoder with a transformer layer to produce the final summary. Our experiments on multiple datasets including Twitter clusters, Reddit threads, and reviews show that WassOS almost always outperforms the state-of-the-art on ROUGE metrics and consistently produces the best summaries with respect to meaning preservation according to human evaluations.

</details>

<details>

<summary>2022-11-27 20:24:56 - BEV-Locator: An End-to-end Visual Semantic Localization Network Using Multi-View Images</summary>

- *Zhihuang Zhang, Meng Xu, Wenqiang Zhou, Tao Peng, Liang Li, Stefan Poslad*

- `2211.14927v1` - [abs](http://arxiv.org/abs/2211.14927v1) - [pdf](http://arxiv.org/pdf/2211.14927v1)

> Accurate localization ability is fundamental in autonomous driving. Traditional visual localization frameworks approach the semantic map-matching problem with geometric models, which rely on complex parameter tuning and thus hinder large-scale deployment. In this paper, we propose BEV-Locator: an end-to-end visual semantic localization neural network using multi-view camera images. Specifically, a visual BEV (Birds-Eye-View) encoder extracts and flattens the multi-view images into BEV space. While the semantic map features are structurally embedded as map queries sequence. Then a cross-model transformer associates the BEV features and semantic map queries. The localization information of ego-car is recursively queried out by cross-attention modules. Finally, the ego pose can be inferred by decoding the transformer outputs. We evaluate the proposed method in large-scale nuScenes and Qcraft datasets. The experimental results show that the BEV-locator is capable to estimate the vehicle poses under versatile scenarios, which effectively associates the cross-model information from multi-view images and global semantic maps. The experiments report satisfactory accuracy with mean absolute errors of 0.052m, 0.135m and 0.251$^\circ$ in lateral, longitudinal translation and heading angle degree.

</details>

<details>

<summary>2022-11-27 22:17:16 - Topic Segmentation in the Wild: Towards Segmentation of Semi-structured & Unstructured Chats</summary>

- *Reshmi Ghosh, Harjeet Singh Kajal, Sharanya Kamath, Dhuri Shrivastava, Samyadeep Basu, Soundararajan Srinivasan*

- `2211.14954v1` - [abs](http://arxiv.org/abs/2211.14954v1) - [pdf](http://arxiv.org/pdf/2211.14954v1)

> Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured texts. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin.

</details>

<details>

<summary>2022-11-28 02:26:24 - An Exploration of Cross-Patch Collaborations via Patch Linkage in OpenStack</summary>

- *Dong Wang, Patanamon Thongtanunam, Raula Gaikovina Kula, Kenichi Matsumoto*

- `2211.15007v1` - [abs](http://arxiv.org/abs/2211.15007v1) - [pdf](http://arxiv.org/pdf/2211.15007v1)

> Contemporary development projects benefit from code review as it improves the quality of a project. Large ecosystems of inter-dependent projects like OpenStack generate a large number of reviews, which poses new challenges for collaboration (improving patches, fixing defects). Review tools allow developers to link between patches, to indicate patch dependency, competing solutions, or provide broader context. We hypothesize that such patch linkage may also simulate cross-collaboration.   With a case study of OpenStack, we take a first step to explore collaborations that occur after a patch linkage was posted between two patches (i.e., cross-patch collaboration). Our empirical results show that although patch linkage that requests collaboration is relatively less prevalent, the probability of collaboration is relatively higher. Interestingly, the results also show that collaborative contributions via patch linkage are non-trivial, i.e, contributions can affect the review outcome (such as voting) or even improve the patch (i.e., revising). This work opens up future directions to understand barriers and opportunities related to this new kind of collaboration, that assists with code review and development tasks in large ecosystems.

</details>

<details>

<summary>2022-11-28 08:48:28 - LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation</summary>

- *Hongcheng Guo, Jiaheng Liu, Haoyang Huang, Jian Yang, Zhoujun Li, Dongdong Zhang, Zheng Cui, Furu Wei*

- `2210.15461v2` - [abs](http://arxiv.org/abs/2210.15461v2) - [pdf](http://arxiv.org/pdf/2210.15461v2)

> Multimodal Machine Translation (MMT) focuses on enhancing text-only translation with visual features, which has attracted considerable attention from both natural language processing and computer vision communities. Recent advances still struggle to train a separate model for each language pair, which is costly and unaffordable when the number of languages increases in the real world. In other words, the multilingual multimodal machine translation (Multilingual MMT) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for multiple languages. Besides, the image modality has no language boundaries, which is superior to bridging the semantic gap between languages. To this end, we first propose the Multilingual MMT task by establishing two new Multilingual MMT benchmark datasets covering seven languages. Then, an effective baseline LVP-M3 using visual prompts is proposed to support translations between different languages, which includes three stages (token encoding, language-aware visual prompt generation, and language translation). Extensive experimental results on our constructed benchmark datasets demonstrate the effectiveness of LVP-M3 method for Multilingual MMT.

</details>

<details>

<summary>2022-11-28 09:08:19 - Deep Semi-supervised Learning with Double-Contrast of Features and Semantics</summary>

- *Quan Feng, Jiayu Yao, Zhison Pan, Guojun Zhou*

- `2211.15671v1` - [abs](http://arxiv.org/abs/2211.15671v1) - [pdf](http://arxiv.org/pdf/2211.15671v1)

> In recent years, the field of intelligent transportation systems (ITS) has achieved remarkable success, which is mainly due to the large amount of available annotation data. However, obtaining these annotated data has to afford expensive costs in reality. Therefore, a more realistic strategy is to leverage semi-supervised learning (SSL) with a small amount of labeled data and a large amount of unlabeled data. Typically, semantic consistency regularization and the two-stage learning methods of decoupling feature extraction and classification have been proven effective. Nevertheless, representation learning only limited to semantic consistency regularization may not guarantee the separation or discriminability of representations of samples with different semantics; due to the inherent limitations of the two-stage learning methods, the extracted features may not match the specific downstream tasks. In order to deal with the above drawbacks, this paper proposes an end-to-end deep semi-supervised learning double contrast of semantic and feature, which extracts effective tasks specific discriminative features by contrasting the semantics/features of positive and negative augmented samples pairs. Moreover, we leverage information theory to explain the rationality of double contrast of semantics and features and slack mutual information to contrastive loss in a simpler way. Finally, the effectiveness of our method is verified in benchmark datasets.

</details>

<details>

<summary>2022-11-28 10:18:06 - Angular triangle distance for ordinal metric learning</summary>

- *Imam Mustafa Kamal, Hyerim Bae*

- `2211.15200v1` - [abs](http://arxiv.org/abs/2211.15200v1) - [pdf](http://arxiv.org/pdf/2211.15200v1)

> Deep metric learning (DML) aims to automatically construct task-specific distances or similarities of data, resulting in a low-dimensional representation. Several significant metric-learning methods have been proposed. Nonetheless, no approach guarantees the preservation of the ordinal nature of the original data in a low-dimensional space. Ordinal data are ubiquitous in real-world problems, such as the severity of symptoms in biomedical cases, production quality in manufacturing, rating level in businesses, and aging level in face recognition. This study proposes a novel angular triangle distance (ATD) and ordinal triplet network (OTD) to obtain an accurate and meaningful embedding space representation for ordinal data. The ATD projects the ordinal relation of data in the angular space, whereas the OTD learns its ordinal projection. We also demonstrated that our new distance measure satisfies the distance metric properties mathematically. The proposed method was assessed using real-world data with an ordinal nature, such as biomedical, facial, and hand-gestured images. Extensive experiments have been conducted, and the results show that our proposed method not only semantically preserves the ordinal nature but is also more accurate than existing DML models. Moreover, we also demonstrate that our proposed method outperforms the state-of-the-art ordinal metric learning method.

</details>

<details>

<summary>2022-11-28 11:27:50 - Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers</summary>

- *Wanqian Yang, Polina Kirichenko, Micah Goldblum, Andrew Gordon Wilson*

- `2211.15231v1` - [abs](http://arxiv.org/abs/2211.15231v1) - [pdf](http://arxiv.org/pdf/2211.15231v1)

> Deep neural networks are susceptible to shortcut learning, using simple features to achieve low training loss without discovering essential semantic structure. Contrary to prior belief, we show that generative models alone are not sufficient to prevent shortcut learning, despite an incentive to recover a more comprehensive representation of the data than discriminative approaches. However, we observe that shortcuts are preferentially encoded with minimal information, a fact that generative models can exploit to mitigate shortcut learning. In particular, we propose Chroma-VAE, a two-pronged approach where a VAE classifier is initially trained to isolate the shortcut in a small latent subspace, allowing a secondary classifier to be trained on the complementary, shortcut-free latent subspace. In addition to demonstrating the efficacy of Chroma-VAE on benchmark and real-world shortcut learning tasks, our work highlights the potential for manipulating the latent space of generative classifiers to isolate or interpret specific correlations.

</details>

<details>

<summary>2022-11-28 12:35:42 - LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection</summary>

- *Zhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Pan, Ningyu Zhang, Wen Zhang*

- `2207.12888v2` - [abs](http://arxiv.org/abs/2207.12888v2) - [pdf](http://arxiv.org/pdf/2207.12888v2)

> Visual question answering (VQA) often requires an understanding of visual concepts and language semantics, which relies on external knowledge. Most existing methods exploit pre-trained language models or/and unstructured text, but the knowledge in these resources are often incomplete and noisy. Some other methods prefer to use knowledge graphs (KGs) which often have intensive structured knowledge, but the research is still quite preliminary. In this paper, we propose LaKo, a knowledge-driven VQA method via Late Knowledge-to-text Injection. To effectively incorporate an external KG, we transfer triples into textual format and propose a late injection mechanism for knowledge fusion. Finally we address VQA as a text generation task with an effective encoder-decoder paradigm, which achieves state-of-the-art results on OKVQA dataset.

</details>

<details>

<summary>2022-11-28 12:36:07 - Investigating Black-Box Function Recognition Using Hardware Performance Counters</summary>

- *Carlton Shepherd, Benjamin Semal, Konstantinos Markantonakis*

- `2204.11639v5` - [abs](http://arxiv.org/abs/2204.11639v5) - [pdf](http://arxiv.org/pdf/2204.11639v5)

> This paper presents new methods and results for recognising black-box program functions using hardware performance counters (HPC), where an investigator can invoke and measure function calls. Important use cases include analysing compiled libraries, e.g. static and dynamic link libraries, and trusted execution environment (TEE) applications. We develop a generic approach to classify a comprehensive set of hardware events, e.g. branch mis-predictions and instruction retirements, to recognise standard benchmarking and cryptographic library functions. This includes various signing, verification and hash functions, and ciphers in numerous modes of operation. Three architectures are evaluated using off-the-shelf Intel/X86-64, ARM, and RISC-V CPUs. Next, we show that several known CVE-numbered OpenSSL vulnerabilities can be detected using HPC differences between patched and unpatched library versions. Further, we demonstrate that standardised cryptographic functions within ARM TrustZone TEE applications can be recognised using non-secure world HPC measurements, applying to platforms that insecurely perturb the performance monitoring unit (PMU) during TEE execution. High accuracy was achieved in all cases (86.22-99.83%) depending on the application, architectural, and compilation assumptions. Lastly, we discuss mitigations, outstanding challenges, and directions for future research.

</details>

<details>

<summary>2022-11-28 14:04:51 - Low-resource Personal Attribute Prediction from Conversation</summary>

- *Yinan Liu, Hu Chen, Wei Shen, Jiaoyan Chen*

- `2211.15324v1` - [abs](http://arxiv.org/abs/2211.15324v1) - [pdf](http://arxiv.org/pdf/2211.15324v1)

> Personal knowledge bases (PKBs) are crucial for a broad range of applications such as personalized recommendation and Web-based chatbots. A critical challenge to build PKBs is extracting personal attribute knowledge from users' conversation data. Given some users of a conversational system, a personal attribute and these users' utterances, our goal is to predict the ranking of the given personal attribute values for each user. Previous studies often rely on a relative number of resources such as labeled utterances and external data, yet the attribute knowledge embedded in unlabeled utterances is underutilized and their performance of predicting some difficult personal attributes is still unsatisfactory. In addition, it is found that some text classification methods could be employed to resolve this task directly. However, they also perform not well over those difficult personal attributes. In this paper, we propose a novel framework PEARL to predict personal attributes from conversations by leveraging the abundant personal attribute knowledge from utterances under a low-resource setting in which no labeled utterances or external data are utilized. PEARL combines the biterm semantic information with the word co-occurrence information seamlessly via employing the updated prior attribute knowledge to refine the biterm topic model's Gibbs sampling process in an iterative manner. The extensive experimental results show that PEARL outperforms all the baseline methods not only on the task of personal attribute prediction from conversations over two data sets, but also on the more general weakly supervised text classification task over one data set.

</details>

<details>

<summary>2022-11-28 15:37:22 - Memory-efficient array redistribution through portable collective communication</summary>

- *Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, Georg Stefan Schmid*

- `2112.01075v2` - [abs](http://arxiv.org/abs/2112.01075v2) - [pdf](http://arxiv.org/pdf/2112.01075v2)

> Modern large-scale deep learning workloads highlight the need for parallel execution across many devices in order to fit model data into hardware accelerator memories. In these settings, array redistribution may be required during a computation, but can also become a bottleneck if not done efficiently. In this paper we address the problem of redistributing multi-dimensional array data in SPMD computations, the most prevalent form of parallelism in deep learning. We present a type-directed approach to synthesizing array redistributions as sequences of MPI-style collective operations. We prove formally that our synthesized redistributions are memory-efficient and perform no excessive data transfers. Array redistribution for SPMD computations using collective operations has also been implemented in the context of the XLA SPMD partitioner, a production-grade tool for partitioning programs across accelerator systems. We evaluate our approach against the XLA implementation and find that our approach delivers a geometric mean speedup of $1.22\times$, with maximum speedups as a high as $5.7\times$, while offering provable memory guarantees, making our system particularly appealing for large-scale models.

</details>

<details>

<summary>2022-11-28 17:35:08 - GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement</summary>

- *Zhi-Qi Cheng, Qi Dai, Siyao Li, Teruko Mitamura, Alexander G. Hauptmann*

- `2208.08965v4` - [abs](http://arxiv.org/abs/2208.08965v4) - [pdf](http://arxiv.org/pdf/2208.08965v4)

> Grounded Situation Recognition (GSR) aims to generate structured semantic summaries of images for "human-like" event understanding. Specifically, GSR task not only detects the salient activity verb (e.g. buying), but also predicts all corresponding semantic roles (e.g. agent and goods). Inspired by object detection and image captioning tasks, existing methods typically employ a two-stage framework: 1) detect the activity verb, and then 2) predict semantic roles based on the detected verb. Obviously, this illogical framework constitutes a huge obstacle to semantic understanding. First, pre-detecting verbs solely without semantic roles inevitably fails to distinguish many similar daily activities (e.g., offering and giving, buying and selling). Second, predicting semantic roles in a closed auto-regressive manner can hardly exploit the semantic relations among the verb and roles. To this end, in this paper we propose a novel two-stage framework that focuses on utilizing such bidirectional relations within verbs and roles. In the first stage, instead of pre-detecting the verb, we postpone the detection step and assume a pseudo label, where an intermediate representation for each corresponding semantic role is learned from images. In the second stage, we exploit transformer layers to unearth the potential semantic relations within both verbs and semantic roles. With the help of a set of support images, an alternate learning scheme is designed to simultaneously optimize the results: update the verb using nouns corresponding to the image, and update nouns using verbs from support images. Extensive experimental results on challenging SWiG benchmarks show that our renovated framework outperforms other state-of-the-art methods under various metrics.

</details>

<details>

<summary>2022-11-28 18:02:06 - Development of an Equation-based Parallelization Method for Multiphase Particle-in-Cell Simulations</summary>

- *Mino Woo, Terry Jordan, Tarak Nandi, Jean Francois Dietiker, Christopher Guenther, Dirk Van Essendelft*

- `2211.15605v1` - [abs](http://arxiv.org/abs/2211.15605v1) - [pdf](http://arxiv.org/pdf/2211.15605v1)

> Manufacturers have been developing new graphics processing unit (GPU) nodes with large capacity, high bandwidth memory and very high bandwidth intra-node interconnects. This enables moving large amounts of data between GPUs on the same node at low cost. However, small packet bandwidths and latencies have not decreased which makes global dot products expensive. These characteristics favor a new kind of problem decomposition called "equation decomposition" rather than traditional domain decomposition. In this approach, each GPU is assigned one equation set to solve in parallel so that the frequent and expensive dot product synchronization points in traditional distributed linear solvers are eliminated. In exchange, the method involves infrequent movement of state variables over the high bandwidth, intra-node interconnects. To test this theory, our flagship code Multiphase Flow with Interphase eXchanges (MFiX) was ported to TensorFlow. This new product is known as MFiX-AI and can produce near identical results to the original version of MFiX with significant acceleration in multiphase particle-in-cell (MP-PIC) simulations. The performance of a single node with 4 NVIDIA A100s connected over NVLINK 2.0 was shown to be competitive to 1000 CPU cores (25 nodes) on the JOULE 2.0 supercomputer, leading to an energy savings of up to 90%. This is a substantial performance benefit for small- to intermediate-sized problems. This benefit is expected to grow as GPU nodes become more powerful. Further, MFiX-AI is poised to accept native artificial intelligence/machine learning models for further acceleration and development.

</details>

<details>

<summary>2022-11-28 18:02:14 - Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation</summary>

- *Kaustubh D. Dhole, Christopher D. Manning*

- `2004.08694v5` - [abs](http://arxiv.org/abs/2004.08694v5) - [pdf](http://arxiv.org/pdf/2004.08694v5)

> Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing SynQG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.

</details>

<details>

<summary>2022-11-28 19:05:32 - The AI Definition and a Program Which Satisfies this Definition</summary>

- *Dimiter Dobrev*

- `2212.03184v1` - [abs](http://arxiv.org/abs/2212.03184v1) - [pdf](http://arxiv.org/pdf/2212.03184v1)

> We will consider all policies of the agent and will prove that one of them is the best performing policy. While that policy is not computable, computable policies do exist in its proximity. We will define AI as a computable policy which is sufficiently proximal to the best performing policy. Before we can define the agent's best performing policy, we need a language for description of the world. We will also use this language to develop a program which satisfies the AI definition. The program will first understand the world by describing it in the selected language. The program will then use the description in order to predict the future and select the best possible move. While this program is extremely inefficient and practically unusable, it can be improved by refining both the language for description of the world and the algorithm used to predict the future. This can yield a program which is both efficient and consistent with the AI definition.

</details>

<details>

<summary>2022-11-28 19:51:30 - Sketch-and-solve approaches to k-means clustering by semidefinite programming</summary>

- *Charles Clum, Dustin G. Mixon, Soledad Villar, Kaiying Xie*

- `2211.15744v1` - [abs](http://arxiv.org/abs/2211.15744v1) - [pdf](http://arxiv.org/pdf/2211.15744v1)

> We introduce a sketch-and-solve approach to speed up the Peng-Wei semidefinite relaxation of k-means clustering. When the data is appropriately separated we identify the k-means optimal clustering. Otherwise, our approach provides a high-confidence lower bound on the optimal k-means value. This lower bound is data-driven; it does not make any assumption on the data nor how it is generated. We provide code and an extensive set of numerical experiments where we use this approach to certify approximate optimality of clustering solutions obtained by k-means++.

</details>

<details>

<summary>2022-11-28 21:32:52 - Towards Preserving Semantic Structure in Argumentative Multi-Agent via Abstract Interpretation</summary>

- *Minal Suresh Patil*

- `2211.15782v1` - [abs](http://arxiv.org/abs/2211.15782v1) - [pdf](http://arxiv.org/pdf/2211.15782v1)

> Over the recent twenty years, argumentation has received considerable attention in the fields of knowledge representation, reasoning, and multi-agent systems. However, argumentation in dynamic multi-agent systems encounters the problem of significant arguments generated by agents, which comes at the expense of representational complexity and computational cost. In this work, we aim to investigate the notion of abstraction from the model-checking perspective, where several arguments are trying to defend the same position from various points of view, thereby reducing the size of the argumentation framework whilst preserving the semantic flow structure in the system.

</details>

<details>

<summary>2022-11-28 21:36:49 - Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing</summary>

- *Yi Nian, Xinyue Hu, Rui Zhang, Jingna Feng, Jingcheng Du, Fang Li, Yong Chen, Cui Tao*

- `2202.08712v4` - [abs](http://arxiv.org/abs/2202.08712v4) - [pdf](http://arxiv.org/pdf/2202.08712v4)

> To date, there are no effective treatments for most neurodegenerative diseases. Knowledge graphs can provide comprehensive and semantic representation for heterogeneous data, and have been successfully leveraged in many biomedical applications including drug repurposing. Our objective is to construct a knowledge graph from literature to study relations between Alzheimer's disease (AD) and chemicals, drugs and dietary supplements in order to identify opportunities to prevent or delay neurodegenerative progression. We collected biomedical annotations and extracted their relations using SemRep via SemMedDB. We used both a BERT-based classifier and rule-based methods during data preprocessing to exclude noise while preserving most AD-related semantic triples. The 1,672,110 filtered triples were used to train with knowledge graph completion algorithms (i.e., TransE, DistMult, and ComplEx) to predict candidates that might be helpful for AD treatment or prevention. Among three knowledge graph completion models, TransE outperformed the other two (MR = 13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further evaluate the prediction results. We found supporting evidence for most highly ranked candidates predicted by our model which indicates that our approach can inform reliable new knowledge. This paper shows that our graph mining model can predict reliable new relationships between AD and other entities (i.e., dietary supplements, chemicals, and drugs). The knowledge graph constructed can facilitate data-driven knowledge discoveries and the generation of novel hypotheses.

</details>

<details>

<summary>2022-11-28 22:29:58 - Deep Inventory Management</summary>

- *Dhruv Madeka, Kari Torkkola, Carson Eisenach, Anna Luo, Dean P. Foster, Sham M. Kakade*

- `2210.03137v3` - [abs](http://arxiv.org/abs/2210.03137v3) - [pdf](http://arxiv.org/pdf/2210.03137v3)

> This work provides a Deep Reinforcement Learning approach to solving a periodic review inventory control system with stochastic vendor lead times, lost sales, correlated demand, and price matching. While this dynamic program has historically been considered intractable, our results show that several policy learning approaches are competitive with or outperform classical methods. In order to train these algorithms, we develop novel techniques to convert historical data into a simulator. On the theoretical side, we present learnability results on a subclass of inventory control problems, where we provide a provable reduction of the reinforcement learning problem to that of supervised learning. On the algorithmic side, we present a model-based reinforcement learning procedure (Direct Backprop) to solve the periodic review inventory control problem by constructing a differentiable simulator. Under a variety of metrics Direct Backprop outperforms model-free RL and newsvendor baselines, in both simulations and real-world deployments.

</details>

<details>

<summary>2022-11-29 00:23:53 - Quasi-stable Coloring for Graph Compression: Approximating Max-Flow, Linear Programs, and Centrality</summary>

- *Moe Kayali, Dan Suciu*

- `2211.11912v2` - [abs](http://arxiv.org/abs/2211.11912v2) - [pdf](http://arxiv.org/pdf/2211.11912v2)

> We propose quasi-stable coloring, an approximate version of stable coloring. Stable coloring, also called color refinement, is a well-studied technique in graph theory for classifying vertices, which can be used to build compact, lossless representations of graphs. However, its usefulness is limited due to its reliance on strict symmetries. Real data compresses very poorly using color refinement. We propose the first, to our knowledge, approximate color refinement scheme, which we call quasi-stable coloring. By using approximation, we alleviate the need for strict symmetry, and allow for a tradeoff between the degree of compression and the accuracy of the representation. We study three applications: Linear Programming, Max-Flow, and Betweenness Centrality, and provide theoretical evidence in each case that a quasi-stable coloring can lead to good approximations on the reduced graph. Next, we consider how to compute a maximal quasi-stable coloring: we prove that, in general, this problem is NP-hard, and propose a simple, yet effective algorithm based on heuristics. Finally, we evaluate experimentally the quasi-stable coloring technique on several real graphs and applications, comparing with prior approximation techniques.   A reference implementation and the experiment code are available at https://github.com/mkyl/QuasiStableColors.jl .

</details>

<details>

<summary>2022-11-29 03:51:40 - Performance Evaluation, Optimization and Dynamic Decision in Blockchain Systems: A Recent Overview</summary>

- *Quan-Lin Li, Yan-Xia Chang, Qing Wang*

- `2211.15907v1` - [abs](http://arxiv.org/abs/2211.15907v1) - [pdf](http://arxiv.org/pdf/2211.15907v1)

> With rapid development of blockchain technology as well as integration of various application areas, performance evaluation, performance optimization, and dynamic decision in blockchain systems are playing an increasingly important role in developing new blockchain technology. This paper provides a recent systematic overview of this class of research, and especially, developing mathematical modeling and basic theory of blockchain systems. Important examples include (a) performance evaluation: Markov processes, queuing theory, Markov reward processes, random walks, fluid and diffusion approximations, and martingale theory; (b) performance optimization: Linear programming, nonlinear programming, integer programming, and multi-objective programming; (c) optimal control and dynamic decision: Markov decision processes, and stochastic optimal control; and (d) artificial intelligence: Machine learning, deep reinforcement learning, and federated learning. So far, a little research has focused on these research lines. We believe that the basic theory with mathematical methods, algorithms and simulations of blockchain systems discussed in this paper will strongly support future development and continuous innovation of blockchain technology.

</details>

<details>

<summary>2022-11-29 04:17:43 - Similarity Distribution based Membership Inference Attack on Person Re-identification</summary>

- *Junyao Gao, Xinyang Jiang, Huishuai Zhang, Yifan Yang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao*

- `2211.15918v1` - [abs](http://arxiv.org/abs/2211.15918v1) - [pdf](http://arxiv.org/pdf/2211.15918v1)

> While person Re-identification (Re-ID) has progressed rapidly due to its wide real-world applications, it also causes severe risks of leaking personal information from training data. Thus, this paper focuses on quantifying this risk by membership inference (MI) attack. Most of the existing MI attack algorithms focus on classification models, while Re-ID follows a totally different training and inference paradigm. Re-ID is a fine-grained recognition task with complex feature embedding, and model outputs commonly used by existing MI like logits and losses are not accessible during inference. Since Re-ID focuses on modelling the relative relationship between image pairs instead of individual semantics, we conduct a formal and empirical analysis which validates that the distribution shift of the inter-sample similarity between training and test set is a critical criterion for Re-ID membership inference. As a result, we propose a novel membership inference attack method based on the inter-sample similarity distribution. Specifically, a set of anchor images are sampled to represent the similarity distribution conditioned on a target image, and a neural network with a novel anchor selection module is proposed to predict the membership of the target image. Our experiments validate the effectiveness of the proposed approach on both the Re-ID task and conventional classification task.

</details>

<details>

<summary>2022-11-29 05:25:10 - Program Merge Conflict Resolution via Neural Transformers</summary>

- *Alexey Svyatkovskiy, Sarah Fakhoury, Negar Ghorbani, Todd Mytkowicz, Elizabeth Dinella, Christian Bird, Jinu Jang, Neel Sundaresan, Shuvendu Lahiri*

- `2109.00084v4` - [abs](http://arxiv.org/abs/2109.00084v4) - [pdf](http://arxiv.org/pdf/2109.00084v4)

> Collaborative software development is an integral part of the modern software development life cycle, essential to the success of large-scale software projects. When multiple developers make concurrent changes around the same lines of code, a merge conflict may occur. Such conflicts stall pull requests and continuous integration pipelines for hours to several days, seriously hurting developer productivity. To address this problem, we introduce MergeBERT, a novel neural program merge framework based on token-level three-way differencing and a transformer encoder model. By exploiting the restricted nature of merge conflict resolutions, we reformulate the task of generating the resolution sequence as a classification task over a set of primitive merge patterns extracted from real-world merge commit data. Our model achieves 63-68% accuracy for merge resolution synthesis, yielding nearly a 3x performance improvement over existing semi-structured, and 2x improvement over neural program merge tools. Finally, we demonstrate that MergeBERT is sufficiently flexible to work with source code files in Java, JavaScript, TypeScript, and C# programming languages. To measure the practical use of MergeBERT, we conduct a user study to evaluate MergeBERT suggestions with 25 developers from large OSS projects on 122 real-world conflicts they encountered. Results suggest that in practice, MergeBERT resolutions would be accepted at a higher rate than estimated by automatic metrics for precision and accuracy. Additionally, we use participant feedback to identify future avenues for improvement of MergeBERT.

</details>

<details>

<summary>2022-11-29 07:06:45 - Democratizing Machine Learning for Interdisciplinary Scholars: Report on Organizing the NLP+CSS Online Tutorial Series</summary>

- *Ian Stewart, Katherine Keith*

- `2211.15971v1` - [abs](http://arxiv.org/abs/2211.15971v1) - [pdf](http://arxiv.org/pdf/2211.15971v1)

> Many scientific fields -- including biology, health, education, and the social sciences -- use machine learning (ML) to help them analyze data at an unprecedented scale. However, ML researchers who develop advanced methods rarely provide detailed tutorials showing how to apply these methods. Existing tutorials are often costly to participants, presume extensive programming knowledge, and are not tailored to specific application fields. In an attempt to democratize ML methods, we organized a year-long, free, online tutorial series targeted at teaching advanced natural language processing (NLP) methods to computational social science (CSS) scholars. Two organizers worked with fifteen subject matter experts to develop one-hour presentations with hands-on Python code for a range of ML methods and use cases, from data pre-processing to analyzing temporal variation of language change. Although live participation was more limited than expected, a comparison of pre- and post-tutorial surveys showed an increase in participants' perceived knowledge of almost one point on a 7-point Likert scale. Furthermore, participants asked thoughtful questions during tutorials and engaged readily with tutorial content afterwards, as demonstrated by 10K~total views of posted tutorial recordings. In this report, we summarize our organizational efforts and distill five principles for democratizing ML+X tutorials. We hope future organizers improve upon these principles and continue to lower barriers to developing ML skills for researchers of all fields.

</details>

<details>

<summary>2022-11-29 08:41:02 - Token-Label Alignment for Vision Transformers</summary>

- *Han Xiao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu*

- `2210.06455v2` - [abs](http://arxiv.org/abs/2210.06455v2) - [pdf](http://arxiv.org/pdf/2210.06455v2)

> Data mixing strategies (e.g., CutMix) have shown the ability to greatly improve the performance of convolutional neural networks (CNNs). They mix two images as inputs for training and assign them with a mixed label with the same ratio. While they are shown effective for vision transformers (ViTs), we identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies. We empirically observe that the contributions of input tokens fluctuate as forward propagating, which might induce a different mixing ratio in the output tokens. The training target computed by the original data mixing strategy can thus be inaccurate, resulting in less effective training. To address this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. We reuse the computed attention at each layer for efficient token-label alignment, introducing only negligible additional training costs. Extensive experiments demonstrate that our method improves the performance of ViTs on image classification, semantic segmentation, objective detection, and transfer learning tasks. Code is available at: https://github.com/Euphoria16/TL-Align.

</details>

<details>

<summary>2022-11-29 08:44:09 - Textual Enhanced Contrastive Learning for Solving Math Word Problems</summary>

- *Yibin Shen, Qianying Liu, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi*

- `2211.16022v1` - [abs](http://arxiv.org/abs/2211.16022v1) - [pdf](http://arxiv.org/pdf/2211.16022v1)

> Solving math word problems is the task that analyses the relation of quantities and requires an accurate understanding of contextual natural language information. Recent studies show that current models rely on shallow heuristics to predict solutions and could be easily misled by small textual perturbations. To address this problem, we propose a Textual Enhanced Contrastive Learning framework, which enforces the models to distinguish semantically similar examples while holding different mathematical logic. We adopt a self-supervised manner strategy to enrich examples with subtle textual variance by textual reordering or problem re-construction. We then retrieve the hardest to differentiate samples from both equation and textual perspectives and guide the model to learn their representations. Experimental results show that our method achieves state-of-the-art on both widely used benchmark datasets and also exquisitely designed challenge datasets in English and Chinese. \footnote{Our code and data is available at \url{https://github.com/yiyunya/Textual_CL_MWP}

</details>

<details>

<summary>2022-11-29 09:34:19 - Neural Feature-Adaptation for Symbolic Predictions Using Pre-Training and Semantic Loss</summary>

- *Vedant Shah, Aditya Agrawal, Lovekesh Vig, Ashwin Srinivasan, Gautam Shroff, Tanmay Verlekar*

- `2211.16047v1` - [abs](http://arxiv.org/abs/2211.16047v1) - [pdf](http://arxiv.org/pdf/2211.16047v1)

> We are interested in neurosymbolic systems consisting of a high-level symbolic layer for explainable prediction in terms of human-intelligible concepts; and a low-level neural layer for extracting symbols required to generate the symbolic explanation. Real data is often imperfect meaning that even if the symbolic theory remains unchanged, we may still need to address the problem of mapping raw data to high-level symbols, each time there is a change in the data acquisition environment or equipment. Manual (re-)annotation of the raw data each time this happens is laborious and expensive; and automated labelling methods are often imperfect, especially for complex problems. NEUROLOG proposed the use of a semantic loss function that allows an existing feature-based symbolic model to guide the extraction of feature-values from raw data, using `abduction'. However, the experiments demonstrating the use of semantic loss through abduction appear to rely heavily on a domain-specific pre-processing step that enables a prior delineation of feature locations in the raw data. We examine the use of semantic loss in domains where such pre-processing is not possible, or is not obvious. We show that without any prior information about the features, the NEUROLOG approach can continue to predict accurately even with substantially incorrect feature predictions. We show also that prior information about the features in the form of even imperfect pre-training can help correct this situation. These findings are replicated on the original problem considered by NEUROLOG, without the use of feature-delineation. This suggests that symbolic explanations constructed for data in a domain could be re-used in a related domain, by `feature-adaptation' of pre-trained neural extractors using the semantic loss function constrained by abductive feedback.

</details>

<details>

<summary>2022-11-29 13:32:38 - Quantization-aware Interval Bound Propagation for Training Certifiably Robust Quantized Neural Networks</summary>

- *Mathias Lechner, ÄorÄe Å½ikeliÄ, Krishnendu Chatterjee, Thomas A. Henzinger, Daniela Rus*

- `2211.16187v1` - [abs](http://arxiv.org/abs/2211.16187v1) - [pdf](http://arxiv.org/pdf/2211.16187v1)

> We study the problem of training and certifying adversarially robust quantized neural networks (QNNs). Quantization is a technique for making neural networks more efficient by running them using low-bit integer arithmetic and is therefore commonly adopted in industry. Recent work has shown that floating-point neural networks that have been verified to be robust can become vulnerable to adversarial attacks after quantization, and certification of the quantized representation is necessary to guarantee robustness. In this work, we present quantization-aware interval bound propagation (QA-IBP), a novel method for training robust QNNs. Inspired by advances in robust learning of non-quantized networks, our training algorithm computes the gradient of an abstract representation of the actual network. Unlike existing approaches, our method can handle the discrete semantics of QNNs. Based on QA-IBP, we also develop a complete verification procedure for verifying the adversarial robustness of QNNs, which is guaranteed to terminate and produce a correct answer. Compared to existing approaches, the key advantage of our verification procedure is that it runs entirely on GPU or other accelerator devices. We demonstrate experimentally that our approach significantly outperforms existing methods and establish the new state-of-the-art for training and certifying the robustness of QNNs.

</details>

<details>

<summary>2022-11-29 14:06:35 - Building Resilience to Out-of-Distribution Visual Data via Input Optimization and Model Finetuning</summary>

- *Christopher J. Holder, Majid Khonji, Jorge Dias, Muhammad Shafique*

- `2211.16228v1` - [abs](http://arxiv.org/abs/2211.16228v1) - [pdf](http://arxiv.org/pdf/2211.16228v1)

> A major challenge in machine learning is resilience to out-of-distribution data, that is data that exists outside of the distribution of a model's training data. Training is often performed using limited, carefully curated datasets and so when a model is deployed there is often a significant distribution shift as edge cases and anomalies not included in the training data are encountered. To address this, we propose the Input Optimisation Network, an image preprocessing model that learns to optimise input data for a specific target vision model. In this work we investigate several out-of-distribution scenarios in the context of semantic segmentation for autonomous vehicles, comparing an Input Optimisation based solution to existing approaches of finetuning the target model with augmented training data and an adversarially trained preprocessing model. We demonstrate that our approach can enable performance on such data comparable to that of a finetuned model, and subsequently that a combined approach, whereby an input optimization network is optimised to target a finetuned model, delivers superior performance to either method in isolation. Finally, we propose a joint optimisation approach, in which input optimization network and target model are trained simultaneously, which we demonstrate achieves significant further performance gains, particularly in challenging edge-case scenarios. We also demonstrate that our architecture can be reduced to a relatively compact size without a significant performance impact, potentially facilitating real time embedded applications.

</details>

<details>

<summary>2022-11-29 14:15:00 - Obtaining Dyadic Fairness by Optimal Transport</summary>

- *Moyi Yang, Junjie Sheng, Xiangfeng Wang, Wenyan Liu, Bo Jin, Jun Wang, Hongyuan Zha*

- `2202.04520v2` - [abs](http://arxiv.org/abs/2202.04520v2) - [pdf](http://arxiv.org/pdf/2202.04520v2)

> Fairness has been taken as a critical metric in machine learning models, which is considered as an important component of trustworthy machine learning. In this paper, we focus on obtaining fairness for popular link prediction tasks, which are measured by dyadic fairness. A novel pre-processing methodology is proposed to establish dyadic fairness through data repairing based on optimal transport theory. With the well-established theoretical connection between the dyadic fairness for graph link prediction and a conditional distribution alignment problem, the dyadic repairing scheme can be equivalently transformed into a conditional distribution alignment problem. Furthermore, an optimal transport-based dyadic fairness algorithm called DyadicOT is obtained by efficiently solving the alignment problem, satisfying flexibility and unambiguity requirements. The proposed DyadicOT algorithm shows superior results in obtaining fairness compared to other fairness methods on two benchmark graph datasets.

</details>

<details>

<summary>2022-11-29 14:47:07 - Measuring the Measuring Tools: An Automatic Evaluation of Semantic Metrics for Text Corpora</summary>

- *George Kour, Samuel Ackerman, Orna Raz, Eitan Farchi, Boaz Carmeli, Ateret Anaby-Tavor*

- `2211.16259v1` - [abs](http://arxiv.org/abs/2211.16259v1) - [pdf](http://arxiv.org/pdf/2211.16259v1)

> The ability to compare the semantic similarity between text corpora is important in a variety of natural language processing applications. However, standard methods for evaluating these metrics have yet to be established. We propose a set of automatic and interpretable measures for assessing the characteristics of corpus-level semantic similarity metrics, allowing sensible comparison of their behavior. We demonstrate the effectiveness of our evaluation measures in capturing fundamental characteristics by evaluating them on a collection of classical and state-of-the-art metrics. Our measures revealed that recently-developed metrics are becoming better in identifying semantic distributional mismatch while classical metrics are more sensitive to perturbations in the surface text levels.

</details>

<details>

<summary>2022-11-29 14:52:38 - Intra-class Adaptive Augmentation with Neighbor Correction for Deep Metric Learning</summary>

- *Zheren Fu, Zhendong Mao, Bo Hu, An-An Liu, Yongdong Zhang*

- `2211.16264v1` - [abs](http://arxiv.org/abs/2211.16264v1) - [pdf](http://arxiv.org/pdf/2211.16264v1)

> Deep metric learning aims to learn an embedding space, where semantically similar samples are close together and dissimilar ones are repelled against. To explore more hard and informative training signals for augmentation and generalization, recent methods focus on generating synthetic samples to boost metric learning losses. However, these methods just use the deterministic and class-independent generations (e.g., simple linear interpolation), which only can cover the limited part of distribution spaces around original samples. They have overlooked the wide characteristic changes of different classes and can not model abundant intra-class variations for generations. Therefore, generated samples not only lack rich semantics within the certain class, but also might be noisy signals to disturb training. In this paper, we propose a novel intra-class adaptive augmentation (IAA) framework for deep metric learning. We reasonably estimate intra-class variations for every class and generate adaptive synthetic samples to support hard samples mining and boost metric learning losses. Further, for most datasets that have a few samples within the class, we propose the neighbor correction to revise the inaccurate estimations, according to our correlation discovery where similar classes generally have similar variation distributions. Extensive experiments on five benchmarks show our method significantly improves and outperforms the state-of-the-art methods on retrieval performances by 3%-6%. Our code is available at https://github.com/darkpromise98/IAA

</details>

<details>

<summary>2022-11-29 15:09:52 - Distance Teaching Experience of Campus-based Teachers at Times of Pandemic Confinement</summary>

- *Abbas Cheddad, Christian Nordahl*

- `2211.16280v1` - [abs](http://arxiv.org/abs/2211.16280v1) - [pdf](http://arxiv.org/pdf/2211.16280v1)

> Amidst the outbreak of the coronavirus (COVID 19) pandemic, distance education, where the learning process is conducted online, has become the norm. Campus-based programs and courses have been redesigned in a timely manner which was a challenge for teachers not used to distance teaching. Students engagement and active participation become an issue; add to that new emerging effects associating with this set-up, such as the so called 'Zoom fatigue', which was coined recently by some authors. In realising this problem, solutions were suggested in the literature to help trigger students engagement and enhance teachers experience in online teaching. This study analyses these effects along with our teachers experience in the new learning environment and concludes by devising some recommendations. To attain the above objectives, we conducted online interviews with six of our teachers, transcribed the content of the videos and then applied the inductive research approach to assess the results.

</details>

<details>

<summary>2022-11-29 16:08:55 - Assessing Software Privacy using the Privacy Flow-Graph</summary>

- *Feiyang Tang, Bjarte M. Ãstvold*

- `2209.02948v3` - [abs](http://arxiv.org/abs/2209.02948v3) - [pdf](http://arxiv.org/pdf/2209.02948v3)

> We increasingly rely on digital services and the conveniences they provide. Processing of personal data is integral to such services and thus privacy and data protection are a growing concern, and governments have responded with regulations such as the EU's GDPR. Following this, organisations that make software have legal obligations to document the privacy and data protection of their software. This work must involve both software developers that understand the code and the organisation's data protection officer or legal department that understands privacy and the requirements of a Data Protection and Impact Assessment (DPIA).   To help developers and non-technical people such as lawyers document the privacy and data protection behaviour of software, we have developed an automatic software analysis technique. This technique is based on static program analysis to characterise the flow of privacy-related data. The results of the analysis can be presented as a graph of privacy flows and operations - that is understandable also for non-technical people. We argue that our technique facilitates collaboration between technical and non-technical people in documenting the privacy behaviour of the software. We explain how to use the results produced by our technique to answer a series of privacy-relevant questions needed for a DPIA. To illustrate our work, we show both detailed and abstract analysis results from applying our analysis technique to the secure messaging service Signal and to the client of the cloud service NextCloud and show how their privacy flow-graphs inform the writing of a DPIA.

</details>

<details>

<summary>2022-11-29 16:15:32 - Improving astroBERT using Semantic Textual Similarity</summary>

- *Felix Grezes, Thomas Allen, Sergi Blanco-Cuaresma, Alberto Accomazzi, Michael J. Kurtz, Golnaz Shapurian, Edwin Henneken, Carolyn S. Grant, Donna M. Thompson, Timothy W. Hostetler, Matthew R. Templeton, Kelly E. Lockhart, Shinyi Chen, Jennifer Koch, Taylor Jacovich, Pavlos Protopapas*

- `2212.00744v1` - [abs](http://arxiv.org/abs/2212.00744v1) - [pdf](http://arxiv.org/pdf/2212.00744v1)

> The NASA Astrophysics Data System (ADS) is an essential tool for researchers that allows them to explore the astronomy and astrophysics scientific literature, but it has yet to exploit recent advances in natural language processing. At ADASS 2021, we introduced astroBERT, a machine learning language model tailored to the text used in astronomy papers in ADS. In this work we:   - announce the first public release of the astroBERT language model;   - show how astroBERT improves over existing public language models on astrophysics specific tasks;   - and detail how ADS plans to harness the unique structure of scientific papers, the citation graph and citation context, to further improve astroBERT.

</details>

<details>

<summary>2022-11-29 17:00:26 - Symmetry Detection in Trajectory Data for More Meaningful Reinforcement Learning Representations</summary>

- *Marissa D'Alonzo, Rebecca Russell*

- `2211.16381v1` - [abs](http://arxiv.org/abs/2211.16381v1) - [pdf](http://arxiv.org/pdf/2211.16381v1)

> Knowledge of the symmetries of reinforcement learning (RL) systems can be used to create compressed and semantically meaningful representations of a low-level state space. We present a method of automatically detecting RL symmetries directly from raw trajectory data without requiring active control of the system. Our method generates candidate symmetries and trains a recurrent neural network (RNN) to discriminate between the original trajectories and the transformed trajectories for each candidate symmetry. The RNN discriminator's accuracy for each candidate reveals how symmetric the system is under that transformation. This information can be used to create high-level representations that are invariant to all symmetries on a dataset level and to communicate properties of the RL behavior to users. We show in experiments on two simulated RL use cases (a pusher robot and a UAV flying in wind) that our method can determine the symmetries underlying both the environment physics and the trained RL policy.

</details>

<details>

<summary>2022-11-29 17:00:52 - Learning Program Synthesis for Integer Sequences from Scratch</summary>

- *Thibault Gauthier, Josef Urban*

- `2202.11908v3` - [abs](http://arxiv.org/abs/2202.11908v3) - [pdf](http://arxiv.org/pdf/2202.11908v3)

> We present a self-learning approach for synthesizing programs from integer sequences. Our method relies on a tree search guided by a learned policy. Our system is tested on the On-Line Encyclopedia of Integer Sequences. There, it discovers, on its own, solutions for 27987 sequences starting from basic operators and without human-written training examples.

</details>

<details>

<summary>2022-11-29 17:40:15 - Parameterisation of Reasoning on Temporal Markov Logic Networks</summary>

- *Victor David, RaphaÃ«l Fournier-S'niehotta, Nicolas Travers*

- `2211.16414v1` - [abs](http://arxiv.org/abs/2211.16414v1) - [pdf](http://arxiv.org/pdf/2211.16414v1)

> We aim at improving reasoning on inconsistent and uncertain data. We focus on knowledge-graph data, extended with time intervals to specify their validity, as regularly found in historical sciences. We propose principles on semantics for efficient Maximum A-Posteriori inference on the new Temporal Markov Logic Networks (TMLN) which extend the Markov Logic Networks (MLN) by uncertain temporal facts and rules. We examine total and partial temporal (in)consistency relations between sets of temporal formulae. Then we propose a new Temporal Parametric Semantics, which may combine several sub-functions, allowing to use different assessment strategies. Finally, we expose the constraints that semantics must respect to satisfy our principles.

</details>

<details>

<summary>2022-11-29 18:56:33 - Coder Reviewer Reranking for Code Generation</summary>

- *Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang*

- `2211.16490v1` - [abs](http://arxiv.org/abs/2211.16490v1) - [pdf](http://arxiv.org/pdf/2211.16490v1)

> Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.

</details>

<details>

<summary>2022-11-29 20:42:01 - Batching and Optimal Multi-stage Bipartite Allocations</summary>

- *Yiding Feng, Rad Niazadeh*

- `2211.16581v1` - [abs](http://arxiv.org/abs/2211.16581v1) - [pdf](http://arxiv.org/pdf/2211.16581v1)

> In several applications of real-time matching of demand to supply in online marketplaces, the platform allows for some latency to batch the demand and improve the efficiency. Motivated by these applications, we study the optimal trade-off between batching and inefficiency under adversarial arrival. As our base model, we consider K-stage variants of the vertex weighted b-matching in the adversarial setting, where online vertices arrive stage-wise and in K batches -- in contrast to online arrival. Our main result for this problem is an optimal (1-(1-1/K)^K)- competitive (fractional) matching algorithm, improving the classic (1-1/e) competitive ratio bound known for its online variant (Mehta et al., 2007; Aggarwal et al., 2011). We also extend this result to the rich model of multi-stage configuration allocation with free-disposals (Devanur et al., 2016), which is motivated by the display advertising in video streaming platforms.   Our main technique is developing tools to vary the trade-off between "greedy-ness" and "hedging" of the algorithm across stages. We rely on a particular family of convex-programming based matchings that distribute the demand in a specifically balanced way among supply in different stages, while carefully modifying the balancedness of the resulting matching across stages. More precisely, we identify a sequence of polynomials with decreasing degrees to be used as strictly concave regularizers of the maximum weight matching linear program to form these convex programs. At each stage, our algorithm returns the corresponding regularized optimal solution as the matching of this stage (by solving the convex program). Using structural properties of these convex programs and recursively connecting the regularizers together, we develop a new multi-stage primal-dual framework to analyze the competitive ratio. We further show this algorithm is optimally competitive.

</details>

<details>

<summary>2022-11-29 23:47:56 - Hierarchical Transformer for Survival Prediction Using Multimodality Whole Slide Images and Genomics</summary>

- *Chunyuan Li, Xinliang Zhu, Jiawen Yao, Junzhou Huang*

- `2211.16632v1` - [abs](http://arxiv.org/abs/2211.16632v1) - [pdf](http://arxiv.org/pdf/2211.16632v1)

> Learning good representation of giga-pixel level whole slide pathology images (WSI) for downstream tasks is critical. Previous studies employ multiple instance learning (MIL) to represent WSIs as bags of sampled patches because, for most occasions, only slide-level labels are available, and only a tiny region of the WSI is disease-positive area. However, WSI representation learning still remains an open problem due to: (1) patch sampling on a higher resolution may be incapable of depicting microenvironment information such as the relative position between the tumor cells and surrounding tissues, while patches at lower resolution lose the fine-grained detail; (2) extracting patches from giant WSI results in large bag size, which tremendously increases the computational cost. To solve the problems, this paper proposes a hierarchical-based multimodal transformer framework that learns a hierarchical mapping between pathology images and corresponding genes. Precisely, we randomly extract instant-level patch features from WSIs with different magnification. Then a co-attention mapping between imaging and genomics is learned to uncover the pairwise interaction and reduce the space complexity of imaging features. Such early fusion makes it computationally feasible to use MIL Transformer for the survival prediction task. Our architecture requires fewer GPU resources compared with benchmark methods while maintaining better WSI representation ability. We evaluate our approach on five cancer types from the Cancer Genome Atlas database and achieved an average c-index of $0.673$, outperforming the state-of-the-art multimodality methods.

</details>

<details>

<summary>2022-11-30 02:18:29 - Automatic Discovery of Multi-perspective Process Model using Reinforcement Learning</summary>

- *Sunghyun Sim, Ling Liu, Hyerim Bae*

- `2211.16687v1` - [abs](http://arxiv.org/abs/2211.16687v1) - [pdf](http://arxiv.org/pdf/2211.16687v1)

> Process mining is a methodology for the derivation and analysis of process models based on the event log. When process mining is employed to analyze business processes, the process discovery step, the conformance checking step, and the enhancements step are repeated. If a user wants to analyze a process from multiple perspectives (such as activity perspectives, originator perspectives, and time perspectives), the above procedure, inconveniently, has to be repeated over and over again. Although past studies involving process mining have applied detailed stepwise methodologies, no attempt has been made to incorporate and optimize multi-perspective process mining procedures. This paper contributes to developing a solution approach to this problem. First, we propose an automatic discovery framework of a multi-perspective process model based on deep Q-Learning. Our Dual Experience Replay with Experience Distribution (DERED) approach can automatically perform process model discovery steps, conformance check steps, and enhancements steps. Second, we propose a new method that further optimizes the experience replay (ER) method, one of the key algorithms of deep Q-learning, to improve the learning performance of reinforcement learning agents. Finally, we validate our approach using six real-world event datasets collected in port logistics, steel manufacturing, finance, IT, and government administration. We show that our DERED approach can provide users with multi-perspective, high-quality process models that can be employed more conveniently for multi-perspective process mining.

</details>

<details>

<summary>2022-11-30 02:44:13 - Interpretability and accessibility of machine learning in selected food processing, agriculture and health applications</summary>

- *N. Ranasinghe, A. Ramanan, S. Fernando, P. N. Hameed, D. Herath, T. Malepathirana, P. Suganthan, M. Niranjan, S. Halgamuge*

- `2211.16699v1` - [abs](http://arxiv.org/abs/2211.16699v1) - [pdf](http://arxiv.org/pdf/2211.16699v1)

> Artificial Intelligence (AI) and its data-centric branch of machine learning (ML) have greatly evolved over the last few decades. However, as AI is used increasingly in real world use cases, the importance of the interpretability of and accessibility to AI systems have become major research areas. The lack of interpretability of ML based systems is a major hindrance to widespread adoption of these powerful algorithms. This is due to many reasons including ethical and regulatory concerns, which have resulted in poorer adoption of ML in some areas. The recent past has seen a surge in research on interpretable ML. Generally, designing a ML system requires good domain understanding combined with expert knowledge. New techniques are emerging to improve ML accessibility through automated model design. This paper provides a review of the work done to improve interpretability and accessibility of machine learning in the context of global problems while also being relevant to developing countries. We review work under multiple levels of interpretability including scientific and mathematical interpretation, statistical interpretation and partial semantic interpretation. This review includes applications in three areas, namely food processing, agriculture and health.

</details>

<details>

<summary>2022-11-30 03:18:16 - Extracting Semantic Knowledge from GANs with Unsupervised Learning</summary>

- *Jianjin Xu, Zhaoxiang Zhang, Xiaolin Hu*

- `2211.16710v1` - [abs](http://arxiv.org/abs/2211.16710v1) - [pdf](http://arxiv.org/pdf/2211.16710v1)

> Recently, unsupervised learning has made impressive progress on various tasks. Despite the dominance of discriminative models, increasing attention is drawn to representations learned by generative models and in particular, Generative Adversarial Networks (GANs). Previous works on the interpretation of GANs reveal that GANs encode semantics in feature maps in a linearly separable form. In this work, we further find that GAN's features can be well clustered with the linear separability assumption. We propose a novel clustering algorithm, named KLiSH, which leverages the linear separability to cluster GAN's features. KLiSH succeeds in extracting fine-grained semantics of GANs trained on datasets of various objects, e.g., car, portrait, animals, and so on. With KLiSH, we can sample images from GANs along with their segmentation masks and synthesize paired image-segmentation datasets. Using the synthesized datasets, we enable two downstream applications. First, we train semantic segmentation networks on these datasets and test them on real images, realizing unsupervised semantic segmentation. Second, we train image-to-image translation networks on the synthesized datasets, enabling semantic-conditional image synthesis without human annotations.

</details>

<details>

<summary>2022-11-30 03:45:45 - Automated Generating Natural Language Requirements based on Domain Ontology</summary>

- *Ziyan Zhao, Li Zhang, Xiaoyun Gao, Xiaoli Lian, Heyang Lv, Lin Shi*

- `2211.16716v1` - [abs](http://arxiv.org/abs/2211.16716v1) - [pdf](http://arxiv.org/pdf/2211.16716v1)

> Software requirements specification is undoubtedly critical for the whole software life-cycle. Nowadays, writing software requirements specifications primarily depends on human work. Although massive studies have been proposed to fasten the process via proposing advanced elicitation and analysis techniques, it is still a time-consuming and error-prone task that needs to take domain knowledge and business information into consideration. In this paper, we propose an approach, named ReqGen, which can provide recommendations by automatically generating natural language requirements specifications based on certain given keywords. Specifically, ReqGen consists of three critical steps. First, keywords-oriented knowledge is selected from domain ontology and is injected to the basic Unified pre-trained Language Model (UniLM) for domain fine-tuning. Second, a copy mechanism is integrated to ensure the occurrence of keywords in the generated statements. Finally, a requirement syntax constrained decoding is designed to close the semantic and syntax distance between the candidate and reference specifications. Experiments on two public datasets from different groups and domains show that ReqGen outperforms six popular natural language generation approaches with respect to the hard constraint of keywords(phrases) inclusion, BLEU, ROUGE and syntax compliance. We believe that ReqGen can promote the efficiency and intelligence of specifying software requirements.

</details>

<details>

<summary>2022-11-30 05:13:21 - Semantic uncertainty intervals for disentangled latent spaces</summary>

- *Swami Sankaranarayanan, Anastasios N. Angelopoulos, Stephen Bates, Yaniv Romano, Phillip Isola*

- `2207.10074v2` - [abs](http://arxiv.org/abs/2207.10074v2) - [pdf](http://arxiv.org/pdf/2207.10074v2)

> Meaningful uncertainty quantification in computer vision requires reasoning about semantic information -- say, the hair color of the person in a photo or the location of a car on the street. To this end, recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces, but providing uncertainties on the semantic latent variables has remained challenging. In this work, we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following: (1) it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space (2) calibrates these uncertainties such that they contain the true value of the latent for a new, unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful, principled, and instance-adaptive uncertainty in inverse problems like image super-resolution and image completion.

</details>

<details>

<summary>2022-11-30 05:42:08 - Nonlinear Monte Carlo Method for Imbalanced Data Learning</summary>

- *Xuli Shen, Qing Xu, Xiangyang Xue*

- `2010.14060v3` - [abs](http://arxiv.org/abs/2010.14060v3) - [pdf](http://arxiv.org/pdf/2010.14060v3)

> For basic machine learning problems, expected error is used to evaluate model performance. Since the distribution of data is usually unknown, we can make simple hypothesis that the data are sampled independently and identically distributed (i.i.d.) and the mean value of loss function is used as the empirical risk by Law of Large Numbers (LLN). This is known as the Monte Carlo method. However, when LLN is not applicable, such as imbalanced data problems, empirical risk will cause overfitting and might decrease robustness and generalization ability. Inspired by the framework of nonlinear expectation theory, we substitute the mean value of loss function with the maximum value of subgroup mean loss. We call it nonlinear Monte Carlo method. In order to use numerical method of optimization, we linearize and smooth the functional of maximum empirical risk and get the descent direction via quadratic programming. With the proposed method, we achieve better performance than SOTA backbone models with less training steps, and more robustness for basic regression and imbalanced classification tasks.

</details>

<details>

<summary>2022-11-30 07:30:43 - Dr.3D: Adapting 3D GANs to Artistic Drawings</summary>

- *Wonjoon Jin, Nuri Ryu, Geonung Kim, Seung-Hwan Baek, Sunghyun Cho*

- `2211.16798v1` - [abs](http://arxiv.org/abs/2211.16798v1) - [pdf](http://arxiv.org/pdf/2211.16798v1)

> While 3D GANs have recently demonstrated the high-quality synthesis of multi-view consistent images and 3D shapes, they are mainly restricted to photo-realistic human portraits. This paper aims to extend 3D GANs to a different, but meaningful visual form: artistic portrait drawings. However, extending existing 3D GANs to drawings is challenging due to the inevitable geometric ambiguity present in drawings. To tackle this, we present Dr.3D, a novel adaptation approach that adapts an existing 3D GAN to artistic drawings. Dr.3D is equipped with three novel components to handle the geometric ambiguity: a deformation-aware 3D synthesis network, an alternating adaptation of pose estimation and image synthesis, and geometric priors. Experiments show that our approach can successfully adapt 3D GANs to drawings and enable multi-view consistent semantic editing of drawings.

</details>

<details>

<summary>2022-11-30 07:50:07 - Generalised Spherical Text Embedding</summary>

- *Souvik Banerjee, Bamdev Mishra, Pratik Jawanpuria, Manish Shrivastava*

- `2211.16801v1` - [abs](http://arxiv.org/abs/2211.16801v1) - [pdf](http://arxiv.org/pdf/2211.16801v1)

> This paper aims to provide an unsupervised modelling approach that allows for a more flexible representation of text embeddings. It jointly encodes the words and the paragraphs as individual matrices of arbitrary column dimension with unit Frobenius norm. The representation is also linguistically motivated with the introduction of a novel similarity metric. The proposed modelling and the novel similarity metric exploits the matrix structure of embeddings. We then go on to show that the same matrices can be reshaped into vectors of unit norm and transform our problem into an optimization problem over the spherical manifold. We exploit manifold optimization to efficiently train the matrix embeddings. We also quantitatively verify the quality of our text embeddings by showing that they demonstrate improved results in document classification, document clustering, and semantic textual similarity benchmark tests.

</details>

<details>

<summary>2022-11-30 09:28:15 - Linking Sketch Patches by Learning Synonymous Proximity for Graphic Sketch Representation</summary>

- *Sicong Zang, Shikui Tu, Lei Xu*

- `2211.16841v1` - [abs](http://arxiv.org/abs/2211.16841v1) - [pdf](http://arxiv.org/pdf/2211.16841v1)

> Graphic sketch representations are effective for representing sketches. Existing methods take the patches cropped from sketches as the graph nodes, and construct the edges based on sketch's drawing order or Euclidean distances on the canvas. However, the drawing order of a sketch may not be unique, while the patches from semantically related parts of a sketch may be far away from each other on the canvas. In this paper, we propose an order-invariant, semantics-aware method for graphic sketch representations. The cropped sketch patches are linked according to their global semantics or local geometric shapes, namely the synonymous proximity, by computing the cosine similarity between the captured patch embeddings. Such constructed edges are learnable to adapt to the variation of sketch drawings, which enable the message passing among synonymous patches. Aggregating the messages from synonymous patches by graph convolutional networks plays a role of denoising, which is beneficial to produce robust patch embeddings and accurate sketch representations. Furthermore, we enforce a clustering constraint over the embeddings jointly with the network learning. The synonymous patches are self-organized as compact clusters, and their embeddings are guided to move towards their assigned cluster centroids. It raises the accuracy of the computed synonymous proximity. Experimental results show that our method significantly improves the performance on both controllable sketch synthesis and sketch healing.

</details>

<details>

<summary>2022-11-30 11:18:54 - Heterogeneous Graph Neural Network with Multi-view Representation Learning</summary>

- *Zezhi Shao, Yongjun Xu, Wei Wei, Fei Wang, Zhao Zhang, Feida Zhu*

- `2108.13650v3` - [abs](http://arxiv.org/abs/2108.13650v3) - [pdf](http://arxiv.org/pdf/2108.13650v3)

> Graph neural networks for heterogeneous graph embedding is to project nodes into a low-dimensional space by exploring the heterogeneity and semantics of the heterogeneous graph. However, on the one hand, most of existing heterogeneous graph embedding methods either insufficiently model the local structure under specific semantic, or neglect the heterogeneity when aggregating information from it. On the other hand, representations from multiple semantics are not comprehensively integrated to obtain versatile node embeddings. To address the problem, we propose a Heterogeneous Graph Neural Network with Multi-View Representation Learning (named MV-HetGNN) for heterogeneous graph embedding by introducing the idea of multi-view representation learning. The proposed model consists of node feature transformation, view-specific ego graph encoding and auto multi-view fusion to thoroughly learn complex structural and semantic information for generating comprehensive node representations. Extensive experiments on three real-world heterogeneous graph datasets show that the proposed MV-HetGNN model consistently outperforms all the state-of-the-art GNN baselines in various downstream tasks, e.g., node classification, node clustering, and link prediction.

</details>

<details>

<summary>2022-11-30 11:23:11 - MinUn: Accurate ML Inference on Microcontrollers</summary>

- *Shikhar Jaiswal, Rahul Kiran Kranti Goli, Aayan Kumar, Vivek Seshadri, Rahul Sharma*

- `2210.16556v2` - [abs](http://arxiv.org/abs/2210.16556v2) - [pdf](http://arxiv.org/pdf/2210.16556v2)

> Running machine learning inference on tiny devices, known as TinyML, is an emerging research area. This task requires generating inference code that uses memory frugally, a task that standard ML frameworks are ill-suited for. A deployment framework for TinyML must be a) parametric in the number representation to take advantage of the emerging representations like posits, b) carefully assign high-precision to a few tensors so that most tensors can be kept in low-precision while still maintaining model accuracy, and c) avoid memory fragmentation. We describe MinUn, the first TinyML framework that holistically addresses these issues to generate efficient code for ARM microcontrollers (e.g., Arduino Uno, Due and STM32H747) that outperforms the prior TinyML frameworks.

</details>

<details>

<summary>2022-11-30 12:26:31 - Deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation</summary>

- *Ahmad Hammoudeh, Bastien Vanderplaetse, StÃ©phane Dupont*

- `2202.05728v2` - [abs](http://arxiv.org/abs/2202.05728v2) - [pdf](http://arxiv.org/pdf/2202.05728v2)

> This work aims at generating captions for soccer videos using deep learning. In this context, this paper introduces a dataset, model, and triple-level evaluation. The dataset consists of 22k caption-clip pairs and three visual features (images, optical flow, inpainting) for ~500 hours of \emph{SoccerNet} videos. The model is divided into three parts: a transformer learns language, ConvNets learn vision, and a fusion of linguistic and visual features generates captions. The paper suggests evaluating generated captions at three levels: syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr), meaning (the quality of descriptions for a domain expert), and corpus (the diversity of generated captions). The paper shows that the diversity of generated captions has improved (from 0.07 reaching 0.18) with semantics-related losses that prioritize selected words. Semantics-related losses and the utilization of more visual features (optical flow, inpainting) improved the normalized captioning score by 28\%. The web page of this work: https://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning

</details>

<details>

<summary>2022-11-30 12:37:08 - BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems</summary>

- *Guangsen Wang, Samson Tan, Shafiq Joty, Gang Wu, Jimmy Au, Steven Hoi*

- `2211.11982v3` - [abs](http://arxiv.org/abs/2211.11982v3) - [pdf](http://arxiv.org/pdf/2211.11982v3)

> We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for commercial text-based task-oriented dialog (TOD) systems. BotSIM consists of three major components: 1) a Generator that can infer semantic-level dialog acts and entities from bot definitions and generate user queries via model-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to simulate conversations with the dialog agents; 3) a Remediator to analyze the simulated conversations, visualize the bot health reports and provide actionable remediation suggestions for bot troubleshooting and improvement. We demonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and multi-intent dialog generation via case studies on two commercial bot platforms. BotSIM's "generation-simulation-remediation" paradigm accelerates the end-to-end bot evaluation and iteration process by: 1) reducing manual test cases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU and end-to-end performance via extensive dialog simulation; 3) improving the bot troubleshooting process with actionable suggestions. A demo of our system can be found at https://tinyurl.com/mryu74cd and a demo video at https://youtu.be/qLi5iSoly30. We have open-sourced the toolkit at https://github.com/salesforce/botsim

</details>

<details>

<summary>2022-11-30 13:51:53 - Reinforcement Learning with Dynamic Convex Risk Measures</summary>

- *Anthony Coache, Sebastian Jaimungal*

- `2112.13414v3` - [abs](http://arxiv.org/abs/2112.13414v3) - [pdf](http://arxiv.org/pdf/2112.13414v3)

> We develop an approach for solving time-consistent risk-sensitive stochastic optimization problems using model-free reinforcement learning (RL). Specifically, we assume agents assess the risk of a sequence of random variables using dynamic convex risk measures. We employ a time-consistent dynamic programming principle to determine the value of a particular policy, and develop policy gradient update rules that aid in obtaining optimal policies. We further develop an actor-critic style algorithm using neural networks to optimize over policies. Finally, we demonstrate the performance and flexibility of our approach by applying it to three optimization problems: statistical arbitrage trading strategies, financial hedging, and obstacle avoidance robot control.

</details>

<details>

<summary>2022-11-30 13:51:58 - Learning with Partial Labels from Semi-supervised Perspective</summary>

- *Ximing Li, Yuanzhi Jiang, Changchun Li, Yiyuan Wang, Jihong Ouyang*

- `2211.13655v2` - [abs](http://arxiv.org/abs/2211.13655v2) - [pdf](http://arxiv.org/pdf/2211.13655v2)

> Partial Label (PL) learning refers to the task of learning from the partially labeled data, where each training instance is ambiguously equipped with a set of candidate labels but only one is valid. Advances in the recent deep PL learning literature have shown that the deep learning paradigms, e.g., self-training, contrastive learning, or class activate values, can achieve promising performance. Inspired by the impressive success of deep Semi-Supervised (SS) learning, we transform the PL learning problem into the SS learning problem, and propose a novel PL learning method, namely Partial Label learning with Semi-supervised Perspective (PLSP). Specifically, we first form the pseudo-labeled dataset by selecting a small number of reliable pseudo-labeled instances with high-confidence prediction scores and treating the remaining instances as pseudo-unlabeled ones. Then we design a SS learning objective, consisting of a supervised loss for pseudo-labeled instances and a semantic consistency regularization for pseudo-unlabeled instances. We further introduce a complementary regularization for those non-candidate labels to constrain the model predictions on them to be as small as possible. Empirical results demonstrate that PLSP significantly outperforms the existing PL baseline methods, especially on high ambiguity levels. Code available: https://github.com/changchunli/PLSP.

</details>

<details>

<summary>2022-11-30 14:12:45 - Real time QKD Post Processing based on Reconfigurable Hardware Acceleration</summary>

- *Foram P Shingala, Natarajan Venkatachalam, Selvagangai C, Hema Priya S, Dillibabu S, Pooja Chandravanshi, Ravindra P. Singh*

- `2211.17019v1` - [abs](http://arxiv.org/abs/2211.17019v1) - [pdf](http://arxiv.org/pdf/2211.17019v1)

> Key Distillation is an essential component of every Quantum Key Distribution system because it compensates the inherent transmission errors of quantum channel. However, throughput and interoperability aspects of post-processing engine design often neglected, and exiting solutions are not providing any guarantee. In this paper, we propose multiple protocol support high throughput key distillation framework implemented in a Field Programmable Gate Array (FPGA) using High-Level Synthesis (HLS). The proposed design uses a Hadoop framework with a map-reduce programming model to efficiently process large chunks of raw data across the limited computing resources of an FPGA. We present a novel hardware-efficient integrated post-processing architecture that offer dynamic error correction, a side-channel resistant authentication scheme, and an inbuilt high-speed encryption application, which uses the key for secure communication. We develop a semi automated High level synthesis framework capable of handling different QKD protocols with promising speedup. Overall, the experimental results shows that there is a significant improvement in performance and compatible with any discrete variable QKD systems.

</details>

<details>

<summary>2022-11-30 15:43:20 - High-Fidelity Guided Image Synthesis with Latent Diffusion Models</summary>

- *Jaskirat Singh, Stephen Gould, Liang Zheng*

- `2211.17084v1` - [abs](http://arxiv.org/abs/2211.17084v1) - [pdf](http://arxiv.org/pdf/2211.17084v1)

> Controllable image synthesis with user scribbles has gained huge public interest with the recent advent of text-conditioned latent diffusion models. The user scribbles control the color composition while the text prompt provides control over the overall image semantics. However, we note that prior works in this direction suffer from an intrinsic domain shift problem, wherein the generated outputs often lack details and resemble simplistic representations of the target domain. In this paper, we propose a novel guided image synthesis framework, which addresses this problem by modeling the output image as the solution of a constrained optimization problem. We show that while computing an exact solution to the optimization is infeasible, an approximation of the same can be achieved while just requiring a single pass of the reverse diffusion process. Additionally, we show that by simply defining a cross-attention based correspondence between the input text tokens and the user stroke-painting, the user is also able to control the semantics of different painted regions without requiring any conditional training or finetuning. Human user study results show that the proposed approach outperforms the previous state-of-the-art by over 85.32% on the overall user satisfaction scores. Project page for our paper is available at https://1jsingh.github.io/gradop.

</details>

<details>

<summary>2022-11-30 17:46:43 - Resource Sharing Through Multi-Round Matchings</summary>

- *Yohai Trabelsi, Abhijin Adiga, Sarit Kraus, S. S. Ravi, Daniel J. Rosenkrantz*

- `2211.17199v1` - [abs](http://arxiv.org/abs/2211.17199v1) - [pdf](http://arxiv.org/pdf/2211.17199v1)

> Applications such as employees sharing office spaces over a workweek can be modeled as problems where agents are matched to resources over multiple rounds. Agents' requirements limit the set of compatible resources and the rounds in which they want to be matched. Viewing such an application as a multi-round matching problem on a bipartite compatibility graph between agents and resources, we show that a solution (i.e., a set of matchings, with one matching per round) can be found efficiently if one exists. To cope with situations where a solution does not exist, we consider two extensions. In the first extension, a benefit function is defined for each agent and the objective is to find a multi-round matching to maximize the total benefit. For a general class of benefit functions satisfying certain properties (including diminishing returns), we show that this multi-round matching problem is efficiently solvable. This class includes utilitarian and Rawlsian welfare functions. For another benefit function, we show that the maximization problem is NP-hard. In the second extension, the objective is to generate advice to each agent (i.e., a subset of requirements to be relaxed) subject to a budget constraint so that the agent can be matched. We show that this budget-constrained advice generation problem is NP-hard. For this problem, we develop an integer linear programming formulation as well as a heuristic based on local search. We experimentally evaluate our algorithms on synthetic networks and apply them to two real-world situations: shared office spaces and matching courses to classrooms.

</details>

<details>

<summary>2022-11-30 18:32:06 - ObjCAViT: Improving Monocular Depth Estimation Using Natural Language Models And Image-Object Cross-Attention</summary>

- *Dylan Auty, Krystian Mikolajczyk*

- `2211.17232v1` - [abs](http://arxiv.org/abs/2211.17232v1) - [pdf](http://arxiv.org/pdf/2211.17232v1)

> While monocular depth estimation (MDE) is an important problem in computer vision, it is difficult due to the ambiguity that results from the compression of a 3D scene into only 2 dimensions. It is common practice in the field to treat it as simple image-to-image translation, without consideration for the semantics of the scene and the objects within it. In contrast, humans and animals have been shown to use higher-level information to solve MDE: prior knowledge of the nature of the objects in the scene, their positions and likely configurations relative to one another, and their apparent sizes have all been shown to help resolve this ambiguity.   In this paper, we present a novel method to enhance MDE performance by encouraging use of known-useful information about the semantics of objects and inter-object relationships within a scene. Our novel ObjCAViT module sources world-knowledge from language models and learns inter-object relationships in the context of the MDE problem using transformer attention, incorporating apparent size information. Our method produces highly accurate depth maps, and we obtain competitive results on the NYUv2 and KITTI datasets. Our ablation experiments show that the use of language and cross-attention within the ObjCAViT module increases performance. Code is released at https://github.com/DylanAuty/ObjCAViT.

</details>

<details>

<summary>2022-11-30 19:03:15 - Part-based Face Recognition with Vision Transformers</summary>

- *Zhonglin Sun, Georgios Tzimiropoulos*

- `2212.00057v1` - [abs](http://arxiv.org/abs/2212.00057v1) - [pdf](http://arxiv.org/pdf/2212.00057v1)

> Holistic methods using CNNs and margin-based losses have dominated research on face recognition. In this work, we depart from this setting in two ways: (a) we employ the Vision Transformer as an architecture for training a very strong baseline for face recognition, simply called fViT, which already surpasses most state-of-the-art face recognition methods. (b) Secondly, we capitalize on the Transformer's inherent property to process information (visual tokens) extracted from irregular grids to devise a pipeline for face recognition which is reminiscent of part-based face recognition methods. Our pipeline, called part fViT, simply comprises a lightweight network to predict the coordinates of facial landmarks followed by the Vision Transformer operating on patches extracted from the predicted landmarks, and it is trained end-to-end with no landmark supervision. By learning to extract discriminative patches, our part-based Transformer further boosts the accuracy of our Vision Transformer baseline achieving state-of-the-art accuracy on several face recognition benchmarks.

</details>

<details>

<summary>2022-11-30 21:47:14 - CatlNet: Learning Communication and Coordination Policies from CaTL+ Specifications</summary>

- *Wenliang Liu, Kevin Leahy, Zachary Serlin, Calin Belta*

- `2212.11792v1` - [abs](http://arxiv.org/abs/2212.11792v1) - [pdf](http://arxiv.org/pdf/2212.11792v1)

> In this paper, we propose a learning-based framework to simultaneously learn the communication and distributed control policies for a heterogeneous multi-agent system (MAS) under complex mission requirements from Capability Temporal Logic plus (CaTL+) specifications. Both policies are trained, implemented, and deployed using a novel neural network model called CatlNet. Taking advantage of the robustness measure of CaTL+, we train CatlNet centrally to maximize it where network parameters are shared among all agents, allowing CatlNet to scale to large teams easily. CatlNet can then be deployed distributedly. A plan repair algorithm is also introduced to guide CatlNet's training and improve both training efficiency and the overall performance of CatlNet. The CatlNet approach is tested in simulation and results show that, after training, CatlNet can steer the decentralized MAS system online to satisfy a CaTL+ specification with a high success rate.

</details>


## 2022-12

<details>

<summary>2022-12-01 02:05:44 - Experimental Observations of the Topology of Convolutional Neural Network Activations</summary>

- *Emilie Purvine, Davis Brown, Brett Jefferson, Cliff Joslyn, Brenda Praggastis, Archit Rathore, Madelyn Shapiro, Bei Wang, Youjia Zhou*

- `2212.00222v1` - [abs](http://arxiv.org/abs/2212.00222v1) - [pdf](http://arxiv.org/pdf/2212.00222v1)

> Topological data analysis (TDA) is a branch of computational mathematics, bridging algebraic topology and data science, that provides compact, noise-robust representations of complex structures. Deep neural networks (DNNs) learn millions of parameters associated with a series of transformations defined by the model architecture, resulting in high-dimensional, difficult-to-interpret internal representations of input data. As DNNs become more ubiquitous across multiple sectors of our society, there is increasing recognition that mathematical methods are needed to aid analysts, researchers, and practitioners in understanding and interpreting how these models' internal representations relate to the final classification. In this paper, we apply cutting edge techniques from TDA with the goal of gaining insight into the interpretability of convolutional neural networks used for image classification. We use two common TDA approaches to explore several methods for modeling hidden-layer activations as high-dimensional point clouds, and provide experimental evidence that these point clouds capture valuable structural information about the model's process. First, we demonstrate that a distance metric based on persistent homology can be used to quantify meaningful differences between layers, and we discuss these distances in the broader context of existing representational similarity metrics for neural network interpretability. Second, we show that a mapper graph can provide semantic insight into how these models organize hierarchical class knowledge at each layer. These observations demonstrate that TDA is a useful tool to help deep learning practitioners unlock the hidden structures of their models.

</details>

<details>

<summary>2022-12-01 02:22:45 - PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals</summary>

- *Zhihao Zhang, Siwen Luo, Junyi Chen, Sijia Lai, Siqu Long, Hyunsuk Chung, Soyeon Caren Han*

- `2211.15940v3` - [abs](http://arxiv.org/abs/2211.15940v3) - [pdf](http://arxiv.org/pdf/2211.15940v3)

> We propose a PiggyBack, a Visual Question Answering platform that allows users to apply the state-of-the-art visual-language pretrained models easily. The PiggyBack supports the full stack of visual question answering tasks, specifically data processing, model fine-tuning, and result visualisation. We integrate visual-language models, pretrained by HuggingFace, an open-source API platform of deep learning technologies; however, it cannot be runnable without programming skills or deep learning understanding. Hence, our PiggyBack supports an easy-to-use browser-based user interface with several deep learning visual language pretrained models for general users and domain experts. The PiggyBack includes the following benefits: Free availability under the MIT License, Portability due to web-based and thus runs on almost any platform, A comprehensive data creation and processing technique, and ease of use on deep learning-based visual language pretrained models. The demo video is available on YouTube and can be found at https://youtu.be/iz44RZ1lF4s.

</details>

<details>

<summary>2022-12-01 02:31:10 - Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder</summary>

- *Bin Sun, Shaoxiong Feng, Yiwei Li, Weichao Wang, Fei Mi, Yitong Li, Kan Li*

- `2212.00231v1` - [abs](http://arxiv.org/abs/2212.00231v1) - [pdf](http://arxiv.org/pdf/2212.00231v1)

> Complex dialogue mappings (CDM), including one-to-many and many-to-one mappings, tend to make dialogue models generate incoherent or dull responses, and modeling these mappings remains a huge challenge for neural dialogue systems. To alleviate these problems, methods like introducing external information, reconstructing the optimization function, and manipulating data samples are proposed, while they primarily focus on avoiding training with CDM, inevitably weakening the model's ability of understanding CDM in human conversations and limiting further improvements in model performance. This paper proposes a Sentence Semantic \textbf{Seg}mentation guided \textbf{C}onditional \textbf{V}ariational \textbf{A}uto-\textbf{E}ncoder (SegCVAE) method which can model and take advantages of the CDM data. Specifically, to tackle the incoherent problem caused by one-to-many, SegCVAE uses response-related prominent semantics to constrained the latent variable. To mitigate the non-diverse problem brought by many-to-one, SegCVAE segments multiple prominent semantics to enrich the latent variables. Three novel components, Internal Separation, External Guidance, and Semantic Norms, are proposed to achieve SegCVAE. On dialogue generation tasks, both the automatic and human evaluation results show that SegCVAE achieves new state-of-the-art performance.

</details>

<details>

<summary>2022-12-01 04:20:07 - PIZZA: A new benchmark for complex end-to-end task-oriented parsing</summary>

- *Konstantine Arkoudas, Nicolas Guenon des Mesnards, Melanie Rubino, Sandesh Swamy, Saarthak Khanna, Weiqi Sun, Khan Haidar*

- `2212.00265v1` - [abs](http://arxiv.org/abs/2212.00265v1) - [pdf](http://arxiv.org/pdf/2212.00265v1)

> Much recent work in task-oriented parsing has focused on finding a middle ground between flat slots and intents, which are inexpressive but easy to annotate, and powerful representations such as the lambda calculus, which are expressive but costly to annotate. This paper continues the exploration of task-oriented parsing by introducing a new dataset for parsing pizza and drink orders, whose semantics cannot be captured by flat slots and intents. We perform an extensive evaluation of deep-learning techniques for task-oriented parsing on this dataset, including different flavors of seq2seq systems and RNNGs. The dataset comes in two main versions, one in a recently introduced utterance-level hierarchical notation that we call TOP, and one whose targets are executable representations (EXR). We demonstrate empirically that training the parser to directly generate EXR notation not only solves the problem of entity resolution in one fell swoop and overcomes a number of expressive limitations of TOP notation, but also results in significantly greater parsing accuracy.

</details>

<details>

<summary>2022-12-01 05:05:17 - Automated anomaly-aware 3D segmentation of bones and cartilages in knee MR images from the Osteoarthritis Initiative</summary>

- *Boyeong Woo, Craig Engstrom, William Baresic, Jurgen Fripp, Stuart Crozier, Shekhar S. Chandra*

- `2211.16696v2` - [abs](http://arxiv.org/abs/2211.16696v2) - [pdf](http://arxiv.org/pdf/2211.16696v2)

> In medical image analysis, automated segmentation of multi-component anatomical structures, which often have a spectrum of potential anomalies and pathologies, is a challenging task. In this work, we develop a multi-step approach using U-Net-based neural networks to initially detect anomalies (bone marrow lesions, bone cysts) in the distal femur, proximal tibia and patella from 3D magnetic resonance (MR) images of the knee in individuals with varying grades of osteoarthritis. Subsequently, the extracted data are used for downstream tasks involving semantic segmentation of individual bone and cartilage volumes as well as bone anomalies. For anomaly detection, the U-Net-based models were developed to reconstruct the bone profiles of the femur and tibia in images via inpainting so anomalous bone regions could be replaced with close to normal appearances. The reconstruction error was used to detect bone anomalies. A second anomaly-aware network, which was compared to anomaly-na\"ive segmentation networks, was used to provide a final automated segmentation of the femoral, tibial and patellar bones and cartilages from the knee MR images containing a spectrum of bone anomalies. The anomaly-aware segmentation approach provided up to 58% reduction in Hausdorff distances for bone segmentations compared to the results from the anomaly-na\"ive segmentation networks. In addition, the anomaly-aware networks were able to detect bone lesions in the MR images with greater sensitivity and specificity (area under the receiver operating characteristic curve [AUC] up to 0.896) compared to the anomaly-na\"ive segmentation networks (AUC up to 0.874).

</details>

<details>

<summary>2022-12-01 05:57:44 - Differentiable Fuzzy $\mathcal{ALC}$: A Neural-Symbolic Representation Language for Symbol Grounding</summary>

- *Xuan Wu, Xinhao Zhu, Yizheng Zhao, Xinyu Dai*

- `2211.12006v2` - [abs](http://arxiv.org/abs/2211.12006v2) - [pdf](http://arxiv.org/pdf/2211.12006v2)

> Neural-symbolic computing aims at integrating robust neural learning and sound symbolic reasoning into a single framework, so as to leverage the complementary strengths of both of these, seemingly unrelated (maybe even contradictory) AI paradigms. The central challenge in neural-symbolic computing is to unify the formulation of neural learning and symbolic reasoning into a single framework with common semantics, that is, to seek a joint representation between a neural model and a logical theory that can support the basic grounding learned by the neural model and also stick to the semantics of the logical theory. In this paper, we propose differentiable fuzzy $\mathcal{ALC}$ (DF-$\mathcal{ALC}$) for this role, as a neural-symbolic representation language with the desired semantics. DF-$\mathcal{ALC}$ unifies the description logic $\mathcal{ALC}$ and neural models for symbol grounding; in particular, it infuses an $\mathcal{ALC}$ knowledge base into neural models through differentiable concept and role embeddings. We define a hierarchical loss to the constraint that the grounding learned by neural models must be semantically consistent with $\mathcal{ALC}$ knowledge bases. And we find that capturing the semantics in grounding solely by maximizing satisfiability cannot revise grounding rationally. We further define a rule-based loss for DF adapting to symbol grounding problems. The experiment results show that DF-$\mathcal{ALC}$ with rule-based loss can improve the performance of image object detectors in an unsupervised learning way, even in low-resource situations.

</details>

<details>

<summary>2022-12-01 06:13:08 - Redactor: A Data-centric and Individualized Defense Against Inference Attacks</summary>

- *Geon Heo, Steven Euijong Whang*

- `2202.02902v2` - [abs](http://arxiv.org/abs/2202.02902v2) - [pdf](http://arxiv.org/pdf/2202.02902v2)

> Information leakage is becoming a critical problem as various information becomes publicly available by mistake, and machine learning models train on that data to provide services. As a result, one's private information could easily be memorized by such trained models. Unfortunately, deleting information is out of the question as the data is already exposed to the Web or third-party platforms. Moreover, we cannot necessarily control the labeling process and the model trainings by other parties either. In this setting, we study the problem of targeted disinformation generation where the goal is to dilute the data and thus make a model safer and more robust against inference attacks on a specific target (e.g., a person's profile) by only inserting new data. Our method finds the closest points to the target in the input space that will be labeled as a different class. Since we cannot control the labeling process, we instead conservatively estimate the labels probabilistically by combining decision boundaries of multiple classifiers using data programming techniques. Our experiments show that a probabilistic decision boundary can be a good proxy for labelers, and that our approach is effective in defending against inference attacks and can scale to large data.

</details>

<details>

<summary>2022-12-01 06:33:40 - Target-Free Text-guided Image Manipulation</summary>

- *Wan-Cyuan Fan, Cheng-Fu Yang, Chiao-An Yang, Yu-Chiang Frank Wang*

- `2211.14544v2` - [abs](http://arxiv.org/abs/2211.14544v2) - [pdf](http://arxiv.org/pdf/2211.14544v2)

> We tackle the problem of target-free text-guided image manipulation, which requires one to modify the input reference image based on the given text instruction, while no ground truth target image is observed during training. To address this challenging task, we propose a Cyclic-Manipulation GAN (cManiGAN) in this paper, which is able to realize where and how to edit the image regions of interest. Specifically, the image editor in cManiGAN learns to identify and complete the input image, while cross-modal interpreter and reasoner are deployed to verify the semantic correctness of the output image based on the input instruction. While the former utilizes factual/counterfactual description learning for authenticating the image semantics, the latter predicts the "undo" instruction and provides pixel-level supervision for the training of cManiGAN. With such operational cycle-consistency, our cManiGAN can be trained in the above weakly supervised setting. We conduct extensive experiments on the datasets of CLEVR and COCO, and the effectiveness and generalizability of our proposed method can be successfully verified. Project page: https://sites.google.com/view/wancyuanfan/projects/cmanigan.

</details>

<details>

<summary>2022-12-01 08:58:45 - Non-Deterministic Approximation Fixpoint Theory and Its Application in Disjunctive Logic Programming</summary>

- *Jesse Heyninck, Ofer Arieli, Bart Bogaerts*

- `2211.17262v2` - [abs](http://arxiv.org/abs/2211.17262v2) - [pdf](http://arxiv.org/pdf/2211.17262v2)

> Approximation fixpoint theory (AFT) is an abstract and general algebraic framework for studying the semantics of nonmonotonic logics. It provides a unifying study of the semantics of different formalisms for nonmonotonic reasoning, such as logic programming, default logic and autoepistemic logic. In this paper, we extend AFT to dealing with non-deterministic constructs that allow to handle indefinite information, represented e.g. by disjunctive formulas. This is done by generalizing the main constructions and corresponding results of AFT to non-deterministic operators, whose ranges are sets of elements rather than single elements. The applicability and usefulness of this generalization is illustrated in the context of disjunctive logic programming.

</details>

<details>

<summary>2022-12-01 10:58:32 - What Physical Layer Security Can Do for 6G Security</summary>

- *Miroslav Mitev, Arsenia Chorti, H. V. Poor, Gerhard Fettweis*

- `2212.00427v1` - [abs](http://arxiv.org/abs/2212.00427v1) - [pdf](http://arxiv.org/pdf/2212.00427v1)

> While existing security protocols were designed with a focus on the core network, the enhancement of the security of the B5G access network becomes of critical importance. Despite the strengthening of 5G security protocols with respect to LTE, there are still open issues that have not been fully addressed. This work is articulated around the premise that rethinking the security design bottom up, starting at the physical layer, is not only viable in 6G but importantly, arises as an efficient way to overcome security hurdles in novel use cases, notably massive machine type communications (mMTC), ultra reliable low latency communications (URLLC) and autonomous cyberphysical systems. Unlike existing review papers that treat physical layer security orthogonally to cryptography, we will try to provide a few insights of underlying connections. Discussing many practical issues, we will present a comprehensive review of the state-of-the-art in i) secret key generation from shared randomness, ii) the wiretap channel and fundamental limits, iii) authentication of devices using physical unclonable functions (PUFs), localization and multi-factor authentication, and, iv) jamming attacks at the physical layer. We finally conclude with the proposers' aspirations for the 6G security landscape, in the hyper-connectivity and semantic communications era.

</details>

<details>

<summary>2022-12-01 11:19:30 - Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models</summary>

- *Lei Wang, Jiabang He, Xing Xu, Ning Liu, Hui Liu*

- `2211.14777v2` - [abs](http://arxiv.org/abs/2211.14777v2) - [pdf](http://arxiv.org/pdf/2211.14777v2)

> Alignment between image and text has shown promising improvements on patch-level pre-trained document image models. However, investigating more effective or finer-grained alignment techniques during pre-training requires a large amount of computation cost and time. Thus, a question naturally arises: Could we fine-tune the pre-trained models adaptive to downstream tasks with alignment objectives and achieve comparable or better performance? In this paper, we propose a new model architecture with alignment-enriched tuning (dubbed AETNet) upon pre-trained document image models, to adapt downstream tasks with the joint task-specific supervised and alignment-aware contrastive objective. Specifically, we introduce an extra visual transformer as the alignment-ware image encoder and an extra text transformer as the alignment-ware text encoder before multimodal fusion. We consider alignment in the following three aspects: 1) document-level alignment by leveraging the cross-modal and intra-modal contrastive loss; 2) global-local alignment for modeling localized and structural information in document images; and 3) local-level alignment for more accurate patch-level information. Experiments on various downstream tasks show that AETNet can achieve state-of-the-art performance on various downstream tasks. Notably, AETNet consistently outperforms state-of-the-art pre-trained models, such as LayoutLMv3 with fine-tuning techniques, on three different downstream tasks.

</details>

<details>

<summary>2022-12-01 15:04:34 - PointCA: Evaluating the Robustness of 3D Point Cloud Completion Models Against Adversarial Examples</summary>

- *Shengshan Hu, Junwei Zhang, Wei Liu, Junhui Hou, Minghui Li, Leo Yu Zhang, Hai Jin, Lichao Sun*

- `2211.12294v2` - [abs](http://arxiv.org/abs/2211.12294v2) - [pdf](http://arxiv.org/pdf/2211.12294v2)

> Point cloud completion, as the upstream procedure of 3D recognition and segmentation, has become an essential part of many tasks such as navigation and scene understanding. While various point cloud completion models have demonstrated their powerful capabilities, their robustness against adversarial attacks, which have been proven to be fatally malicious towards deep neural networks, remains unknown. In addition, existing attack approaches towards point cloud classifiers cannot be applied to the completion models due to different output forms and attack purposes. In order to evaluate the robustness of the completion models, we propose PointCA, the first adversarial attack against 3D point cloud completion models. PointCA can generate adversarial point clouds that maintain high similarity with the original ones, while being completed as another object with totally different semantic information. Specifically, we minimize the representation discrepancy between the adversarial example and the target point set to jointly explore the adversarial point clouds in the geometry space and the feature space. Furthermore, to launch a stealthier attack, we innovatively employ the neighbourhood density information to tailor the perturbation constraint, leading to geometry-aware and distribution-adaptive modifications for each point. Extensive experiments against different premier point cloud completion networks show that PointCA can cause a performance degradation from 77.9% to 16.7%, with the structure chamfer distance kept below 0.01. We conclude that existing completion models are severely vulnerable to adversarial examples, and state-of-the-art defenses for point cloud classification will be partially invalid when applied to incomplete and uneven point cloud data.

</details>

<details>

<summary>2022-12-01 16:38:57 - A General Purpose Supervisory Signal for Embodied Agents</summary>

- *Kunal Pratap Singh, Jordi Salvador, Luca Weihs, Aniruddha Kembhavi*

- `2212.01186v1` - [abs](http://arxiv.org/abs/2212.01186v1) - [pdf](http://arxiv.org/pdf/2212.01186v1)

> Training effective embodied AI agents often involves manual reward engineering, expert imitation, specialized components such as maps, or leveraging additional sensors for depth and localization. Another approach is to use neural architectures alongside self-supervised objectives which encourage better representation learning. In practice, there are few guarantees that these self-supervised objectives encode task-relevant information. We propose the Scene Graph Contrastive (SGC) loss, which uses scene graphs as general-purpose, training-only, supervisory signals. The SGC loss does away with explicit graph decoding and instead uses contrastive learning to align an agent's representation with a rich graphical encoding of its environment. The SGC loss is generally applicable, simple to implement, and encourages representations that encode objects' semantics, relationships, and history. Using the SGC loss, we attain significant gains on three embodied tasks: Object Navigation, Multi-Object Navigation, and Arm Point Navigation. Finally, we present studies and analyses which demonstrate the ability of our trained representation to encode semantic cues about the environment.

</details>

<details>

<summary>2022-12-01 16:58:57 - Hyperbolic Contrastive Learning for Visual Representations beyond Objects</summary>

- *Songwei Ge, Shlok Mishra, Simon Kornblith, Chun-Liang Li, David Jacobs*

- `2212.00653v1` - [abs](http://arxiv.org/abs/2212.00653v1) - [pdf](http://arxiv.org/pdf/2212.00653v1)

> Although self-/un-supervised methods have led to rapid progress in visual representation learning, these methods generally treat objects and scenes using the same lens. In this paper, we focus on learning representations for objects and scenes that preserve the structure among them.   Motivated by the observation that visually similar objects are close in the representation space, we argue that the scenes and objects should instead follow a hierarchical structure based on their compositionality. To exploit such a structure, we propose a contrastive learning framework where a Euclidean loss is used to learn object representations and a hyperbolic loss is used to encourage representations of scenes to lie close to representations of their constituent objects in a hyperbolic space. This novel hyperbolic objective encourages the scene-object hypernymy among the representations by optimizing the magnitude of their norms. We show that when pretraining on the COCO and OpenImages datasets, the hyperbolic loss improves downstream performance of several baselines across multiple datasets and tasks, including image classification, object detection, and semantic segmentation. We also show that the properties of the learned representations allow us to solve various vision tasks that involve the interaction between scenes and objects in a zero-shot fashion. Our code can be found at \url{https://github.com/shlokk/HCL/tree/main/HCL}.

</details>

<details>

<summary>2022-12-01 19:20:22 - Out of Distribution Detection via Neural Network Anchoring</summary>

- *Rushil Anirudh, Jayaraman J. Thiagarajan*

- `2207.04125v2` - [abs](http://arxiv.org/abs/2207.04125v2) - [pdf](http://arxiv.org/pdf/2207.04125v2)

> Our goal in this paper is to exploit heteroscedastic temperature scaling as a calibration strategy for out of distribution (OOD) detection. Heteroscedasticity here refers to the fact that the optimal temperature parameter for each sample can be different, as opposed to conventional approaches that use the same value for the entire distribution. To enable this, we propose a new training strategy called anchoring that can estimate appropriate temperature values for each sample, leading to state-of-the-art OOD detection performance across several benchmarks. Using NTK theory, we show that this temperature function estimate is closely linked to the epistemic uncertainty of the classifier, which explains its behavior. In contrast to some of the best-performing OOD detection approaches, our method does not require exposure to additional outlier datasets, custom calibration objectives, or model ensembling. Through empirical studies with different OOD detection settings -- far OOD, near OOD, and semantically coherent OOD - we establish a highly effective OOD detection approach. Code to reproduce our results is available at github.com/LLNL/AMP

</details>

<details>

<summary>2022-12-01 20:48:06 - An introduction to optimization under uncertainty -- A short survey</summary>

- *Keivan Shariatmadar, Kaizheng Wang, Calvin R. Hubbard, Hans Hallez, David Moens*

- `2212.00862v1` - [abs](http://arxiv.org/abs/2212.00862v1) - [pdf](http://arxiv.org/pdf/2212.00862v1)

> Optimization equips engineers and scientists in a variety of fields with the ability to transcribe their problems into a generic formulation and receive optimal solutions with relative ease. Industries ranging from aerospace to robotics continue to benefit from advancements in optimization theory and the associated algorithmic developments. Nowadays, optimization is used in real time on autonomous systems acting in safety critical situations, such as self-driving vehicles. It has become increasingly more important to produce robust solutions by incorporating uncertainty into optimization programs. This paper provides a short survey about the state of the art in optimization under uncertainty. The paper begins with a brief overview of the main classes of optimization without uncertainty. The rest of the paper focuses on the different methods for handling both aleatoric and epistemic uncertainty. Many of the applications discussed in this paper are within the domain of control. The goal of this survey paper is to briefly touch upon the state of the art in a variety of different methods and refer the reader to other literature for more in-depth treatments of the topics discussed here.

</details>

<details>

<summary>2022-12-01 23:01:41 - ConTextual Masked Auto-Encoder for Dense Passage Retrieval</summary>

- *Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, Songlin Hu*

- `2208.07670v3` - [abs](http://arxiv.org/abs/2208.07670v3) - [pdf](http://arxiv.org/pdf/2208.07670v3)

> Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae.

</details>

<details>

<summary>2022-12-02 00:54:33 - Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus</summary>

- *Yudong Xu, Elias B. Khalil, Scott Sanner*

- `2210.09880v2` - [abs](http://arxiv.org/abs/2210.09880v2) - [pdf](http://arxiv.org/pdf/2210.09880v2)

> The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the performance of general artificial intelligence algorithms. The ARC's focus on broad generalization and few-shot learning has made it difficult to solve using pure machine learning. A more promising approach has been to perform program synthesis within an appropriately designed Domain Specific Language (DSL). However, these too have seen limited success. We propose Abstract Reasoning with Graph Abstractions (ARGA), a new object-centric framework that first represents images using graphs and then performs a search for a correct program in a DSL that is based on the abstracted graph space. The complexity of this combinatorial search is tamed through the use of constraint acquisition, state hashing, and Tabu search. An extensive set of experiments demonstrates the promise of ARGA in tackling some of the complicated object-centric tasks of the ARC rather efficiently, producing programs that are correct and easy to understand.

</details>

<details>

<summary>2022-12-02 01:10:47 - Navigating to Objects in the Real World</summary>

- *Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, Devendra Singh Chaplot*

- `2212.00922v1` - [abs](http://arxiv.org/abs/2212.00922v1) - [pdf](http://arxiv.org/pdf/2212.00922v1)

> Semantic navigation is necessary to deploy mobile robots in uncontrolled environments like our homes, schools, and hospitals. Many learning-based approaches have been proposed in response to the lack of semantic understanding of the classical pipeline for spatial navigation, which builds a geometric map using depth sensors and plans to reach point goals. Broadly, end-to-end learning approaches reactively map sensor inputs to actions with deep neural networks, while modular learning approaches enrich the classical pipeline with learning-based semantic sensing and exploration. But learned visual navigation policies have predominantly been evaluated in simulation. How well do different classes of methods work on a robot? We present a large-scale empirical study of semantic visual navigation methods comparing representative methods from classical, modular, and end-to-end learning approaches across six homes with no prior experience, maps, or instrumentation. We find that modular learning works well in the real world, attaining a 90% success rate. In contrast, end-to-end learning does not, dropping from 77% simulation to 23% real-world success rate due to a large image domain gap between simulation and reality. For practitioners, we show that modular learning is a reliable approach to navigate to objects: modularity and abstraction in policy design enable Sim-to-Real transfer. For researchers, we identify two key issues that prevent today's simulators from being reliable evaluation benchmarks - (A) a large Sim-to-Real gap in images and (B) a disconnect between simulation and real-world error modes - and propose concrete steps forward.

</details>

<details>

<summary>2022-12-02 03:38:26 - ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information</summary>

- *Arnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron VandenBerg, Jamie Callan*

- `2211.15848v2` - [abs](http://arxiv.org/abs/2211.15848v2) - [pdf](http://arxiv.org/pdf/2211.15848v2)

> ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10 billion web pages affiliated with rich information. Its design was influenced by the need for a high quality, large scale web corpus to support a range of academic and industry research, for example, in information systems, retrieval-augmented AI systems, and model pretraining. Compared with earlier ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of higher-quality, and aligned with the document distributions in commercial web search. Besides raw HTML, ClueWeb22 includes rich information about the web pages provided by industry-standard document understanding systems, including the visual representation of pages rendered by a web browser, parsed HTML structure information from a neural network parser, and pre-processed cleaned document text to lower the barrier to entry. Many of these signals have been widely used in industry but are available to the research community for the first time at this scale.

</details>

<details>

<summary>2022-12-02 03:42:23 - Few-Shot Nested Named Entity Recognition</summary>

- *Hong Ming, Jiaoyun Yang, Lili Jiang, Yan Pan, Ning An*

- `2212.00953v1` - [abs](http://arxiv.org/abs/2212.00953v1) - [pdf](http://arxiv.org/pdf/2212.00953v1)

> While Named Entity Recognition (NER) is a widely studied task, making inferences of entities with only a few labeled data has been challenging, especially for entities with nested structures. Unlike flat entities, entities and their nested entities are more likely to have similar semantic feature representations, drastically increasing difficulties in classifying different entity categories in the few-shot setting. Although prior work has briefly discussed nested structures in the context of few-shot learning, to our best knowledge, this paper is the first one specifically dedicated to studying the few-shot nested NER task. Leveraging contextual dependency to distinguish nested entities, we propose a Biaffine-based Contrastive Learning (BCL) framework. We first design a Biaffine span representation module for learning the contextual span dependency representation for each entity span rather than only learning its semantic representation. We then merge these two representations by the residual connection to distinguish nested entities. Finally, we build a contrastive learning framework to adjust the representation distribution for larger margin boundaries and more generalized domain transfer learning ability. We conducted experimental studies on three English, German, and Russian nested NER datasets. The results show that the BCL outperformed three baseline models on the 1-shot and 5-shot tasks in terms of F1 score.

</details>

<details>

<summary>2022-12-02 04:08:28 - Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion</summary>

- *Shruti Agarwal, Liwen Hu, Evonne Ng, Trevor Darrell, Hao Li, Anna Rohrbach*

- `2112.10936v2` - [abs](http://arxiv.org/abs/2112.10936v2) - [pdf](http://arxiv.org/pdf/2112.10936v2)

> In today's era of digital misinformation, we are increasingly faced with new threats posed by video falsification techniques. Such falsifications range from cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g., sophisticated AI media synthesis methods), which are becoming perceptually indistinguishable from real videos. To tackle this challenge, we propose a multi-modal semantic forensic approach to discover clues that go beyond detecting discrepancies in visual quality, thereby handling both simpler cheapfakes and visually persuasive deepfakes. In this work, our goal is to verify that the purported person seen in the video is indeed themselves by detecting anomalous facial movements corresponding to the spoken words. We leverage the idea of attribution to learn person-specific biometric patterns that distinguish a given speaker from others. We use interpretable Action Units (AUs) to capture a person's face and head movement as opposed to deep CNN features, and we are the first to use word-conditioned facial motion analysis. We further demonstrate our method's effectiveness on a range of fakes not seen in training including those without video manipulation, that were not addressed in prior work.

</details>

<details>

<summary>2022-12-02 06:34:27 - Credit Assignment for Trained Neural Networks Based on Koopman Operator Theory</summary>

- *Zhen Liang, Changyuan Zhao, Wanwei Liu, Bai Xue, Wenjing Yang, Zhengbin Pang*

- `2212.00998v1` - [abs](http://arxiv.org/abs/2212.00998v1) - [pdf](http://arxiv.org/pdf/2212.00998v1)

> Credit assignment problem of neural networks refers to evaluating the credit of each network component to the final outputs. For an untrained neural network, approaches to tackling it have made great contributions to parameter update and model revolution during the training phase. This problem on trained neural networks receives rare attention, nevertheless, it plays an increasingly important role in neural network patch, specification and verification. Based on Koopman operator theory, this paper presents an alternative perspective of linear dynamics on dealing with the credit assignment problem for trained neural networks. Regarding a neural network as the composition of sub-dynamics series, we utilize step-delay embedding to capture snapshots of each component, characterizing the established mapping as exactly as possible. To circumvent the dimension-difference problem encountered during the embedding, a composition and decomposition of an auxiliary linear layer, termed minimal linear dimension alignment, is carefully designed with rigorous formal guarantee. Afterwards, each component is approximated by a Koopman operator and we derive the Jacobian matrix and its corresponding determinant, similar to backward propagation. Then, we can define a metric with algebraic interpretability for the credit assignment of each network component. Moreover, experiments conducted on typical neural networks demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2022-12-02 07:30:52 - Better Peer Grading through Bayesian Inference</summary>

- *Hedayat Zarkoob, Greg d'Eon, Lena Podina, Kevin Leyton-Brown*

- `2209.01242v2` - [abs](http://arxiv.org/abs/2209.01242v2) - [pdf](http://arxiv.org/pdf/2209.01242v2)

> Peer grading systems aggregate noisy reports from multiple students to approximate a true grade as closely as possible. Most current systems either take the mean or median of reported grades; others aim to estimate students' grading accuracy under a probabilistic model. This paper extends the state of the art in the latter approach in three key ways: (1) recognizing that students can behave strategically (e.g., reporting grades close to the class average without doing the work); (2) appropriately handling censored data that arises from discrete-valued grading rubrics; and (3) using mixed integer programming to improve the interpretability of the grades assigned to students. We show how to make Bayesian inference practical in this model and evaluate our approach on both synthetic and real-world data obtained by using our implemented system in four large classes. These extensive experiments show that grade aggregation using our model accurately estimates true grades, students' likelihood of submitting uninformative grades, and the variation in their inherent grading error; we also characterize our models' robustness.

</details>

<details>

<summary>2022-12-02 07:43:23 - CLeBPI: Contrastive Learning for Bug Priority Inference</summary>

- *Wenyao Wang, Chenhao Wu, Jie He*

- `2212.01011v1` - [abs](http://arxiv.org/abs/2212.01011v1) - [pdf](http://arxiv.org/pdf/2212.01011v1)

> Automated bug priority inference can reduce the time overhead of bug triagers for priority assignments, improving the efficiency of software maintenance. Currently, there are two orthogonal lines for this task, i.e., traditional machine learning based (TML-based) and neural network based (NN-based) approaches. Although these approaches achieve competitive performance, our observation finds that existing approaches face the following two issues: 1) TML-based approaches require much manual feature engineering and cannot learn the semantic information of bug reports; 2) Both TML-based and NN-based approaches cannot effectively address the label imbalance problem because they are difficult to distinguish the semantic difference between bug reports with different priorities. In this paper, we propose CLeBPI (Contrastive Learning for Bug Priority Inference), which leverages pre-trained language model and contrastive learning to tackle the above-mentioned two issues. Specifically, CLeBPI is first pre-trained on a large-scale bug report corpus in a self-supervised way, thus it can automatically learn contextual representations of bug reports without manual feature engineering. Afterward, it is further pre-trained by a contrastive learning objective, which enables it to distinguish semantic differences between bug reports, learning more precise contextual representations for each bug report. When finishing pre-training, we can connect a classification layer to CLeBPI and fine-tune it for bug priority inference in a supervised way. To verify the effectiveness of CLeBPI, we choose four baseline approaches and conduct comparison experiments on a public dataset. The experimental results show that CLeBPI outperforms all baseline approaches by 23.86%-77.80% in terms of weighted average F1-score, showing its effectiveness.

</details>

<details>

<summary>2022-12-02 08:14:11 - Programming Is Hard -- Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation</summary>

- *Brett A. Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James Prather, Eddie Antonio Santos*

- `2212.01020v1` - [abs](http://arxiv.org/abs/2212.01020v1) - [pdf](http://arxiv.org/pdf/2212.01020v1)

> The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on how to overcome or otherwise mitigate the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community.

</details>

<details>

<summary>2022-12-02 08:31:46 - STL-Based Synthesis of Feedback Controllers Using Reinforcement Learning</summary>

- *Nikhil Kumar Singh, Indranil Saha*

- `2212.01022v1` - [abs](http://arxiv.org/abs/2212.01022v1) - [pdf](http://arxiv.org/pdf/2212.01022v1)

> Deep Reinforcement Learning (DRL) has the potential to be used for synthesizing feedback controllers (agents) for various complex systems with unknown dynamics. These systems are expected to satisfy diverse safety and liveness properties best captured using temporal logic. In RL, the reward function plays a crucial role in specifying the desired behaviour of these agents. However, the problem of designing the reward function for an RL agent to satisfy complex temporal logic specifications has received limited attention in the literature. To address this, we provide a systematic way of generating rewards in real-time by using the quantitative semantics of Signal Temporal Logic (STL), a widely used temporal logic to specify the behaviour of cyber-physical systems. We propose a new quantitative semantics for STL having several desirable properties, making it suitable for reward generation. We evaluate our STL-based reinforcement learning mechanism on several complex continuous control benchmarks and compare our STL semantics with those available in the literature in terms of their efficacy in synthesizing the controller agent. Experimental results establish our new semantics to be the most suitable for synthesizing feedback controllers for complex continuous dynamical systems through reinforcement learning.

</details>

<details>

<summary>2022-12-02 08:38:16 - On interpretability and proper latent decomposition of autoencoders</summary>

- *Luca Magri, Anh Khoa Doan*

- `2211.08345v2` - [abs](http://arxiv.org/abs/2211.08345v2) - [pdf](http://arxiv.org/pdf/2211.08345v2)

> The dynamics of a turbulent flow tend to occupy only a portion of the phase space at a statistically stationary regime. From a dynamical systems point of view, this portion is the attractor. The knowledge of the turbulent attractor is useful for two purposes, at least: (i) We can gain physical insight into turbulence (what is the shape and geometry of the attractor?), and (ii) it provides the minimal number of degrees of freedom to accurately describe the turbulent dynamics. Autoencoders enable the computation of an optimal latent space, which is a low-order representation of the dynamics. If properly trained and correctly designed, autoencoders can learn an approximation of the turbulent attractor, as shown by Doan, Racca and Magri (2022). In this paper, we theoretically interpret the transformations of an autoencoder. First, we remark that the latent space is a curved manifold with curvilinear coordinates, which can be analyzed with simple tools from Riemann geometry. Second, we characterize the geometrical properties of the latent space. We mathematically derive the metric tensor, which provides a mathematical description of the manifold. Third, we propose a method -- proper latent decomposition (PLD) -- that generalizes proper orthogonal decomposition of turbulent flows on the autoencoder latent space. This decomposition finds the dominant directions in the curved latent space. This theoretical work opens up computational opportunities for interpreting autoencoders and creating reduced-order models of turbulent flows.

</details>

<details>

<summary>2022-12-02 08:48:22 - Progress and Challenges for the Application of Machine Learning for Neglected Tropical Diseases</summary>

- *Chung Yuen Khew, Rahmad Akbar, Norfarhan Mohd. Assaad*

- `2212.01027v1` - [abs](http://arxiv.org/abs/2212.01027v1) - [pdf](http://arxiv.org/pdf/2212.01027v1)

> Neglected tropical diseases (NTDs) continue to affect the livelihood of individuals in countries in the Southeast Asia and Western Pacific region. These diseases have been long existing and have caused devastating health problems and economic decline to people in low- and middle-income (developing) countries. An estimated 1.7 billion of the world's population suffer one or more NTDs annually, this puts approximately one in five individuals at risk for NTDs. In addition to health and social impact, NTDs inflict significant financial burden to patients, close relatives, and are responsible for billions of dollars lost in revenue from reduced labor productivity in developing countries alone. There is an urgent need to better improve the control and eradication or elimination efforts towards NTDs. This can be achieved by utilizing machine learning tools to better the surveillance, prediction and detection program, and combat NTDs through the discovery of new therapeutics against these pathogens. This review surveys the current application of machine learning tools for NTDs and the challenges to elevate the state-of-the-art of NTDs surveillance, management, and treatment.

</details>

<details>

<summary>2022-12-02 09:15:39 - Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations</summary>

- *Qian Yang, Yunxin Li, Baotian Hu, Lin Ma, Yuxing Ding, Min Zhang*

- `2207.11401v2` - [abs](http://arxiv.org/abs/2207.11401v2) - [pdf](http://arxiv.org/pdf/2207.11401v2)

> Visual Entailment with natural language explanations aims to infer the relationship between a text-image pair and generate a sentence to explain the decision-making process. Previous methods rely mainly on a pre-trained vision-language model to perform the relation inference and a language model to generate the corresponding explanation. However, the pre-trained vision-language models mainly build token-level alignment between text and image yet ignore the high-level semantic alignment between the phrases (chunks) and visual contents, which is critical for vision-language reasoning. Moreover, the explanation generator based only on the encoded joint representation does not explicitly consider the critical decision-making points of relation inference. Thus the generated explanations are less faithful to visual-language reasoning. To mitigate these problems, we propose a unified Chunk-aware Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence structure inherent in language and various image regions to build chunk-aware semantic alignment. Relation inferrer uses an attention-based reasoning network to incorporate the token-level and chunk-level vision-language representations. LeCG utilizes lexical constraints to expressly incorporate the words or chunks focused by the relation inferrer into explanation generation, improving the faithfulness and informativeness of the explanations. We conduct extensive experiments on three datasets, and experimental results indicate that CALeC significantly outperforms other competitor models on inference accuracy and quality of generated explanations.

</details>

<details>

<summary>2022-12-02 09:52:18 - Matching DNN Compression and Cooperative Training with Resources and Data Availability</summary>

- *Francesco Malandrino, Giuseppe Di Giacomo, Armin Karamzade, Marco Levorato, Carla Fabiana Chiasserini*

- `2212.02304v1` - [abs](http://arxiv.org/abs/2212.02304v1) - [pdf](http://arxiv.org/pdf/2212.02304v1)

> To make machine learning (ML) sustainable and apt to run on the diverse devices where relevant data is, it is essential to compress ML models as needed, while still meeting the required learning quality and time performance. However, how much and when an ML model should be compressed, and {\em where} its training should be executed, are hard decisions to make, as they depend on the model itself, the resources of the available nodes, and the data such nodes own. Existing studies focus on each of those aspects individually, however, they do not account for how such decisions can be made jointly and adapted to one another. In this work, we model the network system focusing on the training of DNNs, formalize the above multi-dimensional problem, and, given its NP-hardness, formulate an approximate dynamic programming problem that we solve through the PACT algorithmic framework. Importantly, PACT leverages a time-expanded graph representing the learning process, and a data-driven and theoretical approach for the prediction of the loss evolution to be expected as a consequence of training decisions. We prove that PACT's solutions can get as close to the optimum as desired, at the cost of an increased time complexity, and that, in any case, such complexity is polynomial. Numerical results also show that, even under the most disadvantageous settings, PACT outperforms state-of-the-art alternatives and closely matches the optimal energy cost.

</details>

<details>

<summary>2022-12-02 09:54:05 - Exploring Faithful Rationale for Multi-hop Fact Verification via Salience-Aware Graph Learning</summary>

- *Jiasheng Si, Yingjie Zhu, Deyu Zhou*

- `2212.01060v1` - [abs](http://arxiv.org/abs/2212.01060v1) - [pdf](http://arxiv.org/pdf/2212.01060v1)

> The opaqueness of the multi-hop fact verification model imposes imperative requirements for explainability. One feasible way is to extract rationales, a subset of inputs, where the performance of prediction drops dramatically when being removed. Though being explainable, most rationale extraction methods for multi-hop fact verification explore the semantic information within each piece of evidence individually, while ignoring the topological information interaction among different pieces of evidence. Intuitively, a faithful rationale bears complementary information being able to extract other rationales through the multi-hop reasoning process. To tackle such disadvantages, we cast explainable multi-hop fact verification as subgraph extraction, which can be solved based on graph convolutional network (GCN) with salience-aware graph learning. In specific, GCN is utilized to incorporate the topological interaction information among multiple pieces of evidence for learning evidence representation. Meanwhile, to alleviate the influence of noisy evidence, the salience-aware graph perturbation is induced into the message passing of GCN. Moreover, the multi-task model with three diagnostic properties of rationale is elaborately designed to improve the quality of an explanation without any explicit annotations. Experimental results on the FEVEROUS benchmark show significant gains over previous state-of-the-art methods for both rationale extraction and fact verification.

</details>

<details>

<summary>2022-12-02 10:44:46 - Membership Inference Attacks Against Semantic Segmentation Models</summary>

- *Tomas Chobola, Dmitrii Usynin, Georgios Kaissis*

- `2212.01082v1` - [abs](http://arxiv.org/abs/2212.01082v1) - [pdf](http://arxiv.org/pdf/2212.01082v1)

> Membership inference attacks aim to infer whether a data record has been used to train a target model by observing its predictions. In sensitive domains such as healthcare, this can constitute a severe privacy violation. In this work we attempt to address the existing knowledge gap by conducting an exhaustive study of membership inference attacks and defences in the domain of semantic image segmentation. Our findings indicate that for certain threat models, these learning settings can be considerably more vulnerable than the previously considered classification settings. We additionally investigate a threat model where a dishonest adversary can perform model poisoning to aid their inference and evaluate the effects that these adaptations have on the success of membership inference attacks. We quantitatively evaluate the attacks on a number of popular model architectures across a variety of semantic segmentation tasks, demonstrating that membership inference attacks in this domain can achieve a high success rate and defending against them may result in unfavourable privacy-utility trade-offs or increased computational costs.

</details>

<details>

<summary>2022-12-02 11:19:16 - Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures</summary>

- *Simone Conia, Edoardo Barba, Alessandro ScirÃ¨, Roberto Navigli*

- `2212.01094v1` - [abs](http://arxiv.org/abs/2212.01094v1) - [pdf](http://arxiv.org/pdf/2212.01094v1)

> One of the common traits of past and present approaches for Semantic Role Labeling (SRL) is that they rely upon discrete labels drawn from a predefined linguistic inventory to classify predicate senses and their arguments. However, we argue this need not be the case. In this paper, we present an approach that leverages Definition Modeling to introduce a generalized formulation of SRL as the task of describing predicate-argument structures using natural language definitions instead of discrete labels. Our novel formulation takes a first step towards placing interpretability and flexibility foremost, and yet our experiments and analyses on PropBank-style and FrameNet-style, dependency-based and span-based SRL also demonstrate that a flexible model with an interpretable output does not necessarily come at the expense of performance. We release our software for research purposes at https://github.com/SapienzaNLP/dsrl.

</details>

<details>

<summary>2022-12-02 12:48:01 - Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation via Hybrid Latent Variables</summary>

- *Bin Sun, Yitong Li, Fei Mi, Weichao Wang, Yiwei Li, Kan Li*

- `2212.01145v1` - [abs](http://arxiv.org/abs/2212.01145v1) - [pdf](http://arxiv.org/pdf/2212.01145v1)

> Conditional variational models, using either continuous or discrete latent variables, are powerful for open-domain dialogue response generation. However, previous works show that continuous latent variables tend to reduce the coherence of generated responses. In this paper, we also found that discrete latent variables have difficulty capturing more diverse expressions. To tackle these problems, we combine the merits of both continuous and discrete latent variables and propose a Hybrid Latent Variable (HLV) method. Specifically, HLV constrains the global semantics of responses through discrete latent variables and enriches responses with continuous latent variables. Thus, we diversify the generated responses while maintaining relevance and coherence. In addition, we propose Conditional Hybrid Variational Transformer (CHVT) to construct and to utilize HLV with transformers for dialogue generation. Through fine-grained symbolic-level semantic information and additive Gaussian mixing, we construct the distribution of continuous variables, prompting the generation of diverse expressions. Meanwhile, to maintain the relevance and coherence, the discrete latent variable is optimized by self-separation training. Experimental results on two dialogue generation datasets (DailyDialog and Opensubtitles) show that CHVT is superior to traditional transformer-based variational mechanism w.r.t. diversity, relevance and coherence metrics. Moreover, we also demonstrate the benefit of applying HLV to fine-tuning two pre-trained dialogue models (PLATO and BART-base).

</details>

<details>

<summary>2022-12-02 14:55:09 - End-to-End Semantic Video Transformer for Zero-Shot Action Recognition</summary>

- *Keval Doshi, Yasin Yilmaz*

- `2203.05156v2` - [abs](http://arxiv.org/abs/2203.05156v2) - [pdf](http://arxiv.org/pdf/2203.05156v2)

> While video action recognition has been an active area of research for several years, zero-shot action recognition has only recently started gaining traction. In this work, we propose a novel end-to-end trained transformer model which is capable of capturing long range spatiotemporal dependencies efficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to address a common ambiguity in the existing works about classes that can be considered as previously unseen, we propose a new experimentation setup that satisfies the zero-shot learning premise for action recognition by avoiding overlap between the training and testing classes. The proposed approach significantly outperforms the state of the arts in zero-shot action recognition in terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet datasets. The code and proposed experimentation setup are available in GitHub: https://github.com/Secure-and-Intelligent-Systems-Lab/SemanticVideoTransformer

</details>

<details>

<summary>2022-12-02 16:10:26 - Scalable Edge Blocking Algorithms for Defending Active Directory Style Attack Graphs</summary>

- *Mingyu Guo, Max Ward, Aneta Neumann, Frank Neumann, Hung Nguyen*

- `2212.04326v1` - [abs](http://arxiv.org/abs/2212.04326v1) - [pdf](http://arxiv.org/pdf/2212.04326v1)

> Active Directory (AD) is the default security management system for Windows domain networks. An AD environment naturally describes an attack graph where nodes represent computers/accounts/security groups, and edges represent existing accesses/known exploits that allow the attacker to gain access from one node to another. Motivated by practical AD use cases, we study a Stackelberg game between one attacker and one defender. There are multiple entry nodes for the attacker to choose from and there is a single target (Domain Admin). Every edge has a failure rate. The attacker chooses the attack path with the maximum success rate. The defender can block a limited number of edges (i.e., revoke accesses) from a set of blockable edges, limited by budget. The defender's aim is to minimize the attacker's success rate.   We exploit the tree-likeness of practical AD graphs to design scalable algorithms. We propose two novel methods that combine theoretical fixed parameter analysis and practical optimisation techniques.   For graphs with small tree widths, we propose a tree decomposition based dynamic program. We then propose a general method for converting tree decomposition based dynamic programs to reinforcement learning environments, which leads to an anytime algorithm that scales better, but loses the optimality guarantee.   For graphs with small numbers of non-splitting paths (a parameter we invent specifically for AD graphs), we propose a kernelization technique that significantly downsizes the model, which is then solved via mixed-integer programming.   Experimentally, our algorithms scale to handle synthetic AD graphs with tens of thousands of nodes.

</details>

<details>

<summary>2022-12-02 17:16:04 - On Solution Functions of Optimization: Universal Approximation and Covering Number Bounds</summary>

- *Ming Jin, Vanshaj Khattar, Harshal Kaushik, Bilgehan Sel, Ruoxi Jia*

- `2212.01314v1` - [abs](http://arxiv.org/abs/2212.01314v1) - [pdf](http://arxiv.org/pdf/2212.01314v1)

> We study the expressibility and learnability of convex optimization solution functions and their multi-layer architectural extension. The main results are: \emph{(1)} the class of solution functions of linear programming (LP) and quadratic programming (QP) is a universal approximant for the $C^k$ smooth model class or some restricted Sobolev space, and we characterize the rate-distortion, \emph{(2)} the approximation power is investigated through a viewpoint of regression error, where information about the target function is provided in terms of data observations, \emph{(3)} compositionality in the form of a deep architecture with optimization as a layer is shown to reconstruct some basic functions used in numerical analysis without error, which implies that \emph{(4)} a substantial reduction in rate-distortion can be achieved with a universal network architecture, and \emph{(5)} we discuss the statistical bounds of empirical covering numbers for LP/QP, as well as a generic optimization problem (possibly nonconvex) by exploiting tame geometry. Our results provide the \emph{first rigorous analysis of the approximation and learning-theoretic properties of solution functions} with implications for algorithmic design and performance guarantees.

</details>

<details>

<summary>2022-12-02 17:29:05 - Performer: A Novel PPG-to-ECG Reconstruction Transformer for a Digital Biomarker of Cardiovascular Disease Detection</summary>

- *Ella Lan*

- `2204.11795v3` - [abs](http://arxiv.org/abs/2204.11795v3) - [pdf](http://arxiv.org/pdf/2204.11795v3)

> Electrocardiography (ECG), an electrical measurement which captures cardiac activities, is the gold standard for diagnosing cardiovascular disease (CVD). However, ECG is infeasible for continuous cardiac monitoring due to its requirement for user participation. By contrast, photoplethysmography (PPG) provides easy-to-collect data, but its limited accuracy constrains its clinical usage. To combine the advantages of both signals, recent studies incorporate various deep learning techniques for the reconstruction of PPG signals to ECG; however, the lack of contextual information as well as the limited abilities to denoise biomedical signals ultimately constrain model performance. In this research, we propose Performer, a novel Transformer-based architecture that reconstructs ECG from PPG and combines the PPG and reconstructed ECG as multiple modalities for CVD detection. This method is the first time that Transformer sequence-to-sequence translation has been performed on biomedical waveform reconstruction, combining the advantages of both PPG and ECG. We also create Shifted Patch-based Attention (SPA), an effective method to encode/decode the biomedical waveforms. Through fetching the various sequence lengths and capturing cross-patch connections, SPA maximizes the signal processing for both local features and global contextual representations. The proposed architecture generates a state-of-the-art performance of 0.29 RMSE for the reconstruction of PPG to ECG on the BIDMC database, surpassing prior studies. We also evaluated this model on the MIMIC-III dataset, achieving a 95.9% accuracy in CVD detection, and on the PPG-BP dataset, achieving 75.9% accuracy in related CVD diabetes detection, indicating its generalizability. As a proof of concept, an earring wearable named PEARL (prototype), was designed to scale up the point-of-care (POC) healthcare system.

</details>

<details>

<summary>2022-12-02 20:24:20 - FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales</summary>

- *Aaron Chan, Shaoliang Nie, Liang Tan, Xiaochang Peng, Hamed Firooz, Maziar Sanjabi, Xiang Ren*

- `2207.00779v2` - [abs](http://arxiv.org/abs/2207.00779v2) - [pdf](http://arxiv.org/pdf/2207.00779v2)

> Following how humans communicate, free-text rationales aim to use natural language to explain neural language model (LM) behavior. However, free-text rationales' unconstrained nature makes them prone to hallucination, so it is important to have metrics for free-text rationale quality. Existing free-text rationale metrics measure how consistent the rationale is with the LM's predicted label, but there is no protocol for assessing such metrics' reliability. Thus, we propose FRAME, a framework for evaluating rationale-label consistency (RLC) metrics for free-text rationales. FRAME is based on three axioms: (1) good metrics should yield highest scores for reference rationales, which maximize RLC by construction; (2) good metrics should be appropriately sensitive to semantic perturbation of rationales; and (3) good metrics should be robust to variation in the LM's task performance. Across three text classification datasets, we show that existing RLC metrics cannot satisfy all three FRAME axioms, since they are implemented via model pretraining which muddles the metric's signal. Then, we introduce a non-pretraining RLC metric that greatly outperforms baselines on (1) and (3), while performing competitively on (2). Finally, we discuss the limitations of using RLC to evaluate free-text rationales.

</details>

<details>

<summary>2022-12-03 01:32:11 - Learning to Reverse DNNs from AI Programs Automatically</summary>

- *Simin Chen, Hamed Khanpour, Cong Liu, Wei Yang*

- `2205.10364v2` - [abs](http://arxiv.org/abs/2205.10364v2) - [pdf](http://arxiv.org/pdf/2205.10364v2)

> With the privatization deployment of DNNs on edge devices, the security of on-device DNNs has raised significant concern. To quantify the model leakage risk of on-device DNNs automatically, we propose NNReverse, the first learning-based method which can reverse DNNs from AI programs without domain knowledge. NNReverse trains a representation model to represent the semantics of binary code for DNN layers. By searching the most similar function in our database, NNReverse infers the layer type of a given function's binary code. To represent assembly instructions semantics precisely, NNReverse proposes a more fine-grained embedding model to represent the textual and structural-semantic of assembly functions.

</details>

<details>

<summary>2022-12-03 08:21:13 - Multi-view deep learning based molecule design and structural optimization accelerates the SARS-CoV-2 inhibitor discovery</summary>

- *Chao Pang, Yu Wang, Yi Jiang, Ruheng Wang, Ran Su, Leyi Wei*

- `2212.01575v1` - [abs](http://arxiv.org/abs/2212.01575v1) - [pdf](http://arxiv.org/pdf/2212.01575v1)

> In this work, we propose MEDICO, a Multi-viEw Deep generative model for molecule generation, structural optimization, and the SARS-CoV-2 Inhibitor disCOvery. To the best of our knowledge, MEDICO is the first-of-this-kind graph generative model that can generate molecular graphs similar to the structure of targeted molecules, with a multi-view representation learning framework to sufficiently and adaptively learn comprehensive structural semantics from targeted molecular topology and geometry. We show that our MEDICO significantly outperforms the state-of-the-art methods in generating valid, unique, and novel molecules under benchmarking comparisons. In particular, we showcase the multi-view deep learning model enables us to generate not only the molecules structurally similar to the targeted molecules but also the molecules with desired chemical properties, demonstrating the strong capability of our model in exploring the chemical space deeply. Moreover, case study results on targeted molecule generation for the SARS-CoV-2 main protease (Mpro) show that by integrating molecule docking into our model as chemical priori, we successfully generate new small molecules with desired drug-like properties for the Mpro, potentially accelerating the de novo design of Covid-19 drugs. Further, we apply MEDICO to the structural optimization of three well-known Mpro inhibitors (N3, 11a, and GC376) and achieve ~88% improvement in their binding affinity to Mpro, demonstrating the application value of our model for the development of therapeutics for SARS-CoV-2 infection.

</details>

<details>

<summary>2022-12-03 09:49:15 - Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field</summary>

- *Chengyue Jiang, Yong Jiang, Weiqi Wu, Pengjun Xie, Kewei Tu*

- `2212.01581v1` - [abs](http://arxiv.org/abs/2212.01581v1) - [pdf](http://arxiv.org/pdf/2212.01581v1)

> Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases that correctly describe the categories of a given entity mention in a sentence. Most recent works infer each entity type independently, ignoring the correlations between types, e.g., when an entity is inferred as a president, it should also be a politician and a leader. To this end, we use an undirected graphical model called pairwise conditional random field (PCRF) to formulate the UFET problem, in which the type variables are not only unarily influenced by the input but also pairwisely relate to all the other type variables. We use various modern backbones for entity typing to compute unary potentials, and derive pairwise potentials from type phrase representations that both capture prior semantic information and facilitate accelerated inference. We use mean-field variational inference for efficient type inference on very large type sets and unfold it as a neural network module to enable end-to-end training. Experiments on UFET show that the Neural-PCRF consistently outperforms its backbones with little cost and results in a competitive performance against cross-encoder based SOTA while being thousands of times faster. We also find Neural- PCRF effective on a widely used fine-grained entity typing dataset with a smaller type set. We pack Neural-PCRF as a network module that can be plugged onto multi-label type classifiers with ease and release it in https://github.com/modelscope/adaseq/tree/master/examples/NPCRF.

</details>

<details>

<summary>2022-12-03 13:23:45 - Securing Optimized Code Against Power Side Channels</summary>

- *Rodothea Myrsini Tsoupidi, Roberto CastaÃ±eda Lozano, Elena Troubitsyna, Panagiotis Papadimitratos*

- `2207.02614v2` - [abs](http://arxiv.org/abs/2207.02614v2) - [pdf](http://arxiv.org/pdf/2207.02614v2)

> Side-channel attacks impose a serious threat to cryptographic algorithms, including widely employed ones, such as AES and RSA. These attacks take advantage of the algorithm implementation in hardware or software to extract secret information via side channels. Software masking is a mitigation approach against power side-channel attacks aiming at hiding the secret-revealing dependencies from the power footprint of a vulnerable implementation. However, this type of software mitigation often depends on general-purpose compilers, which do not preserve non-functional properties. Moreover, microarchitectural features, such as the memory bus and register reuse, may also leak secret information. These abstractions are not visible at the high-level implementation of the program. Instead, they are decided at compile time. To remedy these problems, security engineers often sacrifice code efficiency by turning off compiler optimization and/or performing local, post-compilation transformations. This paper proposes Secure by Construction Code Generation (SecCG), a constraint-based compiler approach that generates optimized yet secure against power side channels code. SecCG controls the quality of the mitigated program by efficiently searching the best possible low-level implementation according to a processor cost model. In our experiments with twelve masked cryptographic functions up to 100 lines of code on Mips32 and ARM Thumb, SecCG speeds up the generated code from 75% to 8 times compared to non-optimized secure code with an overhead of up to 7% compared to non-secure optimized code at the expense of a high compilation cost. In summary, this paper proposes a formal model to generate power side channel free low-level code.

</details>

<details>

<summary>2022-12-03 14:24:03 - CIRCLE: Continual Repair across Programming Languages</summary>

- *Wei Yuan, Quanjun Zhang, Tieke He, Chunrong Fang, Nguyen Quoc Viet Hung, Xiaodong Hao, Hongzhi Yin*

- `2205.10956v4` - [abs](http://arxiv.org/abs/2205.10956v4) - [pdf](http://arxiv.org/pdf/2205.10956v4)

> Automatic Program Repair (APR) aims at fixing buggy source code with less manual debugging efforts, which plays a vital role in improving software reliability and development productivity. Recent APR works have achieved remarkable progress via applying deep learning (DL), particularly neural machine translation (NMT) techniques. However, we observe that existing DL-based APR models suffer from at least two severe drawbacks: (1) Most of them can only generate patches for a single programming language, as a result, to repair multiple languages, we have to build and train many repairing models. (2) Most of them are developed in an offline manner. Therefore, they won't function when there are new-coming requirements. To address the above problems, a T5-based APR framework equipped with continual learning ability across multiple programming languages is proposed, namely \emph{C}ont\emph{I}nual \emph{R}epair a\emph{C}ross Programming \emph{L}anguag\emph{E}s (\emph{CIRCLE}). Specifically, (1) CIRCLE utilizes a prompting function to narrow the gap between natural language processing (NLP) pre-trained tasks and APR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achieve lifelong learning for APR without access to the full historical data. (3) An elastic regularization method is employed to strengthen CIRCLE's continual learning ability further, preventing it from catastrophic forgetting. (4) CIRCLE applies a simple but effective re-repairing method to revise generated errors caused by crossing multiple programming languages. We train CIRCLE for four languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on five commonly used benchmarks. The experimental results demonstrate that CIRCLE not only effectively and efficiently repairs multiple programming languages in continual learning settings, but also achieves state-of-the-art performance with a single repair model.

</details>

<details>

<summary>2022-12-03 16:16:11 - Intermediate Entity-based Sparse Interpretable Representation Learning</summary>

- *Diego Garcia-Olano, Yasumasa Onoe, Joydeep Ghosh, Byron C. Wallace*

- `2212.01641v1` - [abs](http://arxiv.org/abs/2212.01641v1) - [pdf](http://arxiv.org/pdf/2212.01641v1)

> Interpretable entity representations (IERs) are sparse embeddings that are "human-readable" in that dimensions correspond to fine-grained entity types and values are predicted probabilities that a given entity is of the corresponding type. These methods perform well in zero-shot and low supervision settings. Compared to standard dense neural embeddings, such interpretable representations may permit analysis and debugging. However, while fine-tuning sparse, interpretable representations improves accuracy on downstream tasks, it destroys the semantics of the dimensions which were enforced in pre-training. Can we maintain the interpretable semantics afforded by IERs while improving predictive performance on downstream tasks? Toward this end, we propose Intermediate enTity-based Sparse Interpretable Representation Learning (ItsIRL). ItsIRL realizes improved performance over prior IERs on biomedical tasks, while maintaining "interpretability" generally and their ability to support model debugging specifically. The latter is enabled in part by the ability to perform "counterfactual" fine-grained entity type manipulation, which we explore in this work. Finally, we propose a method to construct entity type based class prototypes for revealing global semantic properties of classes learned by our model.

</details>

<details>

<summary>2022-12-03 18:38:03 - T-STAR: Truthful Style Transfer using AMR Graph as Intermediate Representation</summary>

- *Anubhav Jangra, Preksha Nema, Aravindan Raghuveer*

- `2212.01667v1` - [abs](http://arxiv.org/abs/2212.01667v1) - [pdf](http://arxiv.org/pdf/2212.01667v1)

> Unavailability of parallel corpora for training text style transfer (TST) models is a very challenging yet common scenario. Also, TST models implicitly need to preserve the content while transforming a source sentence into the target style. To tackle these problems, an intermediate representation is often constructed that is devoid of style while still preserving the meaning of the source sentence. In this work, we study the usefulness of Abstract Meaning Representation (AMR) graph as the intermediate style agnostic representation. We posit that semantic notations like AMR are a natural choice for an intermediate representation. Hence, we propose T-STAR: a model comprising of two components, text-to-AMR encoder and a AMR-to-text decoder. We propose several modeling improvements to enhance the style agnosticity of the generated AMR. To the best of our knowledge, T-STAR is the first work that uses AMR as an intermediate representation for TST. With thorough experimental evaluation we show T-STAR significantly outperforms state of the art techniques by achieving on an average 15.2% higher content preservation with negligible loss (3% approx.) in style accuracy. Through detailed human evaluation with 90,000 ratings, we also show that T-STAR has up to 50% lesser hallucinations compared to state of the art TST models.

</details>

<details>

<summary>2022-12-03 20:32:04 - Harnessing label semantics to extract higher performance under noisy label for Company to Industry matching</summary>

- *Apoorva Jaiswal, Abhishek Mitra*

- `2212.01685v1` - [abs](http://arxiv.org/abs/2212.01685v1) - [pdf](http://arxiv.org/pdf/2212.01685v1)

> Assigning appropriate industry tag(s) to a company is a critical task in a financial institution as it impacts various financial machineries. Yet, it remains a complex task. Typically, such industry tags are to be assigned by Subject Matter Experts (SME) after evaluating company business lines against the industry definitions. It becomes even more challenging as companies continue to add new businesses and newer industry definitions are formed. Given the periodicity of the task it is reasonable to assume that an Artificial Intelligent (AI) agent could be developed to carry it out in an efficient manner. While this is an exciting prospect, the challenges appear from the need of historical patterns of such tag assignments (or Labeling). Labeling is often considered the most expensive task in Machine Learning (ML) due its dependency on SMEs and manual efforts. Therefore, often, in enterprise set up, an ML project encounters noisy and dependent labels. Such labels create technical hindrances for ML Models to produce robust tag assignments. We propose an ML pipeline which uses semantic similarity matching as an alternative to multi label text classification, while making use of a Label Similarity Matrix and a minimum labeling strategy. We demonstrate this pipeline achieves significant improvements over the noise and exhibit robust predictive capabilities.

</details>

<details>

<summary>2022-12-03 21:03:35 - A Generalist Neural Algorithmic Learner</summary>

- *Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, RÃ³bert CsordÃ¡s, Andrew Dudzik, Matko BoÅ¡njak, Alex Vitvitskyi, Yulia Rubanova, Andreea Deac, Beatrice Bevilacqua, Yaroslav Ganin, Charles Blundell, Petar VeliÄkoviÄ*

- `2209.11142v2` - [abs](http://arxiv.org/abs/2209.11142v2) - [pdf](http://arxiv.org/pdf/2209.11142v2)

> The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner -- a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the CLRS benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over CLRS, improving average single-task performance by over 20% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.

</details>

<details>

<summary>2022-12-04 00:49:16 - A survey on grading format of automated grading tools for programming assignments</summary>

- *Aditi Agrawal, Benjamin Reed*

- `2212.01714v1` - [abs](http://arxiv.org/abs/2212.01714v1) - [pdf](http://arxiv.org/pdf/2212.01714v1)

> The prevalence of online platforms and studies has generated the demand for automated grading tools, and as a result, there are plenty in the market. Such tools are developed to grade coding assignments quickly, accurately, and effortlessly. Since there are varieties of tools available to cater to the diverse options of programming languages and concepts, it is overwhelming for any instructor to decide which one suits one's requirements. There are several surveys studying the tools and giving insights into how they function and what they support. However other than knowing the functionality, it is important for an instructor to know how the assignments are graded and what is the format of the test cases. This is crucial since the instructor has to design the grading format and therefore requires a learning curve. This survey studies and evaluates the automated grading tools based on their evaluation format. This in turn helps a reader in deciding which tool to choose and provides an insight into what are the assessment settings and approaches used in grading the coding assignment in any specific grading tool.

</details>

<details>

<summary>2022-12-04 01:26:00 - The Open Kidney Ultrasound Data Set</summary>

- *Rohit Singla, Cailin Ringstrom, Grace Hu, Victoria Lessoway, Janice Reid, Christopher Nguan, Robert Rohling*

- `2206.06657v2` - [abs](http://arxiv.org/abs/2206.06657v2) - [pdf](http://arxiv.org/pdf/2206.06657v2)

> Ultrasound, because of its low cost, non-ionizing, and non-invasive characteristics, has established itself as a cornerstone radiological examination. Research on ultrasound applications has also expanded, especially with image analysis with machine learning. However, ultrasound data are frequently restricted to closed data sets, with only a few openly available. Despite being a frequently examined organ, the kidney lacks a publicly available ultrasonography data set. The proposed Open Kidney Ultrasound Data Set is the first publicly available set of kidney brightness mode (B-mode) ultrasound data that includes annotations for multi-class semantic segmentation. It is based on data retrospectively collected in a 5-year period from over 500 patients with a mean age of 53.2 +/- 14.7 years, body mass index of 27.0 +/- 5.4 kg/m2, and most common primary diseases being diabetes mellitus, immunoglobulin A (IgA) nephropathy, and hypertension. There are labels for the view and fine-grained manual annotations from two expert sonographers. Notably, this data includes native and transplanted kidneys. Initial bench-marking measurements are performed, demonstrating a state-of-the-art algorithm achieving a Dice Sorenson Coefficient of 0.85 for the kidney capsule. This data set is a high-quality data set, including two sets of expert annotations, with a larger breadth of images than previously available. In increasing access to kidney ultrasound data, future researchers may be able to create novel image analysis techniques for tissue characterization, disease detection, and prognostication.

</details>

<details>

<summary>2022-12-04 05:59:59 - A Fine-grained Chinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification</summary>

- *Kaifa Zhao, Le Yu, Shiyao Zhou, Jing Li, Xiapu Luo, Yat Fei Aemon Chiu, Yutong Liu*

- `2212.04357v1` - [abs](http://arxiv.org/abs/2212.04357v1) - [pdf](http://arxiv.org/pdf/2212.04357v1)

> Privacy protection raises great attention on both legal levels and user awareness. To protect user privacy, countries enact laws and regulations requiring software privacy policies to regulate their behavior. However, privacy policies are written in natural languages with many legal terms and software jargon that prevent users from understanding and even reading them. It is desirable to use NLP techniques to analyze privacy policies for helping users understand them. Furthermore, existing datasets ignore law requirements and are limited to English. In this paper, we construct the first Chinese privacy policy dataset, namely CA4P-483, to facilitate the sequence labeling tasks and regulation compliance identification between privacy policies and software. Our dataset includes 483 Chinese Android application privacy policies, over 11K sentences, and 52K fine-grained annotations. We evaluate families of robust and representative baseline models on our dataset. Based on baseline performance, we provide findings and potential research directions on our dataset. Finally, we investigate the potential applications of CA4P-483 combing regulation requirements and program analysis.

</details>

<details>

<summary>2022-12-04 06:14:55 - Differential Testing of a Verification Framework for Compiler Optimizations (Experience Paper)</summary>

- *Mark Utting, Brae J. Webb, Ian J. Hayes*

- `2212.01748v1` - [abs](http://arxiv.org/abs/2212.01748v1) - [pdf](http://arxiv.org/pdf/2212.01748v1)

> We want to verify the correctness of optimization phases in the GraalVM compiler, which consist of many thousands of lines of complex Java code performing sophisticated graph transformations. We have built high-level models of the data structures and operations of the code using the Isabelle/HOL theorem prover, and can formally verify the correctness of those high-level operations. But the remaining challenge is: how can we be sure that those high-level operations accurately reflect what the Java is doing? This paper addresses that issue by applying several different kinds of differential testing to validate that the formal model and the Java code have the same semantics. Many of these validation techniques should be applicable to other projects that are building formal models of real-world code.

</details>

<details>

<summary>2022-12-04 06:17:11 - Semantic Graph Neural Network with Multi-measure Learning for Semi-supervised Classification</summary>

- *Junchao Lin, Yuan Wan, Jingwen Xu, Xingchen Qi*

- `2212.01749v1` - [abs](http://arxiv.org/abs/2212.01749v1) - [pdf](http://arxiv.org/pdf/2212.01749v1)

> Graph Neural Networks (GNNs) have attracted increasing attention in recent years and have achieved excellent performance in semi-supervised node classification tasks. The success of most GNNs relies on one fundamental assumption, i.e., the original graph structure data is available. However, recent studies have shown that GNNs are vulnerable to the complex underlying structure of the graph, making it necessary to learn comprehensive and robust graph structures for downstream tasks, rather than relying only on the raw graph structure. In light of this, we seek to learn optimal graph structures for downstream tasks and propose a novel framework for semi-supervised classification. Specifically, based on the structural context information of graph and node representations, we encode the complex interactions in semantics and generate semantic graphs to preserve the global structure. Moreover, we develop a novel multi-measure attention layer to optimize the similarity rather than prescribing it a priori, so that the similarity can be adaptively evaluated by integrating measures. These graphs are fused and optimized together with GNN towards semi-supervised classification objective. Extensive experiments and ablation studies on six real-world datasets clearly demonstrate the effectiveness of our proposed model and the contribution of each component.

</details>

<details>

<summary>2022-12-04 07:22:21 - Languages You Know Influence Those You Learn: Impact of Language Characteristics on Multi-Lingual Text-to-Text Transfer</summary>

- *Benjamin Muller, Deepanshu Gupta, Siddharth Patwardhan, Jean-Philippe Fauconnier, David Vandyke, Sachin Agarwal*

- `2212.01757v1` - [abs](http://arxiv.org/abs/2212.01757v1) - [pdf](http://arxiv.org/pdf/2212.01757v1)

> Multi-lingual language models (LM), such as mBERT, XLM-R, mT5, mBART, have been remarkably successful in enabling natural language tasks in low-resource languages through cross-lingual transfer from high-resource ones. In this work, we try to better understand how such models, specifically mT5, transfer *any* linguistic and semantic knowledge across languages, even though no explicit cross-lingual signals are provided during pre-training. Rather, only unannotated texts from each language are presented to the model separately and independently of one another, and the model appears to implicitly learn cross-lingual connections. This raises several questions that motivate our study, such as: Are the cross-lingual connections between every language pair equally strong? What properties of source and target language impact the strength of cross-lingual transfer? Can we quantify the impact of those properties on the cross-lingual transfer?   In our investigation, we analyze a pre-trained mT5 to discover the attributes of cross-lingual connections learned by the model. Through a statistical interpretation framework over 90 language pairs across three tasks, we show that transfer performance can be modeled by a few linguistic and data-derived features. These observations enable us to interpret cross-lingual understanding of the mT5 model. Through these observations, one can favorably choose the best source language for a task, and can anticipate its training data demands. A key finding of this work is that similarity of syntax, morphology and phonology are good predictors of cross-lingual transfer, significantly more than just the lexical similarity of languages. For a given language, we are able to predict zero-shot performance, that increases on a logarithmic scale with the number of few-shot target language data points.

</details>

<details>

<summary>2022-12-04 09:04:30 - To think inside the box, or to think out of the box? Scientific discovery via the reciprocation of insights and concepts</summary>

- *Yu-Zhe Shi, Manjie Xu, Wenjuan Han, Yixin Zhu*

- `2212.00258v2` - [abs](http://arxiv.org/abs/2212.00258v2) - [pdf](http://arxiv.org/pdf/2212.00258v2)

> If scientific discovery is one of the main driving forces of human progress, insight is the fuel for the engine, which has long attracted behavior-level research to understand and model its underlying cognitive process. However, current tasks that abstract scientific discovery mostly focus on the emergence of insight, ignoring the special role played by domain knowledge. In this concept paper, we view scientific discovery as an interplay between $thinking \ out \ of \ the \ box$ that actively seeks insightful solutions and $thinking \ inside \ the \ box$ that generalizes on conceptual domain knowledge to keep correct. Accordingly, we propose Mindle, a semantic searching game that triggers scientific-discovery-like thinking spontaneously, as infrastructure for exploring scientific discovery on a large scale. On this basis, the meta-strategies for insights and the usage of concepts can be investigated reciprocally. In the pilot studies, several interesting observations inspire elaborated hypotheses on meta-strategies, context, and individual diversity for further investigations.

</details>

<details>

<summary>2022-12-04 09:10:29 - Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing</summary>

- *Lunyiu Nie, Jiuding Sun, Yanlin Wang, Lun Du, Lei Hou, Juanzi Li, Shi Han, Dongmei Zhang, Jidong Zhai*

- `2210.01425v2` - [abs](http://arxiv.org/abs/2210.01425v2) - [pdf](http://arxiv.org/pdf/2210.01425v2)

> The recent prevalence of pretrained language models (PLMs) has dramatically shifted the paradigm of semantic parsing, where the mapping from natural language utterances to structured logical forms is now formulated as a Seq2Seq task. Despite the promising performance, previous PLM-based approaches often suffer from hallucination problems due to their negligence of the structural information contained in the sentence, which essentially constitutes the key semantics of the logical forms. Furthermore, most works treat PLM as a black box in which the generation process of the target logical form is hidden beneath the decoder modules, which greatly hinders the model's intrinsic interpretability. To address these two issues, we propose to incorporate the current PLMs with a hierarchical decoder network. By taking the first-principle structures as the semantic anchors, we propose two novel intermediate supervision tasks, namely Semantic Anchor Extraction and Semantic Anchor Alignment, for training the hierarchical decoders and probing the model intermediate representations in a self-adaptive manner alongside the fine-tuning process. We conduct intensive experiments on several semantic parsing benchmarks and demonstrate that our approach can consistently outperform the baselines. More importantly, by analyzing the intermediate representations of the hierarchical decoders, our approach also makes a huge step toward the intrinsic interpretability of PLMs in the domain of semantic parsing.

</details>

<details>

<summary>2022-12-04 09:39:50 - General Cutting Planes for Bound-Propagation-Based Neural Network Verification</summary>

- *Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, J. Zico Kolter*

- `2208.05740v2` - [abs](http://arxiv.org/abs/2208.05740v2) - [pdf](http://arxiv.org/pdf/2208.05740v2)

> Bound propagation methods, when combined with branch and bound, are among the most effective methods to formally verify properties of deep neural networks such as correctness, robustness, and safety. However, existing works cannot handle the general form of cutting plane constraints widely accepted in traditional solvers, which are crucial for strengthening verifiers with tightened convex relaxations. In this paper, we generalize the bound propagation procedure to allow the addition of arbitrary cutting plane constraints, including those involving relaxed integer variables that do not appear in existing bound propagation formulations. Our generalized bound propagation method, GCP-CROWN, opens up the opportunity to apply general cutting plane methods for neural network verification while benefiting from the efficiency and GPU acceleration of bound propagation methods. As a case study, we investigate the use of cutting planes generated by off-the-shelf mixed integer programming (MIP) solver. We find that MIP solvers can generate high-quality cutting planes for strengthening bound-propagation-based verifiers using our new formulation. Since the branching-focused bound propagation procedure and the cutting-plane-focused MIP solver can run in parallel utilizing different types of hardware (GPUs and CPUs), their combination can quickly explore a large number of branches with strong cutting planes, leading to strong verification performance. Experiments demonstrate that our method is the first verifier that can completely solve the oval20 benchmark and verify twice as many instances on the oval21 benchmark compared to the best tool in VNN-COMP 2021, and also noticeably outperforms state-of-the-art verifiers on a wide range of benchmarks. GCP-CROWN is part of the $\alpha,\!\beta$-CROWN verifier, the VNN-COMP 2022 winner. Code is available at http://PaperCode.cc/GCP-CROWN

</details>

<details>

<summary>2022-12-04 10:40:40 - Kernel Inversed Pyramidal Resizing Network for Efficient Pavement Distress Recognition</summary>

- *Rong Qin, Luwen Huangfu, Devon Hood, James Ma, Sheng Huang*

- `2212.01790v1` - [abs](http://arxiv.org/abs/2212.01790v1) - [pdf](http://arxiv.org/pdf/2212.01790v1)

> Pavement Distress Recognition (PDR) is an important step in pavement inspection and can be powered by image-based automation to expedite the process and reduce labor costs. Pavement images are often in high-resolution with a low ratio of distressed to non-distressed areas. Advanced approaches leverage these properties via dividing images into patches and explore discriminative features in the scale space. However, these approaches usually suffer from information loss during image resizing and low efficiency due to complex learning frameworks. In this paper, we propose a novel and efficient method for PDR. A light network named the Kernel Inversed Pyramidal Resizing Network (KIPRN) is introduced for image resizing, and can be flexibly plugged into the image classification network as a pre-network to exploit resolution and scale information. In KIPRN, pyramidal convolution and kernel inversed convolution are specifically designed to mine discriminative information across different feature granularities and scales. The mined information is passed along to the resized images to yield an informative image pyramid to assist the image classification network for PDR. We applied our method to three well-known Convolutional Neural Networks (CNNs), and conducted an evaluation on a large-scale pavement image dataset named CQU-BPDD. Extensive results demonstrate that KIPRN can generally improve the pavement distress recognition of these CNN models and show that the simple combination of KIPRN and EfficientNet-B3 significantly outperforms the state-of-the-art patch-based method in both performance and efficiency.

</details>

<details>

<summary>2022-12-04 13:01:57 - PreQuEL: Quality Estimation of Machine Translation Outputs in Advance</summary>

- *Shachar Don-Yehiya, Leshem Choshen, Omri Abend*

- `2205.09178v2` - [abs](http://arxiv.org/abs/2205.09178v2) - [pdf](http://arxiv.org/pdf/2205.09178v2)

> We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL system predicts how well a given sentence will be translated, without recourse to the actual translation, thus eschewing unnecessary resource allocation when translation quality is bound to be low. PreQuEL can be defined relative to a given MT system (e.g., some industry service) or generally relative to the state-of-the-art. From a theoretical perspective, PreQuEL places the focus on the source text, tracing properties, possibly linguistic features, that make a sentence harder to machine translate.   We develop a baseline model for the task and analyze its performance. We also develop a data augmentation method (from parallel corpora), that improves results substantially. We show that this augmentation method can improve the performance of the Quality-Estimation task as well. We investigate the properties of the input text that our model is sensitive to, by testing it on challenge sets and different languages. We conclude that it is aware of syntactic and semantic distinctions, and correlates and even over-emphasizes the importance of standard NLP features.

</details>

<details>

<summary>2022-12-04 17:19:47 - Towards Devising A Fund Management System Using Blockchain</summary>

- *Nibula Bente Rashid, Joyeeta Saha, Raonak Islam Prova, Nowshin Tasfia, Md. Nazrul Huda Shanto, Jannatun Noor*

- `2211.03613v2` - [abs](http://arxiv.org/abs/2211.03613v2) - [pdf](http://arxiv.org/pdf/2211.03613v2)

> State government operations comprise a large number of transactions for different processes that must be carried out across the state. This comprises new projects, maintenance and repairs, public employee compensation, and agricultural schemes. Low-level corruption, which is sometimes difficult to trace and hinders state growth, is a big challenge for the top administration. In order to eradicate corruption and bring transparency, technology can be used in an efficient way. An important task to exterminate corruption is to keep track of all the financial transactions of an undergoing project. This research uses blockchain technology to keep track of fund management systems and assure the transparency of any financial statement. This paper proposes to use a gateway where all transaction records are updated in the system and visible to all stakeholders. We find research gaps in the literature and focus on including government funds and local currency usage. The proposed model's motive is to generate a funding model that attains two sub-goals: designing a fund management methodology in which authorized individuals can receive and withdraw allocated funds in crypto currency, and evaluating a smart contract to incorporate the money and identify transparency and tracking. The proposed model executes every feature of our system in just 8.3786ms on average.

</details>

<details>

<summary>2022-12-04 18:11:37 - Eliciting Structural and Semantic Global Knowledge in Unsupervised Graph Contrastive Learning</summary>

- *Kaize Ding, Yancheng Wang, Yingzhen Yang, Huan Liu*

- `2202.08480v3` - [abs](http://arxiv.org/abs/2202.08480v3) - [pdf](http://arxiv.org/pdf/2202.08480v3)

> Graph Contrastive Learning (GCL) has recently drawn much research interest for learning generalizable node representations in a self-supervised manner. In general, the contrastive learning process in GCL is performed on top of the representations learned by a graph neural network (GNN) backbone, which transforms and propagates the node contextual information based on its local neighborhoods. However, nodes sharing similar characteristics may not always be geographically close, which poses a great challenge for unsupervised GCL efforts due to their inherent limitations in capturing such global graph knowledge. In this work, we address their inherent limitations by proposing a simple yet effective framework -- Simple Neural Networks with Structural and Semantic Contrastive Learning} (S^3-CL). Notably, by virtue of the proposed structural and semantic contrastive learning algorithms, even a simple neural network can learn expressive node representations that preserve valuable global structural and semantic patterns. Our experiments demonstrate that the node representations learned by S^3-CL achieve superior performance on different downstream tasks compared with the state-of-the-art unsupervised GCL methods. Implementation and more experimental details are publicly available at \url{https://github.com/kaize0409/S-3-CL.}

</details>

<details>

<summary>2022-12-04 23:44:34 - DFEE: Interactive DataFlow Execution and Evaluation Kit</summary>

- *Han He, Song Feng, Daniele Bonadiman, Yi Zhang, Saab Mansour*

- `2212.08099v1` - [abs](http://arxiv.org/abs/2212.08099v1) - [pdf](http://arxiv.org/pdf/2212.08099v1)

> DataFlow has been emerging as a new paradigm for building task-oriented chatbots due to its expressive semantic representations of the dialogue tasks. Despite the availability of a large dataset SMCalFlow and a simplified syntax, the development and evaluation of DataFlow-based chatbots remain challenging due to the system complexity and the lack of downstream toolchains. In this demonstration, we present DFEE, an interactive DataFlow Execution and Evaluation toolkit that supports execution, visualization and benchmarking of semantic parsers given dialogue input and backend database. We demonstrate the system via a complex dialog task: event scheduling that involves temporal reasoning. It also supports diagnosing the parsing results via a friendly interface that allows developers to examine dynamic DataFlow and the corresponding execution results. To illustrate how to benchmark SoTA models, we propose a novel benchmark that covers more sophisticated event scheduling scenarios and a new metric on task success evaluation. The codes of DFEE have been released on https://github.com/amazonscience/dataflow-evaluation-toolkit.

</details>

<details>

<summary>2022-12-04 23:52:24 - Estimating Meetings' Air Flight $CO_2$ Equivalent Emissions An Illustrative Example with IETF meetings</summary>

- *Daniel Migault*

- `2212.03172v1` - [abs](http://arxiv.org/abs/2212.03172v1) - [pdf](http://arxiv.org/pdf/2212.03172v1)

> These notes describe CO2eq a tool that estimates $CO_2$ equivalent emissions associated with air traffic and applies it to the Internet Engineering Task Force (IETF), an international standard developing organization that meets 3 times a year. CO2eq estimates that the participation to IETF meetings (by a single participant) generates as much $CO_2$ equivalent as the $CO_2$ emissions per capita of European countries generating their energy using coal -- like Germany or Poland for example. This suggests some radical changes should be considered by the IETF.   According to the conclusion of the $26^{th}$ Conference of the Parties (COP26) from the United Nations Secretary-General Ant\'onio Guterres; in 2021, the number of meetings should be limited to a maximum of one meeting per year. In addition, the incorporation of sustainability principles into the IETF's strategy, should include, for example, increasing the effort to enhance the experience of 'remote' participation as well as adhering to programs (such as for example the United Nations Global Compact and the caring for climate initiative) to align its strategy and report progress toward sustainability.

</details>

<details>

<summary>2022-12-05 03:27:33 - Automated data validation: an industrial experience report</summary>

- *Lei Zhang, Sean Howard, Tom Montpool, Jessica Moore, Krittika Mahajan, Andriy Miranskyy*

- `1903.03676v2` - [abs](http://arxiv.org/abs/1903.03676v2) - [pdf](http://arxiv.org/pdf/1903.03676v2)

> There has been a massive explosion of data generated by customers and retained by companies in the last decade. However, there is a significant mismatch between the increasing volume of data and the lack of automation methods and tools. The lack of best practices in data science programming may lead to software quality degradation, release schedule slippage, and budget overruns. To mitigate these concerns, we would like to bring software engineering best practices into data science. Specifically, we focus on automated data validation in the data preparation phase of the software development life cycle.   This paper studies a real-world industrial case and applies software engineering best practices to develop an automated test harness called RESTORE. We release RESTORE as an open-source R package. Our experience report, done on the geodemographic data, shows that RESTORE enables efficient and effective detection of errors injected during the data preparation phase. RESTORE also significantly reduced the cost of testing. We hope that the community benefits from the open-source project and the practical advice based on our experience.

</details>

<details>

<summary>2022-12-05 05:08:43 - Empirical Study of Co-Renamed Identifiers</summary>

- *Yuki Osumi, Naotaka Umekawa, Hitomi Komata, Shinpei Hayashi*

- `2212.02035v1` - [abs](http://arxiv.org/abs/2212.02035v1) - [pdf](http://arxiv.org/pdf/2212.02035v1)

> Background: The renaming of program identifiers is the most common refactoring operation. Because some identifiers are related to each other, developers may need to rename related identifiers together. Aims: To understand how developers rename multiple identifiers simultaneously, it is necessary to consider the relationships between identifiers in the program and the brief matching for non-identical but semantically similar identifiers. Method: We investigate the relationships between co-renamed identifiers and identify the types of their relationships that contribute to improving the recommendation using more than 1M of renaming instances collected from the histories of open-source software projects. We also evaluate and compare the impact of co-renaming and the relationships between identifiers when inflections occur in the words in identifiers are taken into account. Results: We revealed several relationships of identifiers that are frequently found in the co-renamed identifiers, such as the identifiers of methods in the same class or an identifier defining a variable and another used for initializing the variable, depending on the type of the renamed identifiers. Additionally, the consideration of inflections did not affect the tendency of the relationships. Conclusion: These results suggest an approach that prioritizes the identifiers to be recommended depending on their types and the type of the renamed identifier.

</details>

<details>

<summary>2022-12-05 05:09:12 - Query Your Model with Definitions in FrameNet: An Effective Method for Frame Semantic Role Labeling</summary>

- *Ce Zheng, Yiming Wang, Baobao Chang*

- `2212.02036v1` - [abs](http://arxiv.org/abs/2212.02036v1) - [pdf](http://arxiv.org/pdf/2212.02036v1)

> Frame Semantic Role Labeling (FSRL) identifies arguments and labels them with frame semantic roles defined in FrameNet. Previous researches tend to divide FSRL into argument identification and role classification. Such methods usually model role classification as naive multi-class classification and treat arguments individually, which neglects label semantics and interactions between arguments and thus hindering performance and generalization of models. In this paper, we propose a query-based framework named ArGument Extractor with Definitions in FrameNet (AGED) to mitigate these problems. Definitions of frames and frame elements (FEs) in FrameNet can be used to query arguments in text. Encoding text-definition pairs can guide models in learning label semantics and strengthening argument interactions. Experiments show that AGED outperforms previous state-of-the-art by up to 1.3 F1-score in two FrameNet datasets and the generalization power of AGED in zero-shot and fewshot scenarios. Our code and technical appendix is available at https://github.com/PKUnlp-icler/AGED.

</details>

<details>

<summary>2022-12-05 07:02:05 - E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance</summary>

- *Can Chang, Ni Mu, Jiajun Wu, Ling Pan, Huazhe Xu*

- `2212.02064v1` - [abs](http://arxiv.org/abs/2212.02064v1) - [pdf](http://arxiv.org/pdf/2212.02064v1)

> A critical challenge in multi-agent reinforcement learning(MARL) is for multiple agents to efficiently accomplish complex, long-horizon tasks. The agents often have difficulties in cooperating on common goals, dividing complex tasks, and planning through several stages to make progress. We propose to address these challenges by guiding agents with programs designed for parallelization, since programs as a representation contain rich structural and semantic information, and are widely used as abstractions for long-horizon tasks. Specifically, we introduce Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance(E-MAPP), a novel framework that leverages parallel programs to guide multiple agents to efficiently accomplish goals that require planning over $10+$ stages. E-MAPP integrates the structural information from a parallel program, promotes the cooperative behaviors grounded in program semantics, and improves the time efficiency via a task allocator. We conduct extensive experiments on a series of challenging, long-horizon cooperative tasks in the Overcooked environment. Results show that E-MAPP outperforms strong baselines in terms of the completion rate, time efficiency, and zero-shot generalization ability by a large margin.

</details>

<details>

<summary>2022-12-05 07:15:41 - Syntactic Multi-view Learning for Open Information Extraction</summary>

- *Kuicai Dong, Aixin Sun, Jung-Jae Kim, Xiaoli Li*

- `2212.02068v1` - [abs](http://arxiv.org/abs/2212.02068v1) - [pdf](http://arxiv.org/pdf/2212.02068v1)

> Open Information Extraction (OpenIE) aims to extract relational tuples from open-domain sentences. Traditional rule-based or statistical models have been developed based on syntactic structures of sentences, identified by syntactic parsers. However, previous neural OpenIE models under-explore the useful syntactic information. In this paper, we model both constituency and dependency trees into word-level graphs, and enable neural OpenIE to learn from the syntactic structures. To better fuse heterogeneous information from both graphs, we adopt multi-view learning to capture multiple relationships from them. Finally, the finetuned constituency and dependency representations are aggregated with sentential semantic representations for tuple generation. Experiments show that both constituency and dependency information, and the multi-view learning are effective.

</details>

<details>

<summary>2022-12-05 07:20:42 - LGDN: Language-Guided Denoising Network for Video-Language Modeling</summary>

- *Haoyu Lu, Mingyu Ding, Nanyi Fei, Yuqi Huo, Zhiwu Lu*

- `2209.11388v3` - [abs](http://arxiv.org/abs/2209.11388v3) - [pdf](http://arxiv.org/pdf/2209.11388v3)

> Video-language modeling has attracted much attention with the rapid growth of web videos. Most existing methods assume that the video frames and text description are semantically correlated, and focus on video-language modeling at video level. However, this hypothesis often fails for two reasons: (1) With the rich semantics of video contents, it is difficult to cover all frames with a single video-level description; (2) A raw video typically has noisy/meaningless information (e.g., scenery shot, transition or teaser). Although a number of recent works deploy attention mechanism to alleviate this problem, the irrelevant/noisy information still makes it very difficult to address. To overcome such challenge, we thus propose an efficient and effective model, termed Language-Guided Denoising Network (LGDN), for video-language modeling. Different from most existing methods that utilize all extracted video frames, LGDN dynamically filters out the misaligned or redundant frames under the language supervision and obtains only 2--4 salient frames per video for cross-modal token-level alignment. Extensive experiments on five public datasets show that our LGDN outperforms the state-of-the-arts by large margins. We also provide detailed ablation study to reveal the critical importance of solving the noise issue, in hope of inspiring future video-language work.

</details>

<details>

<summary>2022-12-05 09:17:49 - A Transformer-Based User Satisfaction Prediction for Proactive Interaction Mechanism in DuerOS</summary>

- *Wei Shen, Xiaonan He, Chuheng Zhang, Xuyun Zhang, Jian XIe*

- `2212.03817v1` - [abs](http://arxiv.org/abs/2212.03817v1) - [pdf](http://arxiv.org/pdf/2212.03817v1)

> Recently, spoken dialogue systems have been widely deployed in a variety of applications, serving a huge number of end-users. A common issue is that the errors resulting from noisy utterances, semantic misunderstandings, or lack of knowledge make it hard for a real system to respond properly, possibly leading to an unsatisfactory user experience. To avoid such a case, we consider a proactive interaction mechanism where the system predicts the user satisfaction with the candidate response before giving it to the user. If the user is not likely to be satisfied according to the prediction, the system will ask the user a suitable question to determine the real intent of the user instead of providing the response directly. With such an interaction with the user, the system can give a better response to the user. Previous models that predict the user satisfaction are not applicable to DuerOS which is a large-scale commercial dialogue system. They are based on hand-crafted features and thus can hardly learn the complex patterns lying behind millions of conversations and temporal dependency in multiple turns of the conversation. Moreover, they are trained and evaluated on the benchmark datasets with adequate labels, which are expensive to obtain in a commercial dialogue system. To face these challenges, we propose a pipeline to predict the user satisfaction to help DuerOS decide whether to ask for clarification in each turn. Specifically, we propose to first generate a large number of weak labels and then train a transformer-based model to predict the user satisfaction with these weak labels. Empirically, we deploy and evaluate our model on DuerOS, and observe a 19% relative improvement on the accuracy of user satisfaction prediction and 2.3% relative improvement on user experience.

</details>

<details>

<summary>2022-12-05 09:42:29 - Learning logic programs by discovering where not to search</summary>

- *Andrew Cropper, CÃ©line Hocquette*

- `2202.09806v2` - [abs](http://arxiv.org/abs/2202.09806v2) - [pdf](http://arxiv.org/pdf/2202.09806v2)

> The goal of inductive logic programming (ILP) is to search for a hypothesis that generalises training examples and background knowledge (BK). To improve performance, we introduce an approach that, before searching for a hypothesis, first discovers where not to search. We use given BK to discover constraints on hypotheses, such as that a number cannot be both even and odd. We use the constraints to bootstrap a constraint-driven ILP system. Our experiments on multiple domains (including program synthesis and game playing) show that our approach can (i) substantially reduce learning times by up to 97%, and (ii) scale to domains with millions of facts.

</details>

<details>

<summary>2022-12-05 11:21:43 - Repair Is Nearly Generation: Multilingual Program Repair with LLMs</summary>

- *Harshit Joshi, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, Gust Verbruggen*

- `2208.11640v3` - [abs](http://arxiv.org/abs/2208.11640v3) - [pdf](http://arxiv.org/pdf/2208.11640v3)

> Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program -- a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages.

</details>

<details>

<summary>2022-12-05 12:14:00 - FedUKD: Federated UNet Model with Knowledge Distillation for Land Use Classification from Satellite and Street Views</summary>

- *Renuga Kanagavelu, Kinshuk Dua, Pratik Garai, Susan Elias, Neha Thomas, Simon Elias, Qingsong Wei, Goh Siow Mong Rick, Liu Yong*

- `2212.02196v1` - [abs](http://arxiv.org/abs/2212.02196v1) - [pdf](http://arxiv.org/pdf/2212.02196v1)

> Federated Deep Learning frameworks can be used strategically to monitor Land Use locally and infer environmental impacts globally. Distributed data from across the world would be needed to build a global model for Land Use classification. The need for a Federated approach in this application domain would be to avoid transfer of data from distributed locations and save network bandwidth to reduce communication cost. We use a Federated UNet model for Semantic Segmentation of satellite and street view images. The novelty of the proposed architecture is the integration of Knowledge Distillation to reduce communication cost and response time. The accuracy obtained was above 95% and we also brought in a significant model compression to over 17 times and 62 times for street View and satellite images respectively. Our proposed framework has the potential to be a game-changer in real-time tracking of climate change across the planet.

</details>

<details>

<summary>2022-12-05 13:01:00 - CORNET: Learning Table Formatting Rules By Example</summary>

- *Mukul Singh, JosÃ© Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Mohammad Raza, Gust Verbruggen*

- `2208.06032v4` - [abs](http://arxiv.org/abs/2208.06032v4) - [pdf](http://arxiv.org/pdf/2208.06032v4)

> Spreadsheets are widely used for table manipulation and presentation. Stylistic formatting of these tables is an important property for both presentation and analysis. As a result, popular spreadsheet software, such as Excel, supports automatically formatting tables based on rules. Unfortunately, writing such formatting rules can be challenging for users as it requires knowledge of the underlying rule language and data logic. We present CORNET, a system that tackles the novel problem of automatically learning such formatting rules from user examples in the form of formatted cells. CORNET takes inspiration from advances in inductive programming and combines symbolic rule enumeration with a neural ranker to learn conditional formatting rules. To motivate and evaluate our approach, we extracted tables with over 450K unique formatting rules from a corpus of over 1.8M real worksheets. Since we are the first to introduce conditional formatting, we compare CORNET to a wide range of symbolic and neural baselines adapted from related domains. Our results show that CORNET accurately learns rules across varying evaluation setups. Additionally, we show that CORNET finds shorter rules than those that a user has written and discovers rules in spreadsheets that users have manually formatted.

</details>

<details>

<summary>2022-12-05 13:45:45 - Applications of human activity recognition in industrial processes -- Synergy of human and technology</summary>

- *Friedrich Niemann, Christopher Reining, HÃ¼lya Bas, Sven Franke*

- `2212.02266v1` - [abs](http://arxiv.org/abs/2212.02266v1) - [pdf](http://arxiv.org/pdf/2212.02266v1)

> Human-technology collaboration relies on verbal and non-verbal communication. Machines must be able to detect and understand the movements of humans to facilitate non-verbal communication. In this article, we introduce ongoing research on human activity recognition in intralogistics, and show how it can be applied in industrial settings. We show how semantic attributes can be used to describe human activities flexibly and how context informantion increases the performance of classifiers to recognise them automatically. Beyond that, we present a concept based on a cyber-physical twin that can reduce the effort and time necessary to create a training dataset for human activity recognition. In the future, it will be possible to train a classifier solely with realistic simulation data, while maintaining or even increasing the classification performance.

</details>

<details>

<summary>2022-12-05 13:50:35 - Entity Set Co-Expansion in StackOverflow</summary>

- *Yu Zhang, Yunyi Zhang, Yucheng Jiang, Martin Michalski, Yu Deng, Lucian Popa, ChengXiang Zhai, Jiawei Han*

- `2212.02271v1` - [abs](http://arxiv.org/abs/2212.02271v1) - [pdf](http://arxiv.org/pdf/2212.02271v1)

> Given a few seed entities of a certain type (e.g., Software or Programming Language), entity set expansion aims to discover an extensive set of entities that share the same type as the seeds. Entity set expansion in software-related domains such as StackOverflow can benefit many downstream tasks (e.g., software knowledge graph construction) and facilitate better IT operations and service management. Meanwhile, existing approaches are less concerned with two problems: (1) How to deal with multiple types of seed entities simultaneously? (2) How to leverage the power of pre-trained language models (PLMs)? Being aware of these two problems, in this paper, we study the entity set co-expansion task in StackOverflow, which extracts Library, OS, Application, and Language entities from StackOverflow question-answer threads. During the co-expansion process, we use PLMs to derive embeddings of candidate entities for calculating similarities between entities. Experimental results show that our proposed SECoExpan framework outperforms previous approaches significantly.

</details>

<details>

<summary>2022-12-05 13:53:51 - EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</summary>

- *Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao*

- `2211.07636v2` - [abs](http://arxiv.org/abs/2211.07636v2) - [pdf](http://arxiv.org/pdf/2211.07636v2)

> We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and models at https://github.com/baaivision/EVA.

</details>

<details>

<summary>2022-12-05 18:23:59 - Quantized Wasserstein Procrustes Alignment of Word Embedding Spaces</summary>

- *Prince O Aboagye, Yan Zheng, Michael Yeh, Junpeng Wang, Zhongfang Zhuang, Huiyuan Chen, Liang Wang, Wei Zhang, Jeff Phillips*

- `2212.02468v1` - [abs](http://arxiv.org/abs/2212.02468v1) - [pdf](http://arxiv.org/pdf/2212.02468v1)

> Optimal Transport (OT) provides a useful geometric framework to estimate the permutation matrix under unsupervised cross-lingual word embedding (CLWE) models that pose the alignment task as a Wasserstein-Procrustes problem. However, linear programming algorithms and approximate OT solvers via Sinkhorn for computing the permutation matrix come with a significant computational burden since they scale cubically and quadratically, respectively, in the input size. This makes it slow and infeasible to compute OT distances exactly for a larger input size, resulting in a poor approximation quality of the permutation matrix and subsequently a less robust learned transfer function or mapper. This paper proposes an unsupervised projection-based CLWE model called quantized Wasserstein Procrustes (qWP). qWP relies on a quantization step of both the source and target monolingual embedding space to estimate the permutation matrix given a cheap sampling procedure. This approach substantially improves the approximation quality of empirical OT solvers given fixed computational cost. We demonstrate that qWP achieves state-of-the-art results on the Bilingual lexicon Induction (BLI) task.

</details>

<details>

<summary>2022-12-05 18:36:45 - Certifying Fairness of Probabilistic Circuits</summary>

- *Nikil Roashan Selvam, Guy Van den Broeck, YooJung Choi*

- `2212.02474v1` - [abs](http://arxiv.org/abs/2212.02474v1) - [pdf](http://arxiv.org/pdf/2212.02474v1)

> With the increased use of machine learning systems for decision making, questions about the fairness properties of such systems start to take center stage. Most existing work on algorithmic fairness assume complete observation of features at prediction time, as is the case for popular notions like statistical parity and equal opportunity. However, this is not sufficient for models that can make predictions with partial observation as we could miss patterns of bias and incorrectly certify a model to be fair. To address this, a recently introduced notion of fairness asks whether the model exhibits any discrimination pattern, in which an individual characterized by (partial) feature observations, receives vastly different decisions merely by disclosing one or more sensitive attributes such as gender and race. By explicitly accounting for partial observations, this provides a much more fine-grained notion of fairness.   In this paper, we propose an algorithm to search for discrimination patterns in a general class of probabilistic models, namely probabilistic circuits. Previously, such algorithms were limited to naive Bayes classifiers which make strong independence assumptions; by contrast, probabilistic circuits provide a unifying framework for a wide range of tractable probabilistic models and can even be compiled from certain classes of Bayesian networks and probabilistic programs, making our method much more broadly applicable. Furthermore, for an unfair model, it may be useful to quickly find discrimination patterns and distill them for better interpretability. As such, we also propose a sampling-based approach to more efficiently mine discrimination patterns, and introduce new classes of patterns such as minimal, maximal, and Pareto optimal patterns that can effectively summarize exponentially many discrimination patterns

</details>

<details>

<summary>2022-12-05 18:58:58 - PEANUT: Predicting and Navigating to Unseen Targets</summary>

- *Albert J. Zhai, Shenlong Wang*

- `2212.02497v1` - [abs](http://arxiv.org/abs/2212.02497v1) - [pdf](http://arxiv.org/pdf/2212.02497v1)

> Efficient ObjectGoal navigation (ObjectNav) in novel environments requires an understanding of the spatial and semantic regularities in environment layouts. In this work, we present a straightforward method for learning these regularities by predicting the locations of unobserved objects from incomplete semantic maps. Our method differs from previous prediction-based navigation methods, such as frontier potential prediction or egocentric map completion, by directly predicting unseen targets while leveraging the global context from all previously explored areas. Our prediction model is lightweight and can be trained in a supervised manner using a relatively small amount of passively collected data. Once trained, the model can be incorporated into a modular pipeline for ObjectNav without the need for any reinforcement learning. We validate the effectiveness of our method on the HM3D and MP3D ObjectNav datasets. We find that it achieves the state-of-the-art on both datasets, despite not using any additional data for training.

</details>

<details>

<summary>2022-12-05 22:09:36 - Katana: Dual Slicing-Based Context for Learning Bug Fixes</summary>

- *Mifta Sintaha, Noor Nashid, Ali Mesbah*

- `2205.00180v3` - [abs](http://arxiv.org/abs/2205.00180v3) - [pdf](http://arxiv.org/pdf/2205.00180v3)

> Contextual information plays a vital role for software developers when understanding and fixing a bug. Consequently, deep learning-based program repair techniques leverage context for bug fixes. However, existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy statement within the enclosing file, class, or method, without any analysis to find actual relations with the bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context. We present a program slicing-based approach, in which instead of arbitrarily including code as context, we analyze statements that have a control or data dependency on the buggy statement. We propose a novel concept called dual slicing, which leverages the context of both buggy and fixed versions of the code to capture relevant repair ingredients. We present our technique and tool called Katana, the first to apply slicing-based context for a program repair task. The results show Katana effectively preserves sufficient information for a model to choose contextual information while reducing noise. We compare against four recent state-of-the-art context-aware program repair techniques. Our results show Katana fixes between 1.5 to 3.7 times more bugs than existing techniques.

</details>

<details>

<summary>2022-12-05 22:23:27 - POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events</summary>

- *Sai Vallurupalli, Sayontan Ghosh, Katrin Erk, Niranjan Balasubramanian, Francis Ferraro*

- `2212.02629v1` - [abs](http://arxiv.org/abs/2212.02629v1) - [pdf](http://arxiv.org/pdf/2212.02629v1)

> Knowledge about outcomes is critical for complex event understanding but is hard to acquire. We show that by pre-identifying a participant in a complex event, crowd workers are able to (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement of participants in causing the situation, and (3) ground the outcome of the situation in state changes of the participants. By creating a multi-step interface and a careful quality control strategy, we collect a high quality annotated dataset of 8K short newswire narratives and ROCStories with high inter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue (Participant Outcome Questions), enables the exploration and development of models that address multiple aspects of semantic understanding. Experimentally, we show that current language models lag behind human performance in subtle ways through our task formulations that target abstract and specific comprehension of a complex event, its outcome, and a participant's influence over the event culmination.

</details>

<details>

<summary>2022-12-05 22:29:46 - Concept Embedding Models: Beyond the Accuracy-Explainability Trade-Off</summary>

- *Mateo Espinosa Zarlenga, Pietro Barbiero, Gabriele Ciravegna, Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Zohreh Shams, Frederic Precioso, Stefano Melacci, Adrian Weller, Pietro Lio, Mateja Jamnik*

- `2209.09056v2` - [abs](http://arxiv.org/abs/2209.09056v2) - [pdf](http://arxiv.org/pdf/2209.09056v2)

> Deploying AI-powered systems requires trustworthy models supporting effective human interactions, going beyond raw prediction accuracy. Concept bottleneck models promote trustworthiness by conditioning classification tasks on an intermediate level of human-like concepts. This enables human interventions which can correct mispredicted concepts to improve the model's performance. However, existing concept bottleneck models are unable to find optimal compromises between high task accuracy, robust concept-based explanations, and effective interventions on concepts -- particularly in real-world conditions where complete and accurate concept supervisions are scarce. To address this, we propose Concept Embedding Models, a novel family of concept bottleneck models which goes beyond the current accuracy-vs-interpretability trade-off by learning interpretable high-dimensional concept representations. Our experiments demonstrate that Concept Embedding Models (1) attain better or competitive task accuracy w.r.t. standard neural models without concepts, (2) provide concept representations capturing meaningful semantics including and beyond their ground truth labels, (3) support test-time concept interventions whose effect in test accuracy surpasses that in standard concept bottleneck models, and (4) scale to real-world conditions where complete concept supervisions are scarce.

</details>

<details>

<summary>2022-12-05 22:56:28 - MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning</summary>

- *Weiguo Pian, Hanyu Peng, Xunzhu Tang, Tiezhu Sun, Haoye Tian, Andrew Habib, Jacques Klein, TegawendÃ© F. BissyandÃ©*

- `2206.06460v2` - [abs](http://arxiv.org/abs/2206.06460v2) - [pdf](http://arxiv.org/pdf/2206.06460v2)

> Representation learning of source code is essential for applying machine learning to software engineering tasks. Learning code representation from a multilingual source code dataset has been shown to be more effective than learning from single-language datasets separately, since more training data from multilingual dataset improves the model's ability to extract language-agnostic information from source code. However, existing multilingual training overlooks the language-specific information which is crucial for modeling source code across different programming languages, while only focusing on learning a unified model with shared parameters among different languages for language-agnostic information modeling. To address this problem, we propose MetaTPTrans, a meta learning approach for multilingual code representation learning. MetaTPTrans generates different parameters for the feature extractor according to the specific programming language type of the input code snippet, enabling the model to learn both language-agnostic and language-specific information with dynamic parameters in the feature extractor. We conduct experiments on the code summarization and code completion tasks to verify the effectiveness of our approach. The results demonstrate the superiority of our approach with significant improvements on state-of-the-art baselines.

</details>

<details>

<summary>2022-12-05 23:33:36 - Transformer-Based Named Entity Recognition for French Using Adversarial Adaptation to Similar Domain Corpora</summary>

- *Arjun Choudhry, Pankaj Gupta, Inder Khatri, Aaryan Gupta, Maxime Nicol, Marie-Jean Meurs, Dinesh Kumar Vishwakarma*

- `2212.03692v1` - [abs](http://arxiv.org/abs/2212.03692v1) - [pdf](http://arxiv.org/pdf/2212.03692v1)

> Named Entity Recognition (NER) involves the identification and classification of named entities in unstructured text into predefined classes. NER in languages with limited resources, like French, is still an open problem due to the lack of large, robust, labelled datasets. In this paper, we propose a transformer-based NER approach for French using adversarial adaptation to similar domain or general corpora for improved feature extraction and better generalization. We evaluate our approach on three labelled datasets and show that our adaptation framework outperforms the corresponding non-adaptive models for various combinations of transformer models, source datasets and target corpora.

</details>

<details>

<summary>2022-12-06 00:53:38 - Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation</summary>

- *Anjan Karmakar, Julian Aron Prenner, Marco D'Ambros, Romain Robbes*

- `2212.02684v1` - [abs](http://arxiv.org/abs/2212.02684v1) - [pdf](http://arxiv.org/pdf/2212.02684v1)

> The Codex model has demonstrated extraordinary competence in synthesizing code from natural language problem descriptions. However, in order to reveal unknown failure modes and hidden biases, such large-scale models must be systematically subjected to multiple and diverse evaluation studies.   In this work, we evaluate the code synthesis capabilities of the Codex model based on a set of 115 Python problem statements from a popular competitive programming portal: HackerRank. Our evaluation shows that Codex is indeed proficient in Python, solving 96% of the problems in a zero-shot setting, and 100% of the problems in a few-shot setting. However, Codex exhibits clear signs of generating memorized code based on our evaluation. This is alarming, especially since the adoption and use of such models could directly impact how code is written and produced in the foreseeable future. With this in mind, we further discuss and highlight some of the prominent risks associated with large-scale models of source code. Finally, we propose a framework for code-synthesis evaluation using variations of problem statements based on mutations.

</details>

<details>

<summary>2022-12-06 04:09:47 - Semantic-aware Message Broadcasting for Efficient Unsupervised Domain Adaptation</summary>

- *Xin Li, Cuiling Lan, Guoqiang Wei, Zhibo Chen*

- `2212.02739v1` - [abs](http://arxiv.org/abs/2212.02739v1) - [pdf](http://arxiv.org/pdf/2212.02739v1)

> Vision transformer has demonstrated great potential in abundant vision tasks. However, it also inevitably suffers from poor generalization capability when the distribution shift occurs in testing (i.e., out-of-distribution data). To mitigate this issue, we propose a novel method, Semantic-aware Message Broadcasting (SAMB), which enables more informative and flexible feature alignment for unsupervised domain adaptation (UDA). Particularly, we study the attention module in the vision transformer and notice that the alignment space using one global class token lacks enough flexibility, where it interacts information with all image tokens in the same manner but ignores the rich semantics of different regions. In this paper, we aim to improve the richness of the alignment features by enabling semantic-aware adaptive message broadcasting. Particularly, we introduce a group of learned group tokens as nodes to aggregate the global information from all image tokens, but encourage different group tokens to adaptively focus on the message broadcasting to different semantic regions. In this way, our message broadcasting encourages the group tokens to learn more informative and diverse information for effective domain alignment. Moreover, we systematically study the effects of adversarial-based feature alignment (ADA) and pseudo-label based self-training (PST) on UDA. We find that one simple two-stage training strategy with the cooperation of ADA and PST can further improve the adaptation capability of the vision transformer. Extensive experiments on DomainNet, OfficeHome, and VisDA-2017 demonstrate the effectiveness of our methods for UDA.

</details>

<details>

<summary>2022-12-06 04:37:51 - UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression</summary>

- *Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, Xiaodan Liang*

- `2212.02746v1` - [abs](http://arxiv.org/abs/2212.02746v1) - [pdf](http://arxiv.org/pdf/2212.02746v1)

> Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.

</details>

<details>

<summary>2022-12-06 05:50:30 - Unsafe's Betrayal: Abusing Unsafe Rust in Binary Reverse Engineering via Machine Learning</summary>

- *Sangdon Park, Xiang Cheng, Taesoo Kim*

- `2211.00111v2` - [abs](http://arxiv.org/abs/2211.00111v2) - [pdf](http://arxiv.org/pdf/2211.00111v2)

> Memory-safety bugs introduce critical software-security issues. Rust provides memory-safe mechanisms to avoid memory-safety bugs in programming, while still allowing unsafe escape hatches via unsafe code. However, the unsafe code that enhances the usability of Rust provides clear spots for finding memory-safety bugs in Rust source code. In this paper, we claim that these unsafe spots can still be identifiable in Rust binary code via machine learning and be leveraged for finding memory-safety bugs. To support our claim, we propose the tool textttrustspot, that enables reverse engineering to learn an unsafe classifier that proposes a list of functions in Rust binaries for downstream analysis. We empirically show that the function proposals by textttrustspot can recall $92.92\%$ of memory-safety bugs, while it covers only $16.79\%$ of the entire binary code. As an application, we demonstrate that the function proposals are used in targeted fuzzing on Rust packages, which contribute to reducing the fuzzing time compared to non-targeted fuzzing.

</details>

<details>

<summary>2022-12-06 06:48:32 - Union-set Multi-source Model Adaptation for Semantic Segmentation</summary>

- *Zongyao Li, Ren Togo, Takahiro Ogawa, Miki haseyama*

- `2212.02785v1` - [abs](http://arxiv.org/abs/2212.02785v1) - [pdf](http://arxiv.org/pdf/2212.02785v1)

> This paper solves a generalized version of the problem of multi-source model adaptation for semantic segmentation. Model adaptation is proposed as a new domain adaptation problem which requires access to a pre-trained model instead of data for the source domain. A general multi-source setting of model adaptation assumes strictly that each source domain shares a common label space with the target domain. As a relaxation, we allow the label space of each source domain to be a subset of that of the target domain and require the union of the source-domain label spaces to be equal to the target-domain label space. For the new setting named union-set multi-source model adaptation, we propose a method with a novel learning strategy named model-invariant feature learning, which takes full advantage of the diverse characteristics of the source-domain models, thereby improving the generalization in the target domain. We conduct extensive experiments in various adaptation settings to show the superiority of our method. The code is available at https://github.com/lzy7976/union-set-model-adaptation.

</details>

<details>

<summary>2022-12-06 07:23:39 - FlowFace: Semantic Flow-guided Shape-aware Face Swapping</summary>

- *Hao Zeng, Wei Zhang, Changjie Fan, Tangjie Lv, Suzhen Wang, Zhimeng Zhang, Bowen Ma, Lincheng Li, Yu Ding, Xin Yu*

- `2212.02797v1` - [abs](http://arxiv.org/abs/2212.02797v1) - [pdf](http://arxiv.org/pdf/2212.02797v1)

> In this work, we propose a semantic flow-guided two-stage framework for shape-aware face swapping, namely FlowFace. Unlike most previous methods that focus on transferring the source inner facial features but neglect facial contours, our FlowFace can transfer both of them to a target face, thus leading to more realistic face swapping. Concretely, our FlowFace consists of a face reshaping network and a face swapping network. The face reshaping network addresses the shape outline differences between the source and target faces. It first estimates a semantic flow (i.e., face shape differences) between the source and the target face, and then explicitly warps the target face shape with the estimated semantic flow. After reshaping, the face swapping network generates inner facial features that exhibit the identity of the source face. We employ a pre-trained face masked autoencoder (MAE) to extract facial features from both the source face and the target face. In contrast to previous methods that use identity embedding to preserve identity information, the features extracted by our encoder can better capture facial appearances and identity information. Then, we develop a cross-attention fusion module to adaptively fuse inner facial features from the source face with the target facial attributes, thus leading to better identity preservation. Extensive quantitative and qualitative experiments on in-the-wild faces demonstrate that our FlowFace outperforms the state-of-the-art significantly.

</details>

<details>

<summary>2022-12-06 08:55:46 - OSC-Qasm: Interfacing Music Software with Quantum Computing</summary>

- *Omar Costa Hamido, Paulo Vitor ItaboraÃ­*

- `2212.01615v2` - [abs](http://arxiv.org/abs/2212.01615v2) - [pdf](http://arxiv.org/pdf/2212.01615v2)

> OSC-Qasm is a cross-platform, Python-based, OSC interface for executing Qasm code. It serves as a simple way to connect creative programming environments like Max (with The QAC Toolkit) and Pure Data with real quantum hardware, using the Open Sound Control protocol. In this paper, the authors introduce the context and meaning of developing a tool like this, and what it can offer to creative artists.

</details>

<details>

<summary>2022-12-06 11:06:32 - Ask "Who", Not "What": Bitcoin Volatility Forecasting with Twitter Data</summary>

- *M. Eren Akbiyik, Mert Erkul, Killian Kaempf, Vaiva Vasiliauskaite, Nino Antulov-Fantulin*

- `2110.14317v2` - [abs](http://arxiv.org/abs/2110.14317v2) - [pdf](http://arxiv.org/pdf/2110.14317v2)

> Understanding the variations in trading price (volatility), and its response to exogenous information, is a well-researched topic in finance. In this study, we focus on finding stable and accurate volatility predictors for a relatively new asset class of cryptocurrencies, in particular Bitcoin, using deep learning representations of public social media data obtained from Twitter. For our experiments, we extracted semantic information and user statistics from over 30 million Bitcoin-related tweets, in conjunction with 15-minute frequency price data over a horizon of 144 days. Using this data, we built several deep learning architectures that utilized different combinations of the gathered information. For each model, we conducted ablation studies to assess the influence of different components and feature sets over the prediction accuracy. We found statistical evidences for the hypotheses that: (i) temporal convolutional networks perform significantly better than both classical autoregressive models and other deep learning-based architectures in the literature, and (ii) tweet author meta-information, even detached from the tweet itself, is a better predictor of volatility than the semantic content and tweet volume statistics. We demonstrate how different information sets gathered from social media can be utilized in different architectures and how they affect the prediction results. As an additional contribution, we make our dataset public for future research.

</details>

<details>

<summary>2022-12-06 11:48:21 - Budge: a programming language and a theorem prover</summary>

- *Boro Sitnikovski*

- `2205.07979v6` - [abs](http://arxiv.org/abs/2205.07979v6) - [pdf](http://arxiv.org/pdf/2205.07979v6)

> We present a simple programming language based on G\"odel numbering and prime factorization, enhanced with explicit, scoped loops, allowing for easy program composition. Further, we will present a theorem prover that allows expressing and working with formal systems. The theorem prover is simple as it relies merely on a substitution rule and set equality to derive theorems. Finally, we will represent the programming language in the theorem prover. We will show the syntax and semantics of both, and then provide a few example programs and their evaluation.

</details>

<details>

<summary>2022-12-06 12:35:56 - The purpose of qualia: What if human thinking is not (only) information processing?</summary>

- *Martin Korth*

- `2212.00800v2` - [abs](http://arxiv.org/abs/2212.00800v2) - [pdf](http://arxiv.org/pdf/2212.00800v2)

> Despite recent breakthroughs in the field of artificial intelligence (AI) - or more specifically machine learning (ML) algorithms for object recognition and natural language processing - it seems to be the majority view that current AI approaches are still no real match for natural intelligence (NI). More importantly, philosophers have collected a long catalogue of features which imply that NI works differently from current AI not only in a gradual sense, but in a more substantial way: NI is closely related to consciousness, intentionality and experiential features like qualia (the subjective contents of mental states) and allows for understanding (e.g., taking insight into causal relationships instead of 'blindly' relying on correlations), as well as aesthetical and ethical judgement beyond what we can put into (explicit or data-induced implicit) rules to program machines with. Additionally, Psychologists find NI to range from unconscious psychological processes to focused information processing, and from embodied and implicit cognition to 'true' agency and creativity. NI thus seems to transcend any neurobiological functionalism by operating on 'bits of meaning' instead of information in the sense of data, quite unlike both the 'good old fashioned', symbolic AI of the past, as well as the current wave of deep neural network based, 'sub-symbolic' AI, which both share the idea of thinking as (only) information processing. In the following I propose an alternative view of NI as information processing plus 'bundle pushing', discuss an example which illustrates how bundle pushing can cut information processing short, and suggest first ideas for scientific experiments in neuro-biology and information theory as further investigations.

</details>

<details>

<summary>2022-12-06 14:13:33 - Knowledge-Bridged Causal Interaction Network for Causal Emotion Entailment</summary>

- *Weixiang Zhao, Yanyan Zhao, Zhuojun Li, Bing Qin*

- `2212.02995v1` - [abs](http://arxiv.org/abs/2212.02995v1) - [pdf](http://arxiv.org/pdf/2212.02995v1)

> Causal Emotion Entailment aims to identify causal utterances that are responsible for the target utterance with a non-neutral emotion in conversations. Previous works are limited in thorough understanding of the conversational context and accurate reasoning of the emotion cause. To this end, we propose Knowledge-Bridged Causal Interaction Network (KBCIN) with commonsense knowledge (CSK) leveraged as three bridges. Specifically, we construct a conversational graph for each conversation and leverage the event-centered CSK as the semantics-level bridge (S-bridge) to capture the deep inter-utterance dependencies in the conversational context via the CSK-Enhanced Graph Attention module. Moreover, social-interaction CSK serves as emotion-level bridge (E-bridge) and action-level bridge (A-bridge) to connect candidate utterances with the target one, which provides explicit causal clues for the Emotional Interaction module and Actional Interaction module to reason the target emotion. Experimental results show that our model achieves better performance over most baseline models. Our source code is publicly available at https://github.com/circle-hit/KBCIN.

</details>

<details>

<summary>2022-12-06 14:32:02 - A Strongly Polynomial Algorithm for Approximate Forster Transforms and its Application to Halfspace Learning</summary>

- *Ilias Diakonikolas, Christos Tzamos, Daniel M. Kane*

- `2212.03008v1` - [abs](http://arxiv.org/abs/2212.03008v1) - [pdf](http://arxiv.org/pdf/2212.03008v1)

> The Forster transform is a method of regularizing a dataset by placing it in {\em radial isotropic position} while maintaining some of its essential properties. Forster transforms have played a key role in a diverse range of settings spanning computer science and functional analysis. Prior work had given {\em weakly} polynomial time algorithms for computing Forster transforms, when they exist. Our main result is the first {\em strongly polynomial time} algorithm to compute an approximate Forster transform of a given dataset or certify that no such transformation exists. By leveraging our strongly polynomial Forster algorithm, we obtain the first strongly polynomial time algorithm for {\em distribution-free} PAC learning of halfspaces. This learning result is surprising because {\em proper} PAC learning of halfspaces is {\em equivalent} to linear programming. Our learning approach extends to give a strongly polynomial halfspace learner in the presence of random classification noise and, more generally, Massart noise.

</details>

<details>

<summary>2022-12-06 15:07:09 - Integration of a systolic array based hardware accelerator into a DNN operator auto-tuning framework</summary>

- *F. N. Peccia, O. Bringmann*

- `2212.03034v1` - [abs](http://arxiv.org/abs/2212.03034v1) - [pdf](http://arxiv.org/pdf/2212.03034v1)

> The deployment of neural networks on heterogeneous SoCs coupled with custom accelerators is a challenging task because of the lack of end-to-end software tools provided for these systems. Moreover, the already available low level schedules and mapping strategies provided by the accelerator developers for typical tensor operations are not necessarily the best possible ones for each particular use case. This is why frameworks which automatically test the performance of the generated code on a specific hardware configuration are of special interest. In this work, the integration between the code generation framework TVM and the systolic array-based accelerator Gemmini is presented. A generic schedule to offload the GEneral Matrix Multiply (GEMM) tensor operation onto Gemmini is detailed, and its suitability is tested by executing the AutoTVM tuning process on it. Our generated code achieves a peak throughput of 46 giga-operations per second (GOPs) under a 100 MHz clock on a Xilinx ZCU102 FPGA, outperforming previous work. Furthermore, the code generated by this integration was able to surpass the default hand-tuned schedules provided by the Gemmini developers in real-world workloads.

</details>

<details>

<summary>2022-12-06 15:15:00 - Towards a more efficient computation of individual attribute and policy contribution for post-hoc explanation of cooperative multi-agent systems using Myerson values</summary>

- *Giorgio Angelotti, Natalia DÃ­az-RodrÃ­guez*

- `2212.03041v1` - [abs](http://arxiv.org/abs/2212.03041v1) - [pdf](http://arxiv.org/pdf/2212.03041v1)

> A quantitative assessment of the global importance of an agent in a team is as valuable as gold for strategists, decision-makers, and sports coaches. Yet, retrieving this information is not trivial since in a cooperative task it is hard to isolate the performance of an individual from the one of the whole team. Moreover, it is not always clear the relationship between the role of an agent and his personal attributes. In this work we conceive an application of the Shapley analysis for studying the contribution of both agent policies and attributes, putting them on equal footing. Since the computational complexity is NP-hard and scales exponentially with the number of participants in a transferable utility coalitional game, we resort to exploiting a-priori knowledge about the rules of the game to constrain the relations between the participants over a graph. We hence propose a method to determine a Hierarchical Knowledge Graph of agents' policies and features in a Multi-Agent System. Assuming a simulator of the system is available, the graph structure allows to exploit dynamic programming to assess the importances in a much faster way. We test the proposed approach in a proof-of-case environment deploying both hardcoded policies and policies obtained via Deep Reinforcement Learning. The proposed paradigm is less computationally demanding than trivially computing the Shapley values and provides great insight not only into the importance of an agent in a team but also into the attributes needed to deploy the policy at its best.

</details>

<details>

<summary>2022-12-06 15:19:33 - Image-based Detection of Surface Defects in Concrete during Construction</summary>

- *Dominik Kuhnke, Monika Kwiatkowski, Olaf Hellwich*

- `2208.02313v2` - [abs](http://arxiv.org/abs/2208.02313v2) - [pdf](http://arxiv.org/pdf/2208.02313v2)

> Defects increase the cost and duration of construction projects as they require significant inspection and documentation efforts. Automating defect detection could significantly reduce these efforts. This work focuses on detecting honeycombs, a substantial defect in concrete structures that may affect structural integrity. We compared honeycomb images scraped from the web with images obtained from real construction inspections. We found that web images do not capture the complete variance found in real-case scenarios and that there is still a lack of data in this domain. Our dataset is therefore freely available for further research. A Mask R-CNN and EfficientNet-B0 were trained for honeycomb detection. The Mask R-CNN model allows detecting honeycombs based on instance segmentation, whereas the EfficientNet-B0 model allows a patch-based classification. Our experiments demonstrate that both approaches are suitable for solving and automating honeycomb detection. In the future, this solution can be incorporated into defect documentation systems.

</details>

<details>

<summary>2022-12-06 16:08:16 - Semantic-Conditional Diffusion Networks for Image Captioning</summary>

- *Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, Tao Mei*

- `2212.03099v1` - [abs](http://arxiv.org/abs/2212.03099v1) - [pdf](http://arxiv.org/pdf/2212.03099v1)

> Recent advances on text-to-image generation have witnessed the rise of diffusion models which act as powerful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the dependency among discrete words and meanwhile pursue complex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learning Transformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image captioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic information. The rich semantics are further regarded as semantic prior to trigger the learning of Diffusion Transformer, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer structures are stacked to progressively strengthen the output sentence with better visional-language alignment and linguistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical sequence training strategy is designed to guide the learning of SCD-Net with the knowledge of a standard autoregressive Transformer model. Extensive experiments on COCO dataset demonstrate the promising potential of using diffusion models in the challenging image captioning task. Source code is available at \url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet}.

</details>

<details>

<summary>2022-12-06 22:03:11 - Formal Modeling and Analysis of Legal Contracts using ContractCheck</summary>

- *Alan Khoja, Martin KÃ¶lbl, Stefan Leue, RÃ¼diger Wilhelmi*

- `2212.03349v1` - [abs](http://arxiv.org/abs/2212.03349v1) - [pdf](http://arxiv.org/pdf/2212.03349v1)

> We describe a method and tool called \textit{ContractCheck} that allows for the consistency analysis of legal contracts, in particular Sales Purchase Agreements (SPAs). The analysis relies on an encoding of the premises for the execution of the clauses of an SPA as well as the proposed consistency constraints using decidable fragments of first-order logic. Textual SPAs are first encoded in a structured natural language format, called blocks. \textit{ContractCheck} interprets these blocks and constraints and translates them in first-oder logic assertions. It then invokes a Satisfiability Modulo Theories (SMT) solver in order to establish the executability of a considered contract by either providing a satisfying model, or by providing evidence of contradictory clauses that impede the execution of the contract. We illustrate the application of \textit{ContractCheck} and conclude by proposing directions for future research.

</details>

<details>

<summary>2022-12-06 23:09:40 - Domain Translation via Latent Space Mapping</summary>

- *Tsiry Mayet, Simon Bernard, Clement Chatelain, Romain Herault*

- `2212.03361v1` - [abs](http://arxiv.org/abs/2212.03361v1) - [pdf](http://arxiv.org/pdf/2212.03361v1)

> In this paper, we investigate the problem of multi-domain translation: given an element $a$ of domain $A$, we would like to generate a corresponding $b$ sample in another domain $B$, and vice versa. Acquiring supervision in multiple domains can be a tedious task, also we propose to learn this translation from one domain to another when supervision is available as a pair $(a,b)\sim A\times B$ and leveraging possible unpaired data when only $a\sim A$ or only $b\sim B$ is available. We introduce a new unified framework called Latent Space Mapping (\model) that exploits the manifold assumption in order to learn, from each domain, a latent space. Unlike existing approaches, we propose to further regularize each latent space using available domains by learning each dependency between pairs of domains. We evaluate our approach in three tasks performing i) synthetic dataset with image translation, ii) real-world task of semantic segmentation for medical images, and iii) real-world task of facial landmark detection.

</details>

<details>

<summary>2022-12-07 01:46:28 - Utilizing Source Code Syntax Patterns to Detect Bug Inducing Commits using Machine Learning Models</summary>

- *Md Nadim, Banani Roy*

- `2212.03399v1` - [abs](http://arxiv.org/abs/2212.03399v1) - [pdf](http://arxiv.org/pdf/2212.03399v1)

> Detecting Bug Inducing Commit (BIC) or Just in Time (JIT) defect prediction using Machine Learning (ML) based models requires tabulated feature values extracted from the source code or historical maintenance data of a software system. Existing studies have utilized meta-data from source code repositories (we named them GitHub Statistics or GS), n-gram-based source code text processing, and developer's information (e.g., the experience of a developer) as the feature values in ML-based bug detection models. However, these feature values do not represent the source code syntax styles or patterns that a developer might prefer over available valid alternatives provided by programming languages. This investigation proposed a method to extract features from its source code syntax patterns to represent software commits and investigate whether they are helpful in detecting bug proneness in software systems. We utilize six manually and two automatically labeled datasets from eight open-source software projects written in Java, C++, and Python programming languages. Our datasets contain 642 manually labeled and 4,014 automatically labeled buggy and non-buggy commits from six and two subject systems, respectively. The subject systems contain a diverse number of revisions, and they are from various application domains. Our investigation shows the inclusion of the proposed features increases the performance of detecting buggy and non-buggy software commits using five different machine learning classification models. Our proposed features also perform better in detecting buggy commits using the Deep Belief Network generated features and classification model. This investigation also implemented a state-of-the-art tool to compare the explainability of predicted buggy commits using our proposed and traditional features and found that our proposed features provide better reasoning about buggy.....

</details>

<details>

<summary>2022-12-07 04:43:34 - Simulating Network Paths with Recurrent Buffering Units</summary>

- *Divyam Anshumaan, Sriram Balasubramanian, Shubham Tiwari, Nagarajan Natarajan, Sundararajan Sellamanickam, Venkata N. Padmanabhan*

- `2202.13870v3` - [abs](http://arxiv.org/abs/2202.13870v3) - [pdf](http://arxiv.org/pdf/2202.13870v3)

> Simulating physical network paths (e.g., Internet) is a cornerstone research problem in the emerging sub-field of AI-for-networking. We seek a model that generates end-to-end packet delay values in response to the time-varying load offered by a sender, which is typically a function of the previously output delays. The problem setting is unique, and renders the state-of-the-art text and time-series generative models inapplicable or ineffective. We formulate an ML problem at the intersection of dynamical systems, sequential decision making, and time-series modeling. We propose a novel grey-box approach to network simulation that embeds the semantics of physical network path in a new RNN-style model called RBU, providing the interpretability of standard network simulator tools, the power of neural models, the efficiency of SGD-based techniques for learning, and yielding promising results on synthetic and real-world network traces.

</details>

<details>

<summary>2022-12-07 04:49:42 - You Don't Know Search: Helping Users Find Code by Automatically Evaluating Alternative Queries</summary>

- *Rijnard van Tonder*

- `2212.03459v1` - [abs](http://arxiv.org/abs/2212.03459v1) - [pdf](http://arxiv.org/pdf/2212.03459v1)

> Tens of thousands of engineers use Sourcegraph day-to-day to search for code and rely on it to make progress on software development tasks. We face a key challenge in designing a query language that accommodates the needs of a broad spectrum of users. Our experience shows that users express different and often contradictory preferences for how queries should be interpreted. These preferences stem from users with differing usage contexts, technical experience, and implicit expectations from using prior tools. At the same time, designing a code search query language poses unique challenges because it intersects traditional search engines and full-fledged programming languages. For example, code search queries adopt certain syntactic conventions in the interest of simplicity and terseness but invariably risk encoding implicit semantics that are ambiguous at face-value (a single space in a query could mean three or more semantically different things depending on surrounding terms). Users often need to disambiguate intent with additional syntax so that a query expresses what they actually want to search. This need to disambiguate is one of the primary frustrations we've seen users experience with writing search queries in the last three years. We share our observations that lead us to a fresh perspective where code search behavior can straddle seemingly ambiguous queries. We develop Automated Query Evaluation (AQE), a new technique that automatically generates and adaptively runs alternative query interpretations in frustration-prone conditions. We evaluate AQE with an A/B test across more than 10,000 unique users on our publicly-available code search instance. Our main result shows that relative to the control group, users are on average 22% more likely to click on a search result at all on any given day when AQE is active.

</details>

<details>

<summary>2022-12-07 05:03:13 - PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with Noisy Labels</summary>

- *Huaxi Huang, Hui Kang, Sheng Liu, Olivier Salvado, Thierry Rakotoarivelo, Dadong Wang, Tongliang Liu*

- `2212.03462v1` - [abs](http://arxiv.org/abs/2212.03462v1) - [pdf](http://arxiv.org/pdf/2212.03462v1)

> Convolutional Neural Networks (CNNs) have demonstrated superiority in learning patterns, but are sensitive to label noises and may overfit noisy labels during training. The early stopping strategy averts updating CNNs during the early training phase and is widely employed in the presence of noisy labels. Motivated by biological findings that the amplitude spectrum (AS) and phase spectrum (PS) in the frequency domain play different roles in the animal's vision system, we observe that PS, which captures more semantic information, can increase the robustness of DNNs to label noise, more so than AS can. We thus propose early stops at different times for AS and PS by disentangling the features of some layer(s) into AS and PS using Discrete Fourier Transform (DFT) during training. Our proposed Phase-AmplituDe DisentangLed Early Stopping (PADDLES) method is shown to be effective on both synthetic and real-world label-noise datasets. PADDLES outperforms other early stopping methods and obtains state-of-the-art performance.

</details>

<details>

<summary>2022-12-07 09:02:01 - Jeopardy: An Invertible Functional Programming Language</summary>

- *Joachim Tilsted Kristensen, Robin Kaarsgaard, Michael Kirkedal Thomsen*

- `2209.02422v3` - [abs](http://arxiv.org/abs/2209.02422v3) - [pdf](http://arxiv.org/pdf/2209.02422v3)

> Algorithms are ways of mapping problems to solutions. An algorithm is invertible precisely when this mapping is injective, such that the initial problem can be uniquely inferred from its solution.   While invertible algorithms can be described in general-purpose languages, no guarantees are generally made by such languages as regards invertibility, so ensuring invertibility requires additional (and often non-trivial) proof. On the other hand, while reversible programming languages guarantee that their programs are invertible by restricting the permissible operations to those which are locally invertible, writing programs in the reversible style can be cumbersome, and may differ significantly from conventional implementations even when the implemented algorithm is, in fact, invertible.   In this paper we introduce Jeopardy, a functional programming language that guarantees program invertibility without imposing local reversibility. In particular, Jeopardy allows the limited use of uninvertible -- and even nondeterministic! -- operations, provided that they are used in a way that can be statically determined to be invertible. To this end, we outline an \emph{implicitly available arguments analysis} and three further approaches that can give a partial static guarantee to the (generally difficult) problem of guaranteeing invertibility.

</details>

<details>

<summary>2022-12-07 10:21:57 - Online AutoML: An adaptive AutoML framework for online learning</summary>

- *Bilge Celik, Prabhant Singh, Joaquin Vanschoren*

- `2201.09750v3` - [abs](http://arxiv.org/abs/2201.09750v3) - [pdf](http://arxiv.org/pdf/2201.09750v3)

> Automated Machine Learning (AutoML) has been used successfully in settings where the learning task is assumed to be static. In many real-world scenarios, however, the data distribution will evolve over time, and it is yet to be shown whether AutoML techniques can effectively design online pipelines in dynamic environments. This study aims to automate pipeline design for online learning while continuously adapting to data drift. For this purpose, we design an adaptive Online Automated Machine Learning (OAML) system, searching the complete pipeline configuration space of online learners, including preprocessing algorithms and ensembling techniques. This system combines the inherent adaptation capabilities of online learners with the fast automated pipeline (re)optimization capabilities of AutoML. Focusing on optimization techniques that can adapt to evolving objectives, we evaluate asynchronous genetic programming and asynchronous successive halving to optimize these pipelines continually. We experiment on real and artificial data streams with varying types of concept drift to test the performance and adaptation capabilities of the proposed system. The results confirm the utility of OAML over popular online learning algorithms and underscore the benefits of continuous pipeline redesign in the presence of data drift.

</details>

<details>

<summary>2022-12-07 13:28:51 - Semantically-enhanced Topic Recommendation System for Software Projects</summary>

- *Maliheh Izadi, Mahtab Nejati, Abbas Heydarnoori*

- `2206.00085v2` - [abs](http://arxiv.org/abs/2206.00085v2) - [pdf](http://arxiv.org/pdf/2206.00085v2)

> Software-related platforms have enabled their users to collaboratively label software entities with topics. Tagging software repositories with relevant topics can be exploited for facilitating various downstream tasks. For instance, a correct and complete set of topics assigned to a repository can increase its visibility. Consequently, this improves the outcome of tasks such as browsing, searching, navigation, and organization of repositories. Unfortunately, assigned topics are usually highly noisy, and some repositories do not have well-assigned topics. Thus, there have been efforts on recommending topics for software projects, however, the semantic relationships among these topics have not been exploited so far.   We propose two recommender models for tagging software projects that incorporate the semantic relationship among topics. Our approach has two main phases; (1) we first take a collaborative approach to curate a dataset of quality topics specifically for the domain of software engineering and development. We also enrich this data with the semantic relationships among these topics and encapsulate them in a knowledge graph we call SED-KGraph. Then, (2) we build two recommender systems; The first one operates only based on the list of original topics assigned to a repository and the relationships specified in our knowledge graph. The second predictive model, however, assumes there are no topics available for a repository, hence it proceeds to predict the relevant topics based on both textual information of a software project and SED-KGraph.   We built SED-KGraph in a crowd-sourced project with 170 contributors from both academia and industry. The experiment results indicate that our solutions outperform baselines that neglect the semantic relationships among topics by at least 25% and 23% in terms of ASR and MAP metrics.

</details>

<details>

<summary>2022-12-07 15:35:50 - Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues</summary>

- *Hung Le, Nancy F. Chen, Steven C. H. Hoi*

- `2103.00820v2` - [abs](http://arxiv.org/abs/2103.00820v2) - [pdf](http://arxiv.org/pdf/2103.00820v2)

> Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.

</details>

<details>

<summary>2022-12-07 16:22:37 - Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer</summary>

- *Hao Shao, Letian Wang, RuoBing Chen, Hongsheng Li, Yu Liu*

- `2207.14024v5` - [abs](http://arxiv.org/abs/2207.14024v5) - [pdf](http://arxiv.org/pdf/2207.14024v5)

> Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer(InterFuser), to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard. Our code will be made available at https://github.com/opendilab/InterFuser

</details>

<details>

<summary>2022-12-07 17:14:46 - Universe Points Representation Learning for Partial Multi-Graph Matching</summary>

- *Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard*

- `2212.00780v2` - [abs](http://arxiv.org/abs/2212.00780v2) - [pdf](http://arxiv.org/pdf/2212.00780v2)

> Many challenges from natural world can be formulated as a graph matching problem. Previous deep learning-based methods mainly consider a full two-graph matching setting. In this work, we study the more general partial matching problem with multi-graph cycle consistency guarantees. Building on a recent progress in deep learning on graphs, we propose a novel data-driven method (URL) for partial multi-graph matching, which uses an object-to-universe formulation and learns latent representations of abstract universe points. The proposed approach advances the state of the art in semantic keypoint matching problem, evaluated on Pascal VOC, CUB, and Willow datasets. Moreover, the set of controlled experiments on a synthetic graph matching dataset demonstrates the scalability of our method to graphs with large number of nodes and its robustness to high partiality.

</details>

<details>

<summary>2022-12-07 19:11:20 - PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers</summary>

- *Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu, Baining Guo*

- `2111.12710v3` - [abs](http://arxiv.org/abs/2111.12710v3) - [pdf](http://arxiv.org/pdf/2111.12710v3)

> This paper explores a better prediction target for BERT pre-training of vision transformers. We observe that current prediction targets disagree with human perception judgment.This contradiction motivates us to learn a perceptual prediction target. We argue that perceptually similar images should stay close to each other in the prediction target space. We surprisingly find one simple yet effective idea: enforcing perceptual similarity during the dVAE training. Moreover, we adopt a self-supervised transformer model for deep feature extraction and show that it works well for calculating perceptual similarity.We demonstrate that such learned visual tokens indeed exhibit better semantic meanings, and help pre-training achieve superior transfer performance in various downstream tasks. For example, we achieve $\textbf{84.5\%}$ Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive method BEiT by $\textbf{+1.3\%}$ under the same pre-training epochs. Our approach also gets significant improvement on object detection and segmentation on COCO and semantic segmentation on ADE20K. Equipped with a larger backbone ViT-H, we achieve the state-of-the-art ImageNet accuracy (\textbf{88.3\%}) among methods using only ImageNet-1K data.

</details>

<details>

<summary>2022-12-08 01:55:51 - Self-supervised Learning with Local Contrastive Loss for Detection and Semantic Segmentation</summary>

- *Ashraful Islam, Ben Lundell, Harpreet Sawhney, Sudipta Sinha, Peter Morales, Richard J. Radke*

- `2207.04398v2` - [abs](http://arxiv.org/abs/2207.04398v2) - [pdf](http://arxiv.org/pdf/2207.04398v2)

> We present a self-supervised learning (SSL) method suitable for semi-global tasks such as object detection and semantic segmentation. We enforce local consistency between self-learned features, representing corresponding image locations of transformed versions of the same image, by minimizing a pixel-level local contrastive (LC) loss during training. LC-loss can be added to existing self-supervised learning methods with minimal overhead. We evaluate our SSL approach on two downstream tasks -- object detection and semantic segmentation, using COCO, PASCAL VOC, and CityScapes datasets. Our method outperforms the existing state-of-the-art SSL approaches by 1.9% on COCO object detection, 1.4% on PASCAL VOC detection, and 0.6% on CityScapes segmentation.

</details>

<details>

<summary>2022-12-08 03:58:46 - Dual Convexified Convolutional Neural Networks</summary>

- *Site Bai, Chuyang Ke, Jean Honorio*

- `2205.14056v2` - [abs](http://arxiv.org/abs/2205.14056v2) - [pdf](http://arxiv.org/pdf/2205.14056v2)

> We propose the framework of dual convexified convolutional neural networks (DCCNNs). In this framework, we first introduce a primal learning problem motivated by convexified convolutional neural networks (CCNNs), and then construct the dual convex training program through careful analysis of the Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach reduces the computational overhead of constructing a large kernel matrix and more importantly, eliminates the ambiguity of factorizing the matrix. Due to the low-rank structure in CCNNs and the related subdifferential of nuclear norms, there is no closed-form expression to recover the primal solution from the dual solution. To overcome this, we propose a highly novel weight recovery algorithm, which takes the dual solution and the kernel information as the input, and recovers the linear weight and the output of convolutional layer, instead of weight parameter. Furthermore, our recovery algorithm exploits the low-rank structure and imposes a small number of filters indirectly, which reduces the parameter size. As a result, DCCNNs inherit all the statistical benefits of CCNNs, while enjoying a more formal and efficient workflow.

</details>

<details>

<summary>2022-12-08 06:18:44 - NP4G : Network Programming for Generalization</summary>

- *Shoichiro Hara, Yuji Watanabe*

- `2212.11118v1` - [abs](http://arxiv.org/abs/2212.11118v1) - [pdf](http://arxiv.org/pdf/2212.11118v1)

> Automatic programming has been actively studied for a long time by various approaches including genetic programming. In recent years, automatic programming using neural networks such as GPT-3 has been actively studied and is attracting a lot of attention. However, these methods are illogical inference based on experience by enormous learning, and their thinking process is unclear. Even using the method by logical inference with a clear thinking process, the system that automatically generates any programs has not yet been realized. Especially, the inductive inference generalized by logical inference from one example is an important issue that the artificial intelligence can acquire knowledge by itself. In this study, we propose NP4G: Network Programming for Generalization, which can automatically generate programs by inductive inference. Because the proposed method can realize "sequence", "selection", and "iteration" in programming and can satisfy the conditions of the structured program theorem, it is expected that NP4G is a method automatically acquire any programs by inductive inference. As an example, we automatically construct a bitwise NOT operation program from several training data by generalization using NP4G. Although NP4G only randomly selects and connects nodes, by adjusting the number of nodes and the number of phase of "Phased Learning", we show the bitwise NOT operation programs are acquired in a comparatively short time and at a rate of about 7 in 10 running. The source code of NP4G is available on GitHub as a public repository.

</details>

<details>

<summary>2022-12-08 06:24:08 - Generating and Weighting Semantically Consistent Sample Pairs for Ultrasound Contrastive Learning</summary>

- *Yixiong Chen, Chunhui Zhang, Chris H. Q. Ding, Li Liu*

- `2212.04097v1` - [abs](http://arxiv.org/abs/2212.04097v1) - [pdf](http://arxiv.org/pdf/2212.04097v1)

> Well-annotated medical datasets enable deep neural networks (DNNs) to gain strong power in extracting lesion-related features. Building such large and well-designed medical datasets is costly due to the need for high-level expertise. Model pre-training based on ImageNet is a common practice to gain better generalization when the data amount is limited. However, it suffers from the domain gap between natural and medical images. In this work, we pre-train DNNs on ultrasound (US) domains instead of ImageNet to reduce the domain gap in medical US applications. To learn US image representations based on unlabeled US videos, we propose a novel meta-learning-based contrastive learning method, namely Meta Ultrasound Contrastive Learning (Meta-USCL). To tackle the key challenge of obtaining semantically consistent sample pairs for contrastive learning, we present a positive pair generation module along with an automatic sample weighting module based on meta-learning. Experimental results on multiple computer-aided diagnosis (CAD) problems, including pneumonia detection, breast cancer classification, and breast tumor segmentation, show that the proposed self-supervised method reaches state-of-the-art (SOTA). The codes are available at https://github.com/Schuture/Meta-USCL.

</details>

<details>

<summary>2022-12-08 08:03:53 - SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to object-oriented SPIN rules</summary>

- *Nick Bassiliades*

- `1801.09061v4` - [abs](http://arxiv.org/abs/1801.09061v4) - [pdf](http://arxiv.org/pdf/1801.09061v4)

> Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language) ontologies with Horn Logic rules of the Rule Markup Language (RuleML) family. Being supported by ontology editors, rule engines and ontology reasoners, it has become a very popular choice for developing rule-based applications on top of ontologies. However, SWRL is probably not go-ing to become a WWW Consortium standard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL Inferencing Notation) has become a de-facto industry standard to rep-resent SPARQL rules and constraints on Semantic Web models, building on the widespread acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper, we ar-gue that the life of existing SWRL rule-based ontology applications can be prolonged by con-verting them to SPIN. To this end, we have developed the SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules, considering the object-orientation of SPIN, i.e. linking rules to the appropriate ontology classes and optimizing them, as derived by analysing the rule conditions.

</details>

<details>

<summary>2022-12-08 08:37:38 - Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning</summary>

- *Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu*

- `2105.03654v3` - [abs](http://arxiv.org/abs/2105.03654v3) - [pdf](http://arxiv.org/pdf/2105.03654v3)

> Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.

</details>

<details>

<summary>2022-12-08 09:06:22 - Measuring Context-Word Biases in Lexical Semantic Datasets</summary>

- *Qianchu Liu, Diana McCarthy, Anna Korhonen*

- `2112.06733v4` - [abs](http://arxiv.org/abs/2112.06733v4) - [pdf](http://arxiv.org/pdf/2112.06733v4)

> State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reflect how well a model represents the coupled word and context semantics. We question this assumption by presenting the first quantitative analysis on the context-word interaction being tested in major contextual lexical semantic tasks. To achieve this, we run probing baselines on masked input, and propose measures to calculate and visualize the degree of context or word biases in existing datasets. The analysis was performed on both models and humans. Our findings demonstrate that models are usually not being tested for word-in-context semantics in the same way as humans are in these tasks, which helps us better understand the model-human gap. Specifically, to PCMs, most existing datasets fall into the extreme ends (the retrieval-based tasks exhibit strong target word bias while WiC-style tasks and WSD show strong context bias); In comparison, humans are less biased and achieve much better performance when both word and context are available than with masked input. We recommend our framework for understanding and controlling these biases for model interpretation and future task design.

</details>

<details>

<summary>2022-12-08 10:16:39 - Automatically Transform Rust Source to Petri Nets for Checking Deadlocks</summary>

- *Kaiwen Zhang, Guanjun Liu*

- `2212.02754v2` - [abs](http://arxiv.org/abs/2212.02754v2) - [pdf](http://arxiv.org/pdf/2212.02754v2)

> This paper presents a method of automatically converting source codes (Rust programs) into Petri nets, focusing on the detection of deadlocks caused by the double locks and lock conflicts in the parallel Rust programs. We construct the transformation rules and develop a tool. Our method can omit those Rust codes without relations to locks when scanning the input codes, and thus tool can handle a large-scale code. We do a number of experiments to show the advantages of our method compared with the state-of-the-art ones.

</details>

<details>

<summary>2022-12-08 10:19:35 - Sound Verification of Security Protocols: From Design to Interoperable Implementations (extended version)</summary>

- *Linard Arquint, Felix A. Wolf, Joseph Lallemand, Ralf Sasse, Christoph Sprenger, Sven N. Wiesner, David Basin, Peter MÃ¼ller*

- `2212.04171v1` - [abs](http://arxiv.org/abs/2212.04171v1) - [pdf](http://arxiv.org/pdf/2212.04171v1)

> We provide a framework consisting of tools and metatheorems for the end-to-end verification of security protocols, which bridges the gap between automated protocol verification and code-level proofs. We automatically translate a Tamarin protocol model into a set of I/O specifications expressed in separation logic. Each such specification describes a protocol role's intended I/O behavior against which the role's implementation is then verified. Our soundness result guarantees that the verified implementation inherits all security (trace) properties proved for the Tamarin model. Our framework thus enables us to leverage the substantial body of prior verification work in Tamarin to verify new and existing implementations. The possibility to use any separation logic code verifier provides flexibility regarding the target language. To validate our approach and show that it scales to real-world protocols, we verify a substantial part of the official Go implementation of the WireGuard VPN key exchange protocol.

</details>

<details>

<summary>2022-12-08 11:23:48 - HyperEnclave: An Open and Cross-platform Trusted Execution Environment</summary>

- *Yuekai Jia, Shuang Liu, Wenhao Wang, Yu Chen, Zhengde Zhai, Shoumeng Yan, Zhengyu He*

- `2212.04197v1` - [abs](http://arxiv.org/abs/2212.04197v1) - [pdf](http://arxiv.org/pdf/2212.04197v1)

> A number of trusted execution environments (TEEs) have been proposed by both academia and industry. However, most of them require specific hardware or firmware changes and are bound to specific hardware vendors (such as Intel, AMD, ARM, and IBM). In this paper, we propose HyperEnclave, an open and cross-platform process-based TEE that relies on the widely-available virtualization extension to create the isolated execution environment. In particular, HyperEnclave is designed to support the flexible enclave operation modes to fulfill the security and performance demands under various enclave workloads. We provide the enclave SDK to run existing SGX programs on HyperEnclave with little or no source code changes. We have implemented HyperEnclave on commodity AMD servers and deployed the system in a world-leading FinTech company to support real-world privacy-preserving computations. The evaluation on both micro-benchmarks and application benchmarks shows the design of HyperEnclave introduces only a small overhead.

</details>

<details>

<summary>2022-12-08 12:26:15 - ICSPatch: Automated Vulnerability Localization and Non-Intrusive Hotpatching in Industrial Control Systems using Data Dependence Graphs</summary>

- *Prashant Hari Narayan Rajput, Constantine Doumanidis, Michail Maniatakos*

- `2212.04229v1` - [abs](http://arxiv.org/abs/2212.04229v1) - [pdf](http://arxiv.org/pdf/2212.04229v1)

> The paradigm shift of enabling extensive intercommunication between the Operational Technology (OT) and Information Technology (IT) devices allows vulnerabilities typical to the IT world to propagate to the OT side. Therefore, the security layer offered in the past by air gapping is removed, making security patching for OT devices a hard requirement. Conventional patching involves a device reboot to load the patched code in the main memory, which does not apply to OT devices controlling critical processes due to downtime, necessitating in-memory vulnerability patching. Furthermore, these control binaries are often compiled by in-house proprietary compilers, further hindering the patching process and placing reliance on OT vendors for rapid vulnerability discovery and patch development. The current state-of-the-art hotpatching approaches only focus on firmware and/or RTOS. Therefore, in this work, we develop ICSPatch, a framework to automate control logic vulnerability localization using Data Dependence Graphs (DDGs). With the help of DDGs, ICSPatch pinpoints the vulnerability in the control application. As an independent second step, ICSPatch can non-intrusively hotpatch vulnerabilities in the control application directly in the main memory of Programmable Logic Controllers while maintaining reliable continuous operation. To evaluate our framework, we test ICSPatch on a synthetic dataset of 24 vulnerable control application binaries from diverse critical infrastructure sectors. Results show that ICSPatch could successfully localize all vulnerabilities and generate patches accordingly. Furthermore, the patch added negligible latency increase in the execution cycle while maintaining correctness and protection against the vulnerability.

</details>

<details>

<summary>2022-12-08 13:27:37 - ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation</summary>

- *Zhaocong Li, Xuebo Liu, Derek F. Wong, Lidia S. Chao, Min Zhang*

- `2212.04262v1` - [abs](http://arxiv.org/abs/2212.04262v1) - [pdf](http://arxiv.org/pdf/2212.04262v1)

> Transfer learning is a simple and powerful method that can be used to boost model performance of low-resource neural machine translation (NMT). Existing transfer learning methods for NMT are static, which simply transfer knowledge from a parent model to a child model once via parameter initialization. In this paper, we propose a novel transfer learning method for NMT, namely ConsistTL, which can continuously transfer knowledge from the parent model during the training of the child model. Specifically, for each training instance of the child model, ConsistTL constructs the semantically-equivalent instance for the parent model and encourages prediction consistency between the parent and child for this instance, which is equivalent to the child model learning each instance under the guidance of the parent model. Experimental results on five low-resource NMT tasks demonstrate that ConsistTL results in significant improvements over strong transfer learning baselines, with a gain up to 1.7 BLEU over the existing back-translation model on the widely-used WMT17 Turkish-English benchmark. Further analysis reveals that ConsistTL can improve the inference calibration of the child model. Code and scripts are freely available at https://github.com/NLP2CT/ConsistTL.

</details>

<details>

<summary>2022-12-08 16:06:13 - BiOcularGAN: Bimodal Synthesis and Annotation of Ocular Images</summary>

- *Darian TomaÅ¡eviÄ, Peter Peer, Vitomir Å truc*

- `2205.01536v3` - [abs](http://arxiv.org/abs/2205.01536v3) - [pdf](http://arxiv.org/pdf/2205.01536v3)

> Current state-of-the-art segmentation techniques for ocular images are critically dependent on large-scale annotated datasets, which are labor-intensive to gather and often raise privacy concerns. In this paper, we present a novel framework, called BiOcularGAN, capable of generating synthetic large-scale datasets of photorealistic (visible light and near-infrared) ocular images, together with corresponding segmentation labels to address these issues. At its core, the framework relies on a novel Dual-Branch StyleGAN2 (DB-StyleGAN2) model that facilitates bimodal image generation, and a Semantic Mask Generator (SMG) component that produces semantic annotations by exploiting latent features of the DB-StyleGAN2 model. We evaluate BiOcularGAN through extensive experiments across five diverse ocular datasets and analyze the effects of bimodal data generation on image quality and the produced annotations. Our experimental results show that BiOcularGAN is able to produce high-quality matching bimodal images and annotations (with minimal manual intervention) that can be used to train highly competitive (deep) segmentation models (in a privacy aware-manner) that perform well across multiple real-world datasets. The source code for the BiOcularGAN framework is publicly available at https://github.com/dariant/BiOcularGAN.

</details>

<details>

<summary>2022-12-08 18:38:46 - What the DAAM: Interpreting Stable Diffusion Using Cross Attention</summary>

- *Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, Ferhan Ture*

- `2210.04885v5` - [abs](http://arxiv.org/abs/2210.04885v5) - [pdf](http://arxiv.org/pdf/2210.04885v5)

> Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce pixel-level attribution maps, we upscale and aggregate cross-attention word-pixel scores in the denoising subnetwork, naming our method DAAM. We evaluate its correctness by testing its semantic segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. We then apply DAAM to study the role of syntax in the pixel space, characterizing head--dependent heat map interaction patterns for ten common dependency relations. Finally, we study several semantic phenomena using DAAM, with a focus on feature entanglement, where we find that cohyponyms worsen generation quality and descriptive adjectives attend too broadly. To our knowledge, we are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future lines of research. Our code is at https://github.com/castorini/daam.

</details>

<details>

<summary>2022-12-08 18:52:27 - Phone2Proc: Bringing Robust Robots Into Our Chaotic World</summary>

- *Matt Deitke, Rose Hendrix, Luca Weihs, Ali Farhadi, Kiana Ehsani, Aniruddha Kembhavi*

- `2212.04819v1` - [abs](http://arxiv.org/abs/2212.04819v1) - [pdf](http://arxiv.org/pdf/2212.04819v1)

> Training embodied agents in simulation has become mainstream for the embodied AI community. However, these agents often struggle when deployed in the physical world due to their inability to generalize to real-world environments. In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan and conditional procedural generation to create a distribution of training scenes that are semantically similar to the target environment. The generated scenes are conditioned on the wall layout and arrangement of large objects from the scan, while also sampling lighting, clutter, surface textures, and instances of smaller objects with randomized placement and materials. Leveraging just a simple RGB camera, training with Phone2Proc shows massive improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav performance across a test suite of over 200 trials in diverse real-world environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's diverse distribution of generated scenes makes agents remarkably robust to changes in the real world, such as human movement, object rearrangement, lighting changes, or clutter.

</details>

<details>

<summary>2022-12-08 18:57:13 - SINE: SINgle Image Editing with Text-to-Image Diffusion Models</summary>

- *Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren*

- `2212.04489v1` - [abs](http://arxiv.org/abs/2212.04489v1) - [pdf](http://arxiv.org/pdf/2212.04489v1)

> Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem--real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation. The code is available for research purposes at https://github.com/zhang-zx/SINE.git .

</details>

<details>

<summary>2022-12-08 20:50:08 - Optimal binning: mathematical programming formulation</summary>

- *Guillermo Navas-Palencia*

- `2001.08025v3` - [abs](http://arxiv.org/abs/2001.08025v3) - [pdf](http://arxiv.org/pdf/2001.08025v3)

> The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target. We present a rigorous and extensible mathematical programming formulation for solving the optimal binning problem for a binary, continuous and multi-class target type, incorporating constraints not previously addressed. For all three target types, we introduce a convex mixed-integer programming formulation. Several algorithmic enhancements, such as automatic determination of the most suitable monotonic trend via a Machine-Learning-based classifier and implementation aspects are thoughtfully discussed. The new mathematical programming formulations are carefully implemented in the open-source python library OptBinning.

</details>

<details>

<summary>2022-12-08 23:24:35 - Traditional Classification Neural Networks are Good Generators: They are Competitive with DDPMs and GANs</summary>

- *Guangrun Wang, Philip H. S. Torr*

- `2211.14794v2` - [abs](http://arxiv.org/abs/2211.14794v2) - [pdf](http://arxiv.org/pdf/2211.14794v2)

> Classifiers and generators have long been separated. We break down this separation and showcase that conventional neural network classifiers can generate high-quality images of a large number of categories, being comparable to the state-of-the-art generative models (e.g., DDPMs and GANs). We achieve this by computing the partial derivative of the classification loss function with respect to the input to optimize the input to produce an image. Since it is widely known that directly optimizing the inputs is similar to targeted adversarial attacks incapable of generating human-meaningful images, we propose a mask-based stochastic reconstruction module to make the gradients semantic-aware to synthesize plausible images. We further propose a progressive-resolution technique to guarantee fidelity, which produces photorealistic images. Furthermore, we introduce a distance metric loss and a non-trivial distribution loss to ensure classification neural networks can synthesize diverse and high-fidelity images. Using traditional neural network classifiers, we can generate good-quality images of 256$\times$256 resolution on ImageNet. Intriguingly, our method is also applicable to text-to-image generation by regarding image-text foundation models as generalized classifiers.   Proving that classifiers have learned the data distribution and are ready for image generation has far-reaching implications, for classifiers are much easier to train than generative models like DDPMs and GANs. We don't even need to train classification models because tons of public ones are available for download. Also, this holds great potential for the interpretability and robustness of classifiers. Project page is at \url{https://classifier-as-generator.github.io/}.

</details>

<details>

<summary>2022-12-08 23:48:00 - The R-algebra of Quasiknowledge and Convex Optimization</summary>

- *Duyal Yolcu*

- `2212.04606v1` - [abs](http://arxiv.org/abs/2212.04606v1) - [pdf](http://arxiv.org/pdf/2212.04606v1)

> This article develops a convex description of a classical or quantum learner's or agent's state of knowledge about its environment, presented as a convex subset of a commutative R-algebra. With caveats, this leads to a generalization of certain semidefinite programs in quantum information (such as those describing the universal query algorithm dual to the quantum adversary bound, related to optimal learning or control of the environment) to the classical and faulty-quantum setting, which would not be possible with a naive description via joint probability distributions over environment and internal memory. More philosophically, it also makes an interpretation of the set of reduced density matrices as "states of knowledge" of an observer of its environment, related to these techniques, more explicit. As another example, I describe and solve a formal differential equation of states of knowledge in that algebra, where an agent obtains experimental data in a Poissonian process, and its state of knowledge evolves as an exponential power series. However, this framework currently lacks impressive applications, and I post it in part to solicit feedback and collaboration on those. In particular, it may be possible to develop it into a new framework for the design of experiments, e.g. the problem of finding maximally informative questions to ask human labelers or the environment in machine-learning problems. The parts of the article not related to quantum information don't assume knowledge of it.

</details>

<details>

<summary>2022-12-09 03:52:56 - Discrete Event Simulation for Port Berth Maintenance Planning</summary>

- *Ruqayah Alsayed Ebrahim, Shivanan Singh, Yitong Li, Wenying Ji*

- `2212.04654v1` - [abs](http://arxiv.org/abs/2212.04654v1) - [pdf](http://arxiv.org/pdf/2212.04654v1)

> Industrial and commercial ports, which are one of the three main hubs to the country, require 24/7 operations to maintain the goods export and import flow. Due to the aging and weather factors, berths require regular maintenance, such as replacing old piles, timber finders, marine ladders, rubber fenders, and deck slabs. For efficient berth maintenance, strategies are highly desired to minimize or eliminate any delays in operations during the maintenance. This paper develops a discrete event simulation model using Simphony.NET for berth maintenance processes in Doha Port, Kuwait. The model derives minimum maintenance duration under limited resources and associated uncertainties. The model can be used as a decision support tool to minimize interruption or delays in the port maintenance operations.

</details>

<details>

<summary>2022-12-09 04:32:47 - Learning to Transpile AMR into SPARQL</summary>

- *Mihaela Bornea, Ramon Fernandez Astudillo, Tahira Naseem, Nandana Mihindukulasooriya, Ibrahim Abdelaziz, Pavan Kapanipathi, Radu Florian, Salim Roukos*

- `2112.07877v2` - [abs](http://arxiv.org/abs/2112.07877v2) - [pdf](http://arxiv.org/pdf/2112.07877v2)

> We propose a transition-based system to transpile Abstract Meaning Representation (AMR) into SPARQL for Knowledge Base Question Answering (KBQA). This allows us to delegate part of the semantic representation to a strongly pre-trained semantic parser, while learning transpiling with small amount of paired data. We depart from recent work relating AMR and SPARQL constructs, but rather than applying a set of rules, we teach a BART model to selectively use these relations. Further, we avoid explicitly encoding AMR but rather encode the parser state in the attention mechanism of BART, following recent semantic parsing works. The resulting model is simple, provides supporting text for its decisions, and outperforms recent approaches in KBQA across two knowledge bases: DBPedia (LC-QuAD 1.0, QALD-9) and Wikidata (WebQSP, SWQ-WD).

</details>

<details>

<summary>2022-12-09 05:16:53 - Machine learning algorithms for three-dimensional mean-curvature computation in the level-set method</summary>

- *Luis Ãngel Larios-CÃ¡rdenas, FrÃ©dÃ©ric Gibou*

- `2208.09047v3` - [abs](http://arxiv.org/abs/2208.09047v3) - [pdf](http://arxiv.org/pdf/2208.09047v3)

> We propose a data-driven mean-curvature solver for the level-set method. This work is the natural extension to $\mathbb{R}^3$ of our two-dimensional strategy in [DOI: 10.1007/s10915-022-01952-2][1] and the hybrid inference system of [DOI: 10.1016/j.jcp.2022.111291][2]. However, in contrast to [1,2], which built resolution-dependent neural-network dictionaries, here we develop a pair of models in $\mathbb{R}^3$, regardless of the mesh size. Our feedforward networks ingest transformed level-set, gradient, and curvature data to fix numerical mean-curvature approximations selectively for interface nodes. To reduce the problem's complexity, we have used the Gaussian curvature to classify stencils and fit our models separately to non-saddle and saddle patterns. Non-saddle stencils are easier to handle because they exhibit a curvature error distribution characterized by monotonicity and symmetry. While the latter has allowed us to train only on half the mean-curvature spectrum, the former has helped us blend the data-driven and the baseline estimations seamlessly near flat regions. On the other hand, the saddle-pattern error structure is less clear; thus, we have exploited no latent information beyond what is known. In this regard, we have trained our models on not only spherical but also sinusoidal and hyperbolic paraboloidal patches. Our approach to building their data sets is systematic but gleans samples randomly while ensuring well-balancedness. We have also resorted to standardization and dimensionality reduction and integrated regularization to minimize outliers. In addition, we leverage curvature rotation/reflection invariance to improve precision at inference time. Several experiments confirm that our proposed system can yield more accurate mean-curvature estimations than modern particle-based interface reconstruction and level-set schemes around under-resolved regions.

</details>

<details>

<summary>2022-12-09 07:00:31 - Representing LLVM-IR in a Code Property Graph</summary>

- *Alexander KÃ¼chler, Christian Banse*

- `2211.05627v2` - [abs](http://arxiv.org/abs/2211.05627v2) - [pdf](http://arxiv.org/pdf/2211.05627v2)

> In the past years, a number of static application security testing tools have been proposed which make use of so-called code property graphs, a graph model which keeps rich information about the source code while enabling its user to write language-agnostic analyses. However, they suffer from several shortcomings. They work mostly on source code and exclude the analysis of third-party dependencies if they are only available as compiled binaries. Furthermore, they are limited in their analysis to whether an individual programming language is supported or not. While often support for well-established languages such as C/C++ or Java is included, languages that are still heavily evolving, such as Rust, are not considered because of the constant changes in the language design. To overcome these limitations, we extend an open source implementation of a code property graph to support LLVM-IR which can be used as output by many compilers and binary lifters. In this paper, we discuss how we address challenges that arise when mapping concepts of an intermediate representation to a CPG. At the same time, we optimize the resulting graph to be minimal and close to the representation of equivalent source code. Our evaluation indicates that existing analyses can be reused without modifications and that the performance requirements are comparable to operating on source code. This makes the approach suitable for an analysis of large-scale projects.

</details>

<details>

<summary>2022-12-09 09:00:38 - Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing</summary>

- *Zhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu, Qing Wang*

- `2212.04732v1` - [abs](http://arxiv.org/abs/2212.04732v1) - [pdf](http://arxiv.org/pdf/2212.04732v1)

> Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.

</details>

<details>

<summary>2022-12-09 09:10:19 - MED-SE: Medical Entity Definition-based Sentence Embedding</summary>

- *Hyeonbin Hwang, Haanju Yoo, Yera Choi*

- `2212.04734v1` - [abs](http://arxiv.org/abs/2212.04734v1) - [pdf](http://arxiv.org/pdf/2212.04734v1)

> We propose Medical Entity Definition-based Sentence Embedding (MED-SE), a novel unsupervised contrastive learning framework designed for clinical texts, which exploits the definitions of medical entities. To this end, we conduct an extensive analysis of multiple sentence embedding techniques in clinical semantic textual similarity (STS) settings. In the entity-centric setting that we have designed, MED-SE achieves significantly better performance, while the existing unsupervised methods including SimCSE show degraded performance. Our experiments elucidate the inherent discrepancies between the general- and clinical-domain texts, and suggest that entity-centric contrastive approaches may help bridge this gap and lead to a better representation of clinical sentences.

</details>

<details>

<summary>2022-12-09 09:42:26 - Weakly Supervised Semantic Segmentation for Large-Scale Point Cloud</summary>

- *Yachao Zhang, Zonghao Li, Yuan Xie, Yanyun Qu, Cuihua Li, Tao Mei*

- `2212.04744v1` - [abs](http://arxiv.org/abs/2212.04744v1) - [pdf](http://arxiv.org/pdf/2212.04744v1)

> Existing methods for large-scale point cloud semantic segmentation require expensive, tedious and error-prone manual point-wise annotations. Intuitively, weakly supervised training is a direct solution to reduce the cost of labeling. However, for weakly supervised large-scale point cloud semantic segmentation, too few annotations will inevitably lead to ineffective learning of network. We propose an effective weakly supervised method containing two components to solve the above problem. Firstly, we construct a pretext task, \textit{i.e.,} point cloud colorization, with a self-supervised learning to transfer the learned prior knowledge from a large amount of unlabeled point cloud to a weakly supervised network. In this way, the representation capability of the weakly supervised network can be improved by the guidance from a heterogeneous task. Besides, to generate pseudo label for unlabeled data, a sparse label propagation mechanism is proposed with the help of generated class prototypes, which is used to measure the classification confidence of unlabeled point. Our method is evaluated on large-scale point cloud datasets with different scenarios including indoor and outdoor. The experimental results show the large gain against existing weakly supervised and comparable results to fully supervised methods\footnote{Code based on mindspore: https://github.com/dmcv-ecnu/MindSpore\_ModelZoo/tree/main/WS3\_MindSpore}.

</details>

<details>

<summary>2022-12-09 13:35:20 - Closed pattern mining of interval data and distributional data</summary>

- *Henry Soldano, Guillaume Santini, Stella Zevio*

- `2212.04849v1` - [abs](http://arxiv.org/abs/2212.04849v1) - [pdf](http://arxiv.org/pdf/2212.04849v1)

> We discuss pattern languages for closed pattern mining and learning of interval data and distributional data. We first introduce pattern languages relying on pairs of intersection-based constraints or pairs of inclusion based constraints, or both, applied to intervals. We discuss the encoding of such interval patterns as itemsets thus allowing to use closed itemsets mining and formal concept analysis programs. We experiment these languages on clustering and supervised learning tasks. Then we show how to extend the approach to address distributional data.

</details>

<details>

<summary>2022-12-09 14:10:54 - A Comparative Performance Analysis of Explainable Machine Learning Models With And Without RFECV Feature Selection Technique Towards Ransomware Classification</summary>

- *Rawshan Ara Mowri, Madhuri Siddula, Kaushik Roy*

- `2212.04864v1` - [abs](http://arxiv.org/abs/2212.04864v1) - [pdf](http://arxiv.org/pdf/2212.04864v1)

> Ransomware has emerged as one of the major global threats in recent days. The alarming increasing rate of ransomware attacks and new ransomware variants intrigue the researchers in this domain to constantly examine the distinguishing traits of ransomware and refine their detection or classification strategies. Among the broad range of different behavioral characteristics, the trait of Application Programming Interface (API) calls and network behaviors have been widely utilized as differentiating factors for ransomware detection, or classification. Although many of the prior approaches have shown promising results in detecting and classifying ransomware families utilizing these features without applying any feature selection techniques, feature selection, however, is one of the potential steps toward an efficient detection or classification Machine Learning model because it reduces the probability of overfitting by removing redundant data, improves the model's accuracy by eliminating irrelevant features, and therefore reduces training time. There have been a good number of feature selection techniques to date that are being used in different security scenarios to optimize the performance of the Machine Learning models. Hence, the aim of this study is to present the comparative performance analysis of widely utilized Supervised Machine Learning models with and without RFECV feature selection technique towards ransomware classification utilizing the API call and network traffic features. Thereby, this study provides insight into the efficiency of the RFECV feature selection technique in the case of ransomware classification which can be used by peers as a reference for future work in choosing the feature selection technique in this domain.

</details>

<details>

<summary>2022-12-09 14:25:47 - A Model Driven Approach on Object Oriented PLC Programming for Manufacturing Systems with regard to Usability</summary>

- *Martin Obermeier, Steven Braun, Birgit Vogel-Heuser*

- `2212.09475v1` - [abs](http://arxiv.org/abs/2212.09475v1) - [pdf](http://arxiv.org/pdf/2212.09475v1)

> This paper presents the modular automation for reuse in manufacturing systems (modAT4rMS) approach to support the model-driven engineering (MDE) of object oriented manufacturing automation software with regard to its usability and software modularity. With usability we refer to the aspects effectiveness, efficiency and user acceptance, as defined by ISO 9241-11. The modAT4rMS notations are based on selected features from the Unified Modeling Language (UML) and the Systems Modeling language (SysML) and iteratively further developed by a series of empirical studies with industrial practitioners as well as mechatronics trainees. With modAT4rMS a MDE approach for Programmable Logic Controller (PLC) programming was developed with the goal to facilitate modular object oriented programming of PLC software by improving the representation of the relationships between the structure and behavior diagram types and by reducing the level of abstraction in the structure model. modAT4rMS notations for PLC software structure and software behavior modeling are presented and illustrated with a modeling example using a modAT4rMS editor prototype. For the evaluation of the developed notations the results from a study with 168 participants is presented, showing the benefits of this new approach in comparison to the classic procedural paradigm (IEC 61131-3) and the domain specific UML profile plcML in regard to programming performance and usability aspects. Finally the advantages and limitations of the approach are discussed and an outlook for further development is given.

</details>

<details>

<summary>2022-12-09 14:51:12 - HieNet: Bidirectional Hierarchy Framework for Automated ICD Coding</summary>

- *Shi Wang, Daniel Tang, Luchen Zhang, Huilin Li, Ding Han*

- `2212.04891v1` - [abs](http://arxiv.org/abs/2212.04891v1) - [pdf](http://arxiv.org/pdf/2212.04891v1)

> International Classification of Diseases (ICD) is a set of classification codes for medical records. Automated ICD coding, which assigns unique International Classification of Diseases codes with each medical record, is widely used recently for its efficiency and error-prone avoidance. However, there are challenges that remain such as heterogeneity, label unbalance, and complex relationships between ICD codes. In this work, we proposed a novel Bidirectional Hierarchy Framework(HieNet) to address the challenges. Specifically, a personalized PageRank routine is developed to capture the co-relation of codes, a bidirectional hierarchy passage encoder to capture the codes' hierarchical representations, and a progressive predicting method is then proposed to narrow down the semantic searching space of prediction. We validate our method on two widely used datasets. Experimental results on two authoritative public datasets demonstrate that our proposed method boosts state-of-the-art performance by a large margin.

</details>

<details>

<summary>2022-12-09 15:17:35 - CKG: Dynamic Representation Based on Context and Knowledge Graph</summary>

- *Xunzhu Tang, Tiezhu Sun, Rujie Zhu, Shi Wang*

- `2212.04909v1` - [abs](http://arxiv.org/abs/2212.04909v1) - [pdf](http://arxiv.org/pdf/2212.04909v1)

> Recently, neural language representation models pre-trained on large corpus can capture rich co-occurrence information and be fine-tuned in downstream tasks to improve the performance. As a result, they have achieved state-of-the-art results in a large range of language tasks. However, there exists other valuable semantic information such as similar, opposite, or other possible meanings in external knowledge graphs (KGs). We argue that entities in KGs could be used to enhance the correct semantic meaning of language sentences. In this paper, we propose a new method CKG: Dynamic Representation Based on \textbf{C}ontext and \textbf{K}nowledge \textbf{G}raph. On the one side, CKG can extract rich semantic information of large corpus. On the other side, it can make full use of inside information such as co-occurrence in large corpus and outside information such as similar entities in KGs. We conduct extensive experiments on a wide range of tasks, including QQP, MRPC, SST-5, SQuAD, CoNLL 2003, and SNLI. The experiment results show that CKG achieves SOTA 89.2 on SQuAD compared with SAN (84.4), ELMo (85.8), and BERT$_{Base}$ (88.5).

</details>

<details>

<summary>2022-12-09 15:45:57 - Moto: Enhancing Embedding with Multiple Joint Factors for Chinese Text Classification</summary>

- *Xunzhu Tang, Rujie Zhu, Tiezhu Sun, Shi Wang*

- `2212.08105v1` - [abs](http://arxiv.org/abs/2212.08105v1) - [pdf](http://arxiv.org/pdf/2212.08105v1)

> Recently, language representation techniques have achieved great performances in text classification. However, most existing representation models are specifically designed for English materials, which may fail in Chinese because of the huge difference between these two languages. Actually, few existing methods for Chinese text classification process texts at a single level. However, as a special kind of hieroglyphics, radicals of Chinese characters are good semantic carriers. In addition, Pinyin codes carry the semantic of tones, and Wubi reflects the stroke structure information, \textit{etc}. Unfortunately, previous researches neglected to find an effective way to distill the useful parts of these four factors and to fuse them. In our works, we propose a novel model called Moto: Enhancing Embedding with \textbf{M}ultiple J\textbf{o}int Fac\textbf{to}rs. Specifically, we design an attention mechanism to distill the useful parts by fusing the four-level information above more effectively. We conduct extensive experiments on four popular tasks. The empirical results show that our Moto achieves SOTA 0.8316 ($F_1$-score, 2.11\% improvement) on Chinese news titles, 96.38 (1.24\% improvement) on Fudan Corpus and 0.9633 (3.26\% improvement) on THUCNews.

</details>

<details>

<summary>2022-12-09 16:32:46 - Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers</summary>

- *Yasheng Sun, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Zhibin Hong, Jingtuo Liu, Errui Ding, Jingdong Wang, Ziwei Liu, Hideki Koike*

- `2212.04970v1` - [abs](http://arxiv.org/abs/2212.04970v1) - [pdf](http://arxiv.org/pdf/2212.04970v1)

> Previous studies have explored generating accurately lip-synced talking faces for arbitrary targets given audio conditions. However, most of them deform or generate the whole facial area, leading to non-realistic results. In this work, we delve into the formulation of altering only the mouth shapes of the target person. This requires masking a large percentage of the original image and seamlessly inpainting it with the aid of audio and reference frames. To this end, we propose the Audio-Visual Context-Aware Transformer (AV-CAT) framework, which produces accurate lip-sync with photo-realistic quality by predicting the masked mouth shapes. Our key insight is to exploit desired contextual information provided in audio and visual modalities thoroughly with delicately designed Transformers. Specifically, we propose a convolution-Transformer hybrid backbone and design an attention-based fusion strategy for filling the masked parts. It uniformly attends to the textural information on the unmasked regions and the reference frame. Then the semantic audio information is involved in enhancing the self-attention computation. Additionally, a refinement network with audio injection improves both image and lip-sync quality. Extensive experiments validate that our model can generate high-fidelity lip-synced results for arbitrary subjects.

</details>

<details>

<summary>2022-12-09 18:32:18 - Sharing Linkable Learning Objects with the use of Metadata and a Taxonomy Assistant for Categorization</summary>

- *Valentina Franzoni, Sergio Tasso, Simonetta Pallottelli, Damiano Perri*

- `2212.05947v1` - [abs](http://arxiv.org/abs/2212.05947v1) - [pdf](http://arxiv.org/pdf/2212.05947v1)

> In this work, a re-design of the Moodledata module functionalities is presented to share learning objects between e-learning content platforms, e.g., Moodle and G-Lorep, in a linkable object format. The e-learning courses content of the Drupal-based Content Management System G-Lorep for academic learning is exchanged designing an object incorporating metadata to support the reuse and the classification in its context. In such an Artificial Intelligence environment, the exchange of Linkable Learning Objects can be used for dialogue between Learning Systems to obtain information, especially with the use of semantic or structural similarity measures to enhance the existent Taxonomy Assistant for advanced automated classification.

</details>

<details>

<summary>2022-12-09 19:29:32 - Regionalized models for Spanish language variations based on Twitter</summary>

- *Eric S. Tellez, Daniela Moctezuma, Sabino Miranda, Mario Graff, Guillermo Ruiz*

- `2110.06128v3` - [abs](http://arxiv.org/abs/2110.06128v3) - [pdf](http://arxiv.org/pdf/2110.06128v3)

> Spanish is one of the most spoken languages in the globe, but not necessarily Spanish is written and spoken in the same way in different countries. Understanding local language variations can help to improve model performances on regional tasks, both understanding local structures and also improving the message's content. For instance, think about a machine learning engineer who automatizes some language classification task on a particular region or a social scientist trying to understand a regional event with echoes on social media; both can take advantage of dialect-based language models to understand what is happening with more contextual information hence more precision.   This manuscript presents and describes a set of regionalized resources for the Spanish language built on four-year Twitter public messages geotagged in 26 Spanish-speaking countries. We introduce word embeddings based on FastText, language models based on BERT, and per-region sample corpora. We also provide a broad comparison among regions covering lexical and semantical similarities; as well as examples of using regional resources on message classification tasks.

</details>

<details>

<summary>2022-12-09 20:37:44 - Automatically Generating CS Learning Materials with Large Language Models</summary>

- *Stephen MacNeil, Andrew Tran, Juho Leinonen, Paul Denny, Joanne Kim, Arto Hellas, Seth Bernstein, Sami Sarsa*

- `2212.05113v1` - [abs](http://arxiv.org/abs/2212.05113v1) - [pdf](http://arxiv.org/pdf/2212.05113v1)

> Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and Codex, now enable software developers to generate code based on a natural language prompt. Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafted prompts. These advances may enable students to interact with code in new ways while helping instructors scale their learning materials. However, LLMs also introduce new implications for academic integrity, curriculum design, and software engineering careers. This workshop will demonstrate the capabilities of LLMs to help attendees evaluate whether and how LLMs might be integrated into their pedagogy and research. We will also engage attendees in brainstorming to consider how LLMs will impact our field.

</details>

<details>

<summary>2022-12-09 22:10:02 - Fault-Aware Neural Code Rankers</summary>

- *Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark EncarnaciÃ³n, Shuvendu K Lahiri, Madanlal Musuvathi, Jianfeng Gao*

- `2206.03865v2` - [abs](http://arxiv.org/abs/2206.03865v2) - [pdf](http://arxiv.org/pdf/2206.03865v2)

> Large language models (LLMs) have demonstrated an impressive ability to generate code for various programming tasks. In many instances, LLMs can generate a correct program for a task when given numerous trials. Consequently, a recent trend is to do large scale sampling of programs using a model and then filtering/ranking the programs based on the program execution on a small number of known unit tests to select one candidate solution. However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development. In this paper, we propose CodeRanker, a neural ranker that can predict the correctness of a sampled program without executing it. Our CodeRanker is fault-aware i.e., it is trained to predict different kinds of execution information such as predicting the exact compile/runtime error type (e.g., an IndexError or a TypeError). We show that CodeRanker can significantly increase the pass@1 accuracy of various code generation models (including Codex, GPT-Neo, GPT-J) on APPS, HumanEval and MBPP datasets.

</details>

<details>

<summary>2022-12-09 23:02:23 - Matrix Profile XXVII: A Novel Distance Measure for Comparing Long Time Series</summary>

- *Audrey Der, Chin-Chia Michael Yeh, Renjie Wu, Junpeng Wang, Yan Zheng, Zhongfang Zhuang, Liang Wang, Wei Zhang, Eamonn Keogh*

- `2212.06146v1` - [abs](http://arxiv.org/abs/2212.06146v1) - [pdf](http://arxiv.org/pdf/2212.06146v1)

> The most useful data mining primitives are distance measures. With an effective distance measure, it is possible to perform classification, clustering, anomaly detection, segmentation, etc. For single-event time series Euclidean Distance and Dynamic Time Warping distance are known to be extremely effective. However, for time series containing cyclical behaviors, the semantic meaningfulness of such comparisons is less clear. For example, on two separate days the telemetry from an athlete workout routine might be very similar. The second day may change the order in of performing push-ups and squats, adding repetitions of pull-ups, or completely omitting dumbbell curls. Any of these minor changes would defeat existing time series distance measures. Some bag-of-features methods have been proposed to address this problem, but we argue that in many cases, similarity is intimately tied to the shapes of subsequences within these longer time series. In such cases, summative features will lack discrimination ability. In this work we introduce PRCIS, which stands for Pattern Representation Comparison in Series. PRCIS is a distance measure for long time series, which exploits recent progress in our ability to summarize time series with dictionaries. We will demonstrate the utility of our ideas on diverse tasks and datasets.

</details>

<details>

<summary>2022-12-09 23:30:34 - Graph theoretical models and algorithms of portfolio compression</summary>

- *MihÃ¡ly PÃ©ter Hanics*

- `2212.09473v1` - [abs](http://arxiv.org/abs/2212.09473v1) - [pdf](http://arxiv.org/pdf/2212.09473v1)

> In portfolio compression, market participants (banks, organizations, companies, financial agents) sign contracts, creating liabilities between each other, which increases the systemic risk. Large, dense markets commonly can be compressed by reducing obligations without lowering the net notional of each participant (an example is if liabilities make a cycle between agents, then it is possible to reduce each of them without any net notional changing), and our target is to eliminate as much excess notional as possible in practice (excess is defined as the difference between gross and net notional). A limiting factor that may reduce the effectiveness of the compression can be the preferences and priorities of compression participants, who may individually define conditions for the compression, which must be considered when designing the clearing process, otherwise, a participant may bail out, resulting in the designed clearing process to be impossible to execute. These markets can be well-represented with edge-weighted graphs. In this paper, I examine cases when preferences of participants on behalf of clearing are given, e.g., in what order would they pay back their liabilities (a key factor can be the rate of interest) and I show a clearing algorithm for these problems. On top of that, since it is a common goal for the compression coordinating authority to maximize the compressed amount, I also show a method to compute the maximum volume conservative compression in a network. I further evaluate the possibility of combining the two models. Examples and program code of the model are also shown, also a0 pseudo-code of the clearing algorithms.

</details>

<details>

<summary>2022-12-10 03:00:29 - QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks</summary>

- *Yedi Zhang, Zhe Zhao, Fu Song, Min Zhang, Taolue Chen, Jun Sun*

- `2212.11138v1` - [abs](http://arxiv.org/abs/2212.11138v1) - [pdf](http://arxiv.org/pdf/2212.11138v1)

> Deep learning has become a promising programming paradigm in software development, owing to its surprising performance in solving many challenging tasks. Deep neural networks (DNNs) are increasingly being deployed in practice, but are limited on resource-constrained devices owing to their demand for computational power. Quantization has emerged as a promising technique to reduce the size of DNNs with comparable accuracy as their floating-point numbered counterparts. The resulting quantized neural networks (QNNs) can be implemented energy-efficiently. Similar to their floating-point numbered counterparts, quality assurance techniques for QNNs, such as testing and formal verification, are essential but are currently less explored. In this work, we propose a novel and efficient formal verification approach for QNNs. In particular, we are the first to propose an encoding that reduces the verification problem of QNNs into the solving of integer linear constraints, which can be solved using off-the-shelf solvers. Our encoding is both sound and complete. We demonstrate the application of our approach on local robustness verification and maximum robustness radius computation. We implement our approach in a prototype tool QVIP and conduct a thorough evaluation. Experimental results on QNNs with different quantization bits confirm the effectiveness and efficiency of our approach, e.g., two orders of magnitude faster and able to solve more verification tasks in the same time limit than the state-of-the-art methods.

</details>

<details>

<summary>2022-12-10 03:52:03 - Walkability Optimization: Formulations, Algorithms, and a Case Study of Toronto</summary>

- *Weimin Huang, Elias B. Khalil*

- `2212.05192v1` - [abs](http://arxiv.org/abs/2212.05192v1) - [pdf](http://arxiv.org/pdf/2212.05192v1)

> The concept of walkable urban development has gained increased attention due to its public health, economic, and environmental sustainability benefits. Unfortunately, land zoning and historic under-investment have resulted in spatial inequality in walkability and social inequality among residents. We tackle the problem of Walkability Optimization through the lens of combinatorial optimization. The task is to select locations in which additional amenities (e.g., grocery stores, schools, restaurants) can be allocated to improve resident access via walking while taking into account existing amenities and providing multiple options (e.g., for restaurants). To this end, we derive Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) models. Moreover, we show that the problem's objective function is submodular in special cases, which motivates an efficient greedy heuristic. We conduct a case study on 31 underserved neighborhoods in the City of Toronto, Canada. MILP finds the best solutions in most scenarios but does not scale well with network size. The greedy algorithm scales well and finds near-optimal solutions. Our empirical evaluation shows that neighbourhoods with low walkability have a great potential for transformation into pedestrian-friendly neighbourhoods by strategically placing new amenities. Allocating 3 additional grocery stores, schools, and restaurants can improve the "WalkScore" by more than 50 points (on a scale of 100) for 4 neighbourhoods and reduce the walking distances to amenities for 75% of all residential locations to 10 minutes for all amenity types. Our code and paper appendix are available at https://github.com/khalil-research/walkability.

</details>

<details>

<summary>2022-12-10 04:44:25 - Neural Controller Synthesis for Signal Temporal Logic Specifications Using Encoder-Decoder Structured Networks</summary>

- *Wataru Hashimoto, Kazumune Hashimoto, Masako Kishida, Shigemasa Takai*

- `2212.05200v1` - [abs](http://arxiv.org/abs/2212.05200v1) - [pdf](http://arxiv.org/pdf/2212.05200v1)

> In this paper, we propose a control synthesis method for signal temporal logic (STL) specifications with neural networks (NNs). Most of the previous works consider training a controller for only a given STL specification. These approaches, however, require retraining the NN controller if a new specification arises and needs to be satisfied, which results in large consumption of memory and inefficient training. To tackle this problem, we propose to construct NN controllers by introducing encoder-decoder structured NNs with an attention mechanism. The encoder takes an STL formula as input and encodes it into an appropriate vector, and the decoder outputs control signals that will meet the given specification. As the encoder, we consider three NN structures: sequential, tree-structured, and graph-structured NNs. All the model parameters are trained in an end-to-end manner to maximize the expected robustness that is known to be a quantitative semantics of STL formulae. We compare the control performances attained by the above NN structures through a numerical experiment of the path planning problem, showing the efficacy of the proposed approach.

</details>

<details>

<summary>2022-12-10 05:19:58 - OpenD: A Benchmark for Language-Driven Door and Drawer Opening</summary>

- *Yizhou Zhao, Qiaozi Gao, Liang Qiu, Govind Thattai, Gaurav S. Sukhatme*

- `2212.05211v1` - [abs](http://arxiv.org/abs/2212.05211v1) - [pdf](http://arxiv.org/pdf/2212.05211v1)

> We introduce OPEND, a benchmark for learning how to use a hand to open cabinet doors or drawers in a photo-realistic and physics-reliable simulation environment driven by language instruction. To solve the task, we propose a multi-step planner composed of a deep neural network and rule-base controllers. The network is utilized to capture spatial relationships from images and understand semantic meaning from language instructions. Controllers efficiently execute the plan based on the spatial and semantic understanding. We evaluate our system by measuring its zero-shot performance in test data set. Experimental results demonstrate the effectiveness of decision planning by our multi-step planner for different hands, while suggesting that there is significant room for developing better models to address the challenge brought by language understanding, spatial reasoning, and long-term manipulation. We will release OPEND and host challenges to promote future research in this area.

</details>

<details>

<summary>2022-12-10 08:43:15 - A Quantitative Flavour of Robust Reachability</summary>

- *SÃ©bastien Bardin, Guillaume Girol*

- `2212.05244v1` - [abs](http://arxiv.org/abs/2212.05244v1) - [pdf](http://arxiv.org/pdf/2212.05244v1)

> Many software analysis techniques attempt to determine whether bugs are reachable, but for security purpose this is only part of the story as it does not indicate whether the bugs found could be easily triggered by an attacker. The recently introduced notion of robust reachability aims at filling this gap by distinguishing the input controlled by the attacker from those that are not. Yet, this qualitative notion may be too strong in practice, leaving apart bugs which are mostly but not fully replicable. We aim here at proposing a quantitative version of robust reachability, more flexible and still amenable to automation. We propose quantitative robustness, a metric expressing how easily an attacker can trigger a bug while taking into account that he can only influence part of the program input, together with a dedicated quantitative symbolic execution technique (QRSE). Interestingly, QRSE relies on a variant of model counting (namely, functional E-MAJSAT) unseen so far in formal verification, but which has been studied in AI domains such as Bayesian network, knowledge representation and probabilistic planning. Yet, the existing solving methods from these fields turn out to be unsatisfactory for formal verification purpose, leading us to propose a novel parametric method. These results have been implemented and evaluated over two security-relevant case studies, allowing to demonstrate the feasibility and relevance of our ideas.

</details>

<details>

<summary>2022-12-10 12:37:07 - Exploring The Relationship Between Road Infrastructure and Crimes in Memphis, Tennessee</summary>

- *Alexandre Signorel*

- `2212.00956v2` - [abs](http://arxiv.org/abs/2212.00956v2) - [pdf](http://arxiv.org/pdf/2212.00956v2)

> Memphis, Tennessee is one of the cities with highest crime rate in the United States. In this work, we explore the relationship between road infrastructure, especially potholes, and crimes. The pothole and crime data are collected from Memphis Data Hub between 2020 and 2022. The crime data report various crimes in the Memphis area, which contain the location, time, and type of the crime. The pothole data is part of the Open 311 data, which contains information of different infrastructure projects, including the location of the project, and the starting and ending dates of the project. We focus on infrastructure projects regarding pothole repairs.

</details>

<details>

<summary>2022-12-10 13:33:56 - Relate to Predict: Towards Task-Independent Knowledge Representations for Reinforcement Learning</summary>

- *Thomas SchnÃ¼rer, Malte Probst, Horst-Michael Gross*

- `2212.05298v1` - [abs](http://arxiv.org/abs/2212.05298v1) - [pdf](http://arxiv.org/pdf/2212.05298v1)

> Reinforcement Learning (RL) can enable agents to learn complex tasks. However, it is difficult to interpret the knowledge and reuse it across tasks. Inductive biases can address such issues by explicitly providing generic yet useful decomposition that is otherwise difficult or expensive to learn implicitly. For example, object-centered approaches decompose a high dimensional observation into individual objects. Expanding on this, we utilize an inductive bias for explicit object-centered knowledge separation that provides further decomposition into semantic representations and dynamics knowledge. For this, we introduce a semantic module that predicts an objects' semantic state based on its context. The resulting affordance-like object state can then be used to enrich perceptual object representations. With a minimal setup and an environment that enables puzzle-like tasks, we demonstrate the feasibility and benefits of this approach. Specifically, we compare three different methods of integrating semantic representations into a model-based RL architecture. Our experiments show that the degree of explicitness in knowledge separation correlates with faster learning, better accuracy, better generalization, and better interpretability.

</details>

<details>

<summary>2022-12-10 16:28:03 - Nonparametric Learning of Two-Layer ReLU Residual Units</summary>

- *Zhunxuan Wang, Linyun He, Chunchuan Lyu, Shay B. Cohen*

- `2008.07648v3` - [abs](http://arxiv.org/abs/2008.07648v3) - [pdf](http://arxiv.org/pdf/2008.07648v3)

> We describe an algorithm that learns two-layer residual units using rectified linear unit (ReLU) activation: suppose the input $\mathbf{x}$ is from a distribution with support space $\mathbb{R}^d$ and the ground-truth generative model is a residual unit of this type, given by $\mathbf{y} = \boldsymbol{B}^\ast\left[\left(\boldsymbol{A}^\ast\mathbf{x}\right)^+ + \mathbf{x}\right]$, where ground-truth network parameters $\boldsymbol{A}^\ast \in \mathbb{R}^{d\times d}$ represent a full-rank matrix with nonnegative entries and $\boldsymbol{B}^\ast \in \mathbb{R}^{m\times d}$ is full-rank with $m \geq d$ and for $\boldsymbol{c} \in \mathbb{R}^d$, $[\boldsymbol{c}^{+}]_i = \max\{0, c_i\}$. We design layer-wise objectives as functionals whose analytic minimizers express the exact ground-truth network in terms of its parameters and nonlinearities. Following this objective landscape, learning residual units from finite samples can be formulated using convex optimization of a nonparametric function: for each layer, we first formulate the corresponding empirical risk minimization (ERM) as a positive semi-definite quadratic program (QP), then we show the solution space of the QP can be equivalently determined by a set of linear inequalities, which can then be efficiently solved by linear programming (LP). We further prove the strong statistical consistency of our algorithm, and demonstrate its robustness and sample efficiency through experimental results on synthetic data and a set of benchmark regression datasets.

</details>

<details>

<summary>2022-12-10 16:38:16 - CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure</summary>

- *Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Li, Xuesong Lu, Ming Gao*

- `2210.04633v4` - [abs](http://arxiv.org/abs/2210.04633v4) - [pdf](http://arxiv.org/pdf/2210.04633v4)

> Code pre-trained models (CodePTMs) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose a novel probing method CAT-probing to quantitatively interpret how CodePTMs attend code structure. We first denoise the input code sequences based on the token types pre-defined by the compilers to filter those tokens whose attention scores are too small. After that, we define a new metric CAT-score to measure the commonality between the token-level attention scores generated in CodePTMs and the pair-wise distances between corresponding AST nodes. The higher the CAT-score, the stronger the ability of CodePTMs to capture code structure. We conduct extensive experiments to integrate CAT-probing with representative CodePTMs for different programming languages. Experimental results show the effectiveness of CAT-probing in CodePTM interpretation. Our codes and data are publicly available at https://github.com/nchen909/CodeAttention.

</details>

<details>

<summary>2022-12-10 20:04:37 - Efficient and Generic Algorithms for Quantitative Attack Tree Analysis</summary>

- *Milan LopuhaÃ¤-Zwakenberg, Carlos E. Budde, MariÃ«lle Stoelinga*

- `2212.05358v1` - [abs](http://arxiv.org/abs/2212.05358v1) - [pdf](http://arxiv.org/pdf/2212.05358v1)

> Numerous analysis methods for quantitative attack tree analysis have been proposed. These algorithms compute relevant security metrics, i.e. performance indicators that quantify how good the security of a system is; typical metrics being the most likely attack, the cheapest, or the most damaging one. However, existing methods are only geared towards specific metrics or do not work on general attack trees. This paper classifies attack trees in two dimensions: proper trees vs. directed acyclic graphs (i.e. with shared subtrees); and static vs. dynamic gates. For three out of these four classes, we propose novel algorithms that work over a generic attribute domain, encompassing a large number of concrete security metrics defined on the attack tree semantics; dynamic attack trees with directed acyclic graph structure are left as an open problem. We also analyse the computational complexity of our methods.

</details>

<details>

<summary>2022-12-11 04:58:00 - Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding</summary>

- *Jiali Zeng, Yongjing Yin, Yufan Jiang, Shuangzhi Wu, Yunbo Cao*

- `2211.03348v2` - [abs](http://arxiv.org/abs/2211.03348v2) - [pdf](http://arxiv.org/pdf/2211.03348v2)

> Contrastive learning has become a new paradigm for unsupervised sentence embeddings. Previous studies focus on instance-wise contrastive learning, attempting to construct positive pairs with textual data augmentation. In this paper, we propose a novel Contrastive learning method with Prompt-derived Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts, we construct virtual semantic prototypes to each instance, and derive negative prototypes by using the negative form of the prompts. Using a prototypical contrastive loss, we enforce the anchor sentence embedding to be close to its corresponding semantic prototypes, and far apart from the negative prototypes as well as the prototypes of other sentences. Extensive experimental results on semantic textual similarity, transfer, and clustering tasks demonstrate the effectiveness of our proposed model compared to strong baselines. Code is available at https://github.com/lemon0830/promptCSE.

</details>

<details>

<summary>2022-12-11 07:29:56 - Toward Robust Graph Semi-Supervised Learning against Extreme Data Scarcity</summary>

- *Kaize Ding, Elnaz Nouri, Guoqing Zheng, Huan Liu, Ryen White*

- `2208.12422v2` - [abs](http://arxiv.org/abs/2208.12422v2) - [pdf](http://arxiv.org/pdf/2208.12422v2)

> The success of graph neural networks on graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only few labeled nodes are available, how to improve their robustness is a key to achieve replicable and sustainable graph semi-supervised learning. Though self-training has been shown to be powerful for semi-supervised learning, its application on graph-structured data may fail because (1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes; and (2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, AGST (Augmented Graph Self-Training), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data.

</details>

<details>

<summary>2022-12-11 07:54:01 - Understanding Concurrency Vulnerabilities in Linux Kernel</summary>

- *Zunchen Huang, Shengjian Guo, Meng Wu, Chao Wang*

- `2212.05438v1` - [abs](http://arxiv.org/abs/2212.05438v1) - [pdf](http://arxiv.org/pdf/2212.05438v1)

> While there is a large body of work on analyzing concurrency related software bugs and developing techniques for detecting and patching them, little attention has been given to concurrency related security vulnerabilities. The two are different in that not all bugs are vulnerabilities: for a bug to be exploitable, there needs be a way for attackers to trigger its execution and cause damage, e.g., by revealing sensitive data or running malicious code. To fill the gap, we conduct the first empirical study of concurrency vulnerabilities reported in the Linux operating system in the past ten years. We focus on analyzing the confirmed vulnerabilities archived in the Common Vulnerabilities and Exposures (CVE) database, which are then categorized into different groups based on bug types, exploit patterns, and patch strategies adopted by developers. We use code snippets to illustrate individual vulnerability types and patch strategies. We also use statistics to illustrate the entire landscape, including the percentage of each vulnerability type. We hope to shed some light on the problem, e.g., concurrency vulnerabilities continue to pose a serious threat to system security, and it is difficult even for kernel developers to analyze and patch them. Therefore, more efforts are needed to develop tools and techniques for analyzing and patching these vulnerabilities.

</details>

<details>

<summary>2022-12-11 14:22:17 - Why Should and How Can Quantum Technologies Be Leveraged at National Levels?</summary>

- *AbdulMalek Baitulmal, Nadia Adem*

- `2212.08040v1` - [abs](http://arxiv.org/abs/2212.08040v1) - [pdf](http://arxiv.org/pdf/2212.08040v1)

> Quantum technologies (QT) promise to change the landscape of technologies disruptively in diverse industries. For this reason, many nations around the globe are investing to emerge within the global quantum ecosystem through initiating national programs and international partnerships. Nonetheless, some other countries are still running behind and yet their governments need to take series actions to help their private and public sectors adapt to the looming changes, considering the new regulations required and the huge influence that QT will present in the near future. In this opinion piece, we provide, for the best of our knowledge, the first generally applicable, yet comprehensive and brief, framework for leveraging the emerging quantum technologies to facilitate the establishment of national initiatives properly. The insights presented in this article were driven based on investigating various approaches, initiatives, and roadmaps adopted globally and meeting with local and regional leaders, professionals, and governmental officials. Furthermore, taken into account socioeconomic and institutional dimensions of the Libyan society, we project the framework for the Libyan nation. This opinion piece is intended to inspire researchers, technical industrial experts, stakeholders, and governmental bodies to find roles they need to play to bring QT forward.

</details>

<details>

<summary>2022-12-11 18:51:57 - MAViC: Multimodal Active Learning for Video Captioning</summary>

- *Gyanendra Das, Xavier Thomas, Anant Raj, Vikram Gupta*

- `2212.11109v1` - [abs](http://arxiv.org/abs/2212.11109v1) - [pdf](http://arxiv.org/pdf/2212.11109v1)

> A large number of annotated video-caption pairs are required for training video captioning models, resulting in high annotation costs. Active learning can be instrumental in reducing these annotation requirements. However, active learning for video captioning is challenging because multiple semantically similar captions are valid for a video, resulting in high entropy outputs even for less-informative samples. Moreover, video captioning algorithms are multimodal in nature with a visual encoder and language decoder. Further, the sequential and combinatorial nature of the output makes the problem even more challenging. In this paper, we introduce MAViC which leverages our proposed Multimodal Semantics Aware Sequential Entropy (M-SASE) based acquisition function to address the challenges of active learning approaches for video captioning. Our approach integrates semantic similarity and uncertainty of both visual and language dimensions in the acquisition function. Our detailed experiments empirically demonstrate the efficacy of M-SASE for active learning for video captioning and improve on the baselines by a large margin.

</details>

<details>

<summary>2022-12-11 22:03:42 - Generic Tagging for RISC-V Binaries</summary>

- *David Demicco, Matthew Cole, Gokturk Yuksek, Ravi Theja Gollapudi, Aravind Prakash, Kanad Ghose, Zerksis Umrigar*

- `2212.05614v1` - [abs](http://arxiv.org/abs/2212.05614v1) - [pdf](http://arxiv.org/pdf/2212.05614v1)

> With the widespread popularity of RISC-V -- an open-source ISA -- custom hardware security solutions targeting specific defense needs are gaining popularity. These solutions often require specialized compilers that can insert metadata (called tags) into the generated binaries, and/or extend the RISC-V ISA with new instructions. Developing such compilers can be a tedious and time-consuming process. In this paper, we present COGENT, a generic instruction tag generator for RISC-V architecture. COGENT is capable of associating a tag of configurable and varying widths (1 to 20 bits) to each instruction. It is also capable of emitting labels that are central to the implementation of control-flow integrity (CFI) solutions. COGENT encodes all tags and labels as nop instructions thereby providing full backward compatibility.   We evaluate COGENT on a subset of programs from the SPEC CPU2017 benchmark suite and report the binary size increase to be 29.3% and 18.27% for the lowest and highest tag coverage levels respectively. Additionally, we executed tagged programs on COTS RISC-V unmodified hardware and found the execution time overhead (with respect to backward compatibility) to be 13.4% and 5.72% for the lowest and highest coverage levels respectively. Finally, using a case study, we present possible use case scenarios where COGENT can be applied.

</details>

<details>

<summary>2022-12-12 01:07:00 - Differentiable Programming Ã  la Moreau</summary>

- *Vincent Roulet, Zaid Harchaoui*

- `2012.15458v2` - [abs](http://arxiv.org/abs/2012.15458v2) - [pdf](http://arxiv.org/pdf/2012.15458v2)

> The notion of a Moreau envelope is central to the analysis of first-order optimization algorithms for machine learning. Yet, it has not been developed and extended to be applied to a deep network and, more broadly, to a machine learning system with a differentiable programming implementation. We define a compositional calculus adapted to Moreau envelopes and show how to integrate it within differentiable programming. The proposed framework casts in a mathematical optimization framework several variants of gradient back-propagation related to the idea of the propagation of virtual targets.

</details>

<details>

<summary>2022-12-12 08:02:35 - Scale-Semantic Joint Decoupling Network for Image-text Retrieval in Remote Sensing</summary>

- *Chengyu Zheng, Ning song, Ruoyu Zhang, Lei Huang, Zhiqiang Wei, Jie Nie*

- `2212.05752v1` - [abs](http://arxiv.org/abs/2212.05752v1) - [pdf](http://arxiv.org/pdf/2212.05752v1)

> Image-text retrieval in remote sensing aims to provide flexible information for data analysis and application. In recent years, state-of-the-art methods are dedicated to ``scale decoupling'' and ``semantic decoupling'' strategies to further enhance the capability of representation. However, these previous approaches focus on either the disentangling scale or semantics but ignore merging these two ideas in a union model, which extremely limits the performance of cross-modal retrieval models. To address these issues, we propose a novel Scale-Semantic Joint Decoupling Network (SSJDN) for remote sensing image-text retrieval. Specifically, we design the Bidirectional Scale Decoupling (BSD) module, which exploits Salience Feature Extraction (SFE) and Salience-Guided Suppression (SGS) units to adaptively extract potential features and suppress cumbersome features at other scales in a bidirectional pattern to yield different scale clues. Besides, we design the Label-supervised Semantic Decoupling (LSD) module by leveraging the category semantic labels as prior knowledge to supervise images and texts probing significant semantic-related information. Finally, we design a Semantic-guided Triple Loss (STL), which adaptively generates a constant to adjust the loss function to improve the probability of matching the same semantic image and text and shorten the convergence time of the retrieval model. Our proposed SSJDN outperforms state-of-the-art approaches in numerical experiments conducted on four benchmark remote sensing datasets.

</details>

<details>

<summary>2022-12-12 10:10:29 - Page Layout Analysis of Text-heavy Historical Documents: a Comparison of Textual and Visual Approaches</summary>

- *Najem-Meyer Sven, Romanello Matteo*

- `2212.13924v1` - [abs](http://arxiv.org/abs/2212.13924v1) - [pdf](http://arxiv.org/pdf/2212.13924v1)

> Page layout analysis is a fundamental step in document processing which enables to segment a page into regions of interest. With highly complex layouts and mixed scripts, scholarly commentaries are text-heavy documents which remain challenging for state-of-the-art models. Their layout considerably varies across editions and their most important regions are mainly defined by semantic rather than graphical characteristics such as position or appearance. This setting calls for a comparison between textual, visual and hybrid approaches. We therefore assess the performances of two transformers (LayoutLMv3 and RoBERTa) and an objection-detection network (YOLOv5). If results show a clear advantage in favor of the latter, we also list several caveats to this finding. In addition to our experiments, we release a dataset of ca. 300 annotated pages sampled from 19th century commentaries.

</details>

<details>

<summary>2022-12-12 10:46:48 - XFL: Naming Functions in Binaries with Extreme Multi-label Learning</summary>

- *James Patrick-Evans, Moritz Dannehl, Johannes Kinder*

- `2107.13404v4` - [abs](http://arxiv.org/abs/2107.13404v4) - [pdf](http://arxiv.org/pdf/2107.13404v4)

> Reverse engineers benefit from the presence of identifiers such as function names in a binary, but usually these are removed for release. Training a machine learning model to predict function names automatically is promising but fundamentally hard: unlike words in natural language, most function names occur only once. In this paper, we address this problem by introducing eXtreme Function Labeling (XFL), an extreme multi-label learning approach to selecting appropriate labels for binary functions. XFL splits function names into tokens, treating each as an informative label akin to the problem of tagging texts in natural language. We relate the semantics of binary code to labels through DEXTER, a novel function embedding that combines static analysis-based features with local context from the call graph and global context from the entire binary. We demonstrate that XFL/DEXTER outperforms the state of the art in function labeling on a dataset of 10,047 binaries from the Debian project, achieving a precision of 83.5%. We also study combinations of XFL with alternative binary embeddings from the literature and show that DEXTER consistently performs best for this task. As a result, we demonstrate that binary function labeling can be effectively phrased in terms of multi-label learning, and that binary function embeddings benefit from including explicit semantic features.

</details>

<details>

<summary>2022-12-12 10:58:46 - Carpet-bombing patch: attacking a deep network without usual requirements</summary>

- *Pol Labarbarie, Adrien Chan-Hon-Tong, StÃ©phane Herbin, Milad Leyli-Abadi*

- `2212.05827v1` - [abs](http://arxiv.org/abs/2212.05827v1) - [pdf](http://arxiv.org/pdf/2212.05827v1)

> Although deep networks have shown vulnerability to evasion attacks, such attacks have usually unrealistic requirements. Recent literature discussed the possibility to remove or not some of these requirements. This paper contributes to this literature by introducing a carpet-bombing patch attack which has almost no requirement. Targeting the feature representations, this patch attack does not require knowing the network task. This attack decreases accuracy on Imagenet, mAP on Pascal Voc, and IoU on Cityscapes without being aware that the underlying tasks involved classification, detection or semantic segmentation, respectively. Beyond the potential safety issues raised by this attack, the impact of the carpet-bombing attack highlights some interesting property of deep network layer dynamic.

</details>

<details>

<summary>2022-12-12 14:36:39 - Forecasting Soil Moisture Using Domain Inspired Temporal Graph Convolution Neural Networks To Guide Sustainable Crop Management</summary>

- *Muneeza Azmat, Malvern Madondo, Kelsey Dipietro, Raya Horesh, Arun Bawa, Michael Jacobs, Raghavan Srinivasan, Fearghal O'Donncha*

- `2212.06565v1` - [abs](http://arxiv.org/abs/2212.06565v1) - [pdf](http://arxiv.org/pdf/2212.06565v1)

> Climate change, population growth, and water scarcity present unprecedented challenges for agriculture. This project aims to forecast soil moisture using domain knowledge and machine learning for crop management decisions that enable sustainable farming. Traditional methods for predicting hydrological response features require significant computational time and expertise. Recent work has implemented machine learning models as a tool for forecasting hydrological response features, but these models neglect a crucial component of traditional hydrological modeling that spatially close units can have vastly different hydrological responses. In traditional hydrological modeling, units with similar hydrological properties are grouped together and share model parameters regardless of their spatial proximity. Inspired by this domain knowledge, we have constructed a novel domain-inspired temporal graph convolution neural network. Our approach involves clustering units based on time-varying hydrological properties, constructing graph topologies for each cluster, and forecasting soil moisture using graph convolutions and a gated recurrent neural network. We have trained, validated, and tested our method on field-scale time series data consisting of approximately 99,000 hydrological response units spanning 40 years in a case study in northeastern United States. Comparison with existing models illustrates the effectiveness of using domain-inspired clustering with time series graph neural networks. The framework is being deployed as part of a pro bono social impact program. The trained models are being deployed on small-holding farms in central Texas.

</details>

<details>

<summary>2022-12-12 14:41:46 - LAMBRETTA: Learning to Rank for Twitter Soft Moderation</summary>

- *Pujan Paudel, Jeremy Blackburn, Emiliano De Cristofaro, Savvas Zannettou, Gianluca Stringhini*

- `2212.05926v1` - [abs](http://arxiv.org/abs/2212.05926v1) - [pdf](http://arxiv.org/pdf/2212.05926v1)

> To curb the problem of false information, social media platforms like Twitter started adding warning labels to content discussing debunked narratives, with the goal of providing more context to their audiences. Unfortunately, these labels are not applied uniformly and leave large amounts of false content unmoderated. This paper presents LAMBRETTA, a system that automatically identifies tweets that are candidates for soft moderation using Learning To Rank (LTR). We run LAMBRETTA on Twitter data to moderate false claims related to the 2020 US Election and find that it flags over 20 times more tweets than Twitter, with only 3.93% false positives and 18.81% false negatives, outperforming alternative state-of-the-art methods based on keyword extraction and semantic search. Overall, LAMBRETTA assists human moderators in identifying and flagging false information on social media.

</details>

<details>

<summary>2022-12-12 15:00:11 - Automated analysis of fibrous cap in intravascular optical coherence tomography images of coronary arteries</summary>

- *Juhwan Lee, Gabriel T. R. Pereira, Yazan Gharaibeh, Chaitanya Kolluru, Vladislav N. Zimin, Luis A. P. Dallan, Justin N. Kim, Ammar Hoori, Sadeer G. Al-Kindi, Giulio Guagliumi, Hiram G. Bezerra, David L. Wilson*

- `2204.10162v2` - [abs](http://arxiv.org/abs/2204.10162v2) - [pdf](http://arxiv.org/pdf/2204.10162v2)

> Thin-cap fibroatheroma (TCFA) and plaque rupture have been recognized as the most frequent risk factor for thrombosis and acute coronary syndrome. Intravascular optical coherence tomography (IVOCT) can identify TCFA and assess cap thickness, which provides an opportunity to assess plaque vulnerability. We developed an automated method that can detect lipidous plaque and assess fibrous cap thickness in IVOCT images. This study analyzed a total of 4,360 IVOCT image frames of 77 lesions among 41 patients. To improve segmentation performance, preprocessing included lumen segmentation, pixel-shifting, and noise filtering on the raw polar (r, theta) IVOCT images. We used the DeepLab-v3 plus deep learning model to classify lipidous plaque pixels. After lipid detection, we automatically detected the outer border of the fibrous cap using a special dynamic programming algorithm and assessed the cap thickness. Our method provided excellent discriminability of lipid plaque with a sensitivity of 85.8% and A-line Dice coefficient of 0.837. By comparing lipid angle measurements between two analysts following editing of our automated software, we found good agreement by Bland-Altman analysis (difference 6.7+/-17 degree; mean 196 degree). Our method accurately detected the fibrous cap from the detected lipid plaque. Automated analysis required a significant modification for only 5.5% frames. Furthermore, our method showed a good agreement of fibrous cap thickness between two analysts with Bland-Altman analysis (4.2+/-14.6 micron; mean 175 micron), indicating little bias between users and good reproducibility of the measurement. We developed a fully automated method for fibrous cap quantification in IVOCT images, resulting in good agreement with determinations by analysts. The method has great potential to enable highly automated, repeatable, and comprehensive evaluations of TCFAs.

</details>

<details>

<summary>2022-12-12 16:03:31 - Reinforcement Learning and Tree Search Methods for the Unit Commitment Problem</summary>

- *Patrick de Mars*

- `2212.06001v1` - [abs](http://arxiv.org/abs/2212.06001v1) - [pdf](http://arxiv.org/pdf/2212.06001v1)

> The unit commitment (UC) problem, which determines operating schedules of generation units to meet demand, is a fundamental task in power systems operation. Existing UC methods using mixed-integer programming are not well-suited to highly stochastic systems. Approaches which more rigorously account for uncertainty could yield large reductions in operating costs by reducing spinning reserve requirements; operating power stations at higher efficiencies; and integrating greater volumes of variable renewables. A promising approach to solving the UC problem is reinforcement learning (RL), a methodology for optimal decision-making which has been used to conquer long-standing grand challenges in artificial intelligence. This thesis explores the application of RL to the UC problem and addresses challenges including robustness under uncertainty; generalisability across multiple problem instances; and scaling to larger power systems than previously studied. To tackle these issues, we develop guided tree search, a novel methodology combining model-free RL and model-based planning. The UC problem is formalised as a Markov decision process and we develop an open-source environment based on real data from Great Britain's power system to train RL agents. In problems of up to 100 generators, guided tree search is shown to be competitive with deterministic UC methods, reducing operating costs by up to 1.4\%. An advantage of RL is that the framework can be easily extended to incorporate considerations important to power systems operators such as robustness to generator failure, wind curtailment or carbon prices. When generator outages are considered, guided tree search saves over 2\% in operating costs as compared with methods using conventional $N-x$ reserve criteria.

</details>

<details>

<summary>2022-12-12 20:28:13 - Synthetic Image Data for Deep Learning</summary>

- *Jason W. Anderson, Marcin Ziolkowski, Ken Kennedy, Amy W. Apon*

- `2212.06232v1` - [abs](http://arxiv.org/abs/2212.06232v1) - [pdf](http://arxiv.org/pdf/2212.06232v1)

> Realistic synthetic image data rendered from 3D models can be used to augment image sets and train image classification semantic segmentation models. In this work, we explore how high quality physically-based rendering and domain randomization can efficiently create a large synthetic dataset based on production 3D CAD models of a real vehicle. We use this dataset to quantify the effectiveness of synthetic augmentation using U-net and Double-U-net models. We found that, for this domain, synthetic images were an effective technique for augmenting limited sets of real training data. We observed that models trained on purely synthetic images had a very low mean prediction IoU on real validation images. We also observed that adding even very small amounts of real images to a synthetic dataset greatly improved accuracy, and that models trained on datasets augmented with synthetic images were more accurate than those trained on real images alone. Finally, we found that in use cases that benefit from incremental training or model specialization, pretraining a base model on synthetic images provided a sizeable reduction in the training cost of transfer learning, allowing up to 90\% of the model training to be front-loaded.

</details>

<details>

<summary>2022-12-12 23:53:23 - Towards a Change Taxonomy for Machine Learning Systems</summary>

- *Aaditya Bhatia, Ellis E. Eghan, Manel Grichi, William G. Cavanagh, Zhen Ming, Jiang, Bram Adams*

- `2203.11365v3` - [abs](http://arxiv.org/abs/2203.11365v3) - [pdf](http://arxiv.org/pdf/2203.11365v3)

> Machine Learning (ML) research publications commonly provide open-source implementations on GitHub, allowing their audience to replicate, validate, or even extend machine learning algorithms, data sets, and metadata.   However, thus far little is known about the degree of collaboration activity happening on such ML research repositories, in particular regarding (1) the degree to which such repositories receive contributions from forks, (2) the nature of such contributions (i.e., the types of changes), and (3) the nature of changes that are not contributed back to forks, which might represent missed opportunities. In this paper, we empirically study contributions to 1,346 ML research repositories and their 67,369 forks, both quantitatively and qualitatively (by building on Hindle et al.'s seminal taxonomy of code changes). We found that while ML research repositories are heavily forked, only 9% of the forks made modifications to the forked repository. 42% of the latter sent changes to the parent repositories, half of which (52%) were accepted by the parent repositories. Our qualitative analysis on 539 contributed and 378 local (fork-only) changes, extends Hindle et al.'s taxonomy with one new top-level change category related to ML (Data), and 15 new sub-categories, including nine ML-specific ones (input data, output data, program data, sharing, change evaluation, parameter tuning, performance, pre-processing, model training). While the changes that are not contributed back by the forks mostly concern domain-specific customizations and local experimentation (e.g., parameter tuning), the origin ML repositories do miss out on a non-negligible 15.4% of Documentation changes, 13.6% of Feature changes and 11.4% of Bug fix changes. The findings in this paper will be useful for practitioners, researchers, toolsmiths, and educators.

</details>

<details>

<summary>2022-12-13 05:30:29 - Self-adaptive algorithms for quasiconvex programming and applications to machine learning</summary>

- *Thang Tran Ngoc, Hai Trinh Ngoc*

- `2212.06379v1` - [abs](http://arxiv.org/abs/2212.06379v1) - [pdf](http://arxiv.org/pdf/2212.06379v1)

> For solving a broad class of nonconvex programming problems on an unbounded constraint set, we provide a self-adaptive step-size strategy that does not include line-search techniques and establishes the convergence of a generic approach under mild assumptions. Specifically, the objective function may not satisfy the convexity condition. Unlike descent line-search algorithms, it does not need a known Lipschitz constant to figure out how big the first step should be. The crucial feature of this process is the steady reduction of the step size until a certain condition is fulfilled. In particular, it can provide a new gradient projection approach to optimization problems with an unbounded constrained set. The correctness of the proposed method is verified by preliminary results from some computational examples. To demonstrate the effectiveness of the proposed technique for large-scale problems, we apply it to some experiments on machine learning, such as supervised feature selection, multi-variable logistic regressions and neural networks for classification.

</details>

<details>

<summary>2022-12-13 09:38:07 - Multivariate Powered Dirichlet Hawkes Process</summary>

- *GaÃ«l Poux-MÃ©dard, Julien Velcin, Sabine Loudcher*

- `2212.05995v2` - [abs](http://arxiv.org/abs/2212.05995v2) - [pdf](http://arxiv.org/pdf/2212.05995v2)

> The publication time of a document carries a relevant information about its semantic content. The Dirichlet-Hawkes process has been proposed to jointly model textual information and publication dynamics. This approach has been used with success in several recent works, and extended to tackle specific challenging problems --typically for short texts or entangled publication dynamics. However, the prior in its current form does not allow for complex publication dynamics. In particular, inferred topics are independent from each other --a publication about finance is assumed to have no influence on publications about politics, for instance.   In this work, we develop the Multivariate Powered Dirichlet-Hawkes Process (MPDHP), that alleviates this assumption. Publications about various topics can now influence each other. We detail and overcome the technical challenges that arise from considering interacting topics. We conduct a systematic evaluation of MPDHP on a range of synthetic datasets to define its application domain and limitations. Finally, we develop a use case of the MPDHP on Reddit data. At the end of this article, the interested reader will know how and when to use MPDHP, and when not to.

</details>

<details>

<summary>2022-12-13 10:29:41 - Edge Computing for Semantic Communication Enabled Metaverse: An Incentive Mechanism Design</summary>

- *Nguyen Cong Luong, Quoc-Viet Pham, Thien Huynh-The, Van-Dinh Nguyen, Derrick Wing Kwan Ng, Symeon Chatzinotas*

- `2212.06463v1` - [abs](http://arxiv.org/abs/2212.06463v1) - [pdf](http://arxiv.org/pdf/2212.06463v1)

> Semantic communication (SemCom) and edge computing are two disruptive solutions to address emerging requirements of huge data communication, bandwidth efficiency and low latency data processing in Metaverse. However, edge computing resources are often provided by computing service providers and thus it is essential to design appealingly incentive mechanisms for the provision of limited resources. Deep learning (DL)- based auction has recently proposed as an incentive mechanism that maximizes the revenue while holding important economic properties, i.e., individual rationality and incentive compatibility. Therefore, in this work, we introduce the design of the DLbased auction for the computing resource allocation in SemComenabled Metaverse. First, we briefly introduce the fundamentals and challenges of Metaverse. Second, we present the preliminaries of SemCom and edge computing. Third, we review various incentive mechanisms for edge computing resource trading. Fourth, we present the design of the DL-based auction for edge resource allocation in SemCom-enabled Metaverse. Simulation results demonstrate that the DL-based auction improves the revenue while nearly satisfying the individual rationality and incentive compatibility constraints.

</details>

<details>

<summary>2022-12-13 10:32:22 - Factorizer: A Scalable Interpretable Approach to Context Modeling for Medical Image Segmentation</summary>

- *Pooya Ashtari, Diana M. Sima, Lieven De Lathauwer, Dominique Sappey-Marinier, Frederik Maes, Sabine Van Huffel*

- `2202.12295v3` - [abs](http://arxiv.org/abs/2202.12295v3) - [pdf](http://arxiv.org/pdf/2202.12295v3)

> Convolutional Neural Networks (CNNs) with U-shaped architectures have dominated medical image segmentation, which is crucial for various clinical purposes. However, the inherent locality of convolution makes CNNs fail to fully exploit global context, essential for better recognition of some structures, e.g., brain lesions. Transformers have recently proven promising performance on vision tasks, including semantic segmentation, mainly due to their capability of modeling long-range dependencies. Nevertheless, the quadratic complexity of attention makes existing Transformer-based models use self-attention layers only after somehow reducing the image resolution, which limits the ability to capture global contexts present at higher resolutions. Therefore, this work introduces a family of models, dubbed Factorizer, which leverages the power of low-rank matrix factorization for constructing an end-to-end segmentation model. Specifically, we propose a linearly scalable approach to context modeling, formulating Nonnegative Matrix Factorization (NMF) as a differentiable layer integrated into a U-shaped architecture. The shifted window technique is also utilized in combination with NMF to effectively aggregate local information. Factorizers compete favorably with CNNs and Transformers in terms of accuracy, scalability, and interpretability, achieving state-of-the-art results on the BraTS dataset for brain tumor segmentation and ISLES'22 dataset for stroke lesion segmentation. Highly meaningful NMF components give an additional interpretability advantage to Factorizers over CNNs and Transformers. Moreover, our ablation studies reveal a distinctive feature of Factorizers that enables a significant speed-up in inference for a trained Factorizer without any extra steps and without sacrificing much accuracy. The code and models are publicly available at https://github.com/pashtari/factorizer.

</details>

<details>

<summary>2022-12-13 13:54:49 - RLogist: Fast Observation Strategy on Whole-slide Images with Deep Reinforcement Learning</summary>

- *Boxuan Zhao, Jun Zhang, Deheng Ye, Jian Cao, Xiao Han, Qiang Fu, Wei Yang*

- `2212.01737v2` - [abs](http://arxiv.org/abs/2212.01737v2) - [pdf](http://arxiv.org/pdf/2212.01737v2)

> Whole-slide images (WSI) in computational pathology have high resolution with gigapixel size, but are generally with sparse regions of interest, which leads to weak diagnostic relevance and data inefficiency for each area in the slide. Most of the existing methods rely on a multiple instance learning framework that requires densely sampling local patches at high magnification. The limitation is evident in the application stage as the heavy computation for extracting patch-level features is inevitable. In this paper, we develop RLogist, a benchmarking deep reinforcement learning (DRL) method for fast observation strategy on WSIs. Imitating the diagnostic logic of human pathologists, our RL agent learns how to find regions of observation value and obtain representative features across multiple resolution levels, without having to analyze each part of the WSI at the high magnification. We benchmark our method on two whole-slide level classification tasks, including detection of metastases in WSIs of lymph node sections, and subtyping of lung cancer. Experimental results demonstrate that RLogist achieves competitive classification performance compared to typical multiple instance learning algorithms, while having a significantly short observation path. In addition, the observation path given by RLogist provides good decision-making interpretability, and its ability of reading path navigation can potentially be used by pathologists for educational/assistive purposes. Our code is available at: \url{https://github.com/tencent-ailab/RLogist}.

</details>

<details>

<summary>2022-12-13 14:06:53 - PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate Relation Selection and Entity Boundary Detection</summary>

- *Yuquan Lan, Dongxu Li, Yunqi Zhang, Hui Zhao, Gang Zhao*

- `2211.14477v2` - [abs](http://arxiv.org/abs/2211.14477v2) - [pdf](http://arxiv.org/pdf/2211.14477v2)

> Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation triplets from unstructured texts under the zero-shot setting, where the relation sets at the training and testing stages are disjoint. Previous state-of-the-art method handles this challenging task by leveraging pretrained language models to generate data as additional training samples, which increases the training cost and severely constrains the model performance. To address the above issues, we propose a novel method named PCRED for ZeroRTE with Potential Candidate Relation Selection and Entity Boundary Detection. The remarkable characteristic of PCRED is that it does not rely on additional data and still achieves promising performance. The model adopts a relation-first paradigm, recognizing unseen relations through candidate relation selection. With this approach, the semantics of relations are naturally infused in the context. Entities are extracted based on the context and the semantics of relations subsequently. We evaluate our model on two ZeroRTE datasets. The experiment results show that our method consistently outperforms previous works. Our code will be available at https://anonymous.4open.science/r/PCRED.

</details>

<details>

<summary>2022-12-13 14:08:55 - TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning</summary>

- *Shiming Chen, Ziming Hong, Wenjin Hou, Guo-Sen Xie, Yibing Song, Jian Zhao, Xinge You, Shuicheng Yan, Ling Shao*

- `2112.08643v3` - [abs](http://arxiv.org/abs/2112.08643v3) - [pdf](http://arxiv.org/pdf/2112.08643v3)

> Zero-shot learning (ZSL) tackles the novel class recognition problem by transferring semantic knowledge from seen classes to unseen ones. Existing attention-based models have struggled to learn inferior region features in a single image by solely using unidirectional attention, which ignore the transferability and discriminative attribute localization of visual features. In this paper, we propose a cross attribute-guided Transformer network, termed TransZero++, to refine visual features and learn accurate attribute localization for semantic-augmented visual embedding representations in ZSL. TransZero++ consists of an attribute$\rightarrow$visual Transformer sub-net (AVT) and a visual$\rightarrow$attribute Transformer sub-net (VAT). Specifically, AVT first takes a feature augmentation encoder to alleviate the cross-dataset problem, and improves the transferability of visual features by reducing the entangled relative geometry relationships among region features. Then, an attribute$\rightarrow$visual decoder is employed to localize the image regions most relevant to each attribute in a given image for attribute-based visual feature representations. Analogously, VAT uses the similar feature augmentation encoder to refine the visual features, which are further applied in visual$\rightarrow$attribute decoder to learn visual-based attribute features. By further introducing semantical collaborative losses, the two attribute-guided transformers teach each other to learn semantic-augmented visual embeddings via semantical collaborative learning. Extensive experiments show that TransZero++ achieves the new state-of-the-art results on three challenging ZSL benchmarks. The codes are available at: \url{https://github.com/shiming-chen/TransZero_pp}.

</details>

<details>

<summary>2022-12-13 14:14:48 - OAMixer: Object-aware Mixing Layer for Vision Transformers</summary>

- *Hyunwoo Kang, Sangwoo Mo, Jinwoo Shin*

- `2212.06595v1` - [abs](http://arxiv.org/abs/2212.06595v1) - [pdf](http://arxiv.org/pdf/2212.06595v1)

> Patch-based models, e.g., Vision Transformers (ViTs) and Mixers, have shown impressive results on various visual recognition tasks, alternating classic convolutional networks. While the initial patch-based models (ViTs) treated all patches equally, recent studies reveal that incorporating inductive bias like spatiality benefits the representations. However, most prior works solely focused on the location of patches, overlooking the scene structure of images. Thus, we aim to further guide the interaction of patches using the object information. Specifically, we propose OAMixer (object-aware mixing layer), which calibrates the patch mixing layers of patch-based models based on the object labels. Here, we obtain the object labels in unsupervised or weakly-supervised manners, i.e., no additional human-annotating cost is necessary. Using the object labels, OAMixer computes a reweighting mask with a learnable scale parameter that intensifies the interaction of patches containing similar objects and applies the mask to the patch mixing layers. By learning an object-centric representation, we demonstrate that OAMixer improves the classification accuracy and background robustness of various patch-based models, including ViTs, MLP-Mixers, and ConvMixers. Moreover, we show that OAMixer enhances various downstream tasks, including large-scale classification, self-supervised learning, and multi-object recognition, verifying the generic applicability of OAMixer

</details>

<details>

<summary>2022-12-13 14:24:32 - Trajectory Privacy Protection Mechanism based on Social Attributes</summary>

- *Hua Wang*

- `2212.06600v1` - [abs](http://arxiv.org/abs/2212.06600v1) - [pdf](http://arxiv.org/pdf/2212.06600v1)

> The current trajectory privacy protection technology only considers the temporal and spatial attributes of trajectory data, but ignores the social attributes. However, there is an intrinsic relationship between social attributes and human activity trajectories, which brings new challenges to trajectory privacy protection, making existing trajectory privacy protection technologies unable to resist trajectory privacy attacks based on social attributes. To this end, this paper first studies the social privacy attack in the trajectory data, builds a social privacy attack model based on the fusion of "space-time" features, and reveals the internal impact of the spatial and temporal features in the trajectory data on social privacy leaks. -Anonymous algorithm and trajectory release privacy protection provide theoretical support. On this basis, integrate social attributes into trajectory privacy protection technology, design trajectory k-anonymity algorithm based on "space-time-social" three-dimensional mobile model, and construct trajectory data based on "space-time-social-semantic" multi-dimensional correlation Publish privacy-preserving models.

</details>

<details>

<summary>2022-12-13 14:26:01 - Plausible deniability for privacy-preserving data synthesis</summary>

- *Song Mei, Zhiqiang Ye*

- `2212.06604v1` - [abs](http://arxiv.org/abs/2212.06604v1) - [pdf](http://arxiv.org/pdf/2212.06604v1)

> In the field of privacy protection, publishing complete data (especially high-dimensional data sets) is one of the most challenging problems. The common encryption technology can not deal with the attacker to take differential attack to obtain sensitive information, while the existing differential privacy protection algorithm model takes a long time for high-dimensional calculation and needs to add noise to reduce data accuracy, which is not suitable for high-dimensional large data sets. In view of this situation, this paper designs a complete data synthesis scheme to protect data privacy around the concept of "plausible denial". Firstly, the paper provides the theoretical support for the difference between "plausible data" and "plausible data". In the process of scheme designing, this paper decomposes the scheme design into construction data synthesis module and privacy test module, then designs algorithm models for them respectively and realizes the function of privacy protection. When evaluating the feasibility of the scheme, the paper selects the Results of the 2013 community census in the United States as the high-dimensional data set, uses the simulation program that is based on Python to test and analyzes the efficiency and reliability of the data synthesis scheme. This portion focuses on the evaluation of the privacy protection effectiveness of the scheme.

</details>

<details>

<summary>2022-12-13 15:01:22 - Aligning Visual and Lexical Semantics</summary>

- *Fausto Giunchiglia, Mayukh Bagchi, Xiaolei Diao*

- `2212.06629v1` - [abs](http://arxiv.org/abs/2212.06629v1) - [pdf](http://arxiv.org/pdf/2212.06629v1)

> We discuss two kinds of semantics relevant to Computer Vision (CV) systems - Visual Semantics and Lexical Semantics. While visual semantics focus on how humans build concepts when using vision to perceive a target reality, lexical semantics focus on how humans build concepts of the same target reality through the use of language. The lack of coincidence between visual and lexical semantics, in turn, has a major impact on CV systems in the form of the Semantic Gap Problem (SGP). The paper, while extensively exemplifying the lack of coincidence as above, introduces a general, domain-agnostic methodology to enforce alignment between visual and lexical semantics.

</details>

<details>

<summary>2022-12-13 15:12:37 - Categorical Tools for Natural Language Processing</summary>

- *Giovanni de Felice*

- `2212.06636v1` - [abs](http://arxiv.org/abs/2212.06636v1) - [pdf](http://arxiv.org/pdf/2212.06636v1)

> This thesis develops the translation between category theory and computational linguistics as a foundation for natural language processing. The three chapters deal with syntax, semantics and pragmatics. First, string diagrams provide a unified model of syntactic structures in formal grammars. Second, functors compute semantics by turning diagrams into logical, tensor, neural or quantum computation. Third, the resulting functorial models can be composed to form games where equilibria are the solutions of language processing tasks. This framework is implemented as part of DisCoPy, the Python library for computing with string diagrams. We describe the correspondence between categorical, linguistic and computational structures, and demonstrate their applications in compositional natural language processing.

</details>

<details>

<summary>2022-12-13 16:34:39 - Benchmarking Large Language Models for Automated Verilog RTL Code Generation</summary>

- *Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg*

- `2212.11140v1` - [abs](http://arxiv.org/abs/2212.11140v1) - [pdf](http://arxiv.org/pdf/2212.11140v1)

> Automating hardware design could obviate a significant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM checkpoints are available: https://github.com/shailja-thakur/VGen.

</details>

<details>

<summary>2022-12-13 16:44:46 - Exploring Consequences of Privacy Policies with Narrative Generation via Answer Set Programming</summary>

- *Chinmaya Dabral, Emma Tosch, Chris Martens*

- `2212.06719v1` - [abs](http://arxiv.org/abs/2212.06719v1) - [pdf](http://arxiv.org/pdf/2212.06719v1)

> Informed consent has become increasingly salient for data privacy and its regulation. Entities from governments to for-profit companies have addressed concerns about data privacy with policies that enumerate the conditions for personal data storage and transfer. However, increased enumeration of and transparency in data privacy policies has not improved end-users' comprehension of how their data might be used: not only are privacy policies written in legal language that users may struggle to understand, but elements of these policies may compose in such a way that the consequences of the policy are not immediately apparent.   We present a framework that uses Answer Set Programming (ASP) -- a type of logic programming -- to formalize privacy policies. Privacy policies thus become constraints on a narrative planning space, allowing end-users to forward-simulate possible consequences of the policy in terms of actors having roles and taking actions in a domain. We demonstrate through the example of the Health Insurance Portability and Accountability Act (HIPAA) how to use the system in various ways, including asking questions about possibilities and identifying which clauses of the law are broken by a given sequence of events.

</details>

<details>

<summary>2022-12-13 17:59:20 - Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders</summary>

- *Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, Hongsheng Li*

- `2212.06785v1` - [abs](http://arxiv.org/abs/2212.06785v1) - [pdf](http://arxiv.org/pdf/2212.06785v1)

> Pre-training by numerous image data has become de-facto for robust 2D representations. In contrast, due to the expensive data acquisition and annotation, a paucity of large-scale 3D datasets severely hinders the learning for high-quality 3D features. In this paper, we propose an alternative to obtain superior 3D representations from 2D pre-trained models via Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised pre-training, we leverage the well learned 2D knowledge to guide 3D masked autoencoding, which reconstructs the masked point tokens with an encoder-decoder architecture. Specifically, we first utilize off-the-shelf 2D models to extract the multi-view visual features of the input point cloud, and then conduct two types of image-to-point learning schemes on top. For one, we introduce a 2D-guided masking strategy that maintains semantically important point tokens to be visible for the encoder. Compared to random masking, the network can better concentrate on significant 3D structures and recover the masked tokens from key spatial cues. For another, we enforce these visible tokens to reconstruct the corresponding multi-view 2D features after the decoder. This enables the network to effectively inherit high-level 2D semantics learned from rich image data for discriminative 3D modeling. Aided by our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning, achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to the fully trained results of existing methods. By further fine-tuning on on ScanObjectNN's hardest split, I2P-MAE attains the state-of-the-art 90.11% accuracy, +3.68% to the second-best, demonstrating superior transferable capacity. Code will be available at https://github.com/ZrrSkywalker/I2P-MAE.

</details>

<details>

<summary>2022-12-13 18:18:50 - The Importance of Image Interpretation: Patterns of Semantic Misclassification in Real-World Adversarial Images</summary>

- *Zhengyu Zhao, Nga Dang, Martha Larson*

- `2206.01467v2` - [abs](http://arxiv.org/abs/2206.01467v2) - [pdf](http://arxiv.org/pdf/2206.01467v2)

> Adversarial images are created with the intention of causing an image classifier to produce a misclassification. In this paper, we propose that adversarial images should be evaluated based on semantic mismatch, rather than label mismatch, as used in current work. In other words, we propose that an image of a "mug" would be considered adversarial if classified as "turnip", but not as "cup", as current systems would assume. Our novel idea of taking semantic misclassification into account in the evaluation of adversarial images offers two benefits. First, it is a more realistic conceptualization of what makes an image adversarial, which is important in order to fully understand the implications of adversarial images for security and privacy. Second, it makes it possible to evaluate the transferability of adversarial images to a real-world classifier, without requiring the classifier's label set to have been available during the creation of the images. The paper carries out an evaluation of a transfer attack on a real-world image classifier that is made possible by our semantic misclassification approach. The attack reveals patterns in the semantics of adversarial misclassifications that could not be investigated using conventional label mismatch.

</details>

<details>

<summary>2022-12-13 18:45:00 - Towards Efficient and Domain-Agnostic Evasion Attack with High-dimensional Categorical Inputs</summary>

- *Hongyan Bao, Yufei Han, Yujun Zhou, Xin Gao, Xiangliang Zhang*

- `2212.06836v1` - [abs](http://arxiv.org/abs/2212.06836v1) - [pdf](http://arxiv.org/pdf/2212.06836v1)

> Our work targets at searching feasible adversarial perturbation to attack a classifier with high-dimensional categorical inputs in a domain-agnostic setting. This is intrinsically an NP-hard knapsack problem where the exploration space becomes explosively larger as the feature dimension increases. Without the help of domain knowledge, solving this problem via heuristic method, such as Branch-and-Bound, suffers from exponential complexity, yet can bring arbitrarily bad attack results. We address the challenge via the lens of multi-armed bandit based combinatorial search. Our proposed method, namely FEAT, treats modifying each categorical feature as pulling an arm in multi-armed bandit programming. Our objective is to achieve highly efficient and effective attack using an Orthogonal Matching Pursuit (OMP)-enhanced Upper Confidence Bound (UCB) exploration strategy. Our theoretical analysis bounding the regret gap of FEAT guarantees its practical attack performance. In empirical analysis, we compare FEAT with other state-of-the-art domain-agnostic attack methods over various real-world categorical data sets of different applications. Substantial experimental observations confirm the expected efficiency and attack effectiveness of FEAT applied in different application scenarios. Our work further hints the applicability of FEAT for assessing the adversarial vulnerability of classification systems with high-dimensional categorical inputs.

</details>

<details>

<summary>2022-12-13 21:21:07 - Enabling the Wireless Metaverse via Semantic Multiverse Communication</summary>

- *Jihong Park, Jinho Choi, Seong-Lyun Kim, Mehdi Bennis*

- `2212.06908v1` - [abs](http://arxiv.org/abs/2212.06908v1) - [pdf](http://arxiv.org/pdf/2212.06908v1)

> Metaverse over wireless networks is an emerging use case of the sixth generation (6G) wireless systems, posing unprecedented challenges in terms of its multi-modal data transmissions with stringent latency and reliability requirements. Towards enabling this wireless metaverse, in this article we propose a novel semantic communication (SC) framework by decomposing the metaverse into human/machine agent-specific semantic multiverses (SMs). An SM stored at each agent comprises a semantic encoder and a generator, leveraging recent advances in generative artificial intelligence (AI). To improve communication efficiency, the encoder learns the semantic representations (SRs) of multi-modal data, while the generator learns how to manipulate them for locally rendering scenes and interactions in the metaverse. Since these learned SMs are biased towards local environments, their success hinges on synchronizing heterogeneous SMs in the background while communicating SRs in the foreground, turning the wireless metaverse problem into the problem of semantic multiverse communication (SMC). Based on this SMC architecture, we propose several promising algorithmic and analytic tools for modeling and designing SMC, ranging from distributed learning and multi-agent reinforcement learning (MARL) to signaling games and symbolic AI.

</details>

<details>

<summary>2022-12-13 23:18:51 - The Role of Lookahead and Approximate Policy Evaluation in Reinforcement Learning with Linear Value Function Approximation</summary>

- *Anna Winnicki, Joseph Lubars, Michael Livesay, R. Srikant*

- `2109.13419v7` - [abs](http://arxiv.org/abs/2109.13419v7) - [pdf](http://arxiv.org/pdf/2109.13419v7)

> Function approximation is widely used in reinforcement learning to handle the computational difficulties associated with very large state spaces. However, function approximation introduces errors which may lead to instabilities when using approximate dynamic programming techniques to obtain the optimal policy. Therefore, techniques such as lookahead for policy improvement and m-step rollout for policy evaluation are used in practice to improve the performance of approximate dynamic programming with function approximation. We quantitatively characterize, for the first time, the impact of lookahead and m-step rollout on the performance of approximate dynamic programming (DP) with function approximation: (i) without a sufficient combination of lookahead and m-step rollout, approximate DP may not converge, (ii) both lookahead and m-step rollout improve the convergence rate of approximate DP, and (iii) lookahead helps mitigate the effect of function approximation and the discount factor on the asymptotic performance of the algorithm. Our results are presented for two approximate DP methods: one which uses least-squares regression to perform function approximation and another which performs several steps of gradient descent of the least-squares objective in each iteration.

</details>

<details>

<summary>2022-12-13 23:19:15 - Heuristically Guided Compilation for Multi-Agent Path Finding</summary>

- *Pavel Surynek*

- `2212.06940v1` - [abs](http://arxiv.org/abs/2212.06940v1) - [pdf](http://arxiv.org/pdf/2212.06940v1)

> Multi-agent path finding (MAPF) is a task of finding non-conflicting paths connecting agents' specified initial and goal positions in a shared environment. We focus on compilation-based solvers in which the MAPF problem is expressed in a different well established formalism such as mixed-integer linear programming (MILP), Boolean satisfiability (SAT), or constraint programming (CP). As the target solvers for these formalisms act as black-boxes it is challenging to integrate MAPF specific heuristics in the MAPF compilation-based solvers. We show in this work how the build a MAPF encoding for the target SAT solver in which domain specific heuristic knowledge is reflected. The heuristic knowledge is transferred to the SAT solver by selecting candidate paths for each agent and by constructing the encoding only for these candidate paths instead of constructing the encoding for all possible paths for an agent. The conducted experiments show that heuristically guided compilation outperforms the vanilla variants of the SAT-based MAPF solver.

</details>

<details>

<summary>2022-12-14 02:03:39 - Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation</summary>

- *Junru Lu, Xingwei Tan, Gabriele Pergola, Lin Gui, Yulan He*

- `2210.12902v2` - [abs](http://arxiv.org/abs/2210.12902v2) - [pdf](http://arxiv.org/pdf/2210.12902v2)

> Human reading comprehension often requires reasoning of event semantic relations in narratives, represented by Event-centric Question-Answering (QA). To address event-centric QA, we propose a novel QA model with contrastive learning and invertible event transformation, call TranCLR. Our proposed model utilizes an invertible transformation matrix to project semantic vectors of events into a common event embedding space, trained with contrastive learning, and thus naturally inject event semantic knowledge into mainstream QA pipelines. The transformation matrix is fine-tuned with the annotated event relation types between events that occurred in questions and those in answers, using event-aware question vectors. Experimental results on the Event Semantic Relation Reasoning (ESTER) dataset show significant improvements in both generative and extractive settings compared to the existing strong baselines, achieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact Match (EM) score under the multi-answer setting. Qualitative analysis reveals the high quality of the generated answers by TranCLR, demonstrating the feasibility of injecting event knowledge into QA model learning. Our code and models can be found at https://github.com/LuJunru/TranCLR.

</details>

<details>

<summary>2022-12-14 03:17:16 - Unsupervised Multi-Granularity Summarization</summary>

- *Ming Zhong, Yang Liu, Suyu Ge, Yuning Mao, Yizhu Jiao, Xingxing Zhang, Yichong Xu, Chenguang Zhu, Michael Zeng, Jiawei Han*

- `2201.12502v3` - [abs](http://arxiv.org/abs/2201.12502v3) - [pdf](http://arxiv.org/pdf/2201.12502v3)

> Text summarization is a user-preference based task, i.e., for one document, users often have different priorities for summary. As a key aspect of customization in summarization, granularity is used to measure the semantic coverage between the summary and source document. However, developing systems that can generate summaries with customizable semantic coverage is still an under-explored topic. In this paper, we propose the first unsupervised multi-granularity summarization framework, GranuSum. We take events as the basic semantic units of the source documents and propose to rank these events by their salience. We also develop a model to summarize input documents with given events as anchors and hints. By inputting different numbers of events, GranuSum is capable of producing multi-granular summaries in an unsupervised manner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple summaries at different granularities for each document cluster. Experimental results confirm the substantial superiority of GranuSum on multi-granularity summarization over strong baselines. Further, by exploiting the event information, GranuSum also exhibits state-of-the-art performance under the conventional unsupervised abstractive setting. Dataset for this paper can be found at: https://github.com/maszhongming/GranuDUC

</details>

<details>

<summary>2022-12-14 03:51:15 - Towards improving discriminative reconstruction via simultaneous dense and sparse coding</summary>

- *Abiy Tasissa, Emmanouil Theodosis, Bahareh Tolooshams, Demba Ba*

- `2006.09534v3` - [abs](http://arxiv.org/abs/2006.09534v3) - [pdf](http://arxiv.org/pdf/2006.09534v3)

> Discriminative features extracted from the sparse coding model have been shown to perform well for classification. Recent deep learning architectures have further improved reconstruction in inverse problems by considering new dense priors learned from data. We propose a novel dense and sparse coding model that integrates both representation capability and discriminative features. The model studies the problem of recovering a dense vector $\mathbf{x}$ and a sparse vector $\mathbf{u}$ given measurements of the form $\mathbf{y} = \mathbf{A}\mathbf{x}+\mathbf{B}\mathbf{u}$. Our first analysis proposes a geometric condition based on the minimal angle between spanning subspaces corresponding to the matrices $\mathbf{A}$ and $\mathbf{B}$ that guarantees unique solution to the model. The second analysis shows that, under mild assumptions, a convex program recovers the dense and sparse components. We validate the effectiveness of the model on simulated data and propose a dense and sparse autoencoder (DenSaE) tailored to learning the dictionaries from the dense and sparse model. We demonstrate that (i) DenSaE denoises natural images better than architectures derived from the sparse coding model ($\mathbf{B}\mathbf{u}$), (ii) in the presence of noise, training the biases in the latter amounts to implicitly learning the $\mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u}$ model, (iii) $\mathbf{A}$ and $\mathbf{B}$ capture low- and high-frequency contents, respectively, and (iv) compared to the sparse coding model, DenSaE offers a balance between discriminative power and representation.

</details>

<details>

<summary>2022-12-14 04:01:19 - Learning and Predicting Multimodal Vehicle Action Distributions in a Unified Probabilistic Model Without Labels</summary>

- *Charles Richter, Patrick R. BarragÃ¡n, Sertac Karaman*

- `2212.07013v1` - [abs](http://arxiv.org/abs/2212.07013v1) - [pdf](http://arxiv.org/pdf/2212.07013v1)

> We present a unified probabilistic model that learns a representative set of discrete vehicle actions and predicts the probability of each action given a particular scenario. Our model also enables us to estimate the distribution over continuous trajectories conditioned on a scenario, representing what each discrete action would look like if executed in that scenario. While our primary objective is to learn representative action sets, these capabilities combine to produce accurate multimodal trajectory predictions as a byproduct. Although our learned action representations closely resemble semantically meaningful categories (e.g., "go straight", "turn left", etc.), our method is entirely self-supervised and does not utilize any manually generated labels or categories. Our method builds upon recent advances in variational inference and deep unsupervised clustering, resulting in full distribution estimates based on deterministic model evaluations.

</details>

<details>

<summary>2022-12-14 05:03:46 - Non-conforming multipatches for NURBS-based finite element analysis of higher-order phase-field models for brittle fracture</summary>

- *Khuong D. Nguyen, Charles E. Augarde, William M. Coombs, H. Nguyen-Xuan, M. Abdel-Wahab*

- `2212.07034v1` - [abs](http://arxiv.org/abs/2212.07034v1) - [pdf](http://arxiv.org/pdf/2212.07034v1)

> This paper proposes an effective computational tool for brittle crack propagation problems based on a combination of a higher-order phase-field model and a non-conforming mesh using a NURBS-based isogeometric approach. This combination, as demonstrated in this paper, is of great benefit in reducing the computational cost of using a local refinement mesh and a higher-order phase-field, which needs higher derivatives of basis functions. Compared with other approaches using a local refinement mesh, the Virtual Uncommon-Knot-Inserted Master-Slave (VUKIMS) method presented here is not only simple to implement but can also reduce the variable numbers. VUKIMS is an outstanding choice in order to establish a local refinement mesh, i.e. a non-conforming mesh, in a multi-patch problem. A phase-field model is an efficient approach for various complicated crack patterns, including those with or without an initial crack path, curved cracks, crack coalescence, and crack propagation through holes. The paper demonstrates that cubic NURBS elements are ideal for balancing the computational cost and the accuracy because they can produce accurate solutions by utilising a lower degree of freedom number than an extremely fine mesh of first-order B-spline elements.

</details>

<details>

<summary>2022-12-14 11:10:03 - MIST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text</summary>

- *Sophie Henning, Nicole Macher, Stefan GrÃ¼newald, Annemarie Friedrich*

- `2212.07156v1` - [abs](http://arxiv.org/abs/2212.07156v1) - [pdf](http://arxiv.org/pdf/2212.07156v1)

> Modal verbs (e.g., "can", "should", or "must") occur highly frequently in scientific articles. Decoding their function is not straightforward: they are often used for hedging, but they may also denote abilities and restrictions. Understanding their meaning is important for various NLP tasks such as writing assistance or accurate information extraction from scientific text.   To foster research on the usage of modals in this genre, we introduce the MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances in five scientific domains annotated for their semantic, pragmatic, or rhetorical function. We systematically evaluate a set of competitive neural architectures on MIST. Transfer experiments reveal that leveraging non-scientific data is of limited benefit for modeling the distinctions in MIST. Our corpus analysis provides evidence that scientific communities differ in their usage of modal verbs, yet, classifiers trained on scientific data generalize to some extent to unseen scientific domains.

</details>

<details>

<summary>2022-12-14 13:48:32 - Evaluating Byte and Wordpiece Level Models for Massively Multilingual Semantic Parsing</summary>

- *Massimo Nicosia, Francesco Piccinno*

- `2212.07223v1` - [abs](http://arxiv.org/abs/2212.07223v1) - [pdf](http://arxiv.org/pdf/2212.07223v1)

> Token free approaches have been successfully applied to a series of word and span level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece based (mT5) sequence to sequence model on the 51 languages of the MASSIVE multilingual semantic parsing dataset. We examine multiple experimental settings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic data. By leveraging a state-of-the-art label projection method for machine translated examples, we are able to reduce the gap in exact match accuracy to only 5 points with respect to a model trained on gold data from all the languages. We additionally provide insights on the cross-lingual transfer of ByT5 and show how the model compares with respect to mT5 across all parameter sizes.

</details>

<details>

<summary>2022-12-14 17:13:22 - Counterfactual Explanations for Support Vector Machine Models</summary>

- *Sebastian Salazar, Samuel Denton, Ansaf Salleb-Aouissi*

- `2212.07432v1` - [abs](http://arxiv.org/abs/2212.07432v1) - [pdf](http://arxiv.org/pdf/2212.07432v1)

> We tackle the problem of computing counterfactual explanations -- minimal changes to the features that flip an undesirable model prediction. We propose a solution to this question for linear Support Vector Machine (SVMs) models. Moreover, we introduce a way to account for weighted actions that allow for more changes in certain features than others. In particular, we show how to find counterfactual explanations with the purpose of increasing model interpretability. These explanations are valid, change only actionable features, are close to the data distribution, sparse, and take into account correlations between features. We cast this as a mixed integer programming optimization problem. Additionally, we introduce two novel scale-invariant cost functions for assessing the quality of counterfactual explanations and use them to evaluate the quality of our approach with a real medical dataset. Finally, we build a support vector machine model to predict whether law students will pass the Bar exam using protected features, and used our algorithms to uncover the inherent biases of the SVM.

</details>

<details>

<summary>2022-12-14 18:46:14 - Cross-Domain Transfer via Semantic Skill Imitation</summary>

- *Karl Pertsch, Ruta Desai, Vikash Kumar, Franziska Meier, Joseph J. Lim, Dhruv Batra, Akshara Rai*

- `2212.07407v1` - [abs](http://arxiv.org/abs/2212.07407v1) - [pdf](http://arxiv.org/pdf/2212.07407v1)

> We propose an approach for semantic imitation, which uses demonstrations from a source domain, e.g. human videos, to accelerate reinforcement learning (RL) in a different target domain, e.g. a robotic manipulator in a simulated kitchen. Instead of imitating low-level actions like joint velocities, our approach imitates the sequence of demonstrated semantic skills like "opening the microwave" or "turning on the stove". This allows us to transfer demonstrations across environments (e.g. real-world to simulated kitchen) and agent embodiments (e.g. bimanual human demonstration to robotic arm). We evaluate on three challenging cross-domain learning problems and match the performance of demonstration-accelerated RL approaches that require in-domain demonstrations. In a simulated kitchen environment, our approach learns long-horizon robot manipulation tasks, using less than 3 minutes of human video demonstrations from a real-world kitchen. This enables scaling robot learning via the reuse of demonstrations, e.g. collected as human videos, for learning in any number of target domains.

</details>

<details>

<summary>2022-12-14 19:43:43 - MABSplit: Faster Forest Training Using Multi-Armed Bandits</summary>

- *Mo Tiwari, Ryan Kang, Je-Yong Lee, Sebastian Thrun, Chris Piech, Ilan Shomorony, Martin Jinye Zhang*

- `2212.07473v1` - [abs](http://arxiv.org/abs/2212.07473v1) - [pdf](http://arxiv.org/pdf/2212.07473v1)

> Random forests are some of the most widely used machine learning models today, especially in domains that necessitate interpretability. We present an algorithm that accelerates the training of random forests and other popular tree-based learning methods. At the core of our algorithm is a novel node-splitting subroutine, dubbed MABSplit, used to efficiently find split points when constructing decision trees. Our algorithm borrows techniques from the multi-armed bandit literature to judiciously determine how to allocate samples and computational power across candidate split points. We provide theoretical guarantees that MABSplit improves the sample complexity of each node split from linear to logarithmic in the number of data points. In some settings, MABSplit leads to 100x faster training (an 99% reduction in training time) without any decrease in generalization performance. We demonstrate similar speedups when MABSplit is used across a variety of forest-based variants, such as Extremely Random Forests and Random Patches. We also show our algorithm can be used in both classification and regression tasks. Finally, we show that MABSplit outperforms existing methods in generalization performance and feature importance calculations under a fixed computational budget. All of our experimental results are reproducible via a one-line script at https://github.com/ThrunGroup/FastForest.

</details>

<details>

<summary>2022-12-14 21:13:08 - Artificial Intelligence for Health Message Generation: Theory, Method, and an Empirical Study Using Prompt Engineering</summary>

- *Sue Lim, Ralf SchmÃ¤lzle*

- `2212.07507v1` - [abs](http://arxiv.org/abs/2212.07507v1) - [pdf](http://arxiv.org/pdf/2212.07507v1)

> This study introduces and examines the potential of an AI system to generate health awareness messages. The topic of folic acid, a vitamin that is critical during pregnancy, served as a test case. Using prompt engineering, we generated messages that could be used to raise awareness and compared them to retweeted human-generated messages via computational and human evaluation methods. The system was easy to use and prolific, and computational analyses revealed that the AI-generated messages were on par with human-generated ones in terms of sentiment, reading ease, and semantic content. Also, the human evaluation study showed that AI-generated messages ranked higher in message quality and clarity. We discuss the theoretical, practical, and ethical implications of these results.

</details>

<details>

<summary>2022-12-14 22:10:46 - Many-valued Argumentation, Conditionals and a Probabilistic Semantics for Gradual Argumentation</summary>

- *Mario Alviano, Laura Giordano, Daniele Theseider DuprÃ©*

- `2212.07523v1` - [abs](http://arxiv.org/abs/2212.07523v1) - [pdf](http://arxiv.org/pdf/2212.07523v1)

> In this paper we propose a general approach to define a many-valued preferential interpretation of gradual argumentation semantics. The approach allows for conditional reasoning over arguments and boolean combination of arguments, with respect to a class of gradual semantics, through the verification of graded (strict or defeasible) implications over a preferential interpretation. As a proof of concept, in the finitely-valued case, an Answer set Programming approach is proposed for conditional reasoning in a many-valued argumentation semantics of weighted argumentation graphs. The paper also develops and discusses a probabilistic semantics for gradual argumentation, which builds on the many-valued conditional semantics.

</details>

<details>

<summary>2022-12-14 23:31:14 - Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology</summary>

- *Valentin Hofmann, Janet B. Pierrehumbert, Hinrich SchÃ¼tze*

- `2212.07547v1` - [abs](http://arxiv.org/abs/2212.07547v1) - [pdf](http://arxiv.org/pdf/2212.07547v1)

> We propose a fully unsupervised method to detect bias in contextualized embeddings. The method leverages the assortative information latently encoded by social networks and combines orthogonality regularization, structured sparsity learning, and graph neural networks to find the embedding subspace capturing this information. As a concrete example, we focus on the phenomenon of ideological bias: we introduce the concept of an ideological subspace, show how it can be found by applying our method to online discussion forums, and present techniques to probe it. Our experiments suggest that the ideological subspace encodes abstract evaluative semantics and reflects changes in the political left-right spectrum during the presidency of Donald Trump.

</details>

<details>

<summary>2022-12-15 01:56:22 - Learning to Detect Semantic Boundaries with Image-level Class Labels</summary>

- *Namyup Kim, Sehyun Hwang, Suha Kwak*

- `2212.07579v1` - [abs](http://arxiv.org/abs/2212.07579v1) - [pdf](http://arxiv.org/pdf/2212.07579v1)

> This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.

</details>

<details>

<summary>2022-12-15 02:33:40 - Continual Learning with Evolving Class Ontologies</summary>

- *Zhiqiu Lin, Deepak Pathak, Yu-Xiong Wang, Deva Ramanan, Shu Kong*

- `2210.04993v4` - [abs](http://arxiv.org/abs/2210.04993v4) - [pdf](http://arxiv.org/pdf/2210.04993v4)

> Lifelong learners must recognize concept vocabularies that evolve over time. A common yet underexplored scenario is learning with class labels that continually refine/expand old classes. For example, humans learn to recognize ${\tt dog}$ before dog breeds. In practical settings, dataset $\textit{versioning}$ often introduces refinement to ontologies, such as autonomous vehicle benchmarks that refine a previous ${\tt vehicle}$ class into ${\tt school-bus}$ as autonomous operations expand to new cities. This paper formalizes a protocol for studying the problem of $\textit{Learning with Evolving Class Ontology}$ (LECO). LECO requires learning classifiers in distinct time periods (TPs); each TP introduces a new ontology of "fine" labels that refines old ontologies of "coarse" labels (e.g., dog breeds that refine the previous ${\tt dog}$). LECO explores such questions as whether to annotate new data or relabel the old, how to leverage coarse labels, and whether to finetune the previous TP's model or train from scratch. To answer these questions, we leverage insights from related problems such as class-incremental learning. We validate them under the LECO protocol through the lens of image classification (CIFAR and iNaturalist) and semantic segmentation (Mapillary). Our experiments lead to surprising conclusions; while the current status quo is to relabel existing datasets with new ontologies (such as COCO-to-LVIS or Mapillary1.2-to-2.0), LECO demonstrates that a far better strategy is to annotate $\textit{new}$ data with the new ontology. However, this produces an aggregate dataset with inconsistent old-vs-new labels, complicating learning. To address this challenge, we adopt methods from semi-supervised and partial-label learning. Such strategies can surprisingly be made near-optimal, approaching an "oracle" that learns on the aggregate dataset exhaustively labeled with the newest ontology.

</details>

<details>

<summary>2022-12-15 06:53:30 - Certified Monotonic Neural Networks</summary>

- *Xingchao Liu, Xing Han, Na Zhang, Qiang Liu*

- `2011.10219v2` - [abs](http://arxiv.org/abs/2011.10219v2) - [pdf](http://arxiv.org/pdf/2011.10219v2)

> Learning monotonic models with respect to a subset of the inputs is a desirable feature to effectively address the fairness, interpretability, and generalization issues in practice. Existing methods for learning monotonic neural networks either require specifically designed model structures to ensure monotonicity, which can be too restrictive/complicated, or enforce monotonicity by adjusting the learning process, which cannot provably guarantee the learned model is monotonic on selected features. In this work, we propose to certify the monotonicity of the general piece-wise linear neural networks by solving a mixed integer linear programming problem.This provides a new general approach for learning monotonic neural networks with arbitrary model structures. Our method allows us to train neural networks with heuristic monotonicity regularizations, and we can gradually increase the regularization magnitude until the learned network is certified monotonic. Compared to prior works, our approach does not require human-designed constraints on the weight space and also yields more accurate approximation. Empirical studies on various datasets demonstrate the efficiency of our approach over the state-of-the-art methods, such as Deep Lattice Networks.

</details>

<details>

<summary>2022-12-15 07:21:01 - Distortion-Aware Network Pruning and Feature Reuse for Real-time Video Segmentation</summary>

- *Hyunsu Rhee, Dongchan Min, Sunil Hwang, Bruno Andreis, Sung Ju Hwang*

- `2206.09604v2` - [abs](http://arxiv.org/abs/2206.09604v2) - [pdf](http://arxiv.org/pdf/2206.09604v2)

> Real-time video segmentation is a crucial task for many real-world applications such as autonomous driving and robot control. Since state-of-the-art semantic segmentation models are often too heavy for real-time applications despite their impressive performance, researchers have proposed lightweight architectures with speed-accuracy trade-offs, achieving real-time speed at the expense of reduced accuracy. In this paper, we propose a novel framework to speed up any architecture with skip-connections for real-time vision tasks by exploiting the temporal locality in videos. Specifically, at the arrival of each frame, we transform the features from the previous frame to reuse them at specific spatial bins. We then perform partial computation of the backbone network on the regions of the current frame that captures temporal differences between the current and previous frame. This is done by dynamically dropping out residual blocks using a gating mechanism which decides which blocks to drop based on inter-frame distortion. We validate our Spatial-Temporal Mask Generator (STMG) on video semantic segmentation benchmarks with multiple backbone networks, and show that our method largely speeds up inference with minimal loss of accuracy.

</details>

<details>

<summary>2022-12-15 10:59:33 - Assessing the Maturity of Digital Twinning Solutions for Ports</summary>

- *Robert Klar, Anna Fredriksson, Vangelis Angelakis*

- `2212.07722v1` - [abs](http://arxiv.org/abs/2212.07722v1) - [pdf](http://arxiv.org/pdf/2212.07722v1)

> Ports are striving for innovative technological solutions to cope with the increasing growth in demand of goods transport, while at the same time improving their environmental footprint. An emerging technology that has the potential to substantially increase the effectiveness of the multifaceted and interconnected port processes is that of digital twins. Innovation-leading ports recognizing the potential of twinning have already started working on it. However, since there is no clear consensus on what a digital twin of a complex system comprises and how it should be designed, deployed digital twin solutions for ports often differ significantly. This article addresses this issue by initially identifying three core aspect underpinning digital twins of complex systems, such as ports, and outlining five successive maturity levels based on these aspects' instantiation. These identified aspects and the derived maturity levels are then used to examine real-world cases by critically evaluating existing digital twinning solutions in the port of Singapore, the Mawan port of Shanghai, and that of Rotterdam. These being three of the world's innovation-leading ports, we naturally find in them most of the identified core aspects to be in line with their twinning implementation, which has reached, in all three, a higher level of maturity. Although, our work on maturity levels and core aspects can provide a guideline for designing and benchmarking future digital twinning solutions for ports, the capacity for innovation via twinning, even in the port domain, is highly contextual with key paragon being the availability of financial and technical resources.

</details>

<details>

<summary>2022-12-15 12:22:08 - Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion</summary>

- *Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, Bo Dai*

- `2212.07409v2` - [abs](http://arxiv.org/abs/2212.07409v2) - [pdf](http://arxiv.org/pdf/2212.07409v2)

> StyleGAN has achieved great progress in 2D face reconstruction and semantic editing via image inversion and latent editing. While studies over extending 2D StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion framework is still missing, limiting the applications of 3D face reconstruction and semantic editing. In this paper, we study the challenging problem of 3D GAN inversion where a latent code is predicted given a single face image to faithfully recover its 3D shapes and detailed textures. The problem is ill-posed: innumerable compositions of shape and texture could be rendered to the current image. Furthermore, with the limited capacity of a global latent code, 2D inversion methods cannot preserve faithful shape and texture at the same time when applied to 3D models. To solve this problem, we devise an effective self-training scheme to constrain the learning of inversion. The learning is done efficiently without any real-world 2D-3D training pairs but proxy samples generated from a 3D GAN. In addition, apart from a global latent code that captures the coarse shape and texture information, we augment the generation network with a local branch, where pixel-aligned features are added to faithfully reconstruct face details. We further consider a new pipeline to perform 3D view-consistent editing. Extensive experiments show that our method outperforms state-of-the-art inversion methods in both shape and texture reconstruction quality. Code and data will be released.

</details>

<details>

<summary>2022-12-15 12:28:37 - Synthesizing Research on Programmers' Mental Models of Programs, Tasks and Concepts -- a Systematic Literature Review</summary>

- *Ava Heinonen, Bettina LehtelÃ¤, Arto Hellas, Fabian Fagerholm*

- `2212.07763v1` - [abs](http://arxiv.org/abs/2212.07763v1) - [pdf](http://arxiv.org/pdf/2212.07763v1)

> Programmers' mental models represent their knowledge and understanding of programs, programming concepts, and programming in general. They guide programmers' work and influence their task performance. Understanding mental models is important for designing work systems and practices that support programmers. Although the importance of programmers' mental models is widely acknowledged, research on mental models has decreased over the years. The results are scattered and do not take into account recent developments in software engineering. We analyze the state of research into programmers' mental models and provide an overview of existing research. We connect results on mental models from different strands of research to form a more unified knowledge base on the topic. We conducted a systematic literature review on programmers' mental models. We analyzed literature addressing mental models in different contexts, including mental models of programs, programming tasks, and programming concepts. Using nine search engines, we found 3678 articles (excluding duplicates). 84 were selected for further analysis. Using the snowballing technique, we obtained a final result set containing 187 articles. We show that the literature shares a kernel of shared understanding of mental models. By collating and connecting results on mental models from different fields of research, we uncovered some well-researched aspects, which we argue are fundamental characteristics of programmers' mental models. This work provides a basis for future work on mental models. The research field on programmers' mental models still faces many challenges rising from a lack of a shared knowledge base and poorly defined constructs. We created a unified knowledge base on the topic. We also point to directions for future studies. In particular, we call for studies that examine programmers working with modern practices and tools.

</details>

<details>

<summary>2022-12-15 14:19:33 - The effects of gender bias in word embeddings on depression prediction</summary>

- *Gizem Sogancioglu, Heysem Kaya*

- `2212.07852v1` - [abs](http://arxiv.org/abs/2212.07852v1) - [pdf](http://arxiv.org/pdf/2212.07852v1)

> Word embeddings are extensively used in various NLP problems as a state-of-the-art semantic feature vector representation. Despite their success on various tasks and domains, they might exhibit an undesired bias for stereotypical categories due to statistical and societal biases that exist in the dataset they are trained on. In this study, we analyze the gender bias in four different pre-trained word embeddings specifically for the depression category in the mental disorder domain. We use contextual and non-contextual embeddings that are trained on domain-independent as well as clinical domain-specific data. We observe that embeddings carry bias for depression towards different gender groups depending on the type of embeddings. Moreover, we demonstrate that these undesired correlations are transferred to the downstream task for depression phenotype recognition. We find that data augmentation by simply swapping gender words mitigates the bias significantly in the downstream task.

</details>

<details>

<summary>2022-12-15 15:25:28 - EpiGRAF: Rethinking training of 3D GANs</summary>

- *Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, Peter Wonka*

- `2206.10535v2` - [abs](http://arxiv.org/abs/2206.10535v2) - [pdf](http://arxiv.org/pdf/2206.10535v2)

> A very recent trend in generative modeling is building 3D-aware generators from 2D image collections. To induce the 3D bias, such models typically rely on volumetric rendering, which is expensive to employ at high resolutions. During the past months, there appeared more than 10 works that address this scaling issue by training a separate 2D decoder to upsample a low-resolution image (or a feature tensor) produced from a pure 3D generator. But this solution comes at a cost: not only does it break multi-view consistency (i.e. shape and texture change when the camera moves), but it also learns the geometry in a low fidelity. In this work, we show that it is possible to obtain a high-resolution 3D generator with SotA image quality by following a completely different route of simply training the model patch-wise. We revisit and improve this optimization scheme in two ways. First, we design a location- and scale-aware discriminator to work on patches of different proportions and spatial positions. Second, we modify the patch sampling strategy based on an annealed beta distribution to stabilize training and accelerate the convergence. The resulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D generator, and we test it on four datasets (two introduced in this work) at $256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality, high-fidelity geometry and trains ${\approx} 2.5 \times$ faster than the upsampler-based counterparts. Project website: https://universome.github.io/epigraf.

</details>

<details>

<summary>2022-12-15 16:17:03 - RWEN-TTS: Relation-aware Word Encoding Network for Natural Text-to-Speech Synthesis</summary>

- *Shinhyeok Oh, HyeongRae Noh, Yoonseok Hong, Insoo Oh*

- `2212.07939v1` - [abs](http://arxiv.org/abs/2212.07939v1) - [pdf](http://arxiv.org/pdf/2212.07939v1)

> With the advent of deep learning, a huge number of text-to-speech (TTS) models which produce human-like speech have emerged. Recently, by introducing syntactic and semantic information w.r.t the input text, various approaches have been proposed to enrich the naturalness and expressiveness of TTS models. Although these strategies showed impressive results, they still have some limitations in utilizing language information. First, most approaches only use graph networks to utilize syntactic and semantic information without considering linguistic features. Second, most previous works do not explicitly consider adjacent words when encoding syntactic and semantic information, even though it is obvious that adjacent words are usually meaningful when encoding the current word. To address these issues, we propose Relation-aware Word Encoding Network (RWEN), which effectively allows syntactic and semantic information based on two modules (i.e., Semantic-level Relation Encoding and Adjacent Word Relation Encoding). Experimental results show substantial improvements compared to previous works.

</details>

<details>

<summary>2022-12-15 16:34:39 - A Data Source Dependency Analysis Framework for Large Scale Data Science Projects</summary>

- *Laurent BouÃ©, Pratap Kunireddy, Pavle SubotiÄ*

- `2212.07951v1` - [abs](http://arxiv.org/abs/2212.07951v1) - [pdf](http://arxiv.org/pdf/2212.07951v1)

> Dependency hell is a well-known pain point in the development of large software projects and machine learning (ML) code bases are not immune from it. In fact, ML applications suffer from an additional form, namely, "data source dependency hell". This term refers to the central role played by data and its unique quirks that often lead to unexpected failures of ML models which cannot be explained by code changes. In this paper, we present an automated dependency mapping framework that allows MLOps engineers to monitor the whole dependency map of their models in a fast paced engineering environment and thus mitigate ahead of time the consequences of any data source changes (e.g., re-train model, ignore data, set default data etc.). Our system is based on a unified and generic approach, employing techniques from static analysis, from which data sources can be identified reliably for any type of dependency on a wide range of source languages and artefacts. The dependency mapping framework is exposed as a REST web API where the only input is the path to the Git repository hosting the code base. Currently used by MLOps engineers at Microsoft, we expect such dependency map APIs to be adopted more widely by MLOps engineers in the future.

</details>

<details>

<summary>2022-12-15 17:21:52 - Improving Developers' Understanding of Regex Denial of Service Tools through Anti-Patterns and Fix Strategies</summary>

- *Sk Adnan Hassan, Zainab Aamir, Dongyoon Lee, James C. Davis, Francisco Servant*

- `2212.07979v1` - [abs](http://arxiv.org/abs/2212.07979v1) - [pdf](http://arxiv.org/pdf/2212.07979v1)

> Regular expressions are used for diverse purposes, including input validation and firewalls. Unfortunately, they can also lead to a security vulnerability called ReDoS (Regular Expression Denial of Service), caused by a super-linear worst-case execution time during regex matching. Due to the severity and prevalence of ReDoS, past work proposed automatic tools to detect and fix regexes. Although these tools were evaluated in automatic experiments, their usability has not yet been studied; usability has not been a focus of prior work. Our insight is that the usability of existing tools to detect and fix regexes will improve if we complement them with anti-patterns and fix strategies of vulnerable regexes. We developed novel anti-patterns for vulnerable regexes, and a collection of fix strategies to fix them. We derived our anti-patterns and fix strategies from a novel theory of regex infinite ambiguity - a necessary condition for regexes vulnerable to ReDoS. We proved the soundness and completeness of our theory. We evaluated the effectiveness of our anti-patterns, both in an automatic experiment and when applied manually. Then, we evaluated how much our anti-patterns and fix strategies improve developers' understanding of the outcome of detection and fixing tools. Our evaluation found that our anti-patterns were effective over a large dataset of regexes (N=209,188): 100% precision and 99% recall, improving the state of the art 50% precision and 87% recall. Our anti-patterns were also more effective than the state of the art when applied manually (N=20): 100% developers applied them effectively vs. 50% for the state of the art. Finally, our anti-patterns and fix strategies increased developers' understanding using automatic tools (N=9): from median "Very weakly" to median "Strongly" when detecting vulnerabilities, and from median "Very weakly" to median "Very strongly" when fixing them.

</details>

<details>

<summary>2022-12-15 17:45:50 - Class-Aware Adversarial Transformers for Medical Image Segmentation</summary>

- *Chenyu You, Ruihan Zhao, Fenglin Liu, Siyuan Dong, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S. Duncan*

- `2201.10737v5` - [abs](http://arxiv.org/abs/2201.10737v5) - [pdf](http://arxiv.org/pdf/2201.10737v5)

> Transformers have made remarkable progress towards modeling long-range dependencies within the medical image analysis domain. However, current transformer-based models suffer from several disadvantages: (1) existing methods fail to capture the important features of the images due to the naive tokenization scheme; (2) the models suffer from information loss because they only consider single-scale feature representations; and (3) the segmentation label maps generated by the models are not accurate enough without considering rich semantic contexts and anatomical textures. In this work, we present CASTformer, a novel type of adversarial transformers, for 2D medical image segmentation. First, we take advantage of the pyramid structure to construct multi-scale representations and handle multi-scale variations. We then design a novel class-aware transformer module to better learn the discriminative regions of objects with semantic structures. Lastly, we utilize an adversarial training strategy that boosts segmentation accuracy and correspondingly allows a transformer-based discriminator to capture high-level semantically correlated contents and low-level anatomical features. Our experiments demonstrate that CASTformer dramatically outperforms previous state-of-the-art transformer-based approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in Dice over previous models. Further qualitative experiments provide a more detailed picture of the model's inner workings, shed light on the challenges in improved transparency, and demonstrate that transfer learning can greatly improve performance and reduce the size of medical image datasets in training, making CASTformer a strong starting point for downstream medical image analysis tasks.

</details>

<details>

<summary>2022-12-15 17:53:19 - Model-based Fault Classification for Automotive Software</summary>

- *Mike Becker, Roland Meyer, Tobias Runge, Ina Schaefer, SÃ¶ren van der Wall, Sebastian Wolff*

- `2208.14290v2` - [abs](http://arxiv.org/abs/2208.14290v2) - [pdf](http://arxiv.org/pdf/2208.14290v2)

> Intensive testing using model-based approaches is the standard way of demonstrating the correctness of automotive software. Unfortunately, state-of-the-art techniques leave a crucial and labor intensive task to the test engineer: identifying bugs in failing tests. Our contribution is a model-based classification algorithm for failing tests that assists the engineer when identifying bugs. It consists of three steps. (i) Fault localization replays the test on the model to identify the moment when the two diverge. (ii) Fault explanation then computes the reason for the divergence. The reason is a subset of actions from the test that is sufficient for divergence. (iii) Fault classification groups together tests that fail for similar reasons. Our approach relies on machinery from formal methods: (i) symbolic execution, (ii) Hoare logic and a new relationship between the intermediary assertions constructed for a test, and (iii) a new relationship among Hoare proofs. A crucial aspect in automotive software is timing requirements, for which we develop appropriate Hoare logic theory. We also briefly report on our prototype implementation for the CAN bus Unified Diagnostic Services in an industrial project.

</details>

<details>

<summary>2022-12-15 18:16:17 - Universal Composability is Robust Compilation</summary>

- *Marco Patrignani, Robert KÃ¼nnemann, Riad S. Wahby*

- `1910.08634v3` - [abs](http://arxiv.org/abs/1910.08634v3) - [pdf](http://arxiv.org/pdf/1910.08634v3)

> This paper discusses the relationship between two frameworks: universal composability (UC) and robust compilation (RC). In cryptography, UC is a framework for the specification and analysis of cryptographic protocols with a strong compositionality guarantee: UC protocols remain secure even when composed with other protocols. In programming language security, RC is a novel framework for determining secure compilation by proving whether compiled programs are as secure as their source-level counterparts no matter what target-level code they interact with. Presently, these disciplines are studied in isolation, though we argue that there is a deep connection between them and exploring this connection will benefit both research fields. This paper formally proves the connection between UC and RC and then it explores the benefits of this connection. For this, this paper first identifies which conditions must programming languages fulfil in order to possibly attain UC-like composition. Then, it proves UC of both an existing and a new commitment protocol as a corollary of the related compilers attaining RC. Finally, it mechanises these proofs in Deepsec, obtaining symbolic guarantees that the protocol is indeed UC. Our connection lays the groundwork towards a better and deeper understanding of both UC and RC, and the benefits we showcase from this connection provide first evidence of scalable mechanised proofs for UC.

</details>

<details>

<summary>2022-12-15 19:23:02 - Learning Inter-Annual Flood Loss Risk Models From Historical Flood Insurance Claims and Extreme Rainfall Data</summary>

- *Joaquin Salas, Anamitra Saha, Sai Ravela*

- `2212.08660v1` - [abs](http://arxiv.org/abs/2212.08660v1) - [pdf](http://arxiv.org/pdf/2212.08660v1)

> Flooding is one of the most disastrous natural hazards, responsible for substantial economic losses. A predictive model for flood-induced financial damages is useful for many applications such as climate change adaptation planning and insurance underwriting. This research assesses the predictive capability of regressors constructed on the National Flood Insurance Program (NFIP) dataset using neural networks (Conditional Generative Adversarial Networks), decision trees (Extreme Gradient Boosting), and kernel-based regressors (Gaussian Process). The assessment highlights the most informative predictors for regression. The distribution for claims amount inference is modeled with a Burr distribution permitting the introduction of a bias correction scheme and increasing the regressor's predictive capability. Aiming to study the interaction with physical variables, we incorporate Daymet rainfall estimation to NFIP as an additional predictor. A study on the coastal counties in the eight US South-West states resulted in an $R^2=0.807$. Further analysis of 11 counties with a significant number of claims in the NFIP dataset reveals that Extreme Gradient Boosting provides the best results, that bias correction significantly improves the similarity with the reference distribution, and that the rainfall predictor strengthens the regressor performance.

</details>

<details>

<summary>2022-12-15 20:26:36 - Softmax Policy Gradient Methods Can Take Exponential Time to Converge</summary>

- *Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen*

- `2102.11270v3` - [abs](http://arxiv.org/abs/2102.11270v3) - [pdf](http://arxiv.org/pdf/2102.11270v3)

> The softmax policy gradient (PG) method, which performs gradient ascent under softmax policy parameterization, is arguably one of the de facto implementations of policy optimization in modern reinforcement learning. For $\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs), remarkable progress has recently been achieved towards establishing global convergence of softmax PG methods in finding a near-optimal policy. However, prior results fall short of delineating clear dependencies of convergence rates on salient parameters such as the cardinality of the state space $\mathcal{S}$ and the effective horizon $\frac{1}{1-\gamma}$, both of which could be excessively large. In this paper, we deliver a pessimistic message regarding the iteration complexity of softmax PG methods, despite assuming access to exact gradient computation. Specifically, we demonstrate that the softmax PG method with stepsize $\eta$ can take \[   \frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}} ~\text{iterations} \] to converge, even in the presence of a benign policy initialization and an initial state distribution amenable to exploration (so that the distribution mismatch coefficient is not exceedingly large). This is accomplished by characterizing the algorithmic dynamics over a carefully-constructed MDP containing only three actions. Our exponential lower bound hints at the necessity of carefully adjusting update rules or enforcing proper regularization in accelerating PG methods.

</details>

<details>

<summary>2022-12-15 20:57:59 - Flexible Diffusion Modeling of Long Videos</summary>

- *William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood*

- `2205.11495v3` - [abs](http://arxiv.org/abs/2205.11495v3) - [pdf](http://arxiv.org/pdf/2205.11495v3)

> We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.

</details>

<details>

<summary>2022-12-15 21:53:05 - A Probabilistic-Logic based Commonsense Representation Framework for Modelling Inferences with Multiple Antecedents and Varying Likelihoods</summary>

- *Shantanu Jaiswal, Liu Yan, Dongkyu Choi, Kenneth Kwok*

- `2211.16822v2` - [abs](http://arxiv.org/abs/2211.16822v2) - [pdf](http://arxiv.org/pdf/2211.16822v2)

> Commonsense knowledge-graphs (CKGs) are important resources towards building machines that can 'reason' on text or environmental inputs and make inferences beyond perception. While current CKGs encode world knowledge for a large number of concepts and have been effectively utilized for incorporating commonsense in neural models, they primarily encode declarative or single-condition inferential knowledge and assume all conceptual beliefs to have the same likelihood. Further, these CKGs utilize a limited set of relations shared across concepts and lack a coherent knowledge organization structure resulting in redundancies as well as sparsity across the larger knowledge graph. Consequently, today's CKGs, while useful for a first level of reasoning, do not adequately capture deeper human-level commonsense inferences which can be more nuanced and influenced by multiple contextual or situational factors.   Accordingly, in this work, we study how commonsense knowledge can be better represented by -- (i) utilizing a probabilistic logic representation scheme to model composite inferential knowledge and represent conceptual beliefs with varying likelihoods and (ii) incorporating a hierarchical conceptual ontology to identify salient concept-relevant relations and organize beliefs at different conceptual levels. Our resulting knowledge representation framework can encode a wider variety of world knowledge and represent beliefs flexibly using grounded concepts as well as free-text phrases. As a result, the framework can be utilized as both a traditional free-text knowledge graph and a grounded logic-based inference system more suitable for neuro-symbolic applications. We describe how we extend the PrimeNet knowledge base with our framework through crowd-sourcing and expert-annotation, and demonstrate its application for more interpretable passage-based semantic parsing and question answering.

</details>

<details>

<summary>2022-12-15 23:55:09 - Essentials of Parallel Graph Analytics</summary>

- *Muhammad Osama, Serban D. Porumbescu, John D. Owens*

- `2212.08200v1` - [abs](http://arxiv.org/abs/2212.08200v1) - [pdf](http://arxiv.org/pdf/2212.08200v1)

> We identify the graph data structure, frontiers, operators, an iterative loop structure, and convergence conditions as essential components of graph analytics systems based on the native-graph approach. Using these essential components, we propose an abstraction that captures all the significant programming models within graph analytics, such as bulk-synchronous, asynchronous, shared-memory, message-passing, and push vs. pull traversals. Finally, we demonstrate the power of our abstraction with an elegant modern C++ implementation of single-source shortest path and its required components.

</details>

<details>

<summary>2022-12-16 03:05:55 - EffMulti: Efficiently Modeling Complex Multimodal Interactions for Emotion Analysis</summary>

- *Feng Qiu, Chengyang Xie, Yu Ding, Wanzeng Kong*

- `2212.08661v1` - [abs](http://arxiv.org/abs/2212.08661v1) - [pdf](http://arxiv.org/pdf/2212.08661v1)

> Humans are skilled in reading the interlocutor's emotion from multimodal signals, including spoken words, simultaneous speech, and facial expressions. It is still a challenge to effectively decode emotions from the complex interactions of multimodal signals. In this paper, we design three kinds of multimodal latent representations to refine the emotion analysis process and capture complex multimodal interactions from different views, including a intact three-modal integrating representation, a modality-shared representation, and three modality-individual representations. Then, a modality-semantic hierarchical fusion is proposed to reasonably incorporate these representations into a comprehensive interaction representation. The experimental results demonstrate that our EffMulti outperforms the state-of-the-art methods. The compelling performance benefits from its well-designed framework with ease of implementation, lower computing complexity, and less trainable parameters.

</details>

<details>

<summary>2022-12-16 05:17:59 - Rich Event Modeling for Script Event Prediction</summary>

- *Long Bai, Saiping Guan, Zixuan Li, Jiafeng Guo, Xiaolong Jin, Xueqi Cheng*

- `2212.08287v1` - [abs](http://arxiv.org/abs/2212.08287v1) - [pdf](http://arxiv.org/pdf/2212.08287v1)

> Script is a kind of structured knowledge extracted from texts, which contains a sequence of events. Based on such knowledge, script event prediction aims to predict the subsequent event. To do so, two aspects should be considered for events, namely, event description (i.e., what the events should contain) and event encoding (i.e., how they should be encoded). Most existing methods describe an event by a verb together with only a few core arguments (i.e., subject, object, and indirect object), which are not precise. In addition, existing event encoders are limited to a fixed number of arguments, which are not flexible to deal with extra information. Thus, in this paper, we propose the Rich Event Prediction (REP) framework for script event prediction. Fundamentally, it is based on the proposed rich event description, which enriches the existing ones with three kinds of important information, namely, the senses of verbs, extra semantic roles, and types of participants. REP contains an event extractor to extract such information from texts. Based on the extracted rich information, a predictor then selects the most probable subsequent event. The core component of the predictor is a transformer-based event encoder to flexibly deal with an arbitrary number of arguments. Experimental results on the widely used Gigaword Corpus show the effectiveness of the proposed framework.

</details>

<details>

<summary>2022-12-16 08:49:08 - Jujutsu: A Two-stage Defense against Adversarial Patch Attacks on Deep Neural Networks</summary>

- *Zitao Chen, Pritam Dash, Karthik Pattabiraman*

- `2108.05075v4` - [abs](http://arxiv.org/abs/2108.05075v4) - [pdf](http://arxiv.org/pdf/2108.05075v4)

> Adversarial patch attacks create adversarial examples by injecting arbitrary distortions within a bounded region of the input to fool deep neural networks (DNNs). These attacks are robust (i.e., physically-realizable) and universally malicious, and hence represent a severe security threat to real-world DNN-based systems.   We propose Jujutsu, a two-stage technique to detect and mitigate robust and universal adversarial patch attacks. We first observe that adversarial patches are crafted as localized features that yield large influence on the prediction output, and continue to dominate the prediction on any input. Jujutsu leverages this observation for accurate attack detection with low false positives. Patch attacks corrupt only a localized region of the input, while the majority of the input remains unperturbed. Therefore, Jujutsu leverages generative adversarial networks (GAN) to perform localized attack recovery by synthesizing the semantic contents of the input that are corrupted by the attacks, and reconstructs a ``clean'' input for correct prediction.   We evaluate Jujutsu on four diverse datasets spanning 8 different DNN models, and find that it achieves superior performance and significantly outperforms four existing defenses. We further evaluate Jujutsu against physical-world attacks, as well as adaptive attacks.

</details>

<details>

<summary>2022-12-16 11:21:46 - Learning C to x86 Translation: An Experiment in Neural Compilation</summary>

- *Jordi Armengol-EstapÃ©, Michael F. P. O'Boyle*

- `2108.07639v2` - [abs](http://arxiv.org/abs/2108.07639v2) - [pdf](http://arxiv.org/pdf/2108.07639v2)

> Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.

</details>

<details>

<summary>2022-12-16 11:52:15 - Context Label Learning: Improving Background Class Representations in Semantic Segmentation</summary>

- *Zeju Li, Konstantinos Kamnitsas, Cheng Ouyang, Chen Chen, Ben Glocker*

- `2212.08423v1` - [abs](http://arxiv.org/abs/2212.08423v1) - [pdf](http://arxiv.org/pdf/2212.08423v1)

> Background samples provide key contextual information for segmenting regions of interest (ROIs). However, they always cover a diverse set of structures, causing difficulties for the segmentation model to learn good decision boundaries with high sensitivity and precision. The issue concerns the highly heterogeneous nature of the background class, resulting in multi-modal distributions. Empirically, we find that neural networks trained with heterogeneous background struggle to map the corresponding contextual samples to compact clusters in feature space. As a result, the distribution over background logit activations may shift across the decision boundary, leading to systematic over-segmentation across different datasets and tasks. In this study, we propose context label learning (CoLab) to improve the context representations by decomposing the background class into several subclasses. Specifically, we train an auxiliary network as a task generator, along with the primary segmentation model, to automatically generate context labels that positively affect the ROI segmentation accuracy. Extensive experiments are conducted on several challenging segmentation tasks and datasets. The results demonstrate that CoLab can guide the segmentation model to map the logits of background samples away from the decision boundary, resulting in significantly improved segmentation accuracy. Code is available.

</details>

<details>

<summary>2022-12-16 13:22:26 - Rise of the Planet of Serverless Computing: A Systematic Review</summary>

- *Jinfeng Wen, Zhenpeng Chen, Xin Jin, Xuanzhe Liu*

- `2206.12275v5` - [abs](http://arxiv.org/abs/2206.12275v5) - [pdf](http://arxiv.org/pdf/2206.12275v5)

> Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This paper provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this paper covers 164 papers on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, etc. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.

</details>

<details>

<summary>2022-12-16 14:54:56 - Check-worthy Claim Detection across Topics for Automated Fact-checking</summary>

- *Amani S. Abumansour, Arkaitz Zubiaga*

- `2212.08514v1` - [abs](http://arxiv.org/abs/2212.08514v1) - [pdf](http://arxiv.org/pdf/2212.08514v1)

> An important component of an automated fact-checking system is the claim check-worthiness detection system, which ranks sentences by prioritising them based on their need to be checked. Despite a body of research tackling the task, previous research has overlooked the challenging nature of identifying check-worthy claims across different topics. In this paper, we assess and quantify the challenge of detecting check-worthy claims for new, unseen topics. After highlighting the problem, we propose the AraCWA model to mitigate the performance deterioration when detecting check-worthy claims across topics. The AraCWA model enables boosting the performance for new topics by incorporating two components for few-shot learning and data augmentation. Using a publicly available dataset of Arabic tweets consisting of 14 different topics, we demonstrate that our proposed data augmentation strategy achieves substantial improvements across topics overall, where the extent of the improvement varies across topics. Further, we analyse the semantic similarities between topics, suggesting that the similarity metric could be used as a proxy to determine the difficulty level of an unseen topic prior to undertaking the task of labelling the underlying sentences.

</details>

<details>

<summary>2022-12-16 15:43:08 - A Comprehensive Survey of Benchmarks for Automated Improvement of Software's Non-Functional Properties</summary>

- *Aymeric Blot, Justyna Petke*

- `2212.08540v1` - [abs](http://arxiv.org/abs/2212.08540v1) - [pdf](http://arxiv.org/pdf/2212.08540v1)

> Performance is a key quality of modern software. Although recent years have seen a spike in research on automated improvement of software's execution time, energy, memory consumption, etc., there is a noticeable lack of standard benchmarks for such work. It is also unclear how such benchmarks are representative of current software. Furthermore, frequently non-functional properties of software are targeted for improvement one-at-a-time, neglecting potential negative impact on other properties.   In order to facilitate more research on automated improvement of non-functional properties of software, we conducted a survey gathering benchmarks used in previous work. We considered 5 major online repositories of software engineering work: ACM Digital Library, IEEE Xplore, Scopus, Google Scholar, and ArXiV. We gathered 5000 publications (3749 unique), which were systematically reviewed to identify work that empirically improves non-functional properties of software. We identified 386 relevant papers.   We find that execution time is the most frequently targeted property for improvement (in 62% of relevant papers), while multi-objective improvement is rarely considered (5%). Static approaches are prevalent (in 53% of papers), with exploratory approaches (evolutionary in 18% and non-evolutionary in 14% of papers) increasingly popular in the last 10 years. Only 40% of 386 papers describe work that uses benchmark suites, rather than single software, of those SPEC is most popular (covered in 33 papers). We also provide recommendations for choice of benchmarks in future work, noting, e.g., lack of work that covers Python or JavaScript. We provide all programs found in the 386 papers on our dedicated webpage at https://bloa.github.io/nfunc_survey/   We hope that this effort will facilitate more research on the topic of automated improvement of software's non-functional properties.

</details>

<details>

<summary>2022-12-16 17:36:23 - MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation</summary>

- *Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru, Asli Celikyilmaz*

- `2212.08607v1` - [abs](http://arxiv.org/abs/2212.08607v1) - [pdf](http://arxiv.org/pdf/2212.08607v1)

> Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.

</details>

<details>

<summary>2022-12-16 18:00:21 - Hippocampus-Inspired Cognitive Architecture (HICA) for Operant Conditioning</summary>

- *Deokgun Park, Md Ashaduzzaman Rubel Mondol, SM Mazharul Islam, Aishwarya Pothula*

- `2212.08626v1` - [abs](http://arxiv.org/abs/2212.08626v1) - [pdf](http://arxiv.org/pdf/2212.08626v1)

> The neural implementation of operant conditioning with few trials is unclear. We propose a Hippocampus-Inspired Cognitive Architecture (HICA) as a neural mechanism for operant conditioning. HICA explains a learning mechanism in which agents can learn a new behavior policy in a few trials, as mammals do in operant conditioning experiments. HICA is composed of two different types of modules. One is a universal learning module type that represents a cortical column in the neocortex gray matter. The working principle is modeled as Modulated Heterarchical Prediction Memory (mHPM). In mHPM, each module learns to predict a succeeding input vector given the sequence of the input vectors from lower layers and the context vectors from higher layers. The prediction is fed into the lower layers as a context signal (top-down feedback signaling), and into the higher layers as an input signal (bottom-up feedforward signaling). Rewards modulate the learning rate in those modules to memorize meaningful sequences effectively. In mHPM, each module updates in a local and distributed way compared to conventional end-to-end learning with backpropagation of the single objective loss. This local structure enables the heterarchical network of modules. The second type is an innate, special-purpose module representing various organs of the brain's subcortical system. Modules modeling organs such as the amygdala, hippocampus, and reward center are pre-programmed to enable instinctive behaviors. The hippocampus plays the role of the simulator. It is an autoregressive prediction model of the top-most level signal with a loop structure of memory, while cortical columns are lower layers that provide detailed information to the simulation. The simulation becomes the basis for learning with few trials and the deliberate planning required for operant conditioning.

</details>

<details>

<summary>2022-12-16 19:06:49 - Plansformer: Generating Symbolic Plans using Transformers</summary>

- *Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, Andrea Loreggia*

- `2212.08681v1` - [abs](http://arxiv.org/abs/2212.08681v1) - [pdf](http://arxiv.org/pdf/2212.08681v1)

> Large Language Models (LLMs) have been the subject of active research, significantly advancing the field of Natural Language Processing (NLP). From BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural language tasks such as question answering, summarization, and text generation. Many ongoing efforts focus on understanding LLMs' capabilities, including their knowledge of the world, syntax, and semantics. However, extending the textual prowess of LLMs to symbolic reasoning has been slow and predominantly focused on tackling problems related to the mathematical field. In this paper, we explore the use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles. We introduce Plansformer; an LLM fine-tuned on planning problems and capable of generating plans with favorable behavior in terms of correctness and length with reduced knowledge-engineering efforts. We also demonstrate the adaptability of Plansformer in solving different planning domains with varying complexities, owing to the transfer learning abilities of LLMs. For one configuration of Plansformer, we achieve ~97% valid plans, out of which ~95% are optimal for Towers of Hanoi - a puzzle-solving domain.

</details>

<details>

<summary>2022-12-16 22:20:33 - GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records</summary>

- *Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Mona G Flores, Ying Zhang, Tanja Magoc, Christopher A Harle, Gloria Lipori, Duane A Mitchell, William R Hogan, Elizabeth A Shenkman, Jiang Bian, Yonghui Wu*

- `2203.03540v3` - [abs](http://arxiv.org/abs/2203.03540v3) - [pdf](http://arxiv.org/pdf/2203.03540v3)

> There is an increasing interest in developing artificial intelligence (AI) systems to process and interpret electronic health records (EHRs). Natural language processing (NLP) powered by pretrained language models is the key technology for medical AI systems utilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how large clinical language models with billions of parameters can help medical AI systems utilize unstructured EHRs. In this study, we develop from scratch a large clinical language model - GatorTron - using >90 billion words of text (including >82 billion words of de-identified clinical text) and systematically evaluate it on 5 clinical NLP tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA). We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these NLP tasks. GatorTron models scale up the clinical language model from 110 million to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6% and 9.5% improvement in accuracy for NLI and MQA), which can be applied to medical AI systems to improve healthcare delivery. The GatorTron models are publicly available at: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.

</details>

<details>

<summary>2022-12-17 02:53:21 - Importance of Synthesizing High-quality Data for Text-to-SQL Parsing</summary>

- *Yiyun Zhao, Jiarong Jiang, Yiqun Hu, Wuwei Lan, Henry Zhu, Anuj Chauhan, Alexander Li, Lin Pan, Jun Wang, Chung-Wei Hang, Sheng Zhang, Marvin Dong, Joe Lilien, Patrick Ng, Zhiguo Wang, Vittorio Castelli, Bing Xiang*

- `2212.08785v1` - [abs](http://arxiv.org/abs/2212.08785v1) - [pdf](http://arxiv.org/pdf/2212.08785v1)

> Recently, there has been increasing interest in synthesizing data to improve downstream text-to-SQL tasks. In this paper, we first examined the existing synthesized datasets and discovered that state-of-the-art text-to-SQL algorithms did not further improve on popular benchmarks when trained with augmented synthetic data. We observed two shortcomings: illogical synthetic SQL queries from independent column sampling and arbitrary table joins. To address these issues, we propose a novel synthesis framework that incorporates key relationships from schema, imposes strong typing, and conducts schema-distance-weighted column sampling. We also adopt an intermediate representation (IR) for the SQL-to-text task to further improve the quality of the generated natural language questions. When existing powerful semantic parsers are pre-finetuned on our high-quality synthesized data, our experiments show that these models have significant accuracy boosts on popular benchmarks, including new state-of-the-art performance on Spider.

</details>

<details>

<summary>2022-12-17 12:55:10 - Agile Effort Estimation: Have We Solved the Problem Yet? Insights From A Replication Study</summary>

- *Vali Tawosi, Rebecca Moussa, Federica Sarro*

- `2201.05401v2` - [abs](http://arxiv.org/abs/2201.05401v2) - [pdf](http://arxiv.org/pdf/2201.05401v2)

> In the last decade, several studies have explored automated techniques to estimate the effort of agile software development. We perform a close replication and extension of a seminal work proposing the use of Deep Learning for Agile Effort Estimation (namely Deep-SE), which has set the state-of-the-art since. Specifically, we replicate three of the original research questions aiming at investigating the effectiveness of Deep-SE for both within-project and cross-project effort estimation. We benchmark Deep-SE against three baselines (i.e., Random, Mean and Median effort estimators) and a previously proposed method to estimate agile software project development effort (dubbed TF/IDF-SVM), as done in the original study. To this end, we use the data from the original study and an additional dataset of 31,960 issues mined from TAWOS, as using more data allows us to strengthen the confidence in the results, and to further mitigate external validity threats. The results of our replication show that Deep-SE outperforms the Median baseline estimator and TF/IDF-SVM in only very few cases with statistical significance (8/42 and 9/32 cases, respectively), thus confounding previous findings on the efficacy of Deep-SE. The two additional RQs revealed that neither augmenting the training set nor pre-training Deep-SE play lead to an improvement of its accuracy and convergence speed. These results suggest that using semantic similarity is not enough to differentiate user stories with respect to their story points; thus, future work has yet to explore and find new techniques and features that obtain accurate agile software development estimates.

</details>

<details>

<summary>2022-12-17 14:43:32 - Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue Systems</summary>

- *Songbo Hu, Ivan VuliÄ, Fangyu Liu, Anna Korhonen*

- `2211.03648v2` - [abs](http://arxiv.org/abs/2211.03648v2) - [pdf](http://arxiv.org/pdf/2211.03648v2)

> End-to-end (E2E) task-oriented dialogue (ToD) systems are prone to fall into the so-called "likelihood trap", resulting in generated responses which are dull, repetitive, and often inconsistent with dialogue history. Comparing ranked lists of multiple generated responses against the "gold response" (from evaluation data) reveals a wide diversity in response quality, with many good responses placed lower in the ranked list. The main challenge, addressed in this work, is then how to reach beyond greedily generated system responses, that is, how to obtain and select such high-quality responses from the list of overgenerated responses at inference without availability of the gold response. To this end, we propose a simple yet effective reranking method which aims to select high-quality items from the lists of responses initially overgenerated by the system. The idea is to use any sequence-level (similarity) scoring function to divide the semantic space of responses into high-scoring versus low-scoring partitions. At training, the high-scoring partition comprises all generated responses whose similarity to the gold response is higher than the similarity of the greedy response to the gold response. At inference, the aim is to estimate the probability that each overgenerated response belongs to the high-scoring partition, given only previous dialogue history. We validate the robustness and versatility of our proposed method on the standard MultiWOZ dataset: our methods improve a state-of-the-art E2E ToD system by 2.0 BLEU, 1.6 ROUGE, and 1.3 METEOR scores, achieving new peak results. Additional experiments on the BiTOD dataset and human evaluation further ascertain the generalisability and effectiveness of the proposed framework.

</details>

<details>

<summary>2022-12-17 19:35:23 - Machine Learning Construction: implications to cybersecurity</summary>

- *Waleed A. Yousef*

- `1906.10019v4` - [abs](http://arxiv.org/abs/1906.10019v4) - [pdf](http://arxiv.org/pdf/1906.10019v4)

> Statistical learning is the process of estimating an unknown probabilistic input-output relationship of a system using a limited number of observations. A statistical learning machine (SLM) is the algorithm, function, model, or rule, that learns such a process; and machine learning (ML) is the conventional name of this field. ML and its applications are ubiquitous in the modern world. Systems such as Automatic target recognition (ATR) in military applications, computer aided diagnosis (CAD) in medical imaging, DNA microarrays in genomics, optical character recognition (OCR), speech recognition (SR), spam email filtering, stock market prediction, etc., are few examples and applications for ML; diverse fields but one theory. In particular, ML has gained a lot of attention in the field of cyberphysical security, especially in the last decade. It is of great importance to this field to design detection algorithms that have the capability of learning from security data to be able to hunt threats, achieve better monitoring, master the complexity of the threat intelligence feeds, and achieve timely remediation of security incidents. The field of ML can be decomposed into two basic subfields: \textit{construction} and \textit{assessment}. We mean by \textit{construction} designing or inventing an appropriate algorithm that learns from the input data and achieves a good performance according to some optimality criterion. We mean by \textit{assessment} attributing some performance measures to the constructed ML algorithm, along with their estimators, to objectively assess this algorithm. \textit{Construction} and \textit{assessment} of a ML algorithm require familiarity with different other fields: probability, statistics, matrix theory, optimization, algorithms, and programming, among others.f

</details>

<details>

<summary>2022-12-17 20:45:59 - Beyond the C: Retargetable Decompilation using Neural Machine Translation</summary>

- *Iman Hosseini, Brendan Dolan-Gavitt*

- `2212.08950v1` - [abs](http://arxiv.org/abs/2212.08950v1) - [pdf](http://arxiv.org/pdf/2212.08950v1)

> The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software. Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation. Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code. In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages. We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation.

</details>

<details>

<summary>2022-12-17 21:13:51 - OpenRAN Gym: AI/ML Development, Data Collection, and Testing for O-RAN on PAWR Platforms</summary>

- *Leonardo Bonati, Michele Polese, Salvatore D'Oro, Stefano Basagni, Tommaso Melodia*

- `2207.12362v3` - [abs](http://arxiv.org/abs/2207.12362v3) - [pdf](http://arxiv.org/pdf/2207.12362v3)

> Open Radio Access Network (RAN) architectures will enable interoperability, openness and programmable data-driven control in next generation cellular networks. However, developing and testing efficient solutions that generalize across heterogeneous cellular deployments and scales, and that optimize network performance in such diverse environments is a complex task that is still largely unexplored. In this paper we present OpenRAN Gym, a unified, open, and O-RAN-compliant experimental toolbox for data collection, design, prototyping and testing of end-to-end data-driven control solutions for next generation Open RAN systems. OpenRAN Gym extends and combines into a unique solution several software frameworks for data collection of RAN statistics and RAN control, and a lightweight O-RAN near-real-time RAN Intelligent Controller (RIC) tailored to run on experimental wireless platforms. We first provide an overview of the various architectural components of OpenRAN Gym and describe how it is used to collect data and design, train and test artificial intelligence and machine learning O-RAN-compliant applications (xApps) at scale. We then describe in detail how to test the developed xApps on softwarized RANs and provide an example of two xApps developed with OpenRAN Gym that are used to control a network with 7 base stations and 42 users deployed on the Colosseum testbed. Finally, we show how solutions developed with OpenRAN Gym on Colosseum can be exported to real-world, heterogeneous wireless platforms, such as the Arena testbed and the POWDER and COSMOS platforms of the PAWR program. OpenRAN Gym and its software components are open-source and publicly-available to the research community. By guiding the readers through running experiments with OpenRAN Gym, we aim at providing a key reference for researchers and practitioners working on experimental Open RAN systems.

</details>

<details>

<summary>2022-12-18 01:59:49 - A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter Dataset</summary>

- *Yu Wang, Hongxia Jin*

- `2212.08987v1` - [abs](http://arxiv.org/abs/2212.08987v1) - [pdf](http://arxiv.org/pdf/2212.08987v1)

> Most recent semantic frame parsing systems for spoken language understanding (SLU) are designed based on recurrent neural networks. These systems display decent performance on benchmark SLU datasets such as ATIS or SNIPS, which contain short utterances with relatively simple patterns. However, the current semantic frame parsing models lack a mechanism to handle out-of-distribution (\emph{OOD}) patterns and out-of-vocabulary (\emph{OOV}) tokens. In this paper, we introduce a robust semantic frame parsing pipeline that can handle both \emph{OOD} patterns and \emph{OOV} tokens in conjunction with a new complex Twitter dataset that contains long tweets with more \emph{OOD} patterns and \emph{OOV} tokens. The new pipeline demonstrates much better results in comparison to state-of-the-art baseline SLU models on both the SNIPS dataset and the new Twitter dataset (Our new Twitter dataset can be downloaded from https://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also build an E2E application to demo the feasibility of our algorithm and show why it is useful in real application.

</details>

<details>

<summary>2022-12-18 03:21:40 - Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning</summary>

- *Xian Zhong, Zipeng Li, Shuqin Chen, Kui Jiang, Chen Chen, Mang Ye*

- `2211.15076v2` - [abs](http://arxiv.org/abs/2211.15076v2) - [pdf](http://arxiv.org/pdf/2211.15076v2)

> Video captioning aims to generate natural language sentences that describe the given video accurately. Existing methods obtain favorable generation by exploring richer visual representations in encode phase or improving the decoding ability. However, the long-tailed problem hinders these attempts at low-frequency tokens, which rarely occur but carry critical semantics, playing a vital role in the detailed generation. In this paper, we introduce a novel Refined Semantic enhancement method towards Frequency Diffusion (RSFD), a captioning model that constantly perceives the linguistic representation of the infrequent tokens. Concretely, a Frequency-Aware Diffusion (FAD) module is proposed to comprehend the semantics of low-frequency tokens to break through generation limitations. In this way, the caption is refined by promoting the absorption of tokens with insufficient occurrence. Based on FAD, we design a Divergent Semantic Supervisor (DSS) module to compensate for the information loss of high-frequency tokens brought by the diffusion process, where the semantics of low-frequency tokens is further emphasized to alleviate the long-tailed problem. Extensive experiments indicate that RSFD outperforms the state-of-the-art methods on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate that the enhancement of low-frequency tokens semantics can obtain a competitive generation effect. Code is available at https://github.com/lzp870/RSFD.

</details>

<details>

<summary>2022-12-18 04:06:37 - Coordinate Descent Methods for DC Minimization: Optimality Conditions and Global Convergence</summary>

- *Ganzhao Yuan*

- `2109.04228v3` - [abs](http://arxiv.org/abs/2109.04228v3) - [pdf](http://arxiv.org/pdf/2109.04228v3)

> Difference-of-Convex (DC) minimization, referring to the problem of minimizing the difference of two convex functions, has been found rich applications in statistical learning and studied extensively for decades. However, existing methods are primarily based on multi-stage convex relaxation, only leading to weak optimality of critical points. This paper proposes a coordinate descent method for minimizing a class of DC functions based on sequential nonconvex approximation. Our approach iteratively solves a nonconvex one-dimensional subproblem globally, and it is guaranteed to converge to a coordinate-wise stationary point. We prove that this new optimality condition is always stronger than the standard critical point condition and directional point condition under a mild \textit{locally bounded nonconvexity assumption}. For comparisons, we also include a naive variant of coordinate descent methods based on sequential convex approximation in our study. When the objective function satisfies a \textit{globally bounded nonconvexity assumption} and \textit{Luo-Tseng error bound assumption}, coordinate descent methods achieve \textit{Q-linear} convergence rate. Also, for many applications of interest, we show that the nonconvex one-dimensional subproblem can be computed exactly and efficiently using a breakpoint searching method. Finally, we have conducted extensive experiments on several statistical learning tasks to show the superiority of our approach.   Keywords: Coordinate Descent, DC Minimization, DC Programming, Difference-of-Convex Programs, Nonconvex Optimization, Sparse Optimization, Binary Optimization.

</details>

<details>

<summary>2022-12-18 04:13:47 - Rare-Seed Generation for Fuzzing</summary>

- *Seemanta Saha, Laboni Sarker, Md Shafiuzzaman, Chaofan Shou, Albert Li, Ganesh Sankaran, Tevfik Bultan*

- `2212.09004v1` - [abs](http://arxiv.org/abs/2212.09004v1) - [pdf](http://arxiv.org/pdf/2212.09004v1)

> Starting with a random initial seed, fuzzers search for inputs that trigger bugs or vulnerabilities. However, fuzzers often fail to generate inputs for program paths guarded by restrictive branch conditions. In this paper, we show that by first identifying rare-paths in programs (i.e., program paths with path constraints that are unlikely to be satisfied by random input generation), and then, generating inputs/seeds that trigger rare-paths, one can improve the coverage of fuzzing tools. In particular, we present techniques 1) that identify rare paths using quantitative symbolic analysis, and 2) generate inputs that can explore these rare paths using path-guided concolic execution. We provide these inputs as initial seed sets to three state of the art fuzzers. Our experimental evaluation on a set of programs (that contain a lot of restrictive branch conditions) shows that the fuzzers achieve better coverage with the rare-path based seed set compared to a random initial seed.

</details>

<details>

<summary>2022-12-18 08:27:37 - MoDi: Unconditional Motion Synthesis from Diverse Data</summary>

- *Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, Daniel Cohen-Or*

- `2206.08010v3` - [abs](http://arxiv.org/abs/2206.08010v3) - [pdf](http://arxiv.org/pdf/2206.08010v3)

> The emergence of neural networks has revolutionized the field of motion synthesis. Yet, learning to unconditionally synthesize motions from a given distribution remains challenging, especially when the motions are highly diverse. In this work, we present MoDi -- a generative model trained in an unsupervised setting from an extremely diverse, unstructured and unlabeled dataset. During inference, MoDi can synthesize high-quality, diverse motions. Despite the lack of any structure in the dataset, our model yields a well-behaved and highly structured latent space, which can be semantically clustered, constituting a strong motion prior that facilitates various applications including semantic editing and crowd simulation. In addition, we present an encoder that inverts real motions into MoDi's natural motion manifold, issuing solutions to various ill-posed challenges such as completion from prefix and spatial editing. Our qualitative and quantitative experiments achieve state-of-the-art results that outperform recent SOTA techniques. Code and trained models are available at https://sigal-raab.github.io/MoDi.

</details>

<details>

<summary>2022-12-18 10:41:55 - BEATs: Audio Pre-Training with Acoustic Tokenizers</summary>

- *Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Furu Wei*

- `2212.09058v1` - [abs](http://arxiv.org/abs/2212.09058v1) - [pdf](http://arxiv.org/pdf/2212.09058v1)

> The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.

</details>

<details>

<summary>2022-12-18 12:00:12 - Disentangling Learnable and Memorizable Data via Contrastive Learning for Semantic Communications</summary>

- *Christina Chaccour, Walid Saad*

- `2212.09071v1` - [abs](http://arxiv.org/abs/2212.09071v1) - [pdf](http://arxiv.org/pdf/2212.09071v1)

> Achieving artificially intelligent-native wireless networks is necessary for the operation of future 6G applications such as the metaverse. Nonetheless, current communication schemes are, at heart, a mere reconstruction process that lacks reasoning. One key solution that enables evolving wireless communication to a human-like conversation is semantic communications. In this paper, a novel machine reasoning framework is proposed to pre-process and disentangle source data so as to make it semantic-ready. In particular, a novel contrastive learning framework is proposed, whereby instance and cluster discrimination are performed on the data. These two tasks enable increasing the cohesiveness between data points mapping to semantically similar content elements and disentangling data points of semantically different content elements. Subsequently, the semantic deep clusters formed are ranked according to their level of confidence. Deep semantic clusters of highest confidence are considered learnable, semantic-rich data, i.e., data that can be used to build a language in a semantic communications system. The least confident ones are considered, random, semantic-poor, and memorizable data that must be transmitted classically. Our simulation results showcase the superiority of our contrastive learning approach in terms of semantic impact and minimalism. In fact, the length of the semantic representation achieved is minimized by 57.22% compared to vanilla semantic communication systems, thus achieving minimalist semantic representations.

</details>

<details>

<summary>2022-12-18 12:13:02 - Multi-task Joint Strategies of Self-supervised Representation Learning on Biomedical Networks for Drug Discovery</summary>

- *Xiaoqi Wang, Yingjie Cheng, Yaning Yang, Yue Yu, Fei Li, Shaoliang Peng*

- `2201.04437v2` - [abs](http://arxiv.org/abs/2201.04437v2) - [pdf](http://arxiv.org/pdf/2201.04437v2)

> Self-supervised representation learning (SSL) on biomedical networks provides new opportunities for drug discovery. However, how to effectively combine multiple SSL models is still challenging and has been rarely explored. Therefore, we propose multi-task joint strategies of self-supervised representation learning on biomedical networks for drug discovery, named MSSL2drug. We design six basic SSL tasks inspired by various modality features including structures, semantics, and attributes in heterogeneous biomedical networks. Importantly, fifteen combinations of multiple tasks are evaluated by a graph attention-based multi-task adversarial learning framework in two drug discovery scenarios. The results suggest two important findings. (1) Combinations of multimodal tasks achieve the best performance compared to other multi-task joint models. (2) The local-global combination models yield higher performance than random two-task combinations when there are the same size of modalities. Therefore, we conjecture that the multimodal and local-global combination strategies can be treated as the guideline of multi-task SSL for drug discovery.

</details>

<details>

<summary>2022-12-18 12:43:24 - Answer-Set Programming for Lexicographical Makespan Optimisation in Parallel Machine Scheduling</summary>

- *Thomas Eiter, Tobias Geibinger, Nysret Musliu, Johannes Oetsch, Peter Skocovsky, Daria Stepanova*

- `2212.09077v1` - [abs](http://arxiv.org/abs/2212.09077v1) - [pdf](http://arxiv.org/pdf/2212.09077v1)

> We deal with a challenging scheduling problem on parallel machines with sequence-dependent setup times and release dates from a real-world application of semiconductor work-shop production. There, jobs can only be processed by dedicated machines, thus few machines can determine the makespan almost regardless of how jobs are scheduled on the remaining ones. This causes problems when machines fail and jobs need to be rescheduled. Instead of optimising only the makespan, we put the individual machine spans in non-ascending order and lexicographically minimise the resulting tuples. This achieves that all machines complete as early as possible and increases the robustness of the schedule. We study the application of Answer-Set Programming (ASP) to solve this problem. While ASP eases modelling, the combination of timing constraints and the considered objective function challenges current solving technology. The former issue is addressed by using an extension of ASP by difference logic. For the latter, we devise different algorithms that use multi-shot solving. To tackle industrial-sized instances, we study different approximations and heuristics. Our experimental results show that ASP is indeed a promising KRR paradigm for this problem and is competitive with state-of-the-art CP and MIP solvers. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2022-12-18 15:10:05 - LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning</summary>

- *Sayan Ghosh, Rakesh R Menon, Shashank Srivastava*

- `2212.09104v1` - [abs](http://arxiv.org/abs/2212.09104v1) - [pdf](http://arxiv.org/pdf/2212.09104v1)

> A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as 'always' or 'rarely') and mimicking humans in compositionally learning complex tasks. Here, we present LaSQuE, a method that can learn zero-shot classifiers from language explanations by using three new strategies - (1) modeling the semantics of linguistic quantifiers in explanations (including exploiting ordinal strength relationships, such as 'always' > 'likely'), (2) aggregating information from multiple explanations using an attention-based mechanism, and (3) model training via curriculum learning. With these strategies, LaSQuE outperforms prior work, showing an absolute gain of up to 7% in generalizing to unseen real-world classification tasks.

</details>

<details>

<summary>2022-12-19 05:06:00 - Natural Language to Code Generation in Interactive Data Science Notebooks</summary>

- *Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton*

- `2212.09248v1` - [abs](http://arxiv.org/abs/2212.09248v1) - [pdf](http://arxiv.org/pdf/2212.09248v1)

> Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1082 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions.

</details>

<details>

<summary>2022-12-19 08:12:25 - XEngine: Optimal Tensor Rematerialization for Neural Networks in Heterogeneous Environments</summary>

- *Manuela Schuler, Richard Membarth, Philipp Slusallek*

- `2212.09290v1` - [abs](http://arxiv.org/abs/2212.09290v1) - [pdf](http://arxiv.org/pdf/2212.09290v1)

> Memory efficiency is crucial in training deep learning networks on resource-restricted devices. During backpropagation, forward tensors are used to calculate gradients. Despite the option of keeping those dependencies in memory until they are reused in backpropagation, some forward tensors can be discarded and recomputed later from saved tensors, so-called checkpoints. This allows, in particular, for resource-constrained heterogeneous environments to make use of all available compute devices. Unfortunately, the definition of these checkpoints is a non-trivial problem and poses a challenge to the programmer - improper or excessive recomputations negate the benefit of checkpointing.   In this article, we present XEngine, an approach that schedules network operators to heterogeneous devices in low memory environments by determining checkpoints and recomputations of tensors. Our approach selects suitable resources per timestep and operator and optimizes the end-to-end time for neural networks taking the memory limitation of each device into account. For this, we formulate a mixed-integer quadratic program (MIQP) to schedule operators of deep learning networks on heterogeneous systems. We compare our MIQP solver XEngine against Checkmate, a mixed-integer linear programming (MILP) approach that solves recomputation on a single device. Our solver finds solutions that are up to 22.5 % faster than the fastest Checkmate schedule in which the network is computed exclusively on a single device. We also find valid schedules for networks making use of both central processing units and graphics processing units if memory limitations do not allow scheduling exclusively to the graphics processing unit.

</details>

<details>

<summary>2022-12-19 09:09:40 - An overview of open source Deep Learning-based libraries for Neuroscience</summary>

- *Louis Fabrice Tshimanga, Manfredo Atzori, Federico Del Pup, Maurizio Corbetta*

- `2301.05057v1` - [abs](http://arxiv.org/abs/2301.05057v1) - [pdf](http://arxiv.org/pdf/2301.05057v1)

> In recent years, deep learning revolutionized machine learning and its applications, producing results comparable to human experts in several domains, including neuroscience. Each year, hundreds of scientific publications present applications of deep neural networks for biomedical data analysis. Due to the fast growth of the domain, it could be a complicated and extremely time-consuming task for worldwide researchers to have a clear perspective of the most recent and advanced software libraries. This work contributes to clarify the current situation in the domain, outlining the most useful libraries that implement and facilitate deep learning application to neuroscience, allowing scientists to identify the most suitable options for their research or clinical projects. This paper summarizes the main developments in Deep Learning and their relevance to Neuroscience; it then reviews neuroinformatic toolboxes and libraries, collected from the literature and from specific hubs of software projects oriented to neuroscience research. The selected tools are presented in tables detailing key features grouped by domain of application (e.g. data type, neuroscience area, task), model engineering (e.g. programming language, model customization) and technological aspect (e.g. interface, code source). The results show that, among a high number of available software tools, several libraries are standing out in terms of functionalities for neuroscience applications. The aggregation and discussion of this information can help the neuroscience community to devolop their research projects more efficiently and quickly, both by means of readily available tools, and by knowing which modules may be improved, connected or added.

</details>

<details>

<summary>2022-12-19 10:03:47 - Multi-sense embeddings through a word sense disambiguation process</summary>

- *Terry Ruas, William Grosky, Akiko Aizawa*

- `2101.08700v2` - [abs](http://arxiv.org/abs/2101.08700v2) - [pdf](http://arxiv.org/pdf/2101.08700v2)

> Natural Language Understanding has seen an increasing number of publications in the last few years, especially after robust word embeddings models became prominent, when they proved themselves able to capture and represent semantic relationships from massive amounts of data. Nevertheless, traditional models often fall short in intrinsic issues of linguistics, such as polysemy and homonymy. Any expert system that makes use of natural language in its core, can be affected by a weak semantic representation of text, resulting in inaccurate outcomes based on poor decisions. To mitigate such issues, we propose a novel approach called Most Suitable Sense Annotation (MSSA), that disambiguates and annotates each word by its specific sense, considering the semantic effects of its context. Our approach brings three main contributions to the semantic representation scenario: (i) an unsupervised technique that disambiguates and annotates words by their senses, (ii) a multi-sense embeddings model that can be extended to any traditional word embeddings algorithm, and (iii) a recurrent methodology that allows our models to be re-used and their representations refined. We test our approach on six different benchmarks for the word similarity task, showing that our approach can produce state-of-the-art results and outperforms several more complex state-of-the-art systems.

</details>

<details>

<summary>2022-12-19 10:16:23 - Enhanced word embeddings using multi-semantic representation through lexical chains</summary>

- *Terry Ruas, Charles Henrique Porto Ferreira, William Grosky, FabrÃ­cio Olivetti de FranÃ§a, DÃ©bora Maria Rossi Medeiros*

- `2101.09023v2` - [abs](http://arxiv.org/abs/2101.09023v2) - [pdf](http://arxiv.org/pdf/2101.09023v2)

> The relationship between words in a sentence often tells us more about the underlying semantic content of a document than its actual words, individually. In this work, we propose two novel algorithms, called Flexible Lexical Chain II and Fixed Lexical Chain II. These algorithms combine the semantic relations derived from lexical chains, prior knowledge from lexical databases, and the robustness of the distributional hypothesis in word embeddings as building blocks forming a single system. In short, our approach has three main contributions: (i) a set of techniques that fully integrate word embeddings and lexical chains; (ii) a more robust semantic representation that considers the latent relation between words in a document; and (iii) lightweight word embeddings models that can be extended to any natural language task. We intend to assess the knowledge of pre-trained models to evaluate their robustness in the document classification task. The proposed techniques are tested against seven word embeddings algorithms using five different machine learning classifiers over six scenarios in the document classification task. Our results show the integration between lexical chains and word embeddings representations sustain state-of-the-art results, even against more complex systems.

</details>

<details>

<summary>2022-12-19 10:30:12 - MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation</summary>

- *Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, Abhinav Jangda*

- `2208.08227v4` - [abs](http://arxiv.org/abs/2208.08227v4) - [pdf](http://arxiv.org/pdf/2208.08227v4)

> Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.   We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.

</details>

<details>

<summary>2022-12-19 12:46:23 - Steel Phase Kinetics Modeling using Symbolic Regression</summary>

- *David Piringer, Bernhard Bloder, Gabriel Kronberger*

- `2212.10284v1` - [abs](http://arxiv.org/abs/2212.10284v1) - [pdf](http://arxiv.org/pdf/2212.10284v1)

> We describe an approach for empirical modeling of steel phase kinetics based on symbolic regression and genetic programming. The algorithm takes processed data gathered from dilatometer measurements and produces a system of differential equations that models the phase kinetics. Our initial results demonstrate that the proposed approach allows to identify compact differential equations that fit the data. The model predicts ferrite, pearlite and bainite formation for a single steel type. Martensite is not yet included in the model. Future work shall incorporate martensite and generalize to multiple steel types with different chemical compositions.

</details>

<details>

<summary>2022-12-19 14:19:07 - Adaptive Control of Client Selection and Gradient Compression for Efficient Federated Learning</summary>

- *Zhida Jiang, Yang Xu, Hongli Xu, Zhiyuan Wang, Chen Qian*

- `2212.09483v1` - [abs](http://arxiv.org/abs/2212.09483v1) - [pdf](http://arxiv.org/pdf/2212.09483v1)

> Federated learning (FL) allows multiple clients cooperatively train models without disclosing local data. However, the existing works fail to address all these practical concerns in FL: limited communication resources, dynamic network conditions and heterogeneous client properties, which slow down the convergence of FL. To tackle the above challenges, we propose a heterogeneity-aware FL framework, called FedCG, with adaptive client selection and gradient compression. Specifically, the parameter server (PS) selects a representative client subset considering statistical heterogeneity and sends the global model to them. After local training, these selected clients upload compressed model updates matching their capabilities to the PS for aggregation, which significantly alleviates the communication load and mitigates the straggler effect. We theoretically analyze the impact of both client selection and gradient compression on convergence performance. Guided by the derived convergence rate, we develop an iteration-based algorithm to jointly optimize client selection and compression ratio decision using submodular maximization and linear programming. Extensive experiments on both real-world prototypes and simulations show that FedCG can provide up to 5.3$\times$ speedup compared to other methods.

</details>

<details>

<summary>2022-12-19 14:57:13 - Fake it, Mix it, Segment it: Bridging the Domain Gap Between Lidar Sensors</summary>

- *Frederik Hasecke, Pascal Colling, Anton Kummert*

- `2212.09517v1` - [abs](http://arxiv.org/abs/2212.09517v1) - [pdf](http://arxiv.org/pdf/2212.09517v1)

> Segmentation of lidar data is a task that provides rich, point-wise information about the environment of robots or autonomous vehicles. Currently best performing neural networks for lidar segmentation are fine-tuned to specific datasets. Switching the lidar sensor without retraining on a big set of annotated data from the new sensor creates a domain shift, which causes the network performance to drop drastically. In this work we propose a new method for lidar domain adaption, in which we use annotated panoptic lidar datasets and recreate the recorded scenes in the structure of a different lidar sensor. We narrow the domain gap to the target data by recreating panoptic data from one domain in another and mixing the generated data with parts of (pseudo) labeled target domain data. Our method improves the nuScenes to SemanticKITTI unsupervised domain adaptation performance by 15.2 mean Intersection over Union points (mIoU) and by 48.3 mIoU in our semi-supervised approach. We demonstrate a similar improvement for the SemanticKITTI to nuScenes domain adaptation by 21.8 mIoU and 51.5 mIoU, respectively. We compare our method with two state of the art approaches for semantic lidar segmentation domain adaptation with a significant improvement for unsupervised and semi-supervised domain adaptation. Furthermore we successfully apply our proposed method to two entirely unlabeled datasets of two state of the art lidar sensors Velodyne Alpha Prime and InnovizTwo, and train well performing semantic segmentation networks for both.

</details>

<details>

<summary>2022-12-19 15:01:48 - Explainable Fuzzer Evaluation</summary>

- *Dylan Wolff, Marcel BÃ¶hme, Abhik Roychoudhury*

- `2212.09519v1` - [abs](http://arxiv.org/abs/2212.09519v1) - [pdf](http://arxiv.org/pdf/2212.09519v1)

> While the aim of fuzzer evaluation is to establish fuzzer performance in general, an evaluation is always conducted on a specific benchmark. In this paper, we investigate the degree to which the benchmarking result depends on the properties of the benchmark and propose a methodology to quantify the impact of benchmark properties on the benchmarking result in relation to the impact of the choice of fuzzer. We found that the measured performance and ranking of a fuzzer substantially depends on properties of the programs and the seed corpora used during evaluation. For instance, if the benchmark contained larger programs or seed corpora with a higher initial coverage, AFL's ranking would improve while LibFuzzer's ranking would worsen. We describe our methodology as explainable fuzzer evaluation because it explains why the specific evaluation setup yields the observed superiority or ranking of the fuzzers and how it might change for different benchmarks. We envision that our analysis can be used to assess the degree to which evaluation results are overfitted to the benchmark and to identify the specific conditions under which different fuzzers performs better than others.

</details>

<details>

<summary>2022-12-19 15:45:33 - Uncertainty Estimation for Heatmap-based Landmark Localization</summary>

- *Lawrence Schobs, Andrew J. Swift, Haiping Lu*

- `2203.02351v2` - [abs](http://arxiv.org/abs/2203.02351v2) - [pdf](http://arxiv.org/pdf/2203.02351v2)

> Automatic anatomical landmark localization has made great strides by leveraging deep learning methods in recent years. The ability to quantify the uncertainty of these predictions is a vital component needed for these methods to be adopted in clinical settings, where it is imperative that erroneous predictions are caught and corrected. We propose Quantile Binning, a data-driven method to categorize predictions by uncertainty with estimated error bounds. Our framework can be applied to any continuous uncertainty measure, allowing straightforward identification of the best subset of predictions with accompanying estimated error bounds. We facilitate easy comparison between uncertainty measures by constructing two evaluation metrics derived from Quantile Binning. We compare and contrast three epistemic uncertainty measures (two baselines, and a proposed method combining aspects of the two), derived from two heatmap-based landmark localization model paradigms (U-Net and patch-based). We show results across three datasets, including a publicly available Cephalometric dataset. We illustrate how filtering out gross mispredictions caught in our Quantile Bins significantly improves the proportion of predictions under an acceptable error threshold. Finally, we demonstrate that Quantile Binning remains effective on landmarks with high aleatoric uncertainty caused by inherent landmark ambiguity, and offer recommendations on which uncertainty measure to use and how to use it. The code and data are available at https://github.com/schobs/qbin.

</details>

<details>

<summary>2022-12-19 17:00:54 - Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding</summary>

- *Haoli Bai, Zhiguang Liu, Xiaojun Meng, Wentao Li, Shuang Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu*

- `2212.09621v1` - [abs](http://arxiv.org/abs/2212.09621v1) - [pdf](http://arxiv.org/pdf/2212.09621v1)

> Unsupervised pre-training on millions of digital-born or scanned documents has shown promising advances in visual document understanding~(VDU). While various vision-language pre-training objectives are studied in existing solutions, the document textline, as an intrinsic granularity in VDU, has seldom been explored so far. A document textline usually contains words that are spatially and semantically correlated, which can be easily obtained from OCR engines. In this paper, we propose Wukong-Reader, trained with new pre-training objectives to leverage the structural knowledge nested in document textlines. We introduce textline-region contrastive learning to achieve fine-grained alignment between the visual regions and texts of document textlines. Furthermore, masked region modeling and textline-grid matching are also designed to enhance the visual and layout representations of textlines. Experiments show that our Wukong-Reader has superior performance on various VDU tasks such as information extraction. The fine-grained alignment over textlines also empowers Wukong-Reader with promising localization ability.

</details>

<details>

<summary>2022-12-19 17:50:05 - MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code Completion</summary>

- *Zi Gong, Yinpeng Guo, Pingyi Zhou, Cuiyun Gao, Yasheng Wang, Zenglin Xu*

- `2212.09666v1` - [abs](http://arxiv.org/abs/2212.09666v1) - [pdf](http://arxiv.org/pdf/2212.09666v1)

> Code completion is a valuable topic in both academia and industry. Recently, large-scale mono-programming-lingual (MonoPL) pre-training models have been proposed to boost the performance of code completion. However, the code completion on low-resource programming languages (PL) is difficult for the data-driven paradigm, while there are plenty of developers using low-resource PLs. On the other hand, there are few studies exploring the effects of multi-programming-lingual (MultiPL) pre-training for the code completion, especially the impact on low-resource programming languages. To this end, we propose the MultiCoder to enhance the low-resource code completion via MultiPL pre-training and MultiPL Mixture-of-Experts (MoE) layers. We further propose a novel PL-level MoE routing strategy (PL-MoE) for improving the code completion on all PLs. Experimental results on CodeXGLUE and MultiCC demonstrate that 1) the proposed MultiCoder significantly outperforms the MonoPL baselines on low-resource programming languages, and 2) the PL-MoE module further boosts the performance on six programming languages. In addition, we analyze the effects of the proposed method in details and explore the effectiveness of our method in a variety of scenarios.

</details>

<details>

<summary>2022-12-19 18:30:26 - Graph-based Semantical Extractive Text Analysis</summary>

- *Mina Samizadeh*

- `2212.09701v1` - [abs](http://arxiv.org/abs/2212.09701v1) - [pdf](http://arxiv.org/pdf/2212.09701v1)

> In the past few decades, there has been an explosion in the amount of available data produced from various sources with different topics. The availability of this enormous data necessitates us to adopt effective computational tools to explore the data. This leads to an intense growing interest in the research community to develop computational methods focused on processing this text data. A line of study focused on condensing the text so that we are able to get a higher level of understanding in a shorter time. The two important tasks to do this are keyword extraction and text summarization. In keyword extraction, we are interested in finding the key important words from a text. This makes us familiar with the general topic of a text. In text summarization, we are interested in producing a short-length text which includes important information about the document. The TextRank algorithm, an unsupervised learning method that is an extension of the PageRank (algorithm which is the base algorithm of Google search engine for searching pages and ranking them) has shown its efficacy in large-scale text mining, especially for text summarization and keyword extraction. this algorithm can automatically extract the important parts of a text (keywords or sentences) and declare them as the result. However, this algorithm neglects the semantic similarity between the different parts. In this work, we improved the results of the TextRank algorithm by incorporating the semantic similarity between parts of the text. Aside from keyword extraction and text summarization, we develop a topic clustering algorithm based on our framework which can be used individually or as a part of generating the summary to overcome coverage problems.

</details>

<details>

<summary>2022-12-19 18:49:50 - MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages</summary>

- *Shashank Sonkar, Zichao Wang, Richard G. Baraniuk*

- `2212.09723v1` - [abs](http://arxiv.org/abs/2212.09723v1) - [pdf](http://arxiv.org/pdf/2212.09723v1)

> This paper investigates the problem of Named Entity Recognition (NER) for extreme low-resource languages with only a few hundred tagged data samples. NER is a fundamental task in Natural Language Processing (NLP). A critical driver accelerating NER systems' progress is the existence of large-scale language corpora that enable NER systems to achieve outstanding performance in languages such as English and French with abundant training data. However, NER for low-resource languages remains relatively unexplored. In this paper, we introduce Mask Augmented Named Entity Recognition (MANER), a new methodology that leverages the distributional hypothesis of pre-trained masked language models (MLMs) for NER. The <mask> token in pre-trained MLMs encodes valuable semantic contextual information. MANER re-purposes the <mask> token for NER prediction. Specifically, we prepend the <mask> token to every word in a sentence for which we would like to predict the named entity tag. During training, we jointly fine-tune the MLM and a new NER prediction head attached to each <mask> token. We demonstrate that MANER is well-suited for NER in low-resource languages; our experiments show that for 100 languages with as few as 100 training examples, it improves on state-of-the-art methods by up to 48% and by 12% on average on F1 score. We also perform detailed analyses and ablation studies to understand the scenarios that are best-suited to MANER.

</details>

<details>

<summary>2022-12-19 19:21:47 - MILAN: Masked Image Pretraining on Language Assisted Representation</summary>

- *Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, Sun-Yuan Kung*

- `2208.06049v3` - [abs](http://arxiv.org/abs/2208.06049v3) - [pdf](http://arxiv.org/pdf/2208.06049v3)

> Self-attention based transformer models have been dominating many computer vision tasks in the past few years. Their superb model qualities heavily depend on the excessively large labeled image datasets. In order to reduce the reliance on large labeled datasets, reconstruction based masked autoencoders are gaining popularity, which learn high quality transferable representations from unlabeled images. For the same purpose, recent weakly supervised image pretraining methods explore language supervision from text captions accompanying the images. In this work, we propose masked image pretraining on language assisted representation, dubbed as MILAN. Instead of predicting raw pixels or low level features, our pretraining objective is to reconstruct the image features with substantial semantic signals that are obtained using caption supervision. Moreover, to accommodate our reconstruction target, we propose a more effective prompting decoder architecture and a semantic aware mask sampling mechanism, which further advance the transfer performance of the pretrained model. Experimental results demonstrate that MILAN delivers higher accuracy than the previous works. When the masked autoencoder is pretrained and finetuned on ImageNet-1K dataset with an input resolution of 224x224, MILAN achieves a top-1 accuracy of 85.4% on ViT-Base, surpassing previous state-of-the-arts by 1%. In the downstream semantic segmentation task, MILAN achieves 52.7 mIoU using ViT-Base on ADE20K dataset, outperforming previous masked pretraining results by 4 points.

</details>

<details>

<summary>2022-12-19 20:57:45 - MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders</summary>

- *Xiaofei Li, Daniel Wiechmann, Yu Qiao, Elma Kerz*

- `2212.09855v1` - [abs](http://arxiv.org/abs/2212.09855v1) - [pdf](http://arxiv.org/pdf/2212.09855v1)

> In this paper we present our contribution to the TSAR-2022 Shared Task on Lexical Simplification of the EMNLP 2022 Workshop on Text Simplification, Accessibility, and Readability. Our approach builds on and extends the unsupervised lexical simplification system with pretrained encoders (LSBert) system in the following ways: For the subtask of simplification candidate selection, it utilizes a RoBERTa transformer language model and expands the size of the generated candidate list. For subsequent substitution ranking, it introduces a new feature weighting scheme and adopts a candidate filtering method based on textual entailment to maximize semantic similarity between the target word and its simplification. Our best-performing system improves LSBert by 5.9% accuracy and achieves second place out of 33 ranked solutions.

</details>

<details>

<summary>2022-12-19 22:15:58 - Open-World Entity Segmentation</summary>

- *Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang Zhao, Zhe Lin, Philip Torr, Jiaya Jia*

- `2107.14228v3` - [abs](http://arxiv.org/abs/2107.14228v3) - [pdf](http://arxiv.org/pdf/2107.14228v3)

> We introduce a new image segmentation task, called Entity Segmentation (ES), which aims to segment all visual entities (objects and stuffs) in an image without predicting their semantic labels. By removing the need of class label prediction, the models trained for such task can focus more on improving segmentation quality. It has many practical applications such as image manipulation and editing where the quality of segmentation masks is crucial but class labels are less important. We conduct the first-ever study to investigate the feasibility of convolutional center-based representation to segment things and stuffs in a unified manner, and show that such representation fits exceptionally well in the context of ES. More specifically, we propose a CondInst-like fully-convolutional architecture with two novel modules specifically designed to exploit the class-agnostic and non-overlapping requirements of ES. Experiments show that the models designed and trained for ES significantly outperforms popular class-specific panoptic segmentation models in terms of segmentation quality. Moreover, an ES model can be easily trained on a combination of multiple datasets without the need to resolve label conflicts in dataset merging, and the model trained for ES on one or more datasets can generalize very well to other test datasets of unseen domains. The code has been released at https://github.com/dvlab-research/Entity/.

</details>

<details>

<summary>2022-12-19 22:50:40 - Dexterous Manipulation from Images: Autonomous Real-World RL via Substep Guidance</summary>

- *Kelvin Xu, Zheyuan Hu, Ria Doshi, Aaron Rovinsky, Vikash Kumar, Abhishek Gupta, Sergey Levine*

- `2212.09902v1` - [abs](http://arxiv.org/abs/2212.09902v1) - [pdf](http://arxiv.org/pdf/2212.09902v1)

> Complex and contact-rich robotic manipulation tasks, particularly those that involve multi-fingered hands and underactuated object manipulation, present a significant challenge to any control method. Methods based on reinforcement learning offer an appealing choice for such settings, as they can enable robots to learn to delicately balance contact forces and dexterously reposition objects without strong modeling assumptions. However, running reinforcement learning on real-world dexterous manipulation systems often requires significant manual engineering. This negates the benefits of autonomous data collection and ease of use that reinforcement learning should in principle provide. In this paper, we describe a system for vision-based dexterous manipulation that provides a "programming-free" approach for users to define new tasks and enable robots with complex multi-fingered hands to learn to perform them through interaction. The core principle underlying our system is that, in a vision-based setting, users should be able to provide high-level intermediate supervision that circumvents challenges in teleoperation or kinesthetic teaching which allow a robot to not only learn a task efficiently but also to autonomously practice. Our system includes a framework for users to define a final task and intermediate sub-tasks with image examples, a reinforcement learning procedure that learns the task autonomously without interventions, and experimental results with a four-finger robotic hand learning multi-stage object manipulation tasks directly in the real world, without simulation, manual modeling, or reward engineering.

</details>

<details>

<summary>2022-12-19 23:50:19 - Generalizing Multimodal Variational Methods to Sets</summary>

- *Jinzhao Zhou, Yiqun Duan, Zhihong Chen, Yu-Cheng Chang, Chin-Teng Lin*

- `2212.09918v1` - [abs](http://arxiv.org/abs/2212.09918v1) - [pdf](http://arxiv.org/pdf/2212.09918v1)

> Making sense of multiple modalities can yield a more comprehensive description of real-world phenomena. However, learning the co-representation of diverse modalities is still a long-standing endeavor in emerging machine learning applications and research. Previous generative approaches for multimodal input approximate a joint-modality posterior by uni-modality posteriors as product-of-experts (PoE) or mixture-of-experts (MoE). We argue that these approximations lead to a defective bound for the optimization process and loss of semantic connection among modalities. This paper presents a novel variational method on sets called the Set Multimodal VAE (SMVAE) for learning a multimodal latent space while handling the missing modality problem. By modeling the joint-modality posterior distribution directly, the proposed SMVAE learns to exchange information between multiple modalities and compensate for the drawbacks caused by factorization. In public datasets of various domains, the experimental results demonstrate that the proposed method is applicable to order-agnostic cross-modal generation while achieving outstanding performance compared to the state-of-the-art multimodal methods. The source code for our method is available online https://anonymous.4open.science/r/SMVAE-9B3C/.

</details>

<details>

<summary>2022-12-20 01:52:46 - Dialog2API: Task-Oriented Dialogue with API Description and Example Programs</summary>

- *Raphael Shu, Elman Mansimov, Tamer Alkhouli, Nikolaos Pappas, Salvatore Romeo, Arshit Gupta, Saab Mansour, Yi Zhang, Dan Roth*

- `2212.09946v1` - [abs](http://arxiv.org/abs/2212.09946v1) - [pdf](http://arxiv.org/pdf/2212.09946v1)

> Functionality and dialogue experience are two important factors of task-oriented dialogue systems. Conventional approaches with closed schema (e.g., conversational semantic parsing) often fail as both the functionality and dialogue experience are strongly constrained by the underlying schema. We introduce a new paradigm for task-oriented dialogue - Dialog2API - to greatly expand the functionality and provide seamless dialogue experience. The conversational model interacts with the environment by generating and executing programs triggering a set of pre-defined APIs. The model also manages the dialogue policy and interact with the user through generating appropriate natural language responses. By allowing generating free-form programs, Dialog2API supports composite goals by combining different APIs, whereas unrestricted program revision provides natural and robust dialogue experience. To facilitate Dialog2API, the core model is provided with API documents, an execution environment and optionally some example dialogues annotated with programs. We propose an approach tailored for the Dialog2API, where the dialogue states are represented by a stack of programs, with most recently mentioned program on the top of the stack. Dialog2API can work with many application scenarios such as software automation and customer service. In this paper, we construct a dataset for AWS S3 APIs and present evaluation results of in-context learning baselines.

</details>

<details>

<summary>2022-12-20 03:28:41 - Insights into undergraduate pathways using course load analytics</summary>

- *Conrad Borchers, Zachary A. Pardos*

- `2212.09974v1` - [abs](http://arxiv.org/abs/2212.09974v1) - [pdf](http://arxiv.org/pdf/2212.09974v1)

> Course load analytics (CLA) inferred from LMS and enrollment features can offer a more accurate representation of course workload to students than credit hours and potentially aid in their course selection decisions. In this study, we produce and evaluate the first machine-learned predictions of student course load ratings and generalize our model to the full 10,000 course catalog of a large public university. We then retrospectively analyze longitudinal differences in the semester load of student course selections throughout their degree. CLA by semester shows that a student's first semester at the university is among their highest load semesters, as opposed to a credit hour-based analysis, which would indicate it is among their lowest. Investigating what role predicted course load may play in program retention, we find that students who maintain a semester load that is low as measured by credit hours but high as measured by CLA are more likely to leave their program of study. This discrepancy in course load is particularly pertinent in STEM and associated with high prerequisite courses. Our findings have implications for academic advising, institutional handling of the freshman experience, and student-facing analytics to help students better plan, anticipate, and prepare for their selected courses.

</details>

<details>

<summary>2022-12-20 03:33:26 - Sophisticated deep learning with on-chip optical diffractive tensor processing</summary>

- *Yuyao Huang, Tingzhao Fu, Honghao Huang, Sigang Yang, Hongwei Chen*

- `2212.09975v1` - [abs](http://arxiv.org/abs/2212.09975v1) - [pdf](http://arxiv.org/pdf/2212.09975v1)

> The ever-growing deep learning technologies are making revolutionary changes for modern life. However, conventional computing architectures are designed to process sequential and digital programs, being extremely burdened with performing massive parallel and adaptive deep learning applications. Photonic integrated circuits provide an efficient approach to mitigate bandwidth limitations and power-wall brought by its electronic counterparts, showing great potential in ultrafast and energy-free high-performance computing. Here, we propose an optical computing architecture enabled by on-chip diffraction to implement convolutional acceleration, termed optical convolution unit (OCU). We demonstrate that any real-valued convolution kernels can be exploited by OCU with a prominent computational throughput boosting via the concept of structral re-parameterization. With OCU as the fundamental unit, we build an optical convolutional neural network (oCNN) to implement two popular deep learning tasks: classification and regression. For classification, Fashion-MNIST and CIFAR-4 datasets are tested with accuracy of 91.63% and 86.25%, respectively. For regression, we build an optical denoising convolutional neural network (oDnCNN) to handle Gaussian noise in gray scale images with noise level {\sigma} = 10, 15, 20, resulting clean images with average PSNR of 31.70dB, 29.39dB and 27.72dB, respectively. The proposed OCU presents remarkable performance of low energy consumption and high information density due to its fully passive nature and compact footprint, providing a highly parallel while lightweight solution for future computing architecture to handle high dimensional tensors in deep learning.

</details>

<details>

<summary>2022-12-20 08:34:56 - A Survey on Pretrained Language Models for Neural Code Intelligence</summary>

- *Yichen Xu, Yanqiao Zhu*

- `2212.10079v1` - [abs](http://arxiv.org/abs/2212.10079v1) - [pdf](http://arxiv.org/pdf/2212.10079v1)

> As the complexity of modern software continues to escalate, software engineering has become an increasingly daunting and error-prone endeavor. In recent years, the field of Neural Code Intelligence (NCI) has emerged as a promising solution, leveraging the power of deep learning techniques to tackle analytical tasks on source code with the goal of improving programming efficiency and minimizing human errors within the software industry. Pretrained language models have become a dominant force in NCI research, consistently delivering state-of-the-art results across a wide range of tasks, including code summarization, generation, and translation. In this paper, we present a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures. We hope this paper will serve as a bridge between the natural language and programming language communities, offering insights for future research in this rapidly evolving field.

</details>

<details>

<summary>2022-12-20 09:10:25 - Visual Transformers for Primates Classification and Covid Detection</summary>

- *Steffen Illium, Robert MÃ¼ller, Andreas Sedlmeier, Claudia-Linnhoff Popien*

- `2212.10093v1` - [abs](http://arxiv.org/abs/2212.10093v1) - [pdf](http://arxiv.org/pdf/2212.10093v1)

> We apply the vision transformer, a deep machine learning model build around the attention mechanism, on mel-spectrogram representations of raw audio recordings. When adding mel-based data augmentation techniques and sample-weighting, we achieve comparable performance on both (PRS and CCS challenge) tasks of ComParE21, outperforming most single model baselines. We further introduce overlapping vertical patching and evaluate the influence of parameter configurations. Index Terms: audio classification, attention, mel-spectrogram, unbalanced data-sets, computational paralinguistics

</details>

<details>

<summary>2022-12-20 09:23:32 - Interactive Model with Structural Loss for Language-based Abductive Reasoning</summary>

- *Linhao Li, Ming Xu, Yongfeng Dong, Xin Li, Ao Wang*

- `2112.00284v2` - [abs](http://arxiv.org/abs/2112.00284v2) - [pdf](http://arxiv.org/pdf/2112.00284v2)

> The abductive natural language inference task ($\alpha$NLI) is proposed to infer the most plausible explanation between the cause and the event. In the $\alpha$NLI task, two observations are given, and the most plausible hypothesis is asked to pick out from the candidates. Existing methods model the relation between each candidate hypothesis separately and penalize the inference network uniformly. In this paper, we argue that it is unnecessary to distinguish the reasoning abilities among correct hypotheses; and similarly, all wrong hypotheses contribute the same when explaining the reasons of the observations. Therefore, we propose to group instead of ranking the hypotheses and design a structural loss called ``joint softmax focal loss'' in this paper. Based on the observation that the hypotheses are generally semantically related, we have designed a novel interactive language model aiming at exploiting the rich interaction among competing hypotheses. We name this new model for $\alpha$NLI: Interactive Model with Structural Loss (IMSL). The experimental results show that our IMSL has achieved the highest performance on the RoBERTa-large pretrained model, with ACC and AUC results increased by about 1\% and 5\% respectively.

</details>

<details>

<summary>2022-12-20 11:17:52 - Document-level Relation Extraction with Relation Correlations</summary>

- *Ridong Han, Tao Peng, Benyou Wang, Lu Liu, Xiang Wan*

- `2212.10171v1` - [abs](http://arxiv.org/abs/2212.10171v1) - [pdf](http://arxiv.org/pdf/2212.10171v1)

> Document-level relation extraction faces two overlooked challenges: long-tail problem and multi-label problem. Previous work focuses mainly on obtaining better contextual representations for entity pairs, hardly address the above challenges. In this paper, we analyze the co-occurrence correlation of relations, and introduce it into DocRE task for the first time. We argue that the correlations can not only transfer knowledge between data-rich relations and data-scarce ones to assist in the training of tailed relations, but also reflect semantic distance guiding the classifier to identify semantically close relations for multi-label entity pairs. Specifically, we use relation embedding as a medium, and propose two co-occurrence prediction sub-tasks from both coarse- and fine-grained perspectives to capture relation correlations. Finally, the learned correlation-aware embeddings are used to guide the extraction of relational facts. Substantial experiments on two popular DocRE datasets are conducted, and our method achieves superior results compared to baselines. Insightful analysis also demonstrates the potential of relation correlations to address the above challenges.

</details>

<details>

<summary>2022-12-20 11:52:52 - Unsupervised Question Duplicate and Related Questions Detection in e-learning platforms</summary>

- *Maksimjeet Chowdhary, Sanyam Goyal, Venktesh V, Mukesh Mohania, Vikram Goyal*

- `2301.05150v1` - [abs](http://arxiv.org/abs/2301.05150v1) - [pdf](http://arxiv.org/pdf/2301.05150v1)

> Online learning platforms provide diverse questions to gauge the learners' understanding of different concepts. The repository of questions has to be constantly updated to ensure a diverse pool of questions to conduct assessments for learners. However, it is impossible for the academician to manually skim through the large repository of questions to check for duplicates when onboarding new questions from external sources. Hence, we propose a tool QDup in this paper that can surface near-duplicate and semantically related questions without any supervised data. The proposed tool follows an unsupervised hybrid pipeline of statistical and neural approaches for incorporating different nuances in similarity for the task of question duplicate detection. We demonstrate that QDup can detect near-duplicate questions and also suggest related questions for practice with remarkable accuracy and speed from a large repository of questions. The demo video of the tool can be found at https://www.youtube.com/watch?v=loh0_-7XLW4.

</details>

<details>

<summary>2022-12-20 14:11:31 - ReCode: Robustness Evaluation of Code Generation Models</summary>

- *Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, Bing Xiang*

- `2212.10264v1` - [abs](http://arxiv.org/abs/2212.10264v1) - [pdf](http://arxiv.org/pdf/2212.10264v1)

> Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model's robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.

</details>

<details>

<summary>2022-12-20 14:29:43 - What do you MEME? Generating Explanations for Visual Semantic Role Labelling in Memes</summary>

- *Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md. Shad Akhtar, Tanmoy Chakraborty*

- `2212.00715v2` - [abs](http://arxiv.org/abs/2212.00715v2) - [pdf](http://arxiv.org/pdf/2212.00715v2)

> Memes are powerful means for effective communication on social media. Their effortless amalgamation of viral visuals and compelling messages can have far-reaching implications with proper marketing. Previous research on memes has primarily focused on characterizing their affective spectrum and detecting whether the meme's message insinuates any intended harm, such as hate, offense, racism, etc. However, memes often use abstraction, which can be elusive. Here, we introduce a novel task - EXCLAIM, generating explanations for visual semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset that offers natural language explanations of connotative roles for three types of entities - heroes, villains, and victims, encompassing 4,680 entities present in 3K memes. We also benchmark ExHVV with several strong unimodal and multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task learning framework that endeavors to address EXCLAIM optimally by jointly learning to predict the correct semantic roles and correspondingly to generate suitable natural language explanations. LUMEN distinctly outperforms the best baseline across 18 standard natural language generation evaluation metrics. Our systematic evaluation and analyses demonstrate that characteristic multimodal cues required for adjudicating semantic roles are also helpful for generating suitable explanations.

</details>

<details>

<summary>2022-12-20 14:54:47 - Efficient and Sound Differentiable Programming in a Functional Array-Processing Language</summary>

- *Amir Shaikhha, Mathieu Huot, Shabnam Ghasemirad, Andrew Fitzgibbon, Simon Peyton Jones, Dimitrios Vytiniotis*

- `2212.10307v1` - [abs](http://arxiv.org/abs/2212.10307v1) - [pdf](http://arxiv.org/pdf/2212.10307v1)

> Automatic differentiation (AD) is a technique for computing the derivative of a function represented by a program. This technique is considered as the de-facto standard for computing the differentiation in many machine learning and optimisation software tools. Despite the practicality of this technique, the performance of the differentiated programs, especially for functional languages and in the presence of vectors, is suboptimal. We present an AD system for a higher-order functional array-processing language. The core functional language underlying this system simultaneously supports both source-to-source forward-mode AD and global optimisations such as loop transformations. In combination, gradient computation with forward-mode AD can be as efficient as reverse mode, and the Jacobian matrices required for numerical algorithms such as Gauss-Newton and Levenberg-Marquardt can be efficiently computed.

</details>

<details>

<summary>2022-12-20 17:13:22 - Is Semantic Communications Secure? A Tale of Multi-Domain Adversarial Attacks</summary>

- *Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener*

- `2212.10438v1` - [abs](http://arxiv.org/abs/2212.10438v1) - [pdf](http://arxiv.org/pdf/2212.10438v1)

> Semantic communications seeks to transfer information from a source while conveying a desired meaning to its destination. We model the transmitter-receiver functionalities as an autoencoder followed by a task classifier that evaluates the meaning of the information conveyed to the receiver. The autoencoder consists of an encoder at the transmitter to jointly model source coding, channel coding, and modulation, and a decoder at the receiver to jointly model demodulation, channel decoding and source decoding. By augmenting the reconstruction loss with a semantic loss, the two deep neural networks (DNNs) of this encoder-decoder pair are interactively trained with the DNN of the semantic task classifier. This approach effectively captures the latent feature space and reliably transfers compressed feature vectors with a small number of channel uses while keeping the semantic loss low. We identify the multi-domain security vulnerabilities of using the DNNs for semantic communications. Based on adversarial machine learning, we introduce test-time (targeted and non-targeted) adversarial attacks on the DNNs by manipulating their inputs at different stages of semantic communications. As a computer vision attack, small perturbations are injected to the images at the input of the transmitter's encoder. As a wireless attack, small perturbations signals are transmitted to interfere with the input of the receiver's decoder. By launching these stealth attacks individually or more effectively in a combined form as a multi-domain attack, we show that it is possible to change the semantics of the transferred information even when the reconstruction loss remains low. These multi-domain adversarial attacks pose as a serious threat to the semantics of information transfer (with larger impact than conventional jamming) and raise the need of defense methods for the safe adoption of semantic communications.

</details>

<details>

<summary>2022-12-20 18:55:54 - RangeAugment: Efficient Online Augmentation with Range Learning</summary>

- *Sachin Mehta, Saeid Naderiparizi, Fartash Faghri, Maxwell Horton, Lailin Chen, Ali Farhadi, Oncel Tuzel, Mohammad Rastegari*

- `2212.10553v1` - [abs](http://arxiv.org/abs/2212.10553v1) - [pdf](http://arxiv.org/pdf/2212.10553v1)

> State-of-the-art automatic augmentation methods (e.g., AutoAugment and RandAugment) for visual recognition tasks diversify training data using a large set of augmentation operations. The range of magnitudes of many augmentation operations (e.g., brightness and contrast) is continuous. Therefore, to make search computationally tractable, these methods use fixed and manually-defined magnitude ranges for each operation, which may lead to sub-optimal policies. To answer the open question on the importance of magnitude ranges for each augmentation operation, we introduce RangeAugment that allows us to efficiently learn the range of magnitudes for individual as well as composite augmentation operations. RangeAugment uses an auxiliary loss based on image similarity as a measure to control the range of magnitudes of augmentation operations. As a result, RangeAugment has a single scalar parameter for search, image similarity, which we simply optimize via linear search. RangeAugment integrates seamlessly with any model and learns model- and task-specific augmentation policies. With extensive experiments on the ImageNet dataset across different networks, we show that RangeAugment achieves competitive performance to state-of-the-art automatic augmentation methods with 4-5 times fewer augmentation operations. Experimental results on semantic segmentation, object detection, foundation models, and knowledge distillation further shows RangeAugment's effectiveness.

</details>

<details>

<summary>2022-12-20 19:29:37 - Temporal Output Discrepancy for Loss Estimation-based Active Learning</summary>

- *Siyu Huang, Tianyang Wang, Haoyi Xiong, Bihan Wen, Jun Huan, Dejing Dou*

- `2212.10613v1` - [abs](http://arxiv.org/abs/2212.10613v1) - [pdf](http://arxiv.org/pdf/2212.10613v1)

> While deep learning succeeds in a wide range of tasks, it highly depends on the massive collection of annotated data which is expensive and time-consuming. To lower the cost of data annotation, active learning has been proposed to interactively query an oracle to annotate a small proportion of informative samples in an unlabeled dataset. Inspired by the fact that the samples with higher loss are usually more informative to the model than the samples with lower loss, in this paper we present a novel deep active learning approach that queries the oracle for data annotation when the unlabeled sample is believed to incorporate high loss. The core of our approach is a measurement Temporal Output Discrepancy (TOD) that estimates the sample loss by evaluating the discrepancy of outputs given by models at different optimization steps. Our theoretical investigation shows that TOD lower-bounds the accumulated sample loss thus it can be used to select informative unlabeled samples. On basis of TOD, we further develop an effective unlabeled data sampling strategy as well as an unsupervised learning criterion for active learning. Due to the simplicity of TOD, our methods are efficient, flexible, and task-agnostic. Extensive experimental results demonstrate that our approach achieves superior performances than the state-of-the-art active learning methods on image classification and semantic segmentation tasks. In addition, we show that TOD can be utilized to select the best model of potentially the highest testing accuracy from a pool of candidate models.

</details>

<details>

<summary>2022-12-20 20:25:47 - An Evaluation of the State-of-the-Art Software and Hardware Implementations of BIKE</summary>

- *Andrea Galimberti, Gabriele Montanaro, William Fornaciari, Davide Zoni*

- `2212.10636v1` - [abs](http://arxiv.org/abs/2212.10636v1) - [pdf](http://arxiv.org/pdf/2212.10636v1)

> NIST is conducting a process for the standardization of post-quantum cryptosystems, i.e., cryptosystems that are resistant to attacks by both traditional and quantum computers and that can thus substitute the traditional public-key cryptography solutions which are expected to be broken by quantum computers in the next decades. This manuscript provides an overview and a comparison of the existing state-of-the-art implementations of the BIKE QC-MDPC code-based post-quantum KEM, a candidate in NIST's PQC standardization process. We consider both software, hardware, and mixed hardware-software implementations and evaluate their performance and, for hardware ones, their resource utilization.

</details>

<details>

<summary>2022-12-20 21:29:16 - AutoMESC: Automatic Framework for Mining and Classifying Ethereum Smart Contract Vulnerabilities and Their Fixes</summary>

- *Majd Soud, Ilham Qasse, Grischa Liebel, Mohammad Hamdaqa*

- `2212.10660v1` - [abs](http://arxiv.org/abs/2212.10660v1) - [pdf](http://arxiv.org/pdf/2212.10660v1)

> Due to the risks associated with vulnerabilities in smart contracts, their security has gained significant attention in recent years. However, there is a lack of open datasets on smart contract vulnerabilities and their fixes that allows for data-driven research. Towards this end, we propose an automated method for mining and classifying Ethereum's smart contract vulnerabilities and their corresponding fixes from GitHub and from the Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database. We implemented the proposed method in a fully automated framework, which we call AutoMESC. AutoMESC uses seven of the most well-known smart contract security tools to classify and label the collected vulnerabilities based on vulnerability types. Furthermore, it collects metadata that can be used in data-intensive smart contract security research (e.g., vulnerability detection, vulnerability classification, severity prediction, and automated repair). We used AutoMESC to construct a sample dataset and made it publicly available. Currently, the dataset contains 6.7K smart contracts' vulnerability-fix pairs written in Solidity. We assess the quality of the constructed dataset in terms of accuracy, provenance, and relevance, and compare it with existing datasets. AutoMESC is designed to collect data continuously and keep the corresponding dataset up-to-date with newly discovered smart contract vulnerabilities and their fixes from GitHub and CVE records.

</details>

<details>

<summary>2022-12-20 22:53:19 - A Physics-Informed Neural Network to Model Port Channels</summary>

- *Marlon S. Mathias, Marcel R. de Barros, Jefferson F. Coelho, Lucas P. de Freitas, Felipe M. Moreno, Caio F. D. Netto, Fabio G. Cozman, Anna H. R. Costa, Eduardo A. Tannuri, Edson S. Gomi, Marcelo Dottori*

- `2212.10681v1` - [abs](http://arxiv.org/abs/2212.10681v1) - [pdf](http://arxiv.org/pdf/2212.10681v1)

> We describe a Physics-Informed Neural Network (PINN) that simulates the flow induced by the astronomical tide in a synthetic port channel, with dimensions based on the Santos - S\~ao Vicente - Bertioga Estuarine System. PINN models aim to combine the knowledge of physical systems and data-driven machine learning models. This is done by training a neural network to minimize the residuals of the governing equations in sample points. In this work, our flow is governed by the Navier-Stokes equations with some approximations. There are two main novelties in this paper. First, we design our model to assume that the flow is periodic in time, which is not feasible in conventional simulation methods. Second, we evaluate the benefit of resampling the function evaluation points during training, which has a near zero computational cost and has been verified to improve the final model, especially for small batch sizes. Finally, we discuss some limitations of the approximations used in the Navier-Stokes equations regarding the modeling of turbulence and how it interacts with PINNs.

</details>

<details>

<summary>2022-12-21 00:53:03 - Empowering First-Year Computer Science Ph.D. Students to Create a Culture that Values Community and Mental Health</summary>

- *Yaniv Yacoby, John Girash, David C. Parkes*

- `2208.12650v3` - [abs](http://arxiv.org/abs/2208.12650v3) - [pdf](http://arxiv.org/pdf/2208.12650v3)

> Doctoral programs often have high rates of depression, anxiety, isolation, and imposter phenomenon. Consequently, graduating students may feel inadequately prepared for research-focused careers, contributing to an attrition of talent. Prior work identifies an important contributing factor to maladjustment: even with prior exposure to research, entering Ph.D. students often have problematically idealized views of science. These preconceptions can become obstacles for students in their own professional growth. Unfortunately, existing curricular and extracurricular programming in many doctoral programs fail to include mechanisms to systematically address students' misconceptions of their profession. In this work, we describe a new initiative at our institution that aims to address Ph.D. mental health via a mandatory seminar for entering doctoral students. The seminar is designed to build professional resilience in students by (1) increasing self-regulatory competence, and (2) teaching students to proactively examine academic cultural values and to participate in shaping them. Our evaluation indicates that students improved in both areas after completing the seminar.

</details>

<details>

<summary>2022-12-21 02:34:12 - Comparison and Evaluation of Methods for a Predict+Optimize Problem in Renewable Energy</summary>

- *Christoph Bergmeir, Frits de Nijs, Abishek Sriramulu, Mahdi Abolghasemi, Richard Bean, John Betts, Quang Bui, Nam Trong Dinh, Nils Einecke, Rasul Esmaeilbeigi, Scott Ferraro, Priya Galketiya, Evgenii Genov, Robert Glasgow, Rakshitha Godahewa, Yanfei Kang, Steffen Limmer, Luis Magdalena, Pablo Montero-Manso, Daniel Peralta, Yogesh Pipada Sunil Kumar, Alejandro Rosales-PÃ©rez, Julian Ruddick, Akylas Stratigakos, Peter Stuckey, Guido Tack, Isaac Triguero, Rui Yuan*

- `2212.10723v1` - [abs](http://arxiv.org/abs/2212.10723v1) - [pdf](http://arxiv.org/pdf/2212.10723v1)

> Algorithms that involve both forecasting and optimization are at the core of solutions to many difficult real-world problems, such as in supply chains (inventory optimization), traffic, and in the transition towards carbon-free energy generation in battery/load/production scheduling in sustainable energy systems. Typically, in these scenarios we want to solve an optimization problem that depends on unknown future values, which therefore need to be forecast. As both forecasting and optimization are difficult problems in their own right, relatively few research has been done in this area. This paper presents the findings of the ``IEEE-CIS Technical Challenge on Predict+Optimize for Renewable Energy Scheduling," held in 2021. We present a comparison and evaluation of the seven highest-ranked solutions in the competition, to provide researchers with a benchmark problem and to establish the state of the art for this benchmark, with the aim to foster and facilitate research in this area. The competition used data from the Monash Microgrid, as well as weather data and energy market data. It then focused on two main challenges: forecasting renewable energy production and demand, and obtaining an optimal schedule for the activities (lectures) and on-site batteries that lead to the lowest cost of energy. The most accurate forecasts were obtained by gradient-boosted tree and random forest models, and optimization was mostly performed using mixed integer linear and quadratic programming. The winning method predicted different scenarios and optimized over all scenarios jointly using a sample average approximation method.

</details>

<details>

<summary>2022-12-21 02:47:52 - Spoken Language Understanding for Conversational AI: Recent Advances and Future Direction</summary>

- *Soyeon Caren Han, Siqu Long, Henry Weld, Josiah Poon*

- `2212.10728v1` - [abs](http://arxiv.org/abs/2212.10728v1) - [pdf](http://arxiv.org/pdf/2212.10728v1)

> When a human communicates with a machine using natural language on the web and online, how can it understand the human's intention and semantic context of their talk? This is an important AI task as it enables the machine to construct a sensible answer or perform a useful action for the human. Meaning is represented at the sentence level, identification of which is known as intent detection, and at the word level, a labelling task called slot filling. This dual-level joint task requires innovative thinking about natural language and deep learning network design, and as a result, many approaches and models have been proposed and applied.   This tutorial will discuss how the joint task is set up and introduce Spoken Language Understanding/Natural Language Understanding (SLU/NLU) with Deep Learning techniques. We will cover the datasets, experiments and metrics used in the field. We will describe how the machine uses the latest NLP and Deep Learning techniques to address the joint task, including recurrent and attention-based Transformer networks and pre-trained models (e.g. BERT). We will then look in detail at a network that allows the two levels of the task, intent classification and slot filling, to interact to boost performance explicitly. We will do a code demonstration of a Python notebook for this model and attendees will have an opportunity to watch coding demo tasks on this joint NLU to further their understanding.

</details>

<details>

<summary>2022-12-21 03:09:06 - Unsupervised Learning of Neurosymbolic Encoders</summary>

- *Eric Zhan, Jennifer J. Sun, Ann Kennedy, Yisong Yue, Swarat Chaudhuri*

- `2107.13132v2` - [abs](http://arxiv.org/abs/2107.13132v2) - [pdf](http://arxiv.org/pdf/2107.13132v2)

> We present a framework for the unsupervised learning of neurosymbolic encoders, which are encoders obtained by composing neural networks with symbolic programs from a domain-specific language. Our framework naturally incorporates symbolic expert knowledge into the learning process, which leads to more interpretable and factorized latent representations compared to fully neural encoders. We integrate modern program synthesis techniques with the variational autoencoding (VAE) framework, in order to learn a neurosymbolic encoder in conjunction with a standard decoder. The programmatic descriptions from our encoders can benefit many analysis workflows, such as in behavior modeling where interpreting agent actions and movements is important. We evaluate our method on learning latent representations for real-world trajectory data from animal biology and sports analytics. We show that our approach offers significantly better separation of meaningful categories than standard VAEs and leads to practical gains on downstream analysis tasks, such as for behavior classification.

</details>

<details>

<summary>2022-12-21 03:22:24 - ToL: A Tensor of List-Based Unified Computation Model</summary>

- *Hongxiao Li, Wanling Gao, Lei Wang, Jianfeng Zhan*

- `2212.10740v1` - [abs](http://arxiv.org/abs/2212.10740v1) - [pdf](http://arxiv.org/pdf/2212.10740v1)

> Previous computation models either have equivalent abilities in representing all computations but fail to provide primitive operators for programming complex algorithms or lack generalized expression ability to represent newly-added computations. This article presents a unified computation model with generalized expression ability and a concise set of primitive operators for programming high-level algorithms. We propose a unified data abstraction -- Tensor of List, and offer a unified computation model based on Tensor of List, which we call the ToL model (in short, ToL). ToL introduces five atomic computations that can represent any elementary computation by finite composition, ensured with strict formal proof. Based on ToL, we design a pure-functional language -- ToLang. ToLang provides a concise set of primitive operators that can be used to program complex big data and AI algorithms. Our evaluations show ToL has generalized expression ability and a built-in performance indicator, born with a strictly defined computation metric -- elementary operation count (EOPs), consistent with FLOPs within a small error range.

</details>

<details>

<summary>2022-12-21 03:37:38 - The Internet of Senses: Building on Semantic Communications and Edge Intelligence</summary>

- *Roghayeh Joda, Medhat Elsayed, Hatem Abou-zeid, Ramy Atawia, Akram Bin Sediq, Gary Boudreau, Melike Erol-Kantarci, Lajos Hanzo*

- `2212.10748v1` - [abs](http://arxiv.org/abs/2212.10748v1) - [pdf](http://arxiv.org/pdf/2212.10748v1)

> The Internet of Senses (IoS) holds the promise of flawless telepresence-style communication for all human `receptors' and therefore blurs the difference of virtual and real environments. We commence by highlighting the compelling use cases empowered by the IoS and also the key network requirements. We then elaborate on how the emerging semantic communications and Artificial Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies may satisfy the requirements of IoS use cases. On one hand, semantic communications can be applied for extracting meaningful and significant information and hence efficiently exploit the resources and for harnessing a priori information at the receiver to satisfy IoS requirements. On the other hand, AI/ML facilitates frugal network resource management by making use of the enormous amount of data generated in IoS edge nodes and devices, as well as by optimizing the IoS performance via intelligent agents. However, the intelligent agents deployed at the edge are not completely aware of each others' decisions and the environments of each other, hence they operate in a partially rather than fully observable environment. Therefore, we present a case study of Partially Observable Markov Decision Processes (POMDP) for improving the User Equipment (UE) throughput and energy consumption, as they are imperative for IoS use cases, using Reinforcement Learning for astutely activating and deactivating the component carriers in carrier aggregation. Finally, we outline the challenges and open issues of IoS implementations and employing semantic communications, edge intelligence as well as learning under partial observability in the IoS context.

</details>

<details>

<summary>2022-12-21 05:02:49 - ImPaKT: A Dataset for Open-Schema Knowledge Base Construction</summary>

- *Luke Vilnis, Zach Fisher, Bhargav Kanagal, Patrick Murray, Sumit Sanghai*

- `2212.10770v1` - [abs](http://arxiv.org/abs/2212.10770v1) - [pdf](http://arxiv.org/pdf/2212.10770v1)

> Large language models have ushered in a golden age of semantic parsing. The seq2seq paradigm allows for open-schema and abstractive attribute and relation extraction given only small amounts of finetuning data. Language model pretraining has simultaneously enabled great strides in natural language inference, reasoning about entailment and implication in free text. These advances motivate us to construct ImPaKT, a dataset for open-schema information extraction, consisting of around 2500 text snippets from the C4 corpus, in the shopping domain (product buying guides), professionally annotated with extracted attributes, types, attribute summaries (attribute schema discovery from idiosyncratic text), many-to-one relations between compound and atomic attributes, and implication relations. We release this data in hope that it will be useful in fine tuning semantic parsers for information extraction and knowledge base construction across a variety of domains. We evaluate the power of this approach by fine-tuning the open source UL2 language model on a subset of the dataset, extracting a set of implication relations from a corpus of product buying guides, and conducting human evaluations of the resulting predictions.

</details>

<details>

<summary>2022-12-21 07:06:55 - ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models</summary>

- *Dheeraj Mekala, Jason Wolfe, Subhro Roy*

- `2212.10815v1` - [abs](http://arxiv.org/abs/2212.10815v1) - [pdf](http://arxiv.org/pdf/2212.10815v1)

> We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes a semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems, enabling us to leverage the ability of LLMs to zero-shot answer reading comprehension questions. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. To address this problem, we fine-tune a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can correctly parse ~16% of utterances in the MTOP dataset without requiring any annotated data.

</details>

<details>

<summary>2022-12-21 07:24:03 - Complete the Missing Half: Augmenting Aggregation Filtering with Diversification for Graph Convolutional Neural Networks</summary>

- *Sitao Luan, Mingde Zhao, Chenqing Hua, Xiao-Wen Chang, Doina Precup*

- `2212.10822v1` - [abs](http://arxiv.org/abs/2212.10822v1) - [pdf](http://arxiv.org/pdf/2212.10822v1)

> The core operation of current Graph Neural Networks (GNNs) is the aggregation enabled by the graph Laplacian or message passing, which filters the neighborhood information of nodes. Though effective for various tasks, in this paper, we show that they are potentially a problematic factor underlying all GNN models for learning on certain datasets, as they force the node representations similar, making the nodes gradually lose their identity and become indistinguishable. Hence, we augment the aggregation operations with their dual, i.e. diversification operators that make the node more distinct and preserve the identity. Such augmentation replaces the aggregation with a two-channel filtering process that, in theory, is beneficial for enriching the node representations. In practice, the proposed two-channel filters can be easily patched on existing GNN methods with diverse training strategies, including spectral and spatial (message passing) methods. In the experiments, we observe desired characteristics of the models and significant performance boost upon the baselines on 9 node classification tasks.

</details>

<details>

<summary>2022-12-21 08:18:13 - EXK-SC: A Semantic Communication Model Based on Information Framework Expansion and Knowledge Collision</summary>

- *Gangtao Xin, Pingyi Fan*

- `2210.13047v2` - [abs](http://arxiv.org/abs/2210.13047v2) - [pdf](http://arxiv.org/pdf/2210.13047v2)

> Semantic communication is not focused on improving the accuracy of transmitted symbols, but is concerned with expressing the expected meaning that the symbol sequence exactly carries. However, the measurement of semantic messages and their corresponding codebook generation are still open issues. Expansion, which integrates simple things into a complex system and even generates intelligence, is truly consistent with the evolution of the human language system. We apply this idea to the semantic communication system, quantifying semantic transmission by symbol sequences and investigating the semantic information system in a similar way as Shannon's method for digital communication systems. This work is the first to discuss semantic expansion and knowledge collision in the semantic information framework. Some important theoretical results are presented, including the relationship between semantic expansion and the transmission information rate. We believe such a semantic information framework may provide a new paradigm for semantic communications, and semantic expansion and knowledge collision will be the cornerstone of semantic information theory.

</details>

<details>

<summary>2022-12-21 08:30:24 - Predicting the Score of Atomic Candidate OWL Class Axioms</summary>

- *Ali Ballout, Andrea G B Tettamanzi, CÃ©lia da Costa Pereira*

- `2212.10841v1` - [abs](http://arxiv.org/abs/2212.10841v1) - [pdf](http://arxiv.org/pdf/2212.10841v1)

> Candidate axiom scoring is the task of assessing the acceptability of a candidate axiom against the evidence provided by known facts or data. The ability to score candidate axioms reliably is required for automated schema or ontology induction, but it can also be valuable for ontology and/or knowledge graph validation. Accurate axiom scoring heuristics are often computationally expensive, which is an issue if you wish to use them in iterative search techniques like level-wise generate-and-test or evolutionary algorithms, which require scoring a large number of candidate axioms. We address the problem of developing a predictive model as a substitute for reasoning that predicts the possibility score of candidate class axioms and is quick enough to be employed in such situations. We use a semantic similarity measure taken from an ontology's subsumption structure for this purpose. We show that the approach provided in this work can accurately learn the possibility scores of candidate OWL class axioms and that it can do so for a variety of OWL class axioms.

</details>

<details>

<summary>2022-12-21 09:08:19 - PABAU: Privacy Analysis of Biometric API Usage</summary>

- *Feiyang Tang*

- `2212.10861v1` - [abs](http://arxiv.org/abs/2212.10861v1) - [pdf](http://arxiv.org/pdf/2212.10861v1)

> Biometric data privacy is becoming a major concern for many organizations in the age of big data, particularly in the ICT sector, because it may be easily exploited in apps. Most apps utilize biometrics by accessing common application programming interfaces (APIs); hence, we aim to categorize their usage. The categorization based on behavior may be closely correlated with the sensitive processing of a user's biometric data, hence highlighting crucial biometric data privacy assessment concerns. We propose PABAU, Privacy Analysis of Biometric API Usage. PABAU learns semantic features of methods in biometric APIs and uses them to detect and categorize the usage of biometric API implementation in the software according to their privacy-related behaviors. This technique bridges the communication and background knowledge gap between technical and non-technical individuals in organizations by providing an automated method for both parties to acquire a rapid understanding of the essential behaviors of biometric API in apps, as well as future support to data protection officers (DPO) with legal documentation, such as conducting a Data Protection Impact Assessment (DPIA).

</details>

<details>

<summary>2022-12-21 09:47:49 - EZNAS: Evolving Zero Cost Proxies For Neural Architecture Scoring</summary>

- *Yash Akhauri, J. Pablo Munoz, Nilesh Jain, Ravi Iyer*

- `2209.07413v3` - [abs](http://arxiv.org/abs/2209.07413v3) - [pdf](http://arxiv.org/pdf/2209.07413v3)

> Neural Architecture Search (NAS) has significantly improved productivity in the design and deployment of neural networks (NN). As NAS typically evaluates multiple models by training them partially or completely, the improved productivity comes at the cost of significant carbon footprint. To alleviate this expensive training routine, zero-shot/cost proxies analyze an NN at initialization to generate a score, which correlates highly with its true accuracy. Zero-cost proxies are currently designed by experts conducting multiple cycles of empirical testing on possible algorithms, datasets, and neural architecture design spaces. This experimentation lowers productivity and is an unsustainable approach towards zero-cost proxy design as deep learning use-cases diversify in nature. Additionally, existing zero-cost proxies fail to generalize across neural architecture design spaces. In this paper, we propose a genetic programming framework to automate the discovery of zero-cost proxies for neural architecture scoring. Our methodology efficiently discovers an interpretable and generalizable zero-cost proxy that gives state of the art score-accuracy correlation on all datasets and search spaces of NASBench-201 and Network Design Spaces (NDS). We believe that this research indicates a promising direction towards automatically discovering zero-cost proxies that can work across network architecture design spaces, datasets, and tasks.

</details>

<details>

<summary>2022-12-21 10:54:59 - Automatic Semantic Modeling for Structural Data Source with the Prior Knowledge from Knowledge Base</summary>

- *Jiakang Xu, Wolfgang Mayer, HongYu Zhang, Keqing He, Zaiwen Feng*

- `2212.10915v1` - [abs](http://arxiv.org/abs/2212.10915v1) - [pdf](http://arxiv.org/pdf/2212.10915v1)

> A critical step in sharing semantic content online is to map the structural data source to a public domain ontology. This problem is denoted as the Relational-To-Ontology Mapping Problem (Rel2Onto). A huge effort and expertise are required for manually modeling the semantics of data. Therefore, an automatic approach for learning the semantics of a data source is desirable. Most of the existing work studies the semantic annotation of source attributes. However, although critical, the research for automatically inferring the relationships between attributes is very limited. In this paper, we propose a novel method for semantically annotating structured data sources using machine learning, graph matching and modified frequent subgraph mining to amend the candidate model. In our work, Knowledge graph is used as prior knowledge. Our evaluation shows that our approach outperforms two state-of-the-art solutions in tricky cases where only a few semantic models are known.

</details>

<details>

<summary>2022-12-21 12:09:08 - Learning Spectral Unions of Partial Deformable 3D Shapes</summary>

- *Luca Moschella, Simone Melzi, Luca Cosmo, Filippo Maggioli, Or Litany, Maks Ovsjanikov, Leonidas Guibas, Emanuele RodolÃ *

- `2104.00514v3` - [abs](http://arxiv.org/abs/2104.00514v3) - [pdf](http://arxiv.org/pdf/2104.00514v3)

> Spectral geometric methods have brought revolutionary changes to the field of geometry processing. Of particular interest is the study of the Laplacian spectrum as a compact, isometry and permutation-invariant representation of a shape. Some recent works show how the intrinsic geometry of a full shape can be recovered from its spectrum, but there are approaches that consider the more challenging problem of recovering the geometry from the spectral information of partial shapes. In this paper, we propose a possible way to fill this gap. We introduce a learning-based method to estimate the Laplacian spectrum of the union of partial non-rigid 3D shapes, without actually computing the 3D geometry of the union or any correspondence between those partial shapes. We do so by operating purely in the spectral domain and by defining the union operation between short sequences of eigenvalues. We show that the approximated union spectrum can be used as-is to reconstruct the complete geometry [MRC*19], perform region localization on a template [RTO*19] and retrieve shapes from a database, generalizing ShapeDNA [RWP06] to work with partialities. Working with eigenvalues allows us to deal with unknown correspondence, different sampling, and different discretizations (point clouds and meshes alike), making this operation especially robust and general. Our approach is data-driven and can generalize to isometric and non-isometric deformations of the surface, as long as these stay within the same semantic class (e.g., human bodies or horses), as well as to partiality artifacts not seen at training time.

</details>

<details>

<summary>2022-12-21 12:27:30 - Proceedings of the Thirteenth International Workshop on Graph Computation Models</summary>

- *Reiko Heckel, Christopher M. Poskitt*

- `2212.10975v1` - [abs](http://arxiv.org/abs/2212.10975v1) - [pdf](http://arxiv.org/pdf/2212.10975v1)

> This volume contains the post-proceedings of the Thirteenth International Workshop on Graph Computation Models (GCM 2022). The workshop took place in Nantes, France on 6th July 2022 as part of STAF 2022 (Software Technologies: Applications and Foundations). Graphs are common mathematical structures that are visual and intuitive. They constitute a natural and seamless way for system modelling in science, engineering, and beyond, including computer science, biology, and business process modelling. Graph computation models constitute a class of very high-level models where graphs are first-class citizens. The aim of the International GCM Workshop series is to bring together researchers interested in all aspects of computation models based on graphs and graph transformation. It promotes the cross-fertilising exchange of ideas and experiences among senior and young researchers from the different communities interested in the foundations, applications, and implementations of graph computation models and related areas.

</details>

<details>

<summary>2022-12-21 16:10:02 - Deconstructing Self-Supervised Monocular Reconstruction: The Design Decisions that Matter</summary>

- *Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden*

- `2208.01489v4` - [abs](http://arxiv.org/abs/2208.01489v4) - [pdf](http://arxiv.org/pdf/2208.01489v4)

> This paper presents an open and comprehensive framework to systematically evaluate state-of-the-art contributions to self-supervised monocular depth estimation. This includes pretraining, backbone, architectural design choices and loss functions. Many papers in this field claim novelty in either architecture design or loss formulation. However, simply updating the backbone of historical systems results in relative improvements of 25%, allowing them to outperform the majority of existing systems. A systematic evaluation of papers in this field was not straightforward. The need to compare like-with-like in previous papers means that longstanding errors in the evaluation protocol are ubiquitous in the field. It is likely that many papers were not only optimized for particular datasets, but also for errors in the data and evaluation criteria. To aid future research in this area, we release a modular codebase (https://github.com/jspenmar/monodepth_benchmark), allowing for easy evaluation of alternate design decisions against corrected data and evaluation criteria. We re-implement, validate and re-evaluate 16 state-of-the-art contributions and introduce a new dataset (SYNS-Patches) containing dense outdoor depth maps in a variety of both natural and urban scenes. This allows for the computation of informative metrics in complex regions such as depth boundaries.

</details>

<details>

<summary>2022-12-21 16:18:55 - Homo in Machina: Improving Fuzz Testing Coverage via Compartment Analysis</summary>

- *Joshua Bundt, Andrew Fasano, Brendan Dolan-Gavitt, William Robertson, Tim Leek*

- `2212.11162v1` - [abs](http://arxiv.org/abs/2212.11162v1) - [pdf](http://arxiv.org/pdf/2212.11162v1)

> Fuzz testing is often automated, but also frequently augmented by experts who insert themselves into the workflow in a greedy search for bugs. In this paper, we propose Homo in Machina, or HM-fuzzing, in which analyses guide the manual efforts, maximizing benefit. As one example of this paradigm, we introduce compartment analysis. Compartment analysis uses a whole-program dominator analysis to estimate the utility of reaching new code, and combines this with a dynamic analysis indicating drastically under-covered edges guarding that code. This results in a prioritized list of compartments, i.e., large, uncovered parts of the program semantically partitioned and largely unreachable given the current corpus of inputs under consideration. A human can use this categorization and ranking of compartments directly to focus manual effort, finding or fashioning inputs that make the compartments available for future fuzzing. We evaluate the effect of compartment analysis on seven projects within the OSS-Fuzz corpus where we see coverage improvements over AFL++ as high as 94%, with a median of 13%. We further observe that the determination of compartments is highly stable and thus can be done early in a fuzzing campaign, maximizing the potential for impact.

</details>

<details>

<summary>2022-12-21 16:56:55 - Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning</summary>

- *Julien Denize, Jaonary Rabarisoa, Astrid Orcesi, Romain HÃ©rault*

- `2212.11187v1` - [abs](http://arxiv.org/abs/2212.11187v1) - [pdf](http://arxiv.org/pdf/2212.11187v1)

> Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.

</details>

<details>

<summary>2022-12-21 17:22:27 - Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks</summary>

- *Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener*

- `2212.11205v1` - [abs](http://arxiv.org/abs/2212.11205v1) - [pdf](http://arxiv.org/pdf/2212.11205v1)

> This paper highlights vulnerabilities of deep learning-driven semantic communications to backdoor (Trojan) attacks. Semantic communications aims to convey a desired meaning while transferring information from a transmitter to its receiver. An encoder-decoder pair that is represented by two deep neural networks (DNNs) as part of an autoencoder is trained to reconstruct signals such as images at the receiver by transmitting latent features of small size over a limited number of channel uses. In the meantime, another DNN of a semantic task classifier at the receiver is jointly trained with the autoencoder to check the meaning conveyed to the receiver. The complex decision space of the DNNs makes semantic communications susceptible to adversarial manipulations. In a backdoor (Trojan) attack, the adversary adds triggers to a small portion of training samples and changes the label to a target label. When the transfer of images is considered, the triggers can be added to the images or equivalently to the corresponding transmitted or received signals. In test time, the adversary activates these triggers by providing poisoned samples as input to the encoder (or decoder) of semantic communications. The backdoor attack can effectively change the semantic information transferred for the poisoned input samples to a target meaning. As the performance of semantic communications improves with the signal-to-noise ratio and the number of channel uses, the success of the backdoor attack increases as well. Also, increasing the Trojan ratio in training data makes the attack more successful. In the meantime, the effect of this attack on the unpoisoned input samples remains limited. Overall, this paper shows that the backdoor attack poses a serious threat to semantic communications and presents novel design guidelines to preserve the meaning of transferred information in the presence of backdoor attacks.

</details>

<details>

<summary>2022-12-21 17:59:11 - Improving Narrative Relationship Embeddings by Training with Additional Inverse-Relationship Constraints</summary>

- *Mikolaj Figurski*

- `2212.11234v1` - [abs](http://arxiv.org/abs/2212.11234v1) - [pdf](http://arxiv.org/pdf/2212.11234v1)

> We consider the problem of embedding character-entity relationships from the reduced semantic space of narratives, proposing and evaluating the assumption that these relationships hold under a reflection operation. We analyze this assumption and compare the approach to a baseline state-of-the-art model with a unique evaluation that simulates efficacy on a downstream clustering task with human-created labels. Although our model creates clusters that achieve Silhouette scores of -.084, outperforming the baseline -.227, our analysis reveals that the models approach the task much differently and perform well on very different examples. We conclude that our assumption might be useful for specific types of data and should be evaluated on a wider range of tasks.

</details>

<details>

<summary>2022-12-21 18:58:41 - Generalized Decoding for Pixel, Image, and Language</summary>

- *Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, Jianfeng Gao*

- `2212.11270v1` - [abs](http://arxiv.org/abs/2212.11270v1) - [pdf](http://arxiv.org/pdf/2212.11270v1)

> We present X-Decoder, a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly. X-Decodert takes as input two types of queries: (i) generic non-semantic queries and (ii) semantic queries induced from text inputs, to decode different pixel-level and token-level outputs in the same semantic space. With such a novel design, X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks. Further, our design enables seamless interactions across tasks at different granularities and brings mutual benefits by learning a common and rich pixel-level visual-semantic understanding space, without any pseudo-labeling. After pretraining on a mixed set of a limited amount of segmentation data and millions of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings. Notably, it achieves (1) state-of-the-art results on open-vocabulary segmentation and referring segmentation on eight datasets; (2) better or competitive finetuned performance to other generalist and specialist models on segmentation and VL tasks; and (3) flexibility for efficient finetuning and novel task composition (e.g., referring captioning and image editing). Code, demo, video, and visualization are available at https://x-decoder-vl.github.io.

</details>

<details>

<summary>2022-12-21 20:24:24 - Structure-guided Image Outpainting</summary>

- *Xi Wang, Weixi Cheng, Wenliang Jia*

- `2212.12326v1` - [abs](http://arxiv.org/abs/2212.12326v1) - [pdf](http://arxiv.org/pdf/2212.12326v1)

> Deep learning techniques have made considerable progress in image inpainting, restoration, and reconstruction in the last few years. Image outpainting, also known as image extrapolation, lacks attention and practical approaches to be fulfilled, owing to difficulties caused by large-scale area loss and less legitimate neighboring information. These difficulties have made outpainted images handled by most of the existing models unrealistic to human eyes and spatially inconsistent. When upsampling through deconvolution to generate fake content, the naive generation methods may lead to results lacking high-frequency details and structural authenticity. Therefore, as our novelties to handle image outpainting problems, we introduce structural prior as a condition to optimize the generation quality and a new semantic embedding term to enhance perceptual sanity. we propose a deep learning method based on Generative Adversarial Network (GAN) and condition edges as structural prior in order to assist the generation. We use a multi-phase adversarial training scheme that comprises edge inference training, contents inpainting training, and joint training. The newly added semantic embedding loss is proved effective in practice.

</details>

<details>

<summary>2022-12-21 20:34:33 - Knowledge-driven Scene Priors for Semantic Audio-Visual Embodied Navigation</summary>

- *Gyan Tatiya, Jonathan Francis, Luca Bondi, Ingrid Navarro, Eric Nyberg, Jivko Sinapov, Jean Oh*

- `2212.11345v1` - [abs](http://arxiv.org/abs/2212.11345v1) - [pdf](http://arxiv.org/pdf/2212.11345v1)

> Generalisation to unseen contexts remains a challenge for embodied navigation agents. In the context of semantic audio-visual navigation (SAVi) tasks, the notion of generalisation should include both generalising to unseen indoor visual scenes as well as generalising to unheard sounding objects. However, previous SAVi task definitions do not include evaluation conditions on truly novel sounding objects, resorting instead to evaluating agents on unheard sound clips of known objects; meanwhile, previous SAVi methods do not include explicit mechanisms for incorporating domain knowledge about object and region semantics. These weaknesses limit the development and assessment of models' abilities to generalise their learned experience. In this work, we introduce the use of knowledge-driven scene priors in the semantic audio-visual embodied navigation task: we combine semantic information from our novel knowledge graph that encodes object-region relations, spatial knowledge from dual Graph Encoder Networks, and background knowledge from a series of pre-training tasks -- all within a reinforcement learning framework for audio-visual navigation. We also define a new audio-visual navigation sub-task, where agents are evaluated on novel sounding objects, as opposed to unheard clips of known objects. We show improvements over strong baselines in generalisation to unseen regions and novel sounding objects, within the Habitat-Matterport3D simulation environment, under the SoundSpaces task.

</details>

<details>

<summary>2022-12-21 20:43:46 - Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning</summary>

- *Chris Lengerich, Gabriel Synnaeve, Amy Zhang, Hugh Leather, Kurt Shuster, FranÃ§ois Charton, Charysse Redwood*

- `2212.11353v1` - [abs](http://arxiv.org/abs/2212.11353v1) - [pdf](http://arxiv.org/pdf/2212.11353v1)

> Traditional approaches to RL have focused on learning decision policies directly from episodic decisions, while slowly and implicitly learning the semantics of compositional representations needed for generalization. While some approaches have been adopted to refine representations via auxiliary self-supervised losses while simultaneously learning decision policies, learning compositional representations from hand-designed and context-independent self-supervised losses (multi-view) still adapts relatively slowly to the real world, which contains many non-IID subspaces requiring rapid distribution shift in both time and spatial attention patterns at varying levels of abstraction. In contrast, supervised language model cascades have shown the flexibility to adapt to many diverse manifolds, and hints of self-learning needed for autonomous task transfer. However, to date, transfer methods for language models like few-shot learning and fine-tuning still require human supervision and transfer learning using self-learning methods has been underexplored. We propose a self-supervised loss policy called contrastive distillation which manifests latent variables with high mutual information with both source and target tasks from weights to tokens. We show how this outperforms common methods of transfer learning and suggests a useful design axis of trading off compute for generalizability for online transfer. Contrastive distillation is improved through sampling from memory and suggests a simple algorithm for more efficiently sampling negative examples for contrastive losses than random sampling.

</details>

<details>

<summary>2022-12-21 23:52:09 - Improving Automated Program Repair with Domain Adaptation</summary>

- *Armin Zirak, Hadi Hemmati*

- `2212.11414v1` - [abs](http://arxiv.org/abs/2212.11414v1) - [pdf](http://arxiv.org/pdf/2212.11414v1)

> Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies the train and test sets are chosen from the same set of projects. In reality, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set's(Domain Shift).   In this study, we first define and measure the domain shift problem in automated program repair. Then, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning using two state-of-the-art domain adaptation tools (TFix and CodeXGLUE) and two APR models on 611 bugs from 19 projects. The results show that our proposed framework can improve the effectiveness of TFix by 13.05% and CodeXGLUE by 23.4%. Another contribution of this study is the proposal of a data synthesis method to address the lack of labelled data in APR. We leverage transformers to create a bug generator model. We use the generated synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), which results in an average improvement of 5.76% and 24.42% for TFix and CodeXGLUE, respectively.

</details>

<details>

<summary>2022-12-22 01:58:04 - A machine learning framework for neighbor generation in metaheuristic search</summary>

- *Defeng Liu, Vincent Perreault, Alain Hertz, Andrea Lodi*

- `2212.11451v1` - [abs](http://arxiv.org/abs/2212.11451v1) - [pdf](http://arxiv.org/pdf/2212.11451v1)

> This paper presents a methodology for integrating machine learning techniques into metaheuristics for solving combinatorial optimization problems. Namely, we propose a general machine learning framework for neighbor generation in metaheuristic search. We first define an efficient neighborhood structure constructed by applying a transformation to a selected subset of variables from the current solution. Then, the key of the proposed methodology is to generate promising neighbors by selecting a proper subset of variables that contains a descent of the objective in the solution space. To learn a good variable selection strategy, we formulate the problem as a classification task that exploits structural information from the characteristics of the problem and from high-quality solutions. We validate our methodology on two metaheuristic applications: a Tabu Search scheme for solving a Wireless Network Optimization problem and a Large Neighborhood Search heuristic for solving Mixed-Integer Programs. The experimental results show that our approach is able to achieve a satisfactory trade-off between the exploration of a larger solution space and the exploitation of high-quality solution regions on both applications.

</details>

<details>

<summary>2022-12-22 01:58:16 - Directed Acyclic Graph Factorization Machines for CTR Prediction via Knowledge Distillation</summary>

- *Zhen Tian, Ting Bai, Zibin Zhang, Zhiyuan Xu, Kangyi Lin, Ji-Rong Wen, Wayne Xin Zhao*

- `2211.11159v2` - [abs](http://arxiv.org/abs/2211.11159v2) - [pdf](http://arxiv.org/pdf/2211.11159v2)

> With the growth of high-dimensional sparse data in web-scale recommender systems, the computational cost to learn high-order feature interaction in CTR prediction task largely increases, which limits the use of high-order interaction models in real industrial applications. Some recent knowledge distillation based methods transfer knowledge from complex teacher models to shallow student models for accelerating the online model inference. However, they suffer from the degradation of model accuracy in knowledge distillation process. It is challenging to balance the efficiency and effectiveness of the shallow student models. To address this problem, we propose a Directed Acyclic Graph Factorization Machine (KD-DAGFM) to learn the high-order feature interactions from existing complex interaction models for CTR prediction via Knowledge Distillation. The proposed lightweight student model DAGFM can learn arbitrary explicit feature interactions from teacher networks, which achieves approximately lossless performance and is proved by a dynamic programming algorithm. Besides, an improved general model KD-DAGFM+ is shown to be effective in distilling both explicit and implicit feature interactions from any complex teacher model. Extensive experiments are conducted on four real-world datasets, including a large-scale industrial dataset from WeChat platform with billions of feature dimensions. KD-DAGFM achieves the best performance with less than 21.5% FLOPs of the state-of-the-art method on both online and offline experiments, showing the superiority of DAGFM to deal with the industrial scale data in CTR prediction task. Our implementation code is available at: https://github.com/RUCAIBox/DAGFM.

</details>

<details>

<summary>2022-12-22 04:17:00 - CHEM: Efficient Secure Aggregation with Cached Homomorphic Encryption in Federated Machine Learning Systems</summary>

- *Dongfang Zhao*

- `2212.11475v1` - [abs](http://arxiv.org/abs/2212.11475v1) - [pdf](http://arxiv.org/pdf/2212.11475v1)

> Although homomorphic encryption can be incorporated into neural network layers for securing machine learning tasks, such as confidential inference over encrypted data samples and encrypted local models in federated learning, the computational overhead has been an Achilles heel. This paper proposes a caching protocol, namely CHEM, such that tensor ciphertexts can be constructed from a pool of cached radixes rather than carrying out expensive encryption operations. From a theoretical perspective, we demonstrate that CHEM is semantically secure and can be parameterized with straightforward analysis under practical assumptions. Experimental results on three popular public data sets show that adopting CHEM only incurs sub-second overhead and yet reduces the encryption cost by 48%--89% for encoding input data samples in confidential inference and 67%--87% for encoding local models in federated learning, respectively.

</details>

<details>

<summary>2022-12-22 04:45:15 - Robust Path Selection in Software-defined WANs using Deep Reinforcement Learning</summary>

- *Shahrooz Pouryousef, Lixin Gao, Don Towsley*

- `2212.11155v2` - [abs](http://arxiv.org/abs/2212.11155v2) - [pdf](http://arxiv.org/pdf/2212.11155v2)

> In the context of an efficient network traffic engineering process where the network continuously measures a new traffic matrix and updates the set of paths in the network, an automated process is required to quickly and efficiently identify when and what set of paths should be used. Unfortunately, the burden of finding the optimal solution for the network updating process in each given time interval is high since the computation complexity of optimization approaches using linear programming increases significantly as the size of the network increases. In this paper, we use deep reinforcement learning to derive a data-driven algorithm that does the path selection in the network considering the overhead of route computation and path updates. Our proposed scheme leverages information about past network behavior to identify a set of robust paths to be used for multiple future time intervals to avoid the overhead of updating the forwarding behavior of routers frequently. We compare the results of our approach to other traffic engineering solutions through extensive simulations across real network topologies. Our results demonstrate that our scheme fares well by a factor of 40% with respect to reducing link utilization compared to traditional TE schemes such as ECMP. Our scheme provides a slightly higher link utilization (around 25%) compared to schemes that only minimize link utilization and do not care about path updating overhead.

</details>

<details>

<summary>2022-12-22 09:04:07 - Implementation of general formal translators</summary>

- *Iosif Iulian Petrila*

- `2212.08482v2` - [abs](http://arxiv.org/abs/2212.08482v2) - [pdf](http://arxiv.org/pdf/2212.08482v2)

> The general translator formalism and computing specific implementations are proposed. The implementation of specific elements necessary to process the source and destination information within the translators are presented. Some common directives or instructions, such as classes and procedures, were unified and generalized in order to allow general translations implementations. In order to cover general cases, two levels of processing are required, related to the source and destination information appropriate transformations, with the related control and processing instructions. The proposed general translator elements are useful for processing natural or artificial information described through any types of languages or systems.

</details>

<details>

<summary>2022-12-22 11:44:00 - Device Tracking via Linux's New TCP Source Port Selection Algorithm (Extended Version)</summary>

- *Moshe Kol, Amit Klein, Yossi Gilad*

- `2209.12993v3` - [abs](http://arxiv.org/abs/2209.12993v3) - [pdf](http://arxiv.org/pdf/2209.12993v3)

> We describe a tracking technique for Linux devices, exploiting a new TCP source port generation mechanism recently introduced to the Linux kernel. This mechanism is based on an algorithm, standardized in RFC 6056, for boosting security by better randomizing port selection. Our technique detects collisions in a hash function used in the said algorithm, based on sampling TCP source ports generated in an attacker-prescribed manner. These hash collisions depend solely on a per-device key, and thus the set of collisions forms a device ID that allows tracking devices across browsers, browser privacy modes, containers, and IPv4/IPv6 networks (including some VPNs). It can distinguish among devices with identical hardware and software, and lasts until the device restarts.   We implemented this technique and then tested it using tracking servers in two different locations and with Linux devices on various networks. We also tested it on an Android device that we patched to introduce the new port selection algorithm. The tracking technique works in real-life conditions, and we report detailed findings about it, including its dwell time, scalability, and success rate in different network types. We worked with the Linux kernel team to mitigate the exploit, resulting in a security patch introduced in May 2022 to the Linux kernel, and we provide recommendations for better securing the port selection algorithm in the paper.

</details>

<details>

<summary>2022-12-22 11:51:10 - A Foundation for Functional Graph Programs: The Graph Transformation Control Algebra (GTA)</summary>

- *Jens H. Weber*

- `2212.11626v1` - [abs](http://arxiv.org/abs/2212.11626v1) - [pdf](http://arxiv.org/pdf/2212.11626v1)

> Applications of graph transformation (GT) systems often require control structures that can be used to direct GT processes. Most existing GT tools follow a stateful computational model, where a single graph is repeatedly modified "in-place" when GT rules are applied. The implementation of control structures in such tools is not trivial. Common challenges include dealing with the non-determinism inherent to rule application and transactional constraints when executing compositions of GTs, in particular atomicity and isolation. The complexity of associated transaction mechanisms and rule application search algorithms (e.g., backtracking) complicates the definition of a formal foundation for these control structures. Compared to these stateful approaches, functional graph rewriting presents a simpler (stateless) computational model, which simplifies the definition of a formal basis for (functional) GT control structures. In this paper, we propose the "Graph Transformation control Algebra" (GTA) as such a foundation. The GTA has been used as the formal basis for implementing the control structures in the (functional) GT tool "GrapeVine".

</details>

<details>

<summary>2022-12-22 11:58:46 - Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5</summary>

- *Nghi D. Q. Bui, Yue Wang, Steven Hoi*

- `2211.14875v3` - [abs](http://arxiv.org/abs/2211.14875v3) - [pdf](http://arxiv.org/pdf/2211.14875v3)

> Automated software debugging is a crucial task for improving the productivity of software developers. Many neural-based techniques have been proven effective for debugging-related tasks such as bug localization and program repair (or bug fixing). However, these techniques often focus only on either one of them or approach them in a stage-wise manner, ignoring the mutual benefits between them. In this work, we propose a novel unified \emph{Detect-Localize-Repair} framework based on a pretrained programming language model CodeT5 to seamlessly address these tasks, named CodeT5-DLR. Specifically, we propose three objectives to adapt the generic CodeT5 for debugging: a bug detection objective to determine whether a given code snippet is buggy or not, a bug localization objective to identify the buggy lines, and a program repair objective to translate the buggy code to its fixed version. We evaluate it on each of these tasks and their combined setting on two newly collected line-level debugging datasets in Java and Python. Extensive results show that our model significantly outperforms existing baselines from both NLP and software engineering domains.

</details>

<details>

<summary>2022-12-22 14:56:41 - Towards an Ontology-Driven Approach for Process-Aware Risk Propagation</summary>

- *Gal Engelberg, Mattia Fumagalli, Adrian Kuboszek, Dan Klein, Pnina Soffer, Giancarlo Guizzardi*

- `2212.11763v1` - [abs](http://arxiv.org/abs/2212.11763v1) - [pdf](http://arxiv.org/pdf/2212.11763v1)

> The rapid development of cyber-physical systems creates an increasing demand for a general approach to risk, especially considering how physical and digital components affect the processes of the system itself. In risk analytics and management, risk propagation is a central technique, which allows the calculation of the cascading effect of risk within a system and supports risk mitigation activities. However, one open challenge is to devise a process-aware risk propagation solution that can be used to assess the impact of risk at different levels of abstraction, accounting for actors, processes, physical-digital objects, and their interrelations. To address this challenge, we propose a process-aware risk propagation approach that builds on two main components: i. an ontology, which supports functionalities typical of Semantic Web technologies (SWT), and semantics-based intelligent systems, representing a system with processes and objects having different levels of abstraction, and ii. a method to calculate the propagation of risk within the given system. We implemented our approach in a proof-of-concept tool, which was validated and demonstrated in the cybersecurity domain.

</details>

<details>

<summary>2022-12-22 15:09:19 - ReViSe: Remote Vital Signs Measurement Using Smartphone Camera</summary>

- *Donghao Qiao, Amtul Haq Ayesha, Farhana Zulkernine, Raihan Masroor, Nauman Jaffar*

- `2206.08748v2` - [abs](http://arxiv.org/abs/2206.08748v2) - [pdf](http://arxiv.org/pdf/2206.08748v2)

> We propose an end-to-end framework to measure people's vital signs including Heart Rate (HR), Heart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP) based on the rPPG methodology from the video of a user's face captured with a smartphone camera. We extract face landmarks with a deep learning-based neural network model in real-time. Multiple face patches also called Regions-of-Interest (RoIs) are extracted by using the predicted face landmarks. Several filters are applied to reduce the noise from the RoIs in the extracted cardiac signals called Blood Volume Pulse (BVP) signal. The measurements of HR, HRV and SpO2 are validated on two public rPPG datasets namely the TokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our models achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73Beats-Per-Minute (bpm) and 3.95bpm respectively; b) for HRV, 18.55ms and 25.03ms respectively, and c) for SpO2, an MAE of 1.64% on the PURE dataset. We validated our end-to-end rPPG framework, ReViSe, in daily living environment, and thereby created the Video-HR dataset. Our HR estimation model achieved an MAE of 2.49bpm on this dataset. Since no publicly available rPPG datasets existed for BP measurement with face videos, we used a dataset with signals from fingertip sensor to train our deep learning-based BP estimation model and also created our own video dataset, Video-BP. On our Video-BP dataset, our BP estimation model achieved an MAE of 6.7mmHg for Systolic Blood Pressure (SBP), and an MAE of 9.6mmHg for Diastolic Blood Pressure (DBP). ReViSe framework has been validated on datasets with videos recorded in daily living environment as opposed to less noisy laboratory environment as reported by most state-of-the-art techniques.

</details>

<details>

<summary>2022-12-22 16:51:17 - A unit-based symbolic execution method for detecting memory corruption vulnerabilities in executable codes</summary>

- *Sara Baradaran, Mahdi Heidari, Ali Kamali, Maryam Mouzarani*

- `2210.04258v2` - [abs](http://arxiv.org/abs/2210.04258v2) - [pdf](http://arxiv.org/pdf/2210.04258v2)

> Memory corruption is a serious class of software vulnerabilities, which requires careful attention to be detected and removed from applications before getting exploited and harming the system users. Symbolic execution is a well-known method for analyzing programs and detecting various vulnerabilities, e.g., memory corruption. Although this method is sound and complete in theory, it faces some challenges, such as path explosion, when applied to real-world complex programs. In this paper, we present a method for improving the efficiency of symbolic execution and detecting four classes of memory corruption vulnerabilities in executable codes, i.e., heap-based buffer overflow, stack-based buffer overflow, use-after-free, and double-free. We perform symbolic execution only on test units rather than the whole program to avoid path explosion. In our method, test units are considered parts of the program's code, which might contain vulnerable statements and are statically identified based on the specifications of memory corruption vulnerabilities. Then, each test unit is symbolically executed to calculate path and vulnerability constraints of each statement of the unit, which determine the conditions on unit input data for executing that statement or activating vulnerabilities in it, respectively. Solving these constraints gives us input values for the test unit, which execute the desired statements and reveal vulnerabilities in them. Finally, we use machine learning to approximate the correlation between system and unit input data. Thereby, we generate system inputs that enter the program, reach vulnerable instructions in the desired test unit, and reveal vulnerabilities in them. This method is implemented as a plugin for angr framework and evaluated using a group of benchmark programs. The experiments show its superiority over similar tools in accuracy and performance.

</details>

<details>

<summary>2022-12-22 17:02:19 - Architecture and Knowledge Representation for Composable Inductive Programming</summary>

- *Edward McDaid, Sarah McDaid*

- `2212.12320v1` - [abs](http://arxiv.org/abs/2212.12320v1) - [pdf](http://arxiv.org/pdf/2212.12320v1)

> We present an update on the current architecture of the Zoea knowledge-based, Composable Inductive Programming system. The Zoea compiler is built using a modern variant of the black-board architecture. Zoea integrates a large number of knowledge sources that encode different aspects of programming language and software development expertise. We describe the use of synthetic test cases as a ubiquitous form of knowledge and hypothesis representation that sup-ports a variety of reasoning strategies. Some future plans are also outlined.

</details>

<details>

<summary>2022-12-22 17:05:32 - Compositional generalization in semantic parsing with pretrained transformers</summary>

- *A. Emin Orhan*

- `2109.15101v3` - [abs](http://arxiv.org/abs/2109.15101v3) - [pdf](http://arxiv.org/pdf/2109.15101v3)

> Large-scale pretraining instills large amounts of knowledge in deep neural networks. This, in turn, improves the generalization behavior of these models in downstream tasks. What exactly are the limits to the generalization benefits of large-scale pretraining? Here, we report observations from some simple experiments aimed at addressing this question in the context of two semantic parsing tasks involving natural language, SCAN and COGS. We show that language models pretrained exclusively with non-English corpora, or even with programming language corpora, significantly improve out-of-distribution generalization in these benchmarks, compared with models trained from scratch, even though both benchmarks are English-based. This demonstrates the surprisingly broad transferability of pretrained representations and knowledge. Pretraining with a large-scale protein sequence prediction task, on the other hand, mostly deteriorates the generalization performance in SCAN and COGS, suggesting that pretrained representations do not transfer universally and that there are constraints on the similarity between the pretraining and downstream domains for successful transfer. Finally, we show that larger models are harder to train from scratch and their generalization accuracy is lower when trained up to convergence on the relatively small SCAN and COGS datasets, but the benefits of large-scale pretraining become much clearer with larger models.

</details>

<details>

<summary>2022-12-22 17:13:50 - EDICT: Exact Diffusion Inversion via Coupled Transformations</summary>

- *Bram Wallace, Akash Gokul, Nikhil Naik*

- `2211.12446v2` - [abs](http://arxiv.org/abs/2211.12446v2) - [pdf](http://arxiv.org/pdf/2211.12446v2)

> Finding an initial noise vector that produces an input image when fed into the diffusion process (known as inversion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The state-of-the-art approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs) to deterministically noise the image to the intermediate state along the path that the denoising would follow given the original conditioning. However, DDIM inversion for real images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incorrect image reconstruction and loss of content. To alleviate these problems, we propose Exact Diffusion Inversion via Coupled Transformations (EDICT), an inversion method that draws inspiration from affine coupling layers. EDICT enables mathematically exact inversion of real and model-generated images by maintaining two coupled noise vectors which are used to invert each other in an alternating fashion. Using Stable Diffusion, a state-of-the-art latent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex image datasets like MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the mean square error of reconstruction by a factor of two. Using noise vectors inverted from real images, EDICT enables a wide range of image edits--from local and global semantic edits to image stylization--while maintaining fidelity to the original image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be combined with any pretrained DDM. Code is available at https://github.com/salesforce/EDICT.

</details>

<details>

<summary>2022-12-22 17:20:36 - Chatbots in a Botnet World</summary>

- *Forrest McKee, David Noever*

- `2212.11126v2` - [abs](http://arxiv.org/abs/2212.11126v2) - [pdf](http://arxiv.org/pdf/2212.11126v2)

> Question-and-answer formats provide a novel experimental platform for investigating cybersecurity questions. Unlike previous chatbots, the latest ChatGPT model from OpenAI supports an advanced understanding of complex coding questions. The research demonstrates thirteen coding tasks that generally qualify as stages in the MITRE ATT&CK framework, ranging from credential access to defense evasion. With varying success, the experimental prompts generate examples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled ransomware. The empirical results illustrate cases that support the broad gain of functionality, including self-replication and self-modification, evasion, and strategic understanding of complex cybersecurity goals. One surprising feature of ChatGPT as a language-only model centers on its ability to spawn coding approaches that yield images that obfuscate or embed executable programming steps or links.

</details>

<details>

<summary>2022-12-22 19:23:22 - On Characterizing the Trade-off in Invariant Representation Learning</summary>

- *Bashir Sadeghi, Sepehr Dehdashtian, Vishnu Boddeti*

- `2109.03386v4` - [abs](http://arxiv.org/abs/2109.03386v4) - [pdf](http://arxiv.org/pdf/2109.03386v4)

> Many applications of representation learning, such as privacy preservation, algorithmic fairness, and domain adaptation, desire explicit control over semantic information being discarded. This goal is formulated as satisfying two objectives: maximizing utility for predicting a target attribute while simultaneously being invariant (independent) to a known semantic attribute. Solutions to invariant representation learning (IRepL) problems lead to a trade-off between utility and invariance when they are competing. While existing works study bounds on this trade-off, two questions remain outstanding: 1) What is the exact trade-off between utility and invariance? and 2) What are the encoders (mapping the data to a representation) that achieve the trade-off, and how can we estimate it from training data? This paper addresses these questions for IRepLs in reproducing kernel Hilbert spaces (RKHS)s. Under the assumption that the distribution of a low-dimensional projection of high-dimensional data is approximately normal, we derive a closed-form solution for the global optima of the underlying optimization problem for encoders in RKHSs. This yields closed formulae for a near-optimal trade-off, corresponding optimal representation dimensionality, and the corresponding encoder(s). We also numerically quantify the trade-off on representative problems and compare them to those achieved by baseline IRepL algorithms.

</details>

<details>

<summary>2022-12-22 22:07:28 - Semantically-consistent Landsat 8 image to Sentinel-2 image translation for alpine areas</summary>

- *M. Sokolov, J. L. Storie, C. J. Henry, C. D. Storie, J. Cameron, R. S. ÃdegÃ¥rd, V. Zubinaite, S. Stikbakke*

- `2212.12056v1` - [abs](http://arxiv.org/abs/2212.12056v1) - [pdf](http://arxiv.org/pdf/2212.12056v1)

> The availability of frequent and cost-free satellite images is in growing demand in the research world. Such satellite constellations as Landsat 8 and Sentinel-2 provide a massive amount of valuable data daily. However, the discrepancy in the sensors' characteristics of these satellites makes it senseless to use a segmentation model trained on either dataset and applied to another, which is why domain adaptation techniques have recently become an active research area in remote sensing. In this paper, an experiment of domain adaptation through style-transferring is conducted using the HRSemI2I model to narrow the sensor discrepancy between Landsat 8 and Sentinel-2. This paper's main contribution is analyzing the expediency of that approach by comparing the results of segmentation using domain-adapted images with those without adaptation. The HRSemI2I model, adjusted to work with 6-band imagery, shows significant intersection-over-union performance improvement for both mean and per class metrics. A second contribution is providing different schemes of generalization between two label schemes - NALCMS 2015 and CORINE. The first scheme is standardization through higher-level land cover classes, and the second is through harmonization validation in the field.

</details>

<details>

<summary>2022-12-23 00:27:55 - Semantic Information G Theory and Logical Bayesian Inference for Machine Learning</summary>

- *Chenguang Lu*

- `1809.01577v2` - [abs](http://arxiv.org/abs/1809.01577v2) - [pdf](http://arxiv.org/pdf/1809.01577v2)

> An important problem with machine learning is that when label number n>2, it is very difficult to construct and optimize a group of learning functions, and we wish that optimized learning functions are still useful when prior distribution P(x) (where x is an instance) is changed. To resolve this problem, the semantic information G theory, Logical Bayesian Inference (LBI), and a group of Channel Matching (CM) algorithms together form a systematic solution. A semantic channel in the G theory consists of a group of truth functions or membership functions. In comparison with likelihood functions, Bayesian posteriors, and Logistic functions used by popular methods, membership functions can be more conveniently used as learning functions without the above problem. In Logical Bayesian Inference (LBI), every label's learning is independent. For Multilabel learning, we can directly obtain a group of optimized membership functions from a big enough sample with labels, without preparing different samples for different labels. A group of Channel Matching (CM) algorithms is developed for machine learning. For the Maximum Mutual Information (MMI) classification of three classes with Gaussian distributions on a two-dimensional feature space, 2-3 iterations can make mutual information between three classes and three labels surpass 99% of the MMI for most initial partitions. For mixture models, the Expectation-Maximization (EM) algorithm is improved and becomes the CM-EM algorithm, which can outperform the EM algorithm when mixture ratios are imbalanced, or local convergence exists. The CM iteration algorithm needs to combine neural networks for MMI classifications on high-dimensional feature spaces. LBI needs further studies for the unification of statistics and logic.

</details>

<details>

<summary>2022-12-23 02:14:09 - Annotation by Clicks: A Point-Supervised Contrastive Variance Method for Medical Semantic Segmentation</summary>

- *Qing En, Yuhong Guo*

- `2212.08774v2` - [abs](http://arxiv.org/abs/2212.08774v2) - [pdf](http://arxiv.org/pdf/2212.08774v2)

> Medical image segmentation methods typically rely on numerous dense annotated images for model training, which are notoriously expensive and time-consuming to collect. To alleviate this burden, weakly supervised techniques have been exploited to train segmentation models with less expensive annotations. In this paper, we propose a novel point-supervised contrastive variance method (PSCV) for medical image semantic segmentation, which only requires one pixel-point from each organ category to be annotated. The proposed method trains the base segmentation network by using a novel contrastive variance (CV) loss to exploit the unlabeled pixels and a partial cross-entropy loss on the labeled pixels. The CV loss function is designed to exploit the statistical spatial distribution properties of organs in medical images and their variance distribution map representations to enforce discriminative predictions over the unlabeled pixels. Experimental results on two standard medical image datasets demonstrate that the proposed method outperforms the state-of-the-art weakly supervised methods on point-supervised medical image semantic segmentation tasks.

</details>

<details>

<summary>2022-12-23 02:17:44 - Detecting Exploit Primitives Automatically for Heap Vulnerabilities on Binary Programs</summary>

- *Jie Liu, Hang An, Jin Li, Hongliang Liang*

- `2212.13990v1` - [abs](http://arxiv.org/abs/2212.13990v1) - [pdf](http://arxiv.org/pdf/2212.13990v1)

> Automated Exploit Generation (AEG) is a well-known difficult task, especially for heap vulnerabilities. Previous works first detected heap vulnerabilities and then searched for exploitable states by using symbolic execution and fuzzing techniques on binary programs. However, it is not always easy to discovery bugs using fuzzing or symbolic technologies and solvable for internal overflow of heap objects. In this paper, we present a solution DEPA to detect exploit primitives based on primitive-crucial-behavior model for heap vulnerabilities. The core of DEPA contains two novel techniques, 1) primitive-crucial-behavior identification through pointer dependence analysis, and 2) exploit primitive determination method which includes triggering both vulnerabilities and exploit primitives. We evaluate DEPA on eleven real-world CTF(capture the flag) programs with heap vulnerabilities and DEPA can discovery arbitrary write and arbitrary jump exploit primitives for ten programs except for program multi-heap. Results showed that primitive-crucial-behavior identification and determining exploit primitives are accurate and effective by using our approach. In addition, DEPA is superior to the state-of-the-art tools in determining exploit primitives for the heap object internal overflow

</details>

<details>

<summary>2022-12-23 04:12:52 - Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing</summary>

- *William Brannon, Yogesh Virkar, Brian Thompson*

- `2212.12137v1` - [abs](http://arxiv.org/abs/2212.12137v1) - [pdf](http://arxiv.org/pdf/2212.12137v1)

> We investigate how humans perform the task of dubbing video content from one language into another, leveraging a novel corpus of 319.57 hours of video from 54 professionally produced titles. This is the first such large-scale study we are aware of. The results challenge a number of assumptions commonly made in both qualitative literature on human dubbing and machine-learning literature on automatic dubbing, arguing for the importance of vocal naturalness and translation quality over commonly emphasized isometric (character length) and lip-sync constraints, and for a more qualified view of the importance of isochronic (timing) constraints. We also find substantial influence of the source-side audio on human dubs through channels other than the words of the translation, pointing to the need for research on ways to preserve speech characteristics, as well as semantic transfer such as emphasis/emotion, in automatic dubbing systems.

</details>

<details>

<summary>2022-12-23 06:41:01 - Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation</summary>

- *Xiaoyu Zhang, Xin Xin, Dongdong Li, Wenxuan Liu, Pengjie Ren, Zhumin Chen, Jun Ma, Zhaochun Ren*

- `2212.11868v2` - [abs](http://arxiv.org/abs/2212.11868v2) - [pdf](http://arxiv.org/pdf/2212.11868v2)

> Conversational recommender systems (CRSs) often utilize external knowledge graphs (KGs) to introduce rich semantic information and recommend relevant items through natural language dialogues. However, original KGs employed in existing CRSs are often incomplete and sparse, which limits the reasoning capability in recommendation. Moreover, only few of existing studies exploit the dialogue context to dynamically refine knowledge from KGs for better recommendation. To address the above issues, we propose the Variational Reasoning over Incomplete KGs Conversational Recommender (VRICR). Our key idea is to incorporate the large dialogue corpus naturally accompanied with CRSs to enhance the incomplete KGs; and perform dynamic knowledge reasoning conditioned on the dialogue context. Specifically, we denote the dialogue-specific subgraphs of KGs as latent variables with categorical priors for adaptive knowledge graphs refactor. We propose a variational Bayesian method to approximate posterior distributions over dialogue-specific subgraphs, which not only leverages the dialogue corpus for restructuring missing entity relations but also dynamically selects knowledge based on the dialogue context. Finally, we infuse the dialogue-specific subgraphs to decode the recommendation and responses. We conduct experiments on two benchmark CRSs datasets. Experimental results confirm the effectiveness of our proposed method.

</details>

<details>

<summary>2022-12-23 08:18:41 - Callee: Recovering Call Graphs for Binaries with Transfer and Contrastive Learning</summary>

- *Wenyu Zhu, Zhiyao Feng, Zihan Zhang, Jianjun Chen, Zhijian Ou, Min Yang, Chao Zhang*

- `2111.01415v4` - [abs](http://arxiv.org/abs/2111.01415v4) - [pdf](http://arxiv.org/pdf/2111.01415v4)

> Recovering binary programs' call graphs is crucial for inter-procedural analysis tasks and applications based on them.transfer One of the core challenges is recognizing targets of indirect calls (i.e., indirect callees). Existing solutions all have high false positives and negatives, making call graphs inaccurate. In this paper, we propose a new solution Callee combining transfer learning and contrastive learning. The key insight is that, deep neural networks (DNNs) can automatically identify patterns concerning indirect calls, which can be more efficient than designing approximation algorithms or heuristic rules to handle various cases. Inspired by the advances in question-answering applications, we utilize contrastive learning to answer the callsite-callee question. However, one of the toughest challenges is that DNNs need large datasets to achieve high performance, while collecting large-scale indirect-call ground-truths can be computational-expensive. Since direct calls and indirect calls share similar calling conventions, it is possible to transfer knowledge learned from direct calls to indirect ones. Therefore, we leverage transfer learning to pre-train DNNs with easy-to-collect direct calls and further fine-tune the indirect-call DNNs. We evaluate Callee on several groups of targets, and results show that our solution could match callsites to callees with an F1-Measure of 94.6%, much better than state-of-the-art solutions. Further, we apply Callee to binary code similarity detection and hybrid fuzzing, and found it could greatly improve their performance.

</details>

<details>

<summary>2022-12-23 08:25:50 - RMove: Recommending Move Method Refactoring Opportunities using Structural and Semantic Representations of Code</summary>

- *Di Cui, Siqi Wang, Yong Luo, Xingyu Li, Jie Dai, Lu Wang, Qingshan Li*

- `2212.12195v1` - [abs](http://arxiv.org/abs/2212.12195v1) - [pdf](http://arxiv.org/pdf/2212.12195v1)

> Incorrect placement of methods within classes is a typical code smell called Feature Envy, which causes additional maintenance and cost during evolution. To remove this design flaw, several Move Method refactoring tools have been proposed. To the best of our knowledge, state-of-the-art related techniques can be broadly divided into two categories: the first line is non-machine-learning-based approaches built on software measurement, while the selection and thresholds of software metrics heavily rely on expert knowledge. The second line is machine learning-based approaches, which suggest Move Method refactoring by learning to extract features from code information. However, most approaches in this line treat different forms of code information identically, disregarding their significant variation on data analysis. In this paper, we propose an approach to recommend Move Method refactoring named RMove by automatically learning structural and semantic representation from code fragment respectively. We concatenate these representations together and further train the machine learning classifiers to guide the movement of method to suitable classes. We evaluate our approach on two publicly available datasets. The results show that our approach outperforms three state-of-the-art refactoring tools including PathMove, JDeodorant, and JMove in effectiveness and usefulness. The results also unveil useful findings and provide new insights that benefit other types of feature envy refactoring techniques.

</details>

<details>

<summary>2022-12-23 08:54:33 - VoronoiPatches: Evaluating A New Data Augmentation Method</summary>

- *Steffen Illium, Gretchen Griffin, Michael KÃ¶lle, Maximilian Zorn, Jonas NÃ¼Ãlein, Claudia Linnhoff-Popien*

- `2212.10054v2` - [abs](http://arxiv.org/abs/2212.10054v2) - [pdf](http://arxiv.org/pdf/2212.10054v2)

> Overfitting is a problem in Convolutional Neural Networks (CNN) that causes poor generalization of models on unseen data. To remediate this problem, many new and diverse data augmentation methods (DA) have been proposed to supplement or generate more training data, and thereby increase its quality. In this work, we propose a new data augmentation algorithm: VoronoiPatches (VP). We primarily utilize non-linear recombination of information within an image, fragmenting and occluding small information patches. Unlike other DA methods, VP uses small convex polygon-shaped patches in a random layout to transport information around within an image. Sudden transitions created between patches and the original image can, optionally, be smoothed. In our experiments, VP outperformed current DA methods regarding model variance and overfitting tendencies. We demonstrate data augmentation utilizing non-linear re-combination of information within images, and non-orthogonal shapes and structures improves CNN model robustness on unseen data.

</details>

<details>

<summary>2022-12-23 10:12:09 - Application of Deep Q Learning with Simulation Results for Elevator Optimization</summary>

- *Zheng Cao, Raymond Guo, Caesar M. Tuguinay, Mark Pock, Jiayi Gao, Ziyu Wang*

- `2210.00065v3` - [abs](http://arxiv.org/abs/2210.00065v3) - [pdf](http://arxiv.org/pdf/2210.00065v3)

> This paper presents a methodology for combining programming and mathematics to optimize elevator wait times. Based on simulated user data generated according to the canonical three-peak model of elevator traffic, we first develop a naive model from an intuitive understanding of the logic behind elevators. We take into consideration a general array of features including capacity, acceleration, and maximum wait time thresholds to adequately model realistic circumstances. Using the same evaluation framework, we proceed to develop a Deep Q Learning model in an attempt to match the hard-coded naive approach for elevator control. Throughout the majority of the paper, we work under a Markov Decision Process (MDP) schema, but later explore how the assumption fails to characterize the highly stochastic overall Elevator Group Control System (EGCS).

</details>

<details>

<summary>2022-12-23 10:58:27 - Generalised agent for solving higher board states of tic tac toe using Reinforcement Learning</summary>

- *Bhavuk Kalra*

- `2212.12252v1` - [abs](http://arxiv.org/abs/2212.12252v1) - [pdf](http://arxiv.org/pdf/2212.12252v1)

> Tic Tac Toe is amongst the most well-known games. It has already been shown that it is a biased game, giving more chances to win for the first player leaving only a draw or a loss as possibilities for the opponent, assuming both the players play optimally. Thus on average majority of the games played result in a draw. The majority of the latest research on how to solve a tic tac toe board state employs strategies such as Genetic Algorithms, Neural Networks, Co-Evolution, and Evolutionary Programming. But these approaches deal with a trivial board state of 3X3 and very little research has been done for a generalized algorithm to solve 4X4,5X5,6X6 and many higher states. Even though an algorithm exists which is Min-Max but it takes a lot of time in coming up with an ideal move due to its recursive nature of implementation. A Sample has been created on this link \url{https://bk-tic-tac-toe.herokuapp.com/} to prove this fact. This is the main problem that this study is aimed at solving i.e providing a generalized algorithm(Approximate method, Learning-Based) for higher board states of tic tac toe to make precise moves in a short period. Also, the code changes needed to accommodate higher board states will be nominal. The idea is to pose the tic tac toe game as a well-posed learning problem. The study and its results are promising, giving a high win to draw ratio with each epoch of training. This study could also be encouraging for other researchers to apply the same algorithm to other similar board games like Minesweeper, Chess, and GO for finding efficient strategies and comparing the results.

</details>

<details>

<summary>2022-12-23 13:44:50 - A Cut-and-solve Algorithm for Virtual Machine Consolidation Problem</summary>

- *Jiang-Yao Luo, Liang Chen, Wei-Kun Chen, Jian-Hua Yuan, Yu-Hong Dai*

- `2212.12341v1` - [abs](http://arxiv.org/abs/2212.12341v1) - [pdf](http://arxiv.org/pdf/2212.12341v1)

> The virtual machine consolidation problem (VMCP) attempts to determine which servers to be activated, how to allocate virtual machines (VMs) to the activated servers, and how to migrate VMs among servers such that the summation of activated, allocation, and migration costs is minimized subject to the resource constraints of the servers and other practical constraints. In this paper, we first propose a new mixed integer linear programming (MILP) formulation for the VMCP. We show that compared with existing formulations, the proposed formulation is much more compact in terms of smaller numbers of variables or constraints, which makes it suitable for solving large-scale problems. We then develop a cut-and-solve (C&S) algorithm, a tree search algorithm to efficiently solve the VMCP to optimality. The proposed C&S algorithm is based on a novel relaxation of the VMCP that provides a stronger lower bound than the natural continuous relaxation of the VMCP, making a smaller search tree. By extensive computational experiments, we show that (i) the proposed formulation significantly outperforms existing formulations in terms of solution efficiency; and (ii) compared with standard MILP solvers, the proposed C&S algorithm is much more efficient.

</details>

<details>

<summary>2022-12-23 14:29:10 - Label-Enhanced Graph Neural Network for Semi-supervised Node Classification</summary>

- *Le Yu, Leilei Sun, Bowen Du, Tongyu Zhu, Weifeng Lv*

- `2205.15653v2` - [abs](http://arxiv.org/abs/2205.15653v2) - [pdf](http://arxiv.org/pdf/2205.15653v2)

> Graph Neural Networks (GNNs) have been widely applied in the semi-supervised node classification task, where a key point lies in how to sufficiently leverage the limited but valuable label information. Most of the classical GNNs solely use the known labels for computing the classification loss at the output. In recent years, several methods have been designed to additionally utilize the labels at the input. One part of the methods augment the node features via concatenating or adding them with the one-hot encodings of labels, while other methods optimize the graph structure by assuming neighboring nodes tend to have the same label. To bring into full play the rich information of labels, in this paper, we present a label-enhanced learning framework for GNNs, which first models each label as a virtual center for intra-class nodes and then jointly learns the representations of both nodes and labels. Our approach could not only smooth the representations of nodes belonging to the same class, but also explicitly encode the label semantics into the learning process of GNNs. Moreover, a training node selection technique is provided to eliminate the potential label leakage issue and guarantee the model generalization ability. Finally, an adaptive self-training strategy is proposed to iteratively enlarge the training set with more reliable pseudo labels and distinguish the importance of each pseudo-labeled node during the model training process. Experimental results on both real-world and synthetic datasets demonstrate our approach can not only consistently outperform the state-of-the-arts, but also effectively smooth the representations of intra-class nodes.

</details>

<details>

<summary>2022-12-23 15:06:29 - An Efficient Black-Box Support of Advanced Coverage Criteria for Klee</summary>

- *Nicolas Berthier, Steven De Oliveira, Nikolai Kosmatov, Delphine Longuet, Romain Soulat*

- `2211.14592v2` - [abs](http://arxiv.org/abs/2211.14592v2) - [pdf](http://arxiv.org/pdf/2211.14592v2)

> Dynamic symbolic execution (DSE) is a powerful test generation approach based on an exploration of the path space of the program under test. Well-adapted for path coverage, this approach is however less efficient for conditions, decisions, advanced coverage criteria (such as multiple conditions, weak mutations, boundary testing) or user-provided test objectives. While theoretical solutions to adapt DSE to a large set of criteria have been proposed, they have never been integrated into publicly available testing tools. This paper presents a first integration of an optimized test generation strategy for advanced coverage criteria into a popular open-source testing tool based on DSE, namely, Klee. The integration is performed in a fully black-box manner, and can therefore inspire an easy integration into other similar tools. The resulting version of the tool, named Klee4labels, is publicly available. We present the design of the proposed technique and evaluate it on several benchmarks. Our results confirm the benefits of the proposed tool for advanced coverage criteria.

</details>

<details>

<summary>2022-12-23 15:06:38 - ADEPT: A DEbiasing PrompT Framework</summary>

- *Ke Yang, Charles Yu, Yi Fung, Manling Li, Heng Ji*

- `2211.05414v2` - [abs](http://arxiv.org/abs/2211.05414v2) - [pdf](http://arxiv.org/pdf/2211.05414v2)

> Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects.

</details>

<details>

<summary>2022-12-23 15:16:20 - Dial-a-ride problem with modular platooning and en-route transfers</summary>

- *Zhexi Fu, Joseph Y. J. Chow*

- `2212.00289v2` - [abs](http://arxiv.org/abs/2212.00289v2) - [pdf](http://arxiv.org/pdf/2212.00289v2)

> Modular vehicles (MV) possess the ability to physically connect/disconnect with each other and travel in platoon with less energy consumption. A fleet of demand-responsive transit vehicles with such technology can serve passengers door to door or have vehicles deviate to platoon with each other to travel at lower cost and allow for en-route passenger transfers before splitting. A mixed integer linear programming (MILP) model is formulated to solve this "modular dial-a-ride problem" (MDARP). A heuristic algorithm based on Steiner-tree-inspired large neighborhood search is developed to solve the MDARP for practical scenarios. A set of small-scale synthetic numerical experiments are tested to evaluate the optimality gap and computation time between exact solutions of the MDARP using commercial software and the proposed heuristic. Large-scale experiments are conducted on the Anaheim network with 378 candidate join/split nodes to further explore the potentials and identify the ideal operation scenarios of MVs. The results show that MV technology can save up to 52.0% in vehicle travel cost, 35.6% in passenger service time, and 29.4% in total cost against existing on-demand mobility services in the scenarios tested. Results suggest that MVs best benefit from platooning by serving "enclave pairs" as a hub-and-spoke service.

</details>

<details>

<summary>2022-12-23 19:13:43 - A Close Look at Spatial Modeling: From Attention to Convolution</summary>

- *Xu Ma, Huan Wang, Can Qin, Kunpeng Li, Xingchen Zhao, Jie Fu, Yun Fu*

- `2212.12552v1` - [abs](http://arxiv.org/abs/2212.12552v1) - [pdf](http://arxiv.org/pdf/2212.12552v1)

> Vision Transformers have shown great promise recently for many vision tasks due to the insightful architecture design and attention mechanism. By revisiting the self-attention responses in Transformers, we empirically observe two interesting issues. First, Vision Transformers present a queryirrelevant behavior at deep layers, where the attention maps exhibit nearly consistent contexts in global scope, regardless of the query patch position (also head-irrelevant). Second, the attention maps are intrinsically sparse, few tokens dominate the attention weights; introducing the knowledge from ConvNets would largely smooth the attention and enhance the performance. Motivated by above observations, we generalize self-attention formulation to abstract a queryirrelevant global context directly and further integrate the global context into convolutions. The resulting model, a Fully Convolutional Vision Transformer (i.e., FCViT), purely consists of convolutional layers and firmly inherits the merits of both attention mechanism and convolutions, including dynamic property, weight sharing, and short- and long-range feature modeling, etc. Experimental results demonstrate the effectiveness of FCViT. With less than 14M parameters, our FCViT-S12 outperforms related work ResT-Lite by 3.7% top1 accuracy on ImageNet-1K. When scaling FCViT to larger models, we still perform better than previous state-of-the-art ConvNeXt with even fewer parameters. FCViT-based models also demonstrate promising transferability to downstream tasks, like object detection, instance segmentation, and semantic segmentation. Codes and models are made available at: https://github.com/ma-xu/FCViT.

</details>

<details>

<summary>2022-12-23 22:39:41 - Efficient Learning of Decision-Making Models: A Penalty Block Coordinate Descent Algorithm for Data-Driven Inverse Optimization</summary>

- *Rishabh Gupta, Qi Zhang*

- `2210.15393v2` - [abs](http://arxiv.org/abs/2210.15393v2) - [pdf](http://arxiv.org/pdf/2210.15393v2)

> Decision-making problems are commonly formulated as optimization problems, which are then solved to make optimal decisions. In this work, we consider the inverse problem where we use prior decision data to uncover the underlying decision-making process in the form of a mathematical optimization model. This statistical learning problem is referred to as data-driven inverse optimization. We focus on problems where the underlying decision-making process is modeled as a convex optimization problem whose parameters are unknown. We formulate the inverse optimization problem as a bilevel program and propose an efficient block coordinate descent-based algorithm to solve large problem instances. Numerical experiments on synthetic datasets demonstrate the computational advantage of our method compared to standard commercial solvers. Moreover, the real-world utility of the proposed approach is highlighted through two realistic case studies in which we consider estimating risk preferences and learning local constraint parameters of agents in a multiplayer Nash bargaining game.

</details>

<details>

<summary>2022-12-24 03:22:26 - Utilizing Priming to Identify Optimal Class Ordering to Alleviate Catastrophic Forgetting</summary>

- *Gabriel Mantione-Holmes, Justin Leo, Jugal Kalita*

- `2212.12643v1` - [abs](http://arxiv.org/abs/2212.12643v1) - [pdf](http://arxiv.org/pdf/2212.12643v1)

> In order for artificial neural networks to begin accurately mimicking biological ones, they must be able to adapt to new exigencies without forgetting what they have learned from previous training. Lifelong learning approaches to artificial neural networks attempt to strive towards this goal, yet have not progressed far enough to be realistically deployed for natural language processing tasks. The proverbial roadblock of catastrophic forgetting still gate-keeps researchers from an adequate lifelong learning model. While efforts are being made to quell catastrophic forgetting, there is a lack of research that looks into the importance of class ordering when training on new classes for incremental learning. This is surprising as the ordering of "classes" that humans learn is heavily monitored and incredibly important. While heuristics to develop an ideal class order have been researched, this paper examines class ordering as it relates to priming as a scheme for incremental class learning. By examining the connections between various methods of priming found in humans and how those are mimicked yet remain unexplained in life-long machine learning, this paper provides a better understanding of the similarities between our biological systems and the synthetic systems while simultaneously improving current practices to combat catastrophic forgetting. Through the merging of psychological priming practices with class ordering, this paper is able to identify a generalizable method for class ordering in NLP incremental learning tasks that consistently outperforms random class ordering.

</details>

<details>

<summary>2022-12-24 11:22:34 - Author Name Disambiguation via Heterogeneous Network Embedding from Structural and Semantic Perspectives</summary>

- *Wenjin Xie, Siyuan Liu, Xiaomeng Wang, Tao Jia*

- `2212.12715v1` - [abs](http://arxiv.org/abs/2212.12715v1) - [pdf](http://arxiv.org/pdf/2212.12715v1)

> Name ambiguity is common in academic digital libraries, such as multiple authors having the same name. This creates challenges for academic data management and analysis, thus name disambiguation becomes necessary. The procedure of name disambiguation is to divide publications with the same name into different groups, each group belonging to a unique author. A large amount of attribute information in publications makes traditional methods fall into the quagmire of feature selection. These methods always select attributes artificially and equally, which usually causes a negative impact on accuracy. The proposed method is mainly based on representation learning for heterogeneous networks and clustering and exploits the self-attention technology to solve the problem. The presentation of publications is a synthesis of structural and semantic representations. The structural representation is obtained by meta-path-based sampling and a skip-gram-based embedding method, and meta-path level attention is introduced to automatically learn the weight of each feature. The semantic representation is generated using NLP tools. Our proposal performs better in terms of name disambiguation accuracy compared with baselines and the ablation experiments demonstrate the improvement by feature selection and the meta-path level attention in our method. The experimental results show the superiority of our new method for capturing the most attributes from publications and reducing the impact of redundant information.

</details>

<details>

<summary>2022-12-24 14:54:43 - Wastewater Pipe Rating Model Using Natural Language Processing</summary>

- *Sai Nethra Betgeri, Shashank Reddy Vadyala, John C. Mattews, Hongfang Lu*

- `2202.13871v2` - [abs](http://arxiv.org/abs/2202.13871v2) - [pdf](http://arxiv.org/pdf/2202.13871v2)

> Closed-circuit video (CCTV) inspection has been the most popular technique for visually evaluating the interior status of pipelines in recent decades. Certified inspectors prepare the pipe repair document based on the CCTV inspection. The traditional manual method of assessing sewage structural conditions from pipe repair documents takes a long time and is prone to human mistakes. The automatic identification of necessary texts has received little attention. By building an automated framework employing Natural Language Processing (NLP), this study presents an effective technique to automate the identification of the pipe defect rating of the pipe repair documents. NLP technologies are employed to break down textual material into grammatical units in this research. Further analysis entails using words to discover pipe defect symptoms and their frequency and then combining that information into a single score. Our model achieves 95.0% accuracy,94.9% sensitivity, 94.4% specificity, 95.9% precision score, and 95.7% F1 score, showing the potential of the proposed model to be used in large-scale pipe repair documents for accurate and efficient pipeline failure detection to improve the quality of the pipeline. Keywords: Sewer pipe inspection, Defect detection, Natural language processing, Text recognition

</details>

<details>

<summary>2022-12-24 18:40:23 - A Marker-based Neural Network System for Extracting Social Determinants of Health</summary>

- *Xingmeng Zhao, Anthony Rios*

- `2212.12800v1` - [abs](http://arxiv.org/abs/2212.12800v1) - [pdf](http://arxiv.org/pdf/2212.12800v1)

> Objective. The impact of social determinants of health (SDoH) on patients' healthcare quality and the disparity is well-known. Many SDoH items are not coded in structured forms in electronic health records. These items are often captured in free-text clinical notes, but there are limited methods for automatically extracting them. We explore a multi-stage pipeline involving named entity recognition (NER), relation classification (RC), and text classification methods to extract SDoH information from clinical notes automatically.   Materials and Methods. The study uses the N2C2 Shared Task data, which was collected from two sources of clinical notes: MIMIC-III and University of Washington Harborview Medical Centers. It contains 4480 social history sections with full annotation for twelve SDoHs. In order to handle the issue of overlapping entities, we developed a novel marker-based NER model. We used it in a multi-stage pipeline to extract SDoH information from clinical notes.   Results. Our marker-based system outperformed the state-of-the-art span-based models at handling overlapping entities based on the overall Micro-F1 score performance. It also achieved state-of-the-art performance compared to the shared task methods.   Conclusion. The major finding of this study is that the multi-stage pipeline effectively extracts SDoH information from clinical notes. This approach can potentially improve the understanding and tracking of SDoHs in clinical settings. However, error propagation may be an issue, and further research is needed to improve the extraction of entities with complex semantic meanings and low-resource entities using external knowledge.

</details>

<details>

<summary>2022-12-25 02:15:40 - Model and Data Agreement for Learning with Noisy Labels</summary>

- *Yuhang Zhang, Weihong Deng, Xingchen Cui, Yunfeng Yin, Hongzhi Shi, Dongchao Wen*

- `2212.01054v2` - [abs](http://arxiv.org/abs/2212.01054v2) - [pdf](http://arxiv.org/pdf/2212.01054v2)

> Learning with noisy labels is a vital topic for practical deep learning as models should be robust to noisy open-world datasets in the wild. The state-of-the-art noisy label learning approach JoCoR fails when faced with a large ratio of noisy labels. Moreover, selecting small-loss samples can also cause error accumulation as once the noisy samples are mistakenly selected as small-loss samples, they are more likely to be selected again. In this paper, we try to deal with error accumulation in noisy label learning from both model and data perspectives. We introduce mean point ensemble to utilize a more robust loss function and more information from unselected samples to reduce error accumulation from the model perspective. Furthermore, as the flip images have the same semantic meaning as the original images, we select small-loss samples according to the loss values of flip images instead of the original ones to reduce error accumulation from the data perspective. Extensive experiments on CIFAR-10, CIFAR-100, and large-scale Clothing1M show that our method outperforms state-of-the-art noisy label learning methods with different levels of label noise. Our method can also be seamlessly combined with other noisy label learning methods to further improve their performance and generalize well to other tasks. The code is available in https://github.com/zyh-uaiaaaa/MDA-noisy-label-learning.

</details>

<details>

<summary>2022-12-25 10:15:15 - Strong Optimistic Solving for Dynamic Symbolic Execution</summary>

- *Darya Parygina, Alexey Vishnyakov, Andrey Fedotov*

- `2209.03710v2` - [abs](http://arxiv.org/abs/2209.03710v2) - [pdf](http://arxiv.org/pdf/2209.03710v2)

> Dynamic symbolic execution (DSE) is an effective method for automated program testing and bug detection. It is increasing the code coverage by the complex branches exploration during hybrid fuzzing. DSE tools invert the branches along some execution path and help fuzzer examine previously unavailable program parts. DSE often faces over- and underconstraint problems. The first one leads to significant analysis complication while the second one causes inaccurate symbolic execution.   We propose strong optimistic solving method that eliminates irrelevant path predicate constraints for target branch inversion. We eliminate such symbolic constraints that the target branch is not control dependent on. Moreover, we separately handle symbolic branches that have nested control transfer instructions that pass control beyond the parent branch scope, e.g. return, goto, break, etc. We implement the proposed method in our dynamic symbolic execution tool Sydr.   We evaluate the strong optimistic strategy, the optimistic strategy that contains only the last constraint negation, and their combination. The results show that the strategies combination helps increase either the code coverage or the average number of correctly inverted branches per one minute. It is optimal to apply both strategies together in contrast with other configurations.

</details>

<details>

<summary>2022-12-25 17:20:03 - GAE-ISumm: Unsupervised Graph-Based Summarization of Indian Languages</summary>

- *Lakshmi Sireesha Vakada, Anudeep Ch, Mounika Marreddy, Subba Reddy Oota, Radhika Mamidi*

- `2212.12937v1` - [abs](http://arxiv.org/abs/2212.12937v1) - [pdf](http://arxiv.org/pdf/2212.12937v1)

> Document summarization aims to create a precise and coherent summary of a text document. Many deep learning summarization models are developed mainly for English, often requiring a large training corpus and efficient pre-trained language models and tools. However, English summarization models for low-resource Indian languages are often limited by rich morphological variation, syntax, and semantic differences. In this paper, we propose GAE-ISumm, an unsupervised Indic summarization model that extracts summaries from text documents. In particular, our proposed model, GAE-ISumm uses Graph Autoencoder (GAE) to learn text representations and a document summary jointly. We also provide a manually-annotated Telugu summarization dataset TELSUM, to experiment with our model GAE-ISumm. Further, we experiment with the most publicly available Indian language summarization datasets to investigate the effectiveness of GAE-ISumm on other Indian languages. Our experiments of GAE-ISumm in seven languages make the following observations: (i) it is competitive or better than state-of-the-art results on all datasets, (ii) it reports benchmark results on TELSUM, and (iii) the inclusion of positional and cluster information in the proposed model improved the performance of summaries.

</details>

<details>

<summary>2022-12-26 02:48:37 - Simultaneously Optimizing Perturbations and Positions for Black-box Adversarial Patch Attacks</summary>

- *Xingxing Wei, Ying Guo, Jie Yu, Bo Zhang*

- `2212.12995v1` - [abs](http://arxiv.org/abs/2212.12995v1) - [pdf](http://arxiv.org/pdf/2212.12995v1)

> Adversarial patch is an important form of real-world adversarial attack that brings serious risks to the robustness of deep neural networks. Previous methods generate adversarial patches by either optimizing their perturbation values while fixing the pasting position or manipulating the position while fixing the patch's content. This reveals that the positions and perturbations are both important to the adversarial attack. For that, in this paper, we propose a novel method to simultaneously optimize the position and perturbation for an adversarial patch, and thus obtain a high attack success rate in the black-box setting. Technically, we regard the patch's position, the pre-designed hyper-parameters to determine the patch's perturbations as the variables, and utilize the reinforcement learning framework to simultaneously solve for the optimal solution based on the rewards obtained from the target model with a small number of queries. Extensive experiments are conducted on the Face Recognition (FR) task, and results on four representative FR models show that our method can significantly improve the attack success rate and query efficiency. Besides, experiments on the commercial FR service and physical environments confirm its practical application value. We also extend our method to the traffic sign recognition task to verify its generalization ability.

</details>

<details>

<summary>2022-12-26 07:01:23 - Key Feature Replacement of In-Distribution Samples for Out-of-Distribution Detection</summary>

- *Jaeyoung Kim, Seo Taek Kong, Dongbin Na, Kyu-Hwan Jung*

- `2301.13012v1` - [abs](http://arxiv.org/abs/2301.13012v1) - [pdf](http://arxiv.org/pdf/2301.13012v1)

> Out-of-distribution (OOD) detection can be used in deep learning-based applications to reject outlier samples from being unreliably classified by deep neural networks. Learning to classify between OOD and in-distribution samples is difficult because data comprising the former is extremely diverse. It has been observed that an auxiliary OOD dataset is most effective in training a "rejection" network when its samples are semantically similar to in-distribution images. We first deduce that OOD images are perceived by a deep neural network to be semantically similar to in-distribution samples when they share a common background, as deep networks are observed to incorrectly classify such images with high confidence. We then propose a simple yet effective Key In-distribution feature Replacement BY inpainting (KIRBY) procedure that constructs a surrogate OOD dataset by replacing class-discriminative features of in-distribution samples with marginal background features. The procedure can be implemented using off-the-shelf vision algorithms, where each step within the algorithm is shown to make the surrogate data increasingly similar to in-distribution data. Design choices in each step are studied extensively, and an exhaustive comparison with state-of-the-art algorithms demonstrates KIRBY's competitiveness on various benchmarks.

</details>

<details>

<summary>2022-12-26 08:08:44 - Kidney and Kidney Tumour Segmentation in CT Images</summary>

- *Qi Ming How, Hoi Leong Lee*

- `2212.13034v1` - [abs](http://arxiv.org/abs/2212.13034v1) - [pdf](http://arxiv.org/pdf/2212.13034v1)

> Automatic segmentation of kidney and kidney tumour in Computed Tomography (CT) images is essential, as it uses less time as compared to the current gold standard of manual segmentation. However, many hospitals are still reliant on manual study and segmentation of CT images by medical practitioners because of its higher accuracy. Thus, this study focuses on the development of an approach for automatic kidney and kidney tumour segmentation in contrast-enhanced CT images. A method based on Convolutional Neural Network (CNN) was proposed, where a 3D U-Net segmentation model was developed and trained to delineate the kidney and kidney tumour from CT scans. Each CT image was pre-processed before inputting to the CNN, and the effect of down-sampled and patch-wise input images on the model performance was analysed. The proposed method was evaluated on the publicly available 2021 Kidney and Kidney Tumour Segmentation Challenge (KiTS21) dataset. The method with the best performing model recorded an average training Dice score of 0.6129, with the kidney and kidney tumour Dice scores of 0.7923 and 0.4344, respectively. For testing, the model obtained a kidney Dice score of 0.8034, and a kidney tumour Dice score of 0.4713, with an average Dice score of 0.6374.

</details>

<details>

<summary>2022-12-26 08:12:41 - Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment</summary>

- *Yechun Tang, Xiaoxia Cheng, Weiming Lu*

- `2212.13036v1` - [abs](http://arxiv.org/abs/2212.13036v1) - [pdf](http://arxiv.org/pdf/2212.13036v1)

> Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an alignment-enhanced complex question answering framework, called ALCQA, which mitigates this gap through question-to-action alignment and question-to-question alignment. We train a question rewriting model to align the question and each action, and utilize a pretrained language model to implicitly align the question and KG artifacts. Moreover, considering that similar questions correspond to similar action sequences, we retrieve top-k similar question-answer pairs at the inference stage through question-to-question alignment and propose a novel reward-guided action sequence selection strategy to select from candidate action sequences. We conduct experiments on CQA and WQSP datasets, and the results show that our approach outperforms state-of-the-art methods and obtains a 9.88\% improvements in the F1 metric on CQA dataset. Our source code is available at https://github.com/TTTTTTTTy/ALCQA.

</details>

<details>

<summary>2022-12-26 14:07:16 - Domain-invariant Feature Exploration for Domain Generalization</summary>

- *Wang Lu, Jindong Wang, Haoliang Li, Yiqiang Chen, Xing Xie*

- `2207.12020v2` - [abs](http://arxiv.org/abs/2207.12020v2) - [pdf](http://arxiv.org/pdf/2207.12020v2)

> Deep learning has achieved great success in the past few years. However, the performance of deep learning is likely to impede in face of non-IID situations. Domain generalization (DG) enables a model to generalize to an unseen test distribution, i.e., to learn domain-invariant representations. In this paper, we argue that domain-invariant features should be originating from both internal and mutual sides. Internal invariance means that the features can be learned with a single domain and the features capture intrinsic semantics of data, i.e., the property within a domain, which is agnostic to other domains. Mutual invariance means that the features can be learned with multiple domains (cross-domain) and the features contain common information, i.e., the transferable features w.r.t. other domains. We then propose DIFEX for Domain-Invariant Feature EXploration. DIFEX employs a knowledge distillation framework to capture the high-level Fourier phase as the internally-invariant features and learn cross-domain correlation alignment as the mutually-invariant features. We further design an exploration loss to increase the feature diversity for better generalization. Extensive experiments on both time-series and visual benchmarks demonstrate that the proposed DIFEX achieves state-of-the-art performance.

</details>

<details>

<summary>2022-12-27 11:00:58 - Personality Detection of Applicants And Employees Using K-mode Algorithm And Ocean Model</summary>

- *Binisha Mohan, Dinju Vattavayalil Joseph, Bharat Plavelil Subhash*

- `2212.14675v1` - [abs](http://arxiv.org/abs/2212.14675v1) - [pdf](http://arxiv.org/pdf/2212.14675v1)

> The combination of conduct, emotion, motivation, and thinking is referred to as personality. To shortlist candidates more effectively, many organizations rely on personality predictions. The firm can hire or pick the best candidate for the desired job description by grouping applicants based on the necessary personality preferences. A model is created to identify applicants' personality types so that employers may find qualified candidates by examining a person's facial expression, speech intonation, and resume. Additionally, the paper emphasises detecting the changes in employee behaviour. Employee attitudes and behaviour towards each set of questions are being examined and analysed. Here, the K-Modes clustering method is used to predict employee well-being, including job pressure, the working environment, and relationships with peers, utilizing the OCEAN Model and the CNN algorithm in the AVI-AI administrative system. Findings imply that AVIs can be used for efficient candidate screening with an AI decision agent. The study of the specific field is beyond the current explorations and needed to be expanded with deeper models and new configurations that can patch extremely complex operations.

</details>

<details>

<summary>2022-12-27 11:50:14 - TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence</summary>

- *Wang Qi, Rui Liu, Yuan Zuo, Yong Chen, Dell Zhang*

- `2212.13456v1` - [abs](http://arxiv.org/abs/2212.13456v1) - [pdf](http://arxiv.org/pdf/2212.13456v1)

> Creating an essay based on a few given topics is a challenging NLP task. Although several effective methods for this problem, topic-to-essay generation, have appeared recently, there is still much room for improvement, especially in terms of the coverage of the given topics and the coherence of the generated text. In this paper, we propose a novel approach called TegFormer which utilizes the Transformer architecture where the encoder is enriched with domain-specific contexts while the decoder is enhanced by a large-scale pre-trained language model. Specifically, a \emph{Topic-Extension} layer capturing the interaction between the given topics and their domain-specific contexts is plugged into the encoder. Since the given topics are usually concise and sparse, such an additional layer can bring more topic-related semantics in to facilitate the subsequent natural language generation. Moreover, an \emph{Embedding-Fusion} module that combines the domain-specific word embeddings learnt from the given corpus and the general-purpose word embeddings provided by a GPT-2 model pre-trained on massive text data is integrated into the decoder. Since GPT-2 is at a much larger scale, it contains a lot more implicit linguistic knowledge which would help the decoder to produce more grammatical and readable text. Extensive experiments have shown that the pieces of text generated by TegFormer have better topic coverage and higher text coherence than those from SOTA topic-to-essay techniques, according to automatic and human evaluations. As revealed by ablation studies, both the Topic-Extension layer and the Embedding-Fusion module contribute substantially to TegFormer's performance advantage.

</details>

<details>

<summary>2022-12-27 13:57:15 - Semantic optical fiber communication system</summary>

- *Zhenming Yu, Hongyu Huang, Liming Cheng, Wei Zhang, Yueqiu Mu, Kun Xu*

- `2212.14739v1` - [abs](http://arxiv.org/abs/2212.14739v1) - [pdf](http://arxiv.org/pdf/2212.14739v1)

> The current optical communication systems minimize bit or symbol errors without considering the semantic meaning behind digital bits, thus transmitting a lot of unnecessary information. We propose and experimentally demonstrate a semantic optical fiber communication (SOFC) system. Instead of encoding information into bits for transmission, semantic information is extracted from the source using deep learning. The generated semantic symbols are then directly transmitted through an optical fiber. Compared with the bit-based structure, the SOFC system achieved higher information compression and a more stable performance, especially in the low received optical power regime, and enhanced the robustness against optical link impairments. This work introduces an intelligent optical communication system at the human analytical thinking level, which is a significant step toward a breakthrough in the current optical communication architecture.

</details>

<details>

<summary>2022-12-27 13:58:30 - MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic Parsing</summary>

- *Longxu Dou, Yan Gao, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Jian-Guang Lou*

- `2212.13492v1` - [abs](http://arxiv.org/abs/2212.13492v1) - [pdf](http://arxiv.org/pdf/2212.13492v1)

> Text-to-SQL semantic parsing is an important NLP task, which greatly facilitates the interaction between users and the database and becomes the key component in many human-computer interaction systems. Much recent progress in text-to-SQL has been driven by large-scale datasets, but most of them are centered on English. In this work, we present MultiSpider, the largest multilingual text-to-SQL dataset which covers seven languages (English, German, French, Spanish, Japanese, Chinese, and Vietnamese). Upon MultiSpider, we further identify the lexical and structural challenges of text-to-SQL (caused by specific language properties and dialect sayings) and their intensity across different languages. Experimental results under three typical settings (zero-shot, monolingual and multilingual) reveal a 6.1% absolute drop in accuracy in non-English languages. Qualitative and quantitative analyses are conducted to understand the reason for the performance drop of each language. Besides the dataset, we also propose a simple schema augmentation framework SAVe (Schema-Augmentation-with-Verification), which significantly boosts the overall performance by about 1.8% and closes the 29.5% performance gap across languages.

</details>

<details>

<summary>2022-12-27 16:26:15 - Learning Spatiotemporal Frequency-Transformer for Low-Quality Video Super-Resolution</summary>

- *Zhongwei Qiu, Huan Yang, Jianlong Fu, Daochang Liu, Chang Xu, Dongmei Fu*

- `2212.14046v1` - [abs](http://arxiv.org/abs/2212.14046v1) - [pdf](http://arxiv.org/pdf/2212.14046v1)

> Video Super-Resolution (VSR) aims to restore high-resolution (HR) videos from low-resolution (LR) videos. Existing VSR techniques usually recover HR frames by extracting pertinent textures from nearby frames with known degradation processes. Despite significant progress, grand challenges are remained to effectively extract and transmit high-quality textures from high-degraded low-quality sequences, such as blur, additive noises, and compression artifacts. In this work, a novel Frequency-Transformer (FTVSR) is proposed for handling low-quality videos that carry out self-attention in a combined space-time-frequency domain. First, video frames are split into patches and each patch is transformed into spectral maps in which each channel represents a frequency band. It permits a fine-grained self-attention on each frequency band, so that real visual texture can be distinguished from artifacts. Second, a novel dual frequency attention (DFA) mechanism is proposed to capture the global frequency relations and local frequency relations, which can handle different complicated degradation processes in real-world scenarios. Third, we explore different self-attention schemes for video processing in the frequency domain and discover that a ``divided attention'' which conducts a joint space-frequency attention before applying temporal-frequency attention, leads to the best video enhancement quality. Extensive experiments on three widely-used VSR datasets show that FTVSR outperforms state-of-the-art methods on different low-quality videos with clear visual margins. Code and pre-trained models are available at https://github.com/researchmm/FTVSR.

</details>

<details>

<summary>2022-12-27 19:24:31 - Co-supervised learning paradigm with conditional generative adversarial networks for sample-efficient classification</summary>

- *Hao Zhen, Yucheng Shi, Jidong J. Yang, Javad Mohammadpour Vehni*

- `2212.13589v1` - [abs](http://arxiv.org/abs/2212.13589v1) - [pdf](http://arxiv.org/pdf/2212.13589v1)

> Classification using supervised learning requires annotating a large amount of classes-balanced data for model training and testing. This has practically limited the scope of applications with supervised learning, in particular deep learning. To address the issues associated with limited and imbalanced data, this paper introduces a sample-efficient co-supervised learning paradigm (SEC-CGAN), in which a conditional generative adversarial network (CGAN) is trained alongside the classifier and supplements semantics-conditioned, confidence-aware synthesized examples to the annotated data during the training process. In this setting, the CGAN not only serves as a co-supervisor but also provides complementary quality examples to aid the classifier training in an end-to-end fashion. Experiments demonstrate that the proposed SEC-CGAN outperforms the external classifier GAN (EC-GAN) and a baseline ResNet-18 classifier. For the comparison, all classifiers in above methods adopt the ResNet-18 architecture as the backbone. Particularly, for the Street View House Numbers dataset, using the 5% of training data, a test accuracy of 90.26% is achieved by SEC-CGAN as opposed to 88.59% by EC-GAN and 87.17% by the baseline classifier; for the highway image dataset, using the 10% of training data, a test accuracy of 98.27% is achieved by SEC-CGAN, compared to 97.84% by EC-GAN and 95.52% by the baseline classifier.

</details>

<details>

<summary>2022-12-27 20:27:04 - Efficiently Hardening SGX Enclaves against Memory Access Pattern Attacks via Dynamic Program Partitioning</summary>

- *Yuzhe Tang, Kai Li, Yibo Wang, Jiaqi Chen, Cheng Xu*

- `2212.12656v2` - [abs](http://arxiv.org/abs/2212.12656v2) - [pdf](http://arxiv.org/pdf/2212.12656v2)

> Intel SGX is known to be vulnerable to a class of practical attacks exploiting memory access pattern side-channels, notably page-fault attacks and cache timing attacks. A promising hardening scheme is to wrap applications in hardware transactions, enabled by Intel TSX, that return control to the software upon unexpected cache misses and interruptions so that the existing side-channel attacks exploiting these micro-architectural events can be detected and mitigated. However, existing hardening schemes scale only to small-data computation, with a typical working set smaller than one or few times (e.g., $8$ times) of a CPU data cache.   This work tackles the data scalability and performance efficiency of security hardening schemes of Intel SGX enclaves against memory-access pattern side channels. The key insight is that the size of TSX transactions in the target computation is critical, both performance- and security-wise. Unlike the existing designs, this work dynamically partitions target computations to enlarge transactions while avoiding aborts, leading to lower performance overhead and improved side-channel security. We materialize the dynamic partitioning scheme and build a C++ library to monitor and model cache utilization at runtime. We further build a data analytical system using the library and implement various external oblivious algorithms. Performance evaluation shows that our work can effectively increase transaction size and reduce the execution time by up to two orders of magnitude compared with the state-of-the-art solutions.

</details>

<details>

<summary>2022-12-28 04:02:19 - Knowledge Distillation from A Stronger Teacher</summary>

- *Tao Huang, Shan You, Fei Wang, Chen Qian, Chang Xu*

- `2205.10536v3` - [abs](http://arxiv.org/abs/2205.10536v3) - [pdf](http://arxiv.org/pdf/2205.10536v3)

> Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically find that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would suffice, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classification, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST_KD .

</details>

<details>

<summary>2022-12-28 09:54:52 - Representation Separation for Semantic Segmentation with Vision Transformers</summary>

- *Yuanduo Hong, Huihui Pan, Weichao Sun, Xinghu Yu, Huijun Gao*

- `2212.13764v1` - [abs](http://arxiv.org/abs/2212.13764v1) - [pdf](http://arxiv.org/pdf/2212.13764v1)

> Vision transformers (ViTs) encoding an image as a sequence of patches bring new paradigms for semantic segmentation.We present an efficient framework of representation separation in local-patch level and global-region level for semantic segmentation with ViTs. It is targeted for the peculiar over-smoothness of ViTs in semantic segmentation, and therefore differs from current popular paradigms of context modeling and most existing related methods reinforcing the advantage of attention. We first deliver the decoupled two-pathway network in which another pathway enhances and passes down local-patch discrepancy complementary to global representations of transformers. We then propose the spatially adaptive separation module to obtain more separate deep representations and the discriminative cross-attention which yields more discriminative region representations through novel auxiliary supervisions. The proposed methods achieve some impressive results: 1) incorporated with large-scale plain ViTs, our methods achieve new state-of-the-art performances on five widely used benchmarks; 2) using masked pre-trained plain ViTs, we achieve 68.9% mIoU on Pascal Context, setting a new record; 3) pyramid ViTs integrated with the decoupled two-pathway network even surpass the well-designed high-resolution ViTs on Cityscapes; 4) the improved representations by our framework have favorable transferability in images with natural corruptions. The codes will be released publicly.

</details>

<details>

<summary>2022-12-28 12:56:15 - Sparse Coding in a Dual Memory System for Lifelong Learning</summary>

- *Fahad Sarfraz, Elahe Arani, Bahram Zonooz*

- `2301.05058v1` - [abs](http://arxiv.org/abs/2301.05058v1) - [pdf](http://arxiv.org/pdf/2301.05058v1)

> Efficient continual learning in humans is enabled by a rich set of neurophysiological mechanisms and interactions between multiple memory systems. The brain efficiently encodes information in non-overlapping sparse codes, which facilitates the learning of new associations faster with controlled interference with previous associations. To mimic sparse coding in DNNs, we enforce activation sparsity along with a dropout mechanism which encourages the model to activate similar units for semantically similar inputs and have less overlap with activation patterns of semantically dissimilar inputs. This provides us with an efficient mechanism for balancing the reusability and interference of features, depending on the similarity of classes across tasks. Furthermore, we employ sparse coding in a multiple-memory replay mechanism. Our method maintains an additional long-term semantic memory that aggregates and consolidates information encoded in the synaptic weights of the working model. Our extensive evaluation and characteristics analysis show that equipped with these biologically inspired mechanisms, the model can further mitigate forgetting.

</details>

<details>

<summary>2022-12-28 17:35:41 - Breaking the Architecture Barrier: A Method for Efficient Knowledge Transfer Across Networks</summary>

- *Maciej A. Czyzewski, Daniel Nowak, Kamil Piechowiak*

- `2212.13970v1` - [abs](http://arxiv.org/abs/2212.13970v1) - [pdf](http://arxiv.org/pdf/2212.13970v1)

> Transfer learning is a popular technique for improving the performance of neural networks. However, existing methods are limited to transferring parameters between networks with same architectures. We present a method for transferring parameters between neural networks with different architectures. Our method, called DPIAT, uses dynamic programming to match blocks and layers between architectures and transfer parameters efficiently. Compared to existing parameter prediction and random initialization methods, it significantly improves training efficiency and validation accuracy. In experiments on ImageNet, our method improved validation accuracy by an average of 1.6 times after 50 epochs of training. DPIAT allows both researchers and neural architecture search systems to modify trained networks and reuse knowledge, avoiding the need for retraining from scratch. We also introduce a network architecture similarity measure, enabling users to choose the best source network without any training.

</details>

<details>

<summary>2022-12-28 17:53:43 - TiG-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning</summary>

- *Peixiang Huang, Li Liu, Renrui Zhang, Song Zhang, Xinli Xu, Baichao Wang, Guoyi Liu*

- `2212.13979v1` - [abs](http://arxiv.org/abs/2212.13979v1) - [pdf](http://arxiv.org/pdf/2212.13979v1)

> To achieve accurate and low-cost 3D object detection, existing methods propose to benefit camera-based multi-view detectors with spatial cues provided by the LiDAR modality, e.g., dense depth supervision and bird-eye-view (BEV) feature distillation. However, they directly conduct point-to-point mimicking from LiDAR to camera, which neglects the inner-geometry of foreground targets and suffers from the modal gap between 2D-3D features. In this paper, we propose the learning scheme of Target Inner-Geometry from the LiDAR modality into camera-based BEV detectors for both dense depth and BEV features, termed as TiG-BEV. First, we introduce an inner-depth supervision module to learn the low-level relative depth relations between different foreground pixels. This enables the camera-based detector to better understand the object-wise spatial structures. Second, we design an inner-feature BEV distillation module to imitate the high-level semantics of different keypoints within foreground targets. To further alleviate the BEV feature gap between two modalities, we adopt both inter-channel and inter-keypoint distillation for feature-similarity modeling. With our target inner-geometry distillation, TiG-BEV can effectively boost BEVDepth by +2.3% NDS and +2.4% mAP, along with BEVDet by +9.1% NDS and +10.3% mAP on nuScenes val set. Code will be available at https://github.com/ADLab3Ds/TiG-BEV.

</details>

<details>

<summary>2022-12-28 20:34:17 - Finding Representative Group Fairness Metrics Using Correlation Estimations</summary>

- *Hadis Anahideh, Nazanin Nezami, Abolfazl Asudeh*

- `2109.05697v2` - [abs](http://arxiv.org/abs/2109.05697v2) - [pdf](http://arxiv.org/pdf/2109.05697v2)

> It is of critical importance to be aware of the historical discrimination embedded in the data and to consider a fairness measure to reduce bias throughout the predictive modeling pipeline. Given various notions of fairness defined in the literature, investigating the correlation and interaction among metrics is vital for addressing unfairness. Practitioners and data scientists should be able to comprehend each metric and examine their impact on one another given the context, use case, and regulations. Exploring the combinatorial space of different metrics for such examination is burdensome. To alleviate the burden of selecting fairness notions for consideration, we propose a framework that estimates the correlation among fairness notions. Our framework consequently identifies a set of diverse and semantically distinct metrics as representative for a given context. We propose a Monte-Carlo sampling technique for computing the correlations between fairness metrics by indirect and efficient perturbation in the model space. Using the estimated correlations, we then find a subset of representative metrics. The paper proposes a generic method that can be generalized to any arbitrary set of fairness metrics. We showcase the validity of the proposal using comprehensive experiments on real-world benchmark datasets.

</details>

<details>

<summary>2022-12-28 21:16:48 - Self-Attentive Pooling for Efficient Deep Learning</summary>

- *Fang Chen, Gourav Datta, Souvik Kundu, Peter Beerel*

- `2209.07659v3` - [abs](http://arxiv.org/abs/2209.07659v3) - [pdf](http://arxiv.org/pdf/2209.07659v3)

> Efficient custom pooling techniques that can aggressively trim the dimensions of a feature map and thereby reduce inference compute and memory footprint for resource-constrained computer vision applications have recently gained significant traction. However, prior pooling works extract only the local context of the activation maps, limiting their effectiveness. In contrast, we propose a novel non-local self-attentive pooling method that can be used as a drop-in replacement to the standard pooling layers, such as max/average pooling or strided convolution. The proposed self-attention module uses patch embedding, multi-head self-attention, and spatial-channel restoration, followed by sigmoid activation and exponential soft-max. This self-attention mechanism efficiently aggregates dependencies between non-local activation patches during down-sampling. Extensive experiments on standard object classification and detection tasks with various convolutional neural network (CNN) architectures demonstrate the superiority of our proposed mechanism over the state-of-the-art (SOTA) pooling techniques. In particular, we surpass the test accuracy of existing pooling techniques on different variants of MobileNet-V2 on ImageNet by an average of 1.2%. With the aggressive down-sampling of the activation maps in the initial layers (providing up to 22x reduction in memory consumption), our approach achieves 1.43% higher test accuracy compared to SOTA techniques with iso-memory footprints. This enables the deployment of our models in memory-constrained devices, such as micro-controllers (without losing significant accuracy), because the initial activation maps consume a significant amount of on-chip memory for high-resolution images required for complex vision tasks. Our proposed pooling method also leverages the idea of channel pruning to further reduce memory footprints.

</details>

<details>

<summary>2022-12-29 01:49:52 - SESNet: sequence-structure feature-integrated deep learning method for data-efficient protein engineering</summary>

- *Mingchen Li, Liqi Kang, Yi Xiong, Yu Guang Wang, Guisheng Fan, Pan Tan, Liang Hong*

- `2301.00004v1` - [abs](http://arxiv.org/abs/2301.00004v1) - [pdf](http://arxiv.org/pdf/2301.00004v1)

> Deep learning has been widely used for protein engineering. However, it is limited by the lack of sufficient experimental data to train an accurate model for predicting the functional fitness of high-order mutants. Here, we develop SESNet, a supervised deep-learning model to predict the fitness for protein mutants by leveraging both sequence and structure information, and exploiting attention mechanism. Our model integrates local evolutionary context from homologous sequences, the global evolutionary context encoding rich semantic from the universal protein sequence space and the structure information accounting for the microenvironment around each residue in a protein. We show that SESNet outperforms state-of-the-art models for predicting the sequence-function relationship on 26 deep mutational scanning datasets. More importantly, we propose a data augmentation strategy by leveraging the data from unsupervised models to pre-train our model. After that, our model can achieve strikingly high accuracy in prediction of the fitness of protein mutants, especially for the higher order variants (> 4 mutation sites), when finetuned by using only a small number of experimental mutation data (<50). The strategy proposed is of great practical value as the required experimental effort, i.e., producing a few tens of experimental mutation data on a given protein, is generally affordable by an ordinary biochemical group and can be applied on almost any protein.

</details>

<details>

<summary>2022-12-29 03:04:16 - Transactions Make Debugging Easy</summary>

- *Qian Li, Peter Kraft, Michael Cafarella, ÃaÄatay Demiralp, Goetz Graefe, Christos Kozyrakis, Michael Stonebraker, Lalith Suresh, Matei Zaharia*

- `2212.14161v1` - [abs](http://arxiv.org/abs/2212.14161v1) - [pdf](http://arxiv.org/pdf/2212.14161v1)

> We propose TROD, a novel transaction-oriented framework for debugging modern distributed web applications and online services. Our critical insight is that if applications store all state in databases and only access state transactionally, TROD can use lightweight always-on tracing to track the history of application state changes and data provenance, and then leverage the captured traces and transaction logs to faithfully replay or even test modified code retroactively on any past event. We demonstrate how TROD can simplify programming and debugging in production applications, list several research challenges and directions, and encourage the database and systems communities to drastically rethink the synergy between the way people develop and debug applications.

</details>

<details>

<summary>2022-12-29 05:13:39 - Rollout Algorithms and Approximate Dynamic Programming for Bayesian Optimization and Sequential Estimation</summary>

- *Dimitri Bertsekas*

- `2212.07998v3` - [abs](http://arxiv.org/abs/2212.07998v3) - [pdf](http://arxiv.org/pdf/2212.07998v3)

> We provide a unifying approximate dynamic programming framework that applies to a broad variety of problems involving sequential estimation. We consider first the construction of surrogate cost functions for the purposes of optimization, and we focus on the special case of Bayesian optimization, using the rollout algorithm and some of its variations. We then discuss the more general case of sequential estimation of a random vector using optimal measurement selection, and its application to problems of stochastic and adaptive control. We distinguish between adaptive control of deterministic and stochastic systems: the former are better suited for the use of rollout, while the latter are well suited for the use of rollout with certainty equivalence approximations. As an example of the deterministic case, we discuss sequential decoding problems, and a rollout algorithm for the approximate solution of the Wordle and Mastermind puzzles, recently developed in the paper [BBB22].

</details>

<details>

<summary>2022-12-29 06:14:21 - A Bayesian Framework for Automated Debugging</summary>

- *Sungmin Kang, Wonkeun Choi, Shin Yoo*

- `2212.13773v2` - [abs](http://arxiv.org/abs/2212.13773v2) - [pdf](http://arxiv.org/pdf/2212.13773v2)

> Debugging takes up a significant portion of developer time. As a result, automated debugging techniques including Fault Localization (FL) and Automated Program Repair (APR) have garnered significant attention due to their potential to aid developers in debugging tasks. Despite intensive research on these subjects, we are unaware of a theoretic framework that highlights the principles behind automated debugging and allows abstract analysis of techniques. Such a framework would heighten our understanding of the endeavor and provide a way to formally analyze techniques and approaches. To this end, we first propose a Bayesian framework of understanding automated repair and find that in conjunction with a concrete statement of the objective of automated debugging, we can recover maximal fault localization formulae from prior work, as well as analyze existing APR techniques and their underlying assumptions.   As a means of empirically demonstrating our framework, we further propose BAPP, a Bayesian Patch Prioritization technique that incorporates intermediate program values to analyze likely patch locations and repair actions, with its core equations being derived by our Bayesian framework. We find that incorporating program values allows BAPP to identify correct patches more precisely: when applied to the patches generated by kPAR, the rankings produced by BAPP reduce the number of required patch validation by 68% and consequently reduce the repair time by 34 minutes on average. Further, BAPP improves the precision of FL, increasing acc@5 on the studied bugs from 8 to 11. These results highlight the potential of value-cognizant automated debugging techniques, and further validates our theoretical framework. Finally, future directions that the framework suggests are provided.

</details>

<details>

<summary>2022-12-29 08:08:04 - Lookback for Learning to Branch</summary>

- *Prateek Gupta, Elias B. Khalil, Didier ChetÃ©lat, Maxime Gasse, Yoshua Bengio, Andrea Lodi, M. Pawan Kumar*

- `2206.14987v2` - [abs](http://arxiv.org/abs/2206.14987v2) - [pdf](http://arxiv.org/pdf/2206.14987v2)

> The expressive and computationally inexpensive bipartite Graph Neural Networks (GNN) have been shown to be an important component of deep learning based Mixed-Integer Linear Program (MILP) solvers. Recent works have demonstrated the effectiveness of such GNNs in replacing the branching (variable selection) heuristic in branch-and-bound (B&B) solvers. These GNNs are trained, offline and on a collection of MILPs, to imitate a very good but computationally expensive branching heuristic, strong branching. Given that B&B results in a tree of sub-MILPs, we ask (a) whether there are strong dependencies exhibited by the target heuristic among the neighboring nodes of the B&B tree, and (b) if so, whether we can incorporate them in our training procedure. Specifically, we find that with the strong branching heuristic, a child node's best choice was often the parent's second-best choice. We call this the "lookback" phenomenon. Surprisingly, the typical branching GNN of Gasse et al. (2019) often misses this simple "answer". To imitate the target behavior more closely by incorporating the lookback phenomenon in GNNs, we propose two methods: (a) target smoothing for the standard cross-entropy loss function, and (b) adding a Parent-as-Target (PAT) Lookback regularizer term. Finally, we propose a model selection framework to incorporate harder-to-formulate objectives such as solving time in the final models. Through extensive experimentation on standard benchmark instances, we show that our proposal results in up to 22% decrease in the size of the B&B tree and up to 15% improvement in the solving times.

</details>

<details>

<summary>2022-12-29 12:01:06 - Industrial Scene Change Detection using Deep Convolutional Neural Networks</summary>

- *Ali Atghaei, Ehsan Rahnama, Kiavash Azimi, Hassan Shahbazi*

- `2212.14278v1` - [abs](http://arxiv.org/abs/2212.14278v1) - [pdf](http://arxiv.org/pdf/2212.14278v1)

> Finding and localizing the conceptual changes in two scenes in terms of the presence or removal of objects in two images belonging to the same scene at different times in special care applications is of great significance. This is mainly due to the fact that addition or removal of important objects for some environments can be harmful. As a result, there is a need to design a program that locates these differences using machine vision. The most important challenge of this problem is the change in lighting conditions and the presence of shadows in the scene. Therefore, the proposed methods must be resistant to these challenges. In this article, a method based on deep convolutional neural networks using transfer learning is introduced, which is trained with an intelligent data synthesis process. The results of this method are tested and presented on the dataset provided for this purpose. It is shown that the presented method is more efficient than other methods and can be used in a variety of real industrial environments.

</details>

<details>

<summary>2022-12-29 13:12:40 - Semantic Communications with Discrete-time Analog Transmission: A PAPR Perspective</summary>

- *Yulin Shao, Deniz Gunduz*

- `2208.08342v3` - [abs](http://arxiv.org/abs/2208.08342v3) - [pdf](http://arxiv.org/pdf/2208.08342v3)

> Recent progress in deep learning (DL)-based joint source-channel coding (DeepJSCC) has led to a new paradigm of semantic communications. Two salient features of DeepJSCC-based semantic communications are the exploitation of semantic-aware features directly from the source signal, and the discrete-time analog transmission (DTAT) of these features. Compared with traditional digital communications, semantic communications with DeepJSCC provide superior reconstruction performance at the receiver and graceful degradation with diminishing channel quality, but also exhibit a large peak-to-average power ratio (PAPR) in the transmitted signal. An open question has been whether the gains of DeepJSCC come from the additional freedom brought by the high-PAPR continuous-amplitude signal. In this paper, we address this question by exploring three PAPR reduction techniques in the application of image transmission. We confirm that the superior image reconstruction performance of DeepJSCC-based semantic communications can be retained while the transmitted PAPR is suppressed to an acceptable level. This observation is an important step towards the implementation of DeepJSCC in practical semantic communication systems.

</details>

<details>

<summary>2022-12-29 14:20:48 - Safe Subgame Resolving for Extensive Form Correlated Equilibrium</summary>

- *Chun Kai Ling, Fei Fang*

- `2212.14317v1` - [abs](http://arxiv.org/abs/2212.14317v1) - [pdf](http://arxiv.org/pdf/2212.14317v1)

> Correlated Equilibrium is a solution concept that is more general than Nash Equilibrium (NE) and can lead to outcomes with better social welfare. However, its natural extension to the sequential setting, the \textit{Extensive Form Correlated Equilibrium} (EFCE), requires a quadratic amount of space to solve, even in restricted settings without randomness in nature. To alleviate these concerns, we apply \textit{subgame resolving}, a technique extremely successful in finding NE in zero-sum games to solving general-sum EFCEs. Subgame resolving refines a correlation plan in an \textit{online} manner: instead of solving for the full game upfront, it only solves for strategies in subgames that are reached in actual play, resulting in significant computational gains. In this paper, we (i) lay out the foundations to quantify the quality of a refined strategy, in terms of the \textit{social welfare} and \textit{exploitability} of correlation plans, (ii) show that EFCEs possess a sufficient amount of independence between subgames to perform resolving efficiently, and (iii) provide two algorithms for resolving, one using linear programming and the other based on regret minimization. Both methods guarantee \textit{safety}, i.e., they will never be counterproductive. Our methods are the first time an online method has been applied to the correlated, general-sum setting.

</details>

<details>

<summary>2022-12-29 15:35:51 - Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods</summary>

- *Davood Zabihzadeh, Zahraa Alitbi, Seyed Jalaleddin Mousavirad*

- `2107.01130v2` - [abs](http://arxiv.org/abs/2107.01130v2) - [pdf](http://arxiv.org/pdf/2107.01130v2)

> Deep Metric Learning (DML) learns a non-linear semantic embedding from input data that brings similar pairs together while keeping dissimilar data away from each other. To this end, many different methods are proposed in the last decade with promising results in various applications. The success of a DML algorithm greatly depends on its loss function. However, no loss function is perfect, and it deals only with some aspects of an optimal similarity embedding. Besides, the generalizability of the DML on unseen categories during the test stage is an important matter that is not considered by existing loss functions. To address these challenges, we propose novel approaches to combine different losses built on top of a shared deep feature extractor. The proposed ensemble of losses enforces the deep model to extract features that are consistent with all losses. Since the selected losses are diverse and each emphasizes different aspects of an optimal semantic embedding, our effective combining methods yield a considerable improvement over any individual loss and generalize well on unseen categories. Here, there is no limitation in choosing loss functions, and our methods can work with any set of existing ones. Besides, they can optimize each loss function as well as its weight in an end-to-end paradigm with no need to adjust any hyper-parameter. We evaluate our methods on some popular datasets from the machine vision domain in conventional Zero-Shot-Learning (ZSL) settings. The results are very encouraging and show that our methods outperform all baseline losses by a large margin in all datasets.

</details>

<details>

<summary>2022-12-29 22:12:39 - An electronic warfare approach for deploying a software-based Wi-Fi jammer</summary>

- *Keshav Kaushik, Rahul Negi, Prabhav Dev*

- `2212.14470v1` - [abs](http://arxiv.org/abs/2212.14470v1) - [pdf](http://arxiv.org/pdf/2212.14470v1)

> Some prominent instances have been centered on electronic warfare. For example, the American military has made significant investments in automation through UAV programs, only for competitors like the Iranians to create strategies to interfere with these systems. Iran managed to capture a top-secret U.S. surveil-lance drone by fooling it into descending in the incorrect place by jamming its control signals and providing it with bogus GPS data. In this paper, the authors have focused on the electronic warfare approach for deploying a software-based Wi-Fi jammer. The software-based Wi-Fi jammer can disconnect the targets using the DoS pursuit mode. The paper describes the unique methodology of how software can also be used for jamming wireless signals.

</details>

<details>

<summary>2022-12-29 23:34:19 - Inching Towards Automated Understanding of the Meaning of Art: An Application to Computational Analysis of Mondrian's Artwork</summary>

- *Alex Doboli, Mahan Agha Zahedi, Niloofar Gholamrezaei*

- `2302.00594v1` - [abs](http://arxiv.org/abs/2302.00594v1) - [pdf](http://arxiv.org/pdf/2302.00594v1)

> Deep Neural Networks (DNNs) have been successfully used in classifying digital images but have been less successful in classifying images with meanings that are not linear combinations of their visualized features, like images of artwork. Moreover, it is unknown what additional features must be included into DNNs, so that they can possibly classify using features beyond visually displayed features, like color, size, and form. Non-displayed features are important in abstract representations, reasoning, and understanding ambiguous expressions, which are arguably topics less studied by current AI methods. This paper attempts to identify capabilities that are related to semantic processing, a current limitation of DNNs. The proposed methodology identifies the missing capabilities by comparing the process of understanding Mondrian's paintings with the process of understanding electronic circuit designs, another creative problem solving instance. The compared entities are cognitive architectures that attempt to loosely mimic cognitive activities. The paper offers a detailed presentation of the characteristics of the architectural components, like goals, concepts, ideas, rules, procedures, beliefs, expectations, and outcomes. To explain the usefulness of the methodology, the paper discusses a new, three-step computational method to distinguish Mondrian's paintings from other artwork. The method includes in a backward order the cognitive architecture's components that operate only with the characteristics of the available data.

</details>

<details>

<summary>2022-12-30 01:06:16 - NNSmith: Generating Diverse and Valid Test Cases for Deep Learning Compilers</summary>

- *Jiawei Liu, Jinkun Lin, Fabian Ruffy, Cheng Tan, Jinyang Li, Aurojit Panda, Lingming Zhang*

- `2207.13066v2` - [abs](http://arxiv.org/abs/2207.13066v2) - [pdf](http://arxiv.org/pdf/2207.13066v2)

> Deep-learning (DL) compilers such as TVM and TensorRT are increasingly being used to optimize deep neural network (DNN) models to meet performance, resource utilization and other requirements. Bugs in these compilers can result in models whose semantics differ from the original ones, producing incorrect results that corrupt the correctness of downstream applications. However, finding bugs in these compilers is challenging due to their complexity. In this work, we propose a new fuzz testing approach for finding bugs in deep-learning compilers. Our core approach consists of (i) generating diverse yet valid DNN test models that can exercise a large part of the compiler's transformation logic using light-weight operator specifications; (ii) performing gradient-based search to find model inputs that avoid any floating-point exceptional values during model execution, reducing the chance of missed bugs or false alarms; and (iii) using differential testing to identify bugs. We implemented this approach in NNSmith which has found 72 new bugs for TVM, TensorRT, ONNXRuntime, and PyTorch to date. Of these 58 have been confirmed and 51 have been fixed by their respective project maintainers.

</details>

<details>

<summary>2022-12-30 03:00:43 - Secure Determinant Codes for Distributed Storage Systems</summary>

- *Adel Elmahdy, Michelle Kleckler, Soheil Mohajer*

- `2201.00313v2` - [abs](http://arxiv.org/abs/2201.00313v2) - [pdf](http://arxiv.org/pdf/2201.00313v2)

> The information-theoretic secure exact-repair regenerating codes for distributed storage systems (DSSs) with parameters $(n,k=d,d,\ell)$ are studied in this paper. We consider distributed storage systems with $n$ nodes, in which the original data can be recovered from any subset of $k=d$ nodes, and the content of any node can be retrieved from those of any $d$ helper nodes. Moreover, we consider two secrecy constraints, namely, Type-I, where the message remains secure against an eavesdropper with access to the content of any subset of up to $\ell$ nodes, and Type-II, in which the message remains secure against an eavesdropper who can observe the incoming repair data from all possible nodes to a fixed but unknown subset of up to $\ell$ compromised nodes. Two classes of secure determinant codes are proposed for Type-I and Type-II secrecy constraints. Each proposed code can be designed for a range of per-node storage capacity and repair bandwidth for any system parameters. They lead to two achievable secrecy trade-offs, for Type-I and Type-II security.

</details>

<details>

<summary>2022-12-30 05:19:11 - CARE: Certifiably Robust Learning with Reasoning via Variational Inference</summary>

- *Jiawei Zhang, Linyi Li, Ce Zhang, Bo Li*

- `2209.05055v3` - [abs](http://arxiv.org/abs/2209.05055v3) - [pdf](http://arxiv.org/pdf/2209.05055v3)

> Despite great recent advances achieved by deep neural networks (DNNs), they are often vulnerable to adversarial attacks. Intensive research efforts have been made to improve the robustness of DNNs; however, most empirical defenses can be adaptively attacked again, and the theoretically certified robustness is limited, especially on large-scale datasets. One potential root cause of such vulnerabilities for DNNs is that although they have demonstrated powerful expressiveness, they lack the reasoning ability to make robust and reliable predictions. In this paper, we aim to integrate domain knowledge to enable robust learning with the reasoning paradigm. In particular, we propose a certifiably robust learning with reasoning pipeline (CARE), which consists of a learning component and a reasoning component. Concretely, we use a set of standard DNNs to serve as the learning component to make semantic predictions, and we leverage the probabilistic graphical models, such as Markov logic networks (MLN), to serve as the reasoning component to enable knowledge/logic reasoning. However, it is known that the exact inference of MLN (reasoning) is #P-complete, which limits the scalability of the pipeline. To this end, we propose to approximate the MLN inference via variational inference based on an efficient expectation maximization algorithm. In particular, we leverage graph convolutional networks (GCNs) to encode the posterior distribution during variational inference and update the parameters of GCNs (E-step) and the weights of knowledge rules in MLN (M-step) iteratively. We conduct extensive experiments on different datasets and show that CARE achieves significantly higher certified robustness compared with the state-of-the-art baselines. We additionally conducted different ablation studies to demonstrate the empirical robustness of CARE and the effectiveness of different knowledge integration.

</details>

<details>

<summary>2022-12-30 09:15:11 - Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference</summary>

- *Maren Pielka, Svetlana Schmidt, Lisa Pucknat, Rafet Sifa*

- `2212.07428v5` - [abs](http://arxiv.org/abs/2212.07428v5) - [pdf](http://arxiv.org/pdf/2212.07428v5)

> We introduce a linguistically enhanced combination of pre-training methods for transformers. The pre-training objectives include POS-tagging, synset prediction based on semantic knowledge graphs, and parent prediction based on dependency parse trees. Our approach achieves competitive results on the Natural Language Inference task, compared to the state of the art. Specifically for smaller models, the method results in a significant performance boost, emphasizing the fact that intelligent pre-training can make up for fewer parameters and help building more efficient models. Combining POS-tagging and synset prediction yields the overall best results.

</details>

<details>

<summary>2022-12-30 10:18:10 - Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets</summary>

- *Zhiying Lu, Hongtao Xie, Chuanbin Liu, Yongdong Zhang*

- `2210.05958v2` - [abs](http://arxiv.org/abs/2210.05958v2) - [pdf](http://arxiv.org/pdf/2210.05958v2)

> There still remains an extreme performance gap between Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) when training from scratch on small datasets, which is concluded to the lack of inductive bias. In this paper, we further consider this problem and point out two weaknesses of ViTs in inductive biases, that is, the spatial relevance and diverse channel representation. First, on spatial aspect, objects are locally compact and relevant, thus fine-grained feature needs to be extracted from a token and its neighbors. While the lack of data hinders ViTs to attend the spatial relevance. Second, on channel aspect, representation exhibits diversity on different channels. But the scarce data can not enable ViTs to learn strong enough representation for accurate recognition. To this end, we propose Dynamic Hybrid Vision Transformer (DHVT) as the solution to enhance the two inductive biases. On spatial aspect, we adopt a hybrid structure, in which convolution is integrated into patch embedding and multi-layer perceptron module, forcing the model to capture the token features as well as their neighboring features. On channel aspect, we introduce a dynamic feature aggregation module in MLP and a brand new "head token" design in multi-head self-attention module to help re-calibrate channel representation and make different channel group representation interacts with each other. The fusion of weak channel representation forms a strong enough representation for classification. With this design, we successfully eliminate the performance gap between CNNs and ViTs, and our DHVT achieves a series of state-of-the-art performance with a lightweight model, 85.68% on CIFAR-100 with 22.8M parameters, 82.3% on ImageNet-1K with 24.0M parameters. Code is available at https://github.com/ArieSeirack/DHVT.

</details>

<details>

<summary>2022-12-30 10:48:23 - Relational Message Passing for Fully Inductive Knowledge Graph Completion</summary>

- *Yuxia Geng, Jiaoyan Chen, Jeff Z. Pan, Mingyang Chen, Song Jiang, Wen Zhang, Huajun Chen*

- `2210.03994v2` - [abs](http://arxiv.org/abs/2210.03994v2) - [pdf](http://arxiv.org/pdf/2210.03994v2)

> In knowledge graph completion (KGC), predicting triples involving emerging entities and/or relations, which are unseen when the KG embeddings are learned, has become a critical challenge. Subgraph reasoning with message passing is a promising and popular solution. Some recent methods have achieved good performance, but they (i) usually can only predict triples involving unseen entities alone, failing to address more realistic fully inductive situations with both unseen entities and unseen relations, and (ii) often conduct message passing over the entities with the relation patterns not fully utilized. In this study, we propose a new method named RMPI which uses a novel Relational Message Passing network for fully Inductive KGC. It passes messages directly between relations to make full use of the relation patterns for subgraph reasoning with new techniques on graph transformation, graph pruning, relation-aware neighborhood attention, addressing empty subgraphs, etc., and can utilize the relation semantics defined in the ontological schema of KG. Extensive evaluation on multiple benchmarks has shown the effectiveness of techniques involved in RMPI and its better performance compared with the existing methods that support fully inductive KGC. RMPI is also comparable to the state-of-the-art partially inductive KGC methods with very promising results achieved. Our codes and data are available at https://github.com/zjukg/RMPI.

</details>

<details>

<summary>2022-12-30 12:20:56 - HPointLoc: Point-based Indoor Place Recognition using Synthetic RGB-D Images</summary>

- *Dmitry Yudin, Yaroslav Solomentsev, Ruslan Musaev, Aleksei Staroverov, Aleksandr I. Panov*

- `2212.14649v1` - [abs](http://arxiv.org/abs/2212.14649v1) - [pdf](http://arxiv.org/pdf/2212.14649v1)

> We present a novel dataset named as HPointLoc, specially designed for exploring capabilities of visual place recognition in indoor environment and loop detection in simultaneous localization and mapping. The loop detection sub-task is especially relevant when a robot with an on-board RGB-D camera can drive past the same place (``Point") at different angles. The dataset is based on the popular Habitat simulator, in which it is possible to generate photorealistic indoor scenes using both own sensor data and open datasets, such as Matterport3D. To study the main stages of solving the place recognition problem on the HPointLoc dataset, we proposed a new modular approach named as PNTR. It first performs an image retrieval with the Patch-NetVLAD method, then extracts keypoints and matches them using R2D2, LoFTR or SuperPoint with SuperGlue, and finally performs a camera pose optimization step with TEASER++. Such a solution to the place recognition problem has not been previously studied in existing publications. The PNTR approach has shown the best quality metrics on the HPointLoc dataset and has a high potential for real use in localization systems for unmanned vehicles. The proposed dataset and framework are publicly available: https://github.com/metra4ok/HPointLoc.

</details>

<details>

<summary>2022-12-30 12:39:26 - Linear programming word problems formulation using EnsembleCRF NER labeler and T5 text generator with data augmentations</summary>

- *JiangLong He, Mamatha N, Shiv Vignesh, Deepak Kumar, Akshay Uppal*

- `2212.14657v1` - [abs](http://arxiv.org/abs/2212.14657v1) - [pdf](http://arxiv.org/pdf/2212.14657v1)

> We propose an ensemble approach to predict the labels in linear programming word problems. The entity identification and the meaning representation are two types of tasks to be solved in the NL4Opt competition. We propose the ensembleCRF method to identify the named entities for the first task. We found that single models didn't improve for the given task in our analysis. A set of prediction models predict the entities. The generated results are combined to form a consensus result in the ensembleCRF method. We present an ensemble text generator to produce the representation sentences for the second task. We thought of dividing the problem into multiple small tasks due to the overflow in the output. A single model generates different representations based on the prompt. All the generated text is combined to form an ensemble and produce a mathematical meaning of a linear programming problem.

</details>

<details>

<summary>2022-12-30 15:38:45 - BlueCov: Integrating Test Coverage and Model Checking with JBMC</summary>

- *Matthias GÃ¼demann, Peter Schrammel*

- `2212.14779v1` - [abs](http://arxiv.org/abs/2212.14779v1) - [pdf](http://arxiv.org/pdf/2212.14779v1)

> Automated test case generation tools help businesses to write tests and increase the safety net provided by high regression test coverage when making code changes. Test generation needs to cover as much as possible of the uncovered code while avoiding generating redundant tests for code that is already covered by an existing test-suite.   In this paper we present our work on a tool for the real world application of integrating formal analysis with automatic test case generation. The test case generation is based on coverage analysis using the Java bounded model checker (JBMC). Counterexamples of the model checker can be translated into Java method calls with specific parameters.   In order to avoid the generation of redundant tests, it is necessary to measure the coverage in the exact same way as JBMC generates its coverage goals. Each existing coverage measurement tool uses a slightly different instrumentation and thus a different coverage criterion. This makes integration with a test case generator based on formal analysis difficult. Therefore, we developed BlueCov as a specific runtime coverage measurement tool which uses the exact same coverage criteria as JBMC does. This approach also allows for incremental test-case generation, only generating test coverage for previously untested code, e.g., to complete existing test suites.

</details>

<details>

<summary>2022-12-30 17:47:43 - Deterministic counting LovÃ¡sz local lemma beyond linear programming</summary>

- *Kun He, Chunyang Wang, Yitong Yin*

- `2212.14847v1` - [abs](http://arxiv.org/abs/2212.14847v1) - [pdf](http://arxiv.org/pdf/2212.14847v1)

> We give a simple combinatorial algorithm to deterministically approximately count the number of satisfying assignments of general constraint satisfaction problems (CSPs). Suppose that the CSP has domain size $q=O(1)$, each constraint contains at most $k=O(1)$ variables, shares variables with at most $\Delta=O(1)$ constraints, and is violated with probability at most $p$ by a uniform random assignment. The algorithm returns in polynomial time in an improved local lemma regime: \[ q^2\cdot k\cdot p\cdot\Delta^5\le C_0\quad\text{for a suitably small absolute constant }C_0. \] Here the key term $\Delta^5$ improves the previously best known $\Delta^7$ for general CSPs [JPV21b] and $\Delta^{5.714}$ for the special case of $k$-CNF [JPV21a, HSW21]. Our deterministic counting algorithm is a derandomization of the very recent fast sampling algorithm in [HWY22]. It departs substantially from all previous deterministic counting Lov\'{a}sz local lemma algorithms which relied on linear programming, and gives a deterministic approximate counting algorithm that straightforwardly derandomizes a fast sampling algorithm, hence unifying the fast sampling and deterministic approximate counting in the same algorithmic framework. To obtain the improved regime, in our analysis we develop a refinement of the $\{2,3\}$-trees that were used in the previous analyses of counting/sampling LLL. Similar techniques can be applied to the previous LP-based algorithms to obtain the same improved regime and may be of independent interests.

</details>

<details>

<summary>2022-12-30 17:50:54 - Symbolic Visual Reinforcement Learning: A Scalable Framework with Object-Level Abstraction and Differentiable Expression Search</summary>

- *Wenqing Zheng, S P Sharan, Zhiwen Fan, Kevin Wang, Yihan Xi, Zhangyang Wang*

- `2212.14849v1` - [abs](http://arxiv.org/abs/2212.14849v1) - [pdf](http://arxiv.org/pdf/2212.14849v1)

> Learning efficient and interpretable policies has been a challenging task in reinforcement learning (RL), particularly in the visual RL setting with complex scenes. While neural networks have achieved competitive performance, the resulting policies are often over-parameterized black boxes that are difficult to interpret and deploy efficiently. More recent symbolic RL frameworks have shown that high-level domain-specific programming logic can be designed to handle both policy learning and symbolic planning. However, these approaches rely on coded primitives with little feature learning, and when applied to high-dimensional visual scenes, they can suffer from scalability issues and perform poorly when images have complex object interactions. To address these challenges, we propose \textit{Differentiable Symbolic Expression Search} (DiffSES), a novel symbolic learning approach that discovers discrete symbolic policies using partially differentiable optimization. By using object-level abstractions instead of raw pixel-level inputs, DiffSES is able to leverage the simplicity and scalability advantages of symbolic expressions, while also incorporating the strengths of neural networks for feature learning and optimization. Our experiments demonstrate that DiffSES is able to generate symbolic policies that are simpler and more and scalable than state-of-the-art symbolic RL methods, with a reduced amount of symbolic prior knowledge.

</details>

<details>

<summary>2022-12-30 17:56:07 - Image Embedding for Denoising Generative Models</summary>

- *Andrea Asperti, Davide Evangelista, Samuele Marro, Fabio Merizzi*

- `2301.07485v1` - [abs](http://arxiv.org/abs/2301.07485v1) - [pdf](http://arxiv.org/pdf/2301.07485v1)

> Denoising Diffusion models are gaining increasing popularity in the field of generative modeling for several reasons, including the simple and stable training, the excellent generative quality, and the solid probabilistic foundation. In this article, we address the problem of {\em embedding} an image into the latent space of Denoising Diffusion Models, that is finding a suitable ``noisy'' image whose denoising results in the original image. We particularly focus on Denoising Diffusion Implicit Models due to the deterministic nature of their reverse diffusion process. As a side result of our investigation, we gain a deeper insight into the structure of the latent space of diffusion models, opening interesting perspectives on its exploration, the definition of semantic trajectories, and the manipulation/conditioning of encodings for editing purposes. A particularly interesting property highlighted by our research, which is also characteristic of this class of generative models, is the independence of the latent representation from the networks implementing the reverse diffusion process. In other words, a common seed passed to different networks (each trained on the same dataset), eventually results in identical images.

</details>

<details>

<summary>2022-12-30 18:28:54 - Depth-First Search performance in a random digraph with geometric outdegree distribution</summary>

- *Philippe Jacquet, Svante Janson*

- `2212.14865v1` - [abs](http://arxiv.org/abs/2212.14865v1) - [pdf](http://arxiv.org/pdf/2212.14865v1)

> We present an analysis of the depth-first search algorithm in a random digraph model with independent outdegrees having a geometric distribution.   The results include asymptotic results for the depth profile of vertices, the height (maximum depth) and average depth, the number of trees in the forest, the size of the largest and second-largest trees, and the numbers of arcs of different types in the depth-first jungle. Most results are first order. For the height we show an asymptotic normal distribution.   This analysis proposed by Donald Knuth in his next to appear volume of The Art of Computer Programming gives interesting insight in one of the most elegant and efficient algorithm for graph analysis due to Tarjan.

</details>

<details>

<summary>2022-12-30 19:21:44 - Modified Query Expansion Through Generative Adversarial Networks for Information Extraction in E-Commerce</summary>

- *Altan Cakir, Mert Gurkan*

- `2301.00036v1` - [abs](http://arxiv.org/abs/2301.00036v1) - [pdf](http://arxiv.org/pdf/2301.00036v1)

> This work addresses an alternative approach for query expansion (QE) using a generative adversarial network (GAN) to enhance the effectiveness of information search in e-commerce. We propose a modified QE conditional GAN (mQE-CGAN) framework, which resolves keywords by expanding the query with a synthetically generated query that proposes semantic information from text input. We train a sequence-to-sequence transformer model as the generator to produce keywords and use a recurrent neural network model as the discriminator to classify an adversarial output with the generator. With the modified CGAN framework, various forms of semantic insights gathered from the query document corpus are introduced to the generation process. We leverage these insights as conditions for the generator model and discuss their effectiveness for the query expansion task. Our experiments demonstrate that the utilization of condition structures within the mQE-CGAN framework can increase the semantic similarity between generated sequences and reference documents up to nearly 10% compared to baseline models

</details>

<details>

<summary>2022-12-30 21:06:34 - Real-time large-scale supplier order assignments across two-tiers of a supply chain with penalty and dual-sourcing</summary>

- *Vinod Kumar Chauhan, Stephen Mak, Ajith Kumar Parlikad, Muhannad Alomari, Linus Casassa, Alexandra Brintrup*

- `2210.11953v3` - [abs](http://arxiv.org/abs/2210.11953v3) - [pdf](http://arxiv.org/pdf/2210.11953v3)

> Supplier selection and order allocation (SSOA) are key strategic decisions in supply chain management which greatly impact the performance of the supply chain. Although, the SSOA problem has been studied extensively but less attention paid to scalability presents a significant gap preventing adoption of SSOA algorithms by industrial practitioners. This paper presents a novel multi-item, multi-supplier double order allocations with dual-sourcing and penalty constraints across two-tiers of a supply chain, resulting in cooperation and in facilitating supplier preferences to work with other suppliers through bidding. We propose Mixed-Integer Programming models for allocations at individual-tiers as well as an integrated allocations. An application to a real-time large-scale case study of a manufacturing company is presented, which is the largest scale studied in terms of supply chain size and number of variables so far in literature. The use case allows us to highlight how problem formulation and implementation can help reduce computational complexity using Mathematical Programming (MP) and Genetic Algorithm (GA) approaches. The results show an interesting observation that MP outperforms GA to solve SSOA. Sensitivity analysis is presented for sourcing strategy, penalty threshold and penalty factor. The developed model was successfully deployed in a large international sourcing conference with multiple bidding rounds, which helped in more than 10% procurement cost reductions to the manufacturing company.

</details>

<details>

<summary>2022-12-30 21:08:57 - Task-Guided IRL in POMDPs that Scales</summary>

- *Franck Djeumou, Christian Ellis, Murat Cubuktepe, Craig Lennon, Ufuk Topcu*

- `2301.01219v1` - [abs](http://arxiv.org/abs/2301.01219v1) - [pdf](http://arxiv.org/pdf/2301.01219v1)

> In inverse reinforcement learning (IRL), a learning agent infers a reward function encoding the underlying task using demonstrations from experts. However, many existing IRL techniques make the often unrealistic assumption that the agent has access to full information about the environment. We remove this assumption by developing an algorithm for IRL in partially observable Markov decision processes (POMDPs). We address two limitations of existing IRL techniques. First, they require an excessive amount of data due to the information asymmetry between the expert and the learner. Second, most of these IRL techniques require solving the computationally intractable forward problem -- computing an optimal policy given a reward function -- in POMDPs. The developed algorithm reduces the information asymmetry while increasing the data efficiency by incorporating task specifications expressed in temporal logic into IRL. Such specifications may be interpreted as side information available to the learner a priori in addition to the demonstrations. Further, the algorithm avoids a common source of algorithmic complexity by building on causal entropy as the measure of the likelihood of the demonstrations as opposed to entropy. Nevertheless, the resulting problem is nonconvex due to the so-called forward problem. We solve the intrinsic nonconvexity of the forward problem in a scalable manner through a sequential linear programming scheme that guarantees to converge to a locally optimal policy. In a series of examples, including experiments in a high-fidelity Unity simulator, we demonstrate that even with a limited amount of data and POMDPs with tens of thousands of states, our algorithm learns reward functions and policies that satisfy the task while inducing similar behavior to the expert by leveraging the provided side information.

</details>

<details>

<summary>2022-12-31 01:12:27 - Autonomous Driving Simulator based on Neurorobotics Platform</summary>

- *Wei Cao, Liguo Zhou, Yuhong Huang, Alois Knoll*

- `2301.00089v1` - [abs](http://arxiv.org/abs/2301.00089v1) - [pdf](http://arxiv.org/pdf/2301.00089v1)

> There are many artificial intelligence algorithms for autonomous driving, but directly installing these algorithms on vehicles is unrealistic and expensive. At the same time, many of these algorithms need an environment to train and optimize. Simulation is a valuable and meaningful solution with training and testing functions, and it can say that simulation is a critical link in the autonomous driving world. There are also many different applications or systems of simulation from companies or academies such as SVL and Carla. These simulators flaunt that they have the closest real-world simulation, but their environment objects, such as pedestrians and other vehicles around the agent-vehicle, are already fixed programmed. They can only move along the pre-setting trajectory, or random numbers determine their movements. What is the situation when all environmental objects are also installed by Artificial Intelligence, or their behaviors are like real people or natural reactions of other drivers? This problem is a blind spot for most of the simulation applications, or these applications cannot be easy to solve this problem. The Neurorobotics Platform from the TUM team of Prof. Alois Knoll has the idea about "Engines" and "Transceiver Functions" to solve the multi-agents problem. This report will start with a little research on the Neurorobotics Platform and analyze the potential and possibility of developing a new simulator to achieve the true real-world simulation goal. Then based on the NRP-Core Platform, this initial development aims to construct an initial demo experiment. The consist of this report starts with the basic knowledge of NRP-Core and its installation, then focus on the explanation of the necessary components for a simulation experiment, at last, about the details of constructions for the autonomous driving system, which is integrated object detection and autonomous control.

</details>

<details>

<summary>2022-12-31 08:46:02 - Knowledge-Based Dataset for Training PE Malware Detection Models</summary>

- *Peter Å vec, Å tefan Balogh, Martin Homola, JÃ¡n KÄ¾uka*

- `2301.00153v1` - [abs](http://arxiv.org/abs/2301.00153v1) - [pdf](http://arxiv.org/pdf/2301.00153v1)

> Ontologies are a standard for semantic schemata in many knowledge-intensive domains of human interest. They are now becoming increasingly important also in areas until very recently dominated by subsymbolic representations and machine-learning-based data processing. One such area is information security, and more specifically malware detection. We propose PE Malware Ontology that offers a reusable semantic schema for Portable Executable (PE, Windows binary format) malware files. The ontology was inspired by the structure of the data in the EMBER dataset and it currently covers the data intended for static malware analysis. With this proposal, we hope to achieve: a) a unified semantic representation for PE malware datasets that are available or will be published in the future; (b) applicability of symbolic, neural-symbolic, or otherwise explainable approaches in the PE Malware domain that may lead to improved interpretability of results which may now be characterized by the terms defined in the ontology; and (c)by joint publishing of semantically treated EMBER data, including fractional datasets, also improved reproducibility of experiments.

</details>

<details>

<summary>2022-12-31 14:20:21 - Neural System Level Synthesis: Learning over All Stabilizing Policies for Nonlinear Systems</summary>

- *Luca Furieri, Clara LucÃ­a Galimberti, Giancarlo Ferrari-Trecate*

- `2203.11812v2` - [abs](http://arxiv.org/abs/2203.11812v2) - [pdf](http://arxiv.org/pdf/2203.11812v2)

> We address the problem of designing stabilizing control policies for nonlinear systems in discrete-time, while minimizing an arbitrary cost function. When the system is linear and the cost is convex, the System Level Synthesis (SLS) approach offers an effective solution based on convex programming. Beyond this case, a globally optimal solution cannot be found in a tractable way, in general. In this paper, we develop a parametrization of all and only the control policies stabilizing a given time-varying nonlinear system in terms of the combined effect of 1) a strongly stabilizing base controller and 2) a stable SLS operator to be freely designed. Based on this result, we propose a Neural SLS (Neur-SLS) approach guaranteeing closed-loop stability during and after parameter optimization, without requiring any constraints to be satisfied. We exploit recent Deep Neural Network (DNN) models based on Recurrent Equilibrium Networks (RENs) to learn over a rich class of nonlinear stable operators, and demonstrate the effectiveness of the proposed approach in numerical examples.

</details>

<details>

<summary>2022-12-31 21:37:14 - MERLIN: Multi-agent offline and transfer learning for occupant-centric energy flexible operation of grid-interactive communities using smart meter data and CityLearn</summary>

- *Kingsley Nweye, Siva Sankaranarayanan, Zoltan Nagy*

- `2301.01148v1` - [abs](http://arxiv.org/abs/2301.01148v1) - [pdf](http://arxiv.org/pdf/2301.01148v1)

> The decarbonization of buildings presents new challenges for the reliability of the electrical grid as a result of the intermittency of renewable energy sources and increase in grid load brought about by end-use electrification. To restore reliability, grid-interactive efficient buildings can provide flexibility services to the grid through demand response. Residential demand response programs are hindered by the need for manual intervention by customers. To maximize the energy flexibility potential of residential buildings, an advanced control architecture is needed. Reinforcement learning is well-suited for the control of flexible resources as it is able to adapt to unique building characteristics compared to expert systems. Yet, factors hindering the adoption of RL in real-world applications include its large data requirements for training, control security and generalizability. Here we address these challenges by proposing the MERLIN framework and using a digital twin of a real-world 17-building grid-interactive residential community in CityLearn. We show that 1) independent RL-controllers for batteries improve building and district level KPIs compared to a reference RBC by tailoring their policies to individual buildings, 2) despite unique occupant behaviours, transferring the RL policy of any one of the buildings to other buildings provides comparable performance while reducing the cost of training, 3) training RL-controllers on limited temporal data that does not capture full seasonality in occupant behaviour has little effect on performance. Although, the zero-net-energy (ZNE) condition of the buildings could be maintained or worsened as a result of controlled batteries, KPIs that are typically improved by ZNE condition (electricity price and carbon emissions) are further improved when the batteries are managed by an advanced controller.

</details>

