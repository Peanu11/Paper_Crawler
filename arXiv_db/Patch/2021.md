# 2021

## TOC

- [2021-01](#2021-01)
- [2021-02](#2021-02)
- [2021-03](#2021-03)
- [2021-04](#2021-04)
- [2021-05](#2021-05)
- [2021-06](#2021-06)
- [2021-07](#2021-07)
- [2021-08](#2021-08)
- [2021-09](#2021-09)
- [2021-10](#2021-10)
- [2021-11](#2021-11)
- [2021-12](#2021-12)

## 2021-01

<details>

<summary>2021-01-04 04:10:38 - Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification</summary>

- *Siyuan Cheng, Yingqi Liu, Shiqing Ma, Xiangyu Zhang*

- `2012.11212v2` - [abs](http://arxiv.org/abs/2012.11212v2) - [pdf](http://arxiv.org/pdf/2012.11212v2)

> Trojan (backdoor) attack is a form of adversarial attack on deep neural networks where the attacker provides victims with a model trained/retrained on malicious data. The backdoor can be activated when a normal input is stamped with a certain pattern called trigger, causing misclassification. Many existing trojan attacks have their triggers being input space patches/objects (e.g., a polygon with solid color) or simple input transformations such as Instagram filters. These simple triggers are susceptible to recent backdoor detection algorithms. We propose a novel deep feature space trojan attack with five characteristics: effectiveness, stealthiness, controllability, robustness and reliance on deep features. We conduct extensive experiments on 9 image classifiers on various datasets including ImageNet to demonstrate these properties and show that our attack can evade state-of-the-art defense.

</details>

<details>

<summary>2021-01-04 22:42:28 - A Research Ecosystem for Secure Computing</summary>

- *Nadya Bliss, Lawrence A. Gordon, Daniel Lopresti, Fred Schneider, Suresh Venkatasubramanian*

- `2101.01264v1` - [abs](http://arxiv.org/abs/2101.01264v1) - [pdf](http://arxiv.org/pdf/2101.01264v1)

> Computing devices are vital to all areas of modern life and permeate every aspect of our society. The ubiquity of computing and our reliance on it has been accelerated and amplified by the COVID-19 pandemic. From education to work environments to healthcare to defense to entertainment - it is hard to imagine a segment of modern life that is not touched by computing. The security of computers, systems, and applications has been an active area of research in computer science for decades. However, with the confluence of both the scale of interconnected systems and increased adoption of artificial intelligence, there are many research challenges the community must face so that our society can continue to benefit and risks are minimized, not multiplied. Those challenges range from security and trust of the information ecosystem to adversarial artificial intelligence and machine learning.   Along with basic research challenges, more often than not, securing a system happens after the design or even deployment, meaning the security community is routinely playing catch-up and attempting to patch vulnerabilities that could be exploited any minute. While security measures such as encryption and authentication have been widely adopted, questions of security tend to be secondary to application capability. There needs to be a sea-change in the way we approach this critically important aspect of the problem: new incentives and education are at the core of this change. Now is the time to refocus research community efforts on developing interconnected technologies with security "baked in by design" and creating an ecosystem that ensures adoption of promising research developments. To realize this vision, two additional elements of the ecosystem are necessary - proper incentive structures for adoption and an educated citizenry that is well versed in vulnerabilities and risks.

</details>

<details>

<summary>2021-01-06 16:17:39 - SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations</summary>

- *Giulio Lovisotto, Henry Turner, Ivo Sluganovic, Martin Strohmeier, Ivan Martinovic*

- `2007.04137v3` - [abs](http://arxiv.org/abs/2007.04137v3) - [pdf](http://arxiv.org/pdf/2007.04137v3)

> Research into adversarial examples (AE) has developed rapidly, yet static adversarial patches are still the main technique for conducting attacks in the real world, despite being obvious, semi-permanent and unmodifiable once deployed.   In this paper, we propose Short-Lived Adversarial Perturbations (SLAP), a novel technique that allows adversaries to realize physically robust real-world AE by using a light projector. Attackers can project a specifically crafted adversarial perturbation onto a real-world object, transforming it into an AE. This allows the adversary greater control over the attack compared to adversarial patches: (i) projections can be dynamically turned on and off or modified at will, (ii) projections do not suffer from the locality constraint imposed by patches, making them harder to detect.   We study the feasibility of SLAP in the self-driving scenario, targeting both object detector and traffic sign recognition tasks, focusing on the detection of stop signs. We conduct experiments in a variety of ambient light conditions, including outdoors, showing how in non-bright settings the proposed method generates AE that are extremely robust, causing misclassifications on state-of-the-art networks with up to 99% success rate for a variety of angles and distances. We also demostrate that SLAP-generated AE do not present detectable behaviours seen in adversarial patches and therefore bypass SentiNet, a physical AE detection method. We evaluate other defences including an adaptive defender using adversarial learning which is able to thwart the attack effectiveness up to 80% even in favourable attacker conditions.

</details>

<details>

<summary>2021-01-08 04:07:19 - DeepPoison: Feature Transfer Based Stealthy Poisoning Attack</summary>

- *Jinyin Chen, Longyuan Zhang, Haibin Zheng, Xueke Wang, Zhaoyan Ming*

- `2101.02562v2` - [abs](http://arxiv.org/abs/2101.02562v2) - [pdf](http://arxiv.org/pdf/2101.02562v2)

> Deep neural networks are susceptible to poisoning attacks by purposely polluted training data with specific triggers. As existing episodes mainly focused on attack success rate with patch-based samples, defense algorithms can easily detect these poisoning samples. We propose DeepPoison, a novel adversarial network of one generator and two discriminators, to address this problem. Specifically, the generator automatically extracts the target class' hidden features and embeds them into benign training samples. One discriminator controls the ratio of the poisoning perturbation. The other discriminator works as the target model to testify the poisoning effects. The novelty of DeepPoison lies in that the generated poisoned training samples are indistinguishable from the benign ones by both defensive methods and manual visual inspection, and even benign test samples can achieve the attack. Extensive experiments have shown that DeepPoison can achieve a state-of-the-art attack success rate, as high as 91.74%, with only 7% poisoned samples on publicly available datasets LFW and CASIA. Furthermore, we have experimented with high-performance defense algorithms such as autodecoder defense and DBSCAN cluster detection and showed the resilience of DeepPoison.

</details>

<details>

<summary>2021-01-08 06:36:56 - (De)Randomized Smoothing for Certifiable Defense against Patch Attacks</summary>

- *Alexander Levine, Soheil Feizi*

- `2002.10733v3` - [abs](http://arxiv.org/abs/2002.10733v3) - [pdf](http://arxiv.org/pdf/2002.10733v3)

> Patch adversarial attacks on images, in which the attacker can distort pixels within a region of bounded size, are an important threat model since they provide a quantitative model for physical adversarial attacks. In this paper, we introduce a certifiable defense against patch attacks that guarantees for a given image and patch attack size, no patch adversarial examples exist. Our method is related to the broad class of randomized smoothing robustness schemes which provide high-confidence probabilistic robustness certificates. By exploiting the fact that patch attacks are more constrained than general sparse attacks, we derive meaningfully large robustness certificates against them. Additionally, in contrast to smoothing-based defenses against L_p and sparse attacks, our defense method against patch attacks is de-randomized, yielding improved, deterministic certificates. Compared to the existing patch certification method proposed by Chiang et al. (2020), which relies on interval bound propagation, our method can be trained significantly faster, achieves high clean and certified robust accuracy on CIFAR-10, and provides certificates at ImageNet scale. For example, for a 5-by-5 patch attack on CIFAR-10, our method achieves up to around 57.6% certified accuracy (with a classifier with around 83.8% clean accuracy), compared to at most 30.3% certified accuracy for the existing method (with a classifier with around 47.8% clean accuracy). Our results effectively establish a new state-of-the-art of certifiable defense against patch attacks on CIFAR-10 and ImageNet. Code is available at https://github.com/alevine0/patchSmoothing.

</details>

<details>

<summary>2021-01-09 00:27:23 - SyReNN: A Tool for Analyzing Deep Neural Networks</summary>

- *Matthew Sotoudeh, Aditya V. Thakur*

- `2101.03263v1` - [abs](http://arxiv.org/abs/2101.03263v1) - [pdf](http://arxiv.org/pdf/2101.03263v1)

> Deep Neural Networks (DNNs) are rapidly gaining popularity in a variety of important domains. Formally, DNNs are complicated vector-valued functions which come in a variety of sizes and applications. Unfortunately, modern DNNs have been shown to be vulnerable to a variety of attacks and buggy behavior. This has motivated recent work in formally analyzing the properties of such DNNs. This paper introduces SyReNN, a tool for understanding and analyzing a DNN by computing its symbolic representation. The key insight is to decompose the DNN into linear functions. Our tool is designed for analyses using low-dimensional subsets of the input space, a unique design point in the space of DNN analysis tools. We describe the tool and the underlying theory, then evaluate its use and performance on three case studies: computing Integrated Gradients, visualizing a DNN's decision boundaries, and patching a DNN.

</details>

<details>

<summary>2021-01-09 22:33:16 - Quantum Generative Models for Small Molecule Drug Discovery</summary>

- *Junde Li, Rasit Topaloglu, Swaroop Ghosh*

- `2101.03438v1` - [abs](http://arxiv.org/abs/2101.03438v1) - [pdf](http://arxiv.org/pdf/2101.03438v1)

> Existing drug discovery pipelines take 5-10 years and cost billions of dollars. Computational approaches aim to sample from regions of the whole molecular and solid-state compounds called chemical space which could be on the order of 1060 . Deep generative models can model the underlying probability distribution of both the physical structures and property of drugs and relate them nonlinearly. By exploiting patterns in massive datasets, these models can distill salient features that characterize the molecules. Generative Adversarial Networks (GANs) discover drug candidates by generating molecular structures that obey chemical and physical properties and show affinity towards binding with the receptor for a target disease. However, classical GANs cannot explore certain regions of the chemical space and suffer from curse-of-dimensionality. A full quantum GAN may require more than 90 qubits even to generate QM9-like small molecules. We propose a qubit-efficient quantum GAN with a hybrid generator (QGAN-HG) to learn richer representation of molecules via searching exponentially large chemical space with few qubits more efficiently than classical GAN. The QGANHG model is composed of a hybrid quantum generator that supports various number of qubits and quantum circuit layers, and, a classical discriminator. QGAN-HG with only 14.93% retained parameters can learn molecular distribution as efficiently as classical counterpart. The QGAN-HG variation with patched circuits considerably accelerates our standard QGANHG training process and avoids potential gradient vanishing issue of deep neural networks. Code is available on GitHub https://github.com/jundeli/quantum-gan.

</details>

<details>

<summary>2021-01-12 00:04:44 - SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities</summary>

- *Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, Zhaoxuan Chen*

- `1807.06756v3` - [abs](http://arxiv.org/abs/1807.06756v3) - [pdf](http://arxiv.org/pdf/1807.06756v3)

> The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been "silently" patched by the vendors when releasing newer versions of the pertinent software products.

</details>

<details>

<summary>2021-01-14 08:31:05 - Broadband Finite-Element Impedance Computation for Parasitic Extraction</summary>

- *Jonathan Stysch, Andreas Klaedtke, Herbert De Gersem*

- `2009.08232v3` - [abs](http://arxiv.org/abs/2009.08232v3) - [pdf](http://arxiv.org/pdf/2009.08232v3)

> Parasitic extraction is a powerful tool in the design process of electromechanical devices, specifically as part of workflows that check electromagnetic compatibility. A novel scheme to extract impedances from CAD device models, suitable for a finite element implementation, is derived from Maxwell's equations in differential form. It provides a foundation for parasitic extraction across a broad frequency range and is able to handle inhomogeneous permittivities and permeabilities, making it more flexible than existing integral equation approaches. The approach allows for the automatic treatment of multi-port models of arbitrary conductor geometry without requiring any significant manual user interaction. This is achieved by computing a connecting source current density that supplies current to the model's terminals, whatever their location in the model, subsequently using this current density to compute the electric field, and finally calculating the impedance via a scalar potential. A mandatory low-frequency stabilization scheme is outlined, ensuring a stable evaluation of the model at low frequencies as well. Two quasistatic approximations and the special case of perfect electric conductors are treated theoretically. The magnetoquasistatic approximation is validated against an analytical model in a numerical experiment. Moreover, the intrinsic capability of the method to treat inhomogeneous permittivities and permeabilities is demonstrated with a simple capacitor-coil model including dielectric insulation and magnetic core materials.

</details>

<details>

<summary>2021-01-19 09:30:58 - The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods</summary>

- *Louis Thiry, Michael Arbel, Eugene Belilovsky, Edouard Oyallon*

- `2101.07528v1` - [abs](http://arxiv.org/abs/2101.07528v1) - [pdf](http://arxiv.org/pdf/2101.07528v1)

> A recent line of work showed that various forms of convolutional kernel methods can be competitive with standard supervised deep convolutional networks on datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while being more amenable to theoretical analysis. In this work, we highlight the importance of a data-dependent feature extraction step that is key to the obtain good performance in convolutional kernel methods. This step typically corresponds to a whitened dictionary of patches, and gives rise to a data-driven convolutional kernel methods. We extensively study its effect, demonstrating it is the key ingredient for high performance of these methods. Specifically, we show that one of the simplest instances of such kernel methods, based on a single layer of image patches followed by a linear classifier is already obtaining classification accuracies on CIFAR-10 in the same range as previous more sophisticated convolutional kernel methods. We scale this method to the challenging ImageNet dataset, showing such a simple approach can exceed all existing non-learned representation methods. This is a new baseline for object recognition without representation learning methods, that initiates the investigation of convolutional kernel models on ImageNet. We conduct experiments to analyze the dictionary that we used, our ablations showing they exhibit low-dimensional properties.

</details>

<details>

<summary>2021-01-20 10:13:57 - Variational Autoencoders with a Structural Similarity Loss in Time of Flight MRAs</summary>

- *Kimberley M. Timmins, Irene C. van der Schaaf, Ynte M. Ruigrok, Birgitta K. Velthuis, Hugo J. Kuijf*

- `2101.08052v1` - [abs](http://arxiv.org/abs/2101.08052v1) - [pdf](http://arxiv.org/pdf/2101.08052v1)

> Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) enable visualization and analysis of cerebral arteries. This analysis may indicate normal variation of the configuration of the cerebrovascular system or vessel abnormalities, such as aneurysms. A model would be useful to represent normal cerebrovascular structure and variabilities in a healthy population and to differentiate from abnormalities. Current anomaly detection using autoencoding convolutional neural networks usually use a voxelwise mean-error for optimization. We propose optimizing a variational-autoencoder (VAE) with structural similarity loss (SSIM) for TOF-MRA reconstruction. A patch-trained 2D fully-convolutional VAE was optimized for TOF-MRA reconstruction by comparing vessel segmentations of original and reconstructed MRAs. The method was trained and tested on two datasets: the IXI dataset, and a subset from the ADAM challenge. Both trained networks were tested on a dataset including subjects with aneurysms. We compared VAE optimization with L2-loss and SSIM-loss. Performance was evaluated between original and reconstructed MRAs using mean square error, mean-SSIM, peak-signal-to-noise-ratio and dice similarity index (DSI) of segmented vessels. The L2-optimized VAE outperforms SSIM, with improved reconstruction metrics and DSIs for both datasets. Optimization using SSIM performed best for visual image quality, but with discrepancy in quantitative reconstruction and vascular segmentation. The larger, more diverse IXI dataset had overall better performance. Reconstruction metrics, including SSIM, were lower for MRAs including aneurysms. A SSIM-optimized VAE improved the visual perceptive image quality of TOF-MRA reconstructions. A L2-optimized VAE performed best for TOF-MRA reconstruction, where the vascular segmentation is important. SSIM is a potential metric for anomaly detection of MRAs.

</details>

<details>

<summary>2021-01-20 10:14:41 - Fast formation and assembly of isogeometric Galerkin matrices for trimmed patches</summary>

- *Benjamin Marussig*

- `2101.08053v1` - [abs](http://arxiv.org/abs/2101.08053v1) - [pdf](http://arxiv.org/pdf/2101.08053v1)

> This work explores the application of the fast assembly and formation strategy from [8, 17] to trimmed bi-variate parameter spaces. Two concepts for the treatment of basis functions cut by the trimming curve are investigated: one employs a hybrid Gauss-point-based approach, and the other computes discontinuous weighted quadrature rules. The concepts' accuracy and efficiency are examined for the formation of mass matrices and their application to L2-projection. Significant speed-ups compared to standard element by element finite element formation are observed. There is no clear preference between the concepts proposed. While the discontinuous weighted scheme scales favorably with the degree of the basis, it also requires additional effort for computing the quadrature weights. The hybrid Gauss approach does not have this overhead, which is determined by the complexity of the trimming curve. Hence, it is well-suited for moderate degrees, whereas discontinuous-weightedquadrature has potential for high degrees, in particular, if the related weights are computed in parallel.

</details>

<details>

<summary>2021-01-20 23:52:11 - Adversarial Network Traffic: Towards Evaluating the Robustness of Deep Learning-Based Network Traffic Classification</summary>

- *Amir Mahdi Sadeghzadeh, Saeed Shiravi, Rasool Jalili*

- `2003.01261v4` - [abs](http://arxiv.org/abs/2003.01261v4) - [pdf](http://arxiv.org/pdf/2003.01261v4)

> Network traffic classification is used in various applications such as network traffic management, policy enforcement, and intrusion detection systems. Although most applications encrypt their network traffic and some of them dynamically change their port numbers, Machine Learning (ML) and especially Deep Learning (DL)-based classifiers have shown impressive performance in network traffic classification. In this paper, we evaluate the robustness of DL-based network traffic classifiers against Adversarial Network Traffic (ANT). ANT causes DL-based network traffic classifiers to predict incorrectly using Universal Adversarial Perturbation (UAP) generating methods. Since there is no need to buffer network traffic before sending ANT, it is generated live. We partition the input space of the DL-based network traffic classification into three categories: packet classification, flow content classification, and flow time series classification. To generate ANT, we propose three new attacks injecting UAP into network traffic. AdvPad attack injects a UAP into the content of packets to evaluate the robustness of packet classifiers. AdvPay attack injects a UAP into the payload of a dummy packet to evaluate the robustness of flow content classifiers. AdvBurst attack injects a specific number of dummy packets with crafted statistical features based on a UAP into a selected burst of a flow to evaluate the robustness of flow time series classifiers. The results indicate injecting a little UAP into network traffic, highly decreases the performance of DL-based network traffic classifiers in all categories.

</details>

<details>

<summary>2021-01-21 19:10:43 - The Internet of Things in Ports: Six Key Security and Governance Challenges for the UK (Policy Brief)</summary>

- *Feja Lesniewska, Uchenna D Ani, Jeremy M Watson, Madeline Carr*

- `2101.08812v1` - [abs](http://arxiv.org/abs/2101.08812v1) - [pdf](http://arxiv.org/pdf/2101.08812v1)

> In January 2019, the UK Government published its Maritime 2050 on Navigating the Future strategy. In the strategy, the government highlighted the importance of digitalization (with well-designed regulatory support) to achieve its goal of ensuring that the UK plays a global leadership role in the maritime sector. Ports, the gateways for 95% of UK trade movements, were identified as key sites for investment in technological innovation. The government identified the potential of the Internet of Things (IoT), in conjunction with other information-sharing technologies, such as shared data platforms, and Artificial Intelligence applications (AI), to synchronize processes within the port ecosystem leading to improved efficiency, safety, and environmental benefits, including improved air quality and lower greenhouse gas emissions.

</details>

<details>

<summary>2021-01-22 18:11:34 - Automatic Cerebral Vessel Extraction in TOF-MRA Using Deep Learning</summary>

- *V. de Vos, K. M. Timmins, I. C. van der Schaaf, Y. Ruigrok, B. K. Velthuis, H. J. Kuijf*

- `2101.09253v1` - [abs](http://arxiv.org/abs/2101.09253v1) - [pdf](http://arxiv.org/pdf/2101.09253v1)

> Deep learning approaches may help radiologists in the early diagnosis and timely treatment of cerebrovascular diseases. Accurate cerebral vessel segmentation of Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) is an essential step in this process. This study investigates deep learning approaches for automatic, fast and accurate cerebrovascular segmentation for TOF-MRAs. The performance of several data augmentation and selection methods for training a 2D and 3D U-Net for vessel segmentation was investigated in five experiments: a) without augmentation, b) Gaussian blur, c) rotation and flipping, d) Gaussian blur, rotation and flipping and e) different input patch sizes. All experiments were performed by patch-training both a 2D and 3D U-Net and predicted on a test set of MRAs. Ground truth was manually defined using an interactive threshold and region growing method. The performance was evaluated using the Dice Similarity Coefficient (DSC), Modified Hausdorff Distance and Volumetric Similarity, between the predicted images and the interactively defined ground truth. The segmentation performance of all trained networks on the test set was found to be good, with DSC scores ranging from 0.72 to 0.83. Both the 2D and 3D U-Net had the best segmentation performance with Gaussian blur, rotation and flipping compared to other experiments without augmentation or only one of those augmentation techniques. Additionally, training on larger patches or slices gave optimal segmentation results. In conclusion, vessel segmentation can be optimally performed on TOF-MRAs using a trained 3D U-Net on larger patches, where data augmentation including Gaussian blur, rotation and flipping was performed on the training data.

</details>

<details>

<summary>2021-01-23 08:07:20 - Improved Training of Sparse Coding Variational Autoencoder via Weight Normalization</summary>

- *Linxing Preston Jiang, Luciano de la Iglesia*

- `2101.09453v1` - [abs](http://arxiv.org/abs/2101.09453v1) - [pdf](http://arxiv.org/pdf/2101.09453v1)

> Learning a generative model of visual information with sparse and compositional features has been a challenge for both theoretical neuroscience and machine learning communities. Sparse coding models have achieved great success in explaining the receptive fields of mammalian primary visual cortex with sparsely activated latent representation. In this paper, we focus on a recently proposed model, sparse coding variational autoencoder (SVAE) (Barello et al., 2018), and show that the end-to-end training scheme of SVAE leads to a large group of decoding filters not fully optimized with noise-like receptive fields. We propose a few heuristics to improve the training of SVAE and show that a unit $L_2$ norm constraint on the decoder is critical to produce sparse coding filters. Such normalization can be considered as local lateral inhibition in the cortex. We verify this claim empirically on both natural image patches and MNIST dataset and show that projection of the filters onto unit norm drastically increases the number of active filters. Our results highlight the importance of weight normalization for learning sparse representation from data and suggest a new way of reducing the number of inactive latent components in VAE learning.

</details>

<details>

<summary>2021-01-23 14:52:50 - Theory-guided hard constraint projection (HCP): a knowledge-based data-driven scientific machine learning method</summary>

- *Yuntian Chen, Dou Huang, Dongxiao Zhang, Junsheng Zeng, Nanzhe Wang, Haoran Zhang, Jinyue Yan*

- `2012.06148v2` - [abs](http://arxiv.org/abs/2012.06148v2) - [pdf](http://arxiv.org/pdf/2012.06148v2)

> Machine learning models have been successfully used in many scientific and engineering fields. However, it remains difficult for a model to simultaneously utilize domain knowledge and experimental observation data. The application of knowledge-based symbolic AI represented by an expert system is limited by the expressive ability of the model, and data-driven connectionism AI represented by neural networks is prone to produce predictions that violate physical mechanisms. In order to fully integrate domain knowledge with observations, and make full use of the prior information and the strong fitting ability of neural networks, this study proposes theory-guided hard constraint projection (HCP). This model converts physical constraints, such as governing equations, into a form that is easy to handle through discretization, and then implements hard constraint optimization through projection. Based on rigorous mathematical proofs, theory-guided HCP can ensure that model predictions strictly conform to physical mechanisms in the constraint patch. The performance of the theory-guided HCP is verified by experiments based on the heterogeneous subsurface flow problem. Due to the application of hard constraints, compared with fully connected neural networks and soft constraint models, such as theory-guided neural networks and physics-informed neural networks, theory-guided HCP requires fewer data, and achieves higher prediction accuracy and stronger robustness to noisy observations.

</details>

<details>

<summary>2021-01-27 08:54:23 - TorchPRISM: Principal Image Sections Mapping, a novel method for Convolutional Neural Network features visualization</summary>

- *Tomasz Szandala*

- `2101.11266v1` - [abs](http://arxiv.org/abs/2101.11266v1) - [pdf](http://arxiv.org/pdf/2101.11266v1)

> In this paper we introduce a tool called Principal Image Sections Mapping - PRISM, dedicated for PyTorch, but can be easily ported to other deep learning frameworks. Presented software relies on Principal Component Analysis to visualize the most significant features recognized by a given Convolutional Neural Network. Moreover, it allows to display comparative set features between images processed in the same batch, therefore PRISM can be a method well synerging with technique Explanation by Example.

</details>

<details>

<summary>2021-01-29 07:21:33 - Spatiotemporal Dilated Convolution with Uncertain Matching for Video-based Crowd Estimation</summary>

- *Yu-Jen Ma, Hong-Han Shuai, Wen-Huang Cheng*

- `2101.12439v1` - [abs](http://arxiv.org/abs/2101.12439v1) - [pdf](http://arxiv.org/pdf/2101.12439v1)

> In this paper, we propose a novel SpatioTemporal convolutional Dense Network (STDNet) to address the video-based crowd counting problem, which contains the decomposition of 3D convolution and the 3D spatiotemporal dilated dense convolution to alleviate the rapid growth of the model size caused by the Conv3D layer. Moreover, since the dilated convolution extracts the multiscale features, we combine the dilated convolution with the channel attention block to enhance the feature representations. Due to the error that occurs from the difficulty of labeling crowds, especially for videos, imprecise or standard-inconsistent labels may lead to poor convergence for the model. To address this issue, we further propose a new patch-wise regression loss (PRL) to improve the original pixel-wise loss. Experimental results on three video-based benchmarks, i.e., the UCSD, Mall and WorldExpo'10 datasets, show that STDNet outperforms both image- and video-based state-of-the-art methods. The source codes are released at \url{https://github.com/STDNet/STDNet}.

</details>


## 2021-02

<details>

<summary>2021-02-03 06:06:03 - Multi-Instance Learning by Utilizing Structural Relationship among Instances</summary>

- *Yangling Ma, Zhouwang Yang*

- `2102.01889v1` - [abs](http://arxiv.org/abs/2102.01889v1) - [pdf](http://arxiv.org/pdf/2102.01889v1)

> Multi-Instance Learning(MIL) aims to learn the mapping between a bag of instances and the bag-level label. Therefore, the relationships among instances are very important for learning the mapping. In this paper, we propose an MIL algorithm based on a graph built by structural relationship among instances within a bag. Then, Graph Convolutional Network(GCN) and the graph-attention mechanism are used to learn bag-embedding. In the task of medical image classification, our GCN-based MIL algorithm makes full use of the structural relationships among patches(instances) in an original image space domain, and experimental results verify that our method is more suitable for handling medical high-resolution images. We also verify experimentally that the proposed method achieves better results than previous methods on five bechmark MIL datasets and four medical image datasets.

</details>

<details>

<summary>2021-02-03 08:45:34 - All Infections are Not Created Equal: Time-Sensitive Prediction of Malware Generated Network Attacks</summary>

- *Zainab Abaid, Dilip Sarkar, Mohamed Ali Kaafar, Sanjay Jha*

- `2102.01944v1` - [abs](http://arxiv.org/abs/2102.01944v1) - [pdf](http://arxiv.org/pdf/2102.01944v1)

> Many techniques have been proposed for quickly detecting and containing malware-generated network attacks such as large-scale denial of service attacks; unfortunately, much damage is already done within the first few minutes of an attack, before it is identified and contained. There is a need for an early warning system that can predict attacks before they actually manifest, so that upcoming attacks can be prevented altogether by blocking the hosts that are likely to engage in attacks. However, blocking responses may disrupt legitimate processes on blocked hosts; in order to minimise user inconvenience, it is important to also foretell the time when the predicted attacks will occur, so that only the most urgent threats result in auto-blocking responses, while less urgent ones are first manually investigated. To this end, we identify a typical infection sequence followed by modern malware; modelling this sequence as a Markov chain and training it on real malicious traffic, we are able to identify behaviour most likely to lead to attacks and predict 98\% of real-world spamming and port-scanning attacks before they occur. Moreover, using a Semi-Markov chain model, we are able to foretell the time of upcoming attacks, a novel capability that allows accurately predicting the times of 97% of real-world malware attacks. Our work represents an important and timely step towards enabling flexible threat response models that minimise disruption to legitimate users.

</details>

<details>

<summary>2021-02-03 22:03:30 - Fuzzing Hardware Like Software</summary>

- *Timothy Trippel, Kang G. Shin, Alex Chernyakhovsky, Garret Kelly, Dominic Rizzo, Matthew Hicks*

- `2102.02308v1` - [abs](http://arxiv.org/abs/2102.02308v1) - [pdf](http://arxiv.org/pdf/2102.02308v1)

> Hardware flaws are permanent and potent: hardware cannot be patched once fabricated, and any flaws may undermine any software executing on top. Consequently, verification time dominates implementation time. The gold standard in hardware Design Verification (DV) is concentrated at two extremes: random dynamic verification and formal verification. Both struggle to root out the subtle flaws in complex hardware that often manifest as security vulnerabilities. The root problem with random verification is its undirected nature, making it inefficient, while formal verification is constrained by the state-space explosion problem, making it infeasible against complex designs. What is needed is a solution that is directed, yet under-constrained.   Instead of making incremental improvements to existing DV approaches, we leverage the observation that existing software fuzzers already provide such a solution, and adapt them for hardware DV. Specifically, we translate RTL hardware to a software model and fuzz that model. The central challenge we address is how best to mitigate the differences between the hardware execution model and software execution model. This includes: 1) how to represent test cases, 2) what is the hardware equivalent of a crash, 3) what is an appropriate coverage metric, and 4) how to create a general-purpose fuzzing harness for hardware.   To evaluate our approach, we fuzz four IP blocks from Google's OpenTitan SoC. Our experiments reveal a two orders-of-magnitude reduction in run time to achieve Finite State Machine (FSM) coverage over traditional dynamic verification schemes. Moreover, with our design-agnostic harness, we achieve over 88% HDL line coverage in three out of four of our designs -- even without any initial seeds.

</details>

<details>

<summary>2021-02-05 17:51:30 - Revisiting the Prepositional-Phrase Attachment Problem Using Explicit Commonsense Knowledge</summary>

- *Yida Xin, Henry Lieberman, Peter Chin*

- `2102.00924v2` - [abs](http://arxiv.org/abs/2102.00924v2) - [pdf](http://arxiv.org/pdf/2102.00924v2)

> We revisit the challenging problem of resolving prepositional-phrase (PP) attachment ambiguity. To date, proposed solutions are either rule-based, where explicit grammar rules direct how to resolve ambiguities; or statistical, where the decision is learned from a corpus of labeled examples. We argue that explicit commonsense knowledge bases can provide an essential ingredient for making good attachment decisions. We implemented a module, named Patch-Comm, that can be used by a variety of conventional parsers, to make attachment decisions. Where the commonsense KB does not provide direct answers, we fall back on a more general system that infers "out-of-knowledge-base" assertions in a manner similar to the way some NLP systems handle out-of-vocabulary words. Our results suggest that the commonsense knowledge-based approach can provide the best of both worlds, integrating rule-based and statistical techniques. As the field is increasingly coming to recognize the importance of explainability in AI, a commonsense approach can enable NLP developers to better understand the behavior of systems, and facilitate natural dialogues with end users.

</details>

<details>

<summary>2021-02-08 12:11:41 - Efficient Certified Defenses Against Patch Attacks on Image Classifiers</summary>

- *Jan Hendrik Metzen, Maksym Yatsura*

- `2102.04154v1` - [abs](http://arxiv.org/abs/2102.04154v1) - [pdf](http://arxiv.org/pdf/2102.04154v1)

> Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.

</details>

<details>

<summary>2021-02-08 15:52:09 - A Real-time Defense against Website Fingerprinting Attacks</summary>

- *Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao*

- `2102.04291v1` - [abs](http://arxiv.org/abs/2102.04291v1) - [pdf](http://arxiv.org/pdf/2102.04291v1)

> Anonymity systems like Tor are vulnerable to Website Fingerprinting (WF) attacks, where a local passive eavesdropper infers the victim's activity. Current WF attacks based on deep learning classifiers have successfully overcome numerous proposed defenses. While recent defenses leveraging adversarial examples offer promise, these adversarial examples can only be computed after the network session has concluded, thus offer users little protection in practical settings.   We propose Dolos, a system that modifies user network traffic in real time to successfully evade WF attacks. Dolos injects dummy packets into traffic traces by computing input-agnostic adversarial patches that disrupt deep learning classifiers used in WF attacks. Patches are then applied to alter and protect user traffic in real time. Importantly, these patches are parameterized by a user-side secret, ensuring that attackers cannot use adversarial training to defeat Dolos. We experimentally demonstrate that Dolos provides 94+% protection against state-of-the-art WF attacks under a variety of settings. Against prior defenses, Dolos outperforms in terms of higher protection performance and lower information leakage and bandwidth overhead. Finally, we show that Dolos is robust against a variety of adaptive countermeasures to detect or disrupt the defense.

</details>

<details>

<summary>2021-02-09 10:55:27 - Bayesian Transformer Language Models for Speech Recognition</summary>

- *Boyang Xue, Jianwei Yu, Junhao Xu, Shansong Liu, Shoukang Hu, Zi Ye, Mengzhe Geng, Xunying Liu, Helen Meng*

- `2102.04754v1` - [abs](http://arxiv.org/abs/2102.04754v1) - [pdf](http://arxiv.org/pdf/2102.04754v1)

> State-of-the-art neural language models (LMs) represented by Transformers are highly complex. Their use of fixed, deterministic parameter estimates fail to account for model uncertainty and lead to over-fitting and poor generalization when given limited training data. In order to address these issues, this paper proposes a full Bayesian learning framework for Transformer LM estimation. Efficient variational inference based approaches are used to estimate the latent parameter posterior distributions associated with different parts of the Transformer model architecture including multi-head self-attention, feed forward and embedding layers. Statistically significant word error rate (WER) reductions up to 0.5\% absolute (3.18\% relative) and consistent perplexity gains were obtained over the baseline Transformer LMs on state-of-the-art Switchboard corpus trained LF-MMI factored TDNN systems with i-Vector speaker adaptation. Performance improvements were also obtained on a cross domain LM adaptation task requiring porting a Transformer LM trained on the Switchboard and Fisher data to a low-resource DementiaBank elderly speech corpus.

</details>

<details>

<summary>2021-02-10 09:23:19 - UniToPatho, a labeled histopathological dataset for colorectal polyps classification and adenoma dysplasia grading</summary>

- *Carlo Alberto Barbano, Daniele Perlo, Enzo Tartaglione, Attilio Fiandrotti, Luca Bertero, Paola Cassoni, Marco Grangetto*

- `2101.09991v2` - [abs](http://arxiv.org/abs/2101.09991v2) - [pdf](http://arxiv.org/pdf/2101.09991v2)

> Histopathological characterization of colorectal polyps allows to tailor patients' management and follow up with the ultimate aim of avoiding or promptly detecting an invasive carcinoma. Colorectal polyps characterization relies on the histological analysis of tissue samples to determine the polyps malignancy and dysplasia grade. Deep neural networks achieve outstanding accuracy in medical patterns recognition, however they require large sets of annotated training images. We introduce UniToPatho, an annotated dataset of 9536 hematoxylin and eosin (H&E) stained patches extracted from 292 whole-slide images, meant for training deep neural networks for colorectal polyps classification and adenomas grading. We present our dataset and provide insights on how to tackle the problem of automatic colorectal polyps characterization.

</details>

<details>

<summary>2021-02-10 14:12:48 - Reference-based Texture transfer for Single Image Super-resolution of Magnetic Resonance images</summary>

- *Madhu Mithra K K, Sriprabha Ramanarayanan, Keerthi Ram, Mohanasankar Sivaprakasam*

- `2102.05450v1` - [abs](http://arxiv.org/abs/2102.05450v1) - [pdf](http://arxiv.org/pdf/2102.05450v1)

> Magnetic Resonance Imaging (MRI) is a valuable clinical diagnostic modality for spine pathologies with excellent characterization for infection, tumor, degenerations, fractures and herniations. However in surgery, image-guided spinal procedures continue to rely on CT and fluoroscopy, as MRI slice resolutions are typically insufficient. Building upon state-of-the-art single image super-resolution, we propose a reference-based, unpaired multi-contrast texture-transfer strategy for deep learning based in-plane and across-plane MRI super-resolution. We use the scattering transform to relate the texture features of image patches to unpaired reference image patches, and additionally a loss term for multi-contrast texture. We apply our scheme in different super-resolution architectures, observing improvement in PSNR and SSIM for 4x super-resolution in most of the cases.

</details>

<details>

<summary>2021-02-11 10:08:09 - PatchX: Explaining Deep Models by Intelligible Pattern Patches for Time-series Classification</summary>

- *Dominique Mercier, Andreas Dengel, Sheraz Ahmed*

- `2102.05917v1` - [abs](http://arxiv.org/abs/2102.05917v1) - [pdf](http://arxiv.org/pdf/2102.05917v1)

> The classification of time-series data is pivotal for streaming data and comes with many challenges. Although the amount of publicly available datasets increases rapidly, deep neural models are only exploited in a few areas. Traditional methods are still used very often compared to deep neural models. These methods get preferred in safety-critical, financial, or medical fields because of their interpretable results. However, their performance and scale-ability are limited, and finding suitable explanations for time-series classification tasks is challenging due to the concepts hidden in the numerical time-series data. Visualizing complete time-series results in a cognitive overload concerning our perception and leads to confusion. Therefore, we believe that patch-wise processing of the data results in a more interpretable representation. We propose a novel hybrid approach that utilizes deep neural networks and traditional machine learning algorithms to introduce an interpretable and scale-able time-series classification approach. Our method first performs a fine-grained classification for the patches followed by sample level classification.

</details>

<details>

<summary>2021-02-11 17:55:08 - Learning local regularization for variational image restoration</summary>

- *Jean Prost, Antoine Houdard, Andrés Almansa, Nicolas Papadakis*

- `2102.06155v1` - [abs](http://arxiv.org/abs/2102.06155v1) - [pdf](http://arxiv.org/pdf/2102.06155v1)

> In this work, we propose a framework to learn a local regularization model for solving general image restoration problems. This regularizer is defined with a fully convolutional neural network that sees the image through a receptive field corresponding to small image patches. The regularizer is then learned as a critic between unpaired distributions of clean and degraded patches using a Wasserstein generative adversarial networks based energy. This yields a regularization function that can be incorporated in any image restoration problem. The efficiency of the framework is finally shown on denoising and deblurring applications.

</details>

<details>

<summary>2021-02-16 14:49:34 - Automated Identification of Vulnerable Devices in Networks using Traffic Data and Deep Learning</summary>

- *Jakob Greis, Artem Yushchenko, Daniel Vogel, Michael Meier, Volker Steinhage*

- `2102.08199v1` - [abs](http://arxiv.org/abs/2102.08199v1) - [pdf](http://arxiv.org/pdf/2102.08199v1)

> Many IoT devices are vulnerable to attacks due to flawed security designs and lacking mechanisms for firmware updates or patches to eliminate the security vulnerabilities. Device-type identification combined with data from vulnerability databases can pinpoint vulnerable IoT devices in a network and can be used to constrain the communications of vulnerable devices for preventing damage. In this contribution, we present and evaluate two deep learning approaches to the reliable IoT device-type identification, namely a recurrent and a convolutional network architecture. Both deep learning approaches show accuracies of 97% and 98%, respectively, and thereby outperform an up-to-date IoT device-type identification approach using hand-crafted fingerprint features obtaining an accuracy of 82%. The runtime performance for the IoT identification of both deep learning approaches outperforms the hand-crafted approach by three magnitudes. Finally, importance metrics explain the results of both deep learning approaches in terms of the utilization of the analyzed traffic data flow.

</details>

<details>

<summary>2021-02-17 23:52:57 - Grid Cell Path Integration For Movement-Based Visual Object Recognition</summary>

- *Niels Leadholm, Marcus Lewis, Subutai Ahmad*

- `2102.09076v1` - [abs](http://arxiv.org/abs/2102.09076v1) - [pdf](http://arxiv.org/pdf/2102.09076v1)

> Grid cells enable the brain to model the physical space of the world and navigate effectively via path integration, updating self-position using information from self-movement. Recent proposals suggest that the brain might use similar mechanisms to understand the structure of objects in diverse sensory modalities, including vision. In machine vision, object recognition given a sequence of sensory samples of an image, such as saccades, is a challenging problem when the sequence does not follow a consistent, fixed pattern - yet this is something humans do naturally and effortlessly. We explore how grid cell-based path integration in a cortical network can support reliable recognition of objects given an arbitrary sequence of inputs. Our network (GridCellNet) uses grid cell computations to integrate visual information and make predictions based on movements. We use local Hebbian plasticity rules to learn rapidly from a handful of examples (few-shot learning), and consider the task of recognizing MNIST digits given only a sequence of image feature patches. We compare GridCellNet to k-Nearest Neighbour (k-NN) classifiers as well as recurrent neural networks (RNNs), both of which lack explicit mechanisms for handling arbitrary sequences of input samples. We show that GridCellNet can reliably perform classification, generalizing to both unseen examples and completely novel sequence trajectories. We further show that inference is often successful after sampling a fraction of the input space, enabling the predictive GridCellNet to reconstruct the rest of the image given just a few movements. We propose that dynamically moving agents with active sensors can use grid cell representations not only for navigation, but also for efficient recognition and feature prediction of seen objects.

</details>

<details>

<summary>2021-02-18 17:10:47 - Assessing the Use of Insecure ICS Protocols via IXP Network Traffic Analysis</summary>

- *Giovanni Barbieri, Mauro Conti, Nils Ole Tippenhauer, Federico Turrin*

- `2007.01114v2` - [abs](http://arxiv.org/abs/2007.01114v2) - [pdf](http://arxiv.org/pdf/2007.01114v2)

> Modern Industrial Control Systems (ICSs) allow remote communication through the Internet using industrial protocols that were not designed to work with external networks. To understand security issues related to this practice, prior work usually relies on active scans by researchers or services such as Shodan. While such scans can identify publicly open ports, they cannot identify legitimate use of insecure industrial traffic. In particular, source-based filtering in Network Address Translation or Firewalls prevent detection by active scanning, but do not ensure that insecure communication is not manipulated in transit. In this work, we compare Shodan-only analysis with large-scale traffic analysis at a local Internet Exchange Point (IXP), based on sFlow sampling. This setup allows us to identify ICS endpoints actually exchanging industrial traffic over the Internet. Besides, we are able to detect scanning activities and what other type of traffic is exchanged by the systems (i.e., IT traffic). We find that Shodan only listed less than 2% of hosts that we identified as exchanging industrial traffic, and only 7% of hosts identified by Shodan actually exchange industrial traffic. Therefore, Shodan do not allow to understand the actual use of insecure industrial protocols on the Internet and the current security practices in ICS communications. We show that 75.6% of ICS hosts still rely on unencrypted communications without integrity protection, leaving those critical systems vulnerable to malicious attacks.

</details>

<details>

<summary>2021-02-19 21:36:47 - Spatial-temporal switching estimators for imaging locally concentrated dynamics</summary>

- *Parisa Karimi, Mark Butala, Zhizhen Zhao, Farzad Kamalabadi*

- `2102.10167v1` - [abs](http://arxiv.org/abs/2102.10167v1) - [pdf](http://arxiv.org/pdf/2102.10167v1)

> The evolution of images with physics-based dynamics is often spatially localized and nonlinear. A switching linear dynamic system (SLDS) is a natural model under which to pose such problems when the system's evolution randomly switches over the observation interval. Because of the high parameter space dimensionality, efficient and accurate recovery of the underlying state is challenging. The work presented in this paper focuses on the common cases where the dynamic evolution may be adequately modeled as a collection of decoupled, locally concentrated dynamic operators. Patch-based hybrid estimators are proposed for real-time reconstruction of images from noisy measurements given perfect or partial information about the underlying system dynamics. Numerical results demonstrate the effectiveness of the proposed approach for denoising in a realistic data-driven simulation of remotely sensed cloud dynamics.

</details>

<details>

<summary>2021-02-22 18:14:48 - Patterns of Routes of Administration and Drug Tampering for Nonmedical Opioid Consumption: Data Mining and Content Analysis of Reddit Discussions</summary>

- *Duilio Balsamo, Paolo Bajardi, Alberto Salomone, Rossano Schifanella*

- `2102.11235v1` - [abs](http://arxiv.org/abs/2102.11235v1) - [pdf](http://arxiv.org/pdf/2102.11235v1)

> The complex unfolding of the US opioid epidemic in the last 20 years has been the subject of a large body of medical and pharmacological research, and it has sparked a multidisciplinary discussion on how to implement interventions and policies to effectively control its impact on public health. This study leverages Reddit as the primary data source to investigate the opioid crisis. We aimed to find a large cohort of Reddit users interested in discussing the use of opioids, trace the temporal evolution of their interest, and extensively characterize patterns of the nonmedical consumption of opioids, with a focus on routes of administration and drug tampering. We used a semiautomatic information retrieval algorithm to identify subreddits discussing nonmedical opioid consumption, finding over 86,000 Reddit users potentially involved in firsthand opioid usage. We developed a methodology based on word embedding to select alternative colloquial and nonmedical terms referring to opioid substances, routes of administration, and drug-tampering methods. We modeled the preferences of adoption of substances and routes of administration, estimating their prevalence and temporal unfolding, observing relevant trends such as the surge in synthetic opioids like fentanyl and an increasing interest in rectal administration. Ultimately, through the evaluation of odds ratios based on co-mentions, we measured the strength of association between opioid substances, routes of administration, and drug tampering, finding evidence of understudied abusive behaviors like chewing fentanyl patches and dissolving buprenorphine sublingually. We believe that our approach may provide a novel perspective for a more comprehensive understanding of nonmedical abuse of opioids substances and inform the prevention, treatment, and control of the public health effects.

</details>

<details>

<summary>2021-02-22 20:46:02 - Individualized Context-Aware Tensor Factorization for Online Games Predictions</summary>

- *Julie Jiang, Kristina Lerman, Emilio Ferrara*

- `2102.11352v1` - [abs](http://arxiv.org/abs/2102.11352v1) - [pdf](http://arxiv.org/pdf/2102.11352v1)

> Individual behavior and decisions are substantially influenced by their contexts, such as location, environment, and time. Changes along these dimensions can be readily observed in Multiplayer Online Battle Arena games (MOBA), where players face different in-game settings for each match and are subject to frequent game patches. Existing methods utilizing contextual information generalize the effect of a context over the entire population, but contextual information tailored to each individual can be more effective. To achieve this, we present the Neural Individualized Context-aware Embeddings (NICE) model for predicting user performance and game outcomes. Our proposed method identifies individual behavioral differences in different contexts by learning latent representations of users and contexts through non-negative tensor factorization. Using a dataset from the MOBA game League of Legends, we demonstrate that our model substantially improves the prediction of winning outcome, individual user performance, and user engagement.

</details>

<details>

<summary>2021-02-24 17:52:11 - Integrated Reasoning Engine for Pointer-related Code Clone Detection</summary>

- *Hongfa Xue, Yongsheng Mei, Kailash Gogineni, Guru Venkataramani, Tian Lan*

- `2105.11933v1` - [abs](http://arxiv.org/abs/2105.11933v1) - [pdf](http://arxiv.org/pdf/2105.11933v1)

> Detecting similar code fragments, usually referred to as code clones, is an important task. In particular, code clone detection can have significant uses in the context of vulnerability discovery, refactoring and plagiarism detection. However, false positives are inevitable and always require manual reviews. In this paper, we propose Twin-Finder+, a novel closed-loop approach for pointer-related code clone detection that integrates machine learning and symbolic execution techniques to achieve precision. Twin-Finder+ introduces a formal verification mechanism to automate such manual reviews process. Our experimental results show Twin-Finder+ that can remove 91.69% false positives in average. We further conduct security analysis for memory safety using real-world applications, Links version 2.14 and libreOffice-6.0.0.1. Twin-Finder+ is able to find 6 unreported bugs in Links version 2.14 and one public patched bug in libreOffice-6.0.0.1.

</details>

<details>

<summary>2021-02-27 08:30:06 - Extracting Concise Bug-Fixing Patches from Human-Written Patches in Version Control Systems</summary>

- *Yanjie Jiang, Hui Liu, Nan Niu, Lu Zhang, Yamin Hu*

- `2103.00156v1` - [abs](http://arxiv.org/abs/2103.00156v1) - [pdf](http://arxiv.org/pdf/2103.00156v1)

> High-quality and large-scale repositories of real bugs and their concise patches collected from real-world applications are critical for research in software engineering community. In such a repository, each real bug is explicitly associated with its fix. Therefore, on one side, the real bugs and their fixes} may inspire novel approaches for finding, locating, and repairing software bugs; on the other side, the real bugs and their fixes are indispensable for rigorous and meaningful evaluation of approaches for software testing, fault localization, and program repair. To this end, a number of such repositories, e.g., Defects4J, have been proposed. However, such repositories are rather small because their construction involves expensive human intervention. Although bug-fixing code commits as well as associated test cases could be retrieved from version control systems automatically, existing approaches could not yet automatically extract concise bug-fixing patches from bug-fixing commits because such commits often involve bug-irrelevant changes. In this paper, we propose an automatic approach, called BugBuilder, to extracting complete and concise bug-fixing patches from human-written patches in version control systems. It excludes refactorings by detecting refactorings involved in bug-fixing commits, and reapplying detected refactorings on the faulty version. It enumerates all subsets of the remaining part and validates them on test cases. If none of the subsets has the potential to be a complete bug-fixing patch, the remaining part as a whole is taken as a complete and concise bug-fixing patch. Evaluation results on 809 real bug-fixing commits in Defects4J suggest that BugBuilder successfully generated complete and concise bug-fixing patches for forty percent of the bug-fixing commits, and its precision (99%) was even higher than human experts.

</details>

<details>

<summary>2021-02-27 20:47:27 - Machine Learning Techniques to Construct Patched Analog Ensembles for Data Assimilation</summary>

- *Lucia Minah Yang, Ian Grooms*

- `2103.00318v1` - [abs](http://arxiv.org/abs/2103.00318v1) - [pdf](http://arxiv.org/pdf/2103.00318v1)

> Using generative models from the machine learning literature to create artificial ensemble members for use within data assimilation schemes has been introduced in [Grooms QJRMS, 2020] as constructed analog ensemble optimal interpolation (cAnEnOI). Specifically, we study general and variational autoencoders for the machine learning component of this method, and combine the ideas of constructed analogs and ensemble optimal interpolation in the data assimilation piece. To extend the scalability of cAnEnOI for use in data assimilation on complex dynamical models, we propose using patching schemes to divide the global spatial domain into digestible chunks. Using patches makes training the generative models possible and has the added benefit of being able to exploit parallelism during the generative step. Testing this new algorithm on a 1D toy model, we find that larger patch sizes make it harder to train an accurate generative model (i.e. a model whose reconstruction error is small), while conversely the data assimilation performance improves at larger patch sizes. There is thus a sweet spot where the patch size is large enough to enable good data assimilation performance, but not so large that it becomes difficult to train an accurate generative model. In our tests the new patched cAnEnOI method outperforms the original (unpatched) cAnEnOI, as well as the ensemble square root filter results from [Grooms QJRMS, 2020].

</details>


## 2021-03

<details>

<summary>2021-03-01 14:36:07 - Predictive Maintenance Tool for Non-Intrusive Inspection Systems</summary>

- *Georgi Nalbantov, Dimitar Todorov, Nikolay Zografov, Stefan Georgiev, Nadia Bojilova*

- `2103.01044v1` - [abs](http://arxiv.org/abs/2103.01044v1) - [pdf](http://arxiv.org/pdf/2103.01044v1)

> Cross-border security is of topmost priority for societies. Economies lose billions each year due to counterfeiters and other threats. Security checkpoints equipped with X-ray Security Systems (NIIS-Non-Intrusive Inspection Systems) like airports, ports, border control and customs authorities tackle the myriad of threats by using NIIS to inspect bags, air, land, sea and rail cargo, and vehicles. The reliance on the X-ray scanning systems necessitates their continuous 24/7 functioning being provided for. Hence the need for their working condition being closely monitored and preemptive actions being taken to reduce the overall X-ray systems downtime. In this paper, we present a predictive maintenance decision support system, abbreviated as PMT4NIIS (Predictive Maintenance Tool for Non-Intrusive Inspection Systems), which is a kind of augmented analytics platforms that provides real-time AI-generated warnings for upcoming risk of system malfunctioning leading to possible downtime. The industrial platform is the basis of a 24/7 Service Desk and Monitoring center for the working condition of various X-ray Security Systems.

</details>

<details>

<summary>2021-03-01 16:14:28 - Histo-fetch -- On-the-fly processing of gigapixel whole slide images simplifies and speeds neural network training</summary>

- *Brendon Lutnick, Leema Krishna Murali, Brandon Ginley, Avi Z. Rosenberg, Pinaki Sarder*

- `2102.11433v2` - [abs](http://arxiv.org/abs/2102.11433v2) - [pdf](http://arxiv.org/pdf/2102.11433v2)

> We created a custom pipeline (histo-fetch) to efficiently extract random patches and labels from pathology whole slide images (WSIs) for input to a neural network on-the-fly. We prefetch these patches as needed during network training, avoiding the need for WSI preparation such as chopping/tiling. We demonstrate the utility of this pipeline to perform artificial stain transfer and image generation using the popular networks CycleGAN and ProGAN, respectively.

</details>

<details>

<summary>2021-03-03 03:19:26 - Too Quiet in the Library: An Empirical Study of Security Updates in Android Apps' Native Code</summary>

- *Sumaya Almanee, Arda Unal, Mathias Payer, Joshua Garcia*

- `1911.09716v2` - [abs](http://arxiv.org/abs/1911.09716v2) - [pdf](http://arxiv.org/pdf/1911.09716v2)

> Android apps include third-party native libraries to increase performance and to reuse functionality. Native code is directly executed from apps through the Java Native Interface or the Android Native Development Kit. Android developers add precompiled native libraries to their projects, enabling their use. Unfortunately, developers often struggle or simply neglect to update these libraries in a timely manner. This results in the continuous use of outdated native libraries with unpatched security vulnerabilities years after patches became available.   To further understand such phenomena, we study the security updates in native libraries in the most popular 200 free apps on Google Play from Sept. 2013 to May 2020. A core difficulty we face in this study is the identification of libraries and their versions. Developers often rename or modify libraries, making their identification challenging. We create an approach called LibRARIAN (LibRAry veRsion IdentificAtioN) that accurately identifies native libraries and their versions as found in Android apps based on our novel similarity metric bin2sim. LibRARIAN leverages different features extracted from libraries based on their metadata and identifying strings in read-only sections.   We discovered 53/200 popular apps (26.5%) with vulnerable versions with known CVEs between Sept. 2013 and May 2020, with 14 of those apps remaining vulnerable. We find that app developers took, on average, 528.71 days to apply security patches, while library developers release a security patch after 54.59 days - a 10 times slower rate of update.

</details>

<details>

<summary>2021-03-03 03:40:24 - Sensing population distribution from satellite imagery via deep learning: model selection, neighboring effect, and systematic biases</summary>

- *Xiao Huang, Di Zhu, Fan Zhang, Tao Liu, Xiao Li, Lei Zou*

- `2103.02155v1` - [abs](http://arxiv.org/abs/2103.02155v1) - [pdf](http://arxiv.org/pdf/2103.02155v1)

> The rapid development of remote sensing techniques provides rich, large-coverage, and high-temporal information of the ground, which can be coupled with the emerging deep learning approaches that enable latent features and hidden geographical patterns to be extracted. This study marks the first attempt to cross-compare performances of popular state-of-the-art deep learning models in estimating population distribution from remote sensing images, investigate the contribution of neighboring effect, and explore the potential systematic population estimation biases. We conduct an end-to-end training of four popular deep learning architectures, i.e., VGG, ResNet, Xception, and DenseNet, by establishing a mapping between Sentinel-2 image patches and their corresponding population count from the LandScan population grid. The results reveal that DenseNet outperforms the other three models, while VGG has the worst performances in all evaluating metrics under all selected neighboring scenarios. As for the neighboring effect, contradicting existing studies, our results suggest that the increase of neighboring sizes leads to reduced population estimation performance, which is found universal for all four selected models in all evaluating metrics. In addition, there exists a notable, universal bias that all selected deep learning models tend to overestimate sparsely populated image patches and underestimate densely populated image patches, regardless of neighboring sizes. The methodological, experimental, and contextual knowledge this study provides is expected to benefit a wide range of future studies that estimate population distribution via remote sensing imagery.

</details>

<details>

<summary>2021-03-03 03:46:31 - Automatic Detection and Resolution of Software Merge Conflicts: Are We There Yet?</summary>

- *Bowen Shen, Cihan Xiao, Na Meng, Fei He*

- `2102.11307v2` - [abs](http://arxiv.org/abs/2102.11307v2) - [pdf](http://arxiv.org/pdf/2102.11307v2)

> Developers create software branches for tentative feature addition and bug fixing, and periodically merge branches to release software with new features or repairing patches. When the program edits from different branches textually overlap (i.e., textual conflicts), or the co-application of those edits lead to compilation or runtime errors (i.e., compiling or dynamic conflicts), it is challenging and time-consuming for developers to eliminate merge conflicts. Prior studies examined %the popularity of merge conflicts and how conflicts were related to code smells or software development process; tools were built to find and solve conflicts.   However, some fundamental research questions are still not comprehensively explored, including (1) how conflicts were introduced, (2) how developers manually resolved conflicts, and (3) what conflicts cannot be handled by current tools.   For this paper, we took a hybrid approach that combines automatic detection with manual inspection to reveal 204 merge conflicts and their resolutions in 15 open-source repositories. %in the version history of 15 open-source projects. Our data analysis reveals three phenomena. First, compiling and dynamic conflicts are harder to detect, although current tools mainly focus on textual conflicts. Second, in the same merging context, developers usually resolved similar textual conflicts with similar strategies. Third, developers manually fixed most of the inspected compiling and dynamic conflicts by similarly editing the merged version as what they did for one of the branches. Our research reveals the challenges and opportunities for automatic detection and resolution of merge conflicts; it also sheds light on related areas like systematic program editing and change recommendation.

</details>

<details>

<summary>2021-03-03 18:43:09 - Shipwright: A Human-in-the-Loop System for Dockerfile Repair</summary>

- *Jordan Henkel, Denini Silva, Leopoldo Teixeira, Marcelo d'Amorim, Thomas Reps*

- `2103.02591v1` - [abs](http://arxiv.org/abs/2103.02591v1) - [pdf](http://arxiv.org/pdf/2103.02591v1)

> Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and -- to our great surprise -- found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6--33.8% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25% of the files and, additionally, provide automated repairs for 18.9% of the files.

</details>

<details>

<summary>2021-03-04 23:01:22 - DeepLocalize: Fault Localization for Deep Neural Networks</summary>

- *Mohammad Wardat, Wei Le, Hridesh Rajan*

- `2103.03376v1` - [abs](http://arxiv.org/abs/2103.03376v1) - [pdf](http://arxiv.org/pdf/2103.03376v1)

> Deep neural networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques do not support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach that automatically determines whether the model is buggy or not, and identifies the root causes. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state of the practice Keras library. For 34 out of 40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32 out of 40 faults. Our approach was able to localize 21 out of 40 bugs whereas Keras did not localize any faults.

</details>

<details>

<summary>2021-03-05 08:39:47 - Extend the FFmpeg Framework to Analyze Media Content</summary>

- *Xintian Wu, Pengfei Qu, Shaofei Wang, Lin Xie, Jie Dong*

- `2103.03539v1` - [abs](http://arxiv.org/abs/2103.03539v1) - [pdf](http://arxiv.org/pdf/2103.03539v1)

> This paper introduces a new set of video analytics plugins developed for the FFmpeg framework. Multimedia applications that increasingly utilize the FFmpeg media features for its comprehensive media encoding, decoding, muxing, and demuxing capabilities can now additionally analyze the video content based on AI models. The plugins are thread optimized for best performance overcoming certain FFmpeg threading limitations. The plugins utilize the Intel OpenVINO Toolkit inference engine as the backend. The analytics workloads are accelerated on different platforms such as CPU, GPU, FPGA or specialized analytics accelerators. With our reference implementation, the feature of OpenVINO as inference backend has been pushed into FFmpeg mainstream repository. We plan to submit more patches later.

</details>

<details>

<summary>2021-03-05 12:36:50 - Kernel Self-Attention in Deep Multiple Instance Learning</summary>

- *Dawid Rymarczyk, Adriana Borowa, Jacek Tabor, Bartosz Zieliński*

- `2005.12991v2` - [abs](http://arxiv.org/abs/2005.12991v2) - [pdf](http://arxiv.org/pdf/2005.12991v2)

> Not all supervised learning problems are described by a pair of a fixed-size input tensor and a label. In some cases, especially in medical image analysis, a label corresponds to a bag of instances (e.g. image patches), and to classify such bag, aggregation of information from all of the instances is needed. There have been several attempts to create a model working with a bag of instances, however, they are assuming that there are no dependencies within the bag and the label is connected to at least one instance. In this work, we introduce Self-Attention Attention-based MIL Pooling (SA-AbMILP) aggregation operation to account for the dependencies between instances. We conduct several experiments on MNIST, histological, microbiological, and retinal databases to show that SA-AbMILP performs better than other models. Additionally, we investigate kernel variations of Self-Attention and their influence on the results.

</details>

<details>

<summary>2021-03-06 15:05:14 - A Real-time Low-cost Artificial Intelligence System for Autonomous Spraying in Palm Plantations</summary>

- *Zhenwang Qin, Wensheng Wang, Karl-Heinz Dammer, Leifeng Guo, Zhen Cao*

- `2103.04132v1` - [abs](http://arxiv.org/abs/2103.04132v1) - [pdf](http://arxiv.org/pdf/2103.04132v1)

> In precision crop protection, (target-orientated) object detection in image processing can help navigate Unmanned Aerial Vehicles (UAV, crop protection drones) to the right place to apply the pesticide. Unnecessary application of non-target areas could be avoided. Deep learning algorithms dominantly use in modern computer vision tasks which require high computing time, memory footprint, and power consumption. Based on the Edge Artificial Intelligence, we investigate the main three paths that lead to dealing with this problem, including hardware accelerators, efficient algorithms, and model compression. Finally, we integrate them and propose a solution based on a light deep neural network (DNN), called Ag-YOLO, which can make the crop protection UAV have the ability to target detection and autonomous operation. This solution is restricted in size, cost, flexible, fast, and energy-effective. The hardware is only 18 grams in weight and 1.5 watts in energy consumption, and the developed DNN model needs only 838 kilobytes of disc space. We tested the developed hardware and software in comparison to the tiny version of the state-of-art YOLOv3 framework, known as YOLOv3-Tiny to detect individual palm in a plantation. An average F1 score of 0.9205 at the speed of 36.5 frames per second (in comparison to similar accuracy at 18 frames per second and 8.66 megabytes of the YOLOv3-Tiny algorithm) was reached. This developed detection system is easily plugged into any machines already purchased as long as the machines have USB ports and run Linux Operating System.

</details>

<details>

<summary>2021-03-07 04:33:13 - ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring</summary>

- *Dongxu Li, Chenchen Xu, Kaihao Zhang, Xin Yu, Yiran Zhong, Wenqi Ren, Hanna Suominen, Hongdong Li*

- `2103.04260v1` - [abs](http://arxiv.org/abs/2103.04260v1) - [pdf](http://arxiv.org/pdf/2103.04260v1)

> Video deblurring models exploit consecutive frames to remove blurs from camera shakes and object motions. In order to utilize neighboring sharp patches, typical methods rely mainly on homography or optical flows to spatially align neighboring blurry frames. However, such explicit approaches are less effective in the presence of fast motions with large pixel displacements. In this work, we propose a novel implicit method to learn spatial correspondence among blurry frames in the feature space. To construct distant pixel correspondences, our model builds a correlation volume pyramid among all the pixel-pairs between neighboring frames. To enhance the features of the reference frame, we design a correlative aggregation module that maximizes the pixel-pair correlations with its neighbors based on the volume pyramid. Finally, we feed the aggregated features into a reconstruction module to obtain the restored frame. We design a generative adversarial paradigm to optimize the model progressively. Our proposed method is evaluated on the widely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative experiments show that our model performs favorably on both datasets against previous state-of-the-art methods, confirming the benefit of modeling all-range spatial correspondence for video deblurring.

</details>

<details>

<summary>2021-03-07 20:53:33 - Comparative Analysis and Enhancement of CFG-based Hardware-Assisted CFI Schemes</summary>

- *Mario Telesklav, Stefan Tauner*

- `2103.04456v1` - [abs](http://arxiv.org/abs/2103.04456v1) - [pdf](http://arxiv.org/pdf/2103.04456v1)

> Subverting the flow of instructions (e.g., by use of code-reuse attacks) still poses a serious threat to the security of today's systems. Various control flow integrity (CFI) schemes have been proposed as a powerful technique to detect and mitigate such attacks. In recent years, many hardware-assisted implementations of CFI enforcement based on control flow graphs (CFGs) have been presented by academia. Such approaches check whether control flow transfers follow the intended CFG by limiting the valid target addresses. However, these papers all target different platforms and were evaluated with different sets of benchmark applications, which makes quantitative comparisons hardly possible.   For this paper, we have implemented multiple promising CFG-based CFI schemes on a common platform comprising a RISC-V SoC within an FPGA. By porting almost 40 benchmark applications to this system we can present a meaningful comparison of the various techniques in terms of run-time performance, hardware utilization, and binary size. In addition, we present an enhanced CFI approach that is inspired by what we consider the best concepts and ideas of previously proposed mechanisms. We have made this approach more practical and feature-complete by tackling some problems largely ignored previously. We show with this fine-grained scheme that CFI can be achieved with even less overheads than previously demonstrated.

</details>

<details>

<summary>2021-03-08 10:23:55 - Quasi-structured quadrilateral meshing in Gmsh -- a robust pipeline for complex CAD models</summary>

- *Maxence Reberol, Christos Georgiadis, Jean-François Remacle*

- `2103.04652v1` - [abs](http://arxiv.org/abs/2103.04652v1) - [pdf](http://arxiv.org/pdf/2103.04652v1)

> We propose an end-to-end pipeline to robustly generate high-quality quadrilateral meshes for complex CAD models. An initial quad-dominant mesh is generated with frontal point insertion guided by a locally integrable cross field and a scalar size map adapted to the small CAD features. After triangle combination and midpoint-subdivision into an all-quadrilateral mesh, the topology of the mesh is modified to reduce the number of irregular vertices. The idea is to preserve the irregular vertices matching cross-field singularities and to eliminate the others. The topological modifications are either local and based on disk quadrangulations, or more global with the remeshing of patches of quads according to predefined patterns. Validity of the quad mesh is guaranteed by monitoring element quality during all operations and reverting the changes when necessary. Advantages of our approach include robustness, strict respect of the CAD features and support for user-prescribed size constraints. The quad mesher, which is available in Gmsh, is validated and illustrated on two datasets of CAD models.

</details>

<details>

<summary>2021-03-09 05:08:01 - DeepSeagrass Dataset</summary>

- *Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett Kettle, Brano Kusy*

- `2103.05226v1` - [abs](http://arxiv.org/abs/2103.05226v1) - [pdf](http://arxiv.org/pdf/2103.05226v1)

> We introduce a dataset of seagrass images collected by a biologist snorkelling in Moreton Bay, Queensland, Australia, as described in our publication: arXiv:2009.09924. The images are labelled at the image-level by collecting images of the same morphotype in a folder hierarchy. We also release pre-trained models and training codes for detection and classification of seagrass species at the patch level at https://github.com/csiro-robotics/deepseagrass.

</details>

<details>

<summary>2021-03-09 07:09:42 - 2D histology meets 3D topology: Cytoarchitectonic brain mapping with Graph Neural Networks</summary>

- *Christian Schiffer, Stefan Harmeling, Katrin Amunts, Timo Dickscheid*

- `2103.05259v1` - [abs](http://arxiv.org/abs/2103.05259v1) - [pdf](http://arxiv.org/pdf/2103.05259v1)

> Cytoarchitecture describes the spatial organization of neuronal cells in the brain, including their arrangement into layers and columns with respect to cell density, orientation, or presence of certain cell types. It allows to segregate the brain into cortical areas and subcortical nuclei, links structure with connectivity and function, and provides a microstructural reference for human brain atlases. Mapping boundaries between areas requires to scan histological sections at microscopic resolution. While recent high-throughput scanners allow to scan a complete human brain in the order of a year, it is practically impossible to delineate regions at the same pace using the established gold standard method. Researchers have recently addressed cytoarchitectonic mapping of cortical regions with deep neural networks, relying on image patches from individual 2D sections for classification. However, the 3D context, which is needed to disambiguate complex or obliquely cut brain regions, is not taken into account. In this work, we combine 2D histology with 3D topology by reformulating the mapping task as a node classification problem on an approximate 3D midsurface mesh through the isocortex. We extract deep features from cortical patches in 2D histological sections which are descriptive of cytoarchitecture, and assign them to the corresponding nodes on the 3D mesh to construct a large attributed graph. By solving the brain mapping problem on this graph using graph neural networks, we obtain significantly improved classification results. The proposed framework lends itself nicely to integration of additional neuroanatomical priors for mapping.

</details>

<details>

<summary>2021-03-10 07:47:27 - Continual Learning in Recurrent Neural Networks</summary>

- *Benjamin Ehret, Christian Henning, Maria R. Cervera, Alexander Meulemans, Johannes von Oswald, Benjamin F. Grewe*

- `2006.12109v3` - [abs](http://arxiv.org/abs/2006.12109v3) - [pdf](http://arxiv.org/pdf/2006.12109v3)

> While a diverse collection of continual learning (CL) methods has been proposed to prevent catastrophic forgetting, a thorough investigation of their effectiveness for processing sequential data with recurrent neural networks (RNNs) is lacking. Here, we provide the first comprehensive evaluation of established CL methods on a variety of sequential data benchmarks. Specifically, we shed light on the particularities that arise when applying weight-importance methods, such as elastic weight consolidation, to RNNs. In contrast to feedforward networks, RNNs iteratively reuse a shared set of weights and require working memory to process input samples. We show that the performance of weight-importance methods is not directly affected by the length of the processed sequences, but rather by high working memory requirements, which lead to an increased need for stability at the cost of decreased plasticity for learning subsequent tasks. We additionally provide theoretical arguments supporting this interpretation by studying linear RNNs. Our study shows that established CL methods can be successfully ported to the recurrent case, and that a recent regularization approach based on hypernetworks outperforms weight-importance methods, thus emerging as a promising candidate for CL in RNNs. Overall, we provide insights on the differences between CL in feedforward networks and RNNs, while guiding towards effective solutions to tackle CL on sequential data.

</details>

<details>

<summary>2021-03-11 10:03:02 - An unsupervised deep learning framework for medical image denoising</summary>

- *Swati Rai, Jignesh S. Bhatt, S. K. Patra*

- `2103.06575v1` - [abs](http://arxiv.org/abs/2103.06575v1) - [pdf](http://arxiv.org/pdf/2103.06575v1)

> Medical image acquisition is often intervented by unwanted noise that corrupts the information content. This paper introduces an unsupervised medical image denoising technique that learns noise characteristics from the available images and constructs denoised images. It comprises of two blocks of data processing, viz., patch-based dictionaries that indirectly learn the noise and residual learning (RL) that directly learns the noise. The model is generalized to account for both 2D and 3D images considering different medical imaging instruments. The images are considered one-by-one from the stack of MRI/CT images as well as the entire stack is considered, and decomposed into overlapping image/volume patches. These patches are given to the patch-based dictionary learning to learn noise characteristics via sparse representation while given to the RL part to directly learn the noise properties. K-singular value decomposition (K-SVD) algorithm for sparse representation is used for training patch-based dictionaries. On the other hand, residue in the patches is trained using the proposed deep residue network. Iterating on these two parts, an optimum noise characterization for each image/volume patch is captured and in turn it is subtracted from the available respective image/volume patch. The obtained denoised image/volume patches are finally assembled to a denoised image or 3D stack. We provide an analysis of the proposed approach with other approaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer and the comparative results show that the proposed algorithm preserves the critical information in the images as well as improves the visual quality of the images.

</details>

<details>

<summary>2021-03-12 18:02:53 - PACO: Global Signal Restoration via PAtch COnsensus</summary>

- *Ignacio Francisco Ramírez Paulino*

- `1808.06942v3` - [abs](http://arxiv.org/abs/1808.06942v3) - [pdf](http://arxiv.org/pdf/1808.06942v3)

> Many signal processing algorithms break the target signal into overlapping segments (also called windows, or patches), process them separately, and then stitch them back into place to produce a unified output. At the overlaps, the final value of those samples that are estimated more than once needs to be decided in some way. Averaging, the simplest approach, often leads to unsatisfactory results. Significant work has been devoted to this issue in recent years. Several works explore the idea of a weighted average of the overlapped patches and/or pixels; others promote agreement (consensus) between the patches at their intersections. Agreement can be either encouraged or imposed as a hard constraint. This work develops on the latter case. The result is a variational signal processing framework, named PACO, which features a number of appealing theoretical and practical properties. The PACO framework consists of a variational formulation that fits a wide variety of problems, and a general ADMMbased algorithm for minimizing the resulting energies. As a byproduct, we show that the consensus step of the algorithm, which is the main bottleneck of similar methods, can be solved efficiently and easily for any arbitrary patch decomposition scheme. We demonstrate the flexibility and power of PACO on three different problems: image inpainting (which we have already covered in previous works), image denoising, and contrast enhancement, using different cost functions including Laplacian and Gaussian Mixture Models.

</details>

<details>

<summary>2021-03-15 01:54:36 - A node-charge graph-based online carshare rebalancing policy with capacitated electric charging</summary>

- *Theodoros P. Pantelidis, Li Li, Tai-Yu Ma, Joseph Y. J. Chow, Saif Eddin G. Jabari*

- `2001.07282v4` - [abs](http://arxiv.org/abs/2001.07282v4) - [pdf](http://arxiv.org/pdf/2001.07282v4)

> Viability of electric car-sharing operations depends on rebalancing algorithms. Earlier methods in the literature suggest a trend toward non-myopic algorithms using queueing principles. We propose a new rebalancing policy using cost function approximation. The cost function is modeled as a p-median relocation problem with minimum cost flow conservation and path-based charging station capacities on a static node-charge graph structure. The cost function is NP-complete, so a heuristic is proposed that ensures feasible solutions that can be solved in an online system. The algorithm is validated in a case study of electric carshare in Brooklyn, New York, with demand data shared from BMW ReachNow operations in September 2017 (262 vehicle fleet, 231 pickups per day, 303 traffic analysis zones (TAZs)) and charging station location data (18 charging stations with 4 port capacities). The proposed non-myopic rebalancing heuristic reduces the cost increase compared to myopic rebalancing by 38%. Other managerial insights are further discussed.

</details>

<details>

<summary>2021-03-15 09:05:05 - Does Code Review Promote Conformance? A Study of OpenStack Patches</summary>

- *Panyawut Sri-iesaranusorn, Raula Gaikovina Kula, Takashi Ishio*

- `2103.08595v1` - [abs](http://arxiv.org/abs/2103.08595v1) - [pdf](http://arxiv.org/pdf/2103.08595v1)

> Code Review plays a crucial role in software quality, by allowing reviewers to discuss and critique any new patches before they can be successfully integrated into the project code. Yet, it is unsure the extent to which coding pattern changes (i.e., repetitive code) from when a patch is first submitted and when the decision is made (i.e., during the review process). In this study, we revisit coding patterns in code reviews, aiming to analyze whether or not the coding pattern changes during the review process. Comparing prior submitted patches, we measure differences in coding pattern between pre-review~(i.e., patch before the review) and post-review~(i.e., patch after a review) from 27,736 reviewed OpenStack patches. Results show that patches after review, tend to conform to similar coding patterns of accepted patches, compared to when they were first submitted. We also find that accepted patches do have similar coding patterns to prior accepted patches. Our study reveals insights into the review process, supporting the potential for automated tool support for newcomers and lays the groundwork for work into understanding conformance and how it makes for an efficient code review process.

</details>

<details>

<summary>2021-03-16 19:20:20 - Colorectal Cancer Segmentation using Atrous Convolution and Residual Enhanced UNet</summary>

- *Nisarg A. Shah, Divij Gupta, Romil Lodaya, Ujjwal Baid, Sanjay Talbar*

- `2103.09289v1` - [abs](http://arxiv.org/abs/2103.09289v1) - [pdf](http://arxiv.org/pdf/2103.09289v1)

> Colorectal cancer is a leading cause of death worldwide. However, early diagnosis dramatically increases the chances of survival, for which it is crucial to identify the tumor in the body. Since its imaging uses high-resolution techniques, annotating the tumor is time-consuming and requires particular expertise. Lately, methods built upon Convolutional Neural Networks(CNNs) have proven to be at par, if not better in many biomedical segmentation tasks. For the task at hand, we propose another CNN-based approach, which uses atrous convolutions and residual connections besides the conventional filters. The training and inference were made using an efficient patch-based approach, which significantly reduced unnecessary computations. The proposed AtResUNet was trained on the DigestPath 2019 Challenge dataset for colorectal cancer segmentation with results having a Dice Coefficient of 0.748.

</details>

<details>

<summary>2021-03-17 19:36:04 - Self-Supervised Learning of Audio Representations from Permutations with Differentiable Ranking</summary>

- *Andrew N Carr, Quentin Berthet, Mathieu Blondel, Olivier Teboul, Neil Zeghidour*

- `2103.09879v1` - [abs](http://arxiv.org/abs/2103.09879v1) - [pdf](http://arxiv.org/pdf/2103.09879v1)

> Self-supervised pre-training using so-called "pretext" tasks has recently shown impressive performance across a wide range of modalities. In this work, we advance self-supervised learning from permutations, by pre-training a model to reorder shuffled parts of the spectrogram of an audio signal, to improve downstream classification performance. We make two main contributions. First, we overcome the main challenges of integrating permutation inversions into an end-to-end training scheme, using recent advances in differentiable ranking. This was heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. Our experiments validate that learning from all possible permutations improves the quality of the pre-trained representations over using a limited, fixed set. Second, we show that inverting permutations is a meaningful pretext task for learning audio representations in an unsupervised fashion. In particular, we improve instrument classification and pitch estimation of musical notes by reordering spectrogram patches in the time-frequency space.

</details>

<details>

<summary>2021-03-18 13:45:07 - Interior Object Detection and Color Harmonization</summary>

- *Sharmin Pathan*

- `1908.04344v2` - [abs](http://arxiv.org/abs/1908.04344v2) - [pdf](http://arxiv.org/pdf/1908.04344v2)

> Confused about renovating your space? Choosing the perfect color for your walls is always a challenging task. One does rounds of color consultation and several patch tests. This paper proposes an AI tool to pitch paint based on attributes of your room and other furniture, and visualize it on your walls. It makes the color selection process easy. It takes in images of a room, detects furniture objects using YOLO object detection. Once these objects have been detected, the tool picks out color of the object. Later this object specific information gets appended to the room attributes (room_type, room_size, preferred_tone, etc) and a deep neural net is trained to make predictions for color/texture/wallpaper for the walls. Finally, these predictions are visualized on the walls from the images provided. The idea is to take the knowledge of a color consultant and pitch colors that suit the walls and provide a good contrast with the furniture and harmonize with different colors in the room. Transfer learning for YOLO object detection from the COCO dataset was used as a starting point and the weights were later fine-tuned by training on additional images. The model was trained on 1000 records listing the room and furniture attributes, to predict colors. Given the room image, this method finds the best color scheme for the walls. These predictions are then visualized on the walls in the image using image segmentation. The results are visually appealing and automatically enhance the color look-and-feel.

</details>

<details>

<summary>2021-03-19 11:37:15 - Multi-layered tensor networks for image classification</summary>

- *Raghavendra Selvan, Silas Ørting, Erik B Dam*

- `2011.06982v2` - [abs](http://arxiv.org/abs/2011.06982v2) - [pdf](http://arxiv.org/pdf/2011.06982v2)

> The recently introduced locally orderless tensor network (LoTeNet) for supervised image classification uses matrix product state (MPS) operations on grids of transformed image patches. The resulting patch representations are combined back together into the image space and aggregated hierarchically using multiple MPS blocks per layer to obtain the final decision rules. In this work, we propose a non-patch based modification to LoTeNet that performs one MPS operation per layer, instead of several patch-level operations. The spatial information in the input images to MPS blocks at each layer is squeezed into the feature dimension, similar to LoTeNet, to maximise retained spatial correlation between pixels when images are flattened into 1D vectors. The proposed multi-layered tensor network (MLTN) is capable of learning linear decision boundaries in high dimensional spaces in a multi-layered setting, which results in a reduction in the computation cost compared to LoTeNet without any degradation in performance.

</details>

<details>

<summary>2021-03-19 20:14:40 - Which contributions count? Analysis of attribution in open source</summary>

- *Jean-Gabriel Young, Amanda Casari, Katie McLaughlin, Milo Z. Trujillo, Laurent Hébert-Dufresne, James P. Bagrow*

- `2103.11007v1` - [abs](http://arxiv.org/abs/2103.11007v1) - [pdf](http://arxiv.org/pdf/2103.11007v1)

> Open source software projects usually acknowledge contributions with text files, websites, and other idiosyncratic methods. These data sources are hard to mine, which is why contributorship is most frequently measured through changes to repositories, such as commits, pushes, or patches. Recently, some open source projects have taken to recording contributor actions with standardized systems; this opens up a unique opportunity to understand how community-generated notions of contributorship map onto codebases as the measure of contribution. Here, we characterize contributor acknowledgment models in open source by analyzing thousands of projects that use a model called All Contributors to acknowledge diverse contributions like outreach, finance, infrastructure, and community management. We analyze the life cycle of projects through this model's lens and contrast its representation of contributorship with the picture given by other methods of acknowledgment, including GitHub's top committers indicator and contributions derived from actions taken on the platform. We find that community-generated systems of contribution acknowledgment make work like idea generation or bug finding more visible, which generates a more extensive picture of collaboration. Further, we find that models requiring explicit attribution lead to more clearly defined boundaries around what is and what is not a contribution.

</details>

<details>

<summary>2021-03-21 10:47:38 - A new public Alsat-2B dataset for single-image super-resolution</summary>

- *Achraf Djerida, Khelifa Djerriri, Moussa Sofiane Karoui, Mohammed El Amin larabi*

- `2103.12547v1` - [abs](http://arxiv.org/abs/2103.12547v1) - [pdf](http://arxiv.org/pdf/2103.12547v1)

> Currently, when reliable training datasets are available, deep learning methods dominate the proposed solutions for image super-resolution. However, for remote sensing benchmarks, it is very expensive to obtain high spatial resolution images. Most of the super-resolution methods use down-sampling techniques to simulate low and high spatial resolution pairs and construct the training samples. To solve this issue, the paper introduces a novel public remote sensing dataset (Alsat2B) of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. Besides, the performance of some super-resolution methods on the dataset is assessed based on common criteria. The obtained results reveal that the proposed scheme is promising and highlight the challenges in the dataset which shows the need for advanced methods to grasp the relationship between the low and high-resolution patches.

</details>

<details>

<summary>2021-03-21 22:45:20 - Computational framework for monolithic coupling for thin fluid flow in contact interfaces</summary>

- *Andrei G. Shvarts, Julien Vignollet, Vladislav A. Yastrebov*

- `1912.11292v3` - [abs](http://arxiv.org/abs/1912.11292v3) - [pdf](http://arxiv.org/pdf/1912.11292v3)

> We developed a computational framework for simulating thin fluid flow in narrow interfaces between contacting solids, which is relevant for a range of engineering, biological and geophysical applications. The treatment of this problem requires coupling between fluid and solid mechanics equations, further complicated by contact constraints and potentially complex geometrical features of contacting surfaces. We developed a monolithic finite-element framework for handling mechanical contact, thin incompressible viscous flow and fluid-induced tractions on the surface of the solid, suitable for both one- and two-way coupling approaches. Additionally, we consider the possibility of fluid entrapment in "pools" delimited by contact patches and its pressurisation following a non-linear compressibility constitutive law. Furthermore, image analysis algorithms were adapted to identify the local status of each interface element within the Newton-Raphson loop. First, an application of the proposed framework for a problem with a model geometry is given, and the robustness is demonstrated by the residual-wise and status-wise convergence. The full capability of the developed two-way coupling framework is demonstrated on a problem of a fluid flow in contact interface between a solid with representative rough surface and a rigid flat. The evolution of the contact pressure, fluid flow pattern and the morphology of trapped fluid zones until the complete sealing of the interface is displayed. Additionally, we demonstrated an almost mesh-independent result of a refined post-processing approach to the real contact-area computation. The developed framework permits not only to study the evolution of effective properties of contact interfaces, but also to highlight the difference between one- and two-way coupling approaches and to quantify the effect of multiple trapped fluid "pools" on the coupled problem.

</details>

<details>

<summary>2021-03-21 23:45:09 - An Empirical Study of OSS-Fuzz Bugs</summary>

- *Zhen Yu Ding, Claire Le Goues*

- `2103.11518v1` - [abs](http://arxiv.org/abs/2103.11518v1) - [pdf](http://arxiv.org/pdf/2103.11518v1)

> Continuous fuzzing is an increasingly popular technique for automated quality and security assurance. Google maintains OSS-Fuzz: a continuous fuzzing service for open source software. We conduct the first empirical study of OSS-Fuzz, analyzing 23,907 bugs found in 316 projects. We examine the characteristics of fuzzer-found faults, the lifecycles of such faults, and the evolution of fuzzing campaigns over time. We find that OSS-Fuzz is often effective at quickly finding bugs, and developers are often quick to patch them. However, flaky bugs, timeouts, and out of memory errors are problematic, people rarely file CVEs for security vulnerabilities, and fuzzing campaigns often exhibit punctuated equilibria, where developers might be surprised by large spikes in bugs found. Our findings have implications on future fuzzing research and practice.

</details>

<details>

<summary>2021-03-22 03:18:34 - ConfInLog: Leveraging Software Logs to Infer Configuration Constraints</summary>

- *Shulin Zhou, Xiaodong Liu, Shanshan Li, Zhouyang Jia, Yuanliang Zhang, Teng Wang, Wang Li, Xiangke Liao*

- `2103.11561v1` - [abs](http://arxiv.org/abs/2103.11561v1) - [pdf](http://arxiv.org/pdf/2103.11561v1)

> Misconfigurations have become the dominant causes of software failures in recent years, drawing tremendous attention for their increasing prevalence and severity. Configuration constraints can preemptively avoid misconfiguration by defining the conditions that configuration options should satisfy. Documentation is the main source of configuration constraints, but it might be incomplete or inconsistent with the source code. In this regard, prior researches have focused on obtaining configuration constraints from software source code through static analysis. However, the difficulty in pointer analysis and context comprehension prevents them from collecting accurate and comprehensive constraints. In this paper, we observed that software logs often contain configuration constraints. We conducted an empirical study and summarized patterns of configuration-related log messages. Guided by the study, we designed and implemented ConfInLog, a static tool to infer configuration constraints from log messages. ConfInLog first selects configuration-related log messages from source code by using the summarized patterns, then infers constraints from log messages based on the summarized natural language patterns. To evaluate the effectiveness of ConfInLog, we applied our tool on seven popular open-source software systems. ConfInLog successfully inferred 22 to 163 constraints, in which 59.5% to 61.6% could not be inferred by the state-of-the-art work. Finally, we submitted 67 documentation patches regarding the constraints inferred by ConfInLog. The constraints in 29 patches have been confirmed by the developers, among which 10 patches have been accepted.

</details>

<details>

<summary>2021-03-22 06:05:20 - PriorityCut: Occlusion-guided Regularization for Warp-based Image Animation</summary>

- *Wai Ting Cheung, Gyeongsu Chae*

- `2103.11600v1` - [abs](http://arxiv.org/abs/2103.11600v1) - [pdf](http://arxiv.org/pdf/2103.11600v1)

> Image animation generates a video of a source image following the motion of a driving video. State-of-the-art self-supervised image animation approaches warp the source image according to the motion of the driving video and recover the warping artifacts by inpainting. These approaches mostly use vanilla convolution for inpainting, and vanilla convolution does not distinguish between valid and invalid pixels. As a result, visual artifacts are still noticeable after inpainting. CutMix is a state-of-the-art regularization strategy that cuts and mixes patches of images and is widely studied in different computer vision tasks. Among the remaining computer vision tasks, warp-based image animation is one of the fields that the effects of CutMix have yet to be studied. This paper first presents a preliminary study on the effects of CutMix on warp-based image animation. We observed in our study that CutMix helps improve only pixel values, but disturbs the spatial relationships between pixels. Based on such observation, we propose PriorityCut, a novel augmentation approach that uses the top-k percent occluded pixels of the foreground to regularize warp-based image animation. By leveraging the domain knowledge in warp-based image animation, PriorityCut significantly reduces the warping artifacts in state-of-the-art warp-based image animation models on diverse datasets.

</details>

<details>

<summary>2021-03-22 16:33:55 - Update Frequently, Update Fast: Retraining Semantic Parsing Systems in a Fraction of Time</summary>

- *Vladislav Lialin, Rahul Goel, Andrey Simanovsky, Anna Rumshisky, Rushin Shah*

- `2010.07865v2` - [abs](http://arxiv.org/abs/2010.07865v2) - [pdf](http://arxiv.org/pdf/2010.07865v2)

> Currently used semantic parsing systems deployed in voice assistants can require weeks to train. Datasets for these models often receive small and frequent updates, data patches. Each patch requires training a new model. To reduce training time, one can fine-tune the previously trained model on each patch, but naive fine-tuning exhibits catastrophic forgetting - degradation of the model performance on the data not represented in the data patch. In this work, we propose a simple method that alleviates catastrophic forgetting and show that it is possible to match the performance of a model trained from scratch in less than 10% of a time via fine-tuning. The key to achieving this is supersampling and EWC regularization. We demonstrate the effectiveness of our method on multiple splits of the Facebook TOP and SNIPS datasets.

</details>

<details>

<summary>2021-03-23 11:45:41 - RPATTACK: Refined Patch Attack on General Object Detectors</summary>

- *Hao Huang, Yongtao Wang, Zhaoyu Chen, Zhi Tang, Wenqiang Zhang, Kai-Kuang Ma*

- `2103.12469v1` - [abs](http://arxiv.org/abs/2103.12469v1) - [pdf](http://arxiv.org/pdf/2103.12469v1)

> Nowadays, general object detectors like YOLO and Faster R-CNN as well as their variants are widely exploited in many applications. Many works have revealed that these detectors are extremely vulnerable to adversarial patch attacks. The perturbed regions generated by previous patch-based attack works on object detectors are very large which are not necessary for attacking and perceptible for human eyes. To generate much less but more efficient perturbation, we propose a novel patch-based method for attacking general object detectors. Firstly, we propose a patch selection and refining scheme to find the pixels which have the greatest importance for attack and remove the inconsequential perturbations gradually. Then, for a stable ensemble attack, we balance the gradients of detectors to avoid over-optimizing one of them during the training phase. Our RPAttack can achieve an amazing missed detection rate of 100% for both Yolo v4 and Faster R-CNN while only modifies 0.32% pixels on VOC 2007 test set. Our code is available at https://github.com/VDIGPKU/RPAttack.

</details>

<details>

<summary>2021-03-23 23:51:22 - SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile Platforms</summary>

- *Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa, Partha Pratim Pande*

- `2103.12896v1` - [abs](http://arxiv.org/abs/2103.12896v1) - [pdf](http://arxiv.org/pdf/2103.12896v1)

> We consider the task of photo-realistic unconditional image generation (generate high quality, diverse samples that carry the same visual content as the image) on mobile platforms using Generative Adversarial Networks (GANs). In this paper, we propose a novel approach to trade-off image generation accuracy of a GAN for the energy consumed (compute) at run-time called Scale-Energy Tradeoff GAN (SETGAN). GANs usually take a long time to train and consume a huge memory hence making it difficult to run on edge devices. The key idea behind SETGAN for an image generation task is for a given input image, we train a GAN on a remote server and use the trained model on edge devices. We use SinGAN, a single image unconditional generative model, that contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. During the training process, we determine the optimal number of scales for a given input image and the energy constraint from the target edge device. Results show that with SETGAN's unique client-server-based architecture, we were able to achieve a 56% gain in energy for a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale training, we obtain around 4x gain in training time on the server.

</details>

<details>

<summary>2021-03-24 19:15:40 - CrossFix: Collaborative bug fixing by recommending similar bugs</summary>

- *Shin Hwei Tan, Ziqiang Li, Lu Yan*

- `2103.13453v1` - [abs](http://arxiv.org/abs/2103.13453v1) - [pdf](http://arxiv.org/pdf/2103.13453v1)

> Many automated program repair techniques have been proposed for fixing bugs. Some of these techniques use the information beyond the given buggy program and test suite to improve the quality of generated patches. However, there are several limitations that hinder the wide adoption of these techniques, including (1) they rely on a fixed set of repair templates for patch generation or reference implementation, (2) searching for the suitable reference implementation is challenging, (3) generated patches are not explainable. Meanwhile, a recent approach shows that similar bugs exist across different projects and one could use the GitHub issue from a different project for finding new bugs for a related project. We propose collaborative bug fixing, a novelapproach that suggests bug reports that describe a similar bug. Our studyredefines similar bugs as bugs that share the (1) same libraries, (2) same functionalities, (3) same reproduction steps, (4) same configurations, (5) sameoutcomes, or (6) same errors. Moreover, our study revealed the usefulness of similar bugs in helping developers in finding more context about the bug and fixing. Based on our study, we design CrossFix, a tool that automatically suggests relevant GitHub issues based on an open GitHub issue. Our evaluation on 249 open issues from Java and Android projects shows that CrossFix could suggest similar bugs to help developers in debugging and fixing.

</details>

<details>

<summary>2021-03-25 08:56:08 - Explainability Guided Multi-Site COVID-19 CT Classification</summary>

- *Ameen Ali, Tal Shaharabany, Lior Wolf*

- `2103.13677v1` - [abs](http://arxiv.org/abs/2103.13677v1) - [pdf](http://arxiv.org/pdf/2103.13677v1)

> Radiologist examination of chest CT is an effective way for screening COVID-19 cases. In this work, we overcome three challenges in the automation of this process: (i) the limited number of supervised positive cases, (ii) the lack of region-based supervision, and (iii) the variability across acquisition sites. These challenges are met by incorporating a recent augmentation solution called SnapMix, by a new patch embedding technique, and by performing a test-time stability analysis. The three techniques are complementary and are all based on utilizing the heatmaps produced by the Class Activation Mapping (CAM) explainability method. Compared to the current state of the art, we obtain an increase of five percent in the F1 score on a site with a relatively high number of cases, and a gap twice as large for a site with much fewer training images.

</details>

<details>

<summary>2021-03-25 09:59:57 - MAD-HTLC: Because HTLC is Crazy-Cheap to Attack</summary>

- *Itay Tsabary, Matan Yechieli, Alex Manuskin, Ittay Eyal*

- `2006.12031v3` - [abs](http://arxiv.org/abs/2006.12031v3) - [pdf](http://arxiv.org/pdf/2006.12031v3)

> Smart Contracts and transactions allow users to implement elaborate constructions on cryptocurrency blockchains like Bitcoin and Ethereum. Many of these constructions, including operational payment channels and atomic swaps, use a building block called Hashed Time-Locked Contract (HTLC).   In this work, we distill from HTLC a specification (HTLC-Spec), and present an implementation called Mutual-Assured-Destruction Hashed Time-Locked Contract (MAD-HTLC). MAD-HTLC employs a novel approach of utilizing the existing blockchain operators, called miners, as part of the design. If a user misbehaves, MAD-HTLC incentivizes the miners to confiscate all her funds. We prove MAD-HTLC's security using the UC framework and game-theoretic analysis. We demonstrate MAD-HTLC's efficacy and analyze its overhead by instantiating it on Bitcoin's and Ethereum's operational blockchains.   Notably, current miner software makes only little effort to optimize revenue, since the advantage is relatively small. However, as the demand grows and other revenue components shrink, miners are more motivated to fully optimize their fund intake. By patching the standard Bitcoin client, we demonstrate such optimization is easy to implement, making the miners natural enforcers of MAD-HTLC.   Finally, we extend previous results regarding HTLC vulnerability to bribery attacks. An attacker can incentivize miners to prefer her transactions by offering high transaction fees. We demonstrate this attack can be easily implemented by patching the Bitcoin client, and use game-theoretic tools to qualitatively tighten the known cost bound of such bribery attacks in presence of rational miners. We identify bribe opportunities occurring on the Bitcoin and Ethereum main networks where a few dollars bribe could yield tens of thousands of dollars in reward (e.g., \$2 for over \$25K).

</details>

<details>

<summary>2021-03-25 15:11:11 - Measure Theoretic Weighted Model Integration</summary>

- *Ivan Miosic, Pedro Zuidberg Dos Martires*

- `2103.13901v1` - [abs](http://arxiv.org/abs/2103.13901v1) - [pdf](http://arxiv.org/pdf/2103.13901v1)

> Weighted model counting (WMC) is a popular framework to perform probabilistic inference with discrete random variables. Recently, WMC has been extended to weighted model integration (WMI) in order to additionally handle continuous variables. At their core, WMI problems consist of computing integrals and sums over weighted logical formulas. From a theoretical standpoint, WMI has been formulated by patching the sum over weighted formulas, which is already present in WMC, with Riemann integration. A more principled approach to integration, which is rooted in measure theory, is Lebesgue integration. Lebesgue integration allows one to treat discrete and continuous variables on equal footing in a principled fashion. We propose a theoretically sound measure theoretic formulation of weighted model integration, which naturally reduces to weighted model counting in the absence of continuous variables. Instead of regarding weighted model integration as an extension of weighted model counting, WMC emerges as a special case of WMI in our formulation.

</details>

<details>

<summary>2021-03-29 03:04:08 - DECOR-GAN: 3D Shape Detailization by Conditional Refinement</summary>

- *Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, Siddhartha Chaudhuri*

- `2012.09159v2` - [abs](http://arxiv.org/abs/2012.09159v2) - [pdf](http://arxiv.org/pdf/2012.09159v2)

> We introduce a deep generative network for 3D shape detailization, akin to stylization with the style being geometric details. We address the challenge of creating large varieties of high-resolution and detailed 3D geometry from a small set of exemplars by treating the problem as that of geometric detail transfer. Given a low-resolution coarse voxel shape, our network refines it, via voxel upsampling, into a higher-resolution shape enriched with geometric details. The output shape preserves the overall structure (or content) of the input, while its detail generation is conditioned on an input "style code" corresponding to a detailed exemplar. Our 3D detailization via conditional refinement is realized by a generative adversarial network, coined DECOR-GAN. The network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D PatchGAN discriminator to enforce local patches of the generated model to be similar to those in the training detailed shapes. During testing, a style code is fed into the generator to condition the refinement. We demonstrate that our method can refine a coarse shape into a variety of detailed shapes with different styles. The generated results are evaluated in terms of content preservation, plausibility, and diversity. Comprehensive ablation studies are conducted to validate our network designs. Code is available at https://github.com/czq142857/DECOR-GAN.

</details>

<details>

<summary>2021-03-29 09:15:12 - Bringing UMAP Closer to the Speed of Light with GPU Acceleration</summary>

- *Corey J. Nolet, Victor Lafargue, Edward Raff, Thejaswi Nanditale, Tim Oates, John Zedlewski, Joshua Patterson*

- `2008.00325v3` - [abs](http://arxiv.org/abs/2008.00325v3) - [pdf](http://arxiv.org/pdf/2008.00325v3)

> The Uniform Manifold Approximation and Projection (UMAP) algorithm has become widely popular for its ease of use, quality of results, and support for exploratory, unsupervised, supervised, and semi-supervised learning. While many algorithms can be ported to a GPU in a simple and direct fashion, such efforts have resulted in inefficient and inaccurate versions of UMAP. We show a number of techniques that can be used to make a faster and more faithful GPU version of UMAP, and obtain speedups of up to 100x in practice. Many of these design choices/lessons are general purpose and may inform the conversion of other graph and manifold learning algorithms to use GPUs. Our implementation has been made publicly available as part of the open source RAPIDS cuML library (https://github.com/rapidsai/cuml).

</details>

<details>

<summary>2021-03-31 06:57:10 - Exploring Plausible Patches Using Source Code Embeddings in JavaScript</summary>

- *Viktor Csuvik, Dániel Horváth, Márk Lajkó, László Vidács*

- `2103.16846v1` - [abs](http://arxiv.org/abs/2103.16846v1) - [pdf](http://arxiv.org/pdf/2103.16846v1)

> Despite the immense popularity of the Automated Program Repair (APR) field, the question of patch validation is still open. Most of the present-day approaches follow the so-called Generate-and-Validate approach, where first a candidate solution is being generated and after validated against an oracle. The latter, however, might not give a reliable result, because of the imperfections in such oracles; one of which is usually the test suite. Although (re-) running the test suite is right under one's nose, in real life applications the problem of over- and underfitting often occurs, resulting in inadequate patches. Efforts that have been made to tackle with this problem include patch filtering, test suite expansion, careful patch producing and many more. Most approaches to date use post-filtering relying either on test execution traces or make use of some similarity concept measured on the generated patches. Our goal is to investigate the nature of these similarity-based approaches. To do so, we trained a Doc2Vec model on an open-source JavaScript project and generated 465 patches for 10 bugs in it. These plausible patches alongside with the developer fix are then ranked based on their similarity to the original program. We analyzed these similarity lists and found that plain document embeddings may lead to misclassification - it fails to capture nuanced code semantics. Nevertheless, in some cases it also provided useful information, thus helping to better understand the area of Automated Program Repair.

</details>

<details>

<summary>2021-03-31 08:49:31 - Lags in the Release, Adoption, and Propagation of npm Vulnerability Fixes</summary>

- *Bodin Chinthanet, Raula Gaikovina Kula, Shane McIntosh, Takashi Ishio, Akinori Ihara, Kenichi Matsumoto*

- `1907.03407v5` - [abs](http://arxiv.org/abs/1907.03407v5) - [pdf](http://arxiv.org/pdf/1907.03407v5)

> Security vulnerability in third-party dependencies is a growing concern not only for developers of the affected software, but for the risks it poses to an entire software ecosystem, e.g., Heartbleed vulnerability. Recent studies show that developers are slow to respond to the threat of vulnerability, sometimes taking four to eleven months to act. To ensure quick adoption and propagation of a release that contains the fix (fixing release), we conduct an empirical investigation to identify lags that may occur between the vulnerable release and its fixing release (package-side fixing release). Through a preliminary study of 231 package-side fixing release of npm projects on GitHub, we observe that a fixing release is rarely released on its own, with up to 85.72% of the bundled commits being unrelated to a fix. We then compare the package-side fixing release with changes on a client-side (client-side fixing release). Through an empirical study of the adoption and propagation tendencies of 1,290 package-side fixing releases that impact throughout a network of 1,553,325 releases of npm packages, we find that stale clients require additional migration effort, even if the package-side fixing release was quick (i.e., package patch landing). Furthermore, we show the influence of factors such as the branch that the package-side fixing release lands on and the severity of vulnerability on its propagation. In addition to these lags we identify and characterize, this paper lays the groundwork for future research on how to mitigate lags in an ecosystem.

</details>

<details>

<summary>2021-03-31 14:20:39 - PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking</summary>

- *Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, Prateek Mittal*

- `2005.10884v5` - [abs](http://arxiv.org/abs/2005.10884v5) - [pdf](http://arxiv.org/pdf/2005.10884v5)

> Localized adversarial patches aim to induce misclassification in machine learning models by arbitrarily modifying pixels within a restricted region of an image. Such attacks can be realized in the physical world by attaching the adversarial patch to the object to be misclassified, and defending against such attacks is an unsolved/open problem. In this paper, we propose a general defense framework called PatchGuard that can achieve high provable robustness while maintaining high clean accuracy against localized adversarial patches. The cornerstone of PatchGuard involves the use of CNNs with small receptive fields to impose a bound on the number of features corrupted by an adversarial patch. Given a bounded number of corrupted features, the problem of designing an adversarial patch defense reduces to that of designing a secure feature aggregation mechanism. Towards this end, we present our robust masking defense that robustly detects and masks corrupted features to recover the correct prediction. Notably, we can prove the robustness of our defense against any adversary within our threat model. Our extensive evaluation on ImageNet, ImageNette (a 10-class subset of ImageNet), and CIFAR-10 datasets demonstrates that our defense achieves state-of-the-art performance in terms of both provable robust accuracy and clean accuracy.

</details>


## 2021-04

<details>

<summary>2021-04-01 11:31:46 - Avalanche: an End-to-End Library for Continual Learning</summary>

- *Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graffieti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, Davide Maltoni*

- `2104.00405v1` - [abs](http://arxiv.org/abs/2104.00405v1) - [pdf](http://arxiv.org/pdf/2104.00405v1)

> Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms.

</details>

<details>

<summary>2021-04-03 09:13:05 - Deepfake Detection Scheme Based on Vision Transformer and Distillation</summary>

- *Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim*

- `2104.01353v1` - [abs](http://arxiv.org/abs/2104.01353v1) - [pdf](http://arxiv.org/pdf/2104.01353v1)

> Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition.

</details>

<details>

<summary>2021-04-04 02:17:58 - Program Behavior Analysis and Clustering using Performance Counters</summary>

- *Sai Praveen Kadiyala, Akella Kartheek, Tram Truong-Huu*

- `2104.01518v1` - [abs](http://arxiv.org/abs/2104.01518v1) - [pdf](http://arxiv.org/pdf/2104.01518v1)

> Understanding the dynamic behavior of computer programs during normal working conditions is an important task, which has multiple security benefits such as the development of behavior-based anomaly detection, vulnerability discovery, and patching. Existing works achieved this goal by collecting and analyzing various data including network traffic, system calls, instruction traces, etc. In this paper, we explore the use of a new type of data, performance counters, to analyze the dynamic behavior of programs. Using existing primitives, we develop a tool named perfextract to capture data from different performance counters for a program during its startup time, thus forming multiple time series to represent the dynamic behavior of the program. We analyze the collected data and develop a semi-supervised clustering algorithm that allows us to classify each program using its performance counter time series into a specific group and to identify the intrinsic behavior of that group. We carry out extensive experiments with 18 real-world programs that belong to 4 groups including web browsers, text editors, image viewers, and audio players. The experimental results show that the examined programs can be accurately differentiated based on their performance counter data regardless of whether programs are run in physical or virtual environments.

</details>

<details>

<summary>2021-04-04 05:23:54 - Code Reviews with Divergent Review Scores: An Empirical Study of the OpenStack and Qt Communities</summary>

- *Toshiki Hirao, Shane McIntosh, Akinori Ihara, Kenichi Matsumoto*

- `2104.01537v1` - [abs](http://arxiv.org/abs/2104.01537v1) - [pdf](http://arxiv.org/pdf/2104.01537v1)

> Code review is a broadly adopted software quality practice where developers critique each others' patches. In addition to providing constructive feedback, reviewers may provide a score to indicate whether the patch should be integrated. Since reviewer opinions may differ, patches can receive both positive and negative scores. If reviews with divergent scores are not carefully resolved, they may contribute to a tense reviewing culture and may slow down integration. In this paper, we study patches with divergent review scores in the OPENSTACK and QT communities. Quantitative analysis indicates that patches with divergent review scores: (1) account for 15%-37% of patches that receive multiple review scores; (2) are integrated more often than they are abandoned; and (3) receive negative scores after positive ones in 70% of cases. Furthermore, a qualitative analysis indicates that patches with strongly divergent scores that: (4) are abandoned more often suffer from external issues (e.g., integration planning, content duplication) than patches with weakly divergent scores and patches without divergent scores; and (5) are integrated often address reviewer concerns indirectly (i.e., without changing patches). Our results suggest that review tooling should integrate with release schedules and detect concurrent development of similar patches to optimize review discussions with divergent scores. Moreover, patch authors should note that even the most divisive patches are often integrated through discussion, integration timing, and careful revision.

</details>

<details>

<summary>2021-04-06 09:22:56 - A large-scale study on human-cloned changes for automated program repair</summary>

- *Fernanda Madeiral, Thomas Durieux*

- `2104.02386v1` - [abs](http://arxiv.org/abs/2104.02386v1) - [pdf](http://arxiv.org/pdf/2104.02386v1)

> Research in automatic program repair has shown that real bugs can be automatically fixed. However, there are several challenges involved in such a task that are not yet fully addressed. As an example, consider that a test-suite-based repair tool performs a change in a program to fix a bug spotted by a failing test case, but then the same or another test case fails. This could mean that the change is a partial fix for the bug or that another bug was manifested. However, the repair tool discards the change and possibly performs other repair attempts. One might wonder if the applied change should be also applied in other locations in the program so that the bug is fully fixed. In this paper, we are interested in investigating the extent of bug fix changes being cloned by developers within patches. Our goal is to investigate the need of multi-location repair by using identical or similar changes in identical or similar contexts. To do so, we analyzed 3,049 multi-hunk patches from the ManySStuBs4J dataset, which is a large dataset of single statement bug fix changes. We found out that 68% of the multi-hunk patches contain at least one change clone group. Moreover, most of these patches (70%) are strictly-cloned ones, which are patches fully composed of changes belonging to one single change clone group. Finally, most of the strictly-cloned patches (89%) contain change clones with identical changes, independently of their contexts. We conclude that automated solutions for creating patches composed of identical or similar changes can be useful for fixing bugs.

</details>

<details>

<summary>2021-04-07 10:47:51 - Multimodal Continuous Visual Attention Mechanisms</summary>

- *António Farinhas, André F. T. Martins, Pedro M. Q. Aguiar*

- `2104.03046v1` - [abs](http://arxiv.org/abs/2104.03046v1) - [pdf](http://arxiv.org/pdf/2104.03046v1)

> Visual attention mechanisms are a key component of neural network models for computer vision. By focusing on a discrete set of objects or image regions, these mechanisms identify the most relevant features and use them to build more powerful representations. Recently, continuous-domain alternatives to discrete attention models have been proposed, which exploit the continuity of images. These approaches model attention as simple unimodal densities (e.g. a Gaussian), making them less suitable to deal with images whose region of interest has a complex shape or is composed of multiple non-contiguous patches. In this paper, we introduce a new continuous attention mechanism that produces multimodal densities, in the form of mixtures of Gaussians. We use the EM algorithm to obtain a clustering of relevant regions in the image, and a description length penalty to select the number of components in the mixture. Our densities decompose as a linear combination of unimodal attention mechanisms, enabling closed-form Jacobians for the backpropagation step. Experiments on visual question answering in the VQA-v2 dataset show competitive accuracies and a selection of regions that mimics human attention more closely in VQA-HAT. We present several examples that suggest how multimodal attention maps are naturally more interpretable than their unimodal counterparts, showing the ability of our model to automatically segregate objects from ground in complex scenes.

</details>

<details>

<summary>2021-04-07 11:15:51 - Differentiable Patch Selection for Image Recognition</summary>

- *Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk Weissenborn, Jakob Uszkoreit, Thomas Unterthiner*

- `2104.03059v1` - [abs](http://arxiv.org/abs/2104.03059v1) - [pdf](http://arxiv.org/pdf/2104.03059v1)

> Neural Networks require large amounts of memory and compute to process high resolution images, even when only a small part of the image is actually informative for the task at hand. We propose a method based on a differentiable Top-K operator to select the most relevant parts of the input to efficiently process high resolution images. Our method may be interfaced with any downstream neural network, is able to aggregate information from different patches in a flexible way, and allows the whole model to be trained end-to-end using backpropagation. We show results for traffic sign recognition, inter-patch relationship reasoning, and fine-grained recognition without using object/part bounding box annotations during training.

</details>

<details>

<summary>2021-04-08 10:28:27 - Secure (S)Hell: Introducing an SSH Deception Proxy Framework</summary>

- *Daniel Reti, David Klaaßen, Simon Duque Anton, Hans Dieter Schotten*

- `2104.03666v1` - [abs](http://arxiv.org/abs/2104.03666v1) - [pdf](http://arxiv.org/pdf/2104.03666v1)

> Deceiving an attacker in the network security domain is a well established approach, mainly achieved through deployment of honeypots consisting of open network ports with the sole purpose of raising an alert on a connection. With attackers becoming more careful to avoid honeypots, other decoy elements on real host systems continue to create uncertainty for attackers. This uncertainty makes an attack more difficult, as an attacker cannot be sure whether the system does contain deceptive elements or not. Consequently, each action of an attacker could lead to the discovery. In this paper a framework is proposed for placing decoy elements through an SSH proxy, allowing to deploy decoy elements on-the-fly without the need for a modification of the protected host system.

</details>

<details>

<summary>2021-04-08 12:17:54 - HindSight: A Graph-Based Vision Model Architecture For Representing Part-Whole Hierarchies</summary>

- *Muhammad AbdurRafae*

- `2104.03722v1` - [abs](http://arxiv.org/abs/2104.03722v1) - [pdf](http://arxiv.org/pdf/2104.03722v1)

> This paper presents a model architecture for encoding the representations of part-whole hierarchies in images in form of a graph. The idea is to divide the image into patches of different levels and then treat all of these patches as nodes for a fully connected graph. A dynamic feature extraction module is used to extract feature representations from these patches in each graph iteration. This enables us to learn a rich graph representation of the image that encompasses the inherent part-whole hierarchical information. Utilizing proper self-supervised training techniques, such a model can be trained as a general purpose vision encoder model which can then be used for various vision related downstream tasks (e.g., Image Classification, Object Detection, Image Captioning, etc.).

</details>

<details>

<summary>2021-04-08 13:59:27 - Progressive Semantic Segmentation</summary>

- *Chuong Huynh, Anh Tran, Khoa Luu, Minh Hoai*

- `2104.03778v1` - [abs](http://arxiv.org/abs/2104.03778v1) - [pdf](http://arxiv.org/pdf/2104.03778v1)

> The objective of this work is to segment high-resolution images without overloading GPU memory usage or losing the fine details in the output segmentation map. The memory constraint means that we must either downsample the big image or divide the image into local patches for separate processing. However, the former approach would lose the fine details, while the latter can be ambiguous due to the lack of a global picture. In this work, we present MagNet, a multi-scale framework that resolves local ambiguity by looking at the image at multiple magnification levels. MagNet has multiple processing stages, where each stage corresponds to a magnification level, and the output of one stage is fed into the next stage for coarse-to-fine information propagation. Each stage analyzes the image at a higher resolution than the previous stage, recovering the previously lost details due to the lossy downsampling step, and the segmentation output is progressively refined through the processing stages. Experiments on three high-resolution datasets of urban views, aerial scenes, and medical images show that MagNet consistently outperforms the state-of-the-art methods by a significant margin.

</details>

<details>

<summary>2021-04-09 10:20:34 - MLF-SC: Incorporating multi-layer features to sparse coding for anomaly detection</summary>

- *Ryuji Imamura, Kohei Azuma, Atsushi Hanamoto, Atsunori Kanemura*

- `2104.04289v1` - [abs](http://arxiv.org/abs/2104.04289v1) - [pdf](http://arxiv.org/pdf/2104.04289v1)

> Anomalies in images occur in various scales from a small hole on a carpet to a large stain. However, anomaly detection based on sparse coding, one of the widely used anomaly detection methods, has an issue in dealing with anomalies that are out of the patch size employed to sparsely represent images. A large anomaly can be considered normal if seen in a small scale, but it is not easy to determine a single scale (patch size) that works well for all images. Then, we propose to incorporate multi-scale features to sparse coding and improve the performance of anomaly detection. The proposed method, multi-layer feature sparse coding (MLF-SC), employs a neural network for feature extraction, and feature maps from intermediate layers of the network are given to sparse coding, whereas the standard sparse-coding-based anomaly detection method directly works on given images. We show that MLF-SC outperforms state-of-the-art anomaly detection methods including those employing deep learning. Our target data are the texture categories of the MVTec Anomaly Detection (MVTec AD) dataset, which is a modern benchmark dataset consisting of images from the real world. Our idea can be a simple and practical option to deal with practical data.

</details>

<details>

<summary>2021-04-09 21:26:32 - Self-Boosted Automated Program Repair</summary>

- *Samuel Benton, Mengshi Zhang, Xia Li, Lingming Zhang*

- `2104.04611v1` - [abs](http://arxiv.org/abs/2104.04611v1) - [pdf](http://arxiv.org/pdf/2104.04611v1)

> Program repair is an integral part of every software system's life-cycle but can be extremely challenging. To date, researchers have proposed various automated program repair (APR) techniques to reduce efforts of manual debugging. However, given a real-world buggy program, a typical APR technique usually generates a large number of patches, each of which needs to be validated against the original test suite which incurs extremely high computation costs. Although existing APR techniques have already leveraged various static and/or dynamic information to find the desired patches faster, they are still rather costly. In a recent work, researchers proposed unified debugging to leverage the patch execution information during APR to help boost fault localization; in this way,the application scope of APR techniques can be extended to all possible bugs, e.g., the patch execution information during APR can help with manual repair of the bugs that cannot be automatically fixed. Inspired by unified debugging, this work proposes SeAPR (Self-Boosted Automated Program Repair), the first technique to leverage the earlier patch execution information during APR to help boost automated repair itself on-the-fly. Our basic intuition is that patches similar to earlier high-quality/low-quality patches should be promoted/degraded to speed up the detection of the desired patches. This experimental study on 12 state-of-the-art APR systems demonstrates that, overall, SeAPR can substantially reduce the number of patch executions with negligible overhead. Our study also investigates the impact of various configurations on SeAPR. Lastly, our study demonstrates that SeAPR can even leverage the patch execution information from other APR tools from the same buggy program to further boost APR.

</details>

<details>

<summary>2021-04-10 13:41:29 - Q-matrix Unaware Double JPEG Detection using DCT-Domain Deep BiLSTM Network</summary>

- *Vinay Verma, Deepak Singh, Nitin Khanna*

- `2104.04765v1` - [abs](http://arxiv.org/abs/2104.04765v1) - [pdf](http://arxiv.org/pdf/2104.04765v1)

> The double JPEG compression detection has received much attention in recent years due to its applicability as a forensic tool for the most widely used JPEG file format. Existing state-of-the-art CNN-based methods either use histograms of all the frequencies or rely on heuristics to select histograms of specific low frequencies to classify single and double compressed images. However, even amidst lower frequencies of double compressed images/patches, histograms of all the frequencies do not have distinguishable features to separate them from single compressed images. This paper directly extracts the quantized DCT coefficients from the JPEG images without decompressing them in the pixel domain, obtains all AC frequencies' histograms, uses a module based on $1\times 1$ depth-wise convolutions to learn the inherent relation between each histogram and corresponding q-factor, and utilizes a tailor-made BiLSTM network for selectively encoding these feature vector sequences. The proposed system outperforms several baseline methods on a relatively large and diverse publicly available dataset of single and double compressed patches. Another essential aspect of any single vs. double JPEG compression detection system is handling the scenario where test patches are compressed with entirely different quantization matrices (Q-matrices) than those used while training; different camera manufacturers and image processing software generally utilize their customized quantization matrices. A set of extensive experiments shows that the proposed system trained on a single dataset generalizes well on other datasets compressed with completely unseen quantization matrices and outperforms the state-of-the-art methods in both seen and unseen quantization matrices scenarios.

</details>

<details>

<summary>2021-04-12 20:40:12 - Semantic Scene Completion using Local Deep Implicit Functions on LiDAR Data</summary>

- *Christoph B. Rist, David Emmerichs, Markus Enzweiler, Dariu M. Gavrila*

- `2011.09141v3` - [abs](http://arxiv.org/abs/2011.09141v3) - [pdf](http://arxiv.org/pdf/2011.09141v3)

> Semantic scene completion is the task of jointly estimating 3D geometry and semantics of objects and surfaces within a given extent. This is a particularly challenging task on real-world data that is sparse and occluded. We propose a scene segmentation network based on local Deep Implicit Functions as a novel learning-based method for scene completion. Unlike previous work on scene completion, our method produces a continuous scene representation that is not based on voxelization. We encode raw point clouds into a latent space locally and at multiple spatial resolutions. A global scene completion function is subsequently assembled from the localized function patches. We show that this continuous representation is suitable to encode geometric and semantic properties of extensive outdoor scenes without the need for spatial discretization (thus avoiding the trade-off between level of scene detail and the scene extent that can be covered).   We train and evaluate our method on semantically annotated LiDAR scans from the Semantic KITTI dataset. Our experiments verify that our method generates a powerful representation that can be decoded into a dense 3D description of a given scene. The performance of our method surpasses the state of the art on the Semantic KITTI Scene Completion Benchmark in terms of geometric completion intersection-over-union (IoU).

</details>

<details>

<summary>2021-04-13 16:02:39 - Probing Contextual Language Models for Common Ground with Visual Representations</summary>

- *Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi*

- `2005.00619v5` - [abs](http://arxiv.org/abs/2005.00619v5) - [pdf](http://arxiv.org/pdf/2005.00619v5)

> The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.

</details>

<details>

<summary>2021-04-14 17:47:21 - COVID-19 identification from volumetric chest CT scans using a progressively resized 3D-CNN incorporating segmentation, augmentation, and class-rebalancing</summary>

- *Md. Kamrul Hasan, Md. Tasnim Jawad, Kazi Nasim Imtiaz Hasan, Sajal Basak Partha, Md. Masum Al Masba, Shumit Saha*

- `2102.06169v2` - [abs](http://arxiv.org/abs/2102.06169v2) - [pdf](http://arxiv.org/pdf/2102.06169v2)

> The novel COVID-19 is a global pandemic disease overgrowing worldwide. Computer-aided screening tools with greater sensitivity is imperative for disease diagnosis and prognosis as early as possible. It also can be a helpful tool in triage for testing and clinical supervision of COVID-19 patients. However, designing such an automated tool from non-invasive radiographic images is challenging as many manually annotated datasets are not publicly available yet, which is the essential core requirement of supervised learning schemes. This article proposes a 3D Convolutional Neural Network (CNN)-based classification approach considering both the inter- and intra-slice spatial voxel information. The proposed system is trained in an end-to-end manner on the 3D patches from the whole volumetric CT images to enlarge the number of training samples, performing the ablation studies on patch size determination. We integrate progressive resizing, segmentation, augmentations, and class-rebalancing to our 3D network. The segmentation is a critical prerequisite step for COVID-19 diagnosis enabling the classifier to learn prominent lung features while excluding the outer lung regions of the CT scans. We evaluate all the extensive experiments on a publicly available dataset, named MosMed, having binary- and multi-class chest CT image partitions. Our experimental results are very encouraging, yielding areas under the ROC curve of 0.914 and 0.893 for the binary- and multi-class tasks, respectively, applying 5-fold cross-validations. Our method's promising results delegate it as a favorable aiding tool for clinical practitioners and radiologists to assess COVID-19.

</details>

<details>

<summary>2021-04-14 21:43:19 - Addressing Visual Search in Open and Closed Set Settings</summary>

- *Nathan Drenkow, Philippe Burlina, Neil Fendley, Onyekachi Odoemene, Jared Markowitz*

- `2012.06509v2` - [abs](http://arxiv.org/abs/2012.06509v2) - [pdf](http://arxiv.org/pdf/2012.06509v2)

> Searching for small objects in large images is a task that is both challenging for current deep learning systems and important in numerous real-world applications, such as remote sensing and medical imaging. Thorough scanning of very large images is computationally expensive, particularly at resolutions sufficient to capture small objects. The smaller an object of interest, the more likely it is to be obscured by clutter or otherwise deemed insignificant. We examine these issues in the context of two complementary problems: closed-set object detection and open-set target search. First, we present a method for predicting pixel-level objectness from a low resolution gist image, which we then use to select regions for performing object detection locally at high resolution. This approach has the benefit of not being fixed to a predetermined grid, thereby requiring fewer costly high-resolution glimpses than existing methods. Second, we propose a novel strategy for open-set visual search that seeks to find all instances of a target class which may be previously unseen and is defined by a single image. We interpret both detection problems through a probabilistic, Bayesian lens, whereby the objectness maps produced by our method serve as priors in a maximum-a-posteriori approach to the detection step. We evaluate the end-to-end performance of both the combination of our patch selection strategy with this target search approach and the combination of our patch selection strategy with standard object detection methods. Both elements of our approach are seen to significantly outperform baseline strategies.

</details>

<details>

<summary>2021-04-15 04:54:48 - Vision Transformer using Low-level Chest X-ray Feature Corpus for COVID-19 Diagnosis and Severity Quantification</summary>

- *Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee, Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Jong Chul Ye*

- `2104.07235v1` - [abs](http://arxiv.org/abs/2104.07235v1) - [pdf](http://arxiv.org/pdf/2104.07235v1)

> Developing a robust algorithm to diagnose and quantify the severity of COVID-19 using Chest X-ray (CXR) requires a large number of well-curated COVID-19 datasets, which is difficult to collect under the global COVID-19 pandemic. On the other hand, CXR data with other findings are abundant. This situation is ideally suited for the Vision Transformer (ViT) architecture, where a lot of unlabeled data can be used through structural modeling by the self-attention mechanism. However, the use of existing ViT is not optimal, since feature embedding through direct patch flattening or ResNet backbone in the standard ViT is not intended for CXR. To address this problem, here we propose a novel Vision Transformer that utilizes low-level CXR feature corpus obtained from a backbone network that extracts common CXR findings. Specifically, the backbone network is first trained with large public datasets to detect common abnormal findings such as consolidation, opacity, edema, etc. Then, the embedded features from the backbone network are used as corpora for a Transformer model for the diagnosis and the severity quantification of COVID-19. We evaluate our model on various external test datasets from totally different institutions to evaluate the generalization capability. The experimental results confirm that our model can achieve the state-of-the-art performance in both diagnosis and severity quantification tasks with superior generalization capability, which are sine qua non of widespread deployment.

</details>

<details>

<summary>2021-04-16 02:09:01 - High-Quality Automated Program Repair</summary>

- *Manish Motwani*

- `2104.07851v1` - [abs](http://arxiv.org/abs/2104.07851v1) - [pdf](http://arxiv.org/pdf/2104.07851v1)

> Automatic program repair (APR) has recently gained attention because it proposes to fix software defects with no human intervention. To automatically fix defects, most APR tools use the developer-written tests to (a) localize the defect, and (b) generate and validate the automatically produced candidate patches based on the constraints imposed by the tests. While APR tools can produce patches that appear to fix the defect for 11-19% of the defects in real-world software, most of the patches produced are not correct or acceptable to developers because they overfit to the tests used during the repair process. This problem is known as the patch overfitting problem. To address this problem, I propose to equip APR tools with additional constraints derived from natural-language software artifacts such as bug reports and requirements specifications that describe the bug and intended software behavior but are not typically used by the APR tools. I hypothesize that patches produced by APR tools while using such additional constraints would be of higher quality. To test this hypothesis, I propose an automated and objective approach to evaluate the quality of patches and propose two novel methods to improve the fault localization and developer-written test suites using natural-language software artifacts. Finally, I propose to use my patch evaluation methodology to analyze the effect of the improved fault localization and test suites on the quality of patches produced by APR tools for real-world defects.

</details>

<details>

<summary>2021-04-19 02:41:15 - Unsupervised Shape Completion via Deep Prior in the Neural Tangent Kernel Perspective</summary>

- *Lei Chu, Hao Pan, Wenping Wang*

- `2104.09023v1` - [abs](http://arxiv.org/abs/2104.09023v1) - [pdf](http://arxiv.org/pdf/2104.09023v1)

> We present a novel approach for completing and reconstructing 3D shapes from incomplete scanned data by using deep neural networks. Rather than being trained on supervised completion tasks and applied on a testing shape, the network is optimized from scratch on the single testing shape, to fully adapt to the shape and complete the missing data using contextual guidance from the known regions. The ability to complete missing data by an untrained neural network is usually referred to as the deep prior. In this paper, we interpret the deep prior from a neural tangent kernel (NTK) perspective and show that the completed shape patches by the trained CNN are naturally similar to existing patches, as they are proximate in the kernel feature space induced by NTK. The interpretation allows us to design more efficient network structures and learning mechanisms for the shape completion and reconstruction task. Being more aware of structural regularities than both traditional and other unsupervised learning-based reconstruction methods, our approach completes large missing regions with plausible shapes and complements supervised learning-based methods that use database priors by requiring no extra training data set and showing flexible adaptation to a particular shape instance.

</details>

<details>

<summary>2021-04-19 09:30:56 - CPR: Classifier-Projection Regularization for Continual Learning</summary>

- *Sungmin Cha, Hsiang Hsu, Taebaek Hwang, Flavio P. Calmon, Taesup Moon*

- `2006.07326v2` - [abs](http://arxiv.org/abs/2006.07326v2) - [pdf](http://arxiv.org/pdf/2006.07326v2)

> We propose a general, yet simple patch that can be applied to existing regularization-based continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier's output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier's output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL.

</details>

<details>

<summary>2021-04-20 08:26:43 - Passive, Transparent, and Selective TLS Decryption for Network Security Monitoring</summary>

- *Florian Wilkens, Steffen Haas, Johanna Amann, Mathias Fischer*

- `2104.09828v1` - [abs](http://arxiv.org/abs/2104.09828v1) - [pdf](http://arxiv.org/pdf/2104.09828v1)

> Internet traffic is increasingly encrypted. While this protects the confidentiality and integrity of communication, it prevents network monitoring systems (NMS) and intrusion detection systems (IDSs) from effectively analyzing the now encrypted payloads. Therefore, many enterprise networks have deployed man-in-the-middle (MitM) proxies that intercept TLS connections at the network border to examine packet payloads and thus retain some visibility. However, recent studies have shown that TLS interception often reduces connection security and potentially introduces additional attack vectors to the network. In this paper, we present a cooperative approach in which end-hosts as cryptographic endpoints selectively provide TLS key material to NMS for decryption. This enables endpoints to control who can decrypt which content and lets users retain privacy for chosen connections. We implement a prototype based on the Zeek NMS that is able to receive key material from hosts, decrypt TLS connections and perform analyzes on the cleartext. The patch is freely available and we plan to upstream our changes to Zeek once they are mature enough. In our evaluation, we discuss how our approach conceptually requires significantly less computational resources compared to the commonly deployed MitM proxies. Our experimental results indicate, that TLS decryption increases a runtime overhead of about 2.5 times of the original runtime on cleartext. Additionally, we show that the latency for transmitting keys between hosts and the NMS can be effectively addressed by buffering traffic at the NMS for at least 40ms, allowing successful decryption of 99.99% of all observed TLS connections.

</details>

<details>

<summary>2021-04-20 10:18:57 - An Attention-based Weakly Supervised framework for Spitzoid Melanocytic Lesion Diagnosis in WSI</summary>

- *Rocío del Amor, Laëtitia Launet, Adrián Colomer, Anaïs Moscardó, Andrés Mosquera-Zamudio, Carlos Monteagudo, Valery Naranjo*

- `2104.09878v1` - [abs](http://arxiv.org/abs/2104.09878v1) - [pdf](http://arxiv.org/pdf/2104.09878v1)

> Melanoma is an aggressive neoplasm responsible for the majority of deaths from skin cancer. Specifically, spitzoid melanocytic tumors are one of the most challenging melanocytic lesions due to their ambiguous morphological features. The gold standard for its diagnosis and prognosis is the analysis of skin biopsies. In this process, dermatopathologists visualize skin histology slides under a microscope, in a high time-consuming and subjective task. In the last years, computer-aided diagnosis (CAD) systems have emerged as a promising tool that could support pathologists in daily clinical practice. Nevertheless, no automatic CAD systems have yet been proposed for the analysis of spitzoid lesions. Regarding common melanoma, no proposed system allows both the selection of the tumoral region and the prediction of the diagnosis as benign or malignant. Motivated by this, we propose a novel end-to-end weakly-supervised deep learning model, based on inductive transfer learning with an improved convolutional neural network (CNN) to refine the embedding features of the latent space. The framework is composed of a source model in charge of finding the tumor patch-level patterns, and a target model focuses on the specific diagnosis of a biopsy. The latter retrains the backbone of the source model through a multiple instance learning workflow to obtain the biopsy-level scoring. To evaluate the performance of the proposed methods, we perform extensive experiments on a private skin database with spitzoid lesions. Test results reach an accuracy of 0.9231 and 0.80 for the source and the target models, respectively. Besides, the heat map findings are directly in line with the clinicians' medical decision and even highlight, in some cases, patterns of interest that were overlooked by the pathologist due to the huge workload.

</details>

<details>

<summary>2021-04-20 15:12:30 - VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization</summary>

- *Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, Gian Luca Foresti*

- `2104.10036v1` - [abs](http://arxiv.org/abs/2104.10036v1) - [pdf](http://arxiv.org/pdf/2104.10036v1)

> We present a transformer-based image anomaly detection and localization network. Our proposed model is a combination of a reconstruction-based approach and patch embedding. The use of transformer networks helps to preserve the spatial information of the embedded patches, which are later processed by a Gaussian mixture density network to localize the anomalous areas. In addition, we also publish BTAD, a real-world industrial anomaly dataset. Our results are compared with other state-of-the-art algorithms using publicly available datasets like MNIST and MVTec.

</details>

<details>

<summary>2021-04-21 12:05:48 - Generating Adversarial yet Inconspicuous Patches with a Single Image</summary>

- *Jinqi Luo, Tao Bai, Jun Zhao*

- `2009.09774v2` - [abs](http://arxiv.org/abs/2009.09774v2) - [pdf](http://arxiv.org/pdf/2009.09774v2)

> Deep neural networks have been shown vulnerable toadversarial patches, where exotic patterns can resultin models wrong prediction. Nevertheless, existing ap-proaches to adversarial patch generation hardly con-sider the contextual consistency between patches andthe image background, causing such patches to be eas-ily detected and adversarial attacks to fail. On the otherhand, these methods require a large amount of data fortraining, which is computationally expensive. To over-come these challenges, we propose an approach to gen-erate adversarial yet inconspicuous patches with onesingle image. In our approach, adversarial patches areproduced in a coarse-to-fine way with multiple scalesof generators and discriminators. Contextual informa-tion is encoded during the Min-Max training to makepatches consistent with surroundings. The selection ofpatch location is based on the perceptual sensitivity ofvictim models. Through extensive experiments, our ap-proach shows strong attacking ability in both the white-box and black-box setting. Experiments on saliency de-tection and user evaluation indicate that our adversar-ial patches can evade human observations, demonstratethe inconspicuousness of our approach. Lastly, we showthat our approach preserves the attack ability in thephysical world.

</details>

<details>

<summary>2021-04-22 05:44:40 - Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box Vulnerabilities</summary>

- *Julia Rosenzweig, Joachim Sicking, Sebastian Houben, Michael Mock, Maram Akila*

- `2104.11691v1` - [abs](http://arxiv.org/abs/2104.11691v1) - [pdf](http://arxiv.org/pdf/2104.11691v1)

> An important pillar for safe machine learning (ML) is the systematic mitigation of weaknesses in neural networks to afford their deployment in critical applications. An ubiquitous class of safety risks are learned shortcuts, i.e. spurious correlations a network exploits for its decisions that have no semantic connection to the actual task. Networks relying on such shortcuts bear the risk of not generalizing well to unseen inputs. Explainability methods help to uncover such network vulnerabilities. However, many of these techniques are not directly applicable if access to the network is constrained, in so-called black-box setups. These setups are prevalent when using third-party ML components. To address this constraint, we present an approach to detect learned shortcuts using an interpretable-by-design network as a proxy to the black-box model of interest. Leveraging the proxy's guarantees on introspection we automatically extract candidates for learned shortcuts. Their transferability to the black box is validated in a systematic fashion. Concretely, as proxy model we choose a BagNet, which bases its decisions purely on local image patches. We demonstrate on the autonomous driving dataset A2D2 that extracted patch shortcuts significantly influence the black box model. By efficiently identifying such patch-based vulnerabilities, we contribute to safer ML models.

</details>

<details>

<summary>2021-04-22 08:57:18 - A Way Around UMIP and Descriptor-Table Exiting via TSX-based Side-Channel</summary>

- *Mohammad Sina Karvandi, Saleh Khalaj Monfared, Mohammad Sina Kiarostami, Dara Rahmati, Saeid Gorgin*

- `2005.10333v2` - [abs](http://arxiv.org/abs/2005.10333v2) - [pdf](http://arxiv.org/pdf/2005.10333v2)

> Nowadays, in operating systems, numerous protection mechanisms prevent or limit the user-mode applicationsto access the kernels internal information. This is regularlycarried out by software-based defenses such as Address Space Layout Randomization (ASLR) and Kernel ASLR(KASLR). They play pronounced roles when the security of sandboxed applications such as Web-browser are considered.Armed with arbitrary write access in the kernel memory, if these protections are bypassed, an adversary could find a suitable where to write in order to get an elevation of privilege or code execution in ring 0. In this paper, we introduce a reliable method based on Transactional Synchronization Extensions (TSX) side-channel leakage to reveal the address of the Global Descriptor Table (GDT) and Interrupt Descriptor Table (IDT). We indicate that by detecting these addresses, one could execute instructions to sidestep the Intels User-Mode InstructionPrevention (UMIP) and the Hypervisor-based mitigation and, consequently, neutralized them. The introduced method is successfully performed after the most recent patches for Meltdown and Spectre. Moreover, the implementation of the proposed approach on different platforms, including the latest releases of Microsoft Windows, Linux, and, Mac OSX with the latest 9th generation of Intel processors, shows that the proposed mechanism is independent from the Operating System implementation. We demonstrate that a combinationof this method with call-gate mechanism (available in modernprocessors) in a chain of events will eventually lead toa system compromise despite the limitations of a super-secure sandboxed environment in the presence of Windows proprietary Virtualization Based Security (VBS). Finally, we suggest the software-based mitigation to avoid these issues with an acceptable overhead cost.

</details>

<details>

<summary>2021-04-22 14:36:08 - Learning Transferable 3D Adversarial Cloaks for Deep Trained Detectors</summary>

- *Arman Maesumi, Mingkang Zhu, Yi Wang, Tianlong Chen, Zhangyang Wang, Chandrajit Bajaj*

- `2104.11101v1` - [abs](http://arxiv.org/abs/2104.11101v1) - [pdf](http://arxiv.org/pdf/2104.11101v1)

> This paper presents a novel patch-based adversarial attack pipeline that trains adversarial patches on 3D human meshes. We sample triangular faces on a reference human mesh, and create an adversarial texture atlas over those faces. The adversarial texture is transferred to human meshes in various poses, which are rendered onto a collection of real-world background images. Contrary to the traditional patch-based adversarial attacks, where prior work attempts to fool trained object detectors using appended adversarial patches, this new form of attack is mapped into the 3D object world and back-propagated to the texture atlas through differentiable rendering. As such, the adversarial patch is trained under deformation consistent with real-world materials. In addition, and unlike existing adversarial patches, our new 3D adversarial patch is shown to fool state-of-the-art deep object detectors robustly under varying views, potentially leading to an attacking scheme that is persistently strong in the physical world.

</details>

<details>

<summary>2021-04-22 15:42:55 - Topology Applied to Machine Learning: From Global to Local</summary>

- *Henry Adams, Michael Moy*

- `2103.05796v2` - [abs](http://arxiv.org/abs/2103.05796v2) - [pdf](http://arxiv.org/pdf/2103.05796v2)

> Through the use of examples, we explain one way in which applied topology has evolved since the birth of persistent homology in the early 2000s. The first applications of topology to data emphasized the global shape of a dataset, such as the three-circle model for $3 \times 3$ pixel patches from natural images, or the configuration space of the cyclo-octane molecule, which is a sphere with a Klein bottle attached via two circles of singularity. In these studies of global shape, short persistent homology bars are disregarded as sampling noise. More recently, however, persistent homology has been used to address questions about the local geometry of data. For instance, how can local geometry be vectorized for use in machine learning problems? Persistent homology and its vectorization methods, including persistence landscapes and persistence images, provide popular techniques for incorporating both local geometry and global topology into machine learning. Our meta-hypothesis is that the short bars are as important as the long bars for many machine learning tasks. In defense of this claim, we survey applications of persistent homology to shape recognition, agent-based modeling, materials science, archaeology, and biology. Additionally, we survey work connecting persistent homology to geometric features of spaces, including curvature and fractal dimension, and various methods that have been used to incorporate persistent homology into machine learning.

</details>

<details>

<summary>2021-04-23 17:49:15 - From Weakly Supervised Learning to Biquality Learning: an Introduction</summary>

- *Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornuéjols, Adam Ouorou*

- `2012.09632v3` - [abs](http://arxiv.org/abs/2012.09632v3) - [pdf](http://arxiv.org/pdf/2012.09632v3)

> The field of Weakly Supervised Learning (WSL) has recently seen a surge of popularity, with numerous papers addressing different types of "supervision deficiencies". In WSL use cases, a variety of situations exists where the collected "information" is imperfect. The paradigm of WSL attempts to list and cover these problems with associated solutions. In this paper, we review the research progress on WSL with the aim to make it as a brief introduction to this field. We present the three axis of WSL cube and an overview of most of all the elements of their facets. We propose three measurable quantities that acts as coordinates in the previously defined cube namely: Quality, Adaptability and Quantity of information. Thus we suggest that Biquality Learning framework can be defined as a plan of the WSL cube and propose to re-discover previously unrelated patches in WSL literature as a unified Biquality Learning literature.

</details>

<details>

<summary>2021-04-23 19:38:48 - Ensembles of GANs for synthetic training data generation</summary>

- *Gabriel Eilertsen, Apostolia Tsirikoglou, Claes Lundström, Jonas Unger*

- `2104.11797v1` - [abs](http://arxiv.org/abs/2104.11797v1) - [pdf](http://arxiv.org/pdf/2104.11797v1)

> Insufficient training data is a major bottleneck for most deep learning practices, not least in medical imaging where data is difficult to collect and publicly available datasets are scarce due to ethics and privacy. This work investigates the use of synthetic images, created by generative adversarial networks (GANs), as the only source of training data. We demonstrate that for this application, it is of great importance to make use of multiple GANs to improve the diversity of the generated data, i.e. to sufficiently cover the data distribution. While a single GAN can generate seemingly diverse image content, training on this data in most cases lead to severe over-fitting. We test the impact of ensembled GANs on synthetic 2D data as well as common image datasets (SVHN and CIFAR-10), and using both DCGANs and progressively growing GANs. As a specific use case, we focus on synthesizing digital pathology patches to provide anonymized training data.

</details>

<details>

<summary>2021-04-25 04:11:55 - The QXS-SAROPT Dataset for Deep Learning in SAR-Optical Data Fusion</summary>

- *Meiyu Huang, Yao Xu, Lixin Qian, Weili Shi, Yaqin Zhang, Wei Bao, Nan Wang, Xuejiao Liu, Xueshuang Xiang*

- `2103.08259v2` - [abs](http://arxiv.org/abs/2103.08259v2) - [pdf](http://arxiv.org/pdf/2103.08259v2)

> Deep learning techniques have made an increasing impact on the field of remote sensing. However, deep neural networks based fusion of multimodal data from different remote sensors with heterogenous characteristics has not been fully explored, due to the lack of availability of big amounts of perfectly aligned multi-sensor image data with diverse scenes of high resolutions, especially for synthetic aperture radar (SAR) data and optical imagery. To promote the development of deep learning based SAR-optical fusion approaches, we release the QXS-SAROPT dataset, which contains 20,000 pairs of SAR-optical image patches. We obtain the SAR patches from SAR satellite GaoFen-3 images and the optical patches from Google Earth images. These images cover three port cities: San Diego, Shanghai and Qingdao. Here, we present a detailed introduction of the construction of the dataset, and show its two representative exemplary applications, namely SAR-optical image matching and SAR ship detection boosted by cross-modal information from optical images. As a large open SAR-optical dataset with multiple scenes of a high resolution, we believe QXS-SAROPT will be of potential value for further research in SAR-optical data fusion technology based on deep learning.

</details>

<details>

<summary>2021-04-28 15:14:42 - Machine Learning based System for Vessel Turnaround Time Prediction</summary>

- *Dejan Stepec, Tomaz Martincic, Fabrice Klein, Daniel Vladusic, Joao Pita Costa*

- `2104.14980v1` - [abs](http://arxiv.org/abs/2104.14980v1) - [pdf](http://arxiv.org/pdf/2104.14980v1)

> In this paper, we present a novel system for predicting vessel turnaround time, based on machine learning and standardized port call data. We also investigate the use of specific external maritime big data, to enhance the accuracy of the available data and improve the performance of the developed system. An extensive evaluation is performed in Port of Bordeaux, where we report the results on 11 years of historical port call data and provide verification on live, operational data from the port. The proposed automated data-driven turnaround time prediction system is able to perform with increased accuracy, in comparison with the current manual expert-based system in Port of Bordeaux.

</details>

<details>

<summary>2021-04-28 23:20:06 - Generating Bug-Fixes Using Pretrained Transformers</summary>

- *Dawn Drain, Chen Wu, Alexey Svyatkovskiy, Neel Sundaresan*

- `2104.07896v2` - [abs](http://arxiv.org/abs/2104.07896v2) - [pdf](http://arxiv.org/pdf/2104.07896v2)

> Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.

</details>

<details>

<summary>2021-04-29 17:49:48 - A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection</summary>

- *Shelly Sheynin, Sagie Benaim, Lior Wolf*

- `2104.14535v1` - [abs](http://arxiv.org/abs/2104.14535v1) - [pdf](http://arxiv.org/pdf/2104.14535v1)

> Anomaly detection, the task of identifying unusual samples in data, often relies on a large set of training samples. In this work, we consider the setting of few-shot anomaly detection in images, where only a few images are given at training. We devise a hierarchical generative model that captures the multi-scale patch distribution of each training image. We further enhance the representation of our model by using image transformations and optimize scale-specific patch-discriminators to distinguish between real and fake patches of the image, as well as between different transformations applied to those patches. The anomaly score is obtained by aggregating the patch-based votes of the correct transformation across scales and image regions. We demonstrate the superiority of our method on both the one-shot and few-shot settings, on the datasets of Paris, CIFAR10, MNIST and FashionMNIST as well as in the setting of defect detection on MVTec. In all cases, our method outperforms the recent baseline methods.

</details>

<details>

<summary>2021-04-30 19:51:51 - Vessel and Port Efficiency Metrics through Validated AIS data</summary>

- *Tomaz Martincic, Dejan Stepec, Joao Pita Costa, Kristijan Cagran, Athanasios Chaldeakis*

- `2105.00063v1` - [abs](http://arxiv.org/abs/2105.00063v1) - [pdf](http://arxiv.org/pdf/2105.00063v1)

> Automatic Identification System (AIS) data represents a rich source of information about maritime traffic and offers a great potential for data analytics and predictive modeling solutions, which can help optimizing logistic chains and to reduce environmental impacts. In this work, we address the main limitations of the validity of AIS navigational data fields, by proposing a machine learning-based data-driven methodology to detect and (to the possible extent) also correct erroneous data. Additionally, we propose a metric that can be used by vessel operators and ports to express numerically their business and environmental efficiency through time and spatial dimensions, enabled with the obtained validated AIS data. We also demonstrate Port Area Vessel Movements (PARES) tool, which demonstrates the proposed solutions.

</details>


## 2021-05

<details>

<summary>2021-05-01 08:34:01 - VulDeeLocator: A Deep Learning-based Fine-grained Vulnerability Detector</summary>

- *Zhen Li, Deqing Zou, Shouhuai Xu, Zhaoxuan Chen, Yawei Zhu, Hai Jin*

- `2001.02350v2` - [abs](http://arxiv.org/abs/2001.02350v2) - [pdf](http://arxiv.org/pdf/2001.02350v2)

> Automatically detecting software vulnerabilities is an important problem that has attracted much attention from the academic research community. However, existing vulnerability detectors still cannot achieve the vulnerability detection capability and the locating precision that would warrant their adoption for real-world use. In this paper, we present a vulnerability detector that can simultaneously achieve a high detection capability and a high locating precision, dubbed Vulnerability Deep learning-based Locator (VulDeeLocator). In the course of designing VulDeeLocator, we encounter difficulties including how to accommodate semantic relations between the definitions of types as well as macros and their uses across files, how to accommodate accurate control flows and variable define-use relations, and how to achieve high locating precision. We solve these difficulties by using two innovative ideas: (i) leveraging intermediate code to accommodate extra semantic information, and (ii) using the notion of granularity refinement to pin down locations of vulnerabilities. When applied to 200 files randomly selected from three real-world software products, VulDeeLocator detects 18 confirmed vulnerabilities (i.e., true-positives). Among them, 16 vulnerabilities correspond to known vulnerabilities; the other two are not reported in the National Vulnerability Database (NVD) but have been "silently" patched by the vendor of Libav when releasing newer versions.

</details>

<details>

<summary>2021-05-01 16:18:46 - Learning to Learn to Compress</summary>

- *Nannan Zou, Honglei Zhang, Francesco Cricri, Hamed R. Tavakoli, Jani Lainema, Miska Hannuksela, Emre Aksu, Esa Rahtu*

- `2007.16054v2` - [abs](http://arxiv.org/abs/2007.16054v2) - [pdf](http://arxiv.org/pdf/2007.16054v2)

> In this paper we present an end-to-end meta-learned system for image compression. Traditional machine learning based approaches to image compression train one or more neural network for generalization performance. However, at inference time, the encoder or the latent tensor output by the encoder can be optimized for each test image. This optimization can be regarded as a form of adaptation or benevolent overfitting to the input content. In order to reduce the gap between training and inference conditions, we propose a new training paradigm for learned image compression, which is based on meta-learning. In a first phase, the neural networks are trained normally. In a second phase, the Model-Agnostic Meta-learning approach is adapted to the specific case of image compression, where the inner-loop performs latent tensor overfitting, and the outer loop updates both encoder and decoder neural networks based on the overfitting performance. Furthermore, after meta-learning, we propose to overfit and cluster the bias terms of the decoder on training image patches, so that at inference time the optimal content-specific bias terms can be selected at encoder-side. Finally, we propose a new probability model for lossless compression, which combines concepts from both multi-scale and super-resolution probability model approaches. We show the benefits of all our proposed ideas via carefully designed experiments.

</details>

<details>

<summary>2021-05-03 09:38:26 - The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges</summary>

- *Beatrice Alex, Clare Llewellyn, Pawel Michal Orzechowski, Maria Boutchkova*

- `2105.07847v1` - [abs](http://arxiv.org/abs/2105.07847v1) - [pdf](http://arxiv.org/pdf/2105.07847v1)

> In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students' learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.

</details>

<details>

<summary>2021-05-04 16:25:36 - Orienting Point Clouds with Dipole Propagation</summary>

- *Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo, Daniel Cohen-Or*

- `2105.01604v1` - [abs](http://arxiv.org/abs/2105.01604v1) - [pdf](http://arxiv.org/pdf/2105.01604v1)

> Establishing a consistent normal orientation for point clouds is a notoriously difficult problem in geometry processing, requiring attention to both local and global shape characteristics. The normal direction of a point is a function of the local surface neighborhood; yet, point clouds do not disclose the full underlying surface structure. Even assuming known geodesic proximity, calculating a consistent normal orientation requires the global context. In this work, we introduce a novel approach for establishing a globally consistent normal orientation for point clouds. Our solution separates the local and global components into two different sub-problems. In the local phase, we train a neural network to learn a coherent normal direction per patch (i.e., consistently oriented normals within a single patch). In the global phase, we propagate the orientation across all coherent patches using a dipole propagation. Our dipole propagation decides to orient each patch using the electric field defined by all previously orientated patches. This gives rise to a global propagation that is stable, as well as being robust to nearby surfaces, holes, sharp features and noise.

</details>

<details>

<summary>2021-05-07 07:11:29 - Automated Patch Assessment for Program Repair at Scale</summary>

- *He Ye, Matias Martinez, Martin Monperrus*

- `1909.13694v3` - [abs](http://arxiv.org/abs/1909.13694v3) - [pdf](http://arxiv.org/pdf/1909.13694v3)

> In this paper, we do automatic correctness assessment for patches generated by program repair systems. We consider the human-written patch as ground truth oracle and randomly generate tests based on it, a technique proposed by Shamshiri et al., called Random testing with Ground Truth (RGT) in this paper. We build a curated dataset of 638 patches for Defects4J generated by 14 state-of-the-art repair systems, we evaluate automated patch assessment on this dataset. The results of this study are novel and significant: First, we improve the state of the art performance of automatic patch assessment with RGT by 190% by improving the oracle; Second, we show that RGT is reliable enough to help scientists to do overfitting analysis when they evaluate program repair systems; Third, we improve the external validity of the program repair knowledge with the largest study ever.

</details>

<details>

<summary>2021-05-10 08:50:41 - Image analysis for Alzheimer's disease prediction: Embracing pathological hallmarks for model architecture design</summary>

- *Sarah C. Brüningk, Felix Hensel, Catherine R. Jutzeler, Bastian Rieck*

- `2011.06531v3` - [abs](http://arxiv.org/abs/2011.06531v3) - [pdf](http://arxiv.org/pdf/2011.06531v3)

> Alzheimer's disease (AD) is associated with local (e.g. brain tissue atrophy) and global brain changes (loss of cerebral connectivity), which can be detected by high-resolution structural magnetic resonance imaging. Conventionally, these changes and their relation to AD are investigated independently. Here, we introduce a novel, highly-scalable approach that simultaneously captures $\textit{local}$ and $\textit{global}$ changes in the diseased brain. It is based on a neural network architecture that combines patch-based, high-resolution 3D-CNNs with global topological features, evaluating multi-scale brain tissue connectivity. Our local-global approach reached competitive results with an average precision score of $0.95\pm0.03$ for the classification of cognitively normal subjects and AD patients (prevalence $\approx 55\%$).

</details>

<details>

<summary>2021-05-10 14:22:09 - Why Aren't Regular Expressions a Lingua Franca? An Empirical Study on the Re-use and Portability of Regular Expressions</summary>

- *James C. Davis, Louis G. Michael IV, Christy A. Coghlan, Francisco Servant, Dongyoon Lee*

- `2105.04397v1` - [abs](http://arxiv.org/abs/2105.04397v1) - [pdf](http://arxiv.org/pdf/2105.04397v1)

> This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics?   In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language. We experimentally evaluated the riskiness of this practice using a novel regex corpus -- 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences.   We report that developers' belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.

</details>

<details>

<summary>2021-05-10 19:00:49 - Enhancing Photorealism Enhancement</summary>

- *Stephan R. Richter, Hassan Abu AlHaija, Vladlen Koltun*

- `2105.04619v1` - [abs](http://arxiv.org/abs/2105.04619v1) - [pdf](http://arxiv.org/pdf/2105.04619v1)

> We present an approach to enhancing the realism of synthetic images. The images are enhanced by a convolutional network that leverages intermediate representations produced by conventional rendering pipelines. The network is trained via a novel adversarial objective, which provides strong supervision at multiple perceptual levels. We analyze scene layout distributions in commonly used datasets and find that they differ in important ways. We hypothesize that this is one of the causes of strong artifacts that can be observed in the results of many prior methods. To address this we propose a new strategy for sampling image patches during training. We also introduce multiple architectural improvements in the deep network modules used for photorealism enhancement. We confirm the benefits of our contributions in controlled experiments and report substantial gains in stability and realism in comparison to recent image-to-image translation methods and a variety of other baselines.

</details>

<details>

<summary>2021-05-11 21:17:13 - Unsupervised Representation Learning from Pathology Images with Multi-directional Contrastive Predictive Coding</summary>

- *Jacob Carse, Frank Carey, Stephen McKenna*

- `2105.05345v1` - [abs](http://arxiv.org/abs/2105.05345v1) - [pdf](http://arxiv.org/pdf/2105.05345v1)

> Digital pathology tasks have benefited greatly from modern deep learning algorithms. However, their need for large quantities of annotated data has been identified as a key challenge. This need for data can be countered by using unsupervised learning in situations where data are abundant but access to annotations is limited. Feature representations learned from unannotated data using contrastive predictive coding (CPC) have been shown to enable classifiers to obtain state of the art performance from relatively small amounts of annotated computer vision data. We present a modification to the CPC framework for use with digital pathology patches. This is achieved by introducing an alternative mask for building the latent context and using a multi-directional PixelCNN autoregressor. To demonstrate our proposed method we learn feature representations from the Patch Camelyon histology dataset. We show that our proposed modification can yield improved deep classification of histology patches.

</details>

<details>

<summary>2021-05-13 10:50:13 - Adaptive Test-Time Augmentation for Low-Power CPU</summary>

- *Luca Mocerino, Roberto G. Rizzo, Valentino Peluso, Andrea Calimera, Enrico Macii*

- `2105.06183v1` - [abs](http://arxiv.org/abs/2105.06183v1) - [pdf](http://arxiv.org/pdf/2105.06183v1)

> Convolutional Neural Networks (ConvNets) are trained offline using the few available data and may therefore suffer from substantial accuracy loss when ported on the field, where unseen input patterns received under unpredictable external conditions can mislead the model. Test-Time Augmentation (TTA) techniques aim to alleviate such common side effect at inference-time, first running multiple feed-forward passes on a set of altered versions of the same input sample, and then computing the main outcome through a consensus of the aggregated predictions. Unfortunately, the implementation of TTA on embedded CPUs introduces latency penalties that limit its adoption on edge applications. To tackle this issue, we propose AdapTTA, an adaptive implementation of TTA that controls the number of feed-forward passes dynamically, depending on the complexity of the input. Experimental results on state-of-the-art ConvNets for image classification deployed on a commercial ARM Cortex-A CPU demonstrate AdapTTA reaches remarkable latency savings, from 1.49X to 2.21X, and hence a higher frame rate compared to static TTA, still preserving the same accuracy gain.

</details>

<details>

<summary>2021-05-20 18:13:58 - Wide & Deep neural network model for patch aggregation in CNN-based prostate cancer detection systems</summary>

- *Lourdes Duran-Lopez, Juan P. Dominguez-Morales, Daniel Gutierrez-Galan, Antonio Rios-Navarro, Angel Jimenez-Fernandez, Saturnino Vicente-Diaz, Alejandro Linares-Barranco*

- `2105.09974v1` - [abs](http://arxiv.org/abs/2105.09974v1) - [pdf](http://arxiv.org/pdf/2105.09974v1)

> Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of the leading causes of death among men, with almost 1.41 million new cases and around 375,000 deaths in 2020. Artificial Intelligence algorithms have had a huge impact in medical image analysis, including digital histopathology, where Convolutional Neural Networks (CNNs) are used to provide a fast and accurate diagnosis, supporting experts in this task. To perform an automatic diagnosis, prostate tissue samples are first digitized into gigapixel-resolution whole-slide images. Due to the size of these images, neural networks cannot use them as input and, therefore, small subimages called patches are extracted and predicted, obtaining a patch-level classification. In this work, a novel patch aggregation method based on a custom Wide & Deep neural network model is presented, which performs a slide-level classification using the patch-level classes obtained from a CNN. The malignant tissue ratio, a 10-bin malignant probability histogram, the least squares regression line of the histogram, and the number of malignant connected components are used by the proposed model to perform the classification. An accuracy of 94.24% and a sensitivity of 98.87% were achieved, proving that the proposed system could aid pathologists by speeding up the screening process and, thus, contribute to the fight against PCa.

</details>

<details>

<summary>2021-05-20 19:16:04 - Document Domain Randomization for Deep Learning Document Layout Extraction</summary>

- *Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S. Laramee, Han-Wei Shen, Jian Wu, C. Lee Giles*

- `2105.14931v1` - [abs](http://arxiv.org/abs/2105.14931v1) - [pdf](http://arxiv.org/pdf/2105.14931v1)

> We present document domain randomization (DDR), the first successful transfer of convolutional neural networks (CNNs) trained only on graphically rendered pseudo-paper pages to real-world document segmentation. DDR renders pseudo-document pages by modeling randomized textual and non-textual contents of interest, with user-defined layout and font styles to support joint learning of fine-grained classes. We demonstrate competitive results using our DDR approach to extract nine document classes from the benchmark CS-150 and papers published in two domains, namely annual meetings of Association for Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to conditions of style mismatch, fewer or more noisy samples that are more easily obtained in the real world. We show that high-fidelity semantic information is not necessary to label semantic classes but style mismatch between train and test can lower model accuracy. Using smaller training samples had a slightly detrimental effect. Finally, network models still achieved high test accuracy when correct labels are diluted towards confusing labels; this behavior hold across several classes.

</details>

<details>

<summary>2021-05-21 09:08:34 - GAN pretraining for deep convolutional autoencoders applied to Software-based Fingerprint Presentation Attack Detection</summary>

- *Tobias Rohrer, Jascha Kolberg*

- `2105.10213v1` - [abs](http://arxiv.org/abs/2105.10213v1) - [pdf](http://arxiv.org/pdf/2105.10213v1)

> The need for reliable systems to determine fingerprint presentation attacks grows with the rising use of the fingerprint for authentication. This work presents a new approach to single-class classification for software-based fingerprint presentation attach detection. The described method utilizes a Wasserstein GAN to apply transfer learning to a deep convolutional autoencoder. By doing so, the autoencoder could be pretrained and finetuned on the LivDet2021 Dermalog sensor dataset with only 1122 bona fide training samples. Without making use of any presentation attack samples, the model could archive an average classification error rate of 16.79%. The Wasserstein GAN implemented to pretrain the autoencoders weights can further be used to generate realistic-looking artificial fingerprint patches. Extensive testing of different autoencoder architectures and hyperparameters led to coarse architectural guidelines as well as multiple implementations which can be utilized for future work.

</details>

<details>

<summary>2021-05-22 23:33:20 - Real-time Detection of Practical Universal Adversarial Perturbations</summary>

- *Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Emil C. Lupu*

- `2105.07334v2` - [abs](http://arxiv.org/abs/2105.07334v2) - [pdf](http://arxiv.org/pdf/2105.07334v2)

> Universal Adversarial Perturbations (UAPs) are a prominent class of adversarial examples that exploit the systemic vulnerabilities and enable physically realizable and robust attacks against Deep Neural Networks (DNNs). UAPs generalize across many different inputs; this leads to realistic and effective attacks that can be applied at scale. In this paper we propose HyperNeuron, an efficient and scalable algorithm that allows for the real-time detection of UAPs by identifying suspicious neuron hyper-activations. Our results show the effectiveness of HyperNeuron on multiple tasks (image classification, object detection), against a wide variety of universal attacks, and in realistic scenarios, like perceptual ad-blocking and adversarial patches. HyperNeuron is able to simultaneously detect both adversarial mask and patch UAPs with comparable or better performance than existing UAP defenses whilst introducing a significantly reduced latency of only 0.86 milliseconds per image. This suggests that many realistic and practical universal attacks can be reliably mitigated in real-time, which shows promise for the robust deployment of machine learning systems.

</details>

<details>

<summary>2021-05-27 13:30:17 - A framework for data-driven solution and parameter estimation of PDEs using conditional generative adversarial networks</summary>

- *Teeratorn Kadeethum, Daniel O'Malley, Jan Niklas Fuhg, Youngsoo Choi, Jonghyun Lee, Hari S. Viswanathan, Nikolaos Bouklas*

- `2105.13136v1` - [abs](http://arxiv.org/abs/2105.13136v1) - [pdf](http://arxiv.org/pdf/2105.13136v1)

> This work is the first to employ and adapt the image-to-image translation concept based on conditional generative adversarial networks (cGAN) towards learning a forward and an inverse solution operator of partial differential equations (PDEs). Even though the proposed framework could be applied as a surrogate model for the solution of any PDEs, here we focus on steady-state solutions of coupled hydro-mechanical processes in heterogeneous porous media. Strongly heterogeneous material properties, which translate to the heterogeneity of coefficients of the PDEs and discontinuous features in the solutions, require specialized techniques for the forward and inverse solution of these problems. Additionally, parametrization of the spatially heterogeneous coefficients is excessively difficult by using standard reduced order modeling techniques. In this work, we overcome these challenges by employing the image-to-image translation concept to learn the forward and inverse solution operators and utilize a U-Net generator and a patch-based discriminator. Our results show that the proposed data-driven reduced order model has competitive predictive performance capabilities in accuracy and computational efficiency as well as training time requirements compared to state-of-the-art data-driven methods for both forward and inverse problems.

</details>

<details>

<summary>2021-05-28 07:09:48 - WeaQA: Weak Supervision via Captions for Visual Question Answering</summary>

- *Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Chitta Baral*

- `2012.02356v2` - [abs](http://arxiv.org/abs/2012.02356v2) - [pdf](http://arxiv.org/pdf/2012.02356v2)

> Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated \textit{Image-Question-Answer} (I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic Q-A pairs generated procedurally from captions. Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models. Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors.

</details>

<details>

<summary>2021-05-29 00:01:08 - Log2NS: Enhancing Deep Learning Based Analysis of Logs With Formal to Prevent Survivorship Bias</summary>

- *Charanraj Thimmisetty, Praveen Tiwari, Didac Gil de la Iglesia, Nandini Ramanan, Marjorie Sayer, Viswesh Ananthakrishnan, Claudionor Nunes Coelho Jr*

- `2105.14149v1` - [abs](http://arxiv.org/abs/2105.14149v1) - [pdf](http://arxiv.org/pdf/2105.14149v1)

> Analysis of large observational data sets generated by a reactive system is a common challenge in debugging system failures and determining their root cause. One of the major problems is that these observational data suffer from survivorship bias. Examples include analyzing traffic logs from networks, and simulation logs from circuit design. In such applications, users want to detect non-spurious correlations from observational data and obtain actionable insights about them. In this paper, we introduce log to Neuro-symbolic (Log2NS), a framework that combines probabilistic analysis from machine learning (ML) techniques on observational data with certainties derived from symbolic reasoning on an underlying formal model. We apply the proposed framework to network traffic debugging by employing the following steps. To detect patterns in network logs, we first generate global embedding vector representations of entities such as IP addresses, ports, and applications. Next, we represent large log flow entries as clusters that make it easier for the user to visualize and detect interesting scenarios that will be further analyzed. To generalize these patterns, Log2NS provides an ability to query from static logs and correlation engines for positive instances, as well as formal reasoning for negative and unseen instances. By combining the strengths of deep learning and symbolic methods, Log2NS provides a very powerful reasoning and debugging tool for log-based data. Empirical evaluations on a real internal data set demonstrate the capabilities of Log2NS.

</details>

<details>

<summary>2021-05-29 13:33:52 - A Measurement Study on the (In)security of End-of-Life (EoL) Embedded Devices</summary>

- *Dingding Wang, Muhui Jiang, Rui Chang, Yajin Zhou, Baolei Hou, Xiapu Luo, Lei Wu, Kui Ren*

- `2105.14298v1` - [abs](http://arxiv.org/abs/2105.14298v1) - [pdf](http://arxiv.org/pdf/2105.14298v1)

> Embedded devices are becoming popular. Meanwhile, researchers are actively working on improving the security of embedded devices. However, previous work ignores the insecurity caused by a special category of devices, i.e., the End-of-Life (EoL in short) devices. Once a product becomes End-of-Life, vendors tend to no longer maintain its firmware or software, including providing bug fixes and security patches. This makes EoL devices susceptible to attacks. For instance, a report showed that an EoL model with thousands of active devices was exploited to redirect web traffic for malicious purposes. In this paper, we conduct the first measurement study to shed light on the (in)security of EoL devices. To this end, our study performs two types of analysis, including the aliveness analysis and the vulnerability analysis. The first one aims to detect the scale of EoL devices that are still alive. The second one is to evaluate the vulnerabilities existing in (active) EoL devices. We have applied our approach to a large number of EoL models from three vendors (i.e., D-Link, Tp-Link, and Netgear) and detect the alive devices in a time period of ten months. Our study reveals some worrisome facts that were unknown by the community. For instance, there exist more than 2 million active EoL devices. Nearly 300,000 of them are still alive even after five years since they became EoL. Although vendors may release security patches after the EoL date, however, the process is ad hoc and incomplete. As a result, more than 1 million active EoL devices are vulnerable, and nearly half of them are threatened by high-risk vulnerabilities. Attackers can achieve a minimum of 2.79 Tbps DDoS attack by compromising a large number of active EoL devices. We believe these facts pose a clear call for more attention to deal with the security issues of EoL devices.

</details>

<details>

<summary>2021-05-31 17:51:25 - A Quasipolynomial $(2+\varepsilon)$-Approximation for Planar Sparsest Cut</summary>

- *Vincent Cohen-Addad, Anupam Gupta, Philip N. Klein, Jason Li*

- `2105.15187v1` - [abs](http://arxiv.org/abs/2105.15187v1) - [pdf](http://arxiv.org/pdf/2105.15187v1)

> The (non-uniform) sparsest cut problem is the following graph-partitioning problem: given a "supply" graph, and demands on pairs of vertices, delete some subset of supply edges to minimize the ratio of the supply edges cut to the total demand of the pairs separated by this deletion. Despite much effort, there are only a handful of nontrivial classes of supply graphs for which constant-factor approximations are known.   We consider the problem for planar graphs, and give a $(2+\varepsilon)$-approximation algorithm that runs in quasipolynomial time. Our approach defines a new structural decomposition of an optimal solution using a "patching" primitive. We combine this decomposition with a Sherali-Adams-style linear programming relaxation of the problem, which we then round. This should be compared with the polynomial-time approximation algorithm of Rao (1999), which uses the metric linear programming relaxation and $\ell_1$-embeddings, and achieves an $O(\sqrt{\log n})$-approximation in polynomial time.

</details>


## 2021-06

<details>

<summary>2021-06-02 13:44:59 - Controlled Update of Software Components using Concurrent Exection of Patched and Unpatched Versions</summary>

- *Stjepan Groš, Ivan Kovačević, Ivan Dujmić, Matej Petrinović*

- `2106.01154v1` - [abs](http://arxiv.org/abs/2106.01154v1) - [pdf](http://arxiv.org/pdf/2106.01154v1)

> Software patching is a common method of removing vulnerabilities in software components to make IT systems more secure. However, there are many cases where software patching is not possible due to the critical nature of the application, especially when the vendor providing the application guarantees correct operation only in a specific configuration. In this paper, we propose a method to solve this problem. The idea is to run unpatched and patched application instances concurrently, with the unpatched one having complete control and the output of the patched one being used only for comparison, to watch for differences that are consequences of introduced bugs. To test this idea, we developed a system that allows us to run web applications in parallel and tested three web applications. The experiments have shown that the idea is promising for web applications from the technical side. Furthermore, we discuss the potential limitations of this system and the idea in general, how long two instances should run in order to be able to claim with some probability that the patched version has not introduced any new bugs, other potential use cases of the proposed system where two application instances run concurrently, and finally the potential uses of this system with different types of applications, such as SCADA systems.

</details>

<details>

<summary>2021-06-02 15:21:53 - IPatch: A Remote Adversarial Patch</summary>

- *Yisroel Mirsky*

- `2105.00113v2` - [abs](http://arxiv.org/abs/2105.00113v2) - [pdf](http://arxiv.org/pdf/2105.00113v2)

> Applications such as autonomous vehicles and medical screening use deep learning models to localize and identify hundreds of objects in a single frame. In the past, it has been shown how an attacker can fool these models by placing an adversarial patch within a scene. However, these patches must be placed in the target location and do not explicitly alter the semantics elsewhere in the image.   In this paper, we introduce a new type of adversarial patch which alters a model's perception of an image's semantics. These patches can be placed anywhere within an image to change the classification or semantics of locations far from the patch. We call this new class of adversarial examples `remote adversarial patches' (RAP).   We implement our own RAP called IPatch and perform an in-depth analysis on image segmentation RAP attacks using five state-of-the-art architectures with eight different encoders on the CamVid street view dataset. Moreover, we demonstrate that the attack can be extended to object recognition models with preliminary results on the popular YOLOv3 model. We found that the patch can change the classification of a remote target region with a success rate of up to 93% on average.

</details>

<details>

<summary>2021-06-03 11:39:56 - Dynamic Analysis of ARINC 653 RTOS with LLVM</summary>

- *Vitaly Cheptsov, Alexey Khoroshilov*

- `2106.01766v1` - [abs](http://arxiv.org/abs/2106.01766v1) - [pdf](http://arxiv.org/pdf/2106.01766v1)

> Existing standards for airborne-embedded software systems impose a number of requirements applicable to the software development cycle of hard real-time operating systems found in modern aircraft. The measures taken are meant to reduce the risks of undesired consequences, but have strongly varying costs. Dynamic instrumentation and static analysis are common practices used to automatically find software defects, from strictly non-conforming code constructions to memory corruptions or invalid control flow. LLVM analyser and sanitizer infrastructure, while regularly applied to general-purpose software, originally was not thought to be introduced to heavily restricted environments. In this paper we discuss the specifics of airborne systems with regards to dynamic instrumentation and provide practical considerations to be taken into account for the effective use of general-purpose instrumentation tools. We bring a complete LLVM stack support to JetOS, a prospective onboard real-time operating system currently being developed at ISP RAS in collaboration with GosNIIAS. As an example, we port AddressSanitizer, MemorySanitizer, and UndefinedBehaviorSanitizer and provide the details against the caveats on all relevant sides: a sanitizer, a compiler, and an operating system. In addition we suggest uninvolved optimisations and enhancements to the runtimes to maximise the effects of the tools.

</details>

<details>

<summary>2021-06-03 13:08:56 - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</summary>

- *Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby*

- `2010.11929v2` - [abs](http://arxiv.org/abs/2010.11929v2) - [pdf](http://arxiv.org/pdf/2010.11929v2)

> While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.

</details>

<details>

<summary>2021-06-04 07:38:54 - Automatic Patch Linkage Detection in Code Review Using TextualContent and File Location Features</summary>

- *Dong Wang, Raula Gaikovina Kula, Takashi Ishio, Kenichi Matsumoto*

- `2106.02306v1` - [abs](http://arxiv.org/abs/2106.02306v1) - [pdf](http://arxiv.org/pdf/2106.02306v1)

> Context: Contemporary code review tools are a popular choice for software quality assurance. Using these tools, reviewers are able to post a linkage between two patches during a review discussion. Large development teams that use a review-then-commit model risk being unaware of these linkages. Objective: Our objective is to first explore how patch linkage impacts the review process. We then propose and evaluate models that detect patch linkage based on realistic time intervals. Method: First, we carry out an exploratory study on three open source projects to conduct linkage impact analysis using 942 manually classified linkages. Second, we propose two techniques using textual and file location similarity to build detection models and evaluate their performance. Results: The study provides evidence of latency in the linkage notification. We show that a patch with the Alternative Solution linkage (i.e., patches that implement similar functionality)undergoes a quicker review and avoids additional revisions after the team has been notified, compared to other linkage types. Our detection model experiments show promising recall rates for the Alternative Solution linkage (from 32% to 95%), but precision has room for improvement. Conclusion: Patch linkage detection is promising, with likely improvements if the practice of posting linkages becomes more prevalent. From our implications, this paper lays the groundwork for future research on how to increase patch linkage awareness to facilitate efficient reviews.

</details>

<details>

<summary>2021-06-04 07:56:43 - Machine Learning Based Texture Analysis of Patella from X-Rays for Detecting Patellofemoral Osteoarthritis</summary>

- *Neslihan Bayramoglu, Miika T. Nieminen, Simo Saarakkala*

- `2106.01700v2` - [abs](http://arxiv.org/abs/2106.01700v2) - [pdf](http://arxiv.org/pdf/2106.01700v2)

> Objective is to assess the ability of texture features for detecting radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view radiographs. We used lateral view knee radiographs from MOST public use datasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically detected using landmark detection tool (BoneFinder). Hand-crafted features, based on LocalBinary Patterns (LBP), were then extracted to describe the patellar texture. First, a machine learning model (Gradient Boosting Machine) was trained to detect radiographic PFOA from the LBP features. Furthermore, we used end-to-end trained deep convolutional neural networks (CNNs) directly on the texture patches for detecting the PFOA. The proposed classification models were eventually compared with more conventional reference models that use clinical assessments and participant characteristics such as age, sex, body mass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL) grade. Atlas-guided visual assessment of PFOA status by expert readers provided in the MOST public use datasets was used as a classification outcome for the models. Performance of prediction models was assessed using the area under the receiver operating characteristic curve (ROC AUC), the area under the precision-recall (PR) curve-average precision (AP)-, and Brier score in the stratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had PFOA. AUC and AP for the strongest reference model including age, sex, BMI, WOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487, respectively. Textural ROI classification using CNN significantly improved the prediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study that analyses patellar bone texture for diagnosing PFOA. Our results demonstrates the potential of using texture features of patella to predict PFOA.

</details>

<details>

<summary>2021-06-04 08:19:44 - Temporally coherent video anonymization through GAN inpainting</summary>

- *Thangapavithraa Balaji, Patrick Blies, Georg Göri, Raphael Mitsch, Marcel Wasserer, Torsten Schön*

- `2106.02328v1` - [abs](http://arxiv.org/abs/2106.02328v1) - [pdf](http://arxiv.org/pdf/2106.02328v1)

> This work tackles the problem of temporally coherent face anonymization in natural video streams.We propose JaGAN, a two-stage system starting with detecting and masking out faces with black image patches in all individual frames of the video. The second stage leverages a privacy-preserving Video Generative Adversarial Network designed to inpaint the missing image patches with artificially generated faces. Our initial experiments reveal that image based generative models are not capable of inpainting patches showing temporal coherent appearance across neighboring video frames. To address this issue we introduce a newly curated video collection, which is made publicly available for the research community along with this paper. We also introduce the Identity Invariance Score IdI as a means to quantify temporal coherency between neighboring frames.

</details>

<details>

<summary>2021-06-05 10:43:58 - An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification</summary>

- *Suvidha Tripathi, Satish Kumar Singh, Hwee Kuan Lee*

- `2106.02864v1` - [abs](http://arxiv.org/abs/2106.02864v1) - [pdf](http://arxiv.org/pdf/2106.02864v1)

> Researchers working on computational analysis of Whole Slide Images (WSIs) in histopathology have primarily resorted to patch-based modelling due to large resolution of each WSI. The large resolution makes WSIs infeasible to be fed directly into the machine learning models due to computational constraints. However, due to patch-based analysis, most of the current methods fail to exploit the underlying spatial relationship among the patches. In our work, we have tried to integrate this relationship along with feature-based correlation among the extracted patches from the particular tumorous region. For the given task of classification, we have used BiLSTMs to model both forward and backward contextual relationship. RNN based models eliminate the limitation of sequence size by allowing the modelling of variable size images within a deep learning model. We have also incorporated the effect of spatial continuity by exploring different scanning techniques used to sample patches. To establish the efficiency of our approach, we trained and tested our model on two datasets, microscopy images and WSI tumour regions. After comparing with contemporary literature we achieved the better performance with accuracy of 90% for microscopy image dataset. For WSI tumour region dataset, we compared the classification results with deep learning networks such as ResNet, DenseNet, and InceptionV3 using maximum voting technique. We achieved the highest performance accuracy of 84%. We found out that BiLSTMs with CNN features have performed much better in modelling patches into an end-to-end Image classification network. Additionally, the variable dimensions of WSI tumour regions were used for classification without the need for resizing. This suggests that our method is independent of tumour image size and can process large dimensional images without losing the resolution details.

</details>

<details>

<summary>2021-06-06 14:00:38 - SPI: Automated Identification of Security Patches via Commits</summary>

- *Yaqin Zhou, Jing Kai Siow, Chenyu Wang, Shangqing Liu, Yang Liu*

- `2105.14565v2` - [abs](http://arxiv.org/abs/2105.14565v2) - [pdf](http://arxiv.org/pdf/2105.14565v2)

> Security patches in open-source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyberattacks. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open-source libraries that are heavily relied on by developers. An extensive security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researchers, e.g., aiding in vulnerability research. To curate security patches including undisclosed patches at a large scale and low cost, we propose a deep neural-network-based approach built upon commits of open-source repositories. We build security patch datasets that include 38,291 security-related commits and 1,045 CVE patches from four C libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security-related. We devise a deep learning-based security patch identification system that consists of two neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset; and one code-revision neural network that takes code before and after revision and learns the distinction on the statement level. Our evaluation results show that our system outperforms SVM and K-fold stacking algorithm, achieving as high as 87.93% F1-score and precision of 86.24%. We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionality. Our experiment results and observation proved that our approach identifies security patches effectively among open-sourced projects.

</details>

<details>

<summary>2021-06-07 14:42:51 - Hierarchical Robot Navigation in Novel Environments using Rough 2-D Maps</summary>

- *Chengguang Xu, Christopher Amato, Lawson L. S. Wong*

- `2106.03665v1` - [abs](http://arxiv.org/abs/2106.03665v1) - [pdf](http://arxiv.org/pdf/2106.03665v1)

> In robot navigation, generalizing quickly to unseen environments is essential. Hierarchical methods inspired by human navigation have been proposed, typically consisting of a high-level landmark proposer and a low-level controller. However, these methods either require precise high-level information to be given in advance or need to construct such guidance from extensive interaction with the environment. In this work, we propose an approach that leverages a rough 2-D map of the environment to navigate in novel environments without requiring further learning. In particular, we introduce a dynamic topological map that can be initialized from the rough 2-D map along with a high-level planning approach for proposing reachable 2-D map patches of the intermediate landmarks between the start and goal locations. To use proposed 2-D patches, we train a deep generative model to generate intermediate landmarks in observation space which are used as subgoals by low-level goal-conditioned reinforcement learning. Importantly, because the low-level controller is only trained with local behaviors (e.g. go across the intersection, turn left at a corner) on existing environments, this framework allows us to generalize to novel environments given only a rough 2-D map, without requiring further learning. Experimental results demonstrate the effectiveness of the proposed framework in both seen and novel environments.

</details>

<details>

<summary>2021-06-07 20:08:48 - Near-Optimal Dispersion on Arbitrary Anonymous Graphs</summary>

- *Ajay D. Kshemkalyani, Gokarna Sharma*

- `2106.03943v1` - [abs](http://arxiv.org/abs/2106.03943v1) - [pdf](http://arxiv.org/pdf/2106.03943v1)

> Given an undirected, anonymous, port-labeled graph of $n$ memory-less nodes, $m$ edges, and degree $\Delta$, we consider the problem of dispersing $k\leq n$ robots (or tokens) positioned initially arbitrarily on one or more nodes of the graph to exactly $k$ different nodes of the graph, one on each node. The objective is to simultaneously minimize time to achieve dispersion and memory requirement at each robot. If all $k$ robots are positioned initially on a single node, depth first search (DFS) traversal solves this problem in $O(\min\{m,k\Delta\})$ time with $\Theta(\log(k+\Delta))$ bits at each robot. However, if robots are positioned initially on multiple nodes, the best previously known algorithm solves this problem in $O(\min\{m,k\Delta\}\cdot \log \ell)$ time storing $\Theta(\log(k+\Delta))$ bits at each robot, where $\ell\leq k/2$ is the number of multiplicity nodes in the initial configuration. In this paper, we present a novel multi-source DFS traversal algorithm solving this problem in $O(\min\{m,k\Delta\})$ time with $\Theta(\log(k+\Delta))$ bits at each robot, improving the time bound of the best previously known algorithm by $O(\log \ell)$ and matching asymptotically the single-source DFS traversal bounds. This is the first algorithm for dispersion that is optimal in both time and memory in arbitrary anonymous graphs of constant degree, $\Delta=O(1)$. Furthermore, the result holds in both synchronous and asynchronous settings.

</details>

<details>

<summary>2021-06-09 06:18:04 - Verification of a Merkle Patricia Tree Library Using F*</summary>

- *Sota Sato, Ryotaro Banno, Jun Furuse, Kohei Suenaga, Atsushi Igarashi*

- `2106.04826v1` - [abs](http://arxiv.org/abs/2106.04826v1) - [pdf](http://arxiv.org/pdf/2106.04826v1)

> A Merkle tree is a data structure for representing a key-value store as a tree. Each node of a Merkle tree is equipped with a hash value computed from those of their descendants. A Merkle tree is often used for representing a state of a blockchain system since it can be used for efficiently auditing the state in a trustless manner. Due to the safety-critical nature of blockchains, ensuring the correctness of their implementation is paramount.   We show our formally verified implementation of the core part of Plebeia using F*. Plebeia is a library to manipulate an extension of Merkle trees (called Plebeia trees). It is being implemented as a part of the storage system of the Tezos blockchain system. To this end, we gradually ported Plebeia to F*; the OCaml code extracted from the modules ported to F* is linked with the unverified part of Plebeia. By this gradual porting process, we can obtain a working code from our partially verified implementation of Plebeia; we confirmed that the binary passes all the unit tests of Plebeia.   More specifically, we verified the following properties on the implementation of Plebeia: (1) Each tree-manipulating function preserves the invariants on the data structure of a Plebeia tree and satisfies the functional requirements as a nested key-value store; (2) Each function for serializing/deserializing a Plebeia tree to/from the low-level storage is implemented correctly; and (3) The hash function for a Plebeia tree is relatively collision-resistant with respect to the cryptographic safety of the blake2b hash function. During porting Plebeia to F*, we found a bug in an old version of Plebeia, which was overlooked by the tests bundled with the original implementation. To the best of our knowledge, this is the first work that verifies a production-level implementation of a Merkle-tree library by F*.

</details>

<details>

<summary>2021-06-09 06:54:15 - Tracking by Joint Local and Global Search: A Target-aware Attention based Approach</summary>

- *Xiao Wang, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, Feng Wu*

- `2106.04840v1` - [abs](http://arxiv.org/abs/2106.04840v1) - [pdf](http://arxiv.org/pdf/2106.04840v1)

> Tracking-by-detection is a very popular framework for single object tracking which attempts to search the target object within a local search window for each frame. Although such local search mechanism works well on simple videos, however, it makes the trackers sensitive to extremely challenging scenarios, such as heavy occlusion and fast motion. In this paper, we propose a novel and general target-aware attention mechanism (termed TANet) and integrate it with tracking-by-detection framework to conduct joint local and global search for robust tracking. Specifically, we extract the features of target object patch and continuous video frames, then we concatenate and feed them into a decoder network to generate target-aware global attention maps. More importantly, we resort to adversarial training for better attention prediction. The appearance and motion discriminator networks are designed to ensure its consistency in spatial and temporal views. In the tracking procedure, we integrate the target-aware attention with multiple trackers by exploring candidate search regions for robust tracking. Extensive experiments on both short-term and long-term tracking benchmark datasets all validated the effectiveness of our algorithm. The project page of this paper can be found at \url{https://sites.google.com/view/globalattentiontracking/home/extend}.

</details>

<details>

<summary>2021-06-09 12:45:28 - SDGMNet: Statistic-based Dynamic Gradient Modulation for Local Descriptor Learning</summary>

- *Jiayi Ma, Yuxin Deng*

- `2106.04434v2` - [abs](http://arxiv.org/abs/2106.04434v2) - [pdf](http://arxiv.org/pdf/2106.04434v2)

> Modifications on triplet loss that rescale the back-propagated gradients of special pairs have made significant progress on local descriptor learning. However, current gradient modulation strategies are mainly static so that they would suffer from changes of training phases or datasets. In this paper, we propose a dynamic gradient modulation, named SDGMNet, to improve triplet loss for local descriptor learning. The core of our method is formulating modulation functions with statistical characteristics which are estimated dynamically. Firstly, we perform deep analysis on back propagation of general triplet-based loss and introduce included angle for distance measure. On this basis, auto-focus modulation is employed to moderate the impact of statistically uncommon individual pairs in stochastic gradient descent optimization; probabilistic margin cuts off the gradients of proportional Siamese pairs that are believed to reach the optimum; power adjustment balances the total weights of negative pairs and positive pairs. Extensive experiments demonstrate that our novel descriptor surpasses previous state-of-the-arts on standard benchmarks including patch verification, matching and retrieval tasks.

</details>

<details>

<summary>2021-06-10 07:38:23 - We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature</summary>

- *Bin Liang, Jiachun Li, Jianjun Huang*

- `2106.05261v2` - [abs](http://arxiv.org/abs/2106.05261v2) - [pdf](http://arxiv.org/pdf/2106.05261v2)

> Recently, the object detection based on deep learning has proven to be vulnerable to adversarial patch attacks. The attackers holding a specially crafted patch can hide themselves from the state-of-the-art person detectors, e.g., YOLO, even in the physical world. This kind of attack can bring serious security threats, such as escaping from surveillance cameras. In this paper, we deeply explore the detection problems about the adversarial patch attacks to the object detection. First, we identify a leverageable signature of existing adversarial patches from the point of the visualization explanation. A fast signature-based defense method is proposed and demonstrated to be effective. Second, we design an improved patch generation algorithm to reveal the risk that the signature-based way may be bypassed by the techniques emerging in the future. The newly generated adversarial patches can successfully evade the proposed signature-based defense. Finally, we present a novel signature-independent detection method based on the internal content semantics consistency rather than any attack-specific prior knowledge. The fundamental intuition is that the adversarial object can appear locally but disappear globally in an input image. The experiments demonstrate that the signature-independent method can effectively detect the existing and improved attacks. It has also proven to be a general method by detecting unforeseen and even other types of attacks without any attack-specific prior knowledge. The two proposed detection methods can be adopted in different scenarios, and we believe that combining them can offer a comprehensive protection.

</details>

<details>

<summary>2021-06-10 14:38:32 - CAT: Cross Attention in Vision Transformer</summary>

- *Hezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Qing Song, Wei Yuan*

- `2106.05786v1` - [abs](http://arxiv.org/abs/2106.05786v1) - [pdf](http://arxiv.org/pdf/2106.05786v1)

> Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps capture global information. Both operations have less computation than standard self-attention in Transformer. By alternately applying attention inner patch and between patches, we implement cross attention to maintain the performance with lower computational cost and build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are available at \url{https://github.com/linhezheng19/CAT}.

</details>

<details>

<summary>2021-06-11 01:35:08 - Vision Transformers with Patch Diversification</summary>

- *Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, Qiang Liu*

- `2104.12753v3` - [abs](http://arxiv.org/abs/2104.12753v3) - [pdf](http://arxiv.org/pdf/2104.12753v3)

> Vision transformer has demonstrated promising performance on challenging computer vision tasks. However, directly training the vision transformers may yield unstable and sub-optimal results. Recent works propose to improve the performance of the vision transformers by modifying the transformer structures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach to stabilize the vision transformer training without modifying the networks. We observe the instability of the training can be attributed to the significant similarity across the extracted patch representations. More specifically, for deep vision transformers, the self-attention blocks tend to map different patches into similar latent representations, yielding information loss and performance degradation. To alleviate this problem, in this work, we introduce novel loss functions in vision transformer training to explicitly encourage diversity across patch representations for more discriminative feature extraction. We empirically show that our proposed techniques stabilize the training and allow us to train wider and deeper vision transformers. We further show the diversified features significantly benefit the downstream tasks in transfer learning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and ADE20k. Our code is available at https://github.com/ChengyueGongR/PatchVisionTransformer.

</details>

<details>

<summary>2021-06-11 09:36:50 - MLP-Mixer: An all-MLP Architecture for Vision</summary>

- *Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy*

- `2105.01601v4` - [abs](http://arxiv.org/abs/2105.01601v4) - [pdf](http://arxiv.org/pdf/2105.01601v4)

> Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.

</details>

<details>

<summary>2021-06-13 18:48:18 - Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning for Whole Slide Image Classification</summary>

- *Yash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A. Moskaluk, Sana Syed, Donald E. Brown*

- `2103.10626v2` - [abs](http://arxiv.org/abs/2103.10626v2) - [pdf](http://arxiv.org/pdf/2103.10626v2)

> In recent years, the availability of digitized Whole Slide Images (WSIs) has enabled the use of deep learning-based computer vision techniques for automated disease diagnosis. However, WSIs present unique computational and algorithmic challenges. WSIs are gigapixel-sized ($\sim$100K pixels), making them infeasible to be used directly for training deep neural networks. Also, often only slide-level labels are available for training as detailed annotations are tedious and can be time-consuming for experts. Approaches using multiple-instance learning (MIL) frameworks have been shown to overcome these challenges. Current state-of-the-art approaches divide the learning framework into two decoupled parts: a convolutional neural network (CNN) for encoding the patches followed by an independent aggregation approach for slide-level prediction. In this approach, the aggregation step has no bearing on the representations learned by the CNN encoder. We have proposed an end-to-end framework that clusters the patches from a WSI into ${k}$-groups, samples ${k}'$ patches from each group for training, and uses an adaptive attention mechanism for slide level prediction; Cluster-to-Conquer (C2C). We have demonstrated that dividing a WSI into clusters can improve the model training by exposing it to diverse discriminative features extracted from the patches. We regularized the clustering mechanism by introducing a KL-divergence loss between the attention weights of patches in a cluster and the uniform distribution. The framework is optimized end-to-end on slide-level cross-entropy, patch-level cross-entropy, and KL-divergence loss (Implementation: https://github.com/YashSharma/C2C).

</details>

<details>

<summary>2021-06-13 22:38:38 - Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack</summary>

- *Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi Alfred Chen*

- `2009.06701v2` - [abs](http://arxiv.org/abs/2009.06701v2) - [pdf](http://arxiv.org/pdf/2009.06701v2)

> Automated Lane Centering (ALC) systems are convenient and widely deployed today, but also highly security and safety critical. In this work, we are the first to systematically study the security of state-of-the-art deep learning based ALC systems in their designed operational domains under physical-world adversarial attacks. We formulate the problem with a safety-critical attack goal, and a novel and domain-specific attack vector: dirty road patches. To systematically generate the attack, we adopt an optimization-based approach and overcome domain-specific design challenges such as camera frame inter-dependencies due to attack-influenced vehicle control, and the lack of objective function design for lane detection models.   We evaluate our attack on a production ALC using 80 scenarios from real-world driving traces. The results show that our attack is highly effective with over 97.5% success rates and less than 0.903 sec average success time, which is substantially lower than the average driver reaction time. This attack is also found (1) robust to various real-world factors such as lighting conditions and view angles, (2) general to different model designs, and (3) stealthy from the driver's view. To understand the safety impacts, we conduct experiments using software-in-the-loop simulation and attack trace injection in a real vehicle. The results show that our attack can cause a 100% collision rate in different scenarios, including when tested with common safety features such as automatic emergency braking. We also evaluate and discuss defenses.

</details>

<details>

<summary>2021-06-14 02:17:43 - Discerning the painter's hand: machine learning on surface topography</summary>

- *F. Ji, M. S. McMaster, S. Schwab, G. Singh, L. N. Smith, S. Adhikari, M. O'Dwyer, F. Sayed, A. Ingrisano, D. Yoder, E. S. Bolman, I. T. Martin, M. Hinczewski, K. D. Singer*

- `2106.07134v1` - [abs](http://arxiv.org/abs/2106.07134v1) - [pdf](http://arxiv.org/pdf/2106.07134v1)

> Attribution of paintings is a critical problem in art history. This study extends machine learning analysis to surface topography of painted works. A controlled study of positive attribution was designed with paintings produced by a class of art students. The paintings were scanned using a confocal optical profilometer to produce surface data. The surface data were divided into virtual patches and used to train an ensemble of convolutional neural networks (CNNs) for attribution. Over a range of patch sizes from 0.5 to 60 mm, the resulting attribution was found to be 60 to 96% accurate, and, when comparing regions of different color, was nearly twice as accurate as CNNs using color images of the paintings. Remarkably, short length scales, as small as twice a bristle diameter, were the key to reliably distinguishing among artists. These results show promise for real-world attribution, particularly in the case of workshop practice.

</details>

<details>

<summary>2021-06-16 07:56:19 - PatchNet: Unsupervised Object Discovery based on Patch Embedding</summary>

- *Hankyu Moon, Heng Hao, Sima Didari, Jae Oh Woo, Patrick Bangert*

- `2106.08599v1` - [abs](http://arxiv.org/abs/2106.08599v1) - [pdf](http://arxiv.org/pdf/2106.08599v1)

> We demonstrate that frequently appearing objects can be discovered by training randomly sampled patches from a small number of images (100 to 200) by self-supervision. Key to this approach is the pattern space, a latent space of patterns that represents all possible sub-images of the given image data. The distance structure in the pattern space captures the co-occurrence of patterns due to the frequent objects. The pattern space embedding is learned by minimizing the contrastive loss between randomly generated adjacent patches. To prevent the embedding from learning the background, we modulate the contrastive loss by color-based object saliency and background dissimilarity. The learned distance structure serves as object memory, and the frequent objects are simply discovered by clustering the pattern vectors from the random patches sampled for inference. Our image representation based on image patches naturally handles the position and scale invariance property that is crucial to multi-object discovery. The method has been proven surprisingly effective, and successfully applied to finding multiple human faces and bodies from natural images.

</details>

<details>

<summary>2021-06-16 16:14:01 - Transductive Few-Shot Learning: Clustering is All You Need?</summary>

- *Imtiaz Masud Ziko, Malik Boudiaf, Jose Dolz, Eric Granger, Ismail Ben Ayed*

- `2106.09516v1` - [abs](http://arxiv.org/abs/2106.09516v1) - [pdf](http://arxiv.org/pdf/2106.09516v1)

> We investigate a general formulation for clustering and transductive few-shot learning, which integrates prototype-based objectives, Laplacian regularization and supervision constraints from a few labeled data points. We propose a concave-convex relaxation of the problem, and derive a computationally efficient block-coordinate bound optimizer, with convergence guarantee. At each iteration,our optimizer computes independent (parallel) updates for each point-to-cluster assignment. Therefore, it could be trivially distributed for large-scale clustering and few-shot tasks. Furthermore, we provides a thorough convergence analysis based on point-to-set maps. Were port comprehensive clustering and few-shot learning experiments over various data sets, showing that our method yields competitive performances, in term of accuracy and optimization quality, while scaling up to large problems. Using standard training on the base classes, without resorting to complex meta-learning and episodic-training strategies, our approach outperforms state-of-the-art few-shot methods by significant margins, across various models, settings and data sets. Surprisingly, we found that even standard clustering procedures (e.g., K-means), which correspond to particular, non-regularized cases of our general model, already achieve competitive performances in comparison to the state-of-the-art in few-shot learning. These surprising results point to the limitations of the current few-shot benchmarks, and question the viability of a large body of convoluted few-shot learning techniques in the recent literature.

</details>

<details>

<summary>2021-06-16 20:34:31 - Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology</summary>

- *Alex Foote, Amina Asif, Ayesha Azam, Tim Marshall-Cox, Nasir Rajpoot, Fayyaz Minhas*

- `2106.08153v2` - [abs](http://arxiv.org/abs/2106.08153v2) - [pdf](http://arxiv.org/pdf/2106.08153v2)

> Deep learning models are routinely employed in computational pathology (CPath) for solving problems of diagnostic and prognostic significance. Typically, the generalization performance of CPath models is analyzed using evaluation protocols such as cross-validation and testing on multi-centric cohorts. However, to ensure that such CPath solutions are robust and safe for use in a clinical setting, a critical analysis of their predictive performance and vulnerability to adversarial attacks is required, which is the focus of this paper. Specifically, we show that a highly accurate model for classification of tumour patches in pathology images (AUC > 0.95) can easily be attacked with minimal perturbations which are imperceptible to lay humans and trained pathologists alike. Our analytical results show that it is possible to generate single-instance white-box attacks on specific input images with high success rate and low perturbation energy. Furthermore, we have also generated a single universal perturbation matrix using the training dataset only which, when added to unseen test images, results in forcing the trained neural network to flip its prediction labels with high confidence at a success rate of > 84%. We systematically analyze the relationship between perturbation energy of an adversarial attack, its impact on morphological constructs of clinical significance, their perceptibility by a trained pathologist and saliency maps obtained using deep learning models. Based on our analysis, we strongly recommend that computational pathology models be critically analyzed using the proposed adversarial validation strategy prior to clinical adoption.

</details>

<details>

<summary>2021-06-16 23:27:01 - Cross-Language Code Search using Static and Dynamic Analyses</summary>

- *George Mathew, Kathryn T. Stolee*

- `2106.09173v1` - [abs](http://arxiv.org/abs/2106.09173v1) - [pdf](http://arxiv.org/pdf/2106.09173v1)

> As code search permeates most activities in software development,code-to-code search has emerged to support using code as a query and retrieving similar code in the search results. Applications include duplicate code detection for refactoring, patch identification for program repair, and language translation. Existing code-to-code search tools rely on static similarity approaches such as the comparison of tokens and abstract syntax trees (AST) to approximate dynamic behavior, leading to low precision. Most tools do not support cross-language code-to-code search, and those that do, rely on machine learning models that require labeled training data.   We present Code-to-Code Search Across Languages (COSAL), a cross-language technique that uses both static and dynamic analyses to identify similar code and does not require a machine learning model. Code snippets are ranked using non-dominated sorting based on code token similarity, structural similarity, and behavioral similarity. We empirically evaluate COSAL on two datasets of 43,146Java and Python files and 55,499 Java files and find that 1) code search based on non-dominated ranking of static and dynamic similarity measures is more effective compared to single or weighted measures; and 2) COSAL has better precision and recall compared to state-of-the-art within-language and cross-language code-to-code search tools. We explore the potential for using COSAL on large open-source repositories and discuss scalability to more languages and similarity metrics, providing a gateway for practical,multi-language code-to-code search.

</details>

<details>

<summary>2021-06-17 17:06:52 - Generating Data Augmentation samples for Semantic Segmentation of Salt Bodies in a Synthetic Seismic Image Dataset</summary>

- *Luis Felipe Henriques, Sérgio Colcher, Ruy Luiz Milidiú, André Bulcão, Pablo Barros*

- `2106.08269v2` - [abs](http://arxiv.org/abs/2106.08269v2) - [pdf](http://arxiv.org/pdf/2106.08269v2)

> Nowadays, subsurface salt body localization and delineation, also called semantic segmentation of salt bodies, are among the most challenging geophysicist tasks. Thus, identifying large salt bodies is notoriously tricky and is crucial for identifying hydrocarbon reservoirs and drill path planning. This work proposes a Data Augmentation method based on training two generative models to augment the number of samples in a seismic image dataset for the semantic segmentation of salt bodies. Our method uses deep learning models to generate pairs of seismic image patches and their respective salt masks for the Data Augmentation. The first model is a Variational Autoencoder and is responsible for generating patches of salt body masks. The second is a Conditional Normalizing Flow model, which receives the generated masks as inputs and generates the associated seismic image patches. We evaluate the proposed method by comparing the performance of ten distinct state-of-the-art models for semantic segmentation, trained with and without the generated augmentations, in a dataset from two synthetic seismic images. The proposed methodology yields an average improvement of 8.57% in the IoU metric across all compared models. The best result is achieved by a DeeplabV3+ model variant, which presents an IoU score of 95.17% when trained with our augmentations. Additionally, our proposal outperformed six selected data augmentation methods, and the most significant improvement in the comparison, of 9.77%, is achieved by composing our DA with augmentations from an elastic transformation. At last, we show that the proposed method is adaptable for a larger context size by achieving results comparable to the obtained on the smaller context size.

</details>

<details>

<summary>2021-06-17 20:33:56 - Intentional Forgetting</summary>

- *Deborah Shands, Carolyn Talcott*

- `2106.09802v1` - [abs](http://arxiv.org/abs/2106.09802v1) - [pdf](http://arxiv.org/pdf/2106.09802v1)

> Many damaging cybersecurity attacks are enabled when an attacker can access residual sensitive information (e.g. cryptographic keys, personal identifiers) left behind from earlier computation. Attackers can sometimes use residual information to take control of a system, impersonate a user, or manipulate data. Current approaches to addressing access to residual sensitive information aim to patch individual software or hardware vulnerabilities. While such patching approaches are necessary to mitigate sometimes serious security vulnerabilities in the near term, they cannot address the underlying issue: explicit requirements for adequately eliminating residual information and explicit representations of the erasure capabilities of systems are necessary to ensure that sensitive information is handled as expected.   This position paper introduces the concept of intentional forgetting and the capabilities that are needed to achieve it. Intentional forgetting enables software and hardware system designers at every level of abstraction to clearly specify and rigorously reason about the forgetting capabilities required of and provided by a system. We identify related work that may help to illuminate challenges or contribute to solutions and consider conceptual and engineering tradeoffs in implementations of forgetting capabilities. We discuss approaches to modeling intentional forgetting and then modeling the strength of a system's forgetting capability by its resistance to disclosing information to different types of detectors. Research is needed in a variety of domains to advance the theory, specification techniques, system foundations, implementation tools, and methodologies for effective, practical forgetting. We highlight research challenges in several domains and encourage cross-disciplinary collaboration to one day create a robust theory and practice of intentional forgetting.

</details>

<details>

<summary>2021-06-18 07:58:20 - A Grounded Theory of the Role of Coordination in Software Security Patch Management</summary>

- *Nesara Dissanayake, Mansooreh Zahedi, Asangi Jayatilaka, Muhammad Ali Babar*

- `2106.03458v2` - [abs](http://arxiv.org/abs/2106.03458v2) - [pdf](http://arxiv.org/pdf/2106.03458v2)

> Several disastrous security attacks can be attributed to delays in patching software vulnerabilities. While researchers and practitioners have paid significant attention to automate vulnerabilities identification and patch development activities of software security patch management, there has been relatively little effort dedicated to gain an in-depth understanding of the socio-technical aspects, e.g., coordination of interdependent activities of the patching process and patching decisions, that may cause delays in applying security patches. We report on a Grounded Theory study of the role of coordination in security patch management. The reported theory consists of four inter-related dimensions, i.e., causes, breakdowns, constraints, and mechanisms. The theory explains the causes that define the need for coordination among interdependent software and hardware components and multiple stakeholders' decisions, the constraints that can negatively impact coordination, the breakdowns in coordination, and the potential corrective measures. This study provides potentially useful insights for researchers and practitioners who can carefully consider the needs of and devise suitable solutions for supporting the coordination of interdependencies involved in security patch management.

</details>

<details>

<summary>2021-06-18 15:33:31 - XCiT: Cross-Covariance Image Transformers</summary>

- *Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, Hervé Jegou*

- `2106.09681v2` - [abs](http://arxiv.org/abs/2106.09681v2) - [pdf](http://arxiv.org/pdf/2106.09681v2)

> Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a "transposed" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.

</details>

<details>

<summary>2021-06-19 08:33:29 - MSN: Efficient Online Mask Selection Network for Video Instance Segmentation</summary>

- *Vidit Goel, Jiachen Li, Shubhika Garg, Harsh Maheshwari, Humphrey Shi*

- `2106.10452v1` - [abs](http://arxiv.org/abs/2106.10452v1) - [pdf](http://arxiv.org/pdf/2106.10452v1)

> In this work we present a novel solution for Video Instance Segmentation(VIS), that is automatically generating instance level segmentation masks along with object class and tracking them in a video. Our method improves the masks from segmentation and propagation branches in an online manner using the Mask Selection Network (MSN) hence limiting the noise accumulation during mask tracking. We propose an effective design of MSN by using patch-based convolutional neural network. The network is able to distinguish between very subtle differences between the masks and choose the better masks out of the associated masks accurately. Further, we make use of temporal consistency and process the video sequences in both forward and reverse manner as a post processing step to recover lost objects. The proposed method can be used to adapt any video object segmentation method for the task of VIS. Our method achieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third place among more than 30 global teams. Our code will be available at https://github.com/SHI-Labs/Mask-Selection-Networks.

</details>

<details>

<summary>2021-06-22 14:07:54 - Meta Adversarial Training against Universal Patches</summary>

- *Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher*

- `2101.11453v2` - [abs](http://arxiv.org/abs/2101.11453v2) - [pdf](http://arxiv.org/pdf/2101.11453v2)

> Recently demonstrated physical-world adversarial attacks have exposed vulnerabilities in perception systems that pose severe risks for safety-critical applications such as autonomous driving. These attacks place adversarial artifacts in the physical world that indirectly cause the addition of a universal patch to inputs of a model that can fool it in a variety of contexts. Adversarial training is the most effective defense against image-dependent adversarial attacks. However, tailoring adversarial training to universal patches is computationally expensive since the optimal universal patch depends on the model weights which change during training. We propose meta adversarial training (MAT), a novel combination of adversarial training with meta-learning, which overcomes this challenge by meta-learning universal patches along with model training. MAT requires little extra computation while continuously adapting a large set of patches to the current model. MAT considerably increases robustness against universal patch attacks on image classification and traffic-light detection.

</details>

<details>

<summary>2021-06-23 17:58:04 - S$^2$-MLP: Spatial-Shift MLP Architecture for Vision</summary>

- *Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, Ping Li*

- `2106.07477v2` - [abs](http://arxiv.org/abs/2106.07477v2) - [pdf](http://arxiv.org/pdf/2106.07477v2)

> Recently, visual Transformer (ViT) and its following works abandon the convolution and exploit the self-attention operation, attaining a comparable or even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the convolution and the self-attention operation, proposing an architecture containing only MLP layers. To achieve cross-patch communications, it devises an additional token-mixing MLP besides the channel-mixing MLP. It achieves promising results when training on an extremely large-scale dataset. But it cannot achieve as outstanding performance as its CNN and ViT counterparts when training on medium-scale datasets such as ImageNet1K and ImageNet21K. The performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We discover that the token-mixing MLP is a variant of the depthwise convolution with a global reception field and spatial-specific configuration. But the global reception field and the spatial-specific property make token-mixing MLP prone to over-fitting. In this paper, we propose a novel pure MLP architecture, spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only contains channel-mixing MLP. We utilize a spatial-shift operation for communications between patches. It has a local reception field and is spatial-agnostic. It is parameter-free and efficient for computation. The proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent performance as ViT on ImageNet-1K dataset with considerably simpler architecture and fewer FLOPs and parameters.

</details>

<details>

<summary>2021-06-24 17:59:46 - Video Swin Transformer</summary>

- *Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu*

- `2106.13230v1` - [abs](http://arxiv.org/abs/2106.13230v1) - [pdf](http://arxiv.org/pdf/2106.13230v1)

> The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.

</details>

<details>

<summary>2021-06-25 09:41:21 - Boolean learning under noise-perturbations in hardware neural networks</summary>

- *Louis Andreoli, Xavier Porte, Stéphane Chrétien, Maxime Jacquot, Laurent Larger, Daniel Brunner*

- `2003.12319v2` - [abs](http://arxiv.org/abs/2003.12319v2) - [pdf](http://arxiv.org/pdf/2003.12319v2)

> A high efficiency hardware integration of neural networks benefits from realizing nonlinearity, network connectivity and learning fully in a physical substrate. Multiple systems have recently implemented some or all of these operations, yet the focus was placed on addressing technological challenges. Fundamental questions regarding learning in hardware neural networks remain largely unexplored. Noise in particular is unavoidable in such architectures, and here we investigate its interaction with a learning algorithm using an opto-electronic recurrent neural network. We find that noise strongly modifies the system's path during convergence, and surprisingly fully decorrelates the final readout weight matrices. This highlights the importance of understanding architecture, noise and learning algorithm as interacting players, and therefore identifies the need for mathematical tools for noisy, analogue system optimization.

</details>

<details>

<summary>2021-06-25 11:04:04 - A Novel Self-Learning Framework for Bladder Cancer Grading Using Histopathological Images</summary>

- *Gabriel García, Anna Esteve, Adrián Colomer, David Ramos, Valery Naranjo*

- `2106.13559v1` - [abs](http://arxiv.org/abs/2106.13559v1) - [pdf](http://arxiv.org/pdf/2106.13559v1)

> Recently, bladder cancer has been significantly increased in terms of incidence and mortality. Currently, two subtypes are known based on tumour growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC). In this work, we focus on the MIBC subtype because it is of the worst prognosis and can spread to adjacent organs. We present a self-learning framework to grade bladder cancer from histological images stained via immunohistochemical techniques. Specifically, we propose a novel Deep Convolutional Embedded Attention Clustering (DCEAC) which allows classifying histological patches into different severity levels of the disease, according to the patterns established in the literature. The proposed DCEAC model follows a two-step fully unsupervised learning methodology to discern between non-tumour, mild and infiltrative patterns from high-resolution samples of 512x512 pixels. Our system outperforms previous clustering-based methods by including a convolutional attention module, which allows refining the features of the latent space before the classification stage. The proposed network exceeds state-of-the-art approaches by 2-3% across different metrics, achieving a final average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported class activation maps evidence that our model is able to learn by itself the same patterns that clinicians consider relevant, without incurring prior annotation steps. This fact supposes a breakthrough in muscle-invasive bladder cancer grading which bridges the gap with respect to train the model on labelled data.

</details>

<details>

<summary>2021-06-25 20:40:42 - Convergence Analysis and Numerical Studies for Linearly Elastic Peridynamics with Dirichlet-Type Boundary Conditions</summary>

- *Mikil Foss, Petronela Radu, Yue Yu*

- `2106.13878v1` - [abs](http://arxiv.org/abs/2106.13878v1) - [pdf](http://arxiv.org/pdf/2106.13878v1)

> The nonlocal models of peridynamics have successfully predicted fractures and deformations for a variety of materials. In contrast to local mechanics, peridynamic boundary conditions must be defined on a finite volume region outside the body. Therefore, theoretical and numerical challenges arise in order to properly formulate Dirichlet-type nonlocal boundary conditions, while connecting them to the local counterparts. While a careless imposition of local boundary conditions leads to a smaller effective material stiffness close to the boundary and an artificial softening of the material, several strategies were proposed to avoid this unphysical surface effect.   In this work, we study convergence of solutions to nonlocal state-based linear elastic model to their local counterparts as the interaction horizon vanishes, under different formulations and smoothness assumptions for nonlocal Dirichlet-type boundary conditions. Our results provide explicit rates of convergence that are sensitive to the compatibility of the nonlocal boundary data and the extension of the solution for the local model. In particular, under appropriate assumptions, constant extensions yield $\frac{1}{2}$ order convergence rates and linear extensions yield $\frac{3}{2}$ order convergence rates. With smooth extensions, these rates are improved to quadratic convergence. We illustrate the theory for any dimension $d\geq 2$ and numerically verify the convergence rates with a number of two dimensional benchmarks, including linear patch tests, manufactured solutions, and domains with curvilinear surfaces. Numerical results show a first order convergence for constant extensions and second order convergence for linear extensions, which suggests a possible room of improvement in the future convergence analysis.

</details>

<details>

<summary>2021-06-28 13:50:31 - Tiled sparse coding in eigenspaces for the COVID-19 diagnosis in chest X-ray images</summary>

- *Juan E. Arco, Andrés Ortiz, Javier Ramírez, Juan M Gorriz*

- `2106.14724v1` - [abs](http://arxiv.org/abs/2106.14724v1) - [pdf](http://arxiv.org/pdf/2106.14724v1)

> The ongoing crisis of the COVID-19 (Coronavirus disease 2019) pandemic has changed the world. According to the World Health Organization (WHO), 4 million people have died due to this disease, whereas there have been more than 180 million confirmed cases of COVID-19. The collapse of the health system in many countries has demonstrated the need of developing tools to automatize the diagnosis of the disease from medical imaging. Previous studies have used deep learning for this purpose. However, the performance of this alternative highly depends on the size of the dataset employed for training the algorithm. In this work, we propose a classification framework based on sparse coding in order to identify the pneumonia patterns associated with different pathologies. Specifically, each chest X-ray (CXR) image is partitioned into different tiles. The most relevant features extracted from PCA are then used to build the dictionary within the sparse coding procedure. Once images are transformed and reconstructed from the elements of the dictionary, classification is performed from the reconstruction errors of individual patches associated with each image. Performance is evaluated in a real scenario where simultaneously differentiation between four different pathologies: control vs bacterial pneumonia vs viral pneumonia vs COVID-19. The accuracy when identifying the presence of pneumonia is 93.85%, whereas 88.11% is obtained in the 4-class classification context. The excellent results and the pioneering use of sparse coding in this scenario evidence the applicability of this approach as an aid for clinicians in a real-world environment.

</details>

<details>

<summary>2021-06-28 22:07:48 - Validating Static Warnings via Testing Code Fragments</summary>

- *Ashwin Kallingal Joshy, Xueyuan Chen, Benjamin Steenhoek, Wei Le*

- `2106.04735v3` - [abs](http://arxiv.org/abs/2106.04735v3) - [pdf](http://arxiv.org/pdf/2106.04735v3)

> Static analysis is an important approach for finding bugs and vulnerabilities in software. However, inspecting and confirming static warnings are challenging and time-consuming. In this paper, we present a novel solution that automatically generates test cases based on static warnings to validate true and false positives. We designed a syntactic patching algorithm that can generate syntactically valid, semantic preserving executable code fragments from static warnings. We developed a build and testing system to automatically test code fragments using fuzzers, KLEE and Valgrind. We evaluated our techniques using 12 real-world C projects and 1955 warnings from two commercial static analysis tools. We successfully built 68.5% code fragments and generated 1003 test cases. Through automatic testing, we identified 48 true positives and 27 false positives, and 205 likely false positives. We matched 4 CVE and real-world bugs using Helium, and they are only triggered by our tool but not other baseline tools. We found that testing code fragments is scalable and useful; it can trigger bugs that testing entire programs or testing procedures failed to trigger.

</details>


## 2021-07

<details>

<summary>2021-07-01 12:20:48 - CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor</summary>

- *Alberto Viale, Alberto Marchisio, Maurizio Martina, Guido Masera, Muhammad Shafique*

- `2107.00401v1` - [abs](http://arxiv.org/abs/2107.00401v1) - [pdf](http://arxiv.org/pdf/2107.00401v1)

> Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip.

</details>

<details>

<summary>2021-07-01 19:59:32 - Verifying Verified Code</summary>

- *Siddharth Priya, Xiang Zhou, Yusen Su, Yakir Vizel, Yuyan Bao, Arie Gurfinkel*

- `2107.00723v1` - [abs](http://arxiv.org/abs/2107.00723v1) - [pdf](http://arxiv.org/pdf/2107.00723v1)

> A recent case study from AWS by Chong et al. proposes an effective methodology for Bounded Model Checking in industry. In this paper, we report on a follow up case study that explores the methodology from the perspective of three research questions: (a) can proof artifacts be used across verification tools; (b) are there bugs in verified code; and (c) can specifications be improved. To study these questions, we port the verification tasks for $\texttt{aws-c-common}$ library to SEAHORN and KLEE. We show the benefits of using compiler semantics and cross-checking specifications with different verification techniques, and call for standardizing proof library extensions to increase specification reuse. The verification tasks discussed are publicly available online.

</details>

<details>

<summary>2021-07-02 11:05:45 - Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning</summary>

- *Thanaphon Suwannaphong, Sawaphob Chavana, Sahapol Tongsom, Duangdao Palasuwan, Thanarat H. Chalidabhongse, Nantheera Anantrasirichai*

- `2107.00968v1` - [abs](http://arxiv.org/abs/2107.00968v1) - [pdf](http://arxiv.org/pdf/2107.00968v1)

> Intestinal parasitic infection leads to several morbidities to humans worldwide, especially in tropical countries. The traditional diagnosis usually relies on manual analysis from microscopic images which is prone to human error due to morphological similarity of different parasitic eggs and abundance of impurities in a sample. Many studies have developed automatic systems for parasite egg detection to reduce human workload. However, they work with high quality microscopes, which unfortunately remain unaffordable in some rural areas. Our work thus exploits a benefit of a low-cost USB microscope. This instrument however provides poor quality of images due to limitation of magnification (10x), causing difficulty in parasite detection and species classification. In this paper, we propose a CNN-based technique using transfer learning strategy to enhance the efficiency of automatic parasite classification in poor-quality microscopic images. The patch-based technique with sliding window is employed to search for location of the eggs. Two networks, AlexNet and ResNet50, are examined with a trade-off between architecture size and classification performance. The results show that our proposed framework outperforms the state-of-the-art object recognition methods. Our system combined with final decision from an expert may improve the real faecal examination with low-cost microscopes.

</details>

<details>

<summary>2021-07-02 16:50:53 - Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study</summary>

- *Gabriel Henrique de Almeida Pereira, André Minoro Fusioka, Bogdan Tomoyuki Nassu, Rodrigo Minetto*

- `2101.03409v2` - [abs](http://arxiv.org/abs/2101.03409v2) - [pdf](http://arxiv.org/pdf/2101.03409v2)

> Active fire detection in satellite imagery is of critical importance to the management of environmental conservation policies, supporting decision-making and law enforcement. This is a well established field, with many techniques being proposed over the years, usually based on pixel or region-level comparisons involving sensor-specific thresholds and neighborhood statistics. In this paper, we address the problem of active fire detection using deep learning techniques. In recent years, deep learning techniques have been enjoying an enormous success in many fields, but their use for active fire detection is relatively new, with open questions and demand for datasets and architectures for evaluation. This paper addresses these issues by introducing a new large-scale dataset for active fire detection, with over 150,000 image patches (more than 200 GB of data) extracted from Landsat-8 images captured around the world in August and September 2020, containing wildfires in several locations. The dataset was split in two parts, and contains 10-band spectral images with associated outputs, produced by three well known handcrafted algorithms for active fire detection in the first part, and manually annotated masks in the second part. We also present a study on how different convolutional neural network architectures can be used to approximate these handcrafted algorithms, and how models trained on automatically segmented patches can be combined to achieve better performance than the original algorithms - with the best combination having 87.2% precision and 92.4% recall on our manually annotated dataset. The proposed dataset, source codes and trained models are available on Github (https://github.com/pereira-gha/activefire), creating opportunities for further advances in the field

</details>

<details>

<summary>2021-07-03 09:13:22 - GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly</summary>

- *Jiulou Zhang, Yuxia Tang, Shouju Wang*

- `2012.12561v2` - [abs](http://arxiv.org/abs/2012.12561v2) - [pdf](http://arxiv.org/pdf/2012.12561v2)

> Intratumoral nanoparticles (NPs) distribution is critical for the success of nanomedicine in imaging and treatment, but computational models to describe the NPs distribution remain unavailable due to the complex tumor-nano interactions. Here, we develop a Generative Adversarial Network for Distribution Analysis (GANDA) to describe and conditionally generates the intratumoral quantum dots (QDs) distribution after i.v. injection. This deep generative model is trained automatically by 27 775 patches of tumor vessels and cell nuclei decomposed from whole-slide images of 4T1 breast cancer sections. The GANDA model can conditionally generate images of intratumoral QDs distribution under the constraint of given tumor vessels and cell nuclei channels with the same spatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE = 1.871) and excellent reliability (intraclass correlation, ICC = 0.94). Quantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea distribution (ICC = 0.99) is allowed on the generated images without knowing the real QDs distribution. We believe this deep generative model may provide opportunities to investigate how influencing factors affect NPs distribution in individual tumors and guide nanomedicine optimization for molecular imaging and personalized treatment.

</details>

<details>

<summary>2021-07-04 16:54:46 - From Library Portability to Para-rehosting: Natively Executing Microcontroller Software on Commodity Hardware</summary>

- *Wenqiang Li, Le Guan, Jingqiang Lin, Jiameng Shi, Fengjun Li*

- `2107.12867v1` - [abs](http://arxiv.org/abs/2107.12867v1) - [pdf](http://arxiv.org/pdf/2107.12867v1)

> Finding bugs in microcontroller (MCU) firmware is challenging, even for device manufacturers who own the source code. The MCU runs different instruction sets than x86 and exposes a very different development environment. This invalidates many existing sophisticated software testing tools on x86. To maintain a unified developing and testing environment, a straightforward way is to re-compile the source code into the native executable for a commodity machine (called rehosting). However, ad-hoc re-hosting is a daunting and tedious task and subject to many issues (library-dependence, kernel-dependence and hardware-dependence). In this work, we systematically explore the portability problem of MCU software and propose pararehosting to ease the porting process. Specifically, we abstract and implement a portable MCU (PMCU) using the POSIX interface. It models common functions of the MCU cores. For peripheral specific logic, we propose HAL-based peripheral function replacement, in which high-level hardware functions are replaced with an equivalent backend driver on the host. These backend drivers are invoked by well-designed para-APIs and can be reused across many MCU OSs. We categorize common HAL functions into four types and implement templates for quick backend development. Using the proposed approach, we have successfully rehosted nine MCU OSs including the widely deployed Amazon FreeRTOS, ARM Mbed OS, Zephyr and LiteOS. To demonstrate the superiority of our approach in terms of security testing, we used off-the-shelf dynamic analysis tools (AFL and ASAN) against the rehosted programs and discovered 28 previously-unknown bugs, among which 5 were confirmed by CVE and the other 19 were confirmed by vendors at the time of writing.

</details>

<details>

<summary>2021-07-07 03:13:48 - A Dual-Port 8-T CAM-Based Network Intrusion Detection Engine for IoT</summary>

- *Dai Li, Kaiyuan Yang*

- `2107.02992v1` - [abs](http://arxiv.org/abs/2107.02992v1) - [pdf](http://arxiv.org/pdf/2107.02992v1)

> This letter presents an energy- and memory-efficient pattern-matching engine for a network intrusion detection system (NIDS) in the Internet of Things. Tightly coupled architecture and circuit co-designs are proposed to fully exploit the statistical behaviors of NIDS pattern matching. The proposed engine performs pattern matching in three phases, where the phase-1 prefix matching employs reconfigurable pipelined automata processing to minimize memory footprint without loss of throughput and efficiency. The processing elements utilize 8-T content-addressable memory (CAM) cells for dual-port search by leveraging proposed fixed-1s encoding. A 65-nm prototype demonstrates best-in-class 1.54-fJ energy per search per pattern byte and 0.9-byte memory usage per pattern byte.

</details>

<details>

<summary>2021-07-12 04:17:39 - TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation</summary>

- *Yao Chang, Hu Menghan, Zhai Guangtao, Zhang Xiao-Ping*

- `2107.05188v1` - [abs](http://arxiv.org/abs/2107.05188v1) - [pdf](http://arxiv.org/pdf/2107.05188v1)

> In recent years, computer-aided diagnosis has become an increasingly popular topic. Methods based on convolutional neural networks have achieved good performance in medical image segmentation and classification. Due to the limitations of the convolution operation, the long-term spatial features are often not accurately obtained. Hence, we propose a TransClaw U-Net network structure, which combines the convolution operation with the transformer operation in the encoding part. The convolution part is applied for extracting the shallow spatial features to facilitate the recovery of the image resolution after upsampling. The transformer part is used to encode the patches, and the self-attention mechanism is used to obtain global information between sequences. The decoding part retains the bottom upsampling structure for better detail segmentation performance. The experimental results on Synapse Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net is better than other network structures. The ablation experiments also prove the generalization performance of TransClaw U-Net.

</details>

<details>

<summary>2021-07-12 14:54:04 - Anatomy-Constrained Contrastive Learning for Synthetic Segmentation without Ground-truth</summary>

- *Bo Zhou, Chi Liu, James S. Duncan*

- `2107.05482v1` - [abs](http://arxiv.org/abs/2107.05482v1) - [pdf](http://arxiv.org/pdf/2107.05482v1)

> A large amount of manual segmentation is typically required to train a robust segmentation network so that it can segment objects of interest in a new imaging modality. The manual efforts can be alleviated if the manual segmentation in one imaging modality (e.g., CT) can be utilized to train a segmentation network in another imaging modality (e.g., CBCT/MRI/PET). In this work, we developed an anatomy-constrained contrastive synthetic segmentation network (AccSeg-Net) to train a segmentation network for a target imaging modality without using its ground truth. Specifically, we proposed to use anatomy-constraint and patch contrastive learning to ensure the anatomy fidelity during the unsupervised adaptation, such that the segmentation network can be trained on the adapted image with correct anatomical structure/content. The training data for our AccSeg-Net consists of 1) imaging data paired with segmentation ground-truth in source modality, and 2) unpaired source and target modality imaging data. We demonstrated successful applications on CBCT, MRI, and PET imaging data, and showed superior segmentation performances as compared to previous methods.

</details>

<details>

<summary>2021-07-13 10:07:09 - Design and engineering of a simplified workflow execution for the MG5aMC event generator on GPUs and vector CPUs</summary>

- *Andrea Valassi, Stefan Roiser, Olivier Mattelaer, Stephan Hageboeck*

- `2106.12631v2` - [abs](http://arxiv.org/abs/2106.12631v2) - [pdf](http://arxiv.org/pdf/2106.12631v2)

> Physics event generators are essential components of the data analysis software chain of high energy physics experiments, and important consumers of their CPU resources. Improving the software performance of these packages on modern hardware architectures, such as those deployed at HPC centers, is essential in view of the upcoming HL-LHC physics programme. In this paper, we describe an ongoing activity to reengineer the Madgraph5_aMC@NLO physics event generator, primarily to port it and allow its efficient execution on GPUs, but also to modernize it and optimize its performance on vector CPUs. We describe the motivation, engineering process and software architecture design of our developments, as well as the current challenges and future directions for this project. This paper is based on our submission to vCHEP2021 in March 2021,complemented with a few preliminary results that we presented during the conference. Further details and updated results will be given in later publications.

</details>

<details>

<summary>2021-07-13 12:12:11 - A Deep Generative Artificial Intelligence system to decipher species coexistence patterns</summary>

- *J. Hirn, J. E. García, A. Montesinos-Navarro, R. Sanchez-Martín, V. Sanz, M. Verdú*

- `2107.06020v1` - [abs](http://arxiv.org/abs/2107.06020v1) - [pdf](http://arxiv.org/pdf/2107.06020v1)

> 1. Deciphering coexistence patterns is a current challenge to understanding diversity maintenance, especially in rich communities where the complexity of these patterns is magnified through indirect interactions that prevent their approximation with classical experimental approaches. 2. We explore cutting-edge Machine Learning techniques called Generative Artificial Intelligence (GenAI) to decipher species coexistence patterns in vegetation patches, training generative adversarial networks (GAN) and variational AutoEncoders (VAE) that are then used to unravel some of the mechanisms behind community assemblage. 3. The GAN accurately reproduces the species composition of real patches as well as the affinity of plant species to different soil types, and the VAE also reaches a high level of accuracy, above 99%. Using the artificially generated patches, we found that high order interactions tend to suppress the positive effects of low order interactions. Finally, by reconstructing successional trajectories we could identify the pioneer species with larger potential to generate a high diversity of distinct patches in terms of species composition. 4. Understanding the complexity of species coexistence patterns in diverse ecological communities requires new approaches beyond heuristic rules. Generative Artificial Intelligence can be a powerful tool to this end as it allows to overcome the inherent dimensionality of this challenge.

</details>

<details>

<summary>2021-07-14 11:45:47 - Detecting when pre-trained nnU-Net models fail silently for Covid-19 lung lesion segmentation</summary>

- *Camila Gonzalez, Karol Gotkowski, Andreas Bucher, Ricarda Fischbach, Isabel Kaltenborn, Anirban Mukhopadhyay*

- `2107.05975v2` - [abs](http://arxiv.org/abs/2107.05975v2) - [pdf](http://arxiv.org/pdf/2107.05975v2)

> Automatic segmentation of lung lesions in computer tomography has the potential to ease the burden of clinicians during the Covid-19 pandemic. Yet predictive deep learning models are not trusted in the clinical routine due to failing silently in out-of-distribution (OOD) data. We propose a lightweight OOD detection method that exploits the Mahalanobis distance in the feature space. The proposed approach can be seamlessly integrated into state-of-the-art segmentation pipelines without requiring changes in model architecture or training procedure, and can therefore be used to assess the suitability of pre-trained models to new data. We validate our method with a patch-based nnU-Net architecture trained with a multi-institutional dataset and find that it effectively detects samples that the model segments incorrectly.

</details>

<details>

<summary>2021-07-14 16:53:08 - Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising</summary>

- *Basit O. Alawode, Mudassir Masood, Tarig Ballal, Tareq Al-Naffouri*

- `2107.05115v2` - [abs](http://arxiv.org/abs/2107.05115v2) - [pdf](http://arxiv.org/pdf/2107.05115v2)

> In spite of the improvements achieved by the several denoising algorithms over the years, many of them still fail at preserving the fine details of the image after denoising. This is as a result of the smooth-out effect they have on the images. Most neural network-based algorithms have achieved better quantitative performance than the classical denoising algorithms. However, they also suffer from qualitative (visual) performance as a result of the smooth-out effect. In this paper, we propose an algorithm to address this shortcoming. We propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image denoising. This algorithm performs collaborative denoising of image patches in the sparse domain using a set of optimized neural network models. This results in a fast algorithm that is able to excellently obtain a trade-off between noise removal and details preservation. Extensive experiments show that the DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and qualitatively (visually) better than many of the state-of-the-art denoising algorithms.

</details>

<details>

<summary>2021-07-16 11:04:35 - Contrastive Predictive Coding for Anomaly Detection</summary>

- *Puck de Haan, Sindy Löwe*

- `2107.07820v1` - [abs](http://arxiv.org/abs/2107.07820v1) - [pdf](http://arxiv.org/pdf/2107.07820v1)

> Reliable detection of anomalies is crucial when deploying machine learning models in practice, but remains challenging due to the lack of labeled data. To tackle this challenge, contrastive learning approaches are becoming increasingly popular, given the impressive results they have achieved in self-supervised representation learning settings. However, while most existing contrastive anomaly detection and segmentation approaches have been applied to images, none of them can use the contrastive losses directly for both anomaly detection and segmentation. In this paper, we close this gap by making use of the Contrastive Predictive Coding model (arXiv:1807.03748). We show that its patch-wise contrastive loss can directly be interpreted as an anomaly score, and how this allows for the creation of anomaly segmentation masks. The resulting model achieves promising results for both anomaly detection and segmentation on the challenging MVTec-AD dataset.

</details>

<details>

<summary>2021-07-16 17:31:54 - Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems</summary>

- *Shaan Desai, Marios Mattheakis, David Sondak, Pavlos Protopapas, Stephen Roberts*

- `2107.08024v1` - [abs](http://arxiv.org/abs/2107.08024v1) - [pdf](http://arxiv.org/pdf/2107.08024v1)

> Accurately learning the temporal behavior of dynamical systems requires models with well-chosen learning biases. Recent innovations embed the Hamiltonian and Lagrangian formalisms into neural networks and demonstrate a significant improvement over other approaches in predicting trajectories of physical systems. These methods generally tackle autonomous systems that depend implicitly on time or systems for which a control signal is known apriori. Despite this success, many real world dynamical systems are non-autonomous, driven by time-dependent forces and experience energy dissipation. In this study, we address the challenge of learning from such non-autonomous systems by embedding the port-Hamiltonian formalism into neural networks, a versatile framework that can capture energy dissipation and time-dependent control forces. We show that the proposed \emph{port-Hamiltonian neural network} can efficiently learn the dynamics of nonlinear physical systems of practical interest and accurately recover the underlying stationary Hamiltonian, time-dependent force, and dissipative coefficient. A promising outcome of our network is its ability to learn and predict chaotic systems such as the Duffing equation, for which the trajectories are typically hard to learn.

</details>

<details>

<summary>2021-07-17 20:47:22 - Making transport more robust and interpretable by moving data through a small number of anchor points</summary>

- *Chi-Heng Lin, Mehdi Azabou, Eva L. Dyer*

- `2012.11589v3` - [abs](http://arxiv.org/abs/2012.11589v3) - [pdf](http://arxiv.org/pdf/2012.11589v3)

> Optimal transport (OT) is a widely used technique for distribution alignment, with applications throughout the machine learning, graphics, and vision communities. Without any additional structural assumptions on trans-port, however, OT can be fragile to outliers or noise, especially in high dimensions. Here, we introduce a new form of structured OT that simultaneously learns low-dimensional structure in data while leveraging this structure to solve the alignment task. Compared with OT, the resulting transport plan has better structural interpretability, highlighting the connections between individual data points and local geometry, and is more robust to noise and sampling. We apply the method to synthetic as well as real datasets, where we show that our method can facilitate alignment in noisy settings and can be used to both correct and interpret domain shift.

</details>

<details>

<summary>2021-07-19 11:34:09 - CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software</summary>

- *Guru Prasad Bhandari, Amara Naseer, Leon Moonen*

- `2107.08760v1` - [abs](http://arxiv.org/abs/2107.08760v1) - [pdf](http://arxiv.org/pdf/2107.08760v1)

> Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the public National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes.   The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits.   CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.

</details>

<details>

<summary>2021-07-19 13:05:51 - Automatic and explainable grading of meningiomas from histopathology images</summary>

- *Jonathan Ganz, Tobias Kirsch, Lucas Hoffmann, Christof A. Bertram, Christoph Hoffmann, Andreas Maier, Katharina Breininger, Ingmar Blümcke, Samir Jabari, Marc Aubreville*

- `2107.08850v1` - [abs](http://arxiv.org/abs/2107.08850v1) - [pdf](http://arxiv.org/pdf/2107.08850v1)

> Meningioma is one of the most prevalent brain tumors in adults. To determine its malignancy, it is graded by a pathologist into three grades according to WHO standards. This grade plays a decisive role in treatment, and yet may be subject to inter-rater discordance. In this work, we present and compare three approaches towards fully automatic meningioma grading from histology whole slide images. All approaches are following a two-stage paradigm, where we first identify a region of interest based on the detection of mitotic figures in the slide using a state-of-the-art object detection deep learning network. This region of highest mitotic rate is considered characteristic for biological tumor behavior. In the second stage, we calculate a score corresponding to tumor malignancy based on information contained in this region using three different settings. In a first approach, image patches are sampled from this region and regression is based on morphological features encoded by a ResNet-based network. We compare this to learning a logistic regression from the determined mitotic count, an approach which is easily traceable and explainable. Lastly, we combine both approaches in a single network. We trained the pipeline on 951 slides from 341 patients and evaluated them on a separate set of 141 slides from 43 patients. All approaches yield a high correlation to the WHO grade. The logistic regression and the combined approach had the best results in our experiments, yielding correct predictions in 32 and 33 of all cases, respectively, with the image-based approach only predicting 25 cases correctly. Spearman's correlation was 0.716, 0.792 and 0.790 respectively. It may seem counterintuitive at first that morphological features provided by image patches do not improve model performance. Yet, this mirrors the criteria of the grading scheme, where mitotic count is the only unequivocal parameter.

</details>

<details>

<summary>2021-07-22 18:40:59 - Access Structures Determined by Uniform Polymatroids</summary>

- *Renata Kawa, Mieczyslaw Kula*

- `2005.04509v2` - [abs](http://arxiv.org/abs/2005.04509v2) - [pdf](http://arxiv.org/pdf/2005.04509v2)

> An access structure is said to be multipartite, if the set of participants is divided into several parts and all participants in the same part play an equivalent role. The search for ideal secret sharing schemes for some special interesting families of multipartite access structures, has been carried out by many authors. In this paper a new concept of study of ideal access structures is proposed. We do not consider special classes of access structures defined by imposing certain prescribed assumptions, but we investigate all access structures obtained from uniform polymatroids using the method developed by Farr\`as, Mart\'i-Farr\'e and Padr\'o. They satisfy necessary condition to be ideal, i.e., they are matroid ports. Moreover some objects in this family can be useful for the applications of secret sharing. The choice of uniform polymatroids is motivated by the fact that each such polymatroid defines ideal access structures. The method presented in this article is universal and can be continued with other classes of polymatroids in further similar studies. Here we are especially interested in hierarchy of participants determined by the access structure and we distinguish two main classes: they are compartmented and hierarchical access structures. The vast majority of papers discussing hierarchical access structures consider access structures which are compartment or totally hierarchical. The main results are summarized in Section 4, which presents situations where partial hierarchy properties may arise. In particular, hierarchical orders of obtained structures are described. It is surprising, that the hierarchical orders of access structures obtained from uniform polymatroids are flat, i.e., every chain has at most 2 elements. The ideality of some families of hierarchical access structures is proved in Section 5.

</details>

<details>

<summary>2021-07-23 02:14:17 - Resource Efficient Mountainous Skyline Extraction using Shallow Learning</summary>

- *Touqeer Ahmad, Ebrahim Emami, Martin Čadík, George Bebis*

- `2107.10997v1` - [abs](http://arxiv.org/abs/2107.10997v1) - [pdf](http://arxiv.org/pdf/2107.10997v1)

> Skyline plays a pivotal role in mountainous visual geo-localization and localization/navigation of planetary rovers/UAVs and virtual/augmented reality applications. We present a novel mountainous skyline detection approach where we adapt a shallow learning approach to learn a set of filters to discriminate between edges belonging to sky-mountain boundary and others coming from different regions. Unlike earlier approaches, which either rely on extraction of explicit feature descriptors and their classification, or fine-tuning general scene parsing deep networks for sky segmentation, our approach learns linear filters based on local structure analysis. At test time, for every candidate edge pixel, a single filter is chosen from the set of learned filters based on pixel's structure tensor, and then applied to the patch around it. We then employ dynamic programming to solve the shortest path problem for the resultant multistage graph to get the sky-mountain boundary. The proposed approach is computationally faster than earlier methods while providing comparable performance and is more suitable for resource constrained platforms e.g., mobile devices, planetary rovers and UAVs. We compare our proposed approach against earlier skyline detection methods using four different data sets. Our code is available at \url{https://github.com/TouqeerAhmad/skyline_detection}.

</details>

<details>

<summary>2021-07-23 18:47:56 - Estimation of excess air coefficient on coal combustion processes via gauss model and artificial neural network</summary>

- *Sedat Golgiyaz, Muhammed Fatih Talu, Mahmut Daskin, Cem Onat*

- `2108.04180v1` - [abs](http://arxiv.org/abs/2108.04180v1) - [pdf](http://arxiv.org/pdf/2108.04180v1)

> It is no doubt that the most important contributing cause of global efficiency of coal fired thermal systems is combustion efficiency. In this study, the relationship between the flame image obtained by a CCD camera and the excess air coefficient ({\lambda}) has been modelled. The model has been obtained with a three-stage approach: 1) Data collection and synchronization: Obtaining the flame images by means of a CCD camera mounted on a 10 cm diameter observation port, {\lambda} data has been coordinately measured and recorded by the flue gas analyzer. 2) Feature extraction: Gridding the flame image, it is divided into small pieces. The uniformity of each piece to the optimal flame image has been calculated by means of modelling with single and multivariable Gaussian, calculating of color probabilities and Gauss mixture approach. 3) Matching and testing: A multilayer artificial neural network (ANN) has been used for the matching of feature-{\lambda}.

</details>

<details>

<summary>2021-07-25 11:39:59 - Vehicle-Rear: A New Dataset to Explore Feature Fusion for Vehicle Identification Using Convolutional Neural Networks</summary>

- *Icaro O. de Oliveira, Rayson Laroca, David Menotti, Keiko V. O. Fonseca, Rodrigo Minetto*

- `1911.05541v3` - [abs](http://arxiv.org/abs/1911.05541v3) - [pdf](http://arxiv.org/pdf/1911.05541v3)

> This work addresses the problem of vehicle identification through non-overlapping cameras. As our main contribution, we introduce a novel dataset for vehicle identification, called Vehicle-Rear, that contains more than three hours of high-resolution videos, with accurate information about the make, model, color and year of nearly 3,000 vehicles, in addition to the position and identification of their license plates. To explore our dataset we design a two-stream CNN that simultaneously uses two of the most distinctive and persistent features available: the vehicle's appearance and its license plate. This is an attempt to tackle a major problem: false alarms caused by vehicles with similar designs or by very close license plate identifiers. In the first network stream, shape similarities are identified by a Siamese CNN that uses a pair of low-resolution vehicle patches recorded by two different cameras. In the second stream, we use a CNN for OCR to extract textual information, confidence scores, and string similarities from a pair of high-resolution license plate patches. Then, features from both streams are merged by a sequence of fully connected layers for decision. In our experiments, we compared the two-stream network against several well-known CNN architectures using single or multiple vehicle features. The architectures, trained models, and dataset are publicly available at https://github.com/icarofua/vehicle-rear.

</details>

<details>

<summary>2021-07-26 14:08:31 - Local2Global: Scaling global representation learning on graphs via local training</summary>

- *Lucas G. S. Jeub, Giovanni Colavizza, Xiaowen Dong, Marya Bazzi, Mihai Cucuringu*

- `2107.12224v1` - [abs](http://arxiv.org/abs/2107.12224v1) - [pdf](http://arxiv.org/pdf/2107.12224v1)

> We propose a decentralised "local2global" approach to graph representation learning, that one can a-priori use to scale any embedding technique. Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or "patches") and training local representations for each patch independently. In a second step, we combine the local representations into a globally consistent representation by estimating the set of rigid motions that best align the local representations using information from the patch overlaps, via group synchronization. A key distinguishing feature of local2global relative to existing work is that patches are trained independently without the need for the often costly parameter synchronisation during distributed training. This allows local2global to scale to large-scale industrial applications, where the input graph may not even fit into memory and may be stored in a distributed manner. Preliminary results on medium-scale data sets (up to $\sim$7K nodes and $\sim$200K edges) are promising, with a graph reconstruction performance for local2global that is comparable to that of globally trained embeddings. A thorough evaluation of local2global on large scale data and applications to downstream tasks, such as node classification and link prediction, constitutes ongoing work.

</details>

<details>

<summary>2021-07-26 18:31:59 - HySec-Flow: Privacy-Preserving Genomic Computing with SGX-based Big-Data Analytics Framework</summary>

- *Chathura Widanage, Weijie Liu, Jiayu Li, Hongbo Chen, XiaoFeng Wang, Haixu Tang, Judy Fox*

- `2107.12423v1` - [abs](http://arxiv.org/abs/2107.12423v1) - [pdf](http://arxiv.org/pdf/2107.12423v1)

> Trusted execution environments (TEE) such as Intel's Software Guard Extension (SGX) have been widely studied to boost security and privacy protection for the computation of sensitive data such as human genomics. However, a performance hurdle is often generated by SGX, especially from the small enclave memory. In this paper, we propose a new Hybrid Secured Flow framework (called "HySec-Flow") for large-scale genomic data analysis using SGX platforms. Here, the data-intensive computing tasks can be partitioned into independent subtasks to be deployed into distinct secured and non-secured containers, therefore allowing for parallel execution while alleviating the limited size of Page Cache (EPC) memory in each enclave. We illustrate our contributions using a workflow supporting indexing, alignment, dispatching, and merging the execution of SGX- enabled containers. We provide details regarding the architecture of the trusted and untrusted components and the underlying Scorn and Graphene support as generic shielding execution frameworks to port legacy code. We thoroughly evaluate the performance of our privacy-preserving reads mapping algorithm using real human genome sequencing data. The results demonstrate that the performance is enhanced by partitioning the time-consuming genomic computation into subtasks compared to the conventional execution of the data-intensive reads mapping algorithm in an enclave. The proposed HySec-Flow framework is made available as an open-source and adapted to the data-parallel computation of other large-scale genomic tasks requiring security and scalable computational resources.

</details>

<details>

<summary>2021-07-27 13:02:06 - SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization</summary>

- *A. F. M. Shahab Uddin, Mst. Sirazam Monira, Wheemyung Shin, TaeChoong Chung, Sung-Ho Bae*

- `2006.01791v2` - [abs](http://arxiv.org/abs/2006.01791v2) - [pdf](http://arxiv.org/pdf/2006.01791v2)

> Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. However, such information removal is undesirable. On the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selection strategies of the patches may not necessarily represent sufficient information about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature representation. Therefore, we propose SaliencyMix that carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image, thus leading the model to learn more appropriate feature representation. SaliencyMix achieves the best known top-1 error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on ImageNet classification, respectively, and also improves the model robustness against adversarial perturbations. Furthermore, models that are trained with SaliencyMix help to improve the object detection performance. Source code is available at https://github.com/SaliencyMix/SaliencyMix.

</details>

<details>

<summary>2021-07-27 15:41:13 - DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation</summary>

- *Li Gao, Jing Zhang, Lefei Zhang, Dacheng Tao*

- `2107.09600v3` - [abs](http://arxiv.org/abs/2107.09600v3) - [pdf](http://arxiv.org/pdf/2107.09600v3)

> Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt a segmentation model trained on the labeled source domain to the unlabeled target domain. Existing methods try to learn domain invariant features while suffering from large domain gaps that make it difficult to correctly align discrepant features, especially in the initial training phase. To address this issue, we propose a novel Dual Soft-Paste (DSP) method in this paper. Specifically, DSP selects some classes from a source domain image using a long-tail class first sampling strategy and softly pastes the corresponding image patch on both the source and target training images with a fusion weight. Technically, we adopt the mean teacher framework for domain adaptation, where the pasted source and target images go through the student network while the original target image goes through the teacher network. Output-level alignment is carried out by aligning the probability maps of the target fused image from both networks using a weighted cross-entropy loss. In addition, feature-level alignment is carried out by aligning the feature maps of the source and target images from student network using a weighted maximum mean discrepancy loss. DSP facilitates the model learning domain-invariant features from the intermediate domains, leading to faster convergence and better performance. Experiments on two challenging benchmarks demonstrate the superiority of DSP over state-of-the-art methods. Code is available at \url{https://github.com/GaoLii/DSP}.

</details>

<details>

<summary>2021-07-29 12:46:51 - Firmware Re-hosting Through Static Binary-level Porting</summary>

- *Mingfeng Xin, Hui Wen, Liting Deng, Hong Li, Qiang Li, Limin Sun*

- `2107.09856v2` - [abs](http://arxiv.org/abs/2107.09856v2) - [pdf](http://arxiv.org/pdf/2107.09856v2)

> The rapid growth of the Industrial Internet of Things (IIoT) has brought embedded systems into focus as major targets for both security analysts and malicious adversaries. Due to the non-standard hardware and diverse software, embedded devices present unique challenges to security analysts for the accurate analysis of firmware binaries. The diversity in hardware components and tight coupling between firmware and hardware makes it hard to perform dynamic analysis, which must have the ability to execute firmware code in virtualized environments. However, emulating the large expanse of hardware peripherals makes analysts have to frequently modify the emulator for executing various firmware code in different virtualized environments, greatly limiting the ability of security analysis.   In this work, we explore the problem of firmware re-hosting related to the real-time operating system (RTOS). Specifically, developers create a Board Support Package (BSP) and develop device drivers to make that RTOS run on their platform. By providing high-level replacements for BSP routines and device drivers, we can make the minimal modification of the firmware that is to be migrated from its original hardware environment into a virtualized one. We show that an approach capable of offering the ability to execute firmware at scale through patching firmware in an automated manner without modifying the existing emulators. Our approach, called static binary-level porting, first identifies the BSP and device drivers in target firmware, then patches the firmware with pre-built BSP routines and drivers that can be adapted to the existing emulators. Finally, we demonstrate the practicality of the proposed method on multiple hardware platforms and firmware samples for security analysis. The result shows that the approach is flexible enough to emulate firmware for vulnerability assessment and exploits development.

</details>

<details>

<summary>2021-07-30 17:02:04 - Tiny Machine Learning for Concept Drift</summary>

- *Simone Disabato, Manuel Roveri*

- `2107.14759v1` - [abs](http://arxiv.org/abs/2107.14759v1) - [pdf](http://arxiv.org/pdf/2107.14759v1)

> Tiny Machine Learning (TML) is a new research area whose goal is to design machine and deep learning techniques able to operate in Embedded Systems and IoT units, hence satisfying the severe technological constraints on memory, computation, and energy characterizing these pervasive devices. Interestingly, the related literature mainly focused on reducing the computational and memory demand of the inference phase of machine and deep learning models. At the same time, the training is typically assumed to be carried out in Cloud or edge computing systems (due to the larger memory and computational requirements). This assumption results in TML solutions that might become obsolete when the process generating the data is affected by concept drift (e.g., due to periodicity or seasonality effect, faults or malfunctioning affecting sensors or actuators, or changes in the users' behavior), a common situation in real-world application scenarios. For the first time in the literature, this paper introduces a Tiny Machine Learning for Concept Drift (TML-CD) solution based on deep learning feature extractors and a k-nearest neighbors classifier integrating a hybrid adaptation module able to deal with concept drift affecting the data-generating process. This adaptation module continuously updates (in a passive way) the knowledge base of TML-CD and, at the same time, employs a Change Detection Test to inspect for changes (in an active way) to quickly adapt to concept drift by removing the obsolete knowledge. Experimental results on both image and audio benchmarks show the effectiveness of the proposed solution, whilst the porting of TML-CD on three off-the-shelf micro-controller units shows the feasibility of what is proposed in real-world pervasive systems.

</details>

<details>

<summary>2021-07-30 19:08:44 - Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning</summary>

- *Faisal Alamri, Anjan Dutta*

- `2108.00045v1` - [abs](http://arxiv.org/abs/2108.00045v1) - [pdf](http://arxiv.org/pdf/2108.00045v1)

> Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are not observed during the training phase. The existing body of works on ZSL mostly relies on pretrained visual features and lacks the explicit attribute localisation mechanism on images. In this work, we propose an attention-based model in the problem settings of ZSL to learn attributes useful for unseen class recognition. Our method uses an attention mechanism adapted from Vision Transformer to capture and learn discriminative attributes by splitting images into small patches. We conduct experiments on three popular ZSL benchmarks (i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results {on all the three datasets}, which illustrate the effectiveness of our proposed method.

</details>

<details>

<summary>2021-07-31 03:37:19 - Applying Declarative Analysis to Software Product Line Models: An Industrial Study</summary>

- *Ramy Shahin, Robert Hackman, Rafael Toledo, Ramesh S, Joanne M. Atlee, Marsha Chechik*

- `2107.07690v2` - [abs](http://arxiv.org/abs/2107.07690v2) - [pdf](http://arxiv.org/pdf/2107.07690v2)

> Software Product Lines (SPLs) are families of related software products developed from a common set of artifacts. Most existing analysis tools can be applied to a single product at a time, but not to an entire SPL. Some tools have been redesigned/re-implemented to support the kind of variability exhibited in SPLs, but this usually takes a lot of effort, and is error-prone. Declarative analyses written in languages like Datalog have been collectively lifted to SPLs in prior work, which makes the process of applying an existing declarative analysis to a product line more straightforward.   In this paper, we take an existing declarative analysis (behaviour alteration) written in the Grok declarative language, port it to Datalog, and apply it to a set of automotive software product lines from General Motors. We discuss the design of the analysis pipeline used in this process, present its scalability results, and provide a means to visualize the analysis results for a subset of products filtered by feature expression. We also reflect on some of the lessons learned throughout this project.

</details>

<details>

<summary>2021-07-31 09:58:57 - DCT2net: an interpretable shallow CNN for image denoising</summary>

- *Sébastien Herbreteau, Charles Kervrann*

- `2107.14803v1` - [abs](http://arxiv.org/abs/2107.14803v1) - [pdf](http://arxiv.org/pdf/2107.14803v1)

> This work tackles the issue of noise removal from images, focusing on the well-known DCT image denoising algorithm. The latter, stemming from signal processing, has been well studied over the years. Though very simple, it is still used in crucial parts of state-of-the-art "traditional" denoising algorithms such as BM3D. Since a few years however, deep convolutional neural networks (CNN) have outperformed their traditional counterparts, making signal processing methods less attractive. In this paper, we demonstrate that a DCT denoiser can be seen as a shallow CNN and thereby its original linear transform can be tuned through gradient descent in a supervised manner, improving considerably its performance. This gives birth to a fully interpretable CNN called DCT2net. To deal with remaining artifacts induced by DCT2net, an original hybrid solution between DCT and DCT2net is proposed combining the best that these two methods can offer; DCT2net is selected to process non-stationary image patches while DCT is optimal for piecewise smooth patches. Experiments on artificially noisy images demonstrate that two-layer DCT2net provides comparable results to BM3D and is as fast as DnCNN algorithm composed of more than a dozen of layers.

</details>


## 2021-08

<details>

<summary>2021-08-04 15:50:09 - Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images</summary>

- *Boon Peng Yap, Beng Koon Ng*

- `2108.02122v1` - [abs](http://arxiv.org/abs/2108.02122v1) - [pdf](http://arxiv.org/pdf/2108.02122v1)

> We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pretraining on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining.

</details>

<details>

<summary>2021-08-04 18:24:18 - Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks</summary>

- *Nitish Bhatt, David Ramon Prados, Nedim Hodzic, Christos Karanassios, H. R. Tizhoosh*

- `2108.02233v1` - [abs](http://arxiv.org/abs/2108.02233v1) - [pdf](http://arxiv.org/pdf/2108.02233v1)

> Lung nodules are commonly missed in chest radiographs. We propose and evaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules in radiographs. P-AnoGAN modifies the fast anomaly detection generative adversarial network (f-AnoGAN) by utilizing a progressive GAN and a convolutional encoder-decoder-encoder pipeline. Model training uses only unlabelled healthy lung patches extracted from the Indiana University Chest X-Ray Collection. External validation and testing are performed using healthy and unhealthy patches extracted from the ChestX-ray14 and Japanese Society for Radiological Technology datasets, respectively. Our model robustly identifies patches containing lung nodules in external validation and test data with ROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised methods may be useful in challenging tasks such as lung nodule detection in radiographs.

</details>

<details>

<summary>2021-08-05 10:48:15 - TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning</summary>

- *Fernando Pérez-García, Rachel Sparks, Sébastien Ourselin*

- `2003.04696v5` - [abs](http://arxiv.org/abs/2003.04696v5) - [pdf](http://arxiv.org/pdf/2003.04696v5)

> Processing of medical images such as MRI or CT presents unique challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment of volumes.   We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be composed, reproduced, traced and extended. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.   Source code, comprehensive tutorials and extensive documentation for TorchIO can be found at https://torchio.rtfd.io/. The package can be installed from the Python Package Index running 'pip install torchio'. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms.   TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages open science, as it supports reproducibility and is version controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.

</details>

<details>

<summary>2021-08-05 16:13:36 - Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images</summary>

- *Juan P. Vigueras-Guillén, Joan Lasenby, Frank Seeliger*

- `2108.02704v1` - [abs](http://arxiv.org/abs/2108.02704v1) - [pdf](http://arxiv.org/pdf/2108.02704v1)

> Regularization in convolutional neural networks (CNNs) is usually addressed with dropout layers. However, dropout is sometimes detrimental in the convolutional part of a CNN as it simply sets to zero a percentage of pixels in the feature maps, adding unrepresentative examples during training. Here, we propose a CNN layer that performs regularization by applying random rotations of reflections to a small percentage of feature maps after every convolutional layer. We prove how this concept is beneficial for images with orientational symmetries, such as in medical images, as it provides a certain degree of rotational invariance. We tested this method in two datasets, a patch-based set of histopathology images (PatchCamelyon) to perform classification using a generic DenseNet, and a set of specular microscopy images of the corneal endothelium to perform segmentation using a tailored U-net, improving the performance in both cases.

</details>

<details>

<summary>2021-08-06 04:52:09 - Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles</summary>

- *Jindi Zhang, Yang Lou, Jianping Wang, Kui Wu, Kejie Lu, Xiaohua Jia*

- `2108.02940v1` - [abs](http://arxiv.org/abs/2108.02940v1) - [pdf](http://arxiv.org/pdf/2108.02940v1)

> In recent years, many deep learning models have been adopted in autonomous driving. At the same time, these models introduce new vulnerabilities that may compromise the safety of autonomous vehicles. Specifically, recent studies have demonstrated that adversarial attacks can cause a significant decline in detection precision of deep learning-based 3D object detection models. Although driving safety is the ultimate concern for autonomous driving, there is no comprehensive study on the linkage between the performance of deep learning models and the driving safety of autonomous vehicles under adversarial attacks. In this paper, we investigate the impact of two primary types of adversarial attacks, perturbation attacks and patch attacks, on the driving safety of vision-based autonomous vehicles rather than the detection precision of deep learning models. In particular, we consider two state-of-the-art models in vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving safety, we propose an end-to-end evaluation framework with a set of driving safety performance metrics. By analyzing the results of our extensive evaluation experiments, we find that (1) the attack's impact on the driving safety of autonomous vehicles and the attack's impact on the precision of 3D object detectors are decoupled, and (2) the DSGN model demonstrates stronger robustness to adversarial attacks than the Stereo R-CNN model. In addition, we further investigate the causes behind the two findings with an ablation study. The findings of this paper provide a new perspective to evaluate adversarial attacks and guide the selection of deep learning models in autonomous driving.

</details>

<details>

<summary>2021-08-06 05:56:14 - HIPPODROME: Data Race Repair using Static Analysis Summaries</summary>

- *Andreea Costea, Abhishek Tiwari, Sigmund Chianasta, Kishore R, Abhik Roychoudhury, Ilya Sergey*

- `2108.02490v2` - [abs](http://arxiv.org/abs/2108.02490v2) - [pdf](http://arxiv.org/pdf/2108.02490v2)

> Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way.   In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting, into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyse and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study conducted on popular open-source projects has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.

</details>

<details>

<summary>2021-08-06 13:35:59 - Automated Classification of Overfitting Patches with Statically Extracted Code Features</summary>

- *He Ye, Jian Gu, Matias Martinez, Thomas Durieux, Martin Monperrus*

- `1910.12057v2` - [abs](http://arxiv.org/abs/1910.12057v2) - [pdf](http://arxiv.org/pdf/1910.12057v2)

> Automatic program repair (APR) aims to reduce the cost of manually fixing software defects. However, APR suffers from generating a multitude of overfitting patches, those patches that fail to correctly repair the defect beyond making the tests pass. This paper presents a novel overfitting patch detection system called ODS to assess the correctness of APR patches. ODS first statically compares a patched program and a buggy program in order to extract code features at the abstract syntax tree (AST) level. Then, ODS uses supervised learning with the captured code features and patch correctness labels to automatically learn a probabilistic model. The learned ODS model can then finally be applied to classify new and unseen program repair patches. We conduct a large-scale experiment to evaluate the effectiveness of ODS on patch correctness classification based on 10,302 patches from Defects4J, Bugs.jar and Bears benchmarks. The empirical evaluation shows that ODS is able to correctly classify 71.9% of program repair patches from 26 projects, which improves the state-of-the-art. ODS is applicable in practice and can be employed as a post-processing procedure to classify the patches generated by different APR systems.

</details>

<details>

<summary>2021-08-08 18:13:53 - OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning</summary>

- *Sheng Liu, Kevin Lin, Lijuan Wang, Junsong Yuan, Zicheng Liu*

- `2108.03704v1` - [abs](http://arxiv.org/abs/2108.03704v1) - [pdf](http://arxiv.org/pdf/2108.03704v1)

> We introduce the task of open-vocabulary visual instance search (OVIS). Given an arbitrary textual search query, Open-vocabulary Visual Instance Search (OVIS) aims to return a ranked list of visual instances, i.e., image patches, that satisfies the search intent from an image database. The term "open vocabulary" means that there are neither restrictions to the visual instance to be searched nor restrictions to the word that can be used to compose the textual search query. We propose to address such a search challenge via visual-semantic aligned representation learning (ViSA). ViSA leverages massive image-caption pairs as weak image-level (not instance-level) supervision to learn a rich cross-modal semantic space where the representations of visual instances (not images) and those of textual queries are aligned, thus allowing us to measure the similarities between any visual instance and an arbitrary textual query. To evaluate the performance of ViSA, we build two datasets named OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through extensive experiments on the two datasets, we demonstrate ViSA's ability to search for visual instances in images not available during training given a wide range of textual queries including those composed of uncommon words. Experimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under the most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600 dataset.

</details>

<details>

<summary>2021-08-10 10:06:54 - Deep Joint Learning of Pathological Region Localization and Alzheimer's Disease Diagnosis</summary>

- *Changhyun Park, Heung-Il Suk*

- `2108.04555v1` - [abs](http://arxiv.org/abs/2108.04555v1) - [pdf](http://arxiv.org/pdf/2108.04555v1)

> The identification of Alzheimer's disease (AD) and its early stages using structural magnetic resonance imaging (MRI) has been attracting the attention of researchers. Various data-driven approaches have been introduced to capture subtle and local morphological changes of the brain accompanied by the disease progression. One of the typical approaches for capturing subtle changes is patch-level feature representation. However, the predetermined regions to extract patches can limit classification performance by interrupting the exploration of potential biomarkers. In addition, the existing patch-level analyses have difficulty explaining their decision-making. To address these problems, we propose the BrainBagNet with a position-based gate (PG-BrainBagNet), a framework for jointly learning pathological region localization and AD diagnosis in an end-to-end manner. In advance, as all scans are aligned to a template in image processing, the position of brain images can be represented through the 3D Cartesian space shared by the overall MRI scans. The proposed method represents the patch-level response from whole-brain MRI scans and discriminative brain-region from position information. Based on the outcomes, the patch-level class evidence is calculated, and then the image-level prediction is inferred by a transparent aggregation. The proposed models were evaluated on the ADNI datasets. In five-fold cross-validation, the classification performance of the proposed method outperformed that of the state-of-the-art methods in both AD diagnosis (AD vs. normal control) and mild cognitive impairment (MCI) conversion prediction (progressive MCI vs. stable MCI) tasks. In addition, changes in the identified discriminant regions and patch-level class evidence according to the patch size used for model training are presented and analyzed.

</details>

<details>

<summary>2021-08-12 08:55:17 - Automatically Locating ARM Instructions Deviation between Real Devices and CPU Emulators</summary>

- *Muhui Jiang, Tianyi Xu, Yajin Zhou, Yufeng Hu, Ming Zhong, Lei Wu, Xiapu Luo, Kui Ren*

- `2105.14273v2` - [abs](http://arxiv.org/abs/2105.14273v2) - [pdf](http://arxiv.org/pdf/2105.14273v2)

> Emulator is widely used to build dynamic analysis frameworks due to its fine-grained tracing capability, full system monitoring functionality, and scalability of running on different operating systemsand architectures. However, whether the emulator is consistent with real devices is unknown. To understand this problem, we aim to automatically locate inconsistent instructions, which behave differently between emulators and real devices.   We target ARM architecture, which provides machine readable specification. Based on the specification, we propose a test case generator by designing and implementing the first symbolic execution engine for ARM architecture specification language (ASL). We generate 2,774,649 representative instruction streams and conduct differential testing with these instruction streams between four ARM real devices in different architecture versions (i.e., ARMv5, ARMv6, ARMv7-a, and ARMv8-a) and the state-of-the-art emulators (i.e., QEMU). We locate 155,642 inconsistent instruction streams, which cover 30% of all instruction encodings and 47.8% of the instructions. We find undefined implementation in ARM manual and implementation bugs of QEMU are the major causes of inconsistencies. Furthermore, we discover four QEMU bugs, which are confirmed and patched by thedevelopers, covering 13 instruction encodings including the most commonly used ones (e.g.,STR,BLX). With the inconsistent instructions, we build three security applications and demonstrate thecapability of these instructions on detecting emulators, anti-emulation, and anti-fuzzing.

</details>

<details>

<summary>2021-08-12 17:54:53 - Probing the State of the Art: A Critical Look at Visual Representation Evaluation</summary>

- *Cinjon Resnick, Zeping Zhan, Joan Bruna*

- `1912.00215v2` - [abs](http://arxiv.org/abs/1912.00215v2) - [pdf](http://arxiv.org/pdf/1912.00215v2)

> Self-supervised research improved greatly over the past half decade, with much of the growth being driven by objectives that are hard to quantitatively compare. These techniques include colorization, cyclical consistency, and noise-contrastive estimation from image patches. Consequently, the field has settled on a handful of measurements that depend on linear probes to adjudicate which approaches are the best. Our first contribution is to show that this test is insufficient and that models which perform poorly (strongly) on linear classification can perform strongly (weakly) on more involved tasks like temporal activity localization. Our second contribution is to analyze the capabilities of five different representations. And our third contribution is a much needed new dataset for temporal activity localization.

</details>

<details>

<summary>2021-08-13 06:29:21 - Asteria: Deep Learning-based AST-Encoding for Cross-platform Binary Code Similarity Detection</summary>

- *Shouguo Yang, Long Cheng, Yicheng Zeng, Zhe Lang, Hongsong Zhu, Zhiqiang Shi*

- `2108.06082v1` - [abs](http://arxiv.org/abs/2108.06082v1) - [pdf](http://arxiv.org/pdf/2108.06082v1)

> Binary code similarity detection is a fundamental technique for many security applications such as vulnerability search, patch analysis, and malware detection. There is an increasing need to detect similar code for vulnerability search across architectures with the increase of critical vulnerabilities in IoT devices. The variety of IoT hardware architectures and software platforms requires to capture semantic equivalence of code fragments in the similarity detection. However, existing approaches are insufficient in capturing the semantic similarity. We notice that the abstract syntax tree (AST) of a function contains rich semantic information. Inspired by successful applications of natural language processing technologies in sentence semantic understanding, we propose a deep learning-based AST-encoding method, named ASTERIA, to measure the semantic equivalence of functions in different platforms. Our method leverages the Tree-LSTM network to learn the semantic representation of a function from its AST. Then the similarity detection can be conducted efficiently and accurately by measuring the similarity between two representation vectors. We have implemented an open-source prototype of ASTERIA. The Tree-LSTM model is trained on a dataset with 1,022,616 function pairs and evaluated on a dataset with 95,078 function pairs. Evaluation results show that our method outperforms the AST-based tool Diaphora and the-state-of-art method Gemini by large margins with respect to the binary similarity detection. And our method is several orders of magnitude faster than Diaphora and Gemini for the similarity calculation. In the application of vulnerability search, our tool successfully identified 75 vulnerable functions in 5,979 IoT firmware images.

</details>

<details>

<summary>2021-08-14 17:08:03 - Few-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models</summary>

- *Guanqun Yang, Shay Dineen, Zhipeng Lin, Xueqing Liu*

- `2108.06590v1` - [abs](http://arxiv.org/abs/2108.06590v1) - [pdf](http://arxiv.org/pdf/2108.06590v1)

> Public security vulnerability reports (e.g., CVE reports) play an important role in the maintenance of computer and network systems. Security companies and administrators rely on information from these reports to prioritize tasks on developing and deploying patches to their customers. Since these reports are unstructured texts, automatic information extraction (IE) can help scale up the processing by converting the unstructured reports to structured forms, e.g., software names and versions and vulnerability types. Existing works on automated IE for security vulnerability reports often rely on a large number of labeled training samples. However, creating massive labeled training set is both expensive and time consuming. In this work, for the first time, we propose to investigate this problem where only a small number of labeled training samples are available. In particular, we investigate the performance of fine-tuning several state-of-the-art pre-trained language models on our small training dataset. The results show that with pre-trained language models and carefully tuned hyperparameters, we have reached or slightly outperformed the state-of-the-art system on this task. Consistent with previous two-step process of first fine-tuning on main category and then transfer learning to others as in [7], if otherwise following our proposed approach, the number of required labeled samples substantially decrease in both stages: 90% reduction in fine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64 labeled samples per category. Our experiments thus demonstrate the effectiveness of few-sample learning on NER for security vulnerability report. This result opens up multiple research opportunities for few-sample learning for security vulnerability reports, which is discussed in the paper. Code: https://github.com/guanqun-yang/FewVulnerability.

</details>

<details>

<summary>2021-08-15 02:06:49 - On Multi-Modal Learning of Editing Source Code</summary>

- *Saikat Chakraborty, Baishakhi Ray*

- `2108.06645v1` - [abs](http://arxiv.org/abs/2108.06645v1) - [pdf](http://arxiv.org/pdf/2108.06645v1)

> In recent years, Neural Machine Translator (NMT) has shown promise in automatically editing source code. Typical NMT based code editor only considers the code that needs to be changed as input and suggests developers with a ranked list of patched code to choose from - where the correct one may not always be at the top of the list. While NMT based code editing systems generate a broad spectrum of plausible patches, the correct one depends on the developers' requirement and often on the context where the patch is applied. Thus, if developers provide some hints, using natural language, or providing patch context, NMT models can benefit from them. As a proof of concept, in this research, we leverage three modalities of information: edit location, edit code context, commit messages (as a proxy of developers' hint in natural language) to automatically generate edits with NMT models. To that end, we build MODIT, a multi-modal NMT based code editing engine. With in-depth investigation and analysis, we show that developers' hint as an input modality can narrow the search space for patches and outperform state-of-the-art models to generate correctly patched code in top-1 position.

</details>

<details>

<summary>2021-08-16 17:02:38 - Patch Attack Invariance: How Sensitive are Patch Attacks to 3D Pose?</summary>

- *Max Lennon, Nathan Drenkow, Philippe Burlina*

- `2108.07229v1` - [abs](http://arxiv.org/abs/2108.07229v1) - [pdf](http://arxiv.org/pdf/2108.07229v1)

> Perturbation-based attacks, while not physically realizable, have been the main emphasis of adversarial machine learning (ML) research. Patch-based attacks by contrast are physically realizable, yet most work has focused on 2D domain with recent forays into 3D. Characterizing the robustness properties of patch attacks and their invariance to 3D pose is important, yet not fully elucidated, and is the focus of this paper. To this end, several contributions are made here: A) we develop a new metric called mean Attack Success over Transformations (mAST) to evaluate patch attack robustness and invariance; and B), we systematically assess robustness of patch attacks to 3D position and orientation for various conditions; in particular, we conduct a sensitivity analysis which provides important qualitative insights into attack effectiveness as a function of the 3D pose of a patch relative to the camera (rotation, translation) and sets forth some properties for patch attack 3D invariance; and C), we draw novel qualitative conclusions including: 1) we demonstrate that for some 3D transformations, namely rotation and loom, increasing the training distribution support yields an increase in patch success over the full range at test time. 2) We provide new insights into the existence of a fundamental cutoff limit in patch attack effectiveness that depends on the extent of out-of-plane rotation angles. These findings should collectively guide future design of 3D patch attacks and defenses.

</details>

<details>

<summary>2021-08-18 04:59:26 - Adaptive Focus for Efficient Video Recognition</summary>

- *Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, Gao Huang*

- `2105.03245v2` - [abs](http://arxiv.org/abs/2105.03245v2) - [pdf](http://arxiv.org/pdf/2105.03245v2)

> In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. It is observed that the most informative region in each frame of a video is usually a small image patch, which shifts smoothly across frames. Therefore, we model the patch localization problem as a sequential decision task, and propose a reinforcement learning based approach for efficient spatially adaptive video recognition (AdaFocus). In specific, a light-weighted ConvNet is first adopted to quickly process the full video sequence, whose features are used by a recurrent policy network to localize the most task-relevant regions. Then the selected patches are inferred by a high-capacity network for the final prediction. During offline inference, once the informative patch sequence has been generated, the bulk of computation can be done in parallel, and is efficient on modern GPU devices. In addition, we demonstrate that the proposed method can be easily extended by further considering the temporal redundancy, e.g., dynamically skipping less valuable frames. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is significantly more efficient than the competitive baselines. Code is available at https://github.com/blackfeather-wang/AdaFocus.

</details>

<details>

<summary>2021-08-19 12:14:50 - Batch Curation for Unsupervised Contrastive Representation Learning</summary>

- *Michael C. Welle, Petra Poklukar, Danica Kragic*

- `2108.08643v1` - [abs](http://arxiv.org/abs/2108.08643v1) - [pdf](http://arxiv.org/pdf/2108.08643v1)

> The state-of-the-art unsupervised contrastive visual representation learning methods that have emerged recently (SimCLR, MoCo, SwAV) all make use of data augmentations in order to construct a pretext task of instant discrimination consisting of similar and dissimilar pairs of images. Similar pairs are constructed by randomly extracting patches from the same image and applying several other transformations such as color jittering or blurring, while transformed patches from different image instances in a given batch are regarded as dissimilar pairs. We argue that this approach can result similar pairs that are \textit{semantically} dissimilar. In this work, we address this problem by introducing a \textit{batch curation} scheme that selects batches during the training process that are more inline with the underlying contrastive objective. We provide insights into what constitutes beneficial similar and dissimilar pairs as well as validate \textit{batch curation} on CIFAR10 by integrating it in the SimCLR model.

</details>

<details>

<summary>2021-08-19 14:03:34 - An Agnostic Domain Specific Language for Implementing Attacks in an Automotive Use Case</summary>

- *Christian Wolschke, Stefan Marksteiner, Tobias Braun, Markus Wolf*

- `2107.02916v3` - [abs](http://arxiv.org/abs/2107.02916v3) - [pdf](http://arxiv.org/pdf/2107.02916v3)

> This paper presents a Domain Specific Language (DSL) for generically describing cyber attacks, agnostic to specific system-under-test(SUT). The creation of the presented DSL is motivated by an automotive use case. The concepts of the DSL are generic such thatattacks on arbitrary systems can be addressed.The ongoing trend to improve the user experience of vehicles with connected services implies an enhanced connectivity as well asremote accessible interface opens potential attack vectors. This might also impact safety and the proprietary nature of potential SUTs.Reusing tests of attack vectors to industrialize testing them on multiple SUTs mandates an abstraction mechanism to port an attackfrom one system to another. The DSL therefore generically describes attacks for the usage with a test case generator (and executionenvironment) also described in this paper. The latter use this description and a database with SUT-specific information to generateattack implementations for a multitude of different (automotive) SUTs.

</details>

<details>

<summary>2021-08-20 02:33:40 - Software Security Patch Management -- A Systematic Literature Review of Challenges, Approaches, Tools and Practices</summary>

- *Nesara Dissanayake, Asangi Jayatilaka, Mansooreh Zahedi, M. Ali Babar*

- `2012.00544v3` - [abs](http://arxiv.org/abs/2012.00544v3) - [pdf](http://arxiv.org/pdf/2012.00544v3)

> Context: Software security patch management purports to support the process of patching known software security vulnerabilities. Given the increasing recognition of the importance of software security patch management, it is important and timely to systematically review and synthesise the relevant literature on this topic.   Objective: This paper aims at systematically reviewing the state of the art of software security patch management to identify the socio-technical challenges in this regard, reported solutions (i.e., approaches, tools, and practices), the rigour of the evaluation and the industrial relevance of the reported solutions, and to identify the gaps for future research.   Method: We conducted a systematic literature review of 72 studies published from 2002 to March 2020, with extended coverage until September 2020 through forward snowballing.   Results: We identify 14 socio-technical challenges, 18 solution approaches, tools and practices mapped onto the software security patch management process. We provide a mapping between the solutions and challenges to enable a reader to obtain a holistic overview of the gap areas. The findings also reveal that only 20.8% of the reported solutions have been rigorously evaluated in industrial settings.   Conclusion: Our results reveal that 50% of the common challenges have not been directly addressed in the solutions and that most of them (38.9%) address the challenges in one phase of the process, namely vulnerability scanning, assessment and prioritisation. Based on the results that highlight the important concerns in software security patch management and the lack of solutions, we recommend a list of future research directions. This study also provides useful insights about different opportunities for practitioners to adopt new solutions and understand the variations of their practical utility.

</details>

<details>

<summary>2021-08-22 07:46:50 - Detecting Video Game Player Burnout with the Use of Sensor Data and Machine Learning</summary>

- *Anton Smerdov, Andrey Somov, Evgeny Burnaev, Bo Zhou, Paul Lukowicz*

- `2012.02299v2` - [abs](http://arxiv.org/abs/2012.02299v2) - [pdf](http://arxiv.org/pdf/2012.02299v2)

> Current research in eSports lacks the tools for proper game practising and performance analytics. The majority of prior work relied only on in-game data for advising the players on how to perform better. However, in-game mechanics and trends are frequently changed by new patches limiting the lifespan of the models trained exclusively on the in-game logs. In this article, we propose the methods based on the sensor data analysis for predicting whether a player will win the future encounter. The sensor data were collected from 10 participants in 22 matches in League of Legends video game. We have trained machine learning models including Transformer and Gated Recurrent Unit to predict whether the player wins the encounter taking place after some fixed time in the future. For 10 seconds forecasting horizon Transformer neural network architecture achieves ROC AUC score 0.706. This model is further developed into the detector capable of predicting that a player will lose the encounter occurring in 10 seconds in 88.3% of cases with 73.5% accuracy. This might be used as a players' burnout or fatigue detector, advising players to retreat. We have also investigated which physiological features affect the chance to win or lose the next in-game encounter.

</details>

<details>

<summary>2021-08-24 08:38:58 - Lossy Medical Image Compression using Residual Learning-based Dual Autoencoder Model</summary>

- *Dipti Mishra, Satish Kumar Singh, Rajat Kumar Singh*

- `2108.10579v1` - [abs](http://arxiv.org/abs/2108.10579v1) - [pdf](http://arxiv.org/pdf/2108.10579v1)

> In this work, we propose a two-stage autoencoder based compressor-decompressor framework for compressing malaria RBC cell image patches. We know that the medical images used for disease diagnosis are around multiple gigabytes size, which is quite huge. The proposed residual-based dual autoencoder network is trained to extract the unique features which are then used to reconstruct the original image through the decompressor module. The two latent space representations (first for the original image and second for the residual image) are used to rebuild the final original image. Color-SSIM has been exclusively used to check the quality of the chrominance part of the cell images after decompression. The empirical results indicate that the proposed work outperformed other neural network related compression technique for medical images by approximately 35%, 10% and 5% in PSNR, Color SSIM and MS-SSIM respectively. The algorithm exhibits a significant improvement in bit savings of 76%, 78%, 75% & 74% over JPEG-LS, JP2K-LM, CALIC and recent neural network approach respectively, making it a good compression-decompression technique.

</details>

<details>

<summary>2021-08-24 11:11:06 - MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks</summary>

- *Alexandre Rame, Remy Sun, Matthieu Cord*

- `2103.06132v3` - [abs](http://arxiv.org/abs/2103.06132v3) - [pdf](http://arxiv.org/pdf/2103.06132v3)

> Recent strategies achieved ensembling "for free" by fitting concurrently diverse subnetworks inside a single base network. The main idea during training is that each subnetwork learns to classify only one of the multiple inputs simultaneously provided. However, the question of how to best mix these multiple inputs has not been studied so far. In this paper, we introduce MixMo, a new generalized framework for learning multi-input multi-output deep subnetworks. Our key motivation is to replace the suboptimal summing operation hidden in previous approaches by a more appropriate mixing mechanism. For that purpose, we draw inspiration from successful mixed sample data augmentations. We show that binary mixing in features - particularly with rectangular patches from CutMix - enhances results by making subnetworks stronger and more diverse. We improve state of the art for image classification on CIFAR-100 and Tiny ImageNet datasets. Our easy to implement models notably outperform data augmented deep ensembles, without the inference and memory overheads. As we operate in features and simply better leverage the expressiveness of large networks, we open a new line of research complementary to previous works.

</details>

<details>

<summary>2021-08-24 13:18:57 - MCUa: Multi-level Context and Uncertainty aware Dynamic Deep Ensemble for Breast Cancer Histology Image Classification</summary>

- *Zakaria Senousy, Mohammed M. Abdelsamea, Mohamed Medhat Gaber, Moloud Abdar, U Rajendra Acharya, Abbas Khosravi, Saeid Nahavandi*

- `2108.10709v1` - [abs](http://arxiv.org/abs/2108.10709v1) - [pdf](http://arxiv.org/pdf/2108.10709v1)

> Breast histology image classification is a crucial step in the early diagnosis of breast cancer. In breast pathological diagnosis, Convolutional Neural Networks (CNNs) have demonstrated great success using digitized histology slides. However, tissue classification is still challenging due to the high visual variability of the large-sized digitized samples and the lack of contextual information. In this paper, we propose a novel CNN, called Multi-level Context and Uncertainty aware (MCUa) dynamic deep learning ensemble model.MCUamodel consists of several multi-level context-aware models to learn the spatial dependency between image patches in a layer-wise fashion. It exploits the high sensitivity to the multi-level contextual information using an uncertainty quantification component to accomplish a novel dynamic ensemble model.MCUamodelhas achieved a high accuracy of 98.11% on a breast cancer histology image dataset. Experimental results show the superior effectiveness of the proposed solution compared to the state-of-the-art histology classification models.

</details>

<details>

<summary>2021-08-24 17:53:19 - A QuadTree Image Representation for Computational Pathology</summary>

- *Rob Jewsbury, Abhir Bhalerao, Nasir Rajpoot*

- `2108.10873v1` - [abs](http://arxiv.org/abs/2108.10873v1) - [pdf](http://arxiv.org/pdf/2108.10873v1)

> The field of computational pathology presents many challenges for computer vision algorithms due to the sheer size of pathology images. Histopathology images are large and need to be split up into image tiles or patches so modern convolutional neural networks (CNNs) can process them. In this work, we present a method to generate an interpretable image representation of computational pathology images using quadtrees and a pipeline to use these representations for highly accurate downstream classification. To the best of our knowledge, this is the first attempt to use quadtrees for pathology image data. We show it is highly accurate, able to achieve as good results as the currently widely adopted tissue mask patch extraction methods all while using over 38% less data.

</details>

<details>

<summary>2021-08-25 23:20:52 - Deep Sensory Substitution: Noninvasively Enabling Biological Neural Networks to Receive Input from Artificial Neural Networks</summary>

- *Andrew Port, Chelhwon Kim, Mitesh Patel*

- `2005.13291v3` - [abs](http://arxiv.org/abs/2005.13291v3) - [pdf](http://arxiv.org/pdf/2005.13291v3)

> As is expressed in the adage "a picture is worth a thousand words", when using spoken language to communicate visual information, brevity can be a challenge. This work describes a novel technique for leveraging machine-learned feature embeddings to sonify visual (and other types of) information into a perceptual audio domain, allowing users to perceive this information using only their aural faculty. The system uses a pretrained image embedding network to extract visual features and embed them in a compact subset of Euclidean space -- this converts the images into feature vectors whose $L^2$ distances can be used as a meaningful measure of similarity. A generative adversarial network (GAN) is then used to find a distance preserving map from this metric space of feature vectors into the metric space defined by a target audio dataset equipped with either the Euclidean metric or a mel-frequency cepstrum-based psychoacoustic distance metric. We demonstrate this technique by sonifying images of faces into human speech-like audio. For both target audio metrics, the GAN successfully found a metric preserving mapping, and in human subject tests, users were able to accurately classify audio sonifications of faces.

</details>

<details>

<summary>2021-08-26 01:01:43 - ChessMix: Spatial Context Data Augmentation for Remote Sensing Semantic Segmentation</summary>

- *Matheus Barros Pereira, Jefersson Alex dos Santos*

- `2108.11535v1` - [abs](http://arxiv.org/abs/2108.11535v1) - [pdf](http://arxiv.org/pdf/2108.11535v1)

> Labeling semantic segmentation datasets is a costly and laborious process if compared with tasks like image classification and object detection. This is especially true for remote sensing applications that not only work with extremely high spatial resolution data but also commonly require the knowledge of experts of the area to perform the manual labeling. Data augmentation techniques help to improve deep learning models under the circumstance of few and imbalanced labeled samples. In this work, we propose a novel data augmentation method focused on exploring the spatial context of remote sensing semantic segmentation. This method, ChessMix, creates new synthetic images from the existing training set by mixing transformed mini-patches across the dataset in a chessboard-like grid. ChessMix prioritizes patches with more examples of the rarest classes to alleviate the imbalance problems. The results in three diverse well-known remote sensing datasets show that this is a promising approach that helps to improve the networks' performance, working especially well in datasets with few available data. The results also show that ChessMix is capable of improving the segmentation of objects with few labeled pixels when compared to the most common data augmentation methods widely used.

</details>

<details>

<summary>2021-08-27 01:00:45 - HyperGI: Automated Detection and Repair of Information Flow Leakage</summary>

- *Ibrahim Mesecan, Daniel Blackwell, David Clark, Myra B. Cohen, Justyna Petke*

- `2108.12075v1` - [abs](http://arxiv.org/abs/2108.12075v1) - [pdf](http://arxiv.org/pdf/2108.12075v1)

> Maintaining confidential information control in software is a persistent security problem where failure means secrets can be revealed via program behaviors. Information flow control techniques traditionally have been based on static or symbolic analyses -- limited in scalability and specialized to particular languages. When programs do leak secrets there are no approaches to automatically repair them unless the leak causes a functional test to fail. We present our vision for HyperGI, a genetic improvement framework tha detects, localizes and repairs information leakage. Key elements of HyperGI include (1) the use of two orthogonal test suites, (2) a dynamic leak detection approach which estimates and localizes potential leaks, and (3) a repair component that produces a candidate patch using genetic improvement. We demonstrate the successful use of HyperGI on several programs which have no failing functional tests. We manually examine the resulting patches and identify trade-offs and future directions for fully realizing our vision.

</details>

<details>

<summary>2021-08-27 03:44:25 - SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation</summary>

- *Xuning Shao, Weidong Zhang*

- `2103.16219v2` - [abs](http://arxiv.org/abs/2103.16219v2) - [pdf](http://arxiv.org/pdf/2103.16219v2)

> For unsupervised image-to-image translation, we propose a discriminator architecture which focuses on the statistical features instead of individual patches. The network is stabilized by distribution matching of key statistical features at multiple scales. Unlike the existing methods which impose more and more constraints on the generator, our method facilitates the shape deformation and enhances the fine details with a greatly simplified framework. We show that the proposed method outperforms the existing state-of-the-art models in various challenging applications including selfie-to-anime, male-to-female and glasses removal.

</details>

<details>

<summary>2021-08-27 05:47:28 - Automated Generation of Accurate \& Fluent Medical X-ray Reports</summary>

- *Hoang T. N. Nguyen, Dong Nie, Taivanbat Badamdorj, Yujie Liu, Yingying Zhu, Jason Truong, Li Cheng*

- `2108.12126v1` - [abs](http://arxiv.org/abs/2108.12126v1) - [pdf](http://arxiv.org/pdf/2108.12126v1)

> Our paper focuses on automating the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Unlike existing medical re-port generation efforts that tend to produce human-readable reports, we aim to generate medical reports that are both fluent and clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm containing three complementary modules: taking the chest X-ray images and clinical his-tory document of patients as inputs, our classification module produces an internal check-list of disease-related topics, referred to as enriched disease embedding; the embedding representation is then passed to our transformer-based generator, giving rise to the medical reports; meanwhile, our generator also pro-duces the weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics.Our approach achieved promising results on commonly-used metrics concerning language fluency and clinical accuracy. Moreover, noticeable performance gains are consistently ob-served when additional input information is available, such as the clinical document and extra scans of different views.

</details>

<details>

<summary>2021-08-27 21:20:19 - Karp's patching algorithm on dense digraphs</summary>

- *Alan Frieze*

- `2006.10804v3` - [abs](http://arxiv.org/abs/2006.10804v3) - [pdf](http://arxiv.org/pdf/2006.10804v3)

> We consider the following question. We are given a dense digraph $D$ with minimum in- and out-degree at least $\alpha n$, where $\alpha>1/2$ is a constant. The edges of $D$ are given edge costs $C(e),e\in E(D)$, where $C(e)$ is an independent copy of the uniform $[0,1]$ random variable $U$. Let $C(i,j),i,j\in[n]$ be the associated $n\times n$ cost matrix where $C(i,j)=\infty$ if $(i,j)\notin E(D)$. We show that w.h.p. the patching algorithm of Karp finds a tour for the asymmetric traveling salesperson problem that is asymptotically equal to that of the associated assignment problem. Karp's algorithm runs in polynomial time.

</details>

<details>

<summary>2021-08-30 03:59:50 - Improving Transferability of Adversarial Patches on Face Recognition with Generative Models</summary>

- *Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao, Xiaolu Zhang, Jun Zhou, Jun Zhu*

- `2106.15058v2` - [abs](http://arxiv.org/abs/2106.15058v2) - [pdf](http://arxiv.org/pdf/2106.15058v2)

> Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.

</details>

<details>

<summary>2021-08-30 06:12:58 - X2Teeth: 3D Teeth Reconstruction from a Single Panoramic Radiograph</summary>

- *Yuan Liang, Weinan Song, Jiawei Yang, Liang Qiu, Kun Wang, Lei He*

- `2108.13004v1` - [abs](http://arxiv.org/abs/2108.13004v1) - [pdf](http://arxiv.org/pdf/2108.13004v1)

> 3D teeth reconstruction from X-ray is important for dental diagnosis and many clinical operations. However, no existing work has explored the reconstruction of teeth for a whole cavity from a single panoramic radiograph. Different from single object reconstruction from photos, this task has the unique challenge of constructing multiple objects at high resolutions. To conquer this task, we develop a novel ConvNet X2Teeth that decomposes the task into teeth localization and single-shape estimation. We also introduce a patch-based training strategy, such that X2Teeth can be end-to-end trained for optimal performance. Extensive experiments show that our method can successfully estimate the 3D structure of the cavity and reflect the details for each tooth. Moreover, X2Teeth achieves a reconstruction IoU of 0.681, which significantly outperforms the encoder-decoder method by $1.71X and the retrieval-based method by $1.52X. Our method can also be promising for other multi-anatomy 3D reconstruction tasks.

</details>


## 2021-09

<details>

<summary>2021-09-02 07:50:28 - Enhancing Real-World Adversarial Patches through 3D Modeling of Complex Target Scenes</summary>

- *Yael Mathov, Lior Rokach, Yuval Elovici*

- `2102.05334v2` - [abs](http://arxiv.org/abs/2102.05334v2) - [pdf](http://arxiv.org/pdf/2102.05334v2)

> Adversarial examples have proven to be a concerning threat to deep learning models, particularly in the image domain. However, while many studies have examined adversarial examples in the real world, most of them relied on 2D photos of the attack scene. As a result, the attacks proposed may have limited effectiveness when implemented in realistic environments with 3D objects or varied conditions. There are few studies on adversarial learning that use 3D objects, and in many cases, other researchers are unable to replicate the real-world evaluation process. In this study, we present a framework that uses 3D modeling to craft adversarial patches for an existing real-world scene. Our approach uses a 3D digital approximation of the scene as a simulation of the real world. With the ability to add and manipulate any element in the digital scene, our framework enables the attacker to improve the adversarial patch's impact in real-world settings. We use the framework to create a patch for an everyday scene and evaluate its performance using a novel evaluation process that ensures that our results are reproducible in both the digital space and the real world. Our evaluation results show that the framework can generate adversarial patches that are robust to different settings in the real world.

</details>

<details>

<summary>2021-09-02 10:36:48 - Segmenter: Transformer for Semantic Segmentation</summary>

- *Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid*

- `2105.05633v3` - [abs](http://arxiv.org/abs/2105.05633v3) - [pdf](http://arxiv.org/pdf/2105.05633v3)

> Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.

</details>

<details>

<summary>2021-09-02 13:55:34 - CURE: Code-Aware Neural Machine Translation for Automatic Program Repair</summary>

- *Nan Jiang, Thibaud Lutellier, Lin Tan*

- `2103.00073v4` - [abs](http://arxiv.org/abs/2103.00073v4) - [pdf](http://arxiv.org/pdf/2103.00073v4)

> Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.   We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.   Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.

</details>

<details>

<summary>2021-09-03 21:50:47 - On the classical complexity of sampling from quantum interference of indistinguishable bosons</summary>

- *Valery Shchesnovich*

- `1904.02013v5` - [abs](http://arxiv.org/abs/1904.02013v5) - [pdf](http://arxiv.org/pdf/1904.02013v5)

> Experimental demonstration of the quantum advantage over classical simulations with Boson Sampling is currently under intensive investigation. There seems to be a scalability issue to the necessary number of bosons on the linear optical platforms and the experiments, such as the recent Boson Sampling with $20$ photons on $60$-port interferometer by H.~Wang~\textit{et al}, \textit{Phys. Rev. Lett.} \textbf{123,} 250503 (2019), are usually carried out on a small interferometer, much smaller than the size necessary for the no-collision regime. Before demonstration of quantum advantage, it is urgent to estimate exactly how the classical computations necessary for sampling from the output distribution of Boson Sampling are reduced when a smaller-size interferometer is used. The present work supplies such a result, valid with arbitrarily close to $1$ probability, which reduces in the no-collision regime to the previous estimate by P.~Clifford and R.~Clifford. One of the results with immediate application to current experiments with Boson Sampling is that classically sampling from the interference of $N$ single bosons on an $M$-port interferometer is at least as hard as that with $\mathcal{N}= \frac{N}{1+N/M}$ single bosons in the no-collision regime, i.e., on a much larger interferometer with at least $\mathcal{M}\gg N^2$ ports.

</details>

<details>

<summary>2021-09-06 12:12:06 - Learning-based decentralized offloading decision making in an adversarial environment</summary>

- *Byungjin Cho, Yu Xiao*

- `2104.12827v3` - [abs](http://arxiv.org/abs/2104.12827v3) - [pdf](http://arxiv.org/pdf/2104.12827v3)

> Vehicular fog computing (VFC) pushes the cloud computing capability to the distributed fog nodes at the edge of the Internet, enabling compute-intensive and latency-sensitive computing services for vehicles through task offloading. However, a heterogeneous mobility environment introduces uncertainties in terms of resource supply and demand, which are inevitable bottlenecks for the optimal offloading decision. Also, these uncertainties bring extra challenges to task offloading under the oblivious adversary attack and data privacy risks. In this article, we develop a new adversarial online learning algorithm with bandit feedback based on the adversarial multi-armed bandit theory, to enable scalable and low-complexity offloading decision making. Specifically, we focus on optimizing fog node selection with the aim of minimizing the offloading service costs in terms of delay and energy. The key is to implicitly tune the exploration bonus in the selection process and the assessment rules of the designed algorithm, taking into account volatile resource supply and demand. We theoretically prove that the input-size dependent selection rule allows to choose a suitable fog node without exploring the sub-optimal actions, and also an appropriate score patching rule allows to quickly adapt to evolving circumstances, which reduce variance and bias simultaneously, thereby achieving a better exploitation-exploration balance. Simulation results verify the effectiveness and robustness of the proposed algorithm.

</details>

<details>

<summary>2021-09-09 07:05:10 - Malware Sight-Seeing: Accelerating Reverse-Engineering via Point-of-Interest-Beacons</summary>

- *August See, Maximilian Gehring, Max Mühlhäuser, Mathias Fischer, Shankar Karuppayah*

- `2109.04065v1` - [abs](http://arxiv.org/abs/2109.04065v1) - [pdf](http://arxiv.org/pdf/2109.04065v1)

> New types of malware are emerging at concerning rates. However, analyzing malware via reverse engineering is still a time-consuming and mostly manual task. For this reason, it is necessary to develop techniques that automate parts of the reverse engineering process and that can evade the built-in countermeasures of modern malware. The main contribution of this paper is a novel method to automatically find so-called Points-of-Interest (POIs) in executed programs. POIs are instructions that interact with data that is known to an analyst. They can be used as beacons in the analysis of malware and can help to guide the analyst to the interesting parts of the malware. Furthermore, we propose a metric for POIs , the so-called confidence score that estimates how exclusively a POI will process data relevant to the malware. With the goal of automatically extract peers in P2P botnet malware, we demonstrate and evaluate our approach by applying it on four botnets (ZeroAccess, Sality, Nugache, and Kelihos). We looked into the identified POIs for known IPs and ports and, by using this information, leverage it to successfully monitor the botnets. Furthermore, using our scoring system, we show that we can extract peers for each botnet with high accuracy.

</details>

<details>

<summary>2021-09-09 12:55:06 - Shuffled Patch-Wise Supervision for Presentation Attack Detection</summary>

- *Alperen Kantarcı, Hasan Dertli, Hazım Kemal Ekenel*

- `2109.03484v2` - [abs](http://arxiv.org/abs/2109.03484v2) - [pdf](http://arxiv.org/pdf/2109.03484v2)

> Face anti-spoofing is essential to prevent false facial verification by using a photo, video, mask, or a different substitute for an authorized person's face. Most of the state-of-the-art presentation attack detection (PAD) systems suffer from overfitting, where they achieve near-perfect scores on a single dataset but fail on a different dataset with more realistic data. This problem drives researchers to develop models that perform well under real-world conditions. This is an especially challenging problem for frame-based presentation attack detection systems that use convolutional neural networks (CNN). To this end, we propose a new PAD approach, which combines pixel-wise binary supervision with patch-based CNN. We believe that training a CNN with face patches allows the model to distinguish spoofs without learning background or dataset-specific traces. We tested the proposed method both on the standard benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset. The proposed approach shows its superiority on challenging experimental setups. Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on inter-dataset real-world experiments.

</details>

<details>

<summary>2021-09-09 14:23:48 - Energy Attack: On Transferring Adversarial Examples</summary>

- *Ruoxi Shi, Borui Yang, Yangzhou Jiang, Chenglong Zhao, Bingbing Ni*

- `2109.04300v1` - [abs](http://arxiv.org/abs/2109.04300v1) - [pdf](http://arxiv.org/pdf/2109.04300v1)

> In this work we propose Energy Attack, a transfer-based black-box $L_\infty$-adversarial attack. The attack is parameter-free and does not require gradient approximation. In particular, we first obtain white-box adversarial perturbations of a surrogate model and divide these perturbations into small patches. Then we extract the unit component vectors and eigenvalues of these patches with principal component analysis (PCA). Base on the eigenvalues, we can model the energy distribution of adversarial perturbations. We then perform black-box attacks by sampling from the perturbation patches according to their energy distribution, and tiling the sampled patches to form a full-size adversarial perturbation. This can be done without the available access to victim models. Extensive experiments well demonstrate that the proposed Energy Attack achieves state-of-the-art performance in black-box attacks on various models and several datasets. Moreover, the extracted distribution is able to transfer among different model architectures and different datasets, and is therefore intrinsic to vision architectures.

</details>

<details>

<summary>2021-09-10 07:11:19 - MEAL: Manifold Embedding-based Active Learning</summary>

- *Deepthi Sreenivasaiah, Johannes Otterbach, Thomas Wollmann*

- `2106.11858v3` - [abs](http://arxiv.org/abs/2106.11858v3) - [pdf](http://arxiv.org/pdf/2106.11858v3)

> Image segmentation is a common and challenging task in autonomous driving. Availability of sufficient pixel-level annotations for the training data is a hurdle. Active learning helps learning from small amounts of data by suggesting the most promising samples for labeling. In this work, we propose a new pool-based method for active learning, which proposes promising patches extracted from full image, in each acquisition step. The problem is framed in an exploration-exploitation framework by combining an embedding based on Uniform Manifold Approximation to model representativeness with entropy as uncertainty measure to model informativeness. We applied our proposed method to the autonomous driving datasets CamVid and Cityscapes and performed a quantitative comparison with state-of-the-art baselines. We find that our active learning method achieves better performance compared to previous methods.

</details>

<details>

<summary>2021-09-12 23:45:49 - Fixing Vulnerabilities Potentially Hinders Maintainability</summary>

- *Sofia Reis, Rui Abreu, Luis Cruz*

- `2106.03271v2` - [abs](http://arxiv.org/abs/2106.03271v2) - [pdf](http://arxiv.org/pdf/2106.03271v2)

> Security is a requirement of utmost importance to produce high-quality software. However, there is still a considerable amount of vulnerabilities being discovered and fixed almost weekly. We hypothesize that developers affect the maintainability of their codebases when patching vulnerabilities. This paper evaluates the impact of patches to improve security on the maintainability of open-source software. Maintainability is measured based on the Better Code Hub's model of 10 guidelines on a dataset, including 1300 security-related commits. Results show evidence of a trade-off between security and maintainability for 41.90% of the cases, i.e., developers may hinder software maintainability. Our analysis shows that 38.29% of patches increased software complexity and 37.87% of patches increased the percentage of LOCs per unit. The implications of our study are that changes to codebases while patching vulnerabilities need to be performed with extra care; tools for patch risk assessment should be integrated into the CI/CD pipeline; computer science curricula needs to be updated; and, more secure programming languages are necessary.

</details>

<details>

<summary>2021-09-13 02:27:41 - Automated Evolution of Feature Logging Statement Levels Using Git Histories and Degree of Interest</summary>

- *Yiming Tang, Allan Spektor, Raffi Khatchadourian, Mehdi Bagherzadeh*

- `2104.07736v4` - [abs](http://arxiv.org/abs/2104.07736v4) - [pdf](http://arxiv.org/pdf/2104.07736v4)

> Logging -- used for system events and security breaches to describe more informational yet essential aspects of software features -- is pervasive. Given the high transactionality of today's software, logging effectiveness can be reduced by information overload. Log levels help alleviate this problem by correlating a priority to logs that can be later filtered. As software evolves, however, levels of logs documenting surrounding feature implementations may also require modification as features once deemed important may have decreased in urgency and vice-versa. We present an automated approach that assists developers in evolving levels of such (feature) logs. The approach, based on mining Git histories and manipulating a degree of interest (DOI) model, transforms source code to revitalize feature log levels based on the "interestingness" of the surrounding code. Built upon JGit and Mylyn, the approach is implemented as an Eclipse IDE plug-in and evaluated on 18 Java projects with $\sim$3 million lines of code and $\sim$4K log statements. Our tool successfully analyzes 99.22% of logging statements, increases log level distributions by $\sim$20%, and increases the focus of logs in bug fix contexts $\sim$83% of the time. Moreover, pull (patch) requests were integrated into large and popular open-source projects. The results indicate that the approach is promising in assisting developers in evolving feature log levels.

</details>

<details>

<summary>2021-09-13 17:06:21 - Malware MultiVerse: From Automatic Logic Bomb Identification to Automatic Patching and Tracing</summary>

- *Marcus Botacin, André Grégio*

- `2109.06127v1` - [abs](http://arxiv.org/abs/2109.06127v1) - [pdf](http://arxiv.org/pdf/2109.06127v1)

> Malware and other suspicious software often hide behaviors and components behind logic bombs and context-sensitive execution paths. Uncovering these is essential to react against modern threats, but current solutions are not ready to detect these paths in a completely automated manner. To bridge this gap, we propose the Malware Multiverse (MalVerse), a solution able to inspect multiple execution paths via symbolic execution aiming to discover function inputs and returns that trigger malicious behaviors. MalVerse automatically patches the context-sensitive functions with the identified symbolic values to allow the software execution in a traditional sandbox. We implemented MalVerse on top of angr and evaluated it with a set of Linux and Windows evasive samples. We found that MalVerse was able to generate automatic patches for the most common evasion techniques (e.g., ptrace checks).

</details>

<details>

<summary>2021-09-13 23:38:42 - Sensor Adversarial Traits: Analyzing Robustness of 3D Object Detection Sensor Fusion Models</summary>

- *Won Park, Nan Liu, Qi Alfred Chen, Z. Morley Mao*

- `2109.06363v1` - [abs](http://arxiv.org/abs/2109.06363v1) - [pdf](http://arxiv.org/pdf/2109.06363v1)

> A critical aspect of autonomous vehicles (AVs) is the object detection stage, which is increasingly being performed with sensor fusion models: multimodal 3D object detection models which utilize both 2D RGB image data and 3D data from a LIDAR sensor as inputs. In this work, we perform the first study to analyze the robustness of a high-performance, open source sensor fusion model architecture towards adversarial attacks and challenge the popular belief that the use of additional sensors automatically mitigate the risk of adversarial attacks. We find that despite the use of a LIDAR sensor, the model is vulnerable to our purposefully crafted image-based adversarial attacks including disappearance, universal patch, and spoofing. After identifying the underlying reason, we explore some potential defenses and provide some recommendations for improved sensor fusion models.

</details>

<details>

<summary>2021-09-14 13:56:40 - Local Mending</summary>

- *Alkida Balliu, Juho Hirvonen, Darya Melnyk, Dennis Olivetti, Joel Rybicki, Jukka Suomela*

- `2102.08703v4` - [abs](http://arxiv.org/abs/2102.08703v4) - [pdf](http://arxiv.org/pdf/2102.08703v4)

> In this work we introduce the graph-theoretic notion of mendability: for each locally checkable graph problem we can define its mending radius, which captures the idea of how far one needs to modify a partial solution in order to "patch a hole."   We explore how mendability is connected to the existence of efficient algorithms, especially in distributed, parallel, and fault-tolerant settings. It is easy to see that $O(1)$-mendable problems are also solvable in $O(\log^* n)$ rounds in the LOCAL model of distributed computing. One of the surprises is that in paths and cycles, a converse also holds in the following sense: if a problem $\Pi$ can be solved in $O(\log^* n)$, there is always a restriction $\Pi' \subseteq \Pi$ that is still efficiently solvable but that is also $O(1)$-mendable.   We also explore the structure of the landscape of mendability. For example, we show that in trees, the mending radius of any locally checkable problem is $O(1)$, $\Theta(\log n)$, or $\Theta(n)$, while in general graphs the structure is much more diverse.

</details>

<details>

<summary>2021-09-15 14:25:28 - DeFungi: Direct Mycological Examination of Microscopic Fungi Images</summary>

- *Camilo Javier Pineda Sopo, Farshid Hajati, Soheila Gheisari*

- `2109.07322v1` - [abs](http://arxiv.org/abs/2109.07322v1) - [pdf](http://arxiv.org/pdf/2109.07322v1)

> Traditionally, diagnosis and treatment of fungal infections in humans depend heavily on face-to-face consultations or examinations made by specialized laboratory scientists known as mycologists. In many cases, such as the recent mucormycosis spread in the COVID-19 pandemic, an initial treatment can be safely suggested to the patient during the earliest stage of the mycological diagnostic process by performing a direct examination of biopsies or samples through a microscope. Computer-aided diagnosis systems using deep learning models have been trained and used for the late mycological diagnostic stages. However, there are no reference literature works made for the early stages. A mycological laboratory in Colombia donated the images used for the development of this research work. They were manually labelled into five classes and curated with a subject matter expert assistance. The images were later cropped and patched with automated code routines to produce the final dataset. This paper presents experimental results classifying five fungi types using two different deep learning approaches and three different convolutional neural network models, VGG16, Inception V3, and ResNet50. The first approach benchmarks the classification performance for the models trained from scratch, while the second approach benchmarks the classification performance using pre-trained models based on the ImageNet dataset. Using k-fold cross-validation testing on the 5-class dataset, the best performing model trained from scratch was Inception V3, reporting 73.2% accuracy. Also, the best performing model using transfer learning was VGG16 reporting 85.04%. The statistics provided by the two approaches create an initial point of reference to encourage future research works to improve classification performance. Furthermore, the dataset built is published in Kaggle and GitHub to foster future research.

</details>

<details>

<summary>2021-09-16 09:34:14 - Design Space Exploration of SABER in 65nm ASIC</summary>

- *Malik Imran, Felipe Almeida, Jaan Raik, Andrea Basso, Sujoy Sinha Roy, Samuel Pagliarini*

- `2109.07824v1` - [abs](http://arxiv.org/abs/2109.07824v1) - [pdf](http://arxiv.org/pdf/2109.07824v1)

> This paper presents a design space exploration for SABER, one of the finalists in NIST's quantum-resistant public-key cryptographic standardization effort. Our design space exploration targets a 65nm ASIC platform and has resulted in the evaluation of 6 different architectures. Our exploration is initiated by setting a baseline architecture which is ported from FPGA. In order to improve the clock frequency (the primary goal in our exploration), we have employed several optimizations: (i) use of compiled memories in a 'smart synthesis' fashion, (ii) pipelining, and (iii) logic sharing between SABER building blocks. The most optimized architecture utilizes four register files, achieves a remarkable clock frequency of 1GHz while only requiring an area of 0.314mm2. Moreover, physical synthesis is carried out for this architecture and a tapeout-ready layout is presented. The estimated dynamic power consumption of the high-frequency architecture is approximately 184mW for key generation and 187mW for encapsulation or decapsulation operations. These results strongly suggest that our optimized accelerator architecture is well suited for high-speed cryptographic applications.

</details>

<details>

<summary>2021-09-16 20:18:10 - Long Short-term Cognitive Networks</summary>

- *Gonzalo Nápoles, Isel Grau, Agnieszka Jastrzebska, Yamisleydi Salgueiro*

- `2106.16233v2` - [abs](http://arxiv.org/abs/2106.16233v2) - [pdf](http://arxiv.org/pdf/2106.16233v2)

> In this paper, we present a recurrent neural system named Long Short-term Cognitive Networks (LSTCNs) as a generalization of the Short-term Cognitive Network (STCN) model. Such a generalization is motivated by the difficulty of forecasting very long time series efficiently. The LSTCN model can be defined as a collection of STCN blocks, each processing a specific time patch of the (multivariate) time series being modeled. In this neural ensemble, each block passes information to the subsequent one in the form of weight matrices representing the prior knowledge. As a second contribution, we propose a deterministic learning algorithm to compute the learnable weights while preserving the prior knowledge resulting from previous learning processes. As a third contribution, we introduce a feature influence score as a proxy to explain the forecasting process in multivariate time series. The simulations using three case studies show that our neural system reports small forecasting errors while being significantly faster than state-of-the-art recurrent models.

</details>

<details>

<summary>2021-09-17 02:27:02 - Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests</summary>

- *Siqi Wang, Guang Yu, Zhiping Cai, Xinwang Liu, En Zhu, Jianping Yin*

- `2108.02356v2` - [abs](http://arxiv.org/abs/2108.02356v2) - [pdf](http://arxiv.org/pdf/2108.02356v2)

> Although deep neural networks (DNNs) enable great progress in video abnormal event detection (VAD), existing solutions typically suffer from two issues: (1) The localization of video events cannot be both precious and comprehensive. (2) The semantics and temporal context are under-explored. To tackle those issues, we are motivated by the prevalent cloze test in education and propose a novel approach named Visual Cloze Completion (VCC), which conducts VAD by learning to complete "visual cloze tests" (VCTs). Specifically, VCC first localizes each video event and encloses it into a spatio-temporal cube (STC). To achieve both precise and comprehensive localization, appearance and motion are used as complementary cues to mark the object region associated with each event. For each marked region, a normalized patch sequence is extracted from current and adjacent frames and stacked into a STC. With each patch and the patch sequence of a STC compared to a visual "word" and "sentence" respectively, we deliberately erase a certain "word" (patch) to yield a VCT. Then, the VCT is completed by training DNNs to infer the erased patch and its optical flow via video semantics. Meanwhile, VCC fully exploits temporal context by alternatively erasing each patch in temporal context and creating multiple VCTs. Furthermore, we propose localization-level, event-level, model-level and decision-level solutions to enhance VCC, which can further exploit VCC's potential and produce significant performance improvement gain. Extensive experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.

</details>

<details>

<summary>2021-09-17 10:48:21 - Using hardware performance counters to speed up autotuning convergence on GPUs</summary>

- *Jiří Filipovič, Jana Hozzová, Amin Nezarat, Jaroslav Oľha, Filip Petrovič*

- `2102.05297v2` - [abs](http://arxiv.org/abs/2102.05297v2) - [pdf](http://arxiv.org/pdf/2102.05297v2)

> Nowadays, GPU accelerators are commonly used to speed up general-purpose computing tasks on a variety of hardware. However, due to the diversity of GPU architectures and processed data, optimization of codes for a particular type of hardware and specific data characteristics can be extremely challenging. The autotuning of performance-relevant source-code parameters allows for automatic optimization of applications and keeps their performance portable. Although the autotuning process typically results in code speed-up, searching the tuning space can bring unacceptable overhead if (i) the tuning space is vast and full of poorly-performing implementations, or (ii) the autotuning process has to be repeated frequently because of changes in processed data or migration to different hardware.   In this paper, we introduce a novel method for searching tuning spaces. The method takes advantage of collecting hardware performance counters (also known as profiling counters) during empirical tuning. Those counters are used to navigate the searching process towards faster implementations. The method requires the tuning space to be sampled on any GPU. It builds a problem-specific model, which can be used during autotuning on various, even previously unseen inputs or GPUs. Using a set of five benchmarks, we experimentally demonstrate that our method can speed up autotuning when an application needs to be ported to different hardware or when it needs to process data with different characteristics. We also compared our method to state of the art and show that our method is superior in terms of the number of searching steps and typically outperforms other searches in terms of convergence time.

</details>

<details>

<summary>2021-09-19 08:27:57 - Towards robustness under occlusion for face recognition</summary>

- *Tomas M. Borges, Teofilo E. de Campos, Ricardo de Queiroz*

- `2109.09083v1` - [abs](http://arxiv.org/abs/2109.09083v1) - [pdf](http://arxiv.org/pdf/2109.09083v1)

> In this paper, we evaluate the effects of occlusions in the performance of a face recognition pipeline that uses a ResNet backbone. The classifier was trained on a subset of the CelebA-HQ dataset containing 5,478 images from 307 classes, to achieve top-1 error rate of 17.91%. We designed 8 different occlusion masks which were applied to the input images. This caused a significant drop in the classifier performance: its error rate for each mask became at least two times worse than before. In order to increase robustness under occlusions, we followed two approaches. The first is image inpainting using the pre-trained pluralistic image completion network. The second is Cutmix, a regularization strategy consisting of mixing training images and their labels using rectangular patches, making the classifier more robust against input corruptions. Both strategies revealed effective and interesting results were observed. In particular, the Cutmix approach makes the network more robust without requiring additional steps at the application time, though its training time is considerably longer. Our datasets containing the different occlusion masks as well as their inpainted counterparts are made publicly available to promote research on the field.

</details>

<details>

<summary>2021-09-20 13:20:51 - IoT Vulnerability Data Crawling and Analysis</summary>

- *Stavros Shiaeles, Nicholas Kolokotronis, Emanuele Bellini*

- `2109.09526v1` - [abs](http://arxiv.org/abs/2109.09526v1) - [pdf](http://arxiv.org/pdf/2109.09526v1)

> Internet of Things (IoT) is a whole new ecosystem comprised of heterogeneous connected devices -i.e. computers, laptops, smart-phones and tablets as well as embedded devices and sensors-that communicate to deliver capabilities making our living, cities, transport, energy, and many other areas more intelligent. The main concerns raised from the IoT ecosystem are the devices poor support for patching/updating and the poor on-board computational power. A number of issues stem from this: inherent vulnerabilities and the inability to detect and defend against external attacks. Also, due to the nature of their operation, the devices tend to be rather open to communication, which makes attacks easy to spread once reaching a network. The aim of this research is to investigate if it is possible to extract useful results regarding attacks' trends and be able to predict them, before it is too late, by crawling Deep/Dark and Surface web. The results of this work show that is possible to find the trend and be able to act proactively in order to protect the IoT ecosystem.

</details>

<details>

<summary>2021-09-20 17:56:22 - SoK: Machine Learning Governance</summary>

- *Varun Chandrasekaran, Hengrui Jia, Anvith Thudi, Adelin Travers, Mohammad Yaghini, Nicolas Papernot*

- `2109.10870v1` - [abs](http://arxiv.org/abs/2109.10870v1) - [pdf](http://arxiv.org/pdf/2109.10870v1)

> The application of machine learning (ML) in computer systems introduces not only many benefits but also risks to society. In this paper, we develop the concept of ML governance to balance such benefits and risks, with the aim of achieving responsible applications of ML. Our approach first systematizes research towards ascertaining ownership of data and models, thus fostering a notion of identity specific to ML systems. Building on this foundation, we use identities to hold principals accountable for failures of ML systems through both attribution and auditing. To increase trust in ML systems, we then survey techniques for developing assurance, i.e., confidence that the system meets its security requirements and does not exhibit certain known failures. This leads us to highlight the need for techniques that allow a model owner to manage the life cycle of their system, e.g., to patch or retire their ML system. Put altogether, our systematization of knowledge standardizes the interactions between principals involved in the deployment of ML throughout its life cycle. We highlight opportunities for future work, e.g., to formalize the resulting game between ML principals.

</details>

<details>

<summary>2021-09-21 16:24:02 - MITOSIS: Practically Scaling Permissioned Blockchains</summary>

- *Giorgia Azzurra Marson, Sebastien Andreina, Lorenzo Alluminio, Konstantin Munichev, Ghassan Karame*

- `2109.10302v1` - [abs](http://arxiv.org/abs/2109.10302v1) - [pdf](http://arxiv.org/pdf/2109.10302v1)

> Scalability remains one of the biggest challenges to the adoption of permissioned blockchain technologies for large-scale deployments. Permissioned blockchains typically exhibit low latencies, compared to permissionless deployments -- however at the cost of poor scalability. Various solutions were proposed to capture "the best of both worlds", targeting low latency and high scalability simultaneously, the most prominent technique being blockchain sharding. However, most existing sharding proposals exploit features of the permissionless model and are therefore restricted to cryptocurrency applications.   We present MITOSIS, a novel approach to practically improve scalability of permissioned blockchains. Our system allows the dynamic creation of blockchains, as more participants join the system, to meet practical scalability requirements. Crucially, it enables the division of an existing blockchain (and its participants) into two -- reminiscent of mitosis, the biological process of cell division. MITOSIS inherits the low latency of permissioned blockchains while preserving high throughput via parallel processing. Newly created chains in our system are fully autonomous, can choose their own consensus protocol, and yet they can interact with each other to share information and assets -- meeting high levels of interoperability. We analyse the security of MITOSIS and evaluate experimentally the performance of our solution when instantiated over Hyperledger Fabric. Our results show that MITOSIS can be ported with little modifications and manageable overhead to existing permissioned blockchains, such as Hyperledger Fabric.

</details>

<details>

<summary>2021-09-22 01:37:30 - AI in Osteoporosis</summary>

- *Sokratis Makrogiannis, Keni Zheng*

- `2109.10478v1` - [abs](http://arxiv.org/abs/2109.10478v1) - [pdf](http://arxiv.org/pdf/2109.10478v1)

> In this chapter we explore and evaluate methods for trabecular bone characterization and osteoporosis diagnosis with increased interest in sparse approximations. We first describe texture representation and classification techniques, patch-based methods such as Bag of Keypoints, and more recent deep neural networks. Then we introduce the concept of sparse representations for pattern recognition and we detail integrative sparse analysis methods and classifier decision fusion methods. We report cross-validation results on osteoporosis datasets of bone radiographs and compare the results produced by the different categories of methods. We conclude that advances in the AI and machine learning fields have enabled the development of methods that can be used as diagnostic tools in clinical settings.

</details>

<details>

<summary>2021-09-22 11:32:00 - VIA: Analyzing Device Interfaces of Protected Virtual Machines</summary>

- *Felicitas Hetzelt, Martin Radev, Robert Buhren, Mathias Morbitzer, Jean-Pierre Seifert*

- `2109.10660v1` - [abs](http://arxiv.org/abs/2109.10660v1) - [pdf](http://arxiv.org/pdf/2109.10660v1)

> Both AMD and Intel have presented technologies for confidential computing in cloud environments. The proposed solutions - AMD SEV (-ES, -SNP) and Intel TDX - protect Virtual Machines (VMs) against attacks from higher privileged layers through memory encryption and integrity protection. This model of computation draws a new trust boundary between virtual devices and the VM, which in so far lacks thorough examination. In this paper, we therefore present an analysis of the virtual device interface and discuss several attack vectors against a protected VM. Further, we develop and evaluate VIA, an automated analysis tool to detect cases of improper sanitization of input recieved via the virtual device interface. VIA improves upon existing approaches for the automated analysis of device interfaces in the following aspects: (i) support for virtualization relevant buses, (ii) efficient Direct Memory Access (DMA) support and (iii) performance. VIA builds upon the Linux Kernel Library and clang's libfuzzer to fuzz the communication between the driver and the device via MMIO, PIO, and DMA. An evaluation of VIA shows that it performs 570 executions per second on average and improves performance compared to existing approaches by an average factor of 2706. Using VIA, we analyzed 22 drivers in Linux 5.10.0-rc6, thereby uncovering 50 bugs and initiating multiple patches to the virtual device driver interface of Linux. To prove our findings criticality under the threat model of AMD SEV and Intel TDX, we showcase three exemplary attacks based on the bugs found. The attacks enable a malicious hypervisor to corrupt the memory and gain code execution in protected VMs with SEV-ES and are theoretically applicable to SEV-SNP and TDX.

</details>

<details>

<summary>2021-09-23 06:29:42 - Puzzle-CAM: Improved localization via matching partial and full features</summary>

- *Sanghyun Jo, In-Jae Yu*

- `2101.11253v4` - [abs](http://arxiv.org/abs/2101.11253v4) - [pdf](http://arxiv.org/pdf/2101.11253v4)

> Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the gap for semantic segmentation performance from pixel-level supervision to image-level supervision. Most advanced approaches are based on class activation maps (CAMs) to generate pseudo-labels to train the segmentation network. The main limitation of WSSS is that the process of generating pseudo-labels from CAMs that use an image classifier is mainly focused on the most discriminative parts of the objects. To address this issue, we propose Puzzle-CAM, a process that minimizes differences between the features from separate patches and the whole image. Our method consists of a puzzle module and two regularization terms to discover the most integrated region in an object. Puzzle-CAM can activate the overall region of an object using image-level supervision without requiring extra parameters. % In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 dataset. Code associated with our experiments is available at https://github.com/OFRIN/PuzzleCAM.

</details>

<details>

<summary>2021-09-24 10:47:55 - Learning-based Noise Component Map Estimation for Image Denoising</summary>

- *Sheyda Ghanbaralizadeh Bahnemiri, Mykola Ponomarenko, Karen Egiazarian*

- `2109.11877v1` - [abs](http://arxiv.org/abs/2109.11877v1) - [pdf](http://arxiv.org/pdf/2109.11877v1)

> A problem of image denoising when images are corrupted by a non-stationary noise is considered in this paper. Since in practice no a priori information on noise is available, noise statistics should be pre-estimated for image denoising. In this paper, deep convolutional neural network (CNN) based method for estimation of a map of local, patch-wise, standard deviations of noise (so-called sigma-map) is proposed. It achieves the state-of-the-art performance in accuracy of estimation of sigma-map for the case of non-stationary noise, as well as estimation of noise variance for the case of additive white Gaussian noise. Extensive experiments on image denoising using estimated sigma-maps demonstrate that our method outperforms recent CNN-based blind image denoising methods by up to 6 dB in PSNR, as well as other state-of-the-art methods based on sigma-map estimation by up to 0.5 dB, providing same time better usage flexibility. Comparison with the ideal case, when denoising is applied using ground-truth sigma-map, shows that a difference of corresponding PSNR values for most of noise levels is within 0.1-0.2 dB and does not exceeds 0.6 dB.

</details>

<details>

<summary>2021-09-25 01:17:17 - Long-Range Feature Propagating for Natural Image Matting</summary>

- *Qinglin Liu, Haozhe Xie, Shengping Zhang, Bineng Zhong, Rongrong Ji*

- `2109.12252v1` - [abs](http://arxiv.org/abs/2109.12252v1) - [pdf](http://arxiv.org/pdf/2109.12252v1)

> Natural image matting estimates the alpha values of unknown regions in the trimap. Recently, deep learning based methods propagate the alpha values from the known regions to unknown regions according to the similarity between them. However, we find that more than 50\% pixels in the unknown regions cannot be correlated to pixels in known regions due to the limitation of small effective reception fields of common convolutional neural networks, which leads to inaccurate estimation when the pixels in the unknown regions cannot be inferred only with pixels in the reception fields. To solve this problem, we propose Long-Range Feature Propagating Network (LFPNet), which learns the long-range context features outside the reception fields for alpha matte estimation. Specifically, we first design the propagating module which extracts the context features from the downsampled image. Then, we present Center-Surround Pyramid Pooling (CSPP) that explicitly propagates the context features from the surrounding context image patch to the inner center image patch. Finally, we use the matting module which takes the image, trimap and context features to estimate the alpha matte. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on the AlphaMatting and Adobe Image Matting datasets.

</details>

<details>

<summary>2021-09-26 08:15:44 - GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion for Tunnel Lining</summary>

- *Bin Liu, Yuxiao Ren, Hanchi Liu, Hui Xu, Zhengfang Wang, Anthony G. Cohn, Peng Jiang*

- `1912.05759v3` - [abs](http://arxiv.org/abs/1912.05759v3) - [pdf](http://arxiv.org/pdf/1912.05759v3)

> A DNN architecture referred to as GPRInvNet was proposed to tackle the challenges of mapping the ground-penetrating radar (GPR) B-Scan data to complex permittivity maps of subsurface structures. The GPRInvNet consisted of a trace-to-trace encoder and a decoder. It was specially designed to take into account the characteristics of GPR inversion when faced with complex GPR B-Scan data, as well as addressing the spatial alignment issues between time-series B-Scan data and spatial permittivity maps. It displayed the ability to fuse features from several adjacent traces on the B-Scan data to enhance each trace, and then further condense the features of each trace separately. As a result, the sensitive zones on the permittivity maps spatially aligned to the enhanced trace could be reconstructed accurately. The GPRInvNet has been utilized to reconstruct the permittivity map of tunnel linings. A diverse range of dielectric models of tunnel linings containing complex defects has been reconstructed using GPRInvNet. The results have demonstrated that the GPRInvNet is capable of effectively reconstructing complex tunnel lining defects with clear boundaries. Comparative results with existing baseline methods also demonstrated the superiority of the GPRInvNet. For the purpose of generalizing the GPRInvNet to real GPR data, some background noise patches recorded from practical model testing were integrated into the synthetic GPR data to retrain the GPRInvNet. The model testing has been conducted for validation, and experimental results revealed that the GPRInvNet had also achieved satisfactory results with regard to the real data.

</details>

<details>

<summary>2021-09-27 11:03:00 - A Survey on Graph-Based Deep Learning for Computational Histopathology</summary>

- *David Ahmedt-Aristizabal, Mohammad Ali Armin, Simon Denman, Clinton Fookes, Lars Petersson*

- `2107.00272v2` - [abs](http://arxiv.org/abs/2107.00272v2) - [pdf](http://arxiv.org/pdf/2107.00272v2)

> With the remarkable success of representation learning for prediction problems, we have witnessed a rapid expansion of the use of machine learning and deep learning for the analysis of digital pathology and biopsy image patches. However, learning over patch-wise features using convolutional neural networks limits the ability of the model to capture global contextual information and comprehensively model tissue composition. The phenotypical and topological distribution of constituent histological entities play a critical role in tissue diagnosis. As such, graph data representations and deep learning have attracted significant attention for encoding tissue representations, and capturing intra- and inter- entity level interactions. In this review, we provide a conceptual grounding for graph analytics in digital pathology, including entity-graph construction and graph architectures, and present their current success for tumor localization and classification, tumor invasion and staging, image retrieval, and survival prediction. We provide an overview of these methods in a systematic manner organized by the graph representation of the input image, scale, and organ on which they operate. We also outline the limitations of existing techniques, and suggest potential future research directions in this domain.

</details>

<details>

<summary>2021-09-28 12:53:56 - Few-shot Neural Human Performance Rendering from Sparse RGBD Videos</summary>

- *Anqi Pang, Xin Chen, Haimin Luo, Minye Wu, Jingyi Yu, Lan Xu*

- `2107.06505v2` - [abs](http://arxiv.org/abs/2107.06505v2) - [pdf](http://arxiv.org/pdf/2107.06505v2)

> Recent neural rendering approaches for human activities achieve remarkable view synthesis results, but still rely on dense input views or dense training with all the capture frames, leading to deployment difficulty and inefficient training overload. However, existing advances will be ill-posed if the input is both spatially and temporally sparse. To fill this gap, in this paper we propose a few-shot neural human rendering approach (FNHR) from only sparse RGBD inputs, which exploits the temporal and spatial redundancy to generate photo-realistic free-view output of human activities. Our FNHR is trained only on the key-frames which expand the motion manifold in the input sequences. We introduce a two-branch neural blending to combine the neural point render and classical graphics texturing pipeline, which integrates reliable observations over sparse key-frames. Furthermore, we adopt a patch-based adversarial training process to make use of the local redundancy and avoids over-fitting to the key-frames, which generates fine-detailed rendering results. Extensive experiments demonstrate the effectiveness of our approach to generate high-quality free view-point results for challenging human performances under the sparse setting.

</details>

<details>

<summary>2021-09-29 19:08:32 - Segmentation of Roads in Satellite Images using specially modified U-Net CNNs</summary>

- *Jonas Bokstaller, Yihang She, Zhehan Fu, Tommaso Macrì*

- `2109.14671v1` - [abs](http://arxiv.org/abs/2109.14671v1) - [pdf](http://arxiv.org/pdf/2109.14671v1)

> The image classification problem has been deeply investigated by the research community, with computer vision algorithms and with the help of Neural Networks. The aim of this paper is to build an image classifier for satellite images of urban scenes that identifies the portions of the images in which a road is located, separating these portions from the rest. Unlike conventional computer vision algorithms, convolutional neural networks (CNNs) provide accurate and reliable results on this task. Our novel approach uses a sliding window to extract patches out of the whole image, data augmentation for generating more training/testing data and lastly a series of specially modified U-Net CNNs. This proposed technique outperforms all other baselines tested in terms of mean F-score metric.

</details>

<details>

<summary>2021-09-30 14:47:29 - You Cannot Easily Catch Me: A Low-Detectable Adversarial Patch for Object Detectors</summary>

- *Zijian Zhu, Hang Su, Chang Liu, Wenzhao Xiang, Shibao Zheng*

- `2109.15177v1` - [abs](http://arxiv.org/abs/2109.15177v1) - [pdf](http://arxiv.org/pdf/2109.15177v1)

> Blind spots or outright deceit can bedevil and deceive machine learning models. Unidentified objects such as digital "stickers," also known as adversarial patches, can fool facial recognition systems, surveillance systems and self-driving cars. Fortunately, most existing adversarial patches can be outwitted, disabled and rejected by a simple classification network called an adversarial patch detector, which distinguishes adversarial patches from original images. An object detector classifies and predicts the types of objects within an image, such as by distinguishing a motorcyclist from the motorcycle, while also localizing each object's placement within the image by "drawing" so-called bounding boxes around each object, once again separating the motorcyclist from the motorcycle. To train detectors even better, however, we need to keep subjecting them to confusing or deceitful adversarial patches as we probe for the models' blind spots. For such probes, we came up with a novel approach, a Low-Detectable Adversarial Patch, which attacks an object detector with small and texture-consistent adversarial patches, making these adversaries less likely to be recognized. Concretely, we use several geometric primitives to model the shapes and positions of the patches. To enhance our attack performance, we also assign different weights to the bounding boxes in terms of loss function. Our experiments on the common detection dataset COCO as well as the driving-video dataset D2-City show that LDAP is an effective attack method, and can resist the adversarial patch detector.

</details>


## 2021-10

<details>

<summary>2021-10-01 12:40:27 - A Modular End-to-End Framework for Secure Firmware Updates on Embedded Systems</summary>

- *Solon Falas, Charalambos Konstantinou, Maria K. Michael*

- `2007.09071v4` - [abs](http://arxiv.org/abs/2007.09071v4) - [pdf](http://arxiv.org/pdf/2007.09071v4)

> Firmware refers to device read-only resident code which includes microcode and macro-instruction -level routines. For Internet-of-Things (IoT) devices without an operating system, firmware includes all the necessary instructions on how such embedded systems operate and communicate. Thus, firmware updates are an essential part of device functionality. They provide the ability to patch vulnerabilities, address operational issues, and improve device reliability and performance during the lifetime of the system. This process, however, is often exploited by attackers in order to inject malicious firmware code into the embedded device. In this paper, we present a framework for secure firmware updates on embedded systems. The approach is based on hardware primitives and cryptographic modules, and it can be deployed in environments where communication channels might be insecure. The implementation of the framework is flexible as it can be adapted in regards to the IoT device's available hardware resources and constraints. Our security analysis shows that our framework is resilient to a variety of attack vectors. The experimental setup demonstrates the feasibility of the approach. By implementing a variety of test cases on FPGA, we demonstrate the adaptability and performance of the framework. Experiments indicate that the update procedure for a 1183kB firmware image could be achieved, in a secure manner, under 1.73 seconds.

</details>

<details>

<summary>2021-10-01 16:25:45 - Optic Disc Segmentation using Disk-Centered Patch Augmentation</summary>

- *Saeid Motevali, Aashis Khanal, Rolando Estrada*

- `2110.00512v1` - [abs](http://arxiv.org/abs/2110.00512v1) - [pdf](http://arxiv.org/pdf/2110.00512v1)

> The optic disc is a crucial diagnostic feature in the eye since changes to its physiognomy is correlated with the severity of various ocular and cardiovascular diseases. While identifying the bulk of the optic disc in a color fundus image is straightforward, accurately segmenting its boundary at the pixel level is very challenging. In this work, we propose disc-centered patch augmentation (DCPA) -- a simple, yet novel training scheme for deep neural networks -- to address this problem. DCPA achieves state-of-the-art results on full-size images even when using small neural networks, specifically a U-Net with only 7 million parameters as opposed to the original 31 million. In DCPA, we restrict the training data to patches that fully contain the optic nerve. In addition, we also train the network using dynamic cost functions to increase its robustness. We tested DCPA-trained networks on five retinal datasets: DRISTI, DRIONS-DB, DRIVE, AV-WIDE, and CHASE-DB. The first two had available optic disc ground truth, and we manually estimated the ground truth for the latter three. Our approach achieved state-of-the-art F1 and IOU results on four datasets (95 % F1, 91 % IOU on DRISTI; 92 % F1, 84 % IOU on DRIVE; 83 % F1, 71 % IOU on AV-WIDE; 83 % F1, 71 % IOU on CHASEDB) and competitive results on the fifth (95 % F1, 91 % IOU on DRIONS-DB), confirming its generality. Our open-source code and ground-truth annotations are available at: https://github.com/saeidmotevali/fundusdisk

</details>

<details>

<summary>2021-10-02 01:56:30 - Unlocking Pixels for Reinforcement Learning via Implicit Attention</summary>

- *Krzysztof Marcin Choromanski, Deepali Jain, Wenhao Yu, Xingyou Song, Jack Parker-Holder, Tingnan Zhang, Valerii Likhosherstov, Aldo Pacchiano, Anirban Santara, Yunhao Tang, Jie Tan, Adrian Weller*

- `2102.04353v5` - [abs](http://arxiv.org/abs/2102.04353v5) - [pdf](http://arxiv.org/pdf/2102.04353v5)

> There has recently been significant interest in training reinforcement learning (RL) agents in vision-based environments. This poses many challenges, such as high dimensionality and the potential for observational overfitting through spurious correlations. A promising approach to solve both of these problems is an attention bottleneck, which provides a simple and effective framework for learning high performing policies, even in the presence of distractions. However, due to poor scalability of attention architectures, these methods cannot be applied beyond low resolution visual inputs, using large patches (thus small attention matrices). In this paper we make use of new efficient attention algorithms, recently shown to be highly effective for Transformers, and demonstrate that these techniques can be successfully adopted for the RL setting. This allows our attention-based controllers to scale to larger visual inputs, and facilitate the use of smaller patches, even individual pixels, improving generalization. We show this on a range of tasks from the Distracting Control Suite to vision-based quadruped robots locomotion. We provide rigorous theoretical analysis of the proposed algorithm.

</details>

<details>

<summary>2021-10-05 00:40:43 - Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images</summary>

- *Kang Zhou, Jing Li, Weixin Luo, Zhengxin Li, Jianlong Yang, Huazhu Fu, Jun Cheng, Jiang Liu, Shenghua Gao*

- `2110.01761v1` - [abs](http://arxiv.org/abs/2110.01761v1) - [pdf](http://arxiv.org/pdf/2110.01761v1)

> Anomaly detection in medical images refers to the identification of abnormal images with only normal images in the training set. Most existing methods solve this problem with a self-reconstruction framework, which tends to learn an identity mapping and reduces the sensitivity to anomalies. To mitigate this problem, in this paper, we propose a novel Proxy-bridged Image Reconstruction Network (ProxyAno) for anomaly detection in medical images. Specifically, we use an intermediate proxy to bridge the input image and the reconstructed image. We study different proxy types, and we find that the superpixel-image (SI) is the best one. We set all pixels' intensities within each superpixel as their average intensity, and denote this image as SI. The proposed ProxyAno consists of two modules, a Proxy Extraction Module and an Image Reconstruction Module. In the Proxy Extraction Module, a memory is introduced to memorize the feature correspondence for normal image to its corresponding SI, while the memorized correspondence does not apply to the abnormal images, which leads to the information loss for abnormal image and facilitates the anomaly detection. In the Image Reconstruction Module, we map an SI to its reconstructed image. Further, we crop a patch from the image and paste it on the normal SI to mimic the anomalies, and enforce the network to reconstruct the normal image even with the pseudo abnormal SI. In this way, our network enlarges the reconstruction error for anomalies. Extensive experiments on brain MR images, retinal OCT images and retinal fundus images verify the effectiveness of our method for both image-level and pixel-level anomaly detection.

</details>

<details>

<summary>2021-10-05 13:37:43 - Semi-Supervised Deep Learning for Multiplex Networks</summary>

- *Anasua Mitra, Priyesh Vijayan, Ranbir Sanasam, Diganta Goswami, Srinivasan Parthasarathy, Balaraman Ravindran*

- `2110.02038v1` - [abs](http://arxiv.org/abs/2110.02038v1) - [pdf](http://arxiv.org/pdf/2110.02038v1)

> Multiplex networks are complex graph structures in which a set of entities are connected to each other via multiple types of relations, each relation representing a distinct layer. Such graphs are used to investigate many complex biological, social, and technological systems. In this work, we present a novel semi-supervised approach for structure-aware representation learning on multiplex networks. Our approach relies on maximizing the mutual information between local node-wise patch representations and label correlated structure-aware global graph representations to model the nodes and cluster structures jointly. Specifically, it leverages a novel cluster-aware, node-contextualized global graph summary generation strategy for effective joint-modeling of node and cluster representations across the layers of a multiplex network. Empirically, we demonstrate that the proposed architecture outperforms state-of-the-art methods in a range of tasks: classification, clustering, visualization, and similarity search on seven real-world multiplex networks for various experiment settings.

</details>

<details>

<summary>2021-10-05 18:23:36 - Improving Self-supervised Learning with Hardness-aware Dynamic Curriculum Learning: An Application to Digital Pathology</summary>

- *Chetan L Srinidhi, Anne L Martel*

- `2108.07183v2` - [abs](http://arxiv.org/abs/2108.07183v2) - [pdf](http://arxiv.org/pdf/2108.07183v2)

> Self-supervised learning (SSL) has recently shown tremendous potential to learn generic visual representations useful for many image analysis tasks. Despite their notable success, the existing SSL methods fail to generalize to downstream tasks when the number of labeled training instances is small or if the domain shift between the transfer domains is significant. In this paper, we attempt to improve self-supervised pretrained representations through the lens of curriculum learning by proposing a hardness-aware dynamic curriculum learning (HaDCL) approach. To improve the robustness and generalizability of SSL, we dynamically leverage progressive harder examples via easy-to-hard and hard-to-very-hard samples during mini-batch downstream fine-tuning. We discover that by progressive stage-wise curriculum learning, the pretrained representations are significantly enhanced and adaptable to both in-domain and out-of-domain distribution data.   We performed extensive validation on three histology benchmark datasets on both patch-wise and slide-level classification problems. Our curriculum based fine-tuning yields a significant improvement over standard fine-tuning, with a minimum improvement in area-under-the-curve (AUC) score of 1.7% and 2.2% on in-domain and out-of-domain distribution data, respectively. Further, we empirically show that our approach is more generic and adaptable to any SSL methods and does not impose any additional overhead complexity. Besides, we also outline the role of patch-based versus slide-based curriculum learning in histopathology to provide practical insights into the success of curriculum based fine-tuning of SSL methods. Code is released at https://github.com/srinidhiPY/ICCV-CDPATH2021-ID-8

</details>

<details>

<summary>2021-10-06 10:33:39 - Isogeometric continuity constraints for multi-patch shells governed by fourth-order deformation and phase field models</summary>

- *Karsten Paul, Christopher Zimmermann, Thang X. Duong, Roger A. Sauer*

- `2001.05964v3` - [abs](http://arxiv.org/abs/2001.05964v3) - [pdf](http://arxiv.org/pdf/2001.05964v3)

> This work presents numerical techniques to enforce continuity constraints on multi-patch surfaces for three distinct problem classes. The first involves structural analysis of thin shells that are described by general Kirchhoff-Love kinematics. Their governing equation is a vector-valued, fourth-order, nonlinear, partial differential equation (PDE) that requires at least $C^1$-continuity within a displacement-based finite element formulation. The second class are surface phase separations modeled by a phase field. Their governing equation is the Cahn-Hilliard equation - a scalar, fourth-order, nonlinear PDE - that can be coupled to the thin shell PDE. The third class are brittle fracture processes modeled by a phase field approach. In this work, these are described by a scalar, fourth-order, nonlinear PDE that is similar to the Cahn-Hilliard equation and is also coupled to the thin shell PDE. Using a direct finite element discretization, the two phase field equations also require at least a $C^1$-continuous formulation. Isogeometric surface discretizations - often composed of multiple patches - thus require constraints that enforce the $C^1$-continuity of displacement and phase field. For this, two numerical strategies are presented: For this, two numerical strategies are presented: A Lagrange multiplier formulation and a penalty method. The curvilinear shell model including the geometrical constraints is taken from Duong et al. (2017) and it is extended to model the coupled phase field problems on thin shells of Zimmermann et al. (2019) and Paul et al. (2020) on multi-patches. Their accuracy and convergence are illustrated by several numerical examples considering deforming shells, phase separations on evolving surfaces, and dynamic brittle fracture of thin shells.

</details>

<details>

<summary>2021-10-07 21:16:40 - MPD: Moving Target Defense through Communication Protocol Dialects</summary>

- *Yongsheng Mei, Kailash Gogineni, Tian Lan, Guru Venkataramani*

- `2110.03798v1` - [abs](http://arxiv.org/abs/2110.03798v1) - [pdf](http://arxiv.org/pdf/2110.03798v1)

> Communication protocol security is among the most significant challenges of the Internet of Things (IoT) due to the wide variety of hardware and software technologies involved. Moving target defense (MTD) has been adopted as an innovative strategy to solve this problem by dynamically changing target system properties and configurations to obfuscate the attack surface. Nevertheless, the existing work of MTD primarily focuses on lower-level properties (e.g., IP addresses or port numbers), and only a limited number of variations can be generated based on these properties. In this paper, we propose a new approach of MTD through communication protocol dialects (MPD) - which dynamically customizes a communication protocol into various protocol dialects and leverages them to create a moving target defense. Specifically, MPD harnesses a dialect generating function to create protocol dialects and then a mapping function to select one specific dialect for each packet during communication. To keep different network entities in synchronization, we also design a self-synchronization mechanism utilizing a pseudo-random number generator with the input of a pre-shared secret key and previously sent packets. We implement a prototype of MPD and evaluate its feasibility on standard network protocol (i.e., File Transfer Protocol) and internet of things protocol (i.e., Message Queuing Telemetry Transport). The results indicate that MPD can create a moving target defense with protocol dialects to effectively address various attacks - including the denial of service attack and malicious packet modifications - with negligible overhead.

</details>

<details>

<summary>2021-10-08 06:56:25 - CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention</summary>

- *Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, Wei Liu*

- `2108.00154v2` - [abs](http://arxiv.org/abs/2108.00154v2) - [pdf](http://arxiv.org/pdf/2108.00154v2)

> Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers do not yet possess the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the self-attention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks. The code has been released: https://github.com/cheerss/CrossFormer.

</details>

<details>

<summary>2021-10-08 08:59:16 - Maximize the Exploration of Congeneric Semantics for Weakly Supervised Semantic Segmentation</summary>

- *Ke Zhang, Sihong Chen, Qi Ju, Yong Jiang, Yucong Li, Xin He*

- `2110.03982v1` - [abs](http://arxiv.org/abs/2110.03982v1) - [pdf](http://arxiv.org/pdf/2110.03982v1)

> With the increase in the number of image data and the lack of corresponding labels, weakly supervised learning has drawn a lot of attention recently in computer vision tasks, especially in the fine-grained semantic segmentation problem. To alleviate human efforts from expensive pixel-by-pixel annotations, our method focuses on weakly supervised semantic segmentation (WSSS) with image-level tags, which are much easier to obtain. As a huge gap exists between pixel-level segmentation and image-level labels, how to reflect the image-level semantic information on each pixel is an important question. To explore the congeneric semantic regions from the same class to the maximum, we construct the patch-level graph neural network (P-GNN) based on the self-detected patches from different images that contain the same class labels. Patches can frame the objects as much as possible and include as little background as possible. The graph network that is established with patches as the nodes can maximize the mutual learning of similar objects. We regard the embedding vectors of patches as nodes, and use transformer-based complementary learning module to construct weighted edges according to the embedding similarity between different nodes. Moreover, to better supplement semantic information, we propose soft-complementary loss functions matched with the whole network structure. We conduct experiments on the popular PASCAL VOC 2012 benchmarks, and our model yields state-of-the-art performance.

</details>

<details>

<summary>2021-10-08 15:28:11 - Understanding Robustness of Transformers for Image Classification</summary>

- *Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, Andreas Veit*

- `2103.14586v2` - [abs](http://arxiv.org/abs/2103.14586v2) - [pdf](http://arxiv.org/pdf/2103.14586v2)

> Deep Convolutional Neural Networks (CNNs) have long been the architecture of choice for computer vision tasks. Recently, Transformer-based architectures like Vision Transformer (ViT) have matched or even surpassed ResNets for image classification. However, details of the Transformer architecture -- such as the use of non-overlapping patches -- lead one to wonder whether these networks are as robust. In this paper, we perform an extensive study of a variety of different measures of robustness of ViT models and compare the findings to ResNet baselines. We investigate robustness to input perturbations as well as robustness to model perturbations. We find that when pre-trained with a sufficient amount of data, ViT models are at least as robust as the ResNet counterparts on a broad range of perturbations. We also find that Transformers are robust to the removal of almost any single layer, and that while activations from later layers are highly correlated with each other, they nevertheless play an important role in classification.

</details>

<details>

<summary>2021-10-08 19:00:16 - Adversarial Token Attacks on Vision Transformers</summary>

- *Ameya Joshi, Gauri Jagatap, Chinmay Hegde*

- `2110.04337v1` - [abs](http://arxiv.org/abs/2110.04337v1) - [pdf](http://arxiv.org/pdf/2110.04337v1)

> Vision transformers rely on a patch token based self attention mechanism, in contrast to convolutional networks. We investigate fundamental differences between these two families of models, by designing a block sparsity based adversarial token attack. We probe and analyze transformer as well as convolutional models with token attacks of varying patch sizes. We infer that transformer models are more sensitive to token attacks than convolutional models, with ResNets outperforming Transformer models by up to $\sim30\%$ in robust accuracy for single token attacks.

</details>

<details>

<summary>2021-10-09 00:59:44 - Deep Interpretable Classification and Weakly-Supervised Segmentation of Histology Images via Max-Min Uncertainty</summary>

- *Soufiane Belharbi, Jérôme Rony, Jose Dolz, Ismail Ben Ayed, Luke McCaffrey, Eric Granger*

- `2011.07221v3` - [abs](http://arxiv.org/abs/2011.07221v3) - [pdf](http://arxiv.org/pdf/2011.07221v3)

> Weakly-supervised learning (WSL) has recently triggered substantial interest as it mitigates the lack of pixel-wise annotations. Given global image labels, WSL methods yield pixel-level predictions (segmentations), which enable to interpret class predictions. Despite their recent success, mostly with natural images, such methods can face important challenges when the foreground and background regions have similar visual cues, yielding high false-positive rates in segmentations, as is the case in challenging histology images. WSL training is commonly driven by standard classification losses, which implicitly maximize model confidence, and locate the discriminative regions linked to classification decisions. Therefore, they lack mechanisms for modeling explicitly non-discriminative regions and reducing false-positive rates. We propose novel regularization terms, which enable the model to seek both non-discriminative and discriminative regions, while discouraging unbalanced segmentations. We introduce high uncertainty as a criterion to localize non-discriminative regions that do not affect classifier decision, and describe it with original Kullback-Leibler (KL) divergence losses evaluating the deviation of posterior predictions from the uniform distribution. Our KL terms encourage high uncertainty of the model when the latter inputs the latent non-discriminative regions. Our loss integrates: (i) a cross-entropy seeking a foreground, where model confidence about class prediction is high; (ii) a KL regularizer seeking a background, where model uncertainty is high; and (iii) log-barrier terms discouraging unbalanced segmentations. Comprehensive experiments and ablation studies over the public GlaS colon cancer data and a Camelyon16 patch-based benchmark for breast cancer show substantial improvements over state-of-the-art WSL methods, and confirm the effect of our new regularizers.

</details>

<details>

<summary>2021-10-10 19:59:28 - Adversarial Attacks in a Multi-view Setting: An Empirical Study of the Adversarial Patches Inter-view Transferability</summary>

- *Bilel Tarchoun, Ihsen Alouani, Anouar Ben Khalifa, Mohamed Ali Mahjoub*

- `2110.04887v1` - [abs](http://arxiv.org/abs/2110.04887v1) - [pdf](http://arxiv.org/pdf/2110.04887v1)

> While machine learning applications are getting mainstream owing to a demonstrated efficiency in solving complex problems, they suffer from inherent vulnerability to adversarial attacks. Adversarial attacks consist of additive noise to an input which can fool a detector. Recently, successful real-world printable adversarial patches were proven efficient against state-of-the-art neural networks. In the transition from digital noise based attacks to real-world physical attacks, the myriad of factors affecting object detection will also affect adversarial patches. Among these factors, view angle is one of the most influential, yet under-explored. In this paper, we study the effect of view angle on the effectiveness of an adversarial patch. To this aim, we propose the first approach that considers a multi-view context by combining existing adversarial patches with a perspective geometric transformation in order to simulate the effect of view angle changes. Our approach has been evaluated on two datasets: the first dataset which contains most real world constraints of a multi-view context, and the second dataset which empirically isolates the effect of view angle. The experiments show that view angle significantly affects the performance of adversarial patches, where in some cases the patch loses most of its effectiveness. We believe that these results motivate taking into account the effect of view angles in future adversarial attacks, and open up new opportunities for adversarial defenses.

</details>

<details>

<summary>2021-10-11 11:01:26 - Privacy preserving local analysis of digital trace data: A proof-of-concept</summary>

- *Laura Boeschoten, Adriënne Mendrik, Emiel van der Veen, Jeroen Vloothuis, Haili Hu, Roos Voorvaart, Daniel Oberski*

- `2110.05154v1` - [abs](http://arxiv.org/abs/2110.05154v1) - [pdf](http://arxiv.org/pdf/2110.05154v1)

> We present PORT, a software platform for local data extraction and analysis of digital trace data. While digital trace data collected by private and public parties hold a huge potential for social-scientific discovery, their most useful parts have been unattainable for academic researchers due to privacy concerns and prohibitive API access. However, the EU General Data Protection Regulation (GDPR) grants all citizens the right to an electronic copy of their personal data. All major data controllers, such as social media platforms, banks, online shops, loyalty card systems and public transportation cards comply with this right by providing their clients with a `Data Download Package' (DDP). Previously, a conceptual workflow was introduced allowing citizens to donate their data to scientific- researchers. In this workflow, citizens' DDPs are processed locally on their machines before they are asked to provide informed consent to share a subset of the processed data with the researchers. In this paper, we present the newly developed software PORT that implements the local processing part of this workflow, protecting privacy by shielding sensitive data from any contact with outside observers -- including the researchers themselves. Thus, PORT enables a host of potential applications of social data science to hitherto unobtainable data.

</details>

<details>

<summary>2021-10-11 17:44:05 - Certified Patch Robustness via Smoothed Vision Transformers</summary>

- *Hadi Salman, Saachi Jain, Eric Wong, Aleksander Mądry*

- `2110.07719v1` - [abs](http://arxiv.org/abs/2110.07719v1) - [pdf](http://arxiv.org/pdf/2110.07719v1)

> Certified patch defenses can guarantee robustness of an image classifier to arbitrary changes within a bounded contiguous region. But, currently, this robustness comes at a cost of degraded standard accuracies and slower inference times. We demonstrate how using vision transformers enables significantly better certified patch robustness that is also more computationally efficient and does not incur a substantial drop in standard accuracy. These improvements stem from the inherent ability of the vision transformer to gracefully handle largely masked images. Our code is available at https://github.com/MadryLab/smoothed-vit.

</details>

<details>

<summary>2021-10-14 11:40:26 - DeepMoCap: Deep Optical Motion Capture Using Multiple Depth Sensors and Retro-Reflectors</summary>

- *Anargyros Chatzitofis, Dimitrios Zarpalas, Stefanos Kollias, Petros Daras*

- `2110.07283v1` - [abs](http://arxiv.org/abs/2110.07283v1) - [pdf](http://arxiv.org/pdf/2110.07283v1)

> In this paper, a marker-based, single-person optical motion capture method (DeepMoCap) is proposed using multiple spatio-temporally aligned infrared-depth sensors and retro-reflective straps and patches (reflectors). DeepMoCap explores motion capture by automatically localizing and labeling reflectors on depth images and, subsequently, on 3D space. Introducing a non-parametric representation to encode the temporal correlation among pairs of colorized depthmaps and 3D optical flow frames, a multi-stage Fully Convolutional Network (FCN) architecture is proposed to jointly learn reflector locations and their temporal dependency among sequential frames. The extracted reflector 2D locations are spatially mapped in 3D space, resulting in robust 3D optical data extraction. The subject's motion is efficiently captured by applying a template-based fitting technique on the extracted optical data. Two datasets have been created and made publicly available for evaluation purposes; one comprising multi-view depth and 3D optical flow annotated images (DMC2.5D), and a second, consisting of spatio-temporally aligned multi-view depth images along with skeleton, inertial and ground truth MoCap data (DMC3D). The FCN model outperforms its competitors on the DMC2.5D dataset using 2D Percentage of Correct Keypoints (PCK) metric, while the motion capture outcome is evaluated against RGB-D and inertial data fusion approaches on DMC3D, outperforming the next best method by 4.5% in total 3D PCK accuracy.

</details>

<details>

<summary>2021-10-15 07:20:46 - Breaking Bad? Semantic Versioning and Impact of Breaking Changes in Maven Central</summary>

- *Lina Ochoa, Thomas Degueule, Jean-Rémy Falleri, Jurgen Vinju*

- `2110.07889v1` - [abs](http://arxiv.org/abs/2110.07889v1) - [pdf](http://arxiv.org/pdf/2110.07889v1)

> Just like any software, libraries evolve to incorporate new features, bug fixes, security patches, and refactorings. However, when a library evolves, it may break the contract previously established with its clients by introducing Breaking Changes (BCs) in its API. These changes might trigger compile-time, link-time, or run-time errors in client code. As a result, clients may hesitate to upgrade their dependencies, raising security concerns and making future upgrades even more difficult.Understanding how libraries evolve helps client developers to know which changes to expect and where to expect them, and library developers to understand how they might impact their clients. In the most extensive study to date, Raemaekers et al. investigate to what extent developers of Java libraries hosted on the Maven Central Repository (MCR) follow semantic versioning conventions to signal the introduction of BCs and how these changes impact client projects. Their results suggest that BCs are widespread without regard for semantic versioning, with a significant impact on clients.In this paper, we conduct an external and differentiated replication study of their work. We identify and address some limitations of the original protocol and expand the analysis to a new corpus spanning seven more years of the MCR. We also present a novel static analysis tool for Java bytecode, Maracas, which provides us with: (i) the set of all BCs between two versions of a library; and (ii) the set of locations in client code impacted by individual BCs. Our key findings, derived from the analysis of 119, 879 library upgrades and 293, 817 clients, contrast with the original study and show that 83.4% of these upgrades do comply with semantic versioning. Furthermore, we observe that the tendency to comply with semantic versioning has significantly increased over time. Finally, we find that most BCs affect code that is not used by any client, and that only 7.9% of all clients are affected by BCs. These findings should help (i) library developers to understand and anticipate the impact of their changes; (ii) library users to estimate library upgrading effort and to pick libraries that are less likely to break; and (iii) researchers to better understand the dynamics of library-client co-evolution in Java.

</details>

<details>

<summary>2021-10-18 15:08:04 - A Generative Model for Texture Synthesis based on Optimal Transport between Feature Distributions</summary>

- *Antoine Houdard, Arthur Leclaire, Nicolas Papadakis, Julien Rabin*

- `2007.03408v2` - [abs](http://arxiv.org/abs/2007.03408v2) - [pdf](http://arxiv.org/pdf/2007.03408v2)

> We propose GOTEX, a general framework for texture synthesis by optimization that constrains the statistical distribution of local features. While our model encompasses several existing texture models, we focus on the case where the comparison between feature distributions relies on optimal transport distances. We show that the semi-dual formulation of optimal transport allows to control the distribution of various possible features, even if these features live in a high-dimensional space. We then study the resulting minimax optimization problem, which corresponds to a Wasserstein generative model, for which the inner concave maximization problem can be solved with standard stochastic gradient methods. The alternate optimization algorithm is shown to be versatile in terms of applications, features and architecture; in particular it allows to produce high-quality synthesized textures with different sets of features. We analyze the results obtained by constraining the distribution of patches or the distribution of responses to a pre-learned VGG neural network. We show that the patch representation can retrieve the desired textural aspect in a more precise manner. We also provide a detailed comparison with state-of-the-art texture synthesis methods. The GOTEX model based on patch features is also adapted to texture inpainting and texture interpolation. Finally, we show how to use our framework to learn a feed-forward neural network that can synthesize on-the-fly new textures of arbitrary size in a very fast manner. Experimental results and comparisons with the mainstream methods from the literature illustrate the relevance of the generative models learned with GOTEX.

</details>

<details>

<summary>2021-10-18 22:08:29 - A ground-truth dataset of real security patches</summary>

- *Sofia Reis, Rui Abreu*

- `2110.09635v1` - [abs](http://arxiv.org/abs/2110.09635v1) - [pdf](http://arxiv.org/pdf/2110.09635v1)

> Training machine learning approaches for vulnerability identification and producing reliable tools to assist developers in implementing quality software -- free of vulnerabilities -- is challenging due to the lack of large datasets and real data. Researchers have been looking at these issues and building datasets. However, these datasets usually miss natural language artifacts and programming language diversity. We scraped the entire CVE details database for GitHub references and augmented the data with 3 security-related datasets. We used the data to create a ground-truth dataset of natural language artifacts (such as commit messages, commits comments, and summaries), meta-data and code changes. Our dataset integrates a total of 8057 security-relevant commits -- the equivalent to 5942 security patches -- from 1339 different projects spanning 146 different types of vulnerabilities and 20 languages. A dataset of 110k non-security-related commits is also provided. Data and scripts are all available on GitHub. Data is stored in a .CSV file. Codebases can be downloaded using our scripts. Our dataset is a valuable asset to answer research questions on different topics such as the identification of security-relevant information using NLP models; software engineering and security best practices; and, vulnerability detection and patching; and, security program analysis.

</details>

<details>

<summary>2021-10-19 04:10:07 - CGNN: Traffic Classification with Graph Neural Network</summary>

- *Bo Pang, Yongquan Fu, Siyuan Ren, Ye Wang, Qing Liao, Yan Jia*

- `2110.09726v1` - [abs](http://arxiv.org/abs/2110.09726v1) - [pdf](http://arxiv.org/pdf/2110.09726v1)

> Traffic classification associates packet streams with known application labels, which is vital for network security and network management. With the rise of NAT, port dynamics, and encrypted traffic, it is increasingly challenging to obtain unified traffic features for accurate classification. Many state-of-the-art traffic classifiers automatically extract features from the packet stream based on deep learning models such as convolution networks. Unfortunately, the compositional and causal relationships between packets are not well extracted in these deep learning models, which affects both prediction accuracy and generalization on different traffic types.   In this paper, we present a chained graph model on the packet stream to keep the chained compositional sequence. Next, we propose CGNN, a graph neural network based traffic classification method, which builds a graph classifier over automatically extracted features over the chained graph.   Extensive evaluation over real-world traffic data sets, including normal, encrypted and malicious labels, show that, CGNN improves the prediction accuracy by 23\% to 29\% for application classification, by 2\% to 37\% for malicious traffic classification, and reaches the same accuracy level for encrypted traffic classification. CGNN is quite robust in terms of the recall and precision metrics. We have extensively evaluated the parameter sensitivity of CGNN, which yields optimized parameters that are quite effective for traffic classification.

</details>

<details>

<summary>2021-10-19 19:36:59 - Patch Based Transformation for Minimum Variance Beamformer Image Approximation Using Delay and Sum Pipeline</summary>

- *Sairoop Bodepudi, A N Madhavanunni, Mahesh Raveendranatha Panicker*

- `2110.10220v1` - [abs](http://arxiv.org/abs/2110.10220v1) - [pdf](http://arxiv.org/pdf/2110.10220v1)

> In the recent past, there have been several efforts in accelerating computationally heavy beamforming algorithms such as minimum variance distortionless response (MVDR) beamforming to achieve real-time performance comparable to the popular delay and sum (DAS) beamforming. This has been achieved using a variety of neural network architectures ranging from fully connected neural networks (FCNNs), convolutional neural networks (CNNs) and general adversarial networks (GANs). However most of these approaches are working with optimizations considering image level losses and hence require a significant amount of dataset to ensure that the process of beamforming is learned. In this work, a patch level U-Net based neural network is proposed, where the delay compensated radio frequency (RF) patch for a fixed region in space (e.g. 32x32) is transformed through a U-Net architecture and multiplied with DAS apodization weights and optimized for similarity with MVDR image of the patch. Instead of framing the beamforming problem as a regression problem to estimate the apodization weights, the proposed approach treats the non-linear transformation of the RF data space that can account for the data driven weight adaptation done by the MVDR approach in the parameters of the network. In this way, it is also observed that by restricting the input to a patch the model will learn the beamforming pipeline as an image non-linear transformation problem.

</details>

<details>

<summary>2021-10-19 20:23:20 - Machine learning based automated identification of thunderstorms from anemometric records using shapelet transform</summary>

- *Monica Arul, Ahsan Kareem*

- `2101.04516v2` - [abs](http://arxiv.org/abs/2101.04516v2) - [pdf](http://arxiv.org/pdf/2101.04516v2)

> Detection of thunderstorms is important to the wind hazard community to better understand extreme winds field characteristics and associated wind induced load effects on structures. This paper contributes to this effort by proposing a new course of research that uses machine learning techniques, independent of wind statistics based parameters, to autonomously identify and separate thunderstorms from large databases containing high frequency sampled continuous wind speed measurements. In this context, the use of Shapelet transform is proposed to identify key individual attributes distinctive to extreme wind events based on similarity of shape of their time series. This novel shape based representation when combined with machine learning algorithms yields a practical event detection procedure with minimal domain expertise. In this paper, the shapelet transform along with Random Forest classifier is employed for the identification of thunderstorms from 1 year of data from 14 ultrasonic anemometers that are a part of an extensive in situ wind monitoring network in the Northern Mediterranean ports. A collective total of 235 non-stationary records associated with thunderstorms were identified using this method. The results lead to enhancing the pool of thunderstorm data for more comprehensive understanding of a wide variety of thunderstorms that have not been previously detected using conventional gust factor-based methods.

</details>

<details>

<summary>2021-10-20 06:47:28 - AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation</summary>

- *Xiangyi Yan, Hao Tang, Shanlin Sun, Haoyu Ma, Deying Kong, Xiaohui Xie*

- `2110.10403v1` - [abs](http://arxiv.org/abs/2110.10403v1) - [pdf](http://arxiv.org/pdf/2110.10403v1)

> Recent advances in transformer-based models have drawn attention to exploring these techniques in medical image segmentation, especially in conjunction with the U-Net model (or its variants), which has shown great success in medical image segmentation, under both 2D and 3D settings. Current 2D based methods either directly replace convolutional layers with pure transformers or consider a transformer as an additional intermediate encoder between the encoder and decoder of U-Net. However, these approaches only consider the attention encoding within one single slice and do not utilize the axial-axis information naturally provided by a 3D volume. In the 3D setting, convolution on volumetric data and transformers both consume large GPU memory. One has to either downsample the image or use cropped local patches to reduce GPU memory usage, which limits its performance. In this paper, we propose Axial Fusion Transformer UNet (AFTer-UNet), which takes both advantages of convolutional layers' capability of extracting detailed features and transformers' strength on long sequence modeling. It considers both intra-slice and inter-slice long-range cues to guide the segmentation. Meanwhile, it has fewer parameters and takes less GPU memory to train than the previous transformer-based models. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.

</details>

<details>

<summary>2021-10-20 15:24:14 - NetRep: Automatic Repair for Network Programs</summary>

- *Lei Shi, Yuepeng Wang, Rajeev Alur, Boon Thau Loo*

- `2110.06303v2` - [abs](http://arxiv.org/abs/2110.06303v2) - [pdf](http://arxiv.org/pdf/2110.06303v2)

> Debugging imperative network programs is a challenging task for developers because understanding various network modules and complicated data structures is typically time-consuming. To address the challenge, this paper presents an automated technique for repairing network programs from unit tests. Specifically, given as input a faulty network program and a set of unit tests, our approach localizes the fault through symbolic reasoning, and synthesizes a patch such that the repaired program can pass all unit tests. It applies domain-specific abstraction to simplify network data structures and utilizes modular analysis to facilitate function summary reuse for symbolic analysis. We implement the proposed techniques in a tool called NetRep and evaluate it on 10 benchmarks adapted from real-world software-defined networking controllers. The evaluation results demonstrate the effectiveness and efficiency of NetRep for repairing network programs.

</details>

<details>

<summary>2021-10-20 18:05:33 - Predicting Tau Accumulation in Cerebral Cortex with Multivariate MRI Morphometry Measurements, Sparse Coding, and Correntropy</summary>

- *Jianfeng Wu, Wenhui Zhu, Yi Su, Jie Gui, Natasha Lepore, Eric M. Reiman, Richard J. Caselli, Paul M. Thompson, Kewei Chen, Yalin Wang*

- `2110.10709v1` - [abs](http://arxiv.org/abs/2110.10709v1) - [pdf](http://arxiv.org/pdf/2110.10709v1)

> Biomarker-assisted diagnosis and intervention in Alzheimer's disease (AD) may be the key to prevention breakthroughs. One of the hallmarks of AD is the accumulation of tau plaques in the human brain. However, current methods to detect tau pathology are either invasive (lumbar puncture) or quite costly and not widely available (Tau PET). In our previous work, structural MRI-based hippocampal multivariate morphometry statistics (MMS) showed superior performance as an effective neurodegenerative biomarker for preclinical AD and Patch Analysis-based Surface Correntropy-induced Sparse coding and max-pooling (PASCS-MP) has excellent ability to generate low-dimensional representations with strong statistical power for brain amyloid prediction. In this work, we apply this framework together with ridge regression models to predict Tau deposition in Braak12 and Braak34 brain regions separately. We evaluate our framework on 925 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Each subject has one pair consisting of a PET image and MRI scan which were collected at about the same times. Experimental results suggest that the representations from our MMS and PASCS-MP have stronger predictive power and their predicted Braak12 and Braak34 are closer to the real values compared to the measures derived from other approaches such as hippocampal surface area and volume, and shape morphometry features based on spherical harmonics (SPHARM).

</details>

<details>

<summary>2021-10-20 20:17:18 - Accelerating quantum many-body configuration interaction with directives</summary>

- *Brandon Cook, Patrick J. Fasano, Pieter Maris, Chao Yang, Dossay Oryspayev*

- `2110.10765v1` - [abs](http://arxiv.org/abs/2110.10765v1) - [pdf](http://arxiv.org/pdf/2110.10765v1)

> Many-Fermion Dynamics-nuclear, or MFDn, is a configuration interaction (CI) code for nuclear structure calculations. It is a platform-independent Fortran 90 code using a hybrid MPI+X programming model. For CPU platforms the application has a robust and optimized OpenMP implementation for shared memory parallelism. As part of the NESAP application readiness program for NERSC's latest Perlmutter system, MFDn has been updated to take advantage of accelerators. The current mainline GPU port is based on OpenACC. In this work we describe some of the key challenges of creating an efficient GPU implementation. Additionally, we compare the support of OpenMP and OpenACC on AMD and NVIDIA GPUs.

</details>

<details>

<summary>2021-10-22 11:31:33 - Floating Isogeometric Analysis</summary>

- *Helge C. Hille, Siddhant Kumar, Laura De Lorenzis*

- `2110.07355v2` - [abs](http://arxiv.org/abs/2110.07355v2) - [pdf](http://arxiv.org/pdf/2110.07355v2)

> We propose Floating Isogeometric Analysis (FLIGA), which extends the concepts of IGA to Lagrangian extreme deformation analysis. The method is based on a novel tensor-product construction of B-Splines for the update of the basis functions in one direction of the parametric space. With basis functions 'floating' deformation-dependently in this direction, mesh distortion is overcome for problems in which extreme deformations occur predominantly along the associated (possibly curved) physical axis. In doing so, we preserve the numerical advantages of splines over many meshless basis functions, while avoiding remeshing. We employ material point integration for numerical quadrature attributing a Lagrangian character to our technique. The paper introduces the method and reviews the fundamental properties of the FLIGA basis functions, including a numerical patch test. The performance of FLIGA is then numerically investigated on the benchmark of Newtonian and viscoelastic Taylor-Couette flow. Finally, we simulate a viscoelastic extrusion-based additive manufacturing process, which served as the original motivation for the new approach.

</details>

<details>

<summary>2021-10-22 21:45:38 - Chasing Sparsity in Vision Transformers: An End-to-End Exploration</summary>

- *Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, Zhangyang Wang*

- `2106.04533v3` - [abs](http://arxiv.org/abs/2106.04533v3) - [pdf](http://arxiv.org/pdf/2106.04533v3)

> Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacrificing the achievable accuracy. We carry out the first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity in ViTs "from end to end". Specifically, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the final output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside ViTs. We further co-explore data and architecture sparsity for additional efficiency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain significantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we find that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing "free lunch". For example, our sparsified DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs and 4.40% running time savings. Our codes are available at https://github.com/VITA-Group/SViTE.

</details>

<details>

<summary>2021-10-25 11:38:51 - Restore from Restored: Single-image Inpainting</summary>

- *Eunhye Lee, Jeongmu Kim, Jisu Kim, Tae Hyun Kim*

- `2110.12822v1` - [abs](http://arxiv.org/abs/2110.12822v1) - [pdf](http://arxiv.org/pdf/2110.12822v1)

> Recent image inpainting methods have shown promising results due to the power of deep learning, which can explore external information available from the large training dataset. However, many state-of-the-art inpainting networks are still limited in exploiting internal information available in the given input image at test time. To mitigate this problem, we present a novel and efficient self-supervised fine-tuning algorithm that can adapt the parameters of fully pre-trained inpainting networks without using ground-truth target images. We update the parameters of the pre-trained state-of-the-art inpainting networks by utilizing existing self-similar patches (i.e., self-exemplars) within the given input image without changing the network architecture and improve the inpainting quality by a large margin. Qualitative and quantitative experimental results demonstrate the superiority of the proposed algorithm, and we achieve state-of-the-art inpainting results on publicly available benchmark datasets.

</details>

<details>

<summary>2021-10-25 14:30:31 - RoBin: Facilitating the Reproduction of Configuration-Related Vulnerability</summary>

- *Ligeng Chen, Jian Guo, Zhongling He, Dongliang Mu, Bing Mao*

- `2110.12989v1` - [abs](http://arxiv.org/abs/2110.12989v1) - [pdf](http://arxiv.org/pdf/2110.12989v1)

> Vulnerability reproduction paves a way in debugging software failures, which need intensive manual efforts. However, some key factors (e.g., software configuration, trigger method) are often missing, so we can not directly reproduce the failure without extra attempts. Even worse, highly customized configuration options of programs create a barrier for reproducing the vulnerabilities that only appear under some specific combinations of configurations. In this paper, we address the problem mentioned above -- reproducing the configuration-related vulnerability. We try to solve it by proposing a binary similarity-based method to infer the specific building configurations via the binary from crash report. The main challenges are as follows: precise compilation option inference, program configuration inference, and source-code-to-binary matching. To achieve the goal, we implement RoBin, a binary similarity-based building configuration inference tool. To demonstrate the effectiveness, we test RoBin on 21 vulnerable cases upon 4 well-known open-source programs. It shows a strong ability in pinpointing the building configurations causing the vulnerability. The result can help developers reproduce and diagnose the vulnerability, and finally, patch the programs.

</details>

<details>

<summary>2021-10-25 20:39:28 - Pediatric Otoscopy Video Screening with Shift Contrastive Anomaly Detection</summary>

- *Weiyao Wang, Aniruddha Tamhane, Christine Santos, John R. Rzasa, James H. Clark, Therese L. Canares, Mathias Unberath*

- `2110.13254v1` - [abs](http://arxiv.org/abs/2110.13254v1) - [pdf](http://arxiv.org/pdf/2110.13254v1)

> Ear related concerns and symptoms represents the leading indication for seeking pediatric healthcare attention. Despite the high incidence of such encounters, the diagnostic process of commonly encountered disease of the middle and external presents significant challenge. Much of this challenge stems from the lack of cost effective diagnostic testing, which necessitating the presence or absence of ear pathology to be determined clinically. Research has however demonstrated considerable variation among clinicians in their ability to accurately diagnose and consequently manage ear pathology. With recent advances in computer vision and machine learning, there is an increasing interest in helping clinicians to accurately diagnose middle and external ear pathology with computer-aided systems. It has been shown that AI has the capacity to analyse a single clinical image captured during examination of the ear canal and eardrum from which it can determine the likelihood of a pathognomonic pattern for a specific diagnosis being present. The capture of such an image can however be challenging especially to inexperienced clinicians. To help mitigate this technical challenge we have developed and tested a method using video sequences. We present a two stage method that first, identifies valid frames by detecting and extracting ear drum patches from the video sequence, and second, performs the proposed shift contrastive anomaly detection to flag the otoscopy video sequences as normal or abnormal. Our method achieves an AUROC of 88.0% on the patient-level and also outperforms the average of a group of 25 clinicians in a comparative study, which is the largest of such published to date. We conclude that the presented method achieves a promising first step towards automated analysis of otoscopy video.

</details>

<details>

<summary>2021-10-26 02:24:24 - Transformer in Transformer</summary>

- *Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang*

- `2103.00112v3` - [abs](http://arxiv.org/abs/2103.00112v3) - [pdf](http://arxiv.org/pdf/2103.00112v3)

> Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\times$16) as "visual sentences" and present to further divide them into smaller patches (e.g., 4$\times$4) as "visual words". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.

</details>

<details>

<summary>2021-10-26 14:08:00 - Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition</summary>

- *Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, Gao Huang*

- `2105.15075v2` - [abs](http://arxiv.org/abs/2105.15075v2) - [pdf](http://arxiv.org/pdf/2105.15075v2)

> Vision Transformers (ViT) have achieved remarkable success in large-scale image recognition. They split every 2D image into a fixed number of patches, each of which is treated as a token. Generally, representing an image with more tokens would lead to higher prediction accuracy, while it also results in drastically increased computational cost. To achieve a decent trade-off between accuracy and speed, the number of tokens is empirically set to 16x16 or 14x14. In this paper, we argue that every image has its own characteristics, and ideally the token number should be conditioned on each individual input. In fact, we have observed that there exist a considerable number of "easy" images which can be accurately predicted with a mere number of 4x4 tokens, while only a small fraction of "hard" ones need a finer representation. Inspired by this phenomenon, we propose a Dynamic Transformer to automatically configure a proper number of tokens for each input image. This is achieved by cascading multiple Transformers with increasing numbers of tokens, which are sequentially activated in an adaptive fashion at test time, i.e., the inference is terminated once a sufficiently confident prediction is produced. We further design efficient feature reuse and relationship reuse mechanisms across different components of the Dynamic Transformer to reduce redundant computations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate that our method significantly outperforms the competitive baselines in terms of both theoretical computational efficiency and practical inference speed. Code and pre-trained models (based on PyTorch and MindSpore) are available at https://github.com/blackfeather-wang/Dynamic-Vision-Transformer and https://github.com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore.

</details>

<details>

<summary>2021-10-26 23:05:00 - CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud Registration</summary>

- *Hao Yu, Fu Li, Mahdi Saleh, Benjamin Busam, Slobodan Ilic*

- `2110.14076v1` - [abs](http://arxiv.org/abs/2110.14076v1) - [pdf](http://arxiv.org/pdf/2110.14076v1)

> We study the problem of extracting correspondences between a pair of point clouds for registration. For correspondence retrieval, existing works benefit from matching sparse keypoints detected from dense points but usually struggle to guarantee their repeatability. To address this issue, we present CoFiNet - Coarse-to-Fine Network which extracts hierarchical correspondences from coarse to fine without keypoint detection. On a coarse scale and guided by a weighting scheme, our model firstly learns to match down-sampled nodes whose vicinity points share more overlap, which significantly shrinks the search space of a consecutive stage. On a finer scale, node proposals are consecutively expanded to patches that consist of groups of points together with associated descriptors. Point correspondences are then refined from the overlap areas of corresponding patches, by a density-adaptive matching module capable to deal with varying point density. Extensive evaluation of CoFiNet on both indoor and outdoor standard benchmarks shows our superiority over existing methods. Especially on 3DLoMatch where point clouds share less overlap, CoFiNet significantly outperforms state-of-the-art approaches by at least 5% on Registration Recall, with at most two-third of their parameters.

</details>

<details>

<summary>2021-10-26 23:22:45 - DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks</summary>

- *Chong Xiang, Prateek Mittal*

- `2102.02956v3` - [abs](http://arxiv.org/abs/2102.02956v3) - [pdf](http://arxiv.org/pdf/2102.02956v3)

> State-of-the-art object detectors are vulnerable to localized patch hiding attacks, where an adversary introduces a small adversarial patch to make detectors miss the detection of salient objects. The patch attacker can carry out a physical-world attack by printing and attaching an adversarial patch to the victim object. In this paper, we propose DetectorGuard as the first general framework for building provably robust object detectors against localized patch hiding attacks. DetectorGuard is inspired by recent advancements in robust image classification research; we ask: can we adapt robust image classifiers for robust object detection? Unfortunately, due to their task difference, an object detector naively adapted from a robust image classifier 1) may not necessarily be robust in the adversarial setting or 2) even maintain decent performance in the clean setting. To build a high-performance robust object detector, we propose an objectness explaining strategy: we adapt a robust image classifier to predict objectness for every image location and then explain each objectness using the bounding boxes predicted by a conventional object detector. If all objectness is well explained, we output the predictions made by the conventional object detector; otherwise, we issue an attack alert. Notably, 1) in the adversarial setting, we formally prove the end-to-end robustness of DetectorGuard on certified objects, i.e., it either detects the object or triggers an alert, against any patch hiding attacker within our threat model; 2) in the clean setting, we have almost the same performance as state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO, and KITTI datasets further demonstrates that DetectorGuard achieves the first provable robustness against localized patch hiding attacks at a negligible cost (<1%) of clean performance.

</details>

<details>

<summary>2021-10-27 12:36:58 - Traffic Forecasting on Traffic Moving Snippets</summary>

- *Nina Wiedemann, Martin Raubal*

- `2110.14383v1` - [abs](http://arxiv.org/abs/2110.14383v1) - [pdf](http://arxiv.org/pdf/2110.14383v1)

> Advances in traffic forecasting technology can greatly impact urban mobility. In the traffic4cast competition, the task of short-term traffic prediction is tackled in unprecedented detail, with traffic volume and speed information available at 5 minute intervals and high spatial resolution. To improve generalization to unknown cities, as required in the 2021 extended challenge, we propose to predict small quadratic city sections, rather than processing a full-city-raster at once. At test time, breaking down the test data into spatially-cropped overlapping snippets improves stability and robustness of the final predictions, since multiple patches covering one cell can be processed independently. With the performance on the traffic4cast test data and further experiments on a validation set it is shown that patch-wise prediction indeed improves accuracy. Further advantages can be gained with a Unet++ architecture and with an increasing number of patches per sample processed at test time. We conclude that our snippet-based method, combined with other successful network architectures proposed in the competition, can leverage performance, in particular on unseen cities. All source code is available at https://github.com/NinaWie/NeurIPS2021-traffic4cast.

</details>

<details>

<summary>2021-10-28 18:31:07 - RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem</summary>

- *Eric Liang, Zhanghao Wu, Michael Luo, Sven Mika, Joseph E. Gonzalez, Ion Stoica*

- `2011.12719v4` - [abs](http://arxiv.org/abs/2011.12719v4) - [pdf](http://arxiv.org/pdf/2011.12719v4)

> Researchers and practitioners in the field of reinforcement learning (RL) frequently leverage parallel computation, which has led to a plethora of new algorithms and systems in the last few years. In this paper, we re-examine the challenges posed by distributed RL and try to view it through the lens of an old idea: distributed dataflow. We show that viewing RL as a dataflow problem leads to highly composable and performant implementations. We propose RLlib Flow, a hybrid actor-dataflow programming model for distributed RL, and validate its practicality by porting the full suite of algorithms in RLlib, a widely adopted distributed RL library. Concretely, RLlib Flow provides 2-9 code savings in real production code and enables the composition of multi-agent algorithms not possible by end users before. The open-source code is available as part of RLlib at https://github.com/ray-project/ray/tree/master/rllib.

</details>

<details>

<summary>2021-10-29 03:52:56 - PEDENet: Image Anomaly Localization via Patch Embedding and Density Estimation</summary>

- *Kaitai Zhang, Bin Wang, C. -C. Jay Kuo*

- `2110.15525v1` - [abs](http://arxiv.org/abs/2110.15525v1) - [pdf](http://arxiv.org/pdf/2110.15525v1)

> A neural network targeting at unsupervised image anomaly localization, called the PEDENet, is proposed in this work. PEDENet contains a patch embedding (PE) network, a density estimation (DE) network, and an auxiliary network called the location prediction (LP) network. The PE network takes local image patches as input and performs dimension reduction to get low-dimensional patch embeddings via a deep encoder structure. Being inspired by the Gaussian Mixture Model (GMM), the DE network takes those patch embeddings and then predicts the cluster membership of an embedded patch. The sum of membership probabilities is used as a loss term to guide the learning process. The LP network is a Multi-layer Perception (MLP), which takes embeddings from two neighboring patches as input and predicts their relative location. The performance of the proposed PEDENet is evaluated extensively and benchmarked with that of state-of-the-art methods.

</details>

<details>

<summary>2021-10-29 15:35:59 - Tree-Cotree Decomposition of Isogeometric Mortared Spaces in H(curl) on Multi-Patch Domains</summary>

- *Bernard Kapidani, Melina Merkel, Sebastian Schöps, Rafael Vázquez*

- `2110.15860v1` - [abs](http://arxiv.org/abs/2110.15860v1) - [pdf](http://arxiv.org/pdf/2110.15860v1)

> When applying isogeometric analysis to engineering problems, one often deals with multi-patch spline spaces that have incompatible discretisations, e.g. in the case of moving objects. In such cases mortaring has been shown to be advantageous. This contribution discusses the appropriate B-spline spaces needed for the solution of Maxwell's equations in the functions space H(curl) and the corresponding mortar spaces. The main contribution of this paper is to show that in formulations requiring gauging, as in the vector potential formulation of magnetostatic equations, one can remove the discrete kernel subspace from the mortared spaces by the graph-theoretical concept of a tree-cotree decomposition. The tree-cotree decomposition is done based on the control mesh, it works for non-contractible domains, and it can be straightforwardly applied independently of the degree of the B-spline bases. Finally, the simulation workflow is demonstrated using a realistic model of a rotating permanent magnet synchronous machine.

</details>

<details>

<summary>2021-10-31 20:41:55 - A Graphical Framework for the Category-Based Metamodel for Access Control and Obligations</summary>

- *Sandra Alves, Jorge Iglésias*

- `2111.00588v1` - [abs](http://arxiv.org/abs/2111.00588v1) - [pdf](http://arxiv.org/pdf/2111.00588v1)

> We design a graph-based framework for the visualisation and analysis of obligations in access control policies. We consider obligation policies in CBACO, the category-based access control model, which has been shown to subsume many of the most well known access control such as MAC, DAC, RBAC. CBACO is an extension of the CBAC metamodel that deals with obligations. We describe the implementation of the proposed model in PORGY, a strategy driven graph-rewriting tool, based on the theory of port-graphs. CBACO policies allow for dynamic behavior in the modelled systems, which is implemented using the strategy language of PORGY.

</details>


## 2021-11

<details>

<summary>2021-11-01 02:41:42 - Mem3DG: Modeling Membrane Mechanochemical Dynamics in 3D using Discrete Differential Geometry</summary>

- *Cuncheng Zhu, Christopher T. Lee, Padmini Rangamani*

- `2111.04460v1` - [abs](http://arxiv.org/abs/2111.04460v1) - [pdf](http://arxiv.org/pdf/2111.04460v1)

> Biomembranes adopt varying morphologies that are vital to cellular functions. Many studies use computational modeling to understand how various mechanochemical factors contribute to membrane shape transformations. Compared to approximation-based methods (e.g., finite element method), the class of discrete mesh models offers greater flexibility to simulate complex physics and shapes in three dimensions; its formulation produces an efficient algorithm while maintaining coordinate-free geometric descriptions. However, ambiguities in geometric definitions in the discrete context have led to a lack of consensus on which discrete mesh model is theoretically and numerically optimal; a bijective relationship between the terms contributing to both the energy and forces from the discrete and smooth geometric theories remains to be established. We address this and present an extensible framework, $\texttt{Mem3DG}$, for modeling 3D mechanochemical dynamics of membranes based on Discrete Differential Geometry (DDG) on triangulated meshes. The formalism of DDG resolves the inconsistency and provides a unifying perspective on how to relate the smooth and discrete energy and forces. To demonstrate, $\texttt{Mem3DG}$ is used to model a sequence of examples with increasing mechanochemical complexity: recovering classical shape transformations such as 1) biconcave disk, dumbbell, and unduloid and 2) spherical bud on spherical, flat-patch membrane; investigating how the coupling of membrane mechanics with protein mobility jointly affects phase and shape transformation. As high-resolution 3D imaging of membrane ultrastructure becomes more readily available, we envision Mem3DG to be applied as an end-to-end tool to simulate realistic cell geometry under user-specified mechanochemical conditions.

</details>

<details>

<summary>2021-11-02 17:59:00 - PatchGame: Learning to Signal Mid-level Patches in Referential Games</summary>

- *Kamal Gupta, Gowthami Somepalli, Anubhav Gupta, Vinoj Jayasundara, Matthias Zwicker, Abhinav Shrivastava*

- `2111.01785v1` - [abs](http://arxiv.org/abs/2111.01785v1) - [pdf](http://arxiv.org/pdf/2111.01785v1)

> We study a referential game (a type of signaling game) where two agents communicate with each other via a discrete bottleneck to achieve a common goal. In our referential game, the goal of the speaker is to compose a message or a symbolic representation of "important" image patches, while the task for the listener is to match the speaker's message to a different view of the same image. We show that it is indeed possible for the two agents to develop a communication protocol without explicit or implicit supervision. We further investigate the developed protocol and show the applications in speeding up recent Vision Transformers by using only important patches, and as pre-training for downstream recognition tasks (e.g., classification). Code available at https://github.com/kampta/PatchGame.

</details>

<details>

<summary>2021-11-03 00:53:16 - Can Vision Transformers Perform Convolution?</summary>

- *Shanda Li, Xiangning Chen, Di He, Cho-Jui Hsieh*

- `2111.01353v2` - [abs](http://arxiv.org/abs/2111.01353v2) - [pdf](http://arxiv.org/pdf/2111.01353v2)

> Several recent studies have demonstrated that attention-based networks, such as Vision Transformer (ViT), can outperform Convolutional Neural Networks (CNNs) on several computer vision tasks without using convolutional layers. This naturally leads to the following questions: Can a self-attention layer of ViT express any convolution operation? In this work, we prove that a single ViT layer with image patches as the input can perform any convolution operation constructively, where the multi-head attention mechanism and the relative positional encoding play essential roles. We further provide a lower bound on the number of heads for Vision Transformers to express CNNs. Corresponding with our analysis, experimental results show that the construction in our proof can help inject convolutional bias into Transformers and significantly improve the performance of ViT in low data regimes.

</details>

<details>

<summary>2021-11-04 04:11:47 - Texture Memory-Augmented Deep Patch-Based Image Inpainting</summary>

- *Rui Xu, Minghao Guo, Jiaqi Wang, Xiaoxiao Li, Bolei Zhou, Chen Change Loy*

- `2009.13240v2` - [abs](http://arxiv.org/abs/2009.13240v2) - [pdf](http://arxiv.org/pdf/2009.13240v2)

> Patch-based methods and deep networks have been employed to tackle image inpainting problem, with their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region with high-quality texture through searching nearest neighbor patches from the unmasked regions. However, these methods bring problematic contents when recovering large missing regions. Deep networks, on the other hand, show promising results in completing large regions. Nonetheless, the results often lack faithful and sharp details that resemble the surrounding area. By bringing together the best of both paradigms, we propose a new deep inpainting framework where texture generation is guided by a texture memory of patch samples extracted from unmasked regions. The framework has a novel design that allows texture memory retrieval to be trained end-to-end with the deep inpainting network. In addition, we introduce a patch distribution loss to encourage high-quality patch synthesis. The proposed method shows superior performance both qualitatively and quantitatively on three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris Street-View datasets.

</details>

<details>

<summary>2021-11-04 07:53:13 - ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers</summary>

- *Husheng Han, Kaidi Xu, Xing Hu, Xiaobing Chen, Ling Liang, Zidong Du, Qi Guo, Yanzhi Wang, Yunji Chen*

- `2110.14120v2` - [abs](http://arxiv.org/abs/2110.14120v2) - [pdf](http://arxiv.org/pdf/2110.14120v2)

> Adversarial patch attacks that craft the pixels in a confined region of the input images show their powerful attack effectiveness in physical environments even with noises or deformations. Existing certified defenses towards adversarial patch attacks work well on small images like MNIST and CIFAR-10 datasets, but achieve very poor certified accuracy on higher-resolution images like ImageNet. It is urgent to design both robust and effective defenses against such a practical and harmful attack in industry-level larger images. In this work, we propose the certified defense methodology that achieves high provable robustness for high-resolution images and largely improves the practicality for real adoption of the certified defense. The basic insight of our work is that the adversarial patch intends to leverage localized superficial important neurons (SIN) to manipulate the prediction results. Hence, we leverage the SIN-based DNN compression techniques to significantly improve the certified accuracy, by reducing the adversarial region searching overhead and filtering the prediction noises. Our experimental results show that the certified accuracy is increased from 36.3% (the state-of-the-art certified detection) to 60.4% on the ImageNet dataset, largely pushing the certified defenses for practical use.

</details>

<details>

<summary>2021-11-05 04:56:34 - Remote Sensing Image Super-resolution and Object Detection: Benchmark and State of the Art</summary>

- *Yi Wang, Syed Muhammad Arsalan Bashir, Mahrukh Khan, Qudrat Ullah, Rui Wang, Yilin Song, Zhe Guo, Yilong Niu*

- `2111.03260v1` - [abs](http://arxiv.org/abs/2111.03260v1) - [pdf](http://arxiv.org/pdf/2111.03260v1)

> For the past two decades, there have been significant efforts to develop methods for object detection in Remote Sensing (RS) images. In most cases, the datasets for small object detection in remote sensing images are inadequate. Many researchers used scene classification datasets for object detection, which has its limitations; for example, the large-sized objects outnumber the small objects in object categories. Thus, they lack diversity; this further affects the detection performance of small object detectors in RS images. This paper reviews current datasets and object detection methods (deep learning-based) for remote sensing images. We also propose a large-scale, publicly available benchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The RSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of very high resolution (VHR) images with a spatial resolution of ~0.05 m. There are five classes with varying frequencies of labels per class. The image patches are extracted from satellite images, including real image distortions such as tangential scale distortion and skew distortion. We also propose a novel Multi-class Cyclic super-resolution Generative adversarial network with Residual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark image super-resolution-based object detection and compare with the existing state-of-the-art methods based on image super-resolution (SR). The proposed MCGR achieved state-of-the-art performance for image SR with an improvement of 1.2dB PSNR compared to the current state-of-the-art NLSN method. MCGR achieved best object detection mAPs of 0.758, 0.881, 0.841, and 0.983, respectively, for five-class, four-class, two-class, and single classes, respectively surpassing the performance of the state-of-the-art object detectors YOLOv5, EfficientDet, Faster RCNN, SSD, and RetinaNet.

</details>

<details>

<summary>2021-11-05 10:50:21 - FlexiTerm: A more efficient implementation of flexible multi-word term recognition</summary>

- *Irena Spasic*

- `2110.06981v2` - [abs](http://arxiv.org/abs/2110.06981v2) - [pdf](http://arxiv.org/pdf/2110.06981v2)

> Terms are linguistic signifiers of domain-specific concepts. Automated recognition of multi-word terms (MWT) in free text is a sequence labelling problem, which is commonly addressed using supervised machine learning methods. Their need for manual annotation of training data makes it difficult to port such methods across domains. FlexiTerm, on the other hand, is a fully unsupervised method for MWT recognition from domain-specific corpora. Originally implemented in Java as a proof of concept, it did not scale well, thus offering little practical value in the context of big data. In this paper, we describe its re-implementation in Python and compare the performance of these two implementations. The results demonstrated major improvements in terms of efficiency, which allow FlexiTerm to transition from the proof of concept to the production-grade application.

</details>

<details>

<summary>2021-11-07 17:29:16 - Deep Multi-Scale Feature Learning for Defocus Blur Estimation</summary>

- *Ali Karaali, Naomi Harte, Claudio Rosito Jung*

- `2009.11939v2` - [abs](http://arxiv.org/abs/2009.11939v2) - [pdf](http://arxiv.org/pdf/2009.11939v2)

> This paper presents an edge-based defocus blur estimation method from a single defocused image. We first distinguish edges that lie at depth discontinuities (called depth edges, for which the blur estimate is ambiguous) from edges that lie at approximately constant depth regions (called pattern edges, for which the blur estimate is well-defined). Then, we estimate the defocus blur amount at pattern edges only, and explore an interpolation scheme based on guided filters that prevents data propagation across the detected depth edges to obtain a dense blur map with well-defined object boundaries. Both tasks (edge classification and blur estimation) are performed by deep convolutional neural networks (CNNs) that share weights to learn meaningful local features from multi-scale patches centered at edge locations. Experiments on naturally defocused images show that the proposed method presents qualitative and quantitative results that outperform state-of-the-art (SOTA) methods, with a good compromise between running time and accuracy.

</details>

<details>

<summary>2021-11-09 16:46:56 - RTLola on Board: Testing Real Driving Emissions on your Phone</summary>

- *Sebastian Biewer, Bernd Finkbeiner, Holger Hermanns, Maximilian A. Köhl, Yannik Schnitzer, Maximilian Schwenger*

- `2111.05255v1` - [abs](http://arxiv.org/abs/2111.05255v1) - [pdf](http://arxiv.org/pdf/2111.05255v1)

> This paper is about shipping runtime verification to the masses. It presents the crucial technology enabling everyday car owners to monitor the behaviour of their cars in-the-wild. Concretely, we present an Android app that deploys RTLola runtime monitors for the purpose of diagnosing automotive exhaust emissions. For this, it harvests the availability of cheap bluetooth adapters to the On-Board-Diagnostics (OBD) ports, which are ubiquitous in cars nowadays. We detail its use in the context of Real Driving Emissions (RDE) tests and report on sample runs that helped identify violations of the regulatory framework currently valid in the European Union.

</details>

<details>

<summary>2021-11-09 17:15:38 - FILIP: Fine-grained Interactive Language-Image Pre-Training</summary>

- *Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu*

- `2111.07783v1` - [abs](http://arxiv.org/abs/2111.07783v1) - [pdf](http://arxiv.org/pdf/2111.07783v1)

> Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.

</details>

<details>

<summary>2021-11-10 14:47:58 - Towards More Reliable Automated Program Repair by Integrating Static Analysis Techniques</summary>

- *Omar I. Al-Bataineh, Anastasiia Grishina, Leon Moonen*

- `2111.05713v1` - [abs](http://arxiv.org/abs/2111.05713v1) - [pdf](http://arxiv.org/pdf/2111.05713v1)

> A long-standing open challenge for automated program repair is the overfitting problem, which is caused by having insufficient or incomplete specifications to validate whether a generated patch is correct or not. Most available repair systems rely on weak specifications (i.e., specifications that are synthesized from test cases) which limits the quality of generated repairs. To strengthen specifications and improve the quality of repairs, we propose to closer integrate static bug detection techniques with automated program repair. The integration combines automated program repair with static analysis techniques in such a way that bug detection patterns can be synthesized into specifications that the repair system can use. We explore the feasibility of such integration using two types of bugs: arithmetic bugs, such as integer overflow, and logical bugs, such as termination bugs. As part of our analysis, we make several observations that help to improve patch generation for these classes of bugs. Moreover, these observations assist with narrowing down the candidate patch search space, and inferring an effective search order.

</details>

<details>

<summary>2021-11-11 00:34:00 - SyzScope: Revealing High-Risk Security Impacts of Fuzzer-Exposed Bugs in Linux kernel</summary>

- *Xiaochen Zou, Guoren Li, Weiteng Chen, Hang Zhang, Zhiyun Qian*

- `2111.06002v1` - [abs](http://arxiv.org/abs/2111.06002v1) - [pdf](http://arxiv.org/pdf/2111.06002v1)

> Fuzzing has become one of the most effective bug finding approach for software. In recent years, 24*7 continuous fuzzing platforms have emerged to test critical pieces of software, e.g., Linux kernel. Though capable of discovering many bugs and providing reproducers (e.g., proof-of-concepts), a major problem is that they neglect a critical function that should have been built-in, i.e., evaluation of a bug's security impact. It is well-known that the lack of understanding of security impact can lead to delayed bug fixes as well as patch propagation. In this paper, we develop SyzScope, a system that can automatically uncover new "high-risk" impacts given a bug with seemingly "low-risk" impacts. From analyzing over a thousand low-risk bugs on syzbot, SyzScope successfully determined that 183 low-risk bugs (more than 15%) in fact contain high-risk impacts, e.g., control flow hijack and arbitrary memory write, some of which still do not have patches available yet.

</details>

<details>

<summary>2021-11-11 09:27:38 - Implementation of Ethically Aligned Design with Ethical User stories in SMART terminal Digitalization project: Use case Passenger Flow</summary>

- *Erika Halme, Mamia Agbese, Hanna-Kaisa Alanen, Jani Antikainen, Marianna Jantunen, Arif Ali Khan, Kai-Kristian Kemell, Ville Vakkuri, Pekka Abrahamsson*

- `2111.06116v1` - [abs](http://arxiv.org/abs/2111.06116v1) - [pdf](http://arxiv.org/pdf/2111.06116v1)

> Digitalization and Smart systems are part of our everyday lives today. So far the development has been rapid and all the implications that comes after the deployment has not been able to foresee or even assess during the development, especially when ethics or trustworthiness is concerned. Artificial Intelligence (AI) and Autonomous Systems (AS) are the direction that software systems are taking today. It is witnessed in banks, stores, internet and it is proceeding to transportation as well as on traveling. Autonomous maritime industry has also taking this direction when taking under development in digitalization on fairway and port terminals. AI ethics has advanced profoundly since the machine learning develop during the last decade and is now being implemented in AI development and workflow of software engineers. It is not an easy task and tools are needed to make the ethical assessment easier. This paper will review a research in an industrial setting, where Ethically Aligned Design practice, Ethical User Stories are used to transfer ethical requirements to ethical user stories to form practical solutions for project use. This project is in the field of maritime industry and concentrates on digitalization of port terminals and this particular paper focuses on the passenger flow. Results are positive towards the practice of Ethical User Stories, drawn from a large empirical data set.

</details>

<details>

<summary>2021-11-12 10:33:33 - How Well do Feature Visualizations Support Causal Understanding of CNN Activations?</summary>

- *Roland S. Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge, Thomas S. A. Wallis, Wieland Brendel*

- `2106.12447v3` - [abs](http://arxiv.org/abs/2106.12447v3) - [pdf](http://arxiv.org/pdf/2106.12447v3)

> A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These synthetic feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating natural dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task ($68 \pm 4$% accuracy; baseline performance without any visualizations is $60 \pm 3$%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance ($66\pm3$% to $67 \pm3$% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better "causal understanding" of unit activations than simple alternative visualizations.

</details>

<details>

<summary>2021-11-12 20:39:52 - Contrastive Feature Loss for Image Prediction</summary>

- *Alex Andonian, Taesung Park, Bryan Russell, Phillip Isola, Jun-Yan Zhu, Richard Zhang*

- `2111.06934v1` - [abs](http://arxiv.org/abs/2111.06934v1) - [pdf](http://arxiv.org/pdf/2111.06934v1)

> Training supervised image synthesis models requires a critic to compare two images: the ground truth to the result. Yet, this basic functionality remains an open problem. A popular line of approaches uses the L1 (mean absolute error) loss, either in the pixel or the feature space of pretrained deep networks. However, we observe that these losses tend to produce overly blurry and grey images, and other techniques such as GANs need to be employed to fight these artifacts. In this work, we introduce an information theory based approach to measuring similarity between two images. We argue that a good reconstruction should have high mutual information with the ground truth. This view enables learning a lightweight critic to "calibrate" a feature space in a contrastive manner, such that reconstructions of corresponding spatial patches are brought together, while other patches are repulsed. We show that our formulation immediately boosts the perceptual realism of output images when used as a drop-in replacement for the L1 loss, with or without an additional GAN loss.

</details>

<details>

<summary>2021-11-14 04:26:31 - Predicting What You Already Know Helps: Provable Self-Supervised Learning</summary>

- *Jason D. Lee, Qi Lei, Nikunj Saunshi, Jiacheng Zhuo*

- `2008.01064v2` - [abs](http://arxiv.org/abs/2008.01064v2) - [pdf](http://arxiv.org/pdf/2008.01064v2)

> Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks) without requiring labeled data to learn useful semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words in text; yet predicting this \textit{known} information helps in learning representations effective for downstream prediction tasks. We posit a mechanism exploiting the statistical connections between certain {\em reconstruction-based} pretext tasks that guarantee to learn a good representation. Formally, we quantify how the approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task by just training a linear layer on top of the learned representation. We prove the linear layer yields small approximation error even for complex ground truth function class and will drastically reduce labeled sample complexity. Next, we show a simple modification of our method leads to nonlinear CCA, analogous to the popular SimSiam algorithm, and show similar guarantees for nonlinear CCA.

</details>

<details>

<summary>2021-11-15 00:26:19 - Scalable Variational Quantum Circuits for Autoencoder-based Drug Discovery</summary>

- *Junde Li, Swaroop Ghosh*

- `2112.12563v1` - [abs](http://arxiv.org/abs/2112.12563v1) - [pdf](http://arxiv.org/pdf/2112.12563v1)

> The de novo design of drug molecules is recognized as a time-consuming and costly process, and computational approaches have been applied in each stage of the drug discovery pipeline. Variational autoencoder is one of the computer-aided design methods which explores the chemical space based on existing molecular dataset. Quantum machine learning has emerged as an atypical learning method that may speed up some classical learning tasks because of its strong expressive power. However, near-term quantum computers suffer from limited number of qubits which hinders the representation learning in high dimensional spaces. We present a scalable quantum generative autoencoder (SQ-VAE) for simultaneously reconstructing and sampling drug molecules, and a corresponding vanilla variant (SQ-AE) for better reconstruction. The architectural strategies in hybrid quantum classical networks such as, adjustable quantum layer depth, heterogeneous learning rates, and patched quantum circuits are proposed to learn high dimensional dataset such as, ligand-targeted drugs. Extensive experimental results are reported for different dimensions including 8x8 and 32x32 after choosing suitable architectural strategies. The performance of quantum generative autoencoder is compared with the corresponding classical counterpart throughout all experiments. The results show that quantum computing advantages can be achieved for normalized low-dimension molecules, and that high-dimension molecules generated from quantum generative autoencoders have better drug properties within the same learning period.

</details>

<details>

<summary>2021-11-15 12:17:15 - cushLEPOR: customising hLEPOR metric using Optuna for higher agreement with human judgments or pre-trained language model LaBSE</summary>

- *Lifeng Han, Irina Sorokina, Gleb Erofeev, Serge Gladkoff*

- `2108.09484v3` - [abs](http://arxiv.org/abs/2108.09484v3) - [pdf](http://arxiv.org/pdf/2108.09484v3)

> Human evaluation has always been expensive while researchers struggle to trust the automatic metrics. To address this, we propose to customise traditional metrics by taking advantages of the pre-trained language models (PLMs) and the limited available human labelled scores. We first re-introduce the hLEPOR metric factors, followed by the Python version we developed (ported) which achieved the automatic tuning of the weighting parameters in hLEPOR metric. Then we present the customised hLEPOR (cushLEPOR) which uses Optuna hyper-parameter optimisation framework to fine-tune hLEPOR weighting parameters towards better agreement to pre-trained language models (using LaBSE) regarding the exact MT language pairs that cushLEPOR is deployed to. We also optimise cushLEPOR towards professional human evaluation data based on MQM and pSQM framework on English-German and Chinese-English language pairs. The experimental investigations show cushLEPOR boosts hLEPOR performances towards better agreements to PLMs like LaBSE with much lower cost, and better agreements to human evaluations including MQM and pSQM scores, and yields much better performances than BLEU (data available at \url{https://github.com/poethan/cushLEPOR}). Official results show that our submissions win three language pairs including \textbf{English-German} and \textbf{Chinese-English} on \textit{News} domain via cushLEPOR(LM) and \textbf{English-Russian} on \textit{TED} domain via hLEPOR.

</details>

<details>

<summary>2021-11-15 13:40:03 - Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements</summary>

- *Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, Tegawendé F. Bissyandé*

- `2111.07739v1` - [abs](http://arxiv.org/abs/2111.07739v1) - [pdf](http://arxiv.org/pdf/2111.07739v1)

> Software Fault Localization refers to the activity of finding code elements (e.g., statements) that are related to a software failure. The state-of-the-art fault localization techniques, however, produce coarse-grained results that can deter manual debugging or mislead automated repair tools. In this work, we focus specifically on the fine-grained identification of code elements (i.e., tokens) that must be changed to fix a buggy program: we refer to it as fix localization. This paper introduces a neural network architecture (named Beep) that builds on AST paths to predict the buggy code element as well as the change action that must be applied to repair a program. Leveraging massive data of bugs and patches within the CoCoNut dataset, we trained a model that was (1) effective in localizing the buggy tokens with the Mean First Rank significantly higher than a statistics based baseline and a machine learning-based baseline, and (2) effective in predicting the repair operators (with the associated buggy code elements) with a Recall@1= 30-45% and the Mean First Rank=7-12 (evaluated by CoCoNut, ManySStuBs4J, and Defects4J datasets). To showcase how fine-grained fix localization can help program repair, we employ it in two repair pipelines where we use either a code completion engine to predict the correct token or a set of heuristics to search for the suitable donor code. A key strength of accurate fix localization for program repair is that it reduces the chance of patch overfitting, a challenge in generate-and-validate automated program repair: both two repair pipelines achieve a correctness ratio of 100%, i.e., all generated patches are found to be correct. Moreover, accurate fix localization helps enhance the efficiency of program repair.

</details>

<details>

<summary>2021-11-15 17:54:50 - NNoculation: Catching BadNets in the Wild</summary>

- *Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg*

- `2002.08313v2` - [abs](http://arxiv.org/abs/2002.08313v2) - [pdf](http://arxiv.org/pdf/2002.08313v2)

> This paper proposes a novel two-stage defense (NNoculation) against backdoored neural networks (BadNets) that, repairs a BadNet both pre-deployment and online in response to backdoored test inputs encountered in the field. In the pre-deployment stage, NNoculation retrains the BadNet with random perturbations of clean validation inputs to partially reduce the adversarial impact of a backdoor. Post-deployment, NNoculation detects and quarantines backdoored test inputs by recording disagreements between the original and pre-deployment patched networks. A CycleGAN is then trained to learn transformations between clean validation and quarantined inputs; i.e., it learns to add triggers to clean validation images. Backdoored validation images along with their correct labels are used to further retrain the pre-deployment patched network, yielding our final defense. Empirical evaluation on a comprehensive suite of backdoor attacks show that NNoculation outperforms all state-of-the-art defenses that make restrictive assumptions and only work on specific backdoor attacks, or fail on adaptive attacks. In contrast, NNoculation makes minimal assumptions and provides an effective defense, even under settings where existing defenses are ineffective due to attackers circumventing their restrictive assumptions.

</details>

<details>

<summary>2021-11-19 15:18:53 - Diabetic Foot Ulcer Grand Challenge 2021: Evaluation and Summary</summary>

- *Bill Cassidy, Connah Kendrick, Neil D. Reeves, Joseph M. Pappachan, Claire O'Shea, David G. Armstrong, Moi Hoon Yap*

- `2111.10376v1` - [abs](http://arxiv.org/abs/2111.10376v1) - [pdf](http://arxiv.org/pdf/2111.10376v1)

> Diabetic foot ulcer classification systems use the presence of wound infection (bacteria present within the wound) and ischaemia (restricted blood supply) as vital clinical indicators for treatment and prediction of wound healing. Studies investigating the use of automated computerised methods of classifying infection and ischaemia within diabetic foot wounds are limited due to a paucity of publicly available datasets and severe data imbalance in those few that exist. The Diabetic Foot Ulcer Challenge 2021 provided participants with a more substantial dataset comprising a total of 15,683 diabetic foot ulcer patches, with 5,955 used for training, 5,734 used for testing and an additional 3,994 unlabelled patches to promote the development of semi-supervised and weakly-supervised deep learning techniques. This paper provides an evaluation of the methods used in the Diabetic Foot Ulcer Challenge 2021, and summarises the results obtained from each network. The best performing network was an ensemble of the results of the top 3 models, with a macro-average F1-score of 0.6307.

</details>

<details>

<summary>2021-11-20 01:26:16 - SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers</summary>

- *Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing Zhang, Antonio Plaza, Jocelyn Chanussot*

- `2107.02988v2` - [abs](http://arxiv.org/abs/2107.02988v2) - [pdf](http://arxiv.org/pdf/2107.02988v2)

> Hyperspectral (HS) images are characterized by approximately contiguous spectral information, enabling the fine identification of materials by capturing subtle spectral discrepancies. Owing to their excellent locally contextual modeling ability, convolutional neural networks (CNNs) have been proven to be a powerful feature extractor in HS image classification. However, CNNs fail to mine and represent the sequence attributes of spectral signatures well due to the limitations of their inherent network backbone. To solve this issue, we rethink HS image classification from a sequential perspective with transformers, and propose a novel backbone network called \ul{SpectralFormer}. Beyond band-wise representations in classic transformers, SpectralFormer is capable of learning spectrally local sequence information from neighboring bands of HS images, yielding group-wise spectral embeddings. More significantly, to reduce the possibility of losing valuable information in the layer-wise propagation process, we devise a cross-layer skip connection to convey memory-like components from shallow to deep layers by adaptively learning to fuse "soft" residuals across layers. It is worth noting that the proposed SpectralFormer is a highly flexible backbone network, which can be applicable to both pixel- and patch-wise inputs. We evaluate the classification performance of the proposed SpectralFormer on three HS datasets by conducting extensive experiments, showing the superiority over classic transformers and achieving a significant improvement in comparison with state-of-the-art backbone networks. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_SpectralFormer for the sake of reproducibility.

</details>

<details>

<summary>2021-11-21 08:43:15 - Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices</summary>

- *Tao Bai, Jinqi Luo, Jun Zhao*

- `2106.15202v2` - [abs](http://arxiv.org/abs/2106.15202v2) - [pdf](http://arxiv.org/pdf/2106.15202v2)

> Deep learning based image recognition systems have been widely deployed on mobile devices in today's world. In recent studies, however, deep learning models are shown vulnerable to adversarial examples. One variant of adversarial examples, called adversarial patch, draws researchers' attention due to its strong attack abilities. Though adversarial patches achieve high attack success rates, they are easily being detected because of the visual inconsistency between the patches and the original images. Besides, it usually requires a large amount of data for adversarial patch generation in the literature, which is computationally expensive and time-consuming. To tackle these challenges, we propose an approach to generate inconspicuous adversarial patches with one single image. In our approach, we first decide the patch locations basing on the perceptual sensitivity of victim models, then produce adversarial patches in a coarse-to-fine way by utilizing multiple-scale generators and discriminators. The patches are encouraged to be consistent with the background images with adversarial training while preserving strong attack abilities. Our approach shows the strong attack abilities in white-box settings and the excellent transferability in black-box settings through extensive experiments on various models with different architectures and training methods. Compared to other adversarial patches, our adversarial patches hold the most negligible risks to be detected and can evade human observations, which is supported by the illustrations of saliency maps and results of user evaluations. Lastly, we show that our adversarial patches can be applied in the physical world.

</details>

<details>

<summary>2021-11-22 09:17:07 - Unsupervised Action Localization Crop in Video Retargeting for 3D ConvNets</summary>

- *Prithwish Jana, Swarnabja Bhaumik, Partha Pratim Mohanta*

- `2111.07426v2` - [abs](http://arxiv.org/abs/2111.07426v2) - [pdf](http://arxiv.org/pdf/2111.07426v2)

> Untrimmed videos on social media or those captured by robots and surveillance cameras are of varied aspect ratios. However, 3D CNNs usually require as input a square-shaped video, whose spatial dimension is smaller than the original. Random- or center-cropping may leave out the video's subject altogether. To address this, we propose an unsupervised video cropping approach by shaping this as a retargeting and video-to-video synthesis problem. The synthesized video maintains a 1:1 aspect ratio, is smaller in size and is targeted at video-subject(s) throughout the entire duration. First, action localization is performed on each frame by identifying patches with homogeneous motion patterns. Thus, a single salient patch is pinpointed per frame. But to avoid viewpoint jitters and flickering, any inter-frame scale or position changes among the patches should be performed gradually over time. This issue is addressed with a polyBezier fitting in 3D space that passes through some chosen pivot timestamps and whose shape is influenced by the in-between control timestamps. To corroborate the effectiveness of the proposed method, we evaluate the video classification task by comparing our dynamic cropping technique with random cropping on three benchmark datasets, viz. UCF-101, HMDB-51 and ActivityNet v1.3. The clip and top-1 accuracy for video classification after our cropping, outperform 3D CNN performances for same-sized random-crop inputs, also surpassing some larger random-crop sizes.

</details>

<details>

<summary>2021-11-24 20:25:15 - Fast mesh denoising with data driven normal filtering using deep variational autoencoders</summary>

- *Stavros Nousias, Gerasimos Arvanitis, Aris S. Lalos, Konstantinos Moustakas*

- `2111.12782v1` - [abs](http://arxiv.org/abs/2111.12782v1) - [pdf](http://arxiv.org/pdf/2111.12782v1)

> Recent advances in 3D scanning technology have enabled the deployment of 3D models in various industrial applications like digital twins, remote inspection and reverse engineering. Despite their evolving performance, 3D scanners, still introduce noise and artifacts in the acquired dense models. In this work, we propose a fast and robust denoising method for dense 3D scanned industrial models. The proposed approach employs conditional variational autoencoders to effectively filter face normals. Training and inference are performed in a sliding patch setup reducing the size of the required training data and execution times. We conducted extensive evaluation studies using 3D scanned and CAD models. The results verify plausible denoising outcomes, demonstrating similar or higher reconstruction accuracy, compared to other state-of-the-art approaches. Specifically, for 3D models with more than 1e4 faces, the presented pipeline is twice as fast as methods with equivalent reconstruction error.

</details>

<details>

<summary>2021-11-25 18:36:22 - Intriguing Properties of Vision Transformers</summary>

- *Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang*

- `2105.10497v3` - [abs](http://arxiv.org/abs/2105.10497v3) - [pdf](http://arxiv.org/pdf/2105.10497v3)

> Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.

</details>

<details>

<summary>2021-11-26 11:45:08 - Towards Explainable End-to-End Prostate Cancer Relapse Prediction from H&E Images Combining Self-Attention Multiple Instance Learning with a Recurrent Neural Network</summary>

- *Esther Dietrich, Patrick Fuhlert, Anne Ernst, Guido Sauter, Maximilian Lennartz, H. Siegfried Stiehl, Marina Zimmermann, Stefan Bonn*

- `2111.13439v1` - [abs](http://arxiv.org/abs/2111.13439v1) - [pdf](http://arxiv.org/pdf/2111.13439v1)

> Clinical decision support for histopathology image data mainly focuses on strongly supervised annotations, which offers intuitive interpretability, but is bound by expert performance. Here, we propose an explainable cancer relapse prediction network (eCaReNet) and show that end-to-end learning without strong annotations offers state-of-the-art performance while interpretability can be included through an attention mechanism. On the use case of prostate cancer survival prediction, using 14,479 images and only relapse times as annotations, we reach a cumulative dynamic AUC of 0.78 on a validation set, being on par with an expert pathologist (and an AUC of 0.77 on a separate test set). Our model is well-calibrated and outputs survival curves as well as a risk score and group per patient. Making use of the attention weights of a multiple instance learning layer, we show that malignant patches have a higher influence on the prediction than benign patches, thus offering an intuitive interpretation of the prediction. Our code is available at www.github.com/imsb-uke/ecarenet.

</details>

<details>

<summary>2021-11-28 07:41:40 - A Comprehensive and Cross-Platform Test Suite for Memory Safety -- Towards an Open Framework for Testing Processor Hardware Supported Security Extensions</summary>

- *Wei Song, Jiameng Ying, Sihao Shen, Boya Li, Hao Ma, Peng Liu*

- `2111.14072v1` - [abs](http://arxiv.org/abs/2111.14072v1) - [pdf](http://arxiv.org/pdf/2111.14072v1)

> Memory safety remains a critical and widely violated property in reality. Numerous defense techniques have been proposed and developed but most of them are not applied or enabled by default in production-ready environment due to their substantial running cost. The situation might change in the near future because the hardware supported defenses against these attacks are finally beginning to be adopted by commercial processors, operating systems and compilers. We then face a question as there is currently no suitable test suite to measure the memory safety extensions supported on different processors. In fact, the issue is not constrained only for memory safety but all aspect of processor security. All of the existing test suites related to processor security lack some of the key properties, such as comprehensiveness, distinguishability and portability.   As an initial step, we propose an expandable test framework for measuring the processor security and open source a memory safety test suite utilizing this framework. The framework is deliberately designed to be flexible so it can be gradually extended to all types of hardware supported security extensions in processors. The initial test suite for memory safety currently contains 160 test cases covering spatial and temporal safety of memory, memory access control, pointer integrity and control-flow integrity. Each type of vulnerabilities and their related defenses have been individually evaluated by one or more test cases. The test suite has been ported to three different instruction set architectures (ISAs) and experimented on six different platforms. We have also utilized the test suite to explore the security benefits of applying different sets of compiler flags available on the latest GNU GCC and LLVM compilers.

</details>

<details>

<summary>2021-11-30 03:51:24 - RawArray: A Simple, Fast, and Extensible Archival Format for Numeric Data</summary>

- *David S. Smith*

- `2112.01273v1` - [abs](http://arxiv.org/abs/2112.01273v1) - [pdf](http://arxiv.org/pdf/2112.01273v1)

> Raw data sizes are growing and proliferating in scientific research, driven by the success of data-hungry computational methods, such as machine learning. The preponderance of proprietary and shoehorned data formats make computations slower and make it harder to reproduce research and to port methods to new platforms. Here we present the RawArray format: a simple, fast, and extensible format for archival storage of multidimensional numeric arrays on disk.   The RawArray file format is a simple concatenation of a header array and a data array. The header comprises seven or more 64-bit unsigned integers. The array data can be anything. Arbitrary user metadata can be appended to an RawArray file if desired, for example to store measurement details, color palettes, or geolocation data.   We present benchmarks showing a factor of 2--3$\times$ speedup over HDF5 for a range of array sizes and a speedup of up to 20$\times$ in reading the common deep learning datasets MNIST and CIFAR10.

</details>

<details>

<summary>2021-11-30 07:49:28 - SamplingAug: On the Importance of Patch Sampling Augmentation for Single Image Super-Resolution</summary>

- *Shizun Wang, Ming Lu, Kaixin Chen, Jiaming Liu, Xiaoqi Li, Chuang zhang, Ming Wu*

- `2111.15185v1` - [abs](http://arxiv.org/abs/2111.15185v1) - [pdf](http://arxiv.org/pdf/2111.15185v1)

> With the development of Deep Neural Networks (DNNs), plenty of methods based on DNNs have been proposed for Single Image Super-Resolution (SISR). However, existing methods mostly train the DNNs on uniformly sampled LR-HR patch pairs, which makes them fail to fully exploit informative patches within the image. In this paper, we present a simple yet effective data augmentation method. We first devise a heuristic metric to evaluate the informative importance of each patch pair. In order to reduce the computational cost for all patch pairs, we further propose to optimize the calculation of our metric by integral image, achieving about two orders of magnitude speedup. The training patch pairs are sampled according to their informative importance with our method. Extensive experiments show our sampling augmentation can consistently improve the convergence and boost the performance of various SISR architectures, including EDSR, RCAN, RDN, SRCNN and ESPCN across different scaling factors (x2, x3, x4). Code is available at https://github.com/littlepure2333/SamplingAug

</details>

<details>

<summary>2021-11-30 17:37:01 - MapReader: A Computer Vision Pipeline for the Semantic Exploration of Maps at Scale</summary>

- *Kasra Hosseini, Daniel C. S. Wilson, Kaspar Beelen, Katherine McDonough*

- `2111.15592v1` - [abs](http://arxiv.org/abs/2111.15592v1) - [pdf](http://arxiv.org/pdf/2111.15592v1)

> We present MapReader, a free, open-source software library written in Python for analyzing large map collections (scanned or born-digital). This library transforms the way historians can use maps by turning extensive, homogeneous map sets into searchable primary sources. MapReader allows users with little or no computer vision expertise to i) retrieve maps via web-servers; ii) preprocess and divide them into patches; iii) annotate patches; iv) train, fine-tune, and evaluate deep neural network models; and v) create structured data about map content. We demonstrate how MapReader enables historians to interpret a collection of $\approx$16K nineteenth-century Ordnance Survey map sheets ($\approx$30.5M patches), foregrounding the challenge of translating visual markers into machine-readable data. We present a case study focusing on British rail infrastructure and buildings as depicted on these maps. We also show how the outputs from the MapReader pipeline can be linked to other, external datasets, which we use to evaluate as well as enrich and interpret the results. We release $\approx$62K manually annotated patches used here for training and evaluating the models.

</details>


## 2021-12

<details>

<summary>2021-12-01 18:59:46 - RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</summary>

- *Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan*

- `2112.00724v1` - [abs](http://arxiv.org/abs/2112.00724v1) - [pdf](http://arxiv.org/pdf/2112.00724v1)

> Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.

</details>

<details>

<summary>2021-12-05 06:35:12 - Face Trees for Expression Recognition</summary>

- *Mojtaba Kolahdouzi, Alireza Sepas-Moghaddam, Ali Etemad*

- `2112.02487v1` - [abs](http://arxiv.org/abs/2112.02487v1) - [pdf](http://arxiv.org/pdf/2112.02487v1)

> We propose an end-to-end architecture for facial expression recognition. Our model learns an optimal tree topology for facial landmarks, whose traversal generates a sequence from which we obtain an embedding to feed a sequential learner. The proposed architecture incorporates two main streams, one focusing on landmark positions to learn the structure of the face, while the other focuses on patches around the landmarks to learn texture information. Each stream is followed by an attention mechanism and the outputs are fed to a two-stream fusion component to perform the final classification. We conduct extensive experiments on two large-scale publicly available facial expression datasets, AffectNet and FER2013, to evaluate the efficacy of our approach. Our method outperforms other solutions in the area and sets new state-of-the-art expression recognition rates on these datasets.

</details>

<details>

<summary>2021-12-07 02:31:29 - Hybrid guiding: A multi-resolution refinement approach for semantic segmentation of gigapixel histopathological images</summary>

- *André Pedersen, Erik Smistad, Tor V. Rise, Vibeke G. Dale, Henrik S. Pettersen, Tor-Arne S. Nordmo, David Bouget, Ingerid Reinertsen, Marit Valla*

- `2112.03455v1` - [abs](http://arxiv.org/abs/2112.03455v1) - [pdf](http://arxiv.org/pdf/2112.03455v1)

> Histopathological cancer diagnostics has become more complex, and the increasing number of biopsies is a challenge for most pathology laboratories. Thus, development of automatic methods for evaluation of histopathological cancer sections would be of value. In this study, we used 624 whole slide images (WSIs) of breast cancer from a Norwegian cohort. We propose a cascaded convolutional neural network design, called H2G-Net, for semantic segmentation of gigapixel histopathological images. The design involves a detection stage using a patch-wise method, and a refinement stage using a convolutional autoencoder. To validate the design, we conducted an ablation study to assess the impact of selected components in the pipeline on tumour segmentation. Guiding segmentation, using hierarchical sampling and deep heatmap refinement, proved to be beneficial when segmenting the histopathological images. We found a significant improvement when using a refinement network for postprocessing the generated tumour segmentation heatmaps. The overall best design achieved a Dice score of 0.933 on an independent test set of 90 WSIs. The design outperformed single-resolution approaches, such as cluster-guided, patch-wise high-resolution classification using MobileNetV2 (0.872) and a low-resolution U-Net (0.874). In addition, segmentation on a representative x400 WSI took ~58 seconds, using only the CPU. The findings demonstrate the potential of utilizing a refinement network to improve patch-wise predictions. The solution is efficient and does not require overlapping patch inference or ensembling. Furthermore, we showed that deep neural networks can be trained using a random sampling scheme that balances on multiple different labels simultaneously, without the need of storing patches on disk. Future work should involve more efficient patch generation and sampling, as well as improved clustering.

</details>

<details>

<summary>2021-12-07 08:47:06 - Comparison of Neural Network based Soft Computing Techniques for Electromagnetic Modeling of a Microstrip Patch Antenna</summary>

- *Yuvraj Singh Malhi, Navneet Gupta*

- `2109.10065v2` - [abs](http://arxiv.org/abs/2109.10065v2) - [pdf](http://arxiv.org/pdf/2109.10065v2)

> This paper presents the comparison of various neural networks and algorithms based on accuracy, quickness, and consistency for antenna modelling. Using MATLAB Nntool, 22 different combinations of networks and training algorithms are used to predict the dimensions of a rectangular microstrip antenna using dielectric constant, height of substrate, and frequency of oper-ation as input. Comparison and characterization of networks is done based on accuracy, mean square error, and training time. Algorithms, on the other hand, are analyzed by their accuracy, speed, reliability, and smoothness in the training process. Finally, these results are analyzed, and recommendations are made for each neural network and algorithm based on uses, advantages, and disadvantages. For example, it is observed that Reduced Radial Bias network is the most accurate network and Scaled Conjugate Gradient is the most reliable algorithm for electromagnetic modelling. This paper will help a researcher find the optimum network and algorithm directly without doing time-taking experimentation.

</details>

<details>

<summary>2021-12-09 06:57:24 - CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification</summary>

- *Huidong Liu, Shaoyuan Xu, Jinmiao Fu, Yang Liu, Ning Xie, Chien-Chih Wang, Bryan Wang, Yi Sun*

- `2112.03562v2` - [abs](http://arxiv.org/abs/2112.03562v2) - [pdf](http://arxiv.org/pdf/2112.03562v2)

> Modern Web systems such as social media and e-commerce contain rich contents expressed in images and text. Leveraging information from multi-modalities can improve the performance of machine learning tasks such as classification and recommendation. In this paper, we propose the Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which unifies two types of cross-modality attentions, sequence-wise attention and modality-wise attention, to effectively fuse information from image and text pairs. The sequence-wise attention enables the framework to capture the fine-grained relationship between image patches and text tokens, while the modality-wise attention weighs each modality by its relevance to the downstream tasks. In addition, by adding task specific modality-wise attentions and multilayer perceptrons, our proposed framework is capable of performing multi-task classification with multi-modalities.   We conduct experiments on a Major Retail Website Product Attribute (MRWPA) dataset and two public datasets, Food101 and Fashion-Gen. The results show that CMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA dataset for multi-task classification. It also surpasses the state-of-the-art method on Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on Food101 Dataset. Through detailed ablation studies, we further demonstrate the effectiveness of both cross-modality attention modules and our method's robustness against noise in image and text inputs, which is a common challenge in practice.

</details>

<details>

<summary>2021-12-09 13:38:39 - Towards a Secure and Reliable IT-Ecosystem in Seaports</summary>

- *Tobias Brandt, Dieter Hutter, Christian Maeder, Rainer Müller*

- `2111.13436v2` - [abs](http://arxiv.org/abs/2111.13436v2) - [pdf](http://arxiv.org/pdf/2111.13436v2)

> Digitalization in seaports dovetails the IT infrastructure of various actors (e.g., shipping companies, terminals, customs, port authorities) to process complex workflows for shipping containers. The security of these workflows relies not only on the security of each individual actor but actors must also provide additional guarantees to other actors like, for instance, respecting obligations related to received data or checking the integrity of workflows observed so far. This paper analyses global security requirements (e.g., accountability, confidentiality) of the workflows and decomposes them - according to the way workflow data is stored and distributed - into requirements and obligations for the individual actors. Security mechanisms are presented to satisfy the resulting requirements, which together with the guarantees of all individual actors will guarantee the security of the overall workflow.

</details>

<details>

<summary>2021-12-11 07:56:48 - A Fragile Points Method, with an interface debonding model, to simulate damage and fracture of U-notched structures</summary>

- *Kailei Wang, Baoying Shen, Mingjing Li, Leiting Dong, Satya N. Atluri*

- `2108.12737v2` - [abs](http://arxiv.org/abs/2108.12737v2) - [pdf](http://arxiv.org/pdf/2108.12737v2)

> Notched components are commonly used in engineering structures, where stress concentration may easily lead to crack initiation and development. The main goal of this work is to develop a simple numerical method to predict the structural strength and crack-growth-path of U-notched specimens made of brittle materials. For this purpose, the Fragile Points Method (FPM), as previously proposed by the authors, has been augmented by an interface debonding model at the interfaces of the FPM domains, to simulate crack initiation and development. The formulations of FPM are based on a discontinuous Galerkin weak form where point-based piece-wise-continuous polynomial test and trial functions are used instead of element-based basis functions. In this work, the numerical fluxes introduced across interior interfaces between subdomains are postulated as the tractions acting on the interface derived from an interface damage model. The interface damage is triggered when the numerical flux reaches the interface strength, and the process of crack-surface separation is governed by the fracture energy. In this way, arbitrary crack initiation and propagation can be naturally simulated without the need for knowing the fracture-patch before-hand. Additionally, a small penalty parameter is sufficient to enforce the weak-form continuity condition before damage initiation, without causing problems such as artificial compliance and numerical ill-conditioning. As validations, the proposed FPM method with the interface damage model is used to predict the structural strength and crack-development from U-notched structures made of brittle materials, which is useful but challenging in engineering structural design practices.

</details>

<details>

<summary>2021-12-11 21:03:20 - Multi-Attention Multiple Instance Learning</summary>

- *Andrei V. Konstantinov, Lev V. Utkin*

- `2112.06071v1` - [abs](http://arxiv.org/abs/2112.06071v1) - [pdf](http://arxiv.org/pdf/2112.06071v1)

> A new multi-attention based method for solving the MIL problem (MAMIL), which takes into account the neighboring patches or instances of each analyzed patch in a bag, is proposed. In the method, one of the attention modules takes into account adjacent patches or instances, several attention modules are used to get a diverse feature representation of patches, and one attention module is used to unite different feature representations to provide an accurate classification of each patch (instance) and the whole bag. Due to MAMIL, a combined representation of patches and their neighbors in the form of embeddings of a small dimensionality for simple classification is realized. Moreover, different types of patches are efficiently processed, and a diverse feature representation of patches in a bag by using several attention modules is implemented. A simple approach for explaining the classification predictions of patches is proposed. Numerical experiments with various datasets illustrate the proposed method.

</details>

<details>

<summary>2021-12-12 02:33:39 - FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack</summary>

- *Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Xiaoya Zhang, Zhiqiang Gong, Wen Yao, Xiaoqian Chen*

- `2109.07193v3` - [abs](http://arxiv.org/abs/2109.07193v3) - [pdf](http://arxiv.org/pdf/2109.07193v3)

> Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle's surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors. The code of FCA will be available at: https://idrl-lab.github.io/Full-coverage-camouflage-adversarial-attack/.

</details>

<details>

<summary>2021-12-12 14:31:50 - Boosting the Capability of Intelligent Vulnerability Detection by Training in a Human-Learning Manner</summary>

- *Shihan Dou, Yueming Wu, Wenxuan Li, Feng Cheng, Wei Yang, Yang Liu*

- `2112.06250v1` - [abs](http://arxiv.org/abs/2112.06250v1) - [pdf](http://arxiv.org/pdf/2112.06250v1)

> Due to its powerful automatic feature extraction, deep learning (DL) has been widely used in source code vulnerability detection. However, although it performs well on artificial datasets, its performance is not satisfactory when detecting real-world vulnerabilities due to the high complexity of real-world samples. In this paper, we propose to train DL-based vulnerability detection models in a human-learning manner, that is, start with the simplest samples and then gradually transition to difficult knowledge. Specifically, we design a novel framework (Humer) that can enhance the detection ability of DL-based vulnerability detectors. To validate the effectiveness of Humer, we select five state-of-the-art DL-based vulnerability detection models (TokenCNN, VulDeePecker, StatementGRU, ASTGRU, and Devign) to complete our evaluations. Through the results, we find that the use of Humer can increase the F1 of these models by an average of 10.5%. Moreover, Humer can make the model detect up to 16.7% more real-world vulnerabilities. Meanwhile, we also conduct a case study to uncover vulnerabilities from real-world open source products by using these enhanced DL-based vulnerability detectors. Through the results, we finally discover 281 unreported vulnerabilities in NVD, of which 98 have been silently patched by vendors in the latest version of corresponding products, but 159 still exist in the products.

</details>

<details>

<summary>2021-12-13 07:16:55 - A Novel Model for Vulnerability Analysis through Enhanced Directed Graphs and Quantitative Metrics</summary>

- *Ángel Longueira-Romero, Rosa Iglesias, Jose Luis Flores, Iñaki Garitano*

- `2112.06453v1` - [abs](http://arxiv.org/abs/2112.06453v1) - [pdf](http://arxiv.org/pdf/2112.06453v1)

> Industrial components are of high importance because they control critical infrastructures that form the lifeline of modern societies. However, the rapid evolution of industrial components, together with the new paradigm of Industry 4.0, and the new connectivity features that will be introduced by the 5G technology, all increase the likelihood of security incidents. These incidents are caused by the vulnerabilities present in these devices. In addition, although international standards define tasks to assess vulnerabilities, they do not specify any particular method. Having a secure design is important, but is also complex, costly, and an extra factor to manage during the lifespan of the device. This paper presents a model to analyze the known vulnerabilities of industrial components over time. The proposed model is based on two main elements: a directed graph representation of the internal structure of the component, and a set of quantitative metrics that are based on international security standards; such as, the Common Vulnerability Scoring System (CVSS). This model is applied throughout the entire lifespan of a device to track vulnerabilities, identify new requirements, root causes, and test cases. The proposed model also helps to prioritize patching activities. To test its potential, the proposed model is applied to the OpenPLC project. The results show that most of the root causes of these vulnerabilities are related to memory buffer operations and are concentrated in the \textit{libssl} library. Consequently, new requirements and test cases were generated from the obtained data.

</details>

<details>

<summary>2021-12-14 07:36:20 - ACE-BERT: Adversarial Cross-modal Enhanced BERT for E-commerce Retrieval</summary>

- *Boxuan Zhang, Chao Wei, Yan Jin, Weiru Zhang*

- `2112.07209v1` - [abs](http://arxiv.org/abs/2112.07209v1) - [pdf](http://arxiv.org/pdf/2112.07209v1)

> Nowadays on E-commerce platforms, products are presented to the customers with multiple modalities. These multiple modalities are significant for a retrieval system while providing attracted products for customers. Therefore, how to take into account those multiple modalities simultaneously to boost the retrieval performance is crucial. This problem is a huge challenge to us due to the following reasons: (1) the way of extracting patch features with the pre-trained image model (e.g., CNN-based model) has much inductive bias. It is difficult to capture the efficient information from the product image in E-commerce. (2) The heterogeneity of multimodal data makes it challenging to construct the representations of query text and product including title and image in a common subspace. We propose a novel Adversarial Cross-modal Enhanced BERT (ACE-BERT) for efficient E-commerce retrieval. In detail, ACE-BERT leverages the patch features and pixel features as image representation. Thus the Transformer architecture can be applied directly to the raw image sequences. With the pre-trained enhanced BERT as the backbone network, ACE-BERT further adopts adversarial learning by adding a domain classifier to ensure the distribution consistency of different modality representations for the purpose of narrowing down the representation gap between query and product. Experimental results demonstrate that ACE-BERT outperforms the state-of-the-art approaches on the retrieval task. It is remarkable that ACE-BERT has already been deployed in our E-commerce's search engine, leading to 1.46% increase in revenue.

</details>

<details>

<summary>2021-12-14 20:11:28 - OpenSSLNTRU: Faster post-quantum TLS key exchange</summary>

- *Daniel J. Bernstein, Billy Bob Brumley, Ming-Shing Chen, Nicola Tuveri*

- `2106.08759v3` - [abs](http://arxiv.org/abs/2106.08759v3) - [pdf](http://arxiv.org/pdf/2106.08759v3)

> Google's CECPQ1 experiment in 2016 integrated a post-quantum key-exchange algorithm, newhope1024, into TLS 1.2. The Google-Cloudflare CECPQ2 experiment in 2019 integrated a more efficient key-exchange algorithm, ntruhrss701, into TLS 1.3.   This paper revisits the choices made in CECPQ2, and shows how to achieve higher performance for post-quantum key exchange in TLS 1.3 using a higher-security algorithm, sntrup761. Previous work had indicated that ntruhrss701 key generation was much faster than sntrup761 key generation, but this paper makes sntrup761 key generation much faster by generating a batch of keys at once.   Batch key generation is invisible at the TLS protocol layer, but raises software-engineering questions regarding the difficulty of integrating batch key exchange into existing TLS libraries and applications. This paper shows that careful choices of software layers make it easy to integrate fast post-quantum software, including batch key exchange, into TLS with minor changes to TLS libraries and no changes to applications.   As a demonstration of feasibility, this paper reports successful integration of its fast sntrup761 library, via a lightly patched OpenSSL, into an unmodified web browser and an unmodified TLS terminator. This paper also reports TLS 1.3 handshake benchmarks, achieving more TLS 1.3 handshakes per second than any software included in OpenSSL.

</details>

<details>

<summary>2021-12-15 15:25:08 - A Comprehensive Study of Code-removal Patches in Automated Program Repair</summary>

- *Davide Ginelli, Matias Martinez, Leonardo Mariani, Martin Monperrus*

- `2012.06264v3` - [abs](http://arxiv.org/abs/2012.06264v3) - [pdf](http://arxiv.org/pdf/2012.06264v3)

> Automatic Program Repair (APR) techniques can promisingly help reducing the cost of debugging. Many relevant APR techniques follow the generate-and-validate approach, that is, the faulty program is iteratively modified with different change operators and then validated with a test suite until a plausible patch is generated. In particular, Kali is a generate-and-validate technique developed to investigate the possibility of generating plausible patches by only removing code. Former studies show that indeed Kali successfully addressed several faults. This paper addresses the case of code-removal patches in automated program repair investigating the reasons and the scenarios that make their creation possible, and the relationship with patches implemented by developers. Our study reveals that code-removal patches are often insufficient to fix bugs, and proposes a comprehensive taxonomy of code-removal patches that provides evidence of the problems that may affect test suites, opening new opportunities for researchers in the field of automatic program repair.

</details>

<details>

<summary>2021-12-16 11:27:48 - Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing</summary>

- *Joonhyung Park, June Yong Yang, Jinwoo Shin, Sung Ju Hwang, Eunho Yang*

- `2112.08796v1` - [abs](http://arxiv.org/abs/2112.08796v1) - [pdf](http://arxiv.org/pdf/2112.08796v1)

> The Mixup scheme suggests mixing a pair of samples to create an augmented training sample and has gained considerable attention recently for improving the generalizability of neural networks. A straightforward and widely used extension of Mixup is to combine with regional dropout-like methods: removing random patches from a sample and replacing it with the features from another sample. Albeit their simplicity and effectiveness, these methods are prone to create harmful samples due to their randomness. To address this issue, 'maximum saliency' strategies were recently proposed: they select only the most informative features to prevent such a phenomenon. However, they now suffer from lack of sample diversification as they always deterministically select regions with maximum saliency, injecting bias into the augmented data. In this paper, we present, a novel, yet simple Mixup-variant that captures the best of both worlds. Our idea is two-fold. By stochastically sampling the features and 'grafting' them onto another sample, our method effectively generates diverse yet meaningful samples. Its second ingredient is to produce the label of the grafted sample by mixing the labels in a saliency-calibrated fashion, which rectifies supervision misguidance introduced by the random sampling procedure. Our experiments under CIFAR, Tiny-ImageNet, and ImageNet datasets show that our scheme outperforms the current state-of-the-art augmentation strategies not only in terms of classification accuracy, but is also superior in coping under stress conditions such as data corruption and object occlusion.

</details>

<details>

<summary>2021-12-16 17:37:11 - Distributed neural network control with dependability guarantees: a compositional port-Hamiltonian approach</summary>

- *Luca Furieri, Clara Lucía Galimberti, Muhammad Zakwan, Giancarlo Ferrari-Trecate*

- `2112.09046v1` - [abs](http://arxiv.org/abs/2112.09046v1) - [pdf](http://arxiv.org/pdf/2112.09046v1)

> Large-scale cyber-physical systems require that control policies are distributed, that is, that they only rely on local real-time measurements and communication with neighboring agents. Optimal Distributed Control (ODC) problems are, however, highly intractable even in seemingly simple cases. Recent work has thus proposed training Neural Network (NN) distributed controllers. A main challenge of NN controllers is that they are not dependable during and after training, that is, the closed-loop system may be unstable, and the training may fail due to vanishing and exploding gradients. In this paper, we address these issues for networks of nonlinear port-Hamiltonian (pH) systems, whose modeling power ranges from energy systems to non-holonomic vehicles and chemical reactions. Specifically, we embrace the compositional properties of pH systems to characterize deep Hamiltonian control policies with built-in closed-loop stability guarantees, irrespective of the interconnection topology and the chosen NN parameters. Furthermore, our setup enables leveraging recent results on well-behaved neural ODEs to prevent the phenomenon of vanishing gradients by design. Numerical experiments corroborate the dependability of the proposed architecture, while matching the performance of general neural network policies.

</details>

<details>

<summary>2021-12-16 21:31:18 - Hierarchical Semantic Segmentation using Psychometric Learning</summary>

- *Lu Yin, Vlado Menkovski, Shiwei Liu, Mykola Pechenizkiy*

- `2107.03212v2` - [abs](http://arxiv.org/abs/2107.03212v2) - [pdf](http://arxiv.org/pdf/2107.03212v2)

> Assigning meaning to parts of image data is the goal of semantic image segmentation. Machine learning methods, specifically supervised learning is commonly used in a variety of tasks formulated as semantic segmentation. One of the major challenges in the supervised learning approaches is expressing and collecting the rich knowledge that experts have with respect to the meaning present in the image data. Towards this, typically a fixed set of labels is specified and experts are tasked with annotating the pixels, patches or segments in the images with the given labels. In general, however, the set of classes does not fully capture the rich semantic information present in the images. For example, in medical imaging such as histology images, the different parts of cells could be grouped and sub-grouped based on the expertise of the pathologist.   To achieve such a precise semantic representation of the concepts in the image, we need access to the full depth of knowledge of the annotator. In this work, we develop a novel approach to collect segmentation annotations from experts based on psychometric testing. Our method consists of the psychometric testing procedure, active query selection, query enhancement, and a deep metric learning model to achieve a patch-level image embedding that allows for semantic segmentation of images. We show the merits of our method with evaluation on the synthetically generated image, aerial image and histology image.

</details>

<details>

<summary>2021-12-17 16:51:00 - NILC-Metrix: assessing the complexity of written and spoken language in Brazilian Portuguese</summary>

- *Sidney Evaldo Leal, Magali Sanches Duran, Carolina Evaristo Scarton, Nathan Siegle Hartmann, Sandra Maria Aluísio*

- `2201.03445v1` - [abs](http://arxiv.org/abs/2201.03445v1) - [pdf](http://arxiv.org/pdf/2201.03445v1)

> This paper presents and makes publicly available the NILC-Metrix, a computational system comprising 200 metrics proposed in studies on discourse, psycholinguistics, cognitive and computational linguistics, to assess textual complexity in Brazilian Portuguese (BP). These metrics are relevant for descriptive analysis and the creation of computational models and can be used to extract information from various linguistic levels of written and spoken language. The metrics in NILC-Metrix were developed during the last 13 years, starting in 2008 with Coh-Metrix-Port, a tool developed within the scope of the PorSimples project. Coh-Metrix-Port adapted some metrics to BP from the Coh-Metrix tool that computes metrics related to cohesion and coherence of texts in English. After the end of PorSimples in 2010, new metrics were added to the initial 48 metrics of Coh-Metrix-Port. Given the large number of metrics, we present them following an organisation similar to the metrics of Coh-Metrix v3.0 to facilitate comparisons made with metrics in Portuguese and English. In this paper, we illustrate the potential of NILC-Metrix by presenting three applications: (i) a descriptive analysis of the differences between children's film subtitles and texts written for Elementary School I and II (Final Years); (ii) a new predictor of textual complexity for the corpus of original and simplified texts of the PorSimples project; (iii) a complexity prediction model for school grades, using transcripts of children's story narratives told by teenagers. For each application, we evaluate which groups of metrics are more discriminative, showing their contribution for each task.

</details>

<details>

<summary>2021-12-18 03:42:23 - Face Deblurring Based on Separable Normalization and Adaptive Denormalization</summary>

- *Xian Zhang, Hao Zhang, Jiancheng Lv, Xiaojie Li*

- `2112.09833v1` - [abs](http://arxiv.org/abs/2112.09833v1) - [pdf](http://arxiv.org/pdf/2112.09833v1)

> Face deblurring aims to restore a clear face image from a blurred input image with more explicit structure and facial details. However, most conventional image and face deblurring methods focus on the whole generated image resolution without consideration of special face part texture and generally produce unsufficient details. Considering that faces and backgrounds have different distribution information, in this study, we designed an effective face deblurring network based on separable normalization and adaptive denormalization (SNADNet). First, We fine-tuned the face parsing network to obtain an accurate face structure. Then, we divided the face parsing feature into face foreground and background. Moreover, we constructed a new feature adaptive denormalization to regularize fafcial structures as a condition of the auxiliary to generate more harmonious and undistorted face structure. In addition, we proposed a texture extractor and multi-patch discriminator to enhance the generated facial texture information. Experimental results on both CelebA and CelebA-HQ datasets demonstrate that the proposed face deblurring network restores face structure with more facial details and performs favorably against state-of-the-art methods in terms of structured similarity indexing method (SSIM), peak signal-to-noise ratio (PSNR), Frechet inception distance (FID) and L1, and qualitative comparisons.

</details>

<details>

<summary>2021-12-19 11:30:29 - Early Detection of Security-Relevant Bug Reports using Machine Learning: How Far Are We?</summary>

- *Arthur D. Sawadogo, Quentin Guimard, Tegawendé F. Bissyandé, Abdoul Kader Kaboré, Jacques Klein, Naouel Moha*

- `2112.10123v1` - [abs](http://arxiv.org/abs/2112.10123v1) - [pdf](http://arxiv.org/pdf/2112.10123v1)

> Bug reports are common artefacts in software development. They serve as the main channel for users to communicate to developers information about the issues that they encounter when using released versions of software programs. In the descriptions of issues, however, a user may, intentionally or not, expose a vulnerability. In a typical maintenance scenario, such security-relevant bug reports are prioritised by the development team when preparing corrective patches. Nevertheless, when security relevance is not immediately expressed (e.g., via a tag) or rapidly identified by triaging teams, the open security-relevant bug report can become a critical leak of sensitive information that attackers can leverage to perform zero-day attacks. To support practitioners in triaging bug reports, the research community has proposed a number of approaches for the detection of security-relevant bug reports. In recent years, approaches in this respect based on machine learning have been reported with promising performance. Our work focuses on such approaches, and revisits their building blocks to provide a comprehensive view on the current achievements. To that end, we built a large experimental dataset and performed extensive experiments with variations in feature sets and learning algorithms. Eventually, our study highlights different approach configurations that yield best performing classifiers.

</details>

<details>

<summary>2021-12-20 08:23:43 - Evaluation and Comparison of Deep Learning Methods for Pavement Crack Identification with Visual Images</summary>

- *Kai-Liang Lu*

- `2112.10390v1` - [abs](http://arxiv.org/abs/2112.10390v1) - [pdf](http://arxiv.org/pdf/2112.10390v1)

> Compared with contact detection techniques, pavement crack identification with visual images via deep learning algorithms has the advantages of not being limited by the material of object to be detected, fast speed and low cost. The fundamental frameworks and typical model architectures of transfer learning (TL), encoder-decoder (ED), generative adversarial networks (GAN), and their common modules were first reviewed, and then the evolution of convolutional neural network (CNN) backbone models and GAN models were summarized. The crack classification, segmentation performance, and effect were tested on the SDNET2018 and CFD public data sets. In the aspect of patch sample classification, the fine-tuned TL models can be equivalent to or even slightly better than the ED models in accuracy, and the predicting time is faster; In the aspect of accurate crack location, both ED and GAN algorithms can achieve pixel-level segmentation and is expected to be detected in real time on low computing power platform. Furthermore, a weakly supervised learning framework of combined TL-SSGAN and its performance enhancement measures are proposed, which can maintain comparable crack identification performance with that of the supervised learning, while greatly reducing the number of labeled samples required.

</details>

<details>

<summary>2021-12-21 00:14:57 - Elixir: Effective object-oriented program repair</summary>

- *Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, Mukul R. Prasad*

- `2112.10915v1` - [abs](http://arxiv.org/abs/2112.10915v1) - [pdf](http://arxiv.org/pdf/2112.10915v1)

> This work is motivated by the pervasive use of method invocations in object-oriented (OO) programs, and indeed their prevalence in patches of OO-program bugs. We propose a generate-and-validate repair technique, called ELIXIR designed to be able to generate such patches. ELIXIR aggressively uses method calls, on par with local variables, fields, or constants, to construct more expressive repair-expressions, that go into synthesizing patches. The ensuing enlargement of the repair space, on account of the wider use of method calls, is effectively tackled by using a machine-learnt model to rank concrete repairs. The machine-learnt model relies on four features derived from the program context, i.e., the code surrounding the potential repair location, and the bug report. We implement ELIXIR and evaluate it on two datasets, the popular Defects4J dataset and a new dataset Bugs.jar created by us, and against 2 baseline versions of our technique, and 5 other techniques representing the state of the art in program repair. Our evaluation shows that ELIXIR is able to increase the number of correctly repaired bugs in Defects4J by 85% (from 14 to 26) and by 57% in Bugs.jar (from 14 to 22), while also significantly out-performing other state-of-the-art repair techniques including ACS, HD-Repair, NOPOL, PAR, and jGenProg.

</details>

<details>

<summary>2021-12-22 09:24:55 - Security Risks of Porting C Programs to WebAssembly</summary>

- *Quentin Stiévenart, Coen De Roover, Mohammad Ghafari*

- `2112.11745v1` - [abs](http://arxiv.org/abs/2112.11745v1) - [pdf](http://arxiv.org/pdf/2112.11745v1)

> WebAssembly is a compilation target for cross-platform applications that is increasingly being used. In this paper, we investigate whether one can transparently cross-compile C programs to WebAssembly, and if not, what impact porting can have on their security. We compile 17,802 programs that exhibit common vulnerabilities to 64-bit x86 and to WebAssembly binaries, and we observe that the execution of 4,911 binaries produces different results across these platforms. Through manual inspection, we identify three classes of root causes for such differences: the use of a different standard library implementation, the lack of security measures in WebAssembly, and the different semantics of the execution environments. We describe our observations and discuss the ones that are critical from a security point of view and need most attention from developers. We conclude that compiling an existing C program to WebAssembly for cross-platform distribution may require source code adaptations; otherwise, the security of the WebAssembly application may be at risk.

</details>

<details>

<summary>2021-12-22 10:58:32 - Locally Shifted Attention With Early Global Integration</summary>

- *Shelly Sheynin, Sagie Benaim, Adam Polyak, Lior Wolf*

- `2112.05080v2` - [abs](http://arxiv.org/abs/2112.05080v2) - [pdf](http://arxiv.org/pdf/2112.05080v2)

> Recent work has shown the potential of transformers for computer vision applications. An image is first partitioned into patches, which are then used as input tokens for the attention mechanism. Due to the expensive quadratic cost of the attention mechanism, either a large patch size is used, resulting in coarse-grained global interactions, or alternatively, attention is applied only on a local region of the image, at the expense of long-range interactions. In this work, we propose an approach that allows for both coarse global interactions and fine-grained local interactions already at early layers of a vision transformer.   At the core of our method is the application of local and global attention layers. In the local attention layer, we apply attention to each patch and its local shifts, resulting in virtually located local patches, which are not bound to a single, specific location. These virtually located patches are then used in a global attention layer. The separation of the attention layer into local and global counterparts allows for a low computational cost in the number of patches, while still supporting data-dependent localization already at the first layer, as opposed to the static positioning in other visual transformers. Our method is shown to be superior to both convolutional and transformer-based methods for image classification on CIFAR10, CIFAR100, and ImageNet. Code is available at: https://github.com/shellysheynin/Locally-SAG-Transformer.

</details>

<details>

<summary>2021-12-23 10:56:36 - C-Ports: a proposal for a comprehensive standardization and implementation plan of digital services offered by the "Port of the Future"</summary>

- *P. Pagano, S. Antonelli, A. Tardo*

- `2104.13175v3` - [abs](http://arxiv.org/abs/2104.13175v3) - [pdf](http://arxiv.org/pdf/2104.13175v3)

> In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called "Port of the Future". How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel \& Marine Navigation, (ii) e-Freight \& (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C- Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.

</details>

<details>

<summary>2021-12-26 17:42:17 - High Quality Segmentation for Ultra High-resolution Images</summary>

- *Tiancheng Shen, Yuechen Zhang, Lu Qi, Jason Kuen, Xingyu Xie, Jianlong Wu, Zhe Lin, Jiaya Jia*

- `2111.14482v3` - [abs](http://arxiv.org/abs/2111.14482v3) - [pdf](http://arxiv.org/pdf/2111.14482v3)

> To segment 4K or 6K ultra high-resolution images needs extra computation consideration in image segmentation. Common strategies, such as down-sampling, patch cropping, and cascade model, cannot address well the balance issue between accuracy and computation cost. Motivated by the fact that humans distinguish among objects continuously from coarse to precise levels, we propose the Continuous Refinement Model~(CRM) for the ultra high-resolution segmentation refinement task. CRM continuously aligns the feature map with the refinement target and aggregates features to reconstruct these images' details. Besides, our CRM shows its significant generalization ability to fill the resolution gap between low-resolution training images and ultra high-resolution testing ones. We present quantitative performance evaluation and visualization to show that our proposed method is fast and effective on image segmentation refinement. Code will be released at https://github.com/dvlab-research/Entity.

</details>

<details>

<summary>2021-12-28 13:50:03 - Fast explicit dynamics finite element algorithm for transient heat transfer</summary>

- *Jinao Zhang, Sunita Chauhan*

- `1909.03355v2` - [abs](http://arxiv.org/abs/1909.03355v2) - [pdf](http://arxiv.org/pdf/1909.03355v2)

> This paper presents a novel methodology for fast simulation and analysis of transient heat transfer. The proposed methodology is suitable for real-time applications owing to (i) establishing the solution method from the viewpoint of computationally efficient explicit dynamics, (ii) proposing an element-level thermal load computation to eliminate the need for assembling global thermal stiffness, leading to (iii) an explicit formulation of nodal temperature computation to eliminate the need for iterations anywhere in the algorithm, (iv) pre-computing the constant matrices and simulation parameters to facilitate online calculation, and (v) utilising computationally efficient finite elements to efficiently obtain thermal responses in the spatial domain, all of which lead to a significant reduction in computation time for fast run-time simulation. The proposed fast explicit dynamics finite element algorithm (FED-FEM) employs nonlinear thermal material properties, such as temperature-dependent thermal conductivity and specific heat capacity, and nonlinear thermal boundary conditions, such as heat convection and radiation, to account for nonlinear characteristics of transient heat transfer. Simulations and comparison analyses demonstrate that not only can the proposed methodology handle isotropic, orthotropic, anisotropic and temperature-dependent thermal properties but also satisfy the standard patch tests and achieve good agreement with those of the commercial finite element analysis packages for numerical accuracy, for 3-D heat conduction, convection, radiation, and thermal gradient concentration problems. Furthermore, the proposed FED-FEM algorithm is computationally efficient and only consumes a small computation time, capable of achieving real-time computational performance, leading to a novel methodology suitable for real-time simulation and analysis of transient heat transfer.

</details>

