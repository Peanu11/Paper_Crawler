# 2015

## TOC

- [2015-01](#2015-01)
- [2015-02](#2015-02)
- [2015-03](#2015-03)
- [2015-04](#2015-04)
- [2015-05](#2015-05)
- [2015-06](#2015-06)
- [2015-07](#2015-07)
- [2015-08](#2015-08)
- [2015-09](#2015-09)
- [2015-10](#2015-10)
- [2015-11](#2015-11)
- [2015-12](#2015-12)

## 2015-01

<details>

<summary>2015-01-08 03:56:54 - Optimizing Path ORAM for Cloud Storage Applications</summary>

- *Nathan Wolfe, Ethan Zou, Ling Ren, Xiangyao Yu*

- `1501.01721v1` - [abs](http://arxiv.org/abs/1501.01721v1) - [pdf](http://arxiv.org/pdf/1501.01721v1)

> We live in a world where our personal data are both valuable and vulnerable to misappropriation through exploitation of security vulnerabilities in online services. For instance, Dropbox, a popular cloud storage tool, has certain security flaws that can be exploited to compromise a user's data, one of which being that a user's access pattern is unprotected. We have thus created an implementation of Path Oblivious RAM (Path ORAM) for Dropbox users to obfuscate path access information to patch this vulnerability. This implementation differs significantly from the standard usage of Path ORAM, in that we introduce several innovations, including a dynamically growing and shrinking tree architecture, multi-block fetching, block packing and the possibility for multi-client use. Our optimizations together produce about a 77% throughput increase and a 60% reduction in necessary tree size; these numbers vary with file size distribution.

</details>

<details>

<summary>2015-01-08 16:56:03 - Predictive Cyber-security Analytics Framework: A non-homogenous Markov model for Security Quantification</summary>

- *Subil Abraham, Suku Nair*

- `1501.01901v1` - [abs](http://arxiv.org/abs/1501.01901v1) - [pdf](http://arxiv.org/pdf/1501.01901v1)

> Numerous security metrics have been proposed in the past for protecting computer networks. However we still lack effective techniques to accurately measure the predictive security risk of an enterprise taking into account the dynamic attributes associated with vulnerabilities that can change over time. In this paper we present a stochastic security framework for obtaining quantitative measures of security using attack graphs. Our model is novel as existing research in attack graph analysis do not consider the temporal aspects associated with the vulnerabilities, such as the availability of exploits and patches which can affect the overall network security based on how the vulnerabilities are interconnected and leveraged to compromise the system. Gaining a better understanding of the relationship between vulnerabilities and their lifecycle events can provide security practitioners a better understanding of their state of security. In order to have a more realistic representation of how the security state of the network would vary over time, a nonhomogeneous model is developed which incorporates a time dependent covariate, namely the vulnerability age. The daily transition-probability matrices are estimated using Frei's Vulnerability Lifecycle model. We also leverage the trusted CVSS metric domain to analyze how the total exploitability and impact measures evolve over a time period for a given network.

</details>

<details>

<summary>2015-01-31 03:40:17 - A Batchwise Monotone Algorithm for Dictionary Learning</summary>

- *Huan Wang, John Wright, Daniel Spielman*

- `1502.00064v1` - [abs](http://arxiv.org/abs/1502.00064v1) - [pdf](http://arxiv.org/pdf/1502.00064v1)

> We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole. To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start. Experiments on both natural image patches and UCI data sets show that the proposed algorithm produces a better approximation with the same sparsity levels compared to the state-of-the-art algorithms.

</details>


## 2015-02

<details>

<summary>2015-02-04 15:47:56 - A Predictive Framework for Cyber Security Analytics using Attack Graphs</summary>

- *Subil Abraham, Suku Nair*

- `1502.01240v1` - [abs](http://arxiv.org/abs/1502.01240v1) - [pdf](http://arxiv.org/pdf/1502.01240v1)

> Security metrics serve as a powerful tool for organizations to understand the effectiveness of protecting computer networks. However majority of these measurement techniques don't adequately help corporations to make informed risk management decisions. In this paper we present a stochastic security framework for obtaining quantitative measures of security by taking into account the dynamic attributes associated with vulnerabilities that can change over time. Our model is novel as existing research in attack graph analysis do not consider the temporal aspects associated with the vulnerabilities, such as the availability of exploits and patches which can affect the overall network security based on how the vulnerabilities are interconnected and leveraged to compromise the system. In order to have a more realistic representation of how the security state of the network would vary over time, a nonhomogeneous model is developed which incorporates a time dependent covariate, namely the vulnerability age. The daily transition-probability matrices are estimated using Frei's Vulnerability Lifecycle model. We also leverage the trusted CVSS metric domain to analyze how the total exploitability and impact measures evolve over a time period for a given network.

</details>

<details>

<summary>2015-02-12 01:38:36 - Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices</summary>

- *Florian Yger, Masashi Sugiyama*

- `1502.03505v1` - [abs](http://arxiv.org/abs/1502.03505v1) - [pdf](http://arxiv.org/pdf/1502.03505v1)

> Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for Symmetric Positive Definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.

</details>

<details>

<summary>2015-02-12 02:14:37 - Handshaking Protocol for Distributed Implementation of Reo</summary>

- *Natallia Kokash*

- `1504.03553v1` - [abs](http://arxiv.org/abs/1504.03553v1) - [pdf](http://arxiv.org/pdf/1504.03553v1)

> Reo, an exogenous channel-based coordination language, is a model for service coordination wherein services communicate through connectors formed by joining binary communication channels. In order to establish transactional communication among services as prescribed by connector semantics, distributed ports exchange handshaking messages signalling which parties are ready to provide or consume data. In this paper, we present a formal implementation model for distributed Reo with communication delays and outline ideas for its proof of correctness. To reason about Reo implementation formally, we introduce Timed Action Constraint Automata (TACA) and explain how to compare TACA with existing automata-based semantics for Reo. We use TACA to describe handshaking behavior of Reo modeling primitives and argue that in any distributed circuit remote Reo nodes and channels exposing such behavior commit to perform transitions envisaged by the network semantics.

</details>


## 2015-03

<details>

<summary>2015-03-05 04:37:24 - Managing Relocation and Delay in Container Terminals with Flexible Service Policies</summary>

- *Setareh Borjian, Vahideh H. Manshadi, Cynthia Barnhart, Patrick Jaillet*

- `1503.01535v1` - [abs](http://arxiv.org/abs/1503.01535v1) - [pdf](http://arxiv.org/pdf/1503.01535v1)

> We introduce a new model and mathematical formulation for planning crane moves in the storage yard of container terminals. Our objective is to develop a tool that captures customer centric elements, especially service time, and helps operators to manage costly relocation moves. Our model incorporates several practical details and provides port operators with expanded capabilities including planning repositioning moves in off-peak hours, controlling wait times of each customer as well as total service time, optimizing the number of relocations and wait time jointly, and optimizing simultaneously the container stacking and retrieval process. We also study a class of flexible service policies which allow for out-of-order retrieval. We show that under such flexible policies, we can decrease the number of relocations and retrieval delays without creating inequities.

</details>

<details>

<summary>2015-03-09 09:54:27 - Distinguishing Views in Symmetric Networks: A Tight Lower Bound</summary>

- *Dariusz Dereniowski, Adrian Kosowski, Dominik Pajak*

- `1407.2511v2` - [abs](http://arxiv.org/abs/1407.2511v2) - [pdf](http://arxiv.org/pdf/1407.2511v2)

> The view of a node in a port-labeled network is an infinite tree encoding all walks in the network originating from this node. We prove that for any integers $n\geq D\geq 1$, there exists a port-labeled network with at most $n$ nodes and diameter at most $D$ which contains a pair of nodes whose (infinite) views are different, but whose views truncated to depth $\Omega(D\log (n/D))$ are identical.

</details>

<details>

<summary>2015-03-11 19:43:38 - Towards the Design and Implementation of Aspect-Oriented Programming for Spreadsheets</summary>

- *Pedro Maia, Jorge Mendes, Jácome Cunha, Henrique Rebêlo, João Saraiva*

- `1503.03463v1` - [abs](http://arxiv.org/abs/1503.03463v1) - [pdf](http://arxiv.org/pdf/1503.03463v1)

> A spreadsheet usually starts as a simple and single-user software artifact, but, as frequent as in other software systems, quickly evolves into a complex system developed by many actors. Often, different users work on different aspects of the same spreadsheet: while a secretary may be only involved in adding plain data to the spreadsheet, an accountant may define new business rules, while an engineer may need to adapt the spreadsheet content so it can be used by other software systems. Unfortunately, spreadsheet systems do not offer modular mechanisms, and as a consequence, some of the previous tasks may be defined by adding intrusive "code" to the spreadsheet.   In this paper we go through the design and implementation of an aspect-oriented language for spreadsheets so that users can work on different aspects of a spreadsheet in a modular way. For example, aspects can be defined in order to introduce new business rules to an existing spreadsheet, or to manipulate the spreadsheet data to be ported to another system. Aspects are defined as aspect-oriented program specifications that are dynamically woven into the underlying spreadsheet by an aspect weaver. In this aspect-oriented style of spreadsheet development, different users develop, or reuse, aspects without adding intrusive code to the original spreadsheet. Such code is added/executed by the spreadsheet weaving mechanism proposed in this paper.

</details>

<details>

<summary>2015-03-17 04:00:21 - A Model of Layered Architectures</summary>

- *Diego Marmsoler, Alexander Malkis, Jonas Eckhardt*

- `1503.04916v1` - [abs](http://arxiv.org/abs/1503.04916v1) - [pdf](http://arxiv.org/pdf/1503.04916v1)

> Architectural styles and patterns play an important role in software engineering. One of the most known ones is the layered architecture style. However, this style is usually only stated informally, which may cause problems such as ambiguity, wrong conclusions, and difficulty when checking the conformance of a system to the style. We address these problems by providing a formal, denotational semantics of the layered architecture style. Mainly, we present a sufficiently abstract and rigorous description of layered architectures. Loosely speaking, a layered architecture consists of a hierarchy of layers, in which services communicate via ports. A layer is modeled as a relation between used and provided services, and layer composition is defined by means of relational composition. Furthermore, we provide a formal definition for the notions of syntactic and semantic dependency between the layers. We show that these dependencies are not comparable in general. Moreover, we identify sufficient conditions under which, in an intuitive sense which we make precise in our treatment, the semantic dependency implies, is implied by, or even coincides with the reflexive-transitive closure of the syntactic dependency. Our results provide a technology-independent characterization of the layered architecture style, which may be used by software architects to ensure that a system is indeed built according to that style.

</details>


## 2015-04

<details>

<summary>2015-04-09 13:03:28 - ROPocop - Dynamic Mitigation of Code-Reuse Attacks</summary>

- *Andreas Follner, Eric Bodden*

- `1504.02288v1` - [abs](http://arxiv.org/abs/1504.02288v1) - [pdf](http://arxiv.org/pdf/1504.02288v1)

> Control-flow attacks, usually achieved by exploiting a buffer-overflow vulnerability, have been a serious threat to system security for over fifteen years. Researchers have answered the threat with various mitigation techniques, but nevertheless, new exploits that successfully bypass these technologies still appear on a regular basis.   In this paper, we propose ROPocop, a novel approach for detecting and preventing the execution of injected code and for mitigating code-reuse attacks such as return-oriented programming (RoP). ROPocop uses dynamic binary instrumentation, requiring neither access to source code nor debug symbols or changes to the operating system. It mitigates attacks by both monitoring the program counter at potentially dangerous points and by detecting suspicious program flows.   We have implemented ROPocop for Windows x86 using PIN, a dynamic program instrumentation framework from Intel. Benchmarks using the SPEC CPU2006 suite show an average overhead of 2.4x, which is comparable to similar approaches, which give weaker guarantees. Real-world applications show only an initially noticeable input lag and no stutter. In our evaluation our tool successfully detected all 11 of the latest real-world code-reuse exploits, with no false alarms. Therefore, despite the overhead, it is a viable, temporary solution to secure critical systems against exploits if a vendor patch is not yet available.

</details>

<details>

<summary>2015-04-14 17:53:51 - Learning to Compare Image Patches via Convolutional Neural Networks</summary>

- *Sergey Zagoruyko, Nikos Komodakis*

- `1504.03641v1` - [abs](http://arxiv.org/abs/1504.03641v1) - [pdf](http://arxiv.org/pdf/1504.03641v1)

> In this paper we show how to learn directly from image data (i.e., without resorting to manually-designed features) a general similarity function for comparing image patches, which is a task of fundamental importance for many computer vision problems. To encode such a function, we opt for a CNN-based model that is trained to account for a wide variety of changes in image appearance. To that end, we explore and study multiple neural network architectures, which are specifically adapted to this task. We show that such an approach can significantly outperform the state-of-the-art on several problems and benchmark datasets.

</details>

<details>

<summary>2015-04-16 17:26:24 - Towards a relation extraction framework for cyber-security concepts</summary>

- *Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall*

- `1504.04317v1` - [abs](http://arxiv.org/abs/1504.04317v1) - [pdf](http://arxiv.org/pdf/1504.04317v1)

> In order to assist security analysts in obtaining information pertaining to their network, such as novel vulnerabilities, exploits, or patches, information retrieval methods tailored to the security domain are needed. As labeled text data is scarce and expensive, we follow developments in semi-supervised Natural Language Processing and implement a bootstrapping algorithm for extracting security entities and their relationships from text. The algorithm requires little input data, specifically, a few relations or patterns (heuristics for identifying relations), and incorporates an active learning component which queries the user on the most important decisions to prevent drifting from the desired relations. Preliminary testing on a small corpus shows promising results, obtaining precision of .82.

</details>

<details>

<summary>2015-04-20 14:55:08 - Automatic Repair of Infinite Loops</summary>

- *Sebastian R. Lamelas Marcote, Martin Monperrus*

- `1504.05078v1` - [abs](http://arxiv.org/abs/1504.05078v1) - [pdf](http://arxiv.org/pdf/1504.05078v1)

> Research on automatic software repair is concerned with the development of systems that automatically detect and repair bugs. One well-known class of bugs is the infinite loop. Every computer programmer or user has, at least once, experienced this type of bug. We state the problem of repairing infinite loops in the context of test-suite based software repair: given a test suite with at least one failing test, generate a patch that makes all test cases pass. Consequently, repairing infinites loop means having at least one test case that hangs by triggering the infinite loop. Our system to automatically repair infinite loops is called $Infinitel$. We develop a technique to manipulate loops so that one can dynamically analyze the number of iterations of loops; decide to interrupt the loop execution; and dynamically examine the state of the loop on a per-iteration basis. Then, in order to synthesize a new loop condition, we encode this set of program states as a code synthesis problem using a technique based on Satisfiability Modulo Theory (SMT). We evaluate our technique on seven seeded-bugs and on seven real-bugs. $Infinitel$ is able to repair all of them, within seconds up to one hour on a standard laptop configuration.

</details>

<details>

<summary>2015-04-21 11:35:07 - Impact assessment for vulnerabilities in open-source software libraries</summary>

- *Henrik Plate, Serena Elisa Ponta, Antonino Sabetta*

- `1504.04971v2` - [abs](http://arxiv.org/abs/1504.04971v2) - [pdf](http://arxiv.org/pdf/1504.04971v2)

> Software applications integrate more and more open-source software (OSS) to benefit from code reuse. As a drawback, each vulnerability discovered in bundled OSS potentially affects the application. Upon the disclosure of every new vulnerability, the application vendor has to decide whether it is exploitable in his particular usage context, hence, whether users require an urgent application patch containing a non-vulnerable version of the OSS. Current decision making is mostly based on high-level vulnerability descriptions and expert knowledge, thus, effort intense and error prone. This paper proposes a pragmatic approach to facilitate the impact assessment, describes a proof-of-concept for Java, and examines one example vulnerability as case study. The approach is independent from specific kinds of vulnerabilities or programming languages and can deliver immediate results.

</details>

<details>

<summary>2015-04-27 06:38:07 - Byzantine Gathering in Networks</summary>

- *Sébastien Bouchard, Yoann Dieudonné, Bertrand Ducourthial*

- `1504.01623v3` - [abs](http://arxiv.org/abs/1504.01623v3) - [pdf](http://arxiv.org/pdf/1504.01623v3)

> This paper investigates an open problem introduced in [14]. Two or more mobile agents start from different nodes of a network and have to accomplish the task of gathering which consists in getting all together at the same node at the same time. An adversary chooses the initial nodes of the agents and assigns a different positive integer (called label) to each of them. Initially, each agent knows its label but does not know the labels of the other agents or their positions relative to its own. Agents move in synchronous rounds and can communicate with each other only when located at the same node. Up to f of the agents are Byzantine. A Byzantine agent can choose an arbitrary port when it moves, can convey arbitrary information to other agents and can change its label in every round, in particular by forging the label of another agent or by creating a completely new one.   What is the minimum number M of good agents that guarantees deterministic gathering of all of them, with termination?   We provide exact answers to this open problem by considering the case when the agents initially know the size of the network and the case when they do not. In the former case, we prove M=f+1 while in the latter, we prove M=f+2. More precisely, for networks of known size, we design a deterministic algorithm gathering all good agents in any network provided that the number of good agents is at least f+1. For networks of unknown size, we also design a deterministic algorithm ensuring the gathering of all good agents in any network but provided that the number of good agents is at least f+2. Both of our algorithms are optimal in terms of required number of good agents, as each of them perfectly matches the respective lower bound on M shown in [14], which is of f+1 when the size of the network is known and of f+2 when it is unknown.

</details>


## 2015-05

<details>

<summary>2015-05-06 17:29:44 - Improved Approximation Algorithms for Stochastic Matching</summary>

- *Marek Adamczyk, Fabrizio Grandoni, Joydeep Mukherjee*

- `1505.01439v1` - [abs](http://arxiv.org/abs/1505.01439v1) - [pdf](http://arxiv.org/pdf/1505.01439v1)

> In this paper we consider the Stochastic Matching problem, which is motivated by applications in kidney exchange and online dating. We are given an undirected graph in which every edge is assigned a probability of existence and a positive profit, and each node is assigned a positive integer called timeout. We know whether an edge exists or not only after probing it. On this random graph we are executing a process, which one-by-one probes the edges and gradually constructs a matching. The process is constrained in two ways: once an edge is taken it cannot be removed from the matching, and the timeout of node $v$ upper-bounds the number of edges incident to $v$ that can be probed. The goal is to maximize the expected profit of the constructed matching.   For this problem Bansal et al. (Algorithmica 2012) provided a $3$-approximation algorithm for bipartite graphs, and a $4$-approximation for general graphs. In this work we improve the approximation factors to $2.845$ and $3.709$, respectively.   We also consider an online version of the bipartite case, where one side of the partition arrives node by node, and each time a node $b$ arrives we have to decide which edges incident to $b$ we want to probe, and in which order. Here we present a $4.07$-approximation, improving on the $7.92$-approximation of Bansal et al.   The main technical ingredient in our result is a novel way of probing edges according to a random but non-uniform permutation. Patching this method with an algorithm that works best for large probability edges (plus some additional ideas) leads to our improved approximation factors.

</details>

<details>

<summary>2015-05-08 11:35:53 - Deep Learning for Medical Image Segmentation</summary>

- *Matthew Lai*

- `1505.02000v1` - [abs](http://arxiv.org/abs/1505.02000v1) - [pdf](http://arxiv.org/pdf/1505.02000v1)

> This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional "stacked 2D" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular "tri-planar" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.

</details>

<details>

<summary>2015-05-11 11:01:43 - Triple State QuickSort, A replacement for the C/C++ library qsort</summary>

- *Ammar Muqaddas*

- `1505.00558v2` - [abs](http://arxiv.org/abs/1505.00558v2) - [pdf](http://arxiv.org/pdf/1505.00558v2)

> An industrial grade Quicksort function along with its new algorithm is presented. Compared to 4 other well known implementations of Quicksort, the new algorithm reduces both the number of comparisons and swaps in most cases while staying close to the best of the 4 in worst cases. We trade space for performance, at the price of n/2 temporary extra spaces in the worst case. Run time tests reveal an overall improvement of at least 15.8% compared to the overall best of the other 4 functions. Furthermore, our function scores a 32.7% run time improvement against Yaroslavskiy's new Dual Pivot Quicksort. Our function is pointer based, which is meant as a replacement for the C/C++ library qsort(). But we also provide an array based function of the same algorithm for easy porting to different programming languages.

</details>

<details>

<summary>2015-05-12 08:42:42 - Contour Detection Using Cost-Sensitive Convolutional Neural Networks</summary>

- *Jyh-Jing Hwang, Tyng-Luh Liu*

- `1412.6857v5` - [abs](http://arxiv.org/abs/1412.6857v5) - [pdf](http://arxiv.org/pdf/1412.6857v5)

> We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.

</details>


## 2015-06

<details>

<summary>2015-06-18 23:11:40 - An Iterative Convolutional Neural Network Algorithm Improves Electron Microscopy Image Segmentation</summary>

- *Xundong Wu*

- `1506.05849v1` - [abs](http://arxiv.org/abs/1506.05849v1) - [pdf](http://arxiv.org/pdf/1506.05849v1)

> To build the connectomics map of the brain, we developed a new algorithm that can automatically refine the Membrane Detection Probability Maps (MDPM) generated to perform automatic segmentation of electron microscopy (EM) images. To achieve this, we executed supervised training of a convolutional neural network to recover the removed center pixel label of patches sampled from a MDPM. MDPM can be generated from other machine learning based algorithms recognizing whether a pixel in an image corresponds to the cell membrane. By iteratively applying this network over MDPM for multiple rounds, we were able to significantly improve membrane segmentation results.

</details>

<details>

<summary>2015-06-19 11:43:36 - Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</summary>

- *Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox*

- `1406.6909v2` - [abs](http://arxiv.org/abs/1406.6909v2) - [pdf](http://arxiv.org/pdf/1406.6909v2)

> Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While such generic features cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.

</details>

<details>

<summary>2015-06-20 17:55:49 - Pose Estimation Based on 3D Models</summary>

- *Chuiwen Ma, Hao Su, Liang Shi*

- `1506.06274v1` - [abs](http://arxiv.org/abs/1506.06274v1) - [pdf](http://arxiv.org/pdf/1506.06274v1)

> In this paper, we proposed a pose estimation system based on rendered image training set, which predicts the pose of objects in real image, with knowledge of object category and tight bounding box. We developed a patch-based multi-class classification algorithm, and an iterative approach to improve the accuracy. We achieved state-of-the-art performance on pose estimation task.

</details>

<details>

<summary>2015-06-25 16:19:44 - Deep Neural Networks for Anatomical Brain Segmentation</summary>

- *Alexandre de Brebisson, Giovanni Montana*

- `1502.02445v2` - [abs](http://arxiv.org/abs/1502.02445v2) - [pdf](http://arxiv.org/pdf/1502.02445v2)

> We present a novel approach to automatically segment magnetic resonance (MR) images of the human brain into anatomical regions. Our methodology is based on a deep artificial neural network that assigns each voxel in an MR image of the brain to its corresponding anatomical region. The inputs of the network capture information at different scales around the voxel of interest: 3D and orthogonal 2D intensity patches capture the local spatial context while large, compressed 2D orthogonal patches and distances to the regional centroids enforce global spatial consistency. Contrary to commonly used segmentation methods, our technique does not require any non-linear registration of the MR images. To benchmark our model, we used the dataset provided for the MICCAI 2012 challenge on multi-atlas labelling, which consists of 35 manually segmented MR images of the brain. We obtained competitive results (mean dice coefficient 0.725, error rate 0.163) showing the potential of our approach. To our knowledge, our technique is the first to tackle the anatomical segmentation of the whole brain using deep neural networks.

</details>

<details>

<summary>2015-06-26 03:45:23 - Time vs. Information Tradeoffs for Leader Election in Anonymous Trees</summary>

- *Christian Glacet, Avery Miller, Andrzej Pelc*

- `1505.04308v2` - [abs](http://arxiv.org/abs/1505.04308v2) - [pdf](http://arxiv.org/pdf/1505.04308v2)

> The leader election task calls for all nodes of a network to agree on a single node. If the nodes of the network are anonymous, the task of leader election is formulated as follows: every node $v$ of the network must output a simple path, coded as a sequence of port numbers, such that all these paths end at a common node, the leader. In this paper, we study deterministic leader election in anonymous trees.   Our aim is to establish tradeoffs between the allocated time $\tau$ and the amount of information that has to be given $\textit{a priori}$ to the nodes to enable leader election in time $\tau$ in all trees for which leader election in this time is at all possible. Following the framework of $\textit{algorithms with advice}$, this information (a single binary string) is provided to all nodes at the start by an oracle knowing the entire tree. The length of this string is called the $\textit{size of advice}$. For an allocated time $\tau$, we give upper and lower bounds on the minimum size of advice sufficient to perform leader election in time $\tau$.   We consider $n$-node trees of diameter $diam \leq D$. While leader election in time $diam$ can be performed without any advice, for time $diam-1$ we give tight upper and lower bounds of $\Theta (\log D)$. For time $diam-2$ we give tight upper and lower bounds of $\Theta (\log D)$ for even values of $diam$, and tight upper and lower bounds of $\Theta (\log n)$ for odd values of $diam$. For the time interval $[\beta \cdot diam, diam-3]$ for constant $\beta >1/2$, we prove an upper bound of $O(\frac{n\log n}{D})$ and a lower bound of $\Omega(\frac{n}{D})$, the latter being valid whenever $diam$ is odd or when the time is at most $diam-4$. Finally, for time $\alpha \cdot diam$ for any constant $\alpha <1/2$ (except for the case of very small diameters), we give tight upper and lower bounds of $\Theta (n)$.

</details>

<details>

<summary>2015-06-27 13:28:18 - Andriod Based Punjabi TTS System</summary>

- *Hardeep, Parminder Singh*

- `1508.05822v1` - [abs](http://arxiv.org/abs/1508.05822v1) - [pdf](http://arxiv.org/pdf/1508.05822v1)

> The usage of mobile phones is nearly 3.5 times more than that of personal computers. Android has the largest share among its counter parts like IOS, Windows and Symbian Android applications have a very few restrictions on them. TTS systems on Android are available for many languages but a very few systems of this type are available for Punjabi language. Our research work had the aim to develop an application that will be able to produce synthetic Punjabi speech. The paper examines the methodology used to develop speech synthesis TTS system for the Punjabi content, which is written in Gurmukhi script. For the development of this system, we use concatenative speech synthesis method with phonemes as the basic units of concatenation. Some challenges like application size, processing time, must be considered, while porting this TTS system to resource-limited devices like mobile phones.

</details>


## 2015-07

<details>

<summary>2015-07-01 13:12:38 - TurboMOR: an Efficient Model Order Reduction Technique for RC Networks with Many Ports</summary>

- *Denis Oyaro, Piero Triverio*

- `1507.00219v1` - [abs](http://arxiv.org/abs/1507.00219v1) - [pdf](http://arxiv.org/pdf/1507.00219v1)

> Model order reduction (MOR) techniques play a crucial role in the computer-aided design of modern integrated circuits, where they are used to reduce the size of parasitic networks. Unfortunately, the efficient reduction of passive networks with many ports is still an open problem. Existing techniques do not scale well with the number of ports, and lead to dense reduced models that burden subsequent simulations. In this paper, we propose TurboMOR, a novel MOR technique for the efficient reduction of passive RC networks. TurboMOR is based on moment-matching, achieved through efficient congruence transformations based on Householder reflections. A novel feature of TurboMOR is the block-diagonal structure of the reduced models, that makes them more efficient than the dense models produced by existing techniques. Moreover, the model structure allows for an insightful interpretation of the reduction process in terms of system theory. Numerical results show that TurboMOR scales more favourably than existing techniques in terms of reduction time, simulation time and memory consumption.

</details>

<details>

<summary>2015-07-03 17:34:13 - Meta learning of bounds on the Bayes classifier error</summary>

- *Kevin R. Moon, Veronique Delouille, Alfred O. Hero III*

- `1504.07116v2` - [abs](http://arxiv.org/abs/1504.07116v2) - [pdf](http://arxiv.org/pdf/1504.07116v2)

> Meta learning uses information from base learners (e.g. classifiers or estimators) as well as information about the learning problem to improve upon the performance of a single base learner. For example, the Bayes error rate of a given feature space, if known, can be used to aid in choosing a classifier, as well as in feature selection and model selection for the base classifiers and the meta classifier. Recent work in the field of f-divergence functional estimation has led to the development of simple and rapidly converging estimators that can be used to estimate various bounds on the Bayes error. We estimate multiple bounds on the Bayes error using an estimator that applies meta learning to slowly converging plug-in estimators to obtain the parametric convergence rate. We compare the estimated bounds empirically on simulated data and then estimate the tighter bounds on features extracted from an image patch analysis of sunspot continuum and magnetogram images.

</details>

<details>

<summary>2015-07-08 19:08:03 - GenASiS: General Astrophysical Simulation System. I. Refinable Mesh and Nonrelativistic Hydrodynamics</summary>

- *Christian Y. Cardall, Reuben D. Budiardja, Eirik Endeve, Anthony Mezzacappa*

- `1207.3392v2` - [abs](http://arxiv.org/abs/1207.3392v2) - [pdf](http://arxiv.org/pdf/1207.3392v2)

> GenASiS (General Astrophysical Simulation System) is a new code being developed initially and primarily, though by no means exclusively, for the simulation of core-collapse supernovae on the world's leading capability supercomputers. This paper---the first in a series---demonstrates a centrally refined coordinate patch suitable for gravitational collapse and documents methods for compressible nonrelativistic hydrodynamics. We benchmark the hydrodynamics capabilities of GenASiS against many standard test problems; the results illustrate the basic competence of our implementation, demonstrate the strengths and limitations of the HLLC relative to the HLL Riemann solver in a number of interesting cases, and provide preliminary indications of the code's ability to scale and to function with cell-by-cell fixed-mesh refinement.

</details>

<details>

<summary>2015-07-18 00:07:52 - Persistent Topology of Syntax</summary>

- *Alexander Port, Iulia Gheorghita, Daniel Guth, John M. Clark, Crystal Liang, Shival Dasu, Matilde Marcolli*

- `1507.05134v1` - [abs](http://arxiv.org/abs/1507.05134v1) - [pdf](http://arxiv.org/pdf/1507.05134v1)

> We study the persistent homology of the data set of syntactic parameters of the world languages. We show that, while homology generators behave erratically over the whole data set, non-trivial persistent homology appears when one restricts to specific language families. Different families exhibit different persistent homology. We focus on the cases of the Indo-European and the Niger-Congo families, for which we compare persistent homology over different cluster filtering values. We investigate the possible significance, in historical linguistic terms, of the presence of persistent generators of the first homology. In particular, we show that the persistent first homology generator we find in the Indo-European family is not due (as one might guess) to the Anglo-Norman bridge in the Indo-European phylogenetic network, but is related to the position of Ancient Greek and the Hellenic branch within the network.

</details>

<details>

<summary>2015-07-21 08:28:02 - Harvesting Fix Hints in the History of Bugs</summary>

- *Tegawendé F. Bissyandé*

- `1507.05742v1` - [abs](http://arxiv.org/abs/1507.05742v1) - [pdf](http://arxiv.org/pdf/1507.05742v1)

> In software development, fixing bugs is an important task that is time consuming and cost-sensitive. While many approaches have been proposed to automatically detect and patch software code, the strategies are limited to a set of identified bugs that were thoroughly studied to define their properties. They thus manage to cover a niche of faults such as infinite loops. We build on the assumption that bugs, and the associated user bug reports, are repetitive and propose a new approach of fix recommendations based on the history of bugs and their associated fixes. In our approach, once a bug is reported, it is automatically compared to all previously fixed bugs using information retrieval techniques and machine learning classification. Based on this comparison, we recommend top-{\em k} fix actions, identified from past fix examples, that may be suitable as hints for software developers to address the new bug.

</details>

<details>

<summary>2015-07-22 08:28:03 - Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm</summary>

- *Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, André van Schaik, Jonathan Tapson*

- `1412.8307v2` - [abs](http://arxiv.org/abs/1412.8307v2) - [pdf](http://arxiv.org/pdf/1412.8307v2)

> Recent advances in training deep (multi-layer) architectures have inspired a renaissance in neural network use. For example, deep convolutional networks are becoming the default option for difficult tasks on large datasets, such as image and speech recognition. However, here we show that error rates below 1% on the MNIST handwritten digit benchmark can be replicated with shallow non-convolutional neural networks. This is achieved by training such networks using the 'Extreme Learning Machine' (ELM) approach, which also enables a very rapid training time (~10 minutes). Adding distortions, as is common practise for MNIST, reduces error rates even further. Our methods are also shown to be capable of achieving less than 5.5% error rates on the NORB image database. To achieve these results, we introduce several enhancements to the standard ELM algorithm, which individually and in combination can significantly improve performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random `receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90% of weights equal to zero. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems.

</details>


## 2015-08

<details>

<summary>2015-08-02 01:36:37 - An Analytic Framework for Maritime Situation Analysis</summary>

- *Hamed Yaghoubi Shahir, Uwe Glässer, Amir Yaghoubi Shahir, Hans Wehn*

- `1508.00181v1` - [abs](http://arxiv.org/abs/1508.00181v1) - [pdf](http://arxiv.org/pdf/1508.00181v1)

> Maritime domain awareness is critical for protecting sea lanes, ports, harbors, offshore structures and critical infrastructures against common threats and illegal activities. Limited surveillance resources constrain maritime domain awareness and compromise full security coverage at all times. This situation calls for innovative intelligent systems for interactive situation analysis to assist marine authorities and security personal in their routine surveillance operations. In this article, we propose a novel situation analysis framework to analyze marine traffic data and differentiate various scenarios of vessel engagement for the purpose of detecting anomalies of interest for marine vessels that operate over some period of time in relative proximity to each other. The proposed framework views vessel behavior as probabilistic processes and uses machine learning to model common vessel interaction patterns. We represent patterns of interest as left-to-right Hidden Markov Models and classify such patterns using Support Vector Machines.

</details>

<details>

<summary>2015-08-17 14:28:16 - Supporting Developers in Porting Software via Combined Textual and Structural Analysis of Software Artifacts</summary>

- *Kostadin Damevski, David Shepherd, Nicholas Kraft, Lori Pollock*

- `1508.04044v1` - [abs](http://arxiv.org/abs/1508.04044v1) - [pdf](http://arxiv.org/pdf/1508.04044v1)

> This is position paper accepted to the Computational Science & Engineering Software Sustainability and Productivity Challenges (CSESSP Challenges) Workshop, sponsored by the Networking and Information Technology Research and Development (NITRD) Software Design and Productivity (SDP) Coordinating Group, held October 15th-16th 2015 in Washington DC, USA. It discusses the role recommendation systems, based on textual and structural information in source code, and further enhanced by mining related applications, can have in improving the portability of scientific and engineering software.

</details>

<details>

<summary>2015-08-27 15:35:11 - Compressive Sensing via Low-Rank Gaussian Mixture Models</summary>

- *Xin Yuan, Hong Jiang, Gang Huang, Paul A. Wilford*

- `1508.06901v1` - [abs](http://arxiv.org/abs/1508.06901v1) - [pdf](http://arxiv.org/pdf/1508.06901v1)

> We develop a new compressive sensing (CS) inversion algorithm by utilizing the Gaussian mixture model (GMM). While the compressive sensing is performed globally on the entire image as implemented in our lensless camera, a low-rank GMM is imposed on the local image patches. This low-rank GMM is derived via eigenvalue thresholding of the GMM trained on the projection of the measurement data, thus learned {\em in situ}. The GMM and the projection of the measurement data are updated iteratively during the reconstruction. Our GMM algorithm degrades to the piecewise linear estimator (PLE) if each patch is represented by a single Gaussian model. Inspired by this, a low-rank PLE algorithm is also developed for CS inversion, constituting an additional contribution of this paper. Extensive results on both simulation data and real data captured by the lensless camera demonstrate the efficacy of the proposed algorithm. Furthermore, we compare the CS reconstruction results using our algorithm with the JPEG compression. Simulation results demonstrate that when limited bandwidth is available (a small number of measurements), our algorithm can achieve comparable results as JPEG.

</details>


## 2015-09

<details>

<summary>2015-09-10 08:01:50 - On the "Naturalness" of Buggy Code</summary>

- *Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, Premkumar Devanbu*

- `1506.01159v2` - [abs](http://arxiv.org/abs/1506.01159v2) - [pdf](http://arxiv.org/pdf/1506.01159v2)

> Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be "natural", like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is "unnatural" in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca.~8,296), from 10 different Java projects, and we focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e., unnatural), becoming less so as bugs are fixed. Focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid language-independent and simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.

</details>

<details>

<summary>2015-09-11 01:20:46 - Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems</summary>

- *Fabian Tschopp*

- `1509.03371v1` - [abs](http://arxiv.org/abs/1509.03371v1) - [pdf](http://arxiv.org/pdf/1509.03371v1)

> This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once.   The implementations of the three models are accessible through a new utility on top of the Caffe library. The utility provides support for a wide range of image input and output formats, pre-processing parameters and methods to equalize the label histogram during training. The Caffe library has been extended by new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs through OpenCL.   On AMD GPUs, speedups of $54\times$ (SK-Net), $437\times$ (U-Net) and $320\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding window) network as the baseline. The label throughput is up to one megapixel per second.   The analyzed neural networks have distinctive characteristics that apply during training or processing, and not every data set is suitable to every architecture. The quality of the predictions is assessed on two neural tissue data sets, of which one is the ISBI 2012 challenge data set. Two different loss functions, Malis loss and Softmax loss, were used during training.   The whole pipeline, consisting of models, interface and modified Caffe library, is available as Open Source software under the working title Project Greentea.

</details>

<details>

<summary>2015-09-14 06:52:20 - Dual-Layer Video Encryption using RSA Algorithm</summary>

- *Aman Chadha, Sushmit Mallik, Ankit Chadha, Ravdeep Johar, M. Mani Roja*

- `1509.04387v1` - [abs](http://arxiv.org/abs/1509.04387v1) - [pdf](http://arxiv.org/pdf/1509.04387v1)

> This paper proposes a video encryption algorithm using RSA and Pseudo Noise (PN) sequence, aimed at applications requiring sensitive video information transfers. The system is primarily designed to work with files encoded using the Audio Video Interleaved (AVI) codec, although it can be easily ported for use with Moving Picture Experts Group (MPEG) encoded files. The audio and video components of the source separately undergo two layers of encryption to ensure a reasonable level of security. Encryption of the video component involves applying the RSA algorithm followed by the PN-based encryption. Similarly, the audio component is first encrypted using PN and further subjected to encryption using the Discrete Cosine Transform. Combining these techniques, an efficient system, invulnerable to security breaches and attacks with favorable values of parameters such as encryption/decryption speed, encryption/decryption ratio and visual degradation; has been put forth. For applications requiring encryption of sensitive data wherein stringent security requirements are of prime concern, the system is found to yield negligible similarities in visual perception between the original and the encrypted video sequence. For applications wherein visual similarity is not of major concern, we limit the encryption task to a single level of encryption which is accomplished by using RSA, thereby quickening the encryption process. Although some similarity between the original and encrypted video is observed in this case, it is not enough to comprehend the happenings in the video.

</details>

<details>

<summary>2015-09-15 14:35:11 - Kernelized Deep Convolutional Neural Network for Describing Complex Images</summary>

- *Zhen Liu*

- `1509.04581v1` - [abs](http://arxiv.org/abs/1509.04581v1) - [pdf](http://arxiv.org/pdf/1509.04581v1)

> With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demon- strated promising performance in various vision-based ap- plications, such as classification, recognition, and objec- t detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves lim- ited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the s- cenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motiva- tion by an experimental study to demonstrate the sensitivi- ty of the global CNN feature to the basic geometric trans- formations. Then, we propose to represent visual content with approximate invariance to the above geometric trans- formations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial repre- sentation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets.

</details>

<details>

<summary>2015-09-23 02:08:02 - Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours</summary>

- *Lerrel Pinto, Abhinav Gupta*

- `1509.06825v1` - [abs](http://arxiv.org/abs/1509.06825v1) - [pdf](http://arxiv.org/pdf/1509.06825v1)

> Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.

</details>


## 2015-10

<details>

<summary>2015-10-07 01:56:08 - Container Relocation Problem: Approximation, Asymptotic, and Incomplete Information</summary>

- *Setareh Borjian, Virgile Galle, Vahideh H. Manshadi, Cynthia Barnhart, Patrick Jaillet*

- `1505.04229v2` - [abs](http://arxiv.org/abs/1505.04229v2) - [pdf](http://arxiv.org/pdf/1505.04229v2)

> The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers respecting a given order of retrieval. While the problem is known to be NP-hard, certain algorithms such as the A* search and heuristics perform reasonably well on many instances of the problem. In this paper, we first focus on the A* search algorithm, and analyze lower and upper bounds that are easy to compute and can be used to prune nodes. Our analysis sheds light on which bounds result in fast computation within a given approximation gap. We present extensive simulation results that improve upon our theoretical analysis, and further show that our method finds the optimum solution on most instances of medium-size bays. On "hard" instances, our method finds an approximate solution with a small gap and within a time frame that is fast for practical applications. We also study the average-case asymptotic behavior of the CRP where the number of columns grows. We calculate the expected number of relocations in the limit, and show that the optimum number of relocations converges to a simple and intuitive lower-bound. We further study the CRP with incomplete information by relaxing the assumption that the order of retrieval of all containers are initially known. This assumption is particularly unrealistic in ports without an appointment system. We assume that the retrieval order of a subset of containers is known initially and the retrieval order of the remaining containers is observed later at a given specific time. Before this time, we assume a probabilistic distribution on the retrieval order of unknown containers. We combine the A* algorithm with sampling technique to solve this two-stage stochastic optimization problem. We show that our algorithm is fast and the error due to sampling and pruning is reasonably small.

</details>

<details>

<summary>2015-10-20 15:08:48 - Computing the Stereo Matching Cost with a Convolutional Neural Network</summary>

- *Jure Žbontar, Yann LeCun*

- `1409.4326v2` - [abs](http://arxiv.org/abs/1409.4326v2) - [pdf](http://arxiv.org/pdf/1409.4326v2)

> We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.

</details>

<details>

<summary>2015-10-22 18:14:42 - ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines</summary>

- *Aleksandar Zlateski, Kisuk Lee, H. Sebastian Seung*

- `1510.06706v1` - [abs](http://arxiv.org/abs/1510.06706v1) - [pdf](http://arxiv.org/pdf/1510.06706v1)

> Convolutional networks (ConvNets) have become a popular approach to computer vision. It is important to accelerate ConvNet training, which is computationally costly. We propose a novel parallel algorithm based on decomposition into a set of tasks, most of which are convolutions or FFTs. Applying Brent's theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the PRAM model of parallel computation, for wide network architectures. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. We implement the algorithm with a publicly available software package called ZNN. Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly equal to the number of physical cores. We also show that ZNN can attain over 90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are achieved for network architectures with widths that are in common use. The task parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism of previous algorithms is compatible with GPUs. Through examples, we show that ZNN can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. ZNN may be less costly to develop and maintain, due to the relative ease of general-purpose CPU programming.

</details>

<details>

<summary>2015-10-23 01:58:24 - Efficient Blind Compressed Sensing Using Sparsifying Transforms with Convergence Guarantees and Application to MRI</summary>

- *Saiprasad Ravishankar, Yoram Bresler*

- `1501.02923v2` - [abs](http://arxiv.org/abs/1501.02923v2) - [pdf](http://arxiv.org/pdf/1501.02923v2)

> Natural signals and images are well-known to be approximately sparse in transform domains such as Wavelets and DCT. This property has been heavily exploited in various applications in image processing and medical imaging. Compressed sensing exploits the sparsity of images or image patches in a transform domain or synthesis dictionary to reconstruct images from undersampled measurements. In this work, we focus on blind compressed sensing, where the underlying sparsifying transform is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements. The proposed block coordinate descent type algorithms involve highly efficient optimal updates. Importantly, we prove that although the proposed blind compressed sensing formulations are highly nonconvex, our algorithms are globally convergent (i.e., they converge from any initialization) to the set of critical points of the objectives defining the formulations. These critical points are guaranteed to be at least partial global and partial local minimizers. The exact point(s) of convergence may depend on initialization. We illustrate the usefulness of the proposed framework for magnetic resonance image reconstruction from highly undersampled k-space measurements. As compared to previous methods involving the synthesis dictionary model, our approach is much faster, while also providing promising reconstruction quality.

</details>


## 2015-11

<details>

<summary>2015-11-01 14:58:04 - No Need for Black Chambers: Testing TLS in the E-mail Ecosystem at Large</summary>

- *Wilfried Mayer, Aaron Zauner, Martin Schmiedecker, Markus Huber*

- `1510.08646v2` - [abs](http://arxiv.org/abs/1510.08646v2) - [pdf](http://arxiv.org/pdf/1510.08646v2)

> TLS is the most widely used cryptographic protocol on the Internet. While many recent studies focused on its use in HTTPS, none so far analyzed TLS usage in e-mail related protocols, which often carry highly sensitive information. Since end-to-end encryption mechanisms like PGP are seldomly used, today confidentiality in the e-mail ecosystem is mainly based on the encryption of the transport layer. A well-positioned attacker may be able to intercept plaintext passively and at global scale. In this paper we are the first to present a scalable methodology to assess the state of security mechanisms in the e-mail ecosystem using commodity hardware and open-source software. We draw a comprehensive picture of the current state of every e-mail related TLS configuration for the entire IPv4 range. We collected and scanned a massive data-set of 20 million IP/port combinations of all related protocols (SMTP, POP3, IMAP) and legacy ports. Over a time span of approx. three months we conducted more than 10 billion TLS handshakes. Additionally, we show that securing server-to-server communication using e.g. SMTP is inherently more difficult than securing client-to-server communication. Lastly, we analyze the volatility of TLS certificates and trust anchors in the e-mail ecosystem and argue that while the overall trend points in the right direction, there are still many steps needed towards secure e-mail.

</details>

<details>

<summary>2015-11-09 07:09:57 - Batch-normalized Maxout Network in Network</summary>

- *Jia-Ren Chang, Yong-Sheng Chen*

- `1511.02583v1` - [abs](http://arxiv.org/abs/1511.02583v1) - [pdf](http://arxiv.org/pdf/1511.02583v1)

> This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.

</details>

<details>

<summary>2015-11-16 18:42:04 - Topic Modeling of Behavioral Modes Using Sensor Data</summary>

- *Yehezkel S. Resheff, Shay Rotics, Ran Nathan, Daphna Weinshall*

- `1511.05082v1` - [abs](http://arxiv.org/abs/1511.05082v1) - [pdf](http://arxiv.org/pdf/1511.05082v1)

> The field of Movement Ecology, like so many other fields, is experiencing a period of rapid growth in availability of data. As the volume rises, traditional methods are giving way to machine learning and data science, which are playing an increasingly large part it turning this data into science-driving insights. One rich and interesting source is the bio-logger. These small electronic wearable devices are attached to animals free to roam in their natural habitats, and report back readings from multiple sensors, including GPS and accelerometer bursts. A common use of accelerometer data is for supervised learning of behavioral modes. However, we need unsupervised analysis tools as well, in order to overcome the inherent difficulties of obtaining a labeled dataset, which in some cases is either infeasible or does not successfully encompass the full repertoire of behavioral modes of interest. Here we present a matrix factorization based topic-model method for accelerometer bursts, derived using a linear mixture property of patch features. Our method is validated via comparison to a labeled dataset, and is further compared to standard clustering algorithms.

</details>

<details>

<summary>2015-11-17 11:59:30 - Probabilistic Modelling of the Impact on Bus Punctuality of a Speed Limit Proposal in Edinburgh (Extended Version)</summary>

- *Daniël Reijsbergen, Rajeev Ratan*

- `1511.05363v1` - [abs](http://arxiv.org/abs/1511.05363v1) - [pdf](http://arxiv.org/pdf/1511.05363v1)

> We propose a data-driven methodology for evaluating the impact of the introduction of a speed limit on the punctuality of bus services. In particular, we use high-frequency Automatic Vehicle Location data to parameterise a model that represents the movement of a bus along predefined patches of the route. We fit the probability distributions of the time spent in each patch to two classes of probability distributions: hyper-Erlang distributions, for which we use the tool HyperStar, and a variation of the three-parameter gamma distributions recommended by the Traffic Engineering Handbook. In both cases we obtain models that can be expressed using the framework of Probabilistic Timed Automata, allowing us to evaluate bus punctuality using the model checking tool UPPAAL. We conduct a case study involving a proposed speed limit in Edinburgh. This is an extended version of a paper presented at ValueTools 2015.

</details>

<details>

<summary>2015-11-21 01:33:12 - Learning visual groups from co-occurrences in space and time</summary>

- *Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson*

- `1511.06811v1` - [abs](http://arxiv.org/abs/1511.06811v1) - [pdf](http://arxiv.org/pdf/1511.06811v1)

> We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.

</details>

<details>

<summary>2015-11-29 18:49:39 - Time and space optimality of rotor-router graph exploration</summary>

- *Artur Menc, Dominik Pająk, Przemysław Uznański*

- `1502.05545v3` - [abs](http://arxiv.org/abs/1502.05545v3) - [pdf](http://arxiv.org/pdf/1502.05545v3)

> We consider the problem of exploration of an anonymous, port-labeled, undirected graph with $n$ nodes and $m$ edges and diameter $D$, by a single mobile agent. Initially the agent does not know the graph topology nor any of the global parameters. Moreover, the agent does not know the incoming port when entering to a vertex. Each vertex is endowed with memory that can be read and modified by the agent upon its visit to that node. However the agent has no operational memory i.e., it cannot carry any state while traversing an edge. In such a model at least $\log_2 d$ bits are needed at each vertex of degree $d$ for the agent to be able to traverse each graph edge. This number of bits is always sufficient to explore any graph in time $O(mD)$ using algorithm Rotor-Router. We show that even if the available node memory is unlimited then time $\Omega(n^3)$ is sometimes required for any algorithm. This shows that Rotor-Router is asymptotically optimal in the worst-case graphs. Secondly we show that for the case of the path the Rotor-Router attains exactly optimal time.

</details>


## 2015-12

<details>

<summary>2015-12-02 14:49:12 - MMSE Estimation for Poisson Noise Removal in Images</summary>

- *Stanislav Pyatykh, Jürgen Hesser*

- `1512.00717v1` - [abs](http://arxiv.org/abs/1512.00717v1) - [pdf](http://arxiv.org/pdf/1512.00717v1)

> Poisson noise suppression is an important preprocessing step in several applications, such as medical imaging, microscopy, and astronomical imaging. In this work, we propose a novel patch-wise Poisson noise removal strategy, in which the MMSE estimator is utilized in order to produce the denoising result for each image patch. Fast and accurate computation of the MMSE estimator is carried out using k-d tree search followed by search in the K-nearest neighbor graph. Our experiments show that the proposed method is the preferable choice for low signal-to-noise ratios.

</details>

<details>

<summary>2015-12-21 21:01:07 - Addressing Complex and Subjective Product-Related Queries with Customer Reviews</summary>

- *Julian McAuley, Alex Yang*

- `1512.06863v1` - [abs](http://arxiv.org/abs/1512.06863v1) - [pdf](http://arxiv.org/pdf/1512.06863v1)

> Online reviews are often our first port of call when considering products and purchases online. When evaluating a potential purchase, we may have a specific query in mind, e.g. `will this baby seat fit in the overhead compartment of a 747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer such questions we must either wade through huge volumes of consumer reviews hoping to find one that is relevant, or otherwise pose our question directly to the community via a Q/A system.   In this paper we hope to fuse these two paradigms: given a large volume of previously answered queries about products, we hope to automatically learn whether a review of a product is relevant to a given query. We formulate this as a machine learning problem using a mixture-of-experts-type framework---here each review is an `expert' that gets to vote on the response to a particular query; simultaneously we learn a relevance function such that `relevant' reviews are those that vote correctly. At test time this learned relevance function allows us to surface reviews that are relevant to new queries on-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million questions (and answers) and 13 million reviews. We show quantitatively that it is effective at addressing both binary and open-ended queries, and qualitatively that it surfaces reviews that human evaluators consider to be relevant.

</details>

<details>

<summary>2015-12-23 11:09:46 - Automatic Repair of Real Bugs: An Experience Report on the Defects4J Dataset</summary>

- *Matias Martinez, Thomas Durieux, Jifeng Xuan, Romain Sommerard, Martin Monperrus*

- `1505.07002v2` - [abs](http://arxiv.org/abs/1505.07002v2) - [pdf](http://arxiv.org/pdf/1505.07002v2)

> Defects4J is a large, peer-reviewed, structured dataset of real-world Java bugs. Each bug in Defects4J is provided with a test suite and at least one failing test case that triggers the bug. In this paper, we report on an experiment to explore the effectiveness of automatic repair on Defects4J. The result of our experiment shows that 47 bugs of the Defects4J dataset can be automatically repaired by state-of- the-art repair. This sets a baseline for future research on automatic repair for Java. We have manually analyzed 84 different patches to assess their real correctness. In total, 9 real Java bugs can be correctly fixed with test-suite based repair. This analysis shows that test-suite based repair suffers from under-specified bugs, for which trivial and incorrect patches still pass the test suite. With respect to practical applicability, it takes in average 14.8 minutes to find a patch. The experiment was done on a scientific grid, totaling 17.6 days of computation time. All their systems and experimental results are publicly available on Github in order to facilitate future research on automatic repair.

</details>

<details>

<summary>2015-12-25 00:18:10 - Towards Approaches to Continuous Assessment of Cyber Risk in Security of Computer Networks</summary>

- *Alexander Kott, Curtis Arnold*

- `1512.07937v1` - [abs](http://arxiv.org/abs/1512.07937v1) - [pdf](http://arxiv.org/pdf/1512.07937v1)

> We review the current status and research challenges in the area of cyber security often called continuous monitoring and risk scoring (CMRS). We focus on two most salient aspects of CMRS. First, continuous collection of data through automated feeds; hence the term continuous monitoring. Typical data collected for continuous monitoring purposes include network traffic information as well as host information from host-based agents. Second, analysis of the collected data in order to assess the risks - the risk scoring. This assessment may include flagging especially egregious vulnerabilities and exposures, or computing metrics that provide an overall characterization of the network's risk level. Currently used risk metrics are often simple sums or counts of vulnerabilities and missing patches.   The research challenges pertaining to CMRS fall mainly into two categories. The first centers on the problem of integrating and fusing highly heterogeneous information. The second group of challenges is the lack of rigorous approaches to computing risk. Existing risk scoring algorithms remain limited to ad hoc heuristics such as simple sums of vulnerability scores or counts of things like missing patches or open ports, etc. Weaknesses and potentially misleading nature of such metrics are well recognized. For example, the individual vulnerability scores are dangerously reliant on subjective, human, qualitative input, potentially inaccurate and expensive to obtain. Further, the total number of vulnerabilities may matters far less than how vulnerabilities are distributed over hosts, or over time. Similarly, neither topology of the network nor the roles and dynamics of inter-host interactions are considered by simple sums of vulnerabilities or missing patches.

</details>

