# 2013

## TOC

- [2013-01](#2013-01)
- [2013-02](#2013-02)
- [2013-03](#2013-03)
- [2013-04](#2013-04)
- [2013-05](#2013-05)
- [2013-06](#2013-06)
- [2013-07](#2013-07)
- [2013-08](#2013-08)
- [2013-09](#2013-09)
- [2013-10](#2013-10)
- [2013-11](#2013-11)
- [2013-12](#2013-12)

## 2013-01

<details>

<summary>2013-01-15 18:47:11 - Pooling-Invariant Image Feature Learning</summary>

- *Yangqing Jia, Oriol Vinyals, Trevor Darrell*

- `1302.5056v1` - [abs](http://arxiv.org/abs/1302.5056v1) - [pdf](http://arxiv.org/pdf/1302.5056v1)

> Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patch-based dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size.

</details>

<details>

<summary>2013-01-16 10:12:37 - Regularized Discriminant Embedding for Visual Descriptor Learning</summary>

- *Kye-Hyeon Kim, Rui Cai, Lei Zhang, Seungjin Choi*

- `1301.3644v1` - [abs](http://arxiv.org/abs/1301.3644v1) - [pdf](http://arxiv.org/pdf/1301.3644v1)

> Images can vary according to changes in viewpoint, resolution, noise, and illumination. In this paper, we aim to learn representations for an image, which are robust to wide changes in such environmental conditions, using training pairs of matching and non-matching local image patches that are collected under various environmental conditions. We present a regularized discriminant analysis that emphasizes two challenging categories among the given training pairs: (1) matching, but far apart pairs and (2) non-matching, but close pairs in the original feature space (e.g., SIFT feature space). Compared to existing work on metric learning and discriminant analysis, our method can better distinguish relevant images from irrelevant, but look-alike images.

</details>


## 2013-02

<details>

<summary>2013-02-01 10:20:15 - Proceedings of the 12th International Colloquium on Implementation of Constraint and LOgic Programming Systems</summary>

- *Nicos Angelopoulos, Roberto Bagnara*

- `1302.0126v1` - [abs](http://arxiv.org/abs/1302.0126v1) - [pdf](http://arxiv.org/pdf/1302.0126v1)

> This volume contains the papers presented at CICLOPS'12: 12th International Colloquium on Implementation of Constraint and LOgic Programming Systems held on Tueseday September 4th, 2012 in Budapest.   The program included 1 invited talk, 9 technical presentations and a panel discussion on Prolog open standards (open.pl). Each programme paper was reviewed by 3 reviewers.   CICLOPS'12 continues a tradition of successful workshops on Implementations of Logic Programming Systems, previously held in Budapest (1993) and Ithaca (1994), the Compulog Net workshops on Parallelism and Implementation Technologies held in Madrid (1993 and 1994), Utrecht (1995) and Bonn (1996), the Workshop on Parallelism and Implementation Technology for (Constraint) Logic Programming Languages held in Port Jefferson (1997), Manchester (1998), Las Cruces (1999), and London (2000), and more recently the Colloquium on Implementation of Constraint and LOgic Programming Systems in Paphos (2001), Copenhagen (2002), Mumbai (2003), Saint Malo (2004), Sitges (2005), Seattle (2006), Porto (2007), Udine (2008), Pasadena (2009), Edinburgh (2010) - together with WLPE, Lexington (2011).   We would like to thank all the authors, Tom Schrijvers for his invited talk, the programme committee members, and the ICLP 2012 organisers. We would like to also thank arXiv.org for providing permanent hosting.

</details>


## 2013-03

<details>

<summary>2013-03-04 10:41:34 - Boltzmann Machines and Denoising Autoencoders for Image Denoising</summary>

- *Kyunghyun Cho*

- `1301.3468v6` - [abs](http://arxiv.org/abs/1301.3468v6) - [pdf](http://arxiv.org/pdf/1301.3468v6)

> Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.

</details>

<details>

<summary>2013-03-12 21:41:59 - A local constant-factor approximation algorithm for MDS problem in anonymous network</summary>

- *Wojciech Wawrzyniak*

- `1303.2514v2` - [abs](http://arxiv.org/abs/1303.2514v2) - [pdf](http://arxiv.org/pdf/1303.2514v2)

> In research on distributed local algorithms it is commonly assumed that each vertex has a unique identifier in the entire graph. However, it turns out that in case of certain classes of graphs (for example not lift-closed bounded degree graphs) identifiers are unnecessary and only a port ordering is needed. One of the open issues was whether identifiers are essential in planar graphs. In this paper, we answer this question and we propose an algorithm which returns constant approximation of the MDS problem in CONGEST model. The algorithm doesn't use any additional information about the structure of the graph and the nodes don't have unique identifiers. We hope that this paper will be very helpful as a hint for further comparisons of the unique identifier model and the model with only a port numbering in other classes of graphs.

</details>

<details>

<summary>2013-03-21 12:40:05 - Separable Dictionary Learning</summary>

- *Simon Hawe, Matthias Seibert, Martin Kleinsteuber*

- `1303.5244v1` - [abs](http://arxiv.org/abs/1303.5244v1) - [pdf](http://arxiv.org/pdf/1303.5244v1)

> Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary properties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.

</details>


## 2013-04

<details>

<summary>2013-04-21 23:19:28 - Nonsingular Efficient Modeling of Rotations in 3-space using three components</summary>

- *Norman J. Goldstein*

- `1005.4661v2` - [abs](http://arxiv.org/abs/1005.4661v2) - [pdf](http://arxiv.org/pdf/1005.4661v2)

> This article introduces yet another representation of rotations in 3-space. The rotations form a 3-dimensional projective space, which fact has not been exploited in Computer Science. We use the four affine patches of this projective space to parametrize the rotations. This affine patch representation is more compact than quaternions (which require 4 components for calculations), encompasses the entire rotation group without singularities (unlike the Euler angles and rotation vector approaches), and requires only ratios of linear or quadratic polynomials for basic computations (unlike the Euler angles and rotation vector approaches which require transcendental functions).   As an example, we derive the differential equation for the integration of angular velocity using this affine patch representation of rotations. We remark that the complexity of this equation is the same as the corresponding quaternion equation, but has advantages over the quaternion approach e.g. renormalization to unit length is not required, and state space has no dead directions.

</details>

<details>

<summary>2013-04-25 14:26:04 - Unsupervised Feature Learning for low-level Local Image Descriptors</summary>

- *Christian Osendorfer, Justin Bayer, Sebastian Urban, Patrick van der Smagt*

- `1301.2840v4` - [abs](http://arxiv.org/abs/1301.2840v4) - [pdf](http://arxiv.org/pdf/1301.2840v4)

> Unsupervised feature learning has shown impressive results for a wide range of input modalities, in particular for object classification tasks in computer vision. Using a large amount of unlabeled data, unsupervised feature learning methods are utilized to construct high-level representations that are discriminative enough for subsequently trained supervised classification algorithms. However, it has never been \emph{quantitatively} investigated yet how well unsupervised learning methods can find \emph{low-level representations} for image patches without any additional supervision. In this paper we examine the performance of pure unsupervised methods on a low-level correspondence task, a problem that is central to many Computer Vision applications. We find that a special type of Restricted Boltzmann Machines (RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple binarization scheme produces compact representations that perform better than several state-of-the-art descriptors.

</details>

<details>

<summary>2013-04-30 04:15:02 - ACL2 Meets the GPU: Formalizing a CUDA-based Parallelizable All-Pairs Shortest Path Algorithm in ACL2</summary>

- *David S. Hardin, Samuel S. Hardin*

- `1304.7863v1` - [abs](http://arxiv.org/abs/1304.7863v1) - [pdf](http://arxiv.org/pdf/1304.7863v1)

> As Graphics Processing Units (GPUs) have gained in capability and GPU development environments have matured, developers are increasingly turning to the GPU to off-load the main host CPU of numerically-intensive, parallelizable computations. Modern GPUs feature hundreds of cores, and offer programming niceties such as double-precision floating point, and even limited recursion. This shift from CPU to GPU, however, raises the question: how do we know that these new GPU-based algorithms are correct?   In order to explore this new verification frontier, we formalized a parallelizable all-pairs shortest path (APSP) algorithm for weighted graphs, originally coded in NVIDIA's CUDA language, in ACL2. The ACL2 specification is written using a single-threaded object (stobj) and tail recursion, as the stobj/tail recursion combination yields the most straightforward translation from imperative programming languages, as well as efficient, scalable executable specifications within ACL2 itself. The ACL2 version of the APSP algorithm can process millions of vertices and edges with little to no garbage generation, and executes at one-sixth the speed of a host-based version of APSP coded in C- a very respectable result for a theorem prover.   In addition to formalizing the APSP algorithm (which uses Dijkstra's shortest path algorithm at its core), we have also provided capability that the original APSP code lacked, namely shortest path recovery. Path recovery is accomplished using a secondary ACL2 stobj implementing a LIFO stack, which is proven correct. To conclude the experiment, we ported the ACL2 version of the APSP kernels back to C, resulting in a less than 5% slowdown, and also performed a partial back-port to CUDA, which, surprisingly, yielded a slight performance increase.

</details>


## 2013-05

<details>

<summary>2013-05-08 20:14:03 - Semantic-based Anomalous Pattern Discovery in Moving Object Trajectories</summary>

- *Elena Camossi, Paola Villa, Luca Mazzola*

- `1305.1946v1` - [abs](http://arxiv.org/abs/1305.1946v1) - [pdf](http://arxiv.org/pdf/1305.1946v1)

> In this work, we investigate a novel semantic approach for pattern discovery in trajectories that, relying on ontologies, enhances object movement information with event semantics. The approach can be applied to the detection of movement patterns and behaviors whenever the semantics of events occurring along the trajectory is, explicitly or implicitly, available. In particular, we tested it against an exacting case scenario in maritime surveillance, i.e., the discovery of suspicious container transportations.   The methodology we have developed entails the formalization of the application domain through a domain ontology, extending the Moving Object Ontology (MOO) described in this paper. Afterwards, movement patterns have to be formalized, either as Description Logic (DL) axioms or queries, enabling the retrieval of the trajectories that follow the patterns.   In our experimental evaluation, we have considered a real world dataset of 18 Million of container events describing the deed undertaken in a port to accomplish the shipping (e.g., loading on a vessel, export operation). Leveraging events, we have reconstructed almost 300 thousand container trajectories referring to 50 thousand containers travelling along three years. We have formalized the anomalous itinerary patterns as DL axioms, testing different ontology APIs and DL reasoners to retrieve the suspicious transportations.   Our experiments demonstrate that the approach is feasible and efficient. In particular, the joint use of Pellet and SPARQL-DL enables to detect the trajectories following a given pattern in a reasonable time with big size datasets.

</details>

<details>

<summary>2013-05-12 02:44:15 - Practical Fine-grained Privilege Separation in Multithreaded Applications</summary>

- *Jun Wang, Xi Xiong, Peng Liu*

- `1305.2553v1` - [abs](http://arxiv.org/abs/1305.2553v1) - [pdf](http://arxiv.org/pdf/1305.2553v1)

> An inherent security limitation with the classic multithreaded programming model is that all the threads share the same address space and, therefore, are implicitly assumed to be mutually trusted. This assumption, however, does not take into consideration of many modern multithreaded applications that involve multiple principals which do not fully trust each other. It remains challenging to retrofit the classic multithreaded programming model so that the security and privilege separation in multi-principal applications can be resolved.   This paper proposes ARBITER, a run-time system and a set of security primitives, aimed at fine-grained and data-centric privilege separation in multithreaded applications. While enforcing effective isolation among principals, ARBITER still allows flexible sharing and communication between threads so that the multithreaded programming paradigm can be preserved. To realize controlled sharing in a fine-grained manner, we created a novel abstraction named ARBITER Secure Memory Segment (ASMS) and corresponding OS support. Programmers express security policies by labeling data and principals via ARBITER's API following a unified model. We ported a widely-used, in-memory database application (memcached) to ARBITER system, changing only around 100 LOC. Experiments indicate that only an average runtime overhead of 5.6% is induced to this security enhanced version of application.

</details>

<details>

<summary>2013-05-27 05:40:01 - Modelling and Refinement in CODA</summary>

- *Michael Butler, John Colley, Andrew Edmunds, Colin Snook, Neil Evans, Neil Grant, Helen Marshall*

- `1305.6112v1` - [abs](http://arxiv.org/abs/1305.6112v1) - [pdf](http://arxiv.org/pdf/1305.6112v1)

> This paper provides an overview of the CODA framework for modelling and refinement of component-based embedded systems. CODA is an extension of Event-B and UML-B and is supported by a plug-in for the Rodin toolset. CODA augments Event-B with constructs for component-based modelling including components, communications ports, port connectors, timed communications and timing triggers. Component behaviour is specified through a combination of UML-B state machines and Event-B. CODA communications and timing are given an Event-B semantics through translation rules. Refinement is based on Event-B refinement and allows layered construction of CODA models in a consistent way.

</details>

<details>

<summary>2013-05-30 15:46:43 - Modelling and Analysing Cargo Screening Processes: A Project Outline</summary>

- *Peer-Olaf Siebers, Uwe Aickelin, David Menachof, Galina Sherman, Peter Zimmerman*

- `1305.7145v1` - [abs](http://arxiv.org/abs/1305.7145v1) - [pdf](http://arxiv.org/pdf/1305.7145v1)

> The efficiency of current cargo screening processes at sea and air ports is unknown as no benchmarks exists against which they could be measured. Some manufacturer benchmarks exist for individual sensors but we have not found any benchmarks that take a holistic view of the screening procedures assessing a combination of sensors and also taking operator variability into account. Just adding up resources and manpower used is not an effective way for assessing systems where human decision-making and operator compliance to rules play a vital role. For such systems more advanced assessment methods need to be used, taking into account that the cargo screening process is of a dynamic and stochastic nature. Our project aim is to develop a decision support tool (cargo-screening system simulator) that will map the right technology and manpower to the right commodity-threat combination in order to maximize detection rates. In this paper we present a project outline and highlight the research challenges we have identified so far. In addition we introduce our first case study, where we investigate the cargo screening process at the ferry port in Calais.

</details>

<details>

<summary>2013-05-31 14:36:59 - Evaluating Different Cost-Benefit Analysis Methods for Port Security Operations</summary>

- *Galina Sherman, Peer-Olaf Siebers, David Menachof, Uwe Aickelin*

- `1305.7422v1` - [abs](http://arxiv.org/abs/1305.7422v1) - [pdf](http://arxiv.org/pdf/1305.7422v1)

> Service industries, such as ports, are attentive to their standards, a smooth service flow and economic viability. Cost benefit analysis has proven itself as a useful tool to support this type of decision making; it has been used by businesses and governmental agencies for many years. In this book chapter we demonstrate different modelling methods that are used for estimating input factors required for conducting cost benefit analysis based on a single case study. These methods are: scenario analysis, decision trees, Monte-Carlo simulation modelling and discrete event simulation modelling. Our aims are, on the one hand, to guide the analyst through the modelling processes and, on the other hand, to demonstrate what additional decision support information can be obtained from applying each of these modelling methods.

</details>

<details>

<summary>2013-05-31 15:40:49 - Validation of a Microsimulation of the Port of Dover</summary>

- *Chris Roadknight, Uwe Aickelin, Galina Sherman*

- `1305.7458v1` - [abs](http://arxiv.org/abs/1305.7458v1) - [pdf](http://arxiv.org/pdf/1305.7458v1)

> Modelling and simulating the traffic of heavily used but secure environments such as seaports and airports is of increasing importance. Errors made when simulating these environments can have long standing economic, social and environmental implications. This paper discusses issues and problems that may arise when designing a simulation strategy. Data for the Port is presented, methods for lightweight vehicle assessment that can be used to calibrate and validate simulations are also discussed along with a diagnosis of overcalibration issues. We show that decisions about where the intelligence lies in a system has important repercussions for the reliability of system statistics. Finally, conclusions are drawn about how microsimulations can be moved forward as a robust planning tool for the 21st century.

</details>


## 2013-06

<details>

<summary>2013-06-06 19:09:18 - SPATA: A Seeding and Patching Algorithm for Hybrid Transcriptome Assembly</summary>

- *Tin Chi Nguyen, Zhiyu Zhao, Dongxiao Zhu*

- `1306.1511v1` - [abs](http://arxiv.org/abs/1306.1511v1) - [pdf](http://arxiv.org/pdf/1306.1511v1)

> Transcriptome assembly from RNA-Seq reads is an active area of bioinformatics research. The ever-declining cost and the increasing depth of RNA-Seq have provided unprecedented opportunities to better identify expressed transcripts. However, the nonlinear transcript structures and the ultra-high throughput of RNA-Seq reads pose significant algorithmic and computational challenges to the existing transcriptome assembly approaches, either reference-guided or de novo. While reference-guided approaches offer good sensitivity, they rely on alignment results of the splice-aware aligners and are thus unsuitable for species with incomplete reference genomes. In contrast, de novo approaches do not depend on the reference genome but face a computational daunting task derived from the complexity of the graph built for the whole transcriptome. In response to these challenges, we present a hybrid approach to exploit an incomplete reference genome without relying on splice-aware aligners. We have designed a split-and-align procedure to efficiently localize the reads to individual genomic loci, which is followed by an accurate de novo assembly to assemble reads falling into each locus. Using extensive simulation data, we demonstrate a high accuracy and precision in transcriptome reconstruction by comparing to selected transcriptome assembly tools. Our method is implemented in assemblySAM, a GUI software freely available at http://sammate.sourceforge.net.

</details>

<details>

<summary>2013-06-11 10:17:18 - A Systematically Empirical Evaluation of Vulnerability Discovery Models: a Study on Browsers' Vulnerabilities</summary>

- *Viet Hung Nguyen, Fabio Massacci*

- `1306.2476v1` - [abs](http://arxiv.org/abs/1306.2476v1) - [pdf](http://arxiv.org/pdf/1306.2476v1)

> A precise vulnerability discovery model (VDM) will provide a useful insight to assess software security, and could be a good prediction instrument for both software vendors and users to understand security trends and plan ahead patching schedule accordingly. Thus far, several models have been proposed and validated. Yet, no systematically independent validation by somebody other than the author exists. Furthermore, there are a number of issues that might bias previous studies in the field. In this work, we fill in the gap by introducing an empirical methodology that systematically evaluates the performance of a VDM in two aspects: quality and predictability. We further apply this methodology to assess existing VDMs. The results show that some models should be rejected outright, while some others might be adequate to capture the discovery process of vulnerabilities. We also consider different usage scenarios of VDMs and find that the simplest linear model is the most appropriate choice in terms of both quality and predictability when browsers are young. Otherwise, logistics-based models are better choices.

</details>

<details>

<summary>2013-06-19 08:21:21 - Modeling a repository of modules for ports Terminals Operating System (TOS)</summary>

- *Ahmed Faouzi, Charif Mabrouki, Alami Semma*

- `1306.4450v1` - [abs](http://arxiv.org/abs/1306.4450v1) - [pdf](http://arxiv.org/pdf/1306.4450v1)

> The purpose of this paper is the modeling of a repository for modules and interfaces that must include all integrated information system management of a port terminal.Modules will provide a basic framework necessary for automatic management of internal operations and activities of all Port Terminals worldwide.

</details>

<details>

<summary>2013-06-21 14:47:02 - Scenario Analysis, Decision Trees and Simulation for Cost Benefit Analysis of the Cargo Screening Process</summary>

- *Galina Sherman, Peer-Olaf Siebers, Uwe Aickelin, David Menachof*

- `1306.5158v1` - [abs](http://arxiv.org/abs/1306.5158v1) - [pdf](http://arxiv.org/pdf/1306.5158v1)

> In this paper we present our ideas for conducting a cost benefit analysis by using three different methods: scenario analysis, decision trees and simulation. Then we introduce our case study and examine these methods in a real world situation. We show how these tools can be used and what the results are for each of them. Our aim is to conduct a comparison of these different probabilistic methods of estimating costs for port security risk assessment studies. Methodologically, we are trying to understand the limits of all the tools mentioned above by focusing on rare events.

</details>

<details>

<summary>2013-06-21 14:49:29 - Towards modelling cost and risks of infrequent events in the cargo screening process</summary>

- *Galina Sherman, David Menachof, Peer-Olaf Siebers, Uwe Aickelin*

- `1306.5160v1` - [abs](http://arxiv.org/abs/1306.5160v1) - [pdf](http://arxiv.org/pdf/1306.5160v1)

> We introduce a simulation model of the port of Calais with a focus on the operation of immigration controls. Our aim is to compare the cost and benefits of different screening policies. Methodologically, we are trying to understand the limits of discrete event simulation of rare events. When will they become 'too rare' for simulation to give meaningful results?

</details>


## 2013-07

<details>

<summary>2013-07-02 16:31:44 - Comparing Decison Support Tools for Cargo Screening Processes</summary>

- *Peer-Olaf Siebers, Galina Sherman, Uwe Aickelin, David Menachof*

- `1307.0749v1` - [abs](http://arxiv.org/abs/1307.0749v1) - [pdf](http://arxiv.org/pdf/1307.0749v1)

> When planning to change operations at ports there are two key stake holders with very different interests involved in the decision making processes. Port operators are attentive to their standards, a smooth service flow and economic viability while border agencies are concerned about national security. The time taken for security checks often interferes with the compliance to service standards that port operators would like to achieve. Decision support tools as for example Cost-Benefit Analysis or Multi Criteria Analysis are useful helpers to better understand the impact of changes to a system. They allow investigating future scenarios and helping to find solutions that are acceptable for all parties involved in port operations. In this paper we evaluate two different modelling methods, namely scenario analysis and discrete event simulation. These are useful for driving the decision support tools (i.e. they provide the inputs the decision support tools require). Our aims are, on the one hand, to guide the reader through the modelling processes and, on the other hand, to demonstrate what kind of decision support information one can obtain from the different modelling methods presented.

</details>

<details>

<summary>2013-07-05 12:46:33 - Extending a Microsimulation of the Port of Dover</summary>

- *Christopher M. Roadknight, Uwe Aickelin*

- `1307.1598v1` - [abs](http://arxiv.org/abs/1307.1598v1) - [pdf](http://arxiv.org/pdf/1307.1598v1)

> Modelling and simulating the traffic of heavily used but secure environments such as seaports and airports is of increasing importance. This paper discusses issues and problems that may arise when extending an existing microsimulation strategy. We also discuss how extensions of these simulations can aid planners with optimal physical and operational feedback. Conclusions are drawn about how microsimulations can be moved forward as a robust planning tool for the 21st century.

</details>

<details>

<summary>2013-07-15 14:55:46 - CUDA Leaks: Information Leakage in GPU Architectures</summary>

- *Roberto Di Pietro, Flavio Lombardi, Antonio Villani*

- `1305.7383v2` - [abs](http://arxiv.org/abs/1305.7383v2) - [pdf](http://arxiv.org/pdf/1305.7383v2)

> Graphics Processing Units (GPUs) are deployed on most present server, desktop, and even mobile platforms. Nowadays, a growing number of applications leverage the high parallelism offered by this architecture to speed-up general purpose computation. This phenomenon is called GPGPU computing (General Purpose GPU computing). The aim of this work is to discover and highlight security issues related to CUDA, the most widespread platform for GPGPU computing. In particular, we provide details and proofs-of-concept about a novel set of vulnerabilities CUDA architectures are subject to, that could be exploited to cause severe information leak. Following (detailed) intuitions rooted on sound engineering security, we performed several experiments targeting the last two generations of CUDA devices: Fermi and Kepler. We discovered that these two families do suffer from information leakage vulnerabilities. In particular, some vulnerabilities are shared between the two architectures, while others are idiosyncratic of the Kepler architecture. As a case study, we report the impact of one of these vulnerabilities on a GPU implementation of the AES encryption algorithm. We also suggest software patches and alternative approaches to tackle the presented vulnerabilities. To the best of our knowledge this is the first work showing that information leakage in CUDA is possible using just standard CUDA instructions. We expect our work to pave the way for further research in the field.

</details>


## 2013-08

<details>

<summary>2013-08-08 16:07:15 - Strand-Based Approach to Patch Security Protocols</summary>

- *Dieter Hutter, Raul Monroy*

- `1308.1888v1` - [abs](http://arxiv.org/abs/1308.1888v1) - [pdf](http://arxiv.org/pdf/1308.1888v1)

> In this paper, we introduce a mechanism that aims to speed up the development cycle of security protocols, by adding automated aid for diagnosis and repair. Our mechanism relies on existing verification tools analyzing intermediate protocols and synthesizing potential attacks if the protocol is flawed. The analysis of these attacks (including type flaw attacks) pinpoints the source of the failure and controls the synthesis of appropriate patches to the protocol. Using strand spaces, we have developed general guidelines for protocol repair, and captured them into formal requirements on (sets of) protocol steps. For each requirement, there is a collection of rules that transform a set of protocol steps violating the requirement into a set conforming it. We have implemented our mechanism into a tool, called SHRIMP. We have successfully tested SHRIMP on numerous faulty protocols, all of which were successfully repaired, fully automatically.

</details>

<details>

<summary>2013-08-22 16:16:31 - Reinforcement learning for port-Hamiltonian systems</summary>

- *Olivier Sprangers, Gabriel A. D. Lopes, Robert Babuska*

- `1212.5524v2` - [abs](http://arxiv.org/abs/1212.5524v2) - [pdf](http://arxiv.org/pdf/1212.5524v2)

> Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning approach into the energy-balancing passivity-based control (EB-PBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems's PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closed-loop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.

</details>

<details>

<summary>2013-08-29 10:04:30 - Development of a language and its enacting engine for the unified discovery of heterogeneous services</summary>

- *Michael Pantazoglou*

- `1308.6413v1` - [abs](http://arxiv.org/abs/1308.6413v1) - [pdf](http://arxiv.org/pdf/1308.6413v1)

> Service orientation fosters a high-level model for distributed applications development, which is based on the discovery, composition and reuse of existing software services. However, the heterogeneity among current service-oriented technologies renders the important task of service discovery tedious and ineffective. This dissertation proposes a new approach to address this challenge. Specifically, it contributes a framework supporting the unified discovery of heterogeneous services, with a focus on web, peer-to-peer, and grid services. The framework comprises a service query language and its enacting service discovery engine. Overall, the proposed solution is characterized by generality and flexibility, which are ensured by appropriate abstractions, extension points, and their sup- porting mechanisms. The viability, performance, and effectiveness of the proposed framework are demonstrated by experimental measurements.

</details>


## 2013-09

<details>

<summary>2013-09-30 00:48:02 - Retargeting GCC: Do We Reinvent the Wheel Every Time?</summary>

- *Saravana Perumal P, Amey Karkare*

- `1309.7685v1` - [abs](http://arxiv.org/abs/1309.7685v1) - [pdf](http://arxiv.org/pdf/1309.7685v1)

> Porting GCC to new architecture requires writing a Machine Description (MD) file that contains mapping from GCC's intermediate form to the target assembly code. Constructing an MD file is a difficult task because it requires the user to understand both (a) the internals of GCC, and (b) the intricacies of the target architecture. Instruction sets of different architectures exhibit significant amount of semantic similarities across a large class (for example, the instruction sets for RISC architectures) and differ only in syntax. Therefore, it is expected that MD files of machines with similar architectures should also have similarities. To confirm our hypothesis, we created "mdcompare", a tool to (a) extract RTL patterns (machine independent abstraction of RTL templates) from MD files of well known architectures and (b) compare the similarity of patterns across architectures. The results are encouraging; we found that 28% -- 70% RTL expressions are similar across pairs of MD files, the similarity percentage being on the higher side for pairs of similar architectures.

</details>


## 2013-10

<details>

<summary>2013-10-01 02:45:20 - Structured learning of sum-of-submodular higher order energy functions</summary>

- *Alexander Fix, Thorsten Joachims, Sam Park, Ramin Zabih*

- `1309.7512v2` - [abs](http://arxiv.org/abs/1309.7512v2) - [pdf](http://arxiv.org/pdf/1309.7512v2)

> Submodular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow \cite{KZ:PAMI04} has had significant impact in computer vision \cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we address the important class of sum-of-submodular (SoS) functions \cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via a variant of max flow called submodular flow \cite{Edmonds:ADM77}. SoS functions can naturally express higher order priors involving, e.g., local image patches; however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach \cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the training problem in terms of quadratic programming; as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems \cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flow problem. Experimental comparisons are made against the OpenCV implementation of the GrabCut interactive segmentation technique \cite{Rother:GrabCut04}, which uses hand-tuned parameters instead of machine learning. On a standard dataset \cite{Gulshan:CVPR10} our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels.

</details>

<details>

<summary>2013-10-06 14:27:59 - Impacting the bioscience progress by backporting software for Bio-Linux</summary>

- *Sasa Paporovic*

- `1310.1588v1` - [abs](http://arxiv.org/abs/1310.1588v1) - [pdf](http://arxiv.org/pdf/1310.1588v1)

> In year 2006 Bio-Linux with the work of Tim Booth and team gives its rising and provide an operating system that was and still specialized in providing a bioinformatic specific software environment for the working needs in this corner of bioscience. It is shown that Bio-Linux is affected by a 2 year release cycle and with this the final releases of Bio-Linux will not have the latest bioinformatic software on board. The paper shows how to get around this huge time gap and bring new software for Bio-Linux on board through a process that is called backporting. A summary of within the work to this paper just backported bioinformatic tools is given. A describtion of a workflow for continuously integration of the newest bioinformatic tools gives an outlook to further concrete planned developments and the influence of speeding up scientific progress.

</details>

<details>

<summary>2013-10-09 06:08:38 - The Unified Approach For Organizational Network Vulnerability Assessment</summary>

- *Mrs. Dhanamma Jagli, Mrs. Rohini Temkar*

- `1310.2365v1` - [abs](http://arxiv.org/abs/1310.2365v1) - [pdf](http://arxiv.org/pdf/1310.2365v1)

> The present business network infrastructure is quickly varying with latest servers, services, connections, and ports added often, at times day by day, and with a uncontrollably inflow of laptops, storage media and wireless networks. With the increasing amount of vulnerabilities and exploits coupled with the recurrent evolution of IT infrastructure, organizations at present require more numerous vulnerability assessments. In this paper new approach the Unified process for Network vulnerability Assessments hereafter called as a unified NVA is proposed for network vulnerability assessment derived from Unified Software Development Process or Unified Process, it is a popular iterative and incremental software development process framework.

</details>


## 2013-11

<details>

<summary>2013-11-01 19:02:57 - Application of Cybernetics and Control Theory for a New Paradigm in Cybersecurity</summary>

- *Michael D. Adams, Seth D. Hitefield, Bruce Hoy, Michael C. Fowler, T. Charles Clancy*

- `1311.0257v1` - [abs](http://arxiv.org/abs/1311.0257v1) - [pdf](http://arxiv.org/pdf/1311.0257v1)

> A significant limitation of current cyber security research and techniques is its reactive and applied nature. This leads to a continuous 'cyber cycle' of attackers scanning networks, developing exploits and attacking systems, with defenders detecting attacks, analyzing exploits and patching systems. This reactive nature leaves sensitive systems highly vulnerable to attack due to un-patched systems and undetected exploits. Some current research attempts to address this major limitation by introducing systems that implement moving target defense. However, these ideas are typically based on the intuition that a moving target defense will make it much harder for attackers to find and scan vulnerable systems, and not on theoretical mathematical foundations. The continuing lack of fundamental science and principles for developing more secure systems has drawn increased interest into establishing a 'science of cyber security'. This paper introduces the concept of using cybernetics, an interdisciplinary approach of control theory, systems theory, information theory and game theory applied to regulatory systems, as a foundational approach for developing cyber security principles. It explores potential applications of cybernetics to cyber security from a defensive perspective, while suggesting the potential use for offensive applications. Additionally, this paper introduces the fundamental principles for building non-stationary systems, which is a more general solution than moving target defenses. Lastly, the paper discusses related works concerning the limitations of moving target defense and one implementation based on non-stationary principles.

</details>

<details>

<summary>2013-11-20 16:52:07 - Android Based Emergency Alert Button</summary>

- *Rupam Kumar Sharma, Dhrubajyoti gogoi*

- `1311.5133v1` - [abs](http://arxiv.org/abs/1311.5133v1) - [pdf](http://arxiv.org/pdf/1311.5133v1)

> Android is a java based operating system which runs on the Linux kernel. It is lightweight and full featured. Android applications are developed using Java and can be ported to new platform easily thereby fostering huge number of useful mobile applications. This paper describes about a SOS application being developed and its successful implementation with tested results. The application has target users those sections of the people who surprisingly falls into a situation where instant communication of their whereabouts becomes indispensable to be informed to certain authorized persons at remote end

</details>

<details>

<summary>2013-11-26 17:32:56 - Supervised Texture Classification Using a Novel Compression-Based Similarity Measure</summary>

- *Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel*

- `1207.3071v2` - [abs](http://arxiv.org/abs/1207.3071v2) - [pdf](http://arxiv.org/pdf/1207.3071v2)

> Supervised pixel-based texture classification is usually performed in the feature space. We propose to perform this task in (dis)similarity space by introducing a new compression-based (dis)similarity measure. The proposed measure utilizes two dimensional MPEG-1 encoder, which takes into consideration the spatial locality and connectivity of pixels in the images. The proposed formulation has been carefully designed based on MPEG encoder functionality. To this end, by design, it solely uses P-frame coding to find the (dis)similarity among patches/images. We show that the proposed measure works properly on both small and large patch sizes. Experimental results show that the proposed approach significantly improves the performance of supervised pixel-based texture classification on Brodatz and outdoor images compared to other compression-based dissimilarity measures as well as approaches performed in feature space. It also improves the computation speed by about 40% compared to its rivals.

</details>


## 2013-12

<details>

<summary>2013-12-20 00:21:36 - Unsupervised Feature Learning by Deep Sparse Coding</summary>

- *Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi*

- `1312.5783v1` - [abs](http://arxiv.org/abs/1312.5783v1) - [pdf](http://arxiv.org/pdf/1312.5783v1)

> In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.

</details>

