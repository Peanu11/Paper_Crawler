# 2010

## TOC

- [2010-01](#2010-01)
- [2010-02](#2010-02)
- [2010-03](#2010-03)
- [2010-06](#2010-06)
- [2010-07](#2010-07)
- [2010-08](#2010-08)
- [2010-10](#2010-10)
- [2010-11](#2010-11)

## 2010-01

<details>

<summary>2010-01-14 10:41:00 - Dendritic Cells for Real-Time Anomaly Detection</summary>

- *Julie Greensmith, Uwe Aickelin*

- `1001.2405v1` - [abs](http://arxiv.org/abs/1001.2405v1) - [pdf](http://arxiv.org/pdf/1001.2405v1)

> Dendritic Cells (DCs) are innate immune system cells which have the power to activate or suppress the immune system. The behaviour of human of human DCs is abstracted to form an algorithm suitable for anomaly detection. We test this algorithm on the real-time problem of port scan detection. Our results show a significant difference in artificial DC behaviour for an outgoing portscan when compared to behaviour for normal processes.

</details>


## 2010-02

<details>

<summary>2010-02-01 15:53:04 - Dendritic Cells for SYN Scan Detection</summary>

- *Julie Greensmith, Uwe Aickelin*

- `1002.0276v1` - [abs](http://arxiv.org/abs/1002.0276v1) - [pdf](http://arxiv.org/pdf/1002.0276v1)

> Artificial immune systems have previously been applied to the problem of intrusion detection. The aim of this research is to develop an intrusion detection system based on the function of Dendritic Cells (DCs). DCs are antigen presenting cells and key to activation of the human immune system, behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA). In algorithmic terms, individual DCs perform multi-sensor data fusion, asynchronously correlating the the fused data signals with a secondary data stream. Aggregate output of a population of cells, is analysed and forms the basis of an anomaly detection system. In this paper the DCA is applied to the detection of outgoing port scans using TCP SYN packets. Results show that detection can be achieved with the DCA, yet some false positives can be encountered when simultaneously scanning and using other network services. Suggestions are made for using adaptive signals to alleviate this uncovered problem.

</details>


## 2010-03

<details>

<summary>2010-03-03 12:04:01 - Information Fusion for Anomaly Detection with the Dendritic Cell Algorithm</summary>

- *Julie Greensmith, Uwe Aickelin, Gianni Tedesco*

- `1003.0789v1` - [abs](http://arxiv.org/abs/1003.0789v1) - [pdf](http://arxiv.org/pdf/1003.0789v1)

> Dendritic cells are antigen presenting cells that provide a vital link between the innate and adaptive immune system, providing the initial detection of pathogenic invaders. Research into this family of cells has revealed that they perform information fusion which directs immune responses. We have derived a Dendritic Cell Algorithm based on the functionality of these cells, by modelling the biological signals and differentiation pathways to build a control mechanism for an artificial immune system. We present algorithmic details in addition to experimental results, when the algorithm was applied to anomaly detection for the detection of port scans. The results show the Dendritic Cell Algorithm is sucessful at detecting port scans.

</details>

<details>

<summary>2010-03-22 15:32:48 - Development of a Cargo Screening Process Simulator: A First Approach</summary>

- *Peer-Olaf Siebers, Galina Sherman, Uwe Aickelin*

- `1003.4196v1` - [abs](http://arxiv.org/abs/1003.4196v1) - [pdf](http://arxiv.org/pdf/1003.4196v1)

> The efficiency of current cargo screening processes at sea and air ports is largely unknown as few benchmarks exists against which they could be measured. Some manufacturers provide benchmarks for individual sensors but we found no benchmarks that take a holistic view of the overall screening procedures and no benchmarks that take operator variability into account. Just adding up resources and manpower used is not an effective way for assessing systems where human decision-making and operator compliance to rules play a vital role. Our aim is to develop a decision support tool (cargo-screening system simulator) that will map the right technology and manpower to the right commodity-threat combination in order to maximise detection rates. In this paper we present our ideas for developing such a system and highlight the research challenges we have identified. Then we introduce our first case study and report on the progress we have made so far.

</details>

<details>

<summary>2010-03-28 18:06:53 - How to prevent type-flaw attacks on security protocols under algebraic properties</summary>

- *Sreekanth Malladi, Pascal Lafourcade*

- `1003.5385v1` - [abs](http://arxiv.org/abs/1003.5385v1) - [pdf](http://arxiv.org/pdf/1003.5385v1)

> Type-flaw attacks upon security protocols wherein agents are led to misinterpret message types have been reported frequently in the literature. Preventing them is crucial for protocol security and verification. Heather et al. proved that tagging every message field with it's type prevents all type-flaw attacks under a free message algebra and perfect encryption system. In this paper, we prove that type-flaw attacks can be prevented with the same technique even under the ACUN algebraic properties of XOR which is commonly used in "real-world" protocols such as SSL 3.0. Our proof method is general and can be easily extended to other monoidal operators that possess properties such as Inverse and Idempotence as well. We also discuss how tagging could be used to prevent type-flaw attacks under other properties such as associativity of pairing, commutative encryption, prefix property and homomorphic encryption.

</details>


## 2010-06

<details>

<summary>2010-06-08 10:07:34 - The Deterministic Dendritic Cell Algorithm</summary>

- *Julie Greensmith, Uwe Aickelin*

- `1006.1512v1` - [abs](http://arxiv.org/abs/1006.1512v1) - [pdf](http://arxiv.org/pdf/1006.1512v1)

> The Dendritic Cell Algorithm is an immune-inspired algorithm orig- inally based on the function of natural dendritic cells. The original instantiation of the algorithm is a highly stochastic algorithm. While the performance of the algorithm is good when applied to large real-time datasets, it is difficult to anal- yse due to the number of random-based elements. In this paper a deterministic version of the algorithm is proposed, implemented and tested using a port scan dataset to provide a controllable system. This version consists of a controllable amount of parameters, which are experimented with in this paper. In addition the effects are examined of the use of time windows and variation on the number of cells, both which are shown to influence the algorithm. Finally a novel metric for the assessment of the algorithms output is introduced and proves to be a more sensitive metric than the metric used with the original Dendritic Cell Algorithm.

</details>

<details>

<summary>2010-06-08 10:41:56 - The DCA:SOMe Comparison A comparative study between two biologically-inspired algorithms</summary>

- *Julie Greensmith, Jan Feyereisl, Uwe Aickelin*

- `1006.1518v1` - [abs](http://arxiv.org/abs/1006.1518v1) - [pdf](http://arxiv.org/pdf/1006.1518v1)

> The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm, developed for the purpose of anomaly detection. The algorithm performs multi-sensor data fusion and correlation which results in a 'context aware' detection system. Previous applications of the DCA have included the detection of potentially malicious port scanning activity, where it has produced high rates of true positives and low rates of false positives. In this work we aim to compare the performance of the DCA and of a Self-Organizing Map (SOM) when applied to the detection of SYN port scans, through experimental analysis. A SOM is an ideal candidate for comparison as it shares similarities with the DCA in terms of the data fusion method employed. It is shown that the results of the two systems are comparable, and both produce false positives for the same processes. This shows that the DCA can produce anomaly detection results to the same standard as an established technique.

</details>

<details>

<summary>2010-06-19 23:14:03 - Towards Plugging Privacy Leaks in Domain Name System</summary>

- *Yanbin Lu, Gene Tsudik*

- `0910.2472v3` - [abs](http://arxiv.org/abs/0910.2472v3) - [pdf](http://arxiv.org/pdf/0910.2472v3)

> Privacy leaks are an unfortunate and an integral part of the current Internet domain name resolution. Each DNS query generated by a user reveals -- to one or more DNS servers -- the origin and target of that query. Over time, a user's browsing behavior might be exposed to entities with little or no trust. Current DNS privacy leaks stem from fundamental DNS features and are not easily fixable by simple patches. Moreover, privacy issues have been overlooked by DNS security efforts (i.e. DNSSEC) and are thus likely to propagate into future versions of DNS.   In order to mitigate privacy issues in current DNS, this paper proposes a Privacy-Preserving Domain Name System (PPDNS), which maintains privacy during domain name resolution. PPDNS is based on distributed hash tables (DHTs), an alternative naming infrastructure, and computational private information retrieval (cPIR), an advanced cryptographic construct. PPDNS takes advantage of the DHT's index structure to improve name resolution query privacy, while leveraging cPIR to reduce communication overhead for bandwidth-sensitive clients. Our analysis shows that PPDNS is a viable approach for obtaining a higher degree of privacy for name resolution queries. PPDNS also serves as a demonstration of blending advanced systems techniques with their cryptographic counterparts.

</details>

<details>

<summary>2010-06-25 15:30:45 - Detecting Danger: The Dendritic Cell Algorithm</summary>

- *Julie Greensmith, Uwe Aickelin, Steve Cayzer*

- `1006.5008v1` - [abs](http://arxiv.org/abs/1006.5008v1) - [pdf](http://arxiv.org/pdf/1006.5008v1)

> The Dendritic Cell Algorithm (DCA) is inspired by the function of the dendritic cells of the human immune system. In nature, dendritic cells are the intrusion detection agents of the human body, policing the tissue and organs for potential invaders in the form of pathogens. In this research, and abstract model of DC behaviour is developed and subsequently used to form an algorithm, the DCA. The abstraction process was facilitated through close collaboration with laboratory- based immunologists, who performed bespoke experiments, the results of which are used as an integral part of this algorithm. The DCA is a population based algorithm, with each agent in the system represented as an 'artificial DC'. Each DC has the ability to combine multiple data streams and can add context to data suspected as anomalous. In this chapter the abstraction process and details of the resultant algorithm are given. The algorithm is applied to numerous intrusion detection problems in computer security including the detection of port scans and botnets, where it has produced impressive results with relatively low rates of false positives.

</details>


## 2010-07

<details>

<summary>2010-07-06 15:12:12 - Quantifying Information Leak Vulnerabilities</summary>

- *Jonathan Heusser, Pasquale Malacaria*

- `1007.0918v1` - [abs](http://arxiv.org/abs/1007.0918v1) - [pdf](http://arxiv.org/pdf/1007.0918v1)

> Leakage of confidential information represents a serious security risk. Despite a number of novel, theoretical advances, it has been unclear if and how quantitative approaches to measuring leakage of confidential information could be applied to substantial, real-world programs. This is mostly due to the high complexity of computing precise leakage quantities. In this paper, we introduce a technique which makes it possible to decide if a program conforms to a quantitative policy which scales to large state-spaces with the help of bounded model checking.   Our technique is applied to a number of officially reported information leak vulnerabilities in the Linux Kernel. Additionally, we also analysed authentication routines in the Secure Remote Password suite and of a Internet Message Support Protocol implementation. Our technique shows when there is unacceptable leakage; the same technique is also used to verify, for the first time, that the applied software patches indeed plug the information leaks.   This is the first demonstration of quantitative information flow addressing security concerns of real-world industrial programs.

</details>


## 2010-08

<details>

<summary>2010-08-19 14:00:57 - An Analysis on the Influence of Network Topologies on Local and Global Dynamics of Metapopulation Systems</summary>

- *Daniela Besozzi, Paolo Cazzaniga, Dario Pescini, Giancarlo Mauri*

- `1008.3304v1` - [abs](http://arxiv.org/abs/1008.3304v1) - [pdf](http://arxiv.org/pdf/1008.3304v1)

> Metapopulations are models of ecological systems, describing the interactions and the behavior of populations that live in fragmented habitats. In this paper, we present a model of metapopulations based on the multivolume simulation algorithm tau-DPP, a stochastic class of membrane systems, that we utilize to investigate the influence that different habitat topologies can have on the local and global dynamics of metapopulations. In particular, we focus our analysis on the migration rate of individuals among adjacent patches, and on their capability of colonizing the empty patches in the habitat. We compare the simulation results obtained for each habitat topology, and conclude the paper with some proposals for other research issues concerning metapopulations.

</details>


## 2010-10

<details>

<summary>2010-10-18 02:31:21 - Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition</summary>

- *Koray Kavukcuoglu, Marc'Aurelio Ranzato, Yann LeCun*

- `1010.3467v1` - [abs](http://arxiv.org/abs/1010.3467v1) - [pdf](http://arxiv.org/pdf/1010.3467v1)

> Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.

</details>


## 2010-11

<details>

<summary>2010-11-28 20:54:58 - In All Likelihood, Deep Belief Is Not Enough</summary>

- *Lucas Theis, Sebastian Gerwinn, Fabian Sinz, Matthias Bethge*

- `1011.6086v1` - [abs](http://arxiv.org/abs/1011.6086v1) - [pdf](http://arxiv.org/pdf/1011.6086v1)

> Statistical models of natural stimuli provide an important tool for researchers in the fields of machine learning and computational neuroscience. A canonical way to quantitatively assess and compare the performance of statistical models is given by the likelihood. One class of statistical models which has recently gained increasing popularity and has been applied to a variety of complex data are deep belief networks. Analyses of these models, however, have been typically limited to qualitative analyses based on samples due to the computationally intractable nature of the model likelihood. Motivated by these circumstances, the present article provides a consistent estimator for the likelihood that is both computationally tractable and simple to apply in practice. Using this estimator, a deep belief network which has been suggested for the modeling of natural image patches is quantitatively investigated and compared to other models of natural image patches. Contrary to earlier claims based on qualitative results, the results presented in this article provide evidence that the model under investigation is not a particularly good model for natural images

</details>

