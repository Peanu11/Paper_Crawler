# 2020

## TOC

- [2020-01](#2020-01)
- [2020-02](#2020-02)
- [2020-03](#2020-03)
- [2020-04](#2020-04)
- [2020-05](#2020-05)
- [2020-06](#2020-06)
- [2020-07](#2020-07)
- [2020-08](#2020-08)
- [2020-09](#2020-09)
- [2020-10](#2020-10)
- [2020-11](#2020-11)
- [2020-12](#2020-12)

## 2020-01

<details>

<summary>2020-01-01 07:10:09 - Dual Adversarial Domain Adaptation</summary>

- *Yuntao Du, Zhiwen Tan, Qian Chen, Xiaowen Zhang, Yirong Yao, Chongjun Wang*

- `2001.00153v1` - [abs](http://arxiv.org/abs/2001.00153v1) - [pdf](http://arxiv.org/pdf/2001.00153v1)

> Unsupervised domain adaptation aims at transferring knowledge from the labeled source domain to the unlabeled target domain. Previous adversarial domain adaptation methods mostly adopt the discriminator with binary or $K$-dimensional output to perform marginal or conditional alignment independently. Recent experiments have shown that when the discriminator is provided with domain information in both domains and label information in the source domain, it is able to preserve the complex multimodal information and high semantic information in both domains. Following this idea, we adopt a discriminator with $2K$-dimensional output to perform both domain-level and class-level alignments simultaneously in a single discriminator. However, a single discriminator can not capture all the useful information across domains and the relationships between the examples and the decision boundary are rarely explored before. Inspired by multi-view learning and latest advances in domain adaptation, besides the adversarial process between the discriminator and the feature extractor, we also design a novel mechanism to make two discriminators pit against each other, so that they can provide diverse information for each other and avoid generating target features outside the support of the source domain. To the best of our knowledge, it is the first time to explore a dual adversarial strategy in domain adaptation. Moreover, we also use the semi-supervised learning regularization to make the representations more discriminative. Comprehensive experiments on two real-world datasets verify that our method outperforms several state-of-the-art domain adaptation methods.

</details>

<details>

<summary>2020-01-01 10:18:26 - TextScanner: Reading Characters in Order for Robust Scene Text Recognition</summary>

- *Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, Cong Yao*

- `1912.12422v2` - [abs](http://arxiv.org/abs/1912.12422v2) - [pdf](http://arxiv.org/pdf/1912.12422v2)

> Driven by deep learning and the large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention based methods have dominated this field, but suffer from the problem of \textit{attention drift} in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such Chinese transcripts and aligning with target characters.

</details>

<details>

<summary>2020-01-01 20:39:22 - Smart Summarizer for Blind People</summary>

- *Mona teja K, Mohan Sai. S, H S S S Raviteja D, Sai Kushagra P V*

- `2001.00575v1` - [abs](http://arxiv.org/abs/2001.00575v1) - [pdf](http://arxiv.org/pdf/2001.00575v1)

> In today's world, time is a very important resource. In our busy lives, most of us hardly have time to read the complete news so what we have to do is just go through the headlines and satisfy ourselves with that. As a result, we might miss a part of the news or misinterpret the complete thing. The situation is even worse for the people who are visually impaired or have lost their ability to see. The inability of these people to read text has a huge impact on their lives. There are a number of methods for blind people to read the text. Braille script, in particular, is one of the examples, but it is a highly inefficient method as it is really time taking and requires a lot of practice. So, we present a method for visually impaired people based on the sense of sound which is obviously better and more accurate than the sense of touch. This paper deals with an efficient method to summarize news into important keywords so as to save the efforts to go through the complete text every single time. This paper deals with many API's and modules like the tesseract, GTTS, and many algorithms that have been discussed and implemented in detail such as Luhn's Algorithm, Latent Semantic Analysis Algorithm, Text Ranking Algorithm. And the other functionality that this paper deals with is converting the summarized text to speech so that the system can aid even the blind people.

</details>

<details>

<summary>2020-01-02 01:07:09 - Learning Reinforced Attentional Representation for End-to-End Visual Tracking</summary>

- *Peng Gao, Qiquan Zhang, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang*

- `1908.10009v3` - [abs](http://arxiv.org/abs/1908.10009v3) - [pdf](http://arxiv.org/pdf/1908.10009v3)

> Although numerous recent tracking approaches have made tremendous advances in the last decade, achieving high-performance visual tracking remains a challenge. In this paper, we propose an end-to-end network model to learn reinforced attentional representation for accurate target object discrimination and localization. We utilize a novel hierarchical attentional module with long short-term memory and multi-layer perceptrons to leverage both inter- and intra-frame attention to effectively facilitate visual pattern emphasis. Moreover, we incorporate a contextual attentional correlation filter into the backbone network to make our model trainable in an end-to-end fashion. Our proposed approach not only takes full advantage of informative geometries and semantics but also updates correlation filters online without fine-tuning the backbone network to enable the adaptation of variations in the target object's appearance. Extensive experiments conducted on several popular benchmark datasets demonstrate that our proposed approach is effective and computationally efficient.

</details>

<details>

<summary>2020-01-02 01:28:05 - Direction Concentration Learning: Enhancing Congruency in Machine Learning</summary>

- *Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, Qi Zhao*

- `1912.08136v2` - [abs](http://arxiv.org/abs/1912.08136v2) - [pdf](http://arxiv.org/pdf/1912.08136v2)

> One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency.

</details>

<details>

<summary>2020-01-02 02:24:53 - Chemical-induced Disease Relation Extraction with Dependency Information and Prior Knowledge</summary>

- *Huiwei Zhou, Shixian Ning, Yunlong Yang, Zhuang Liu, Chengkun Lang, Yingyu Lin*

- `2001.00295v1` - [abs](http://arxiv.org/abs/2001.00295v1) - [pdf](http://arxiv.org/pdf/2001.00295v1)

> Chemical-disease relation (CDR) extraction is significantly important to various areas of biomedical research and health care. Nowadays, many large-scale biomedical knowledge bases (KBs) containing triples about entity pairs and their relations have been built. KBs are important resources for biomedical relation extraction. However, previous research pays little attention to prior knowledge. In addition, the dependency tree contains important syntactic and semantic information, which helps to improve relation extraction. So how to effectively use it is also worth studying. In this paper, we propose a novel convolutional attention network (CAN) for CDR extraction. Firstly, we extract the shortest dependency path (SDP) between chemical and disease pairs in a sentence, which includes a sequence of words, dependency directions, and dependency relation tags. Then the convolution operations are performed on the SDP to produce deep semantic dependency features. After that, an attention mechanism is employed to learn the importance/weight of each semantic dependency vector related to knowledge representations learned from KBs. Finally, in order to combine dependency information and prior knowledge, the concatenation of weighted semantic dependency representations and knowledge representations is fed to the softmax layer for classification. Experiments on the BioCreative V CDR dataset show that our method achieves comparable performance with the state-of-the-art systems, and both dependency information and prior knowledge play important roles in CDR extraction task.

</details>

<details>

<summary>2020-01-02 10:05:02 - Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation</summary>

- *Yirong Pan, Xiao Li, Yating Yang, Rui Dong*

- `2001.01589v1` - [abs](http://arxiv.org/abs/2001.01589v1) - [pdf](http://arxiv.org/pdf/2001.01589v1)

> Neural machine translation (NMT) has achieved impressive performance on machine translation task in recent years. However, in consideration of efficiency, a limited-size vocabulary that only contains the top-N highest frequency words are employed for model training, which leads to many rare and unknown words. It is rather difficult when translating from the low-resource and morphologically-rich agglutinative languages, which have complex morphology and large vocabulary. In this paper, we propose a morphological word segmentation method on the source-side for NMT that incorporates morphology knowledge to preserve the linguistic and semantic information in the word structure while reducing the vocabulary size at training time. It can be utilized as a preprocessing tool to segment the words in agglutinative languages for other natural language processing (NLP) tasks. Experimental results show that our morphologically motivated word segmentation method is better suitable for the NMT model, which achieves significant improvements on Turkish-English and Uyghur-Chinese machine translation tasks on account of reducing data sparseness and language complexity.

</details>

<details>

<summary>2020-01-03 09:42:12 - Applying Abstract Argumentation Theory to Cooperative Game Theory</summary>

- *Anthony P. Young, David Kohan Marzagao, Josh Murphy*

- `1905.10922v3` - [abs](http://arxiv.org/abs/1905.10922v3) - [pdf](http://arxiv.org/pdf/1905.10922v3)

> We apply ideas from abstract argumentation theory to study cooperative game theory. Building on Dung's results in his seminal paper, we further the correspondence between Dung's four argumentation semantics and solution concepts in cooperative game theory by showing that complete extensions (the grounded extension) correspond to Roth's subsolutions (respectively, the supercore). We then investigate the relationship between well-founded argumentation frameworks and convex games, where in each case the semantics (respectively, solution concepts) coincide; we prove that three-player convex games do not in general have well-founded argumentation frameworks.

</details>

<details>

<summary>2020-01-03 17:06:41 - Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation</summary>

- *Goran Glavaš, Swapna Somasundaran*

- `2001.00891v1` - [abs](http://arxiv.org/abs/2001.00891v1) - [pdf](http://arxiv.org/pdf/2001.00891v1)

> Breaking down the structure of long texts into semantically coherent segments makes the texts more readable and supports downstream applications like summarization and retrieval. Starting from an apparent link between text coherence and segmentation, we introduce a novel supervised model for text segmentation with simple but explicit coherence modeling. Our model -- a neural architecture consisting of two hierarchically connected Transformer networks -- is a multi-task learning model that couples the sentence-level segmentation objective with the coherence objective that differentiates correct sequences of sentences from corrupt ones. The proposed model, dubbed Coherence-Aware Text Segmentation (CATS), yields state-of-the-art segmentation performance on a collection of benchmark datasets. Furthermore, by coupling CATS with cross-lingual word embeddings, we demonstrate its effectiveness in zero-shot language transfer: it can successfully segment texts in languages unseen in training.

</details>

<details>

<summary>2020-01-03 17:22:06 - A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces</summary>

- *Anne Lauscher, Goran Glavaš, Simone Paolo Ponzetto, Ivan Vulić*

- `1909.06092v2` - [abs](http://arxiv.org/abs/1909.06092v2) - [pdf](http://arxiv.org/pdf/1909.06092v2)

> Distributional word vectors have recently been shown to encode many of the human biases, most notably gender and racial biases, and models for attenuating such biases have consequently been proposed. However, existing models and studies (1) operate on under-specified and mutually differing bias definitions, (2) are tailored for a particular bias (e.g., gender bias) and (3) have been evaluated inconsistently and non-rigorously. In this work, we introduce a general framework for debiasing word embeddings. We operationalize the definition of a bias by discerning two types of bias specification: explicit and implicit. We then propose three debiasing models that operate on explicit or implicit bias specifications and that can be composed towards more robust debiasing. Finally, we devise a full-fledged evaluation framework in which we couple existing bias metrics with newly proposed ones. Experimental findings across three embedding methods suggest that the proposed debiasing models are robust and widely applicable: they often completely remove the bias both implicitly and explicitly without degradation of semantic information encoded in any of the input distributional spaces. Moreover, we successfully transfer debiasing models, by means of cross-lingual embedding spaces, and remove or attenuate biases in distributional word vector spaces of languages that lack readily available bias specifications.

</details>

<details>

<summary>2020-01-04 14:06:19 - Revisiting Paraphrase Question Generator using Pairwise Discriminator</summary>

- *Badri N. Patro, Dev Chauhan, Vinod K. Kurmi, Vinay P. Namboodiri*

- `1912.13149v2` - [abs](http://arxiv.org/abs/1912.13149v2) - [pdf](http://arxiv.org/pdf/1912.13149v2)

> In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.

</details>

<details>

<summary>2020-01-04 23:19:04 - A Study of Bug Resolution Characteristics in Popular Programming Languages</summary>

- *Jie M. Zhang, Feng Li, Dan Hao, Meng Wang, Hao Tang, Lu Zhang, Mark Harman*

- `1801.01025v2` - [abs](http://arxiv.org/abs/1801.01025v2) - [pdf](http://arxiv.org/pdf/1801.01025v2)

> This paper presents a large-scale study that investigates the bug resolution characteristics among popular Github projects written in different programming languages. We explore correlations but, of course, we cannot infer causation. Specifically, we analyse bug resolution data from approximately 70 million Source Line of Code, drawn from 3 million commits to 600 GitHub projects, primarily written in 10 programming languages. We find notable variations in apparent bug resolution time and patch (fix) size. While interpretation of results from such large-scale empirical studies is inherently difficult, we believe that the differences in medians are sufficiently large to warrant further investigation, replication, re-analysis and follow up research. For example, in our corpus, the median apparent bug resolution time (elapsed time from raise to resolve) for Ruby was 4X that for Go and 2.5X for Java. We also found that patches tend to touch more files for the corpus of strongly typed and for statically typed programs. However, we also found evidence for a lower elapsed resolution time for bug resolution committed to projects constructed from statically typed languages. These findings, if replicated in subsequent follow on studies, may shed further empirical light on the debate about the importance of static typing.

</details>

<details>

<summary>2020-01-05 06:28:59 - COCO-GAN: Generation by Parts via Conditional Coordinating</summary>

- *Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen*

- `1904.00284v4` - [abs](http://arxiv.org/abs/1904.00284v4) - [pdf](http://arxiv.org/pdf/1904.00284v4)

> Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose \underline{CO}nditional \underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce \textbf{state-of-the-art-quality} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.

</details>

<details>

<summary>2020-01-05 07:07:53 - Semantic Image Completion and Enhancement using Deep Learning</summary>

- *Vaishnav Chandak, Priyansh Saxena, Manisha Pattanaik, Gaurav Kaushal*

- `1911.02222v2` - [abs](http://arxiv.org/abs/1911.02222v2) - [pdf](http://arxiv.org/pdf/1911.02222v2)

> In real-life applications, certain images utilized are corrupted in which the image pixels are damaged or missing, which increases the complexity of computer vision tasks. In this paper, a deep learning architecture is proposed to deal with image completion and enhancement. Generative Adversarial Networks (GAN), has been turned out to be helpful in picture completion tasks. Therefore, in GANs, Wasserstein GAN architecture is used for image completion which creates the coarse patches to filling the missing region in the distorted picture, and the enhancement network will additionally refine the resultant pictures utilizing residual learning procedures and hence give better complete pictures for computer vision applications. Experimental outcomes show that the proposed approach improves the Peak Signal to Noise ratio and Structural Similarity Index values by 2.45% and 4% respectively when compared to the recently reported data.

</details>

<details>

<summary>2020-01-05 07:58:28 - CNNTOP: a CNN-based Trajectory Owner Prediction Method</summary>

- *Xucheng Luo, Shengyang Li, Yuxiang Peng*

- `2001.01185v1` - [abs](http://arxiv.org/abs/2001.01185v1) - [pdf](http://arxiv.org/pdf/2001.01185v1)

> Trajectory owner prediction is the basis for many applications such as personalized recommendation, urban planning. Although much effort has been put on this topic, the results archived are still not good enough. Existing methods mainly employ RNNs to model trajectories semantically due to the inherent sequential attribute of trajectories. However, these approaches are weak at Point of Interest (POI) representation learning and trajectory feature detection. Thus, the performance of existing solutions is far from the requirements of practical applications. In this paper, we propose a novel CNN-based Trajectory Owner Prediction (CNNTOP) method. Firstly, we connect all POI according to trajectories from all users. The result is a connected graph that can be used to generate more informative POI sequences than other approaches. Secondly, we employ the Node2Vec algorithm to encode each POI into a low-dimensional real value vector. Then, we transform each trajectory into a fixed-dimensional matrix, which is similar to an image. Finally, a CNN is designed to detect features and predict the owner of a given trajectory. The CNN can extract informative features from the matrix representations of trajectories by convolutional operations, Batch normalization, and $K$-max pooling operations. Extensive experiments on real datasets demonstrate that CNNTOP substantially outperforms existing solutions in terms of macro-Precision, macro-Recall, macro-F1, and accuracy.

</details>

<details>

<summary>2020-01-05 12:39:05 - Black-Box Adversarial Attack with Transferable Model-based Embedding</summary>

- *Zhichao Huang, Tong Zhang*

- `1911.07140v2` - [abs](http://arxiv.org/abs/1911.07140v2) - [pdf](http://arxiv.org/pdf/1911.07140v2)

> We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate.

</details>

<details>

<summary>2020-01-05 14:19:11 - Automatic Business Process Structure Discovery using Ordered Neurons LSTM: A Preliminary Study</summary>

- *Xue Han, Lianxue Hu, Yabin Dang, Shivali Agarwal, Lijun Mei, Shaochun Li, Xin Zhou*

- `2001.01243v1` - [abs](http://arxiv.org/abs/2001.01243v1) - [pdf](http://arxiv.org/pdf/2001.01243v1)

> Automatic process discovery from textual process documentations is highly desirable to reduce time and cost of Business Process Management (BPM) implementation in organizations. However, existing automatic process discovery approaches mainly focus on identifying activities out of the documentations. Deriving the structural relationships between activities, which is important in the whole process discovery scope, is still a challenge. In fact, a business process has latent semantic hierarchical structure which defines different levels of detail to reflect the complex business logic. Recent findings in neural machine learning area show that the meaningful linguistic structure can be induced by joint language modeling and structure learning. Inspired by these findings, we propose to retrieve the latent hierarchical structure present in the textual business process documents by building a neural network that leverages a novel recurrent architecture, Ordered Neurons LSTM (ON-LSTM), with process-level language model objective. We tested the proposed approach on data set of Process Description Documents (PDD) from our practical Robotic Process Automation (RPA) projects. Preliminary experiments showed promising results.

</details>

<details>

<summary>2020-01-05 16:47:28 - Bi-Semantic Reconstructing Generative Network for Zero-shot Learning</summary>

- *Shibing Xu, Zishu Gao, Guojun Xie*

- `1912.03877v3` - [abs](http://arxiv.org/abs/1912.03877v3) - [pdf](http://arxiv.org/pdf/1912.03877v3)

> Many recent methods of zero-shot learning (ZSL) attempt to utilize generative model to generate the unseen visual samples from semantic descriptions and random noise. Therefore, the ZSL problem becomes a traditional supervised classification problem. However, most of the existing methods based on the generative model only focus on the quality of synthesized samples at the training stage, and ignore the importance of the zero-shot recognition stage. In this paper, we consider both the above two points and propose a novel approach. Specially, we select the Generative Adversarial Network (GAN) as our generative model. In order to improve the quality of synthesized samples, considering the internal relation of the semantic description in the semantic space as well as the fact that the seen and unseen visual information belong to different domains, we propose a bi-semantic reconstructing (BSR) component which contain two different semantic reconstructing regressors to lead the training of GAN. Since the semantic descriptions are available during the training stage, to further improve the ability of classifier, we combine the visual samples and semantic descriptions to train a classifier. At the recognition stage, we naturally utilize the BSR component to transfer the visual features and semantic descriptions, and concatenate them for classification. Experimental results show that our method outperforms the state of the art on several ZSL benchmark datasets with significant improvements.

</details>

<details>

<summary>2020-01-06 07:47:42 - Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs</summary>

- *Andriy Myronenko, Ali Hatamizadeh*

- `2001.02040v1` - [abs](http://arxiv.org/abs/2001.02040v1) - [pdf](http://arxiv.org/pdf/2001.02040v1)

> Multimodal brain tumor segmentation challenge (BraTS) brings together researchers to improve automated methods for 3D MRI brain tumor segmentation. Tumor segmentation is one of the fundamental vision tasks necessary for diagnosis and treatment planning of the disease. Previous years winning methods were all deep-learning based, thanks to the advent of modern GPUs, which allow fast optimization of deep convolutional neural network architectures. In this work, we explore best practices of 3D semantic segmentation, including conventional encoder-decoder architecture, as well combined loss functions, in attempt to further improve the segmentation accuracy. We evaluate the method on BraTS 2019 challenge.

</details>

<details>

<summary>2020-01-06 10:25:32 - TableNet: Deep Learning model for end-to-end Table detection and Tabular data extraction from Scanned Document Images</summary>

- *Shubham Paliwal, Vishwanath D, Rohit Rahul, Monika Sharma, Lovekesh Vig*

- `2001.01469v1` - [abs](http://arxiv.org/abs/2001.01469v1) - [pdf](http://arxiv.org/pdf/2001.01469v1)

> With the widespread use of mobile phones and scanners to photograph and upload documents, the need for extracting the information trapped in unstructured document images such as retail receipts, insurance claim forms and financial invoices is becoming more acute. A major hurdle to this objective is that these images often contain information in the form of tables and extracting data from tabular sub-images presents a unique set of challenges. This includes accurate detection of the tabular region within an image, and subsequently detecting and extracting information from the rows and columns of the detected table. While some progress has been made in table detection, extracting the table contents is still a challenge since this involves more fine grained table structure(rows & columns) recognition. Prior approaches have attempted to solve the table detection and structure recognition problems independently using two separate models. In this paper, we propose TableNet: a novel end-to-end deep learning model for both table detection and structure recognition. The model exploits the interdependence between the twin tasks of table detection and table structure recognition to segment out the table and column regions. This is followed by semantic rule-based row extraction from the identified tabular sub-regions. The proposed model and extraction approach was evaluated on the publicly available ICDAR 2013 and Marmot Table datasets obtaining state of the art results. Additionally, we demonstrate that feeding additional semantic features further improves model performance and that the model exhibits transfer learning across datasets. Another contribution of this paper is to provide additional table structure annotations for the Marmot data, which currently only has annotations for table detection.

</details>

<details>

<summary>2020-01-06 17:09:21 - Topic Extraction of Crawled Documents Collection using Correlated Topic Model in MapReduce Framework</summary>

- *Mi Khine Oo, May Aye Khine*

- `2001.01669v1` - [abs](http://arxiv.org/abs/2001.01669v1) - [pdf](http://arxiv.org/pdf/2001.01669v1)

> The tremendous increase in the amount of available research documents impels researchers to propose topic models to extract the latent semantic themes of a documents collection. However, how to extract the hidden topics of the documents collection has become a crucial task for many topic model applications. Moreover, conventional topic modeling approaches suffer from the scalability problem when the size of documents collection increases. In this paper, the Correlated Topic Model with variational Expectation-Maximization algorithm is implemented in MapReduce framework to solve the scalability problem. The proposed approach utilizes the dataset crawled from the public digital library. In addition, the full-texts of the crawled documents are analysed to enhance the accuracy of MapReduce CTM. The experiments are conducted to demonstrate the performance of the proposed algorithm. From the evaluation, the proposed approach has a comparable performance in terms of topic coherences with LDA implemented in MapReduce framework.

</details>

<details>

<summary>2020-01-06 21:43:25 - Plug-and-Play Rescaling Based Crowd Counting in Static Images</summary>

- *Usman Sajid, Guanghui Wang*

- `2001.01786v1` - [abs](http://arxiv.org/abs/2001.01786v1) - [pdf](http://arxiv.org/pdf/2001.01786v1)

> Crowd counting is a challenging problem especially in the presence of huge crowd diversity across images and complex cluttered crowd-like background regions, where most previous approaches do not generalize well and consequently produce either huge crowd underestimation or overestimation. To address these challenges, we propose a new image patch rescaling module (PRM) and three independent PRM employed crowd counting methods. The proposed frameworks use the PRM module to rescale the image regions (patches) that require special treatment, whereas the classification process helps in recognizing and discarding any cluttered crowd-like background regions which may result in overestimation. Experiments on three standard benchmarks and cross-dataset evaluation show that our approach outperforms the state-of-the-art models in the RMSE evaluation metric with an improvement up to 10.4%, and possesses superior generalization ability to new datasets.

</details>

<details>

<summary>2020-01-06 22:19:17 - Character-Aware Attention-Based End-to-End Speech Recognition</summary>

- *Zhong Meng, Yashesh Gaur, Jinyu Li, Yifan Gong*

- `2001.01795v1` - [abs](http://arxiv.org/abs/2001.01795v1) - [pdf](http://arxiv.org/pdf/2001.01795v1)

> Predicting words and subword units (WSUs) as the output has shown to be effective for the attention-based encoder-decoder (AED) model in end-to-end speech recognition. However, as one input to the decoder recurrent neural network (RNN), each WSU embedding is learned independently through context and acoustic information in a purely data-driven fashion. Little effort has been made to explicitly model the morphological relationships among WSUs. In this work, we propose a novel character-aware (CA) AED model in which each WSU embedding is computed by summarizing the embeddings of its constituent characters using a CA-RNN. This WSU-independent CA-RNN is jointly trained with the encoder, the decoder and the attention network of a conventional AED to predict WSUs. With CA-AED, the embeddings of morphologically similar WSUs are naturally and directly correlated through the CA-RNN in addition to the semantic and acoustic relations modeled by a traditional AED. Moreover, CA-AED significantly reduces the model parameters in a traditional AED by replacing the large pool of WSU embeddings with a much smaller set of character embeddings. On a 3400 hours Microsoft Cortana dataset, CA-AED achieves up to 11.9% relative WER improvement over a strong AED baseline with 27.1% fewer model parameters.

</details>

<details>

<summary>2020-01-06 23:22:17 - Granular Learning with Deep Generative Models using Highly Contaminated Data</summary>

- *John Just*

- `2001.04297v1` - [abs](http://arxiv.org/abs/2001.04297v1) - [pdf](http://arxiv.org/pdf/2001.04297v1)

> An approach to utilize recent advances in deep generative models for anomaly detection in a granular (continuous) sense on a real-world image dataset with quality issues is detailed using recent normalizing flow models, with implications in many other applications/domains/data types. The approach is completely unsupervised (no annotations available) but qualitatively shown to provide accurate semantic labeling for images via heatmaps of the scaled log-likelihood overlaid on the images. When sorted based on the median values per image, clear trends in quality are observed. Furthermore, downstream classification is shown to be possible and effective via a weakly supervised approach using the log-likelihood output from a normalizing flow model as a training signal for a feature-extracting convolutional neural network. The pre-linear dense layer outputs on the CNN are shown to disentangle high level representations and efficiently cluster various quality issues. Thus, an entirely non-annotated (fully unsupervised) approach is shown possible for accurate estimation and classification of quality issues..

</details>

<details>

<summary>2020-01-07 09:22:58 - Paraphrase Generation with Latent Bag of Words</summary>

- *Yao Fu, Yansong Feng, John P. Cunningham*

- `2001.01941v1` - [abs](http://arxiv.org/abs/2001.01941v1) - [pdf](http://arxiv.org/pdf/2001.01941v1)

> Paraphrase generation is a longstanding important problem in natural language processing.   In addition, recent progress in deep generative models has shown promising results on discrete latent variables for text generation.   Inspired by variational autoencoders with discrete latent structures, in this work, we propose a latent bag of words (BOW) model for paraphrase generation.   We ground the semantics of a discrete latent variable by the BOW from the target sentences.   We use this latent variable to build a fully differentiable content planning and surface realization model.   Specifically, we use source words to predict their neighbors and model the target BOW with a mixture of softmax.   We use Gumbel top-k reparameterization to perform differentiable subset sampling from the predicted BOW distribution.   We retrieve the sampled word embeddings and use them to augment the decoder and guide its generation search space.   Our latent BOW model not only enhances the decoder, but also exhibits clear interpretability.   We show the model interpretability with regard to \emph{(i)} unsupervised learning of word neighbors \emph{(ii)} the step-by-step generation procedure.   Extensive experiments demonstrate the transparent and effective generation process of this model.\footnote{Our code can be found at \url{https://github.com/FranxYao/dgm_latent_bow}}

</details>

<details>

<summary>2020-01-07 09:42:28 - Provenance-based Classification Policy based on Encrypted Search</summary>

- *Xinyu Fan, Faen Zhang, Jiahong Wu, Jingming Guo*

- `2001.01946v1` - [abs](http://arxiv.org/abs/2001.01946v1) - [pdf](http://arxiv.org/pdf/2001.01946v1)

> As an important type of cloud data, digital provenance is arousing increasing attention on improving system performance. Currently, provenance has been employed to provide cues regarding access control and to estimate data quality. However, provenance itself might also be sensitive information. Therefore, provenance might be encrypted and stored in the Cloud. In this paper, we provide a mechanism to classify cloud documents by searching specific keywords from their encrypted provenance, and we prove our scheme achieves semantic security. In term of application of the proposed techniques, considering that files are classified to store separately in the cloud, in order to facilitate the regulation and security protection for the files, the classification policies can use provenance as conditions to determine the category of a document. Such as the easiest sample policy goes like: the documents have been reviewed twice can be classified as "public accessible", which can be accessed by the public.

</details>

<details>

<summary>2020-01-07 10:14:04 - WAF-A-MoLE: Evading Web Application Firewalls through Adversarial Machine Learning</summary>

- *Luca Demetrio, Andrea Valenza, Gabriele Costa, Giovanni Lagorio*

- `2001.01952v1` - [abs](http://arxiv.org/abs/2001.01952v1) - [pdf](http://arxiv.org/pdf/2001.01952v1)

> Web Application Firewalls are widely used in production environments to mitigate security threats like SQL injections. Many industrial products rely on signature-based techniques, but machine learning approaches are becoming more and more popular. The main goal of an adversary is to craft semantically malicious payloads to bypass the syntactic analysis performed by a WAF. In this paper, we present WAF-A-MoLE, a tool that models the presence of an adversary. This tool leverages on a set of mutation operators that alter the syntax of a payload without affecting the original semantics. We evaluate the performance of the tool against existing WAFs, that we trained using our publicly available SQL query dataset. We show that WAF-A-MoLE bypasses all the considered machine learning based WAFs.

</details>

<details>

<summary>2020-01-07 11:50:54 - Latent Opinions Transfer Network for Target-Oriented Opinion Words Extraction</summary>

- *Zhen Wu, Fei Zhao, Xin-Yu Dai, Shujian Huang, Jiajun Chen*

- `2001.01989v1` - [abs](http://arxiv.org/abs/2001.01989v1) - [pdf](http://arxiv.org/pdf/2001.01989v1)

> Target-oriented opinion words extraction (TOWE) is a new subtask of ABSA, which aims to extract the corresponding opinion words for a given opinion target in a sentence. Recently, neural network methods have been applied to this task and achieve promising results. However, the difficulty of annotation causes the datasets of TOWE to be insufficient, which heavily limits the performance of neural models. By contrast, abundant review sentiment classification data are easily available at online review sites. These reviews contain substantial latent opinions information and semantic patterns. In this paper, we propose a novel model to transfer these opinions knowledge from resource-rich review sentiment classification datasets to low-resource task TOWE. To address the challenges in the transfer process, we design an effective transformation method to obtain latent opinions, then integrate them into TOWE. Extensive experimental results show that our model achieves better performance compared to other state-of-the-art methods and significantly outperforms the base model without transferring opinions knowledge. Further analysis validates the effectiveness of our model.

</details>

<details>

<summary>2020-01-07 13:26:55 - Exploring Unknown Universes in Probabilistic Relational Models</summary>

- *Tanya Braun, Ralf Möller*

- `2001.02021v1` - [abs](http://arxiv.org/abs/2001.02021v1) - [pdf](http://arxiv.org/pdf/2001.02021v1)

> Large probabilistic models are often shaped by a pool of known individuals (a universe) and relations between them. Lifted inference algorithms handle sets of known individuals for tractable inference. Universes may not always be known, though, or may only described by assumptions such as "small universes are more likely". Without a universe, inference is no longer possible for lifted algorithms, losing their advantage of tractable inference. The aim of this paper is to define a semantics for models with unknown universes decoupled from a specific constraint language to enable lifted and thereby, tractable inference.

</details>

<details>

<summary>2020-01-07 14:26:06 - Multimodal Semantic Transfer from Text to Image. Fine-Grained Image Classification by Distributional Semantics</summary>

- *Simon Donig, Maria Christoforaki, Bernhard Bermeitinger, Siegfried Handschuh*

- `2001.02372v1` - [abs](http://arxiv.org/abs/2001.02372v1) - [pdf](http://arxiv.org/pdf/2001.02372v1)

> In the last years, image classification processes like neural networks in the area of art-history and Heritage Informatics have experienced a broad distribution (Lang and Ommer 2018). These methods face several challenges, including the handling of comparatively small amounts of data as well as high-dimensional data in the Digital Humanities. Here, a Convolutional Neural Network (CNN) is used that output is not, as usual, a series of flat text labels but a series of semantically loaded vectors. These vectors result from a Distributional Semantic Model (DSM) which is generated from an in-domain text corpus.   -----   In den letzten Jahren hat die Verwendung von Bildklassifizierungsverfahren wie neuronalen Netzwerken auch im Bereich der historischen Bildwissenschaften und der Heritage Informatics weite Verbreitung gefunden (Lang und Ommer 2018). Diese Verfahren stehen dabei vor einer Reihe von Herausforderungen, darunter dem Umgangmit den vergleichsweise kleinen Datenmengen sowie zugleich hochdimensionalen Da-tenr\"aumen in den digitalen Geisteswissenschaften. Meist bilden diese Methoden dieKlassifizierung auf einen vergleichsweise flachen Raum ab. Dieser flache Zugang verliert im Bem\"uhen um ontologische Eindeutigkeit eine Reihe von relevanten Dimensionen, darunter taxonomische, mereologische und assoziative Beziehungen zwischenden Klassen beziehungsweise dem nicht formalisierten Kontext. Dabei wird ein Convolutional Neural Network (CNN) genutzt, dessen Ausgabe im Trainingsprozess, anders als herk\"ommlich, nicht auf einer Serie flacher Textlabel beruht, sondern auf einer Serie von Vektoren. Diese Vektoren resultieren aus einem Distributional Semantic Model (DSM), welches aus einem Dom\"ane-Textkorpus generiert wird.

</details>

<details>

<summary>2020-01-07 18:26:38 - Attributed Multi-Relational Attention Network for Fact-checking URL Recommendation</summary>

- *Di You, Nguyen Vo, Kyumin Lee, Qiang Liu*

- `2001.02214v1` - [abs](http://arxiv.org/abs/2001.02214v1) - [pdf](http://arxiv.org/pdf/2001.02214v1)

> To combat fake news, researchers mostly focused on detecting fake news and journalists built and maintained fact-checking sites (e.g., Snopes.com and Politifact.com). However, fake news dissemination has been greatly promoted via social media sites, and these fact-checking sites have not been fully utilized. To overcome these problems and complement existing methods against fake news, in this paper we propose a deep-learning based fact-checking URL recommender system to mitigate impact of fake news in social media sites such as Twitter and Facebook. In particular, our proposed framework consists of a multi-relational attentive module and a heterogeneous graph attention network to learn complex/semantic relationship between user-URL pairs, user-user pairs, and URL-URL pairs. Extensive experiments on a real-world dataset show that our proposed framework outperforms eight state-of-the-art recommendation models, achieving at least 3~5.3% improvement.

</details>

<details>

<summary>2020-01-07 20:51:34 - Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling</summary>

- *Hao Zhang, Bo Chen, Long Tian, Zhengjue Wang, Mingyuan Zhou*

- `1905.08622v3` - [abs](http://arxiv.org/abs/1905.08622v3) - [pdf](http://arxiv.org/pdf/1905.08622v3)

> For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN), a versatile deep generative model that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a deep topic model, a ladder-structured image encoder, and StackGAN++, into VHE-GAN, which already achieves competitive performance. This further motivates the development of VHE-raster-scan-GAN that generates photo-realistic images in not only a multi-scale low-to-high-resolution manner, but also a hierarchical-semantic coarse-to-fine fashion. By capturing and relating hierarchical semantic and visual concepts with end-to-end training, VHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of image-text multi-modality learning and generation tasks.

</details>

<details>

<summary>2020-01-08 01:19:08 - Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</summary>

- *Pengda Qin, Xin Wang, Wenhu Chen, Chunyun Zhang, Weiran Xu, William Yang Wang*

- `2001.02332v1` - [abs](http://arxiv.org/abs/2001.02332v1) - [pdf](http://arxiv.org/pdf/2001.02332v1)

> Large-scale knowledge graphs (KGs) are shown to become more important in current information systems. To expand the coverage of KGs, previous studies on knowledge graph completion need to collect adequate training instances for newly-added relations. In this paper, we consider a novel formulation, zero-shot learning, to free this cumbersome curation. For newly-added relations, we attempt to learn their semantic features from their text descriptions and hence recognize the facts of unseen relations with no examples being seen. For this purpose, we leverage Generative Adversarial Networks (GANs) to establish the connection between text and knowledge graph domain: The generator learns to generate the reasonable relation embeddings merely with noisy text descriptions. Under this setting, zero-shot learning is naturally converted to a traditional supervised classification task. Empirically, our method is model-agnostic that could be potentially applied to any version of KG embeddings, and consistently yields performance improvements on NELL and Wiki dataset.

</details>

<details>

<summary>2020-01-08 07:40:13 - Predicting Research Trends with Semantic and Neural Networks with an application in Quantum Physics</summary>

- *Mario Krenn, Anton Zeilinger*

- `1906.06843v2` - [abs](http://arxiv.org/abs/1906.06843v2) - [pdf](http://arxiv.org/pdf/1906.06843v2)

> The vast and growing number of publications in all disciplines of science cannot be comprehended by a single human researcher. As a consequence, researchers have to specialize in narrow sub-disciplines, which makes it challenging to uncover scientific connections beyond the own field of research. Thus access to structured knowledge from a large corpus of publications could help pushing the frontiers of science. Here we demonstrate a method to build a semantic network from published scientific literature, which we call SemNet. We use SemNet to predict future trends in research and to inspire new, personalized and surprising seeds of ideas in science. We apply it in the discipline of quantum physics, which has seen an unprecedented growth of activity in recent years. In SemNet, scientific knowledge is represented as an evolving network using the content of 750,000 scientific papers published since 1919. The nodes of the network correspond to physical concepts, and links between two nodes are drawn when two physical concepts are concurrently studied in research articles. We identify influential and prize-winning research topics from the past inside SemNet thus confirm that it stores useful semantic knowledge. We train a deep neural network using states of SemNet of the past, to predict future developments in quantum physics research, and confirm high quality predictions using historic data. With the neural network and theoretical network tools we are able to suggest new, personalized, out-of-the-box ideas, by identifying pairs of concepts which have unique and extremal semantic network properties. Finally, we consider possible future developments and implications of our findings.

</details>

<details>

<summary>2020-01-08 08:54:00 - Casting Geometric Constraints in Semantic Segmentation as Semi-Supervised Learning</summary>

- *Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit*

- `1904.12534v3` - [abs](http://arxiv.org/abs/1904.12534v3) - [pdf](http://arxiv.org/pdf/1904.12534v3)

> We propose a simple yet effective method to learn to segment new indoor scenes from video frames: State-of-the-art methods trained on one dataset, even as large as the SUNRGB-D dataset, can perform poorly when applied to images that are not part of the dataset, because of the dataset bias, a common phenomenon in computer vision. To make semantic segmentation more useful in practice, one can exploit geometric constraints. Our main contribution is to show that these constraints can be cast conveniently as semi-supervised terms, which enforce the fact that the same class should be predicted for the projections of the same 3D location in different images. This is interesting as we can exploit general existing techniques developed for semi-supervised learning to efficiently incorporate the constraints. We show that this approach can efficiently and accurately learn to segment target sequences of ScanNet and our own target sequences using only annotations from SUNRGB-D, and geometric relations between the video frames of target sequences.

</details>

<details>

<summary>2020-01-08 11:44:47 - From Natural Language Instructions to Complex Processes: Issues in Chaining Trigger Action Rules</summary>

- *Nobuhiro Ito, Yuya Suzuki, Akiko Aizawa*

- `2001.02462v1` - [abs](http://arxiv.org/abs/2001.02462v1) - [pdf](http://arxiv.org/pdf/2001.02462v1)

> Automation services for complex business processes usually require a high level of information technology literacy. There is a strong demand for a smartly assisted process automation (IPA: intelligent process automation) service that enables even general users to easily use advanced automation. A natural language interface for such automation is expected as an elemental technology for the IPA realization. The workflow targeted by IPA is generally composed of a combination of multiple tasks. However, semantic parsing, one of the natural language processing methods, for such complex workflows has not yet been fully studied. The reasons are that (1) the formal expression and grammar of the workflow required for semantic analysis have not been sufficiently examined and (2) the dataset of the workflow formal expression with its corresponding natural language description required for learning workflow semantics did not exist. This paper defines a new grammar for complex workflows with chaining machine-executable meaning representations for semantic parsing. The representations are at a high abstraction level. Additionally, an approach to creating datasets is proposed based on this grammar.

</details>

<details>

<summary>2020-01-08 20:36:17 - SirenLess: reveal the intention behind news</summary>

- *Xumeng Chen, Leo Yu-Ho Lo, Huamin Qu*

- `2001.02731v1` - [abs](http://arxiv.org/abs/2001.02731v1) - [pdf](http://arxiv.org/pdf/2001.02731v1)

> News articles tend to be increasingly misleading nowadays, preventing readers from making subjective judgments towards certain events. While some machine learning approaches have been proposed to detect misleading news, most of them are black boxes that provide limited help for humans in decision making. In this paper, we present SirenLess, a visual analytical system for misleading news detection by linguistic features. The system features article explorer, a novel interactive tool that integrates news metadata and linguistic features to reveal semantic structures of news articles and facilitate textual analysis. We use SirenLess to analyze 18 news articles from different sources and summarize some helpful patterns for misleading news detection. A user study with journalism professionals and university students is conducted to confirm the usefulness and effectiveness of our system.

</details>

<details>

<summary>2020-01-08 22:32:19 - Efficient Sentence Embedding using Discrete Cosine Transform</summary>

- *Nada Almarwani, Hanan Aldarmaki, Mona Diab*

- `1909.03104v2` - [abs](http://arxiv.org/abs/1909.03104v2) - [pdf](http://arxiv.org/pdf/1909.03104v2)

> Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for syntactic structure. While more complex sequential or convolutional networks potentially yield superior classification performance, the improvements in classification accuracy are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information.

</details>

<details>

<summary>2020-01-09 04:47:14 - Multiplex Word Embeddings for Selectional Preference Acquisition</summary>

- *Hongming Zhang, Jiaxin Bai, Yan Song, Kun Xu, Changlong Yu, Yangqiu Song, Wilfred Ng, Dong Yu*

- `2001.02836v1` - [abs](http://arxiv.org/abs/2001.02836v1) - [pdf](http://arxiv.org/pdf/2001.02836v1)

> Conventional word embeddings represent words with fixed vectors, which are usually trained based on co-occurrence patterns among words. In doing so, however, the power of such representations is limited, where the same word might be functionalized separately under different syntactic relations. To address this limitation, one solution is to incorporate relational dependencies of different words into their embeddings. Therefore, in this paper, we propose a multiplex word embedding model, which can be easily extended according to various relations among words. As a result, each word has a center embedding to represent its overall semantics, and several relational embeddings to represent its relational dependencies. Compared to existing models, our model can effectively distinguish words with respect to different relations without introducing unnecessary sparseness. Moreover, to accommodate various relations, we use a small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance.

</details>

<details>

<summary>2020-01-09 20:37:49 - Simulating Lexical Semantic Change from Sense-Annotated Data</summary>

- *Dominik Schlechtweg, Sabine Schulte im Walde*

- `2001.03216v1` - [abs](http://arxiv.org/abs/2001.03216v1) - [pdf](http://arxiv.org/pdf/2001.03216v1)

> We present a novel procedure to simulate lexical semantic change from synchronic sense-annotated data, and demonstrate its usefulness for assessing lexical semantic change detection models. The induced dataset represents a stronger correspondence to empirically observed lexical semantic change than previous synthetic datasets, because it exploits the intimate relationship between synchronic polysemy and diachronic change. We publish the data and provide the first large-scale evaluation gold standard for LSC detection models.

</details>

<details>

<summary>2020-01-10 01:25:04 - Open Domain Question Answering Using Web Tables</summary>

- *Kaushik Chakrabarti, Zhimin Chen, Siamak Shakeri, Guihong Cao*

- `2001.03272v1` - [abs](http://arxiv.org/abs/2001.03272v1) - [pdf](http://arxiv.org/pdf/2001.03272v1)

> Tables extracted from web documents can be used to directly answer many web search queries. Previous works on question answering (QA) using web tables have focused on factoid queries, i.e., those answerable with a short string like person name or a number. However, many queries answerable using tables are non-factoid in nature. In this paper, we develop an open-domain QA approach using web tables that works for both factoid and non-factoid queries. Our key insight is to combine deep neural network-based semantic similarity between the query and the table with features that quantify the dominance of the table in the document as well as the quality of the information in the table. Our experiments on real-life web search queries show that our approach significantly outperforms state-of-the-art baseline approaches. Our solution is used in production in a major commercial web search engine and serves direct answers for tens of millions of real user queries per month.

</details>

<details>

<summary>2020-01-10 03:12:28 - Learning to Multi-Task Learn for Better Neural Machine Translation</summary>

- *Poorya Zaremoodi, Gholamreza Haffari*

- `2001.03294v1` - [abs](http://arxiv.org/abs/2001.03294v1) - [pdf](http://arxiv.org/pdf/2001.03294v1)

> Scarcity of parallel sentence pairs is a major challenge for training high quality neural machine translation (NMT) models in bilingually low-resource scenarios, as NMT is data-hungry. Multi-task learning is an elegant approach to inject linguistic-related inductive biases into NMT, using auxiliary syntactic and semantic tasks, to improve generalisation. The challenge, however, is to devise effective training schedules, prescribing when to make use of the auxiliary tasks during the training process to fill the knowledge gaps of the main translation task, a setting referred to as biased-MTL. Current approaches for the training schedule are based on hand-engineering heuristics, whose effectiveness vary in different MTL settings. We propose a novel framework for learning the training schedule, ie learning to multi-task learn, for the MTL setting of interest. We formulate the training schedule as a Markov decision process which paves the way to employ policy learning methods to learn the scheduling policy. We effectively and efficiently learn the training schedule policy within the imitation learning framework using an oracle policy algorithm that dynamically sets the importance weights of auxiliary tasks based on their contributions to the generalisability of the main NMT task. Experiments on low-resource NMT settings show the resulting automatically learned training schedulers are competitive with the best heuristics, and lead to up to +1.1 BLEU score improvements.

</details>

<details>

<summary>2020-01-10 13:17:17 - Walk-Steered Convolution for Graph Classification</summary>

- *Jiatao Jiang, Chunyan Xu, Zhen Cui, Tong Zhang, Wenming Zheng, Jian Yang*

- `1804.05837v2` - [abs](http://arxiv.org/abs/1804.05837v2) - [pdf](http://arxiv.org/pdf/1804.05837v2)

> Graph classification is a fundamental but challenging issue for numerous real-world applications. Despite recent great progress in image/video classification, convolutional neural networks (CNNs) cannot yet cater to graphs well because of graphical non-Euclidean topology. In this work, we propose a walk-steered convolutional (WSC) network to assemble the essential success of standard convolutional neural networks as well as the powerful representation ability of random walk. Instead of deterministic neighbor searching used in previous graphical CNNs, we construct multi-scale walk fields (a.k.a. local receptive fields) with random walk paths to depict subgraph structures and advocate graph scalability. To express the internal variations of a walk field, Gaussian mixture models are introduced to encode principal components of walk paths therein. As an analogy to a standard convolution kernel on image, Gaussian models implicitly coordinate those unordered vertices/nodes and edges in a local receptive field after projecting to the gradient space of Gaussian parameters. We further stack graph coarsening upon Gaussian encoding by using dynamic clustering, such that high-level semantics of graph can be well learned like the conventional pooling on image. The experimental results on several public datasets demonstrate the superiority of our proposed WSC method over many state-of-the-arts for graph classification.

</details>

<details>

<summary>2020-01-10 15:29:28 - Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</summary>

- *Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, Hongsheng Li*

- `1910.06809v3` - [abs](http://arxiv.org/abs/1910.06809v3) - [pdf](http://arxiv.org/pdf/1910.06809v3)

> Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.

</details>

<details>

<summary>2020-01-10 19:38:58 - Understanding and Mitigating the Security Risks of Content Inclusion in Web Browsers</summary>

- *Sajjad Arshad*

- `2001.03643v1` - [abs](http://arxiv.org/abs/2001.03643v1) - [pdf](http://arxiv.org/pdf/2001.03643v1)

> Thanks to the wide range of features offered by web browsers, modern websites include various types of content such as JavaScript and CSS in order to create interactive user interfaces. Browser vendors also provided extensions to enhance web browsers with additional useful capabilities that are not necessarily maintained or supported by default.   However, included content can introduce security risks to users of these websites, unbeknownst to both website operators and users. In addition, the browser's interpretation of the resource URLs may be very different from how the web server resolves the URL to determine which resource should be returned to the browser. The URL may not correspond to an actual server-side file system structure at all, or the web server may internally rewrite parts of the URL. This semantic disconnect between web browsers and web servers in interpreting relative paths (path confusion) could be exploited by Relative Path Overwrite (RPO). On the other hand, even tough extensions provide useful additional functionality for web browsers, they are also an increasingly popular vector for attacks. Due to the high degree of privilege extensions can hold, extensions have been abused to inject advertisements into web pages that divert revenue from content publishers and potentially expose users to malware.   In this thesis, I propose novel research into understanding and mitigating the security risks of content inclusion in web browsers to protect website publishers as well as their users.

</details>

<details>

<summary>2020-01-11 05:48:52 - Improving Spoken Language Understanding By Exploiting ASR N-best Hypotheses</summary>

- *Mingda Li, Weitong Ruan, Xinyue Liu, Luca Soldaini, Wael Hamza, Chengwei Su*

- `2001.05284v1` - [abs](http://arxiv.org/abs/2001.05284v1) - [pdf](http://arxiv.org/pdf/2001.05284v1)

> In a modern spoken language understanding (SLU) system, the natural language understanding (NLU) module takes interpretations of a speech from the automatic speech recognition (ASR) module as the input. The NLU module usually uses the first best interpretation of a given speech in downstream tasks such as domain and intent classification. However, the ASR module might misrecognize some speeches and the first best interpretation could be erroneous and noisy. Solely relying on the first best interpretation could make the performance of downstream tasks non-optimal. To address this issue, we introduce a series of simple yet efficient models for improving the understanding of semantics of the input speeches by collectively exploiting the n-best speech interpretations from the ASR module.

</details>

<details>

<summary>2020-01-11 05:50:19 - MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding</summary>

- *Geondo Park, Chihye Han, Wonjun Yoon, Daeshik Kim*

- `2001.03712v1` - [abs](http://arxiv.org/abs/2001.03712v1) - [pdf](http://arxiv.org/pdf/2001.03712v1)

> Visual-semantic embedding enables various tasks such as image-text retrieval, image captioning, and visual question answering. The key to successful visual-semantic embedding is to express visual and textual data properly by accounting for their intricate relationship. While previous studies have achieved much advance by encoding the visual and textual data into a joint space where similar concepts are closely located, they often represent data by a single vector ignoring the presence of multiple important components in an image or text. Thus, in addition to the joint embedding space, we propose a novel multi-head self-attention network to capture various components of visual and textual data by attending to important parts in data. Our approach achieves the new state-of-the-art results in image-text retrieval tasks on MS-COCO and Flicker30K datasets. Through the visualization of the attention maps that capture distinct semantic components at multiple positions in the image and the text, we demonstrate that our method achieves an effective and interpretable visual-semantic joint space.

</details>

<details>

<summary>2020-01-12 14:28:33 - Tensor Graph Convolutional Networks for Text Classification</summary>

- *Xien Liu, Xinxin You, Xiao Zhang, Ji Wu, Ping Lv*

- `2001.05313v1` - [abs](http://arxiv.org/abs/2001.05313v1) - [pdf](http://arxiv.org/pdf/2001.05313v1)

> Compared to sequential learning models, graph-based neural networks exhibit some excellent properties, such as ability capturing global information. In this paper, we investigate graph-based neural networks for text classification problem. A new framework TensorGCN (tensor graph convolutional networks), is presented for this task. A text graph tensor is firstly constructed to describe semantic, syntactic, and sequential contextual information. Then, two kinds of propagation learning perform on the text graph tensor. The first is intra-graph propagation used for aggregating information from neighborhood nodes in a single graph. The second is inter-graph propagation used for harmonizing heterogeneous information between graphs. Extensive experiments are conducted on benchmark datasets, and the results illustrate the effectiveness of our proposed framework. Our proposed TensorGCN presents an effective way to harmonize and integrate heterogeneous information from different kinds of graphs.

</details>

<details>

<summary>2020-01-12 21:54:52 - Detecting New Word Meanings: A Comparison of Word Embedding Models in Spanish</summary>

- *Andrés Torres-Rivera, Juan-Manuel Torres-Moreno*

- `2001.05285v1` - [abs](http://arxiv.org/abs/2001.05285v1) - [pdf](http://arxiv.org/pdf/2001.05285v1)

> Semantic neologisms (SN) are defined as words that acquire a new word meaning while maintaining their form. Given the nature of this kind of neologisms, the task of identifying these new word meanings is currently performed manually by specialists at observatories of neology. To detect SN in a semi-automatic way, we developed a system that implements a combination of the following strategies: topic modeling, keyword extraction, and word sense disambiguation. The role of topic modeling is to detect the themes that are treated in the input text. Themes within a text give clues about the particular meaning of the words that are used, for example: viral has one meaning in the context of computer science (CS) and another when talking about health. To extract keywords, we used TextRank with POS tag filtering. With this method, we can obtain relevant words that are already part of the Spanish lexicon. We use a deep learning model to determine if a given keyword could have a new meaning. Embeddings that are different from all the known meanings (or topics) indicate that a word might be a valid SN candidate. In this study, we examine the following word embedding models: Word2Vec, Sense2Vec, and FastText. The models were trained with equivalent parameters using Wikipedia in Spanish as corpora. Then we used a list of words and their concordances (obtained from our database of neologisms) to show the different embeddings that each model yields. Finally, we present a comparison of these outcomes with the concordances of each word to show how we can determine if a word could be a valid candidate for SN.

</details>

<details>

<summary>2020-01-13 12:02:37 - Reliable and interoperable computational molecular engineering: 2. Semantic interoperability based on the European Materials and Modelling Ontology</summary>

- *Martin Thomas Horsch, Silvia Chiacchiera, Youness Bami, Georg J. Schmitz, Gabriele Mogni, Gerhard Goldbeck, Emanuele Ghedini*

- `2001.04175v1` - [abs](http://arxiv.org/abs/2001.04175v1) - [pdf](http://arxiv.org/pdf/2001.04175v1)

> The European Materials and Modelling Ontology (EMMO) is a top-level ontology designed by the European Materials Modelling Council to facilitate semantic interoperability between platforms, models, and tools in computational molecular engineering, integrated computational materials engineering, and related applications of materials modelling and characterization. Additionally, domain ontologies exist based on data technology developments from specific platforms. The present work discusses the ongoing work on establishing a European Virtual Marketplace Framework, into which diverse platforms can be integrated. It addresses common challenges that arise when marketplace-level domain ontologies are combined with a top-level ontology like the EMMO by ontology alignment.

</details>

<details>

<summary>2020-01-13 12:47:49 - A logic-based relational learning approach to relation extraction: The OntoILPER system</summary>

- *Rinaldo Lima, Bernard Espinasse, Fred Freitas*

- `2001.04192v1` - [abs](http://arxiv.org/abs/2001.04192v1) - [pdf](http://arxiv.org/pdf/2001.04192v1)

> Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute-value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein-protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution.

</details>

<details>

<summary>2020-01-13 13:44:46 - Abstracting Probabilistic Models: A Logical Perspective</summary>

- *Vaishak Belle*

- `1810.02434v3` - [abs](http://arxiv.org/abs/1810.02434v3) - [pdf](http://arxiv.org/pdf/1810.02434v3)

> Abstraction is a powerful idea widely used in science, to model, reason and explain the behavior of systems in a more tractable search space, by omitting irrelevant details. While notions of abstraction have matured for deterministic systems, the case for abstracting probabilistic models is not yet fully understood.   In this paper, we provide a semantical framework for analyzing such abstractions from first principles. We develop the framework in a general way, allowing for expressive languages, including logic-based ones that admit relational and hierarchical constructs with stochastic primitives. We motivate a definition of consistency between a high-level model and its low-level counterpart, but also treat the case when the high-level model is missing critical information present in the low-level model. We prove properties of abstractions, both at the level of the parameter as well as the structure of the models. We conclude with some observations about how abstractions can be derived automatically.

</details>

<details>

<summary>2020-01-13 13:55:44 - Semiring Programming: A Declarative Framework for Generalized Sum Product Problems</summary>

- *Vaishak Belle, Luc De Raedt*

- `1609.06954v2` - [abs](http://arxiv.org/abs/1609.06954v2) - [pdf](http://arxiv.org/pdf/1609.06954v2)

> To solve hard problems, AI relies on a variety of disciplines such as logic, probabilistic reasoning, machine learning and mathematical programming. Although it is widely accepted that solving real-world problems requires an integration amongst these, contemporary representation methodologies offer little support for this.   In an attempt to alleviate this situation, we introduce a new declarative programming framework that provides abstractions of well-known problems such as SAT, Bayesian inference, generative models, and convex optimization. The semantics of programs is defined in terms of first-order structures with semiring labels, which allows us to freely combine and integrate problems from different AI disciplines.

</details>

<details>

<summary>2020-01-13 20:59:14 - On Demand Solid Texture Synthesis Using Deep 3D Networks</summary>

- *Jorge Gutierrez, Julien Rabin, Bruno Galerne, Thomas Hurtut*

- `2001.04528v1` - [abs](http://arxiv.org/abs/2001.04528v1) - [pdf](http://arxiv.org/pdf/2001.04528v1)

> This paper describes a novel approach for on demand volumetric texture synthesis based on a deep learning framework that allows for the generation of high quality 3D data at interactive rates. Based on a few example images of textures, a generative network is trained to synthesize coherent portions of solid textures of arbitrary sizes that reproduce the visual characteristics of the examples along some directions. To cope with memory limitations and computation complexity that are inherent to both high resolution and 3D processing on the GPU, only 2D textures referred to as "slices" are generated during the training stage. These synthetic textures are compared to exemplar images via a perceptual loss function based on a pre-trained deep network. The proposed network is very light (less than 100k parameters), therefore it only requires sustainable training (i.e. few hours) and is capable of very fast generation (around a second for $256^3$ voxels) on a single GPU. Integrated with a spatially seeded PRNG the proposed generator network directly returns an RGB value given a set of 3D coordinates. The synthesized volumes have good visual results that are at least equivalent to the state-of-the-art patch based approaches. They are naturally seamlessly tileable and can be fully generated in parallel.

</details>

<details>

<summary>2020-01-14 02:05:14 - Bi-Decoder Augmented Network for Neural Machine Translation</summary>

- *Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang, Deng Cai*

- `2001.04586v1` - [abs](http://arxiv.org/abs/2001.04586v1) - [pdf](http://arxiv.org/pdf/2001.04586v1)

> Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder-decoder framework is the mainstream among all the methods. It's obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach.

</details>

<details>

<summary>2020-01-14 04:53:30 - Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval</summary>

- *Lu Wang, Jie Yang*

- `2001.04625v1` - [abs](http://arxiv.org/abs/2001.04625v1) - [pdf](http://arxiv.org/pdf/2001.04625v1)

> Due to the superiority in similarity computation and database storage for large-scale multiple modalities data, cross-modal hashing methods have attracted extensive attention in similarity retrieval across the heterogeneous modalities. However, there are still some limitations to be further taken into account: (1) most current CMH methods transform real-valued data points into discrete compact binary codes under the binary constraints, limiting the capability of representation for original data on account of abundant loss of information and producing suboptimal hash codes; (2) the discrete binary constraint learning model is hard to solve, where the retrieval performance may greatly reduce by relaxing the binary constraints for large quantization error; (3) handling the learning problem of CMH in a symmetric framework, leading to difficult and complex optimization objective. To address above challenges, in this paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method is proposed. Specifically, ACQH learns the projection matrixs of heterogeneous modalities data points for transforming query into a low-dimensional real-valued vector in latent semantic space and constructs the stacked compositional quantization embedding in a coarse-to-fine manner for indicating database points by a series of learnt real-valued codeword in the codebook with the help of pointwise label information regression simultaneously. Besides, the unified hash codes across modalities can be directly obtained by the discrete iterative optimization framework devised in the paper. Comprehensive experiments on diverse three benchmark datasets have shown the effectiveness and rationality of ACQH.

</details>

<details>

<summary>2020-01-14 08:39:51 - Effects of annotation granularity in deep learning models for histopathological images</summary>

- *Jiangbo Shi, Zeyu Gao, Haichuan Zhang, Pargorn Puttapirat, Chunbao Wang, Xiangrong Zhang, Chen Li*

- `2001.04663v1` - [abs](http://arxiv.org/abs/2001.04663v1) - [pdf](http://arxiv.org/pdf/2001.04663v1)

> Pathological is crucial to cancer diagnosis. Usually, Pathologists draw their conclusion based on observed cell and tissue structure on histology slides. Rapid development in machine learning, especially deep learning have established robust and accurate classifiers. They are being used to analyze histopathological slides and assist pathologists in diagnosis. Most machine learning systems rely heavily on annotated data sets to gain experiences and knowledge to correctly and accurately perform various tasks such as classification and segmentation. This work investigates different granularity of annotations in histopathological data set including image-wise, bounding box, ellipse-wise, and pixel-wise to verify the influence of annotation in pathological slide on deep learning models. We design corresponding experiments to test classification and segmentation performance of deep learning models based on annotations with different annotation granularity. In classification, state-of-the-art deep learning-based classifiers perform better when trained by pixel-wise annotation dataset. On average, precision, recall and F1-score improves by 7.87%, 8.83% and 7.85% respectively. Thus, it is suggested that finer granularity annotations are better utilized by deep learning algorithms in classification tasks. Similarly, semantic segmentation algorithms can achieve 8.33% better segmentation accuracy when trained by pixel-wise annotations. Our study shows not only that finer-grained annotation can improve the performance of deep learning models, but also help extracts more accurate phenotypic information from histopathological slides. Intelligence systems trained on granular annotations may help pathologists inspecting certain regions for better diagnosis. The compartmentalized prediction approach similar to this work may contribute to phenotype and genotype association studies.

</details>

<details>

<summary>2020-01-14 10:12:50 - Balancing the composition of word embeddings across heterogenous data sets</summary>

- *Stephanie Brandl, David Lassner, Maximilian Alber*

- `2001.04693v1` - [abs](http://arxiv.org/abs/2001.04693v1) - [pdf](http://arxiv.org/pdf/2001.04693v1)

> Word embeddings capture semantic relationships based on contextual information and are the basis for a wide variety of natural language processing applications. Notably these relationships are solely learned from the data and subsequently the data composition impacts the semantic of embeddings which arguably can lead to biased word vectors. Given qualitatively different data subsets, we aim to align the influence of single subsets on the resulting word vectors, while retaining their quality. In this regard we propose a criteria to measure the shift towards a single data subset and develop approaches to meet both objectives. We find that a weighted average of the two subset embeddings balances the influence of those subsets while word similarity performance decreases. We further propose a promising optimization approach to balance influences and quality of word embeddings.

</details>

<details>

<summary>2020-01-14 11:45:14 - Epistemic Graphs for Representing and Reasoning with Positive and Negative Influences of Arguments</summary>

- *Anthony Hunter, Sylwia Polberg, Matthias Thimm*

- `1802.07489v2` - [abs](http://arxiv.org/abs/1802.07489v2) - [pdf](http://arxiv.org/pdf/1802.07489v2)

> This paper introduces epistemic graphs as a generalization of the epistemic approach to probabilistic argumentation. In these graphs, an argument can be believed or disbelieved up to a given degree, thus providing a more fine--grained alternative to the standard Dung's approaches when it comes to determining the status of a given argument. Furthermore, the flexibility of the epistemic approach allows us to both model the rationale behind the existing semantics as well as completely deviate from them when required. Epistemic graphs can model both attack and support as well as relations that are neither support nor attack. The way other arguments influence a given argument is expressed by the epistemic constraints that can restrict the belief we have in an argument with a varying degree of specificity. The fact that we can specify the rules under which arguments should be evaluated and we can include constraints between unrelated arguments permits the framework to be more context--sensitive. It also allows for better modelling of imperfect agents, which can be important in multi--agent applications.

</details>

<details>

<summary>2020-01-14 12:07:02 - Learning with Interpretable Structure from Gated RNN</summary>

- *Bo-Jian Hou, Zhi-Hua Zhou*

- `1810.10708v2` - [abs](http://arxiv.org/abs/1810.10708v2) - [pdf](http://arxiv.org/pdf/1810.10708v2)

> The interpretability of deep learning models has raised extended attention these years. It will be beneficial if we can learn an interpretable structure from deep learning models. In this paper, we focus on Recurrent Neural Networks~(RNNs) especially gated RNNs whose inner mechanism is still not clearly understood. We find that Finite State Automaton~(FSA) that processes sequential data has more interpretable inner mechanism according to the definition of interpretability and can be learned from RNNs as the interpretable structure. We propose two methods to learn FSA from RNN based on two different clustering methods. With the learned FSA and via experiments on artificial and real datasets, we find that FSA is more trustable than the RNN from which it learned, which gives FSA a chance to substitute RNNs in applications involving humans' lives or dangerous facilities. Besides, we analyze how the number of gates affects the performance of RNN. Our result suggests that gate in RNN is important but the less the better, which could be a guidance to design other RNNs. Finally, we observe that the FSA learned from RNN gives semantic aggregated states and its transition graph shows us a very interesting vision of how RNNs intrinsically handle text classification tasks.

</details>

<details>

<summary>2020-01-14 13:10:01 - On Equivalence and Cores for Incomplete Databases in Open and Closed Worlds</summary>

- *Henrik Forssell, Evgeny Kharlamov, Evgenij Thorstensen*

- `2001.04757v1` - [abs](http://arxiv.org/abs/2001.04757v1) - [pdf](http://arxiv.org/pdf/2001.04757v1)

> Data exchange heavily relies on the notion of incomplete database instances. Several semantics for such instances have been proposed and include open (OWA), closed (CWA), and open-closed (OCWA) world. For all these semantics important questions are: whether one incomplete instance semantically implies another; when two are semantically equivalent; and whether a smaller or smallest semantically equivalent instance exists. For OWA and CWA these questions are fully answered. For several variants of OCWA, however, they remain open. In this work we adress these questions for Closed Powerset semantics and the OCWA semantics of Libkin and Sirangelo, 2011. We define a new OCWA semantics, called OCWA*, in terms of homomorphic covers that subsumes both semantics, and characterize semantic implication and equivalence in terms of such covers. This characterization yields a guess-and-check algorithm to decide equivalence, and shows that the problem is NP-complete. For the minimization problem we show that for several common notions of minimality there is in general no unique minimal equivalent instance for Closed Powerset semantics, and consequently not for the more expressive OCWA* either. However, for Closed Powerset semantics we show that one can find, for any incomplete database, a unique finite set of its subinstances which are subinstances (up to renaming of nulls) of all instances semantically equivalent to the original incomplete one. We study properties of this set, and extend the analysis to OCWA*.

</details>

<details>

<summary>2020-01-14 17:48:52 - Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning</summary>

- *Roei Schuster, Tal Schuster, Yoav Meri, Vitaly Shmatikov*

- `2001.04935v1` - [abs](http://arxiv.org/abs/2001.04935v1) - [pdf](http://arxiv.org/pdf/2001.04935v1)

> Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word "meaning" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks.   Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the "meaning" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another.   An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model.

</details>

<details>

<summary>2020-01-14 18:15:30 - The geometry of syntax and semantics for directed file transformations</summary>

- *Steve Huntsman, Michael Robinson*

- `2001.04952v1` - [abs](http://arxiv.org/abs/2001.04952v1) - [pdf](http://arxiv.org/pdf/2001.04952v1)

> We introduce a conceptual framework that associates syntax and semantics with vertical and horizontal directions in principal bundles and related constructions. This notion of geometry corresponds to a mechanism for performing goal-directed file transformations such as "eliminate unsafe syntax" and suggests various engineering practices.

</details>

<details>

<summary>2020-01-15 03:13:18 - One-to-one Mapping for Unpaired Image-to-image Translation</summary>

- *Zengming Shen, S. Kevin Zhou, Yifan Chen, Bogdan Georgescu, Xuqi Liu, Thomas S. Huang*

- `1909.04110v6` - [abs](http://arxiv.org/abs/1909.04110v6) - [pdf](http://arxiv.org/pdf/1909.04110v6)

> Recently image-to-image translation has attracted significant interests in the literature, starting from the successful use of the generative adversarial network (GAN), to the introduction of cyclic constraint, to extensions to multiple domains. However, in existing approaches, there is no guarantee that the mapping between two image domains is unique or one-to-one. Here we propose a self-inverse network learning approach for unpaired image-to-image translation. Building on top of CycleGAN, we learn a self-inverse function by simply augmenting the training samples by swapping inputs and outputs during training and with separated cycle consistency loss for each mapping direction. The outcome of such learning is a proven one-to-one mapping function. Our extensive experiments on a variety of datasets, including cross-modal medical image synthesis, object transfiguration, and semantic labeling, consistently demonstrate clear improvement over the CycleGAN method both qualitatively and quantitatively. Especially our proposed method reaches the state-of-the-art result on the cityscapes benchmark dataset for the label to photo unpaired directional image translation.

</details>

<details>

<summary>2020-01-15 08:46:42 - The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings</summary>

- *Tomer Galanti, Lior Wolf, Sagie Benaim*

- `1709.00074v3` - [abs](http://arxiv.org/abs/1709.00074v3) - [pdf](http://arxiv.org/pdf/1709.00074v3)

> We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem.   We identify the abstract notion of aligning two domains in a semantic way with concrete terms of minimal relative complexity. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that it is reasonable to expect the minimal complexity mapping to be unique. The measured complexity used is directly related to the depth of the neural networks being learned and a semantically aligned mapping could then be captured simply by learning using architectures that are not much bigger than the minimal architecture.   Various predictions are made based on the hypothesis that semantic alignment can be captured by the minimal mapping. These are verified extensively. In addition, a new mapping algorithm is proposed and shown to lead to better mapping results.

</details>

<details>

<summary>2020-01-15 11:09:43 - Weakly-Supervised Video Moment Retrieval via Semantic Completion Network</summary>

- *Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, Huasheng Liu*

- `1911.08199v3` - [abs](http://arxiv.org/abs/1911.08199v3) - [pdf](http://arxiv.org/pdf/1911.08199v3)

> Video moment retrieval is to search the moment that is most relevant to the given natural language query. Existing methods are mostly trained in a fully-supervised setting, which requires the full annotations of temporal boundary for each query. However, manually labeling the annotations is actually time-consuming and expensive. In this paper, we propose a novel weakly-supervised moment retrieval framework requiring only coarse video-level annotations for training. Specifically, we devise a proposal generation module that aggregates the context information to generate and score all candidate proposals in one single pass. We then devise an algorithm that considers both exploitation and exploration to select top-K proposals. Next, we build a semantic completion module to measure the semantic similarity between the selected proposals and query, compute reward and provide feedbacks to the proposal generation module for scoring refinement. Experiments on the ActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed method.

</details>

<details>

<summary>2020-01-15 12:01:23 - Ready, set, Go! Data-race detection and the Go language</summary>

- *Daniel Schnetzer Fava, Martin Steffen*

- `1910.12643v3` - [abs](http://arxiv.org/abs/1910.12643v3) - [pdf](http://arxiv.org/pdf/1910.12643v3)

> Data races are often discussed in the context of lock acquisition and release, with race-detection algorithms routinely relying on vector clocks as a means of capturing the relative ordering of events from different threads. In this paper, we present a data-race detector for a language with channel communication as its sole synchronization primitive, and provide a semantics directly tied to the happens-before relation, thus forging the notion of vector clocks.

</details>

<details>

<summary>2020-01-15 16:49:26 - Scout: Rapid Exploration of Interface Layout Alternatives through High-Level Design Constraints</summary>

- *Amanda Swearngin, Chenglong Wang, Alannah Oleson, James Fogarty, Amy J. Ko*

- `2001.05424v1` - [abs](http://arxiv.org/abs/2001.05424v1) - [pdf](http://arxiv.org/pdf/2001.05424v1)

> Although exploring alternatives is fundamental to creating better interface designs, current processes for creating alternatives are generally manual, limiting the alternatives a designer can explore. We present Scout, a system that helps designers rapidly explore alternatives through mixed-initiative interaction with high-level constraints and design feedback. Prior constraint-based layout systems use low-level spatial constraints and generally produce a single design. Tosupport designer exploration of alternatives, Scout introduces high-level constraints based on design concepts (e.g.,~semantic structure, emphasis, order) and formalizes them into low-level spatial constraints that a solver uses to generate potential layouts. In an evaluation with 18 interface designers, we found that Scout: (1) helps designers create more spatially diverse layouts with similar quality to those created with a baseline tool and (2) can help designers avoid a linear design process and quickly ideate layouts they do not believe they would have thought of on their own.

</details>

<details>

<summary>2020-01-15 23:02:14 - NERO: A Neural Rule Grounding Framework for Label-Efficient Relation Extraction</summary>

- *Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, Xiang Ren*

- `1909.02177v4` - [abs](http://arxiv.org/abs/1909.02177v4) - [pdf](http://arxiv.org/pdf/1909.02177v4)

> Deep neural models for relation extraction tend to be less reliable when perfectly labeled data is limited, despite their success in label-sufficient scenarios. Instead of seeking more instance-level labels from human annotators, here we propose to annotate frequent surface patterns to form labeling rules. These rules can be automatically mined from large text corpora and generalized via a soft rule matching mechanism. Prior works use labeling rules in an exact matching fashion, which inherently limits the coverage of sentence matching and results in the low-recall issue. In this paper, we present a neural approach to ground rules for RE, named NERO, which jointly learns a relation extraction module and a soft matching module. One can employ any neural relation extraction models as the instantiation for the RE module. The soft matching module learns to match rules with semantically similar sentences such that raw corpora can be automatically labeled and leveraged by the RE module (in a much better coverage) as augmented supervision, in addition to the exactly matched sentences. Extensive experiments and analysis on two public and widely-used datasets demonstrate the effectiveness of the proposed NERO framework, comparing with both rule-based and semi-supervised methods. Through user studies, we find that the time efficiency for a human to annotate rules and sentences are similar (0.30 vs. 0.35 min per label). In particular, NERO's performance using 270 rules is comparable to the models trained using 3,000 labeled sentences, yielding a 9.5x speedup. Moreover, NERO can predict for unseen relations at test time and provide interpretable predictions. We release our code to the community for future research.

</details>

<details>

<summary>2020-01-16 02:08:54 - Action Semantics Network: Considering the Effects of Actions in Multiagent Systems</summary>

- *Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao*

- `1907.11461v3` - [abs](http://arxiv.org/abs/1907.11461v3) - [pdf](http://arxiv.org/pdf/1907.11461v3)

> In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them explicitly consider action semantics between agents that different actions have different influences on other agents. In this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between them. ASN can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance of state-of-the-art DRL approaches compared with several network architectures.

</details>

<details>

<summary>2020-01-16 17:09:15 - Elements of Scheduling</summary>

- *Jan Karel Lenstra, David B. Shmoys*

- `2001.06005v1` - [abs](http://arxiv.org/abs/2001.06005v1) - [pdf](http://arxiv.org/pdf/2001.06005v1)

> In the winter of 1976, Alexander Rinnooy Kan and Jan Karel Lenstra defended their PhD theses at the University of Amsterdam. Gene Lawler was on their committees. It was a natural idea to turn the theses into a textbook on scheduling. They set out to compile a survey with Ron Graham (1979), but progress on the book was hampered by the many research opportunities offered by the field. After David Shmoys joined the team in the mid 1980's, several chapters were drafted, and the survey was rewritten (1993). Gene passed away in 1994. Colleagues were asked to contribute chapters or to complete existing drafts. However, by the turn of the century the project was losing its momentum, and finite convergence to completion fell beyond our reach.   Over the years, several chapters have been used in the classroom. We continue to receive requests from colleagues who look for a text on the elements of scheduling at an advanced undergraduate or early graduate level. This document is a compilation of what currently exists. We have made a marginal effort in patching it up at some places but is essentially what was written long ago. We did make an attempt to include most of the citations in the bibliography.

</details>

<details>

<summary>2020-01-16 17:30:36 - Lexical Sememe Prediction using Dictionary Definitions by Capturing Local Semantic Correspondence</summary>

- *Jiaju Du, Fanchao Qi, Maosong Sun, Zhiyuan Liu*

- `2001.05954v1` - [abs](http://arxiv.org/abs/2001.05954v1) - [pdf](http://arxiv.org/pdf/2001.05954v1)

> Sememes, defined as the minimum semantic units of human languages in linguistics, have been proven useful in many NLP tasks. Since manual construction and update of sememe knowledge bases (KBs) are costly, the task of automatic sememe prediction has been proposed to assist sememe annotation. In this paper, we explore the approach of applying dictionary definitions to predicting sememes for unannotated words. We find that sememes of each word are usually semantically matched to different words in its dictionary definition, and we name this matching relationship local semantic correspondence. Accordingly, we propose a Sememe Correspondence Pooling (SCorP) model, which is able to capture this kind of matching to predict sememes. We evaluate our model and baseline methods on a famous sememe KB HowNet and find that our model achieves state-of-the-art performance. Moreover, further quantitative analysis shows that our model can properly learn the local semantic correspondence between sememes and words in dictionary definitions, which explains the effectiveness of our model. The source codes of this paper can be obtained from https://github.com/thunlp/scorp.

</details>

<details>

<summary>2020-01-16 18:05:46 - #MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media</summary>

- *Viet Duong, Phu Pham, Ritwik Bose, Jiebo Luo*

- `2001.05970v1` - [abs](http://arxiv.org/abs/2001.05970v1) - [pdf](http://arxiv.org/pdf/2001.05970v1)

> Recently, the emergence of the #MeToo trend on social media has empowered thousands of people to share their own sexual harassment experiences. This viral trend, in conjunction with the massive personal information and content available on Twitter, presents a promising opportunity to extract data driven insights to complement the ongoing survey based studies about sexual harassment in college. In this paper, we analyze the influence of the #MeToo trend on a pool of college followers. The results show that the majority of topics embedded in those #MeToo tweets detail sexual harassment stories, and there exists a significant correlation between the prevalence of this trend and official reports on several major geographical regions. Furthermore, we discover the outstanding sentiments of the #MeToo tweets using deep semantic meaning representations and their implications on the affected users experiencing different types of sexual harassment. We hope this study can raise further awareness regarding sexual misconduct in academia.

</details>

<details>

<summary>2020-01-16 18:06:43 - User-in-the-loop Adaptive Intent Detection for Instructable Digital Assistant</summary>

- *Nicolas Lair, Clément Delgrange, David Mugisha, Jean-Michel Dussoux, Pierre-Yves Oudeyer, Peter Ford Dominey*

- `2001.06007v1` - [abs](http://arxiv.org/abs/2001.06007v1) - [pdf](http://arxiv.org/pdf/2001.06007v1)

> People are becoming increasingly comfortable using Digital Assistants (DAs) to interact with services or connected objects. However, for non-programming users, the available possibilities for customizing their DA are limited and do not include the possibility of teaching the assistant new tasks. To make the most of the potential of DAs, users should be able to customize assistants by instructing them through Natural Language (NL). To provide such functionalities, NL interpretation in traditional assistants should be improved: (1) The intent identification system should be able to recognize new forms of known intents, and to acquire new intents as they are expressed by the user. (2) In order to be adaptive to novel intents, the Natural Language Understanding module should be sample efficient, and should not rely on a pretrained model. Rather, the system should continuously collect the training data as it learns new intents from the user. In this work, we propose AidMe (Adaptive Intent Detection in Multi-Domain Environments), a user-in-the-loop adaptive intent detection framework that allows the assistant to adapt to its user by learning his intents as their interaction progresses. AidMe builds its repertoire of intents and collects data to train a model of semantic similarity evaluation that can discriminate between the learned intents and autonomously discover new forms of known intents. AidMe addresses two major issues - intent learning and user adaptation - for instructable digital assistants. We demonstrate the capabilities of AidMe as a standalone system by comparing it with a one-shot learning system and a pretrained NLU module through simulations of interactions with a user. We also show how AidMe can smoothly integrate to an existing instructable digital assistant.

</details>

<details>

<summary>2020-01-16 21:20:48 - FasterSeg: Searching for Faster Real-time Semantic Segmentation</summary>

- *Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang Wang*

- `1912.10917v2` - [abs](http://arxiv.org/abs/1912.10917v2) - [pdf](http://arxiv.org/pdf/1912.10917v2)

> We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to "collapsing" to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model's accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy.

</details>

<details>

<summary>2020-01-17 10:08:32 - Adding Context to Concept Trees</summary>

- *Kieran Greer*

- `1606.05597v5` - [abs](http://arxiv.org/abs/1606.05597v5) - [pdf](http://arxiv.org/pdf/1606.05597v5)

> A Concept Tree is a structure for storing knowledge where the trees are stored in a database called a Concept Base. It sits between the highly distributed neural architectures and the distributed information systems, with the intention of bringing brain-like and computer systems closer together. Concept Trees can grow from the semi-structured sources when consistent sequences of concepts are presented. Each tree ideally represents a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore also a type of semantic network and would benefit from having a consistent level of context for each node. A consistent build process is managed through a 'counting rule' and some other rules that can normalise the database structure. This restricted structure can then be complimented and enriched by the more dynamic context. It is also suggested to use the linking structure of the licas system [15] as a basis for the context links, where the mathematical model is extended further to define this. A number of tests have demonstrated the soundness of the architecture. Building the trees from text documents shows that the tree structure could be inherent in natural language. Then, two types of query language are described. Both of these can perform consistent query processes to return knowledge to the user and even enhance the query with new knowledge. This is supported even further with direct comparisons to a cognitive model, also being developed by the author.

</details>

<details>

<summary>2020-01-17 10:09:24 - Cut-Based Graph Learning Networks to Discover Compositional Structure of Sequential Video Data</summary>

- *Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo, Byoung-Tak Zhang*

- `2001.07613v1` - [abs](http://arxiv.org/abs/2001.07613v1) - [pdf](http://arxiv.org/pdf/2001.07613v1)

> Conventional sequential learning methods such as Recurrent Neural Networks (RNNs) focus on interactions between consecutive inputs, i.e. first-order Markovian dependency. However, most of sequential data, as seen with videos, have complex dependency structures that imply variable-length semantic flows and their compositions, and those are hard to be captured by conventional methods. Here, we propose Cut-Based Graph Learning Networks (CB-GLNs) for learning video data by discovering these complex structures of the video. The CB-GLNs represent video data as a graph, with nodes and edges corresponding to frames of the video and their dependencies respectively. The CB-GLNs find compositional dependencies of the data in multilevel graph forms via a parameterized kernel with graph-cut and a message passing framework. We evaluate the proposed method on the two different tasks for video understanding: Video theme classification (Youtube-8M dataset) and Video Question and Answering (TVQA dataset). The experimental results show that our model efficiently learns the semantic compositional structure of video data. Furthermore, our model achieves the highest performance in comparison to other baseline methods.

</details>

<details>

<summary>2020-01-17 11:22:18 - Channels' Confirmation and Predictions' Confirmation: from the Medical Test to the Raven Paradox</summary>

- *Chenguang Lu*

- `2001.07566v1` - [abs](http://arxiv.org/abs/2001.07566v1) - [pdf](http://arxiv.org/pdf/2001.07566v1)

> After long arguments between positivism and falsificationism, the verification of universal hypotheses was replaced with the confirmation of uncertain major premises. Unfortunately, Hemple discovered the Raven Paradox (RP). Then, Carnap used the logical probability increment as the confirmation measure. So far, many confirmation measures have been proposed. Measure F among them proposed by Kemeny and Oppenheim possesses symmetries and asymmetries proposed by Elles and Fitelson, monotonicity proposed by Greco et al., and normalizing property suggested by many researchers. Based on the semantic information theory, a measure b* similar to F is derived from the medical test. Like the likelihood ratio, b* and F can only indicate the quality of channels or the testing means instead of the quality of probability predictions. And, it is still not easy to use b*, F, or another measure to clarify the RP. For this reason, measure c* similar to the correct rate is derived. The c* has the simple form: (a-c)/max(a, c); it supports the Nicod Criterion and undermines the Equivalence Condition, and hence, can be used to eliminate the RP. Some examples are provided to show why it is difficult to use one of popular confirmation measures to eliminate the RP. Measure F, b*, and c* indicate that fewer counterexamples' existence is more essential than more positive examples' existence, and hence, are compatible with Popper's falsification thought.

</details>

<details>

<summary>2020-01-17 14:02:50 - Grover's Algorithm and Many-Valued Quantum Logic</summary>

- *Samuel Hunt, Maximilien Gadouleau*

- `2001.06316v1` - [abs](http://arxiv.org/abs/2001.06316v1) - [pdf](http://arxiv.org/pdf/2001.06316v1)

> As the engineering endeavour to realise quantum computers progresses, we consider that such machines need not rely on binary as their de facto unit of information. We investigate Grover's algorithm under a generalised quantum circuit model, in which the information and transformations can be expressed in any arity, and analyse the structural and behavioural properties while preserving the semantics; namely, searching for the unique preimage to an output a function. We conclude by demonstrating that the generalised procedure retains $O(\sqrt{N})$ time complexity.

</details>

<details>

<summary>2020-01-17 16:20:00 - InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</summary>

- *Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, Jian Tang*

- `1908.01000v3` - [abs](http://arxiv.org/abs/1908.01000v3) - [pdf](http://arxiv.org/pdf/1908.01000v3)

> This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.

</details>

<details>

<summary>2020-01-18 08:18:19 - Adaptive Parameterization for Neural Dialogue Generation</summary>

- *Hengyi Cai, Hongshen Chen, Cheng Zhang, Yonghao Song, Xiaofang Zhao, Dawei Yin*

- `2001.06626v1` - [abs](http://arxiv.org/abs/2001.06626v1) - [pdf](http://arxiv.org/pdf/2001.06626v1)

> Neural conversation systems generate responses based on the sequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned parameters to generate responses for given input contexts. When confronting diverse conversations, its adaptability is rather limited and the model is hence prone to generate generic responses. In this work, we propose an {\bf Ada}ptive {\bf N}eural {\bf D}ialogue generation model, \textsc{AdaND}, which manages various conversations with conversation-specific parameterization. For each conversation, the model generates parameters of the encoder-decoder by referring to the input context. In particular, we propose two adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization mechanism. The context-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics. Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations.

</details>

<details>

<summary>2020-01-18 20:01:55 - Charting the Right Manifold: Manifold Mixup for Few-shot Learning</summary>

- *Puneet Mangla, Mayank Singh, Abhishek Sinha, Nupur Kumari, Vineeth N Balasubramanian, Balaji Krishnamurthy*

- `1907.12087v4` - [abs](http://arxiv.org/abs/1907.12087v4) - [pdf](http://arxiv.org/pdf/1907.12087v4)

> Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique - Manifold Mixup focuses on learning a general-purpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance. We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3-8 %. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.

</details>

<details>

<summary>2020-01-19 10:39:37 - An Approach for Time-aware Domain-based Social Influence Prediction</summary>

- *Bilal Abu-Salih, Kit Yan Chan, Omar Al-Kadi, Marwan Al-Tawil, Pornpit Wongthongtham, Tomayess Issa, Heba Saadeh, Malak Al-Hassan, Bushra Bremie, Abdulaziz Albahlal*

- `2001.07838v1` - [abs](http://arxiv.org/abs/2001.07838v1) - [pdf](http://arxiv.org/pdf/2001.07838v1)

> Online Social Networks(OSNs) have established virtual platforms enabling people to express their opinions, interests and thoughts in a variety of contexts and domains, allowing legitimate users as well as spammers and other untrustworthy users to publish and spread their content. Hence, the concept of social trust has attracted the attention of information processors/data scientists and information consumers/business firms. One of the main reasons for acquiring the value of Social Big Data (SBD) is to provide frameworks and methodologies using which the credibility of OSNs users can be evaluated. These approaches should be scalable to accommodate large-scale social data. Hence, there is a need for well comprehending of social trust to improve and expand the analysis process and inferring the credibility of SBD. Given the exposed environment's settings and fewer limitations related to OSNs, the medium allows legitimate and genuine users as well as spammers and other low trustworthy users to publish and spread their content. Hence, this paper presents an approach incorporates semantic analysis and machine learning modules to measure and predict users' trustworthiness in numerous domains in different time periods. The evaluation of the conducted experiment validates the applicability of the incorporated machine learning techniques to predict highly trustworthy domain-based users.

</details>

<details>

<summary>2020-01-19 23:03:47 - Correcting Knowledge Base Assertions</summary>

- *Jiaoyan Chen, Xi Chen, Ian Horrocks, Ernesto Jimenez-Ruiz, Erik B. Myklebus*

- `2001.06917v1` - [abs](http://arxiv.org/abs/2001.06917v1) - [pdf](http://arxiv.org/pdf/2001.06917v1)

> The usefulness and usability of knowledge bases (KBs) is often limited by quality issues. One common issue is the presence of erroneous assertions, often caused by lexical or semantic confusion. We study the problem of correcting such assertions, and present a general correction framework which combines lexical matching, semantic embedding, soft constraint mining and semantic consistency checking. The framework is evaluated using DBpedia and an enterprise medical KB.

</details>

<details>

<summary>2020-01-20 02:19:13 - Nested-Wasserstein Self-Imitation Learning for Sequence Generation</summary>

- *Ruiyi Zhang, Changyou Chen, Zhe Gan, Zheng Wen, Wenlin Wang, Lawrence Carin*

- `2001.06944v1` - [abs](http://arxiv.org/abs/2001.06944v1) - [pdf](http://arxiv.org/pdf/2001.06944v1)

> Reinforcement learning (RL) has been widely studied for improving sequence-generation models. However, the conventional rewards used for RL training typically cannot capture sufficient semantic information and therefore render model bias. Further, the sparse and delayed rewards make RL exploration inefficient. To alleviate these issues, we propose the concept of nested-Wasserstein distance for distributional semantic matching. To further exploit it, a novel nested-Wasserstein self-imitation learning framework is developed, encouraging the model to exploit historical high-rewarded sequences for enhanced exploration and better semantic matching. Our solution can be understood as approximately executing proximal policy optimization with Wasserstein trust-regions. Experiments on a variety of unconditional and conditional sequence-generation tasks demonstrate the proposed approach consistently leads to improved performance.

</details>

<details>

<summary>2020-01-20 14:45:21 - Checking Smart Contracts with Structural Code Embedding</summary>

- *Zhipeng Gao, Lingxiao Jiang, Xin Xia, David Lo, John Grundy*

- `2001.07125v1` - [abs](http://arxiv.org/abs/2001.07125v1) - [pdf](http://arxiv.org/pdf/2001.07125v1)

> Smart contracts have been increasingly used together with blockchains to automate financial and business transactions. However, many bugs and vulnerabilities have been identified in many contracts which raises serious concerns about smart contract security, not to mention that the blockchain systems on which the smart contracts are built can be buggy. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability. In this paper, we propose an automated approach to learn characteristics of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. Our new approach is based on word embeddings and vector space comparison. We parse smart contract code into word streams with code structural information, convert code elements (e.g., statements, functions) into numerical vectors that are supposed to encode the code syntax and semantics, and compare the similarities among the vectors encoding code and known bugs, to identify potential issues. We have implemented the approach in a prototype, named SmartEmbed. Results show that our tool can effectively identify many repetitive instances of Solidity code, where the clone ratio is around 90\%. Code clones such as type-III or even type-IV semantic clones can also be detected accurately. Our tool can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. Our tool can also help to efficiently validate any given smart contract against a known set of bugs, which can help to improve the users' confidence in the reliability of the contract.   The anonymous replication packages can be accessed at: https://drive.google.com/file/d/1kauLT3y2IiHPkUlVx4FSTda-dVAyL4za/view?usp=sharing, and evaluated it with more than 22,000 smart contracts collected from the Ethereum blockchain.

</details>

<details>

<summary>2020-01-20 16:04:51 - HSCJN: A Holistic Semantic Constraint Joint Network for Diverse Response Generation</summary>

- *Yiru Wang, Pengda Si, Zeyang Lei, Guangxu Xun, Yujiu Yang*

- `1912.00380v2` - [abs](http://arxiv.org/abs/1912.00380v2) - [pdf](http://arxiv.org/pdf/1912.00380v2)

> The sequence-to-sequence (Seq2Seq) model generates target words iteratively given the previously observed words during decoding process, which results in the loss of the holistic semantics in the target response and the complete semantic relationship between responses and dialogue histories. In this paper, we propose a generic diversity-promoting joint network, called Holistic Semantic Constraint Joint Network (HSCJN), enhancing the global sentence information, and then regularizing the objective function with penalizing the low entropy output. Our network introduces more target information to improve diversity, and captures direct semantic information to better constrain the relevance simultaneously. Moreover, the proposed method can be easily applied to any Seq2Seq structure. Extensive experiments on several dialogue corpuses show that our method effectively improves both semantic consistency and diversity of generated responses, and achieves better performance than other competitive methods.

</details>

<details>

<summary>2020-01-20 23:17:27 - LATTE: Latent Type Modeling for Biomedical Entity Linking</summary>

- *Ming Zhu, Busra Celikkaya, Parminder Bhatia, Chandan K. Reddy*

- `1911.09787v2` - [abs](http://arxiv.org/abs/1911.09787v2) - [pdf](http://arxiv.org/pdf/1911.09787v2)

> Entity linking is the task of linking mentions of named entities in natural language text, to entities in a curated knowledge-base. This is of significant importance in the biomedical domain, where it could be used to semantically annotate a large volume of clinical records and biomedical literature, to standardized concepts described in an ontology such as Unified Medical Language System (UMLS). We observe that with precise type information, entity disambiguation becomes a straightforward task. However, fine-grained type information is usually not available in biomedical domain. Thus, we propose LATTE, a LATent Type Entity Linking model, that improves entity linking by modeling the latent fine-grained type information about mentions and entities. Unlike previous methods that perform entity linking directly between the mentions and the entities, LATTE jointly does entity disambiguation, and latent fine-grained type learning, without direct supervision. We evaluate our model on two biomedical datasets: MedMentions, a large scale public dataset annotated with UMLS concepts, and a de-identified corpus of dictated doctor's notes that has been annotated with ICD concepts. Extensive experimental evaluation shows our model achieves significant performance improvements over several state-of-the-art techniques.

</details>

<details>

<summary>2020-01-21 00:33:40 - AutoMATES: Automated Model Assembly from Text, Equations, and Software</summary>

- *Adarsh Pyarelal, Marco A. Valenzuela-Escarcega, Rebecca Sharp, Paul D. Hein, Jon Stephens, Pratik Bhandari, HeuiChan Lim, Saumya Debray, Clayton T. Morrison*

- `2001.07295v1` - [abs](http://arxiv.org/abs/2001.07295v1) - [pdf](http://arxiv.org/pdf/2001.07295v1)

> Models of complicated systems can be represented in different ways - in scientific papers, they are represented using natural language text as well as equations. But to be of real use, they must also be implemented as software, thus making code a third form of representing models. We introduce the AutoMATES project, which aims to build semantically-rich unified representations of models from scientific code and publications to facilitate the integration of computational models from different domains and allow for modeling large, complicated systems that span multiple domains and levels of abstraction.

</details>

<details>

<summary>2020-01-21 08:04:13 - A multi-agent ontologies-based clinical decision support system</summary>

- *Ying Shen, Jacquet-Andrieu Armelle, Joël Colloc*

- `2001.07374v1` - [abs](http://arxiv.org/abs/2001.07374v1) - [pdf](http://arxiv.org/pdf/2001.07374v1)

> Clinical decision support systems combine knowledge and data from a variety of sources, represented by quantitative models based on stochastic methods, or qualitative based rather on expert heuristics and deductive reasoning. At the same time, case-based reasoning (CBR) memorizes and returns the experience of solving similar problems. The cooperation of heterogeneous clinical knowledge bases (knowledge objects, semantic distances, evaluation functions, logical rules, databases...) is based on medical ontologies. A multi-agent decision support system (MADSS) enables the integration and cooperation of agents specialized in different fields of knowledge (semiology, pharmacology, clinical cases, etc.). Each specialist agent operates a knowledge base defining the conduct to be maintained in conformity with the state of the art associated with an ontological basis that expresses the semantic relationships between the terms of the domain in question. Our approach is based on the specialization of agents adapted to the knowledge models used during the clinical steps and ontologies. This modular approach is suitable for the realization of MADSS in many areas.

</details>

<details>

<summary>2020-01-21 08:14:35 - Manifold Modeling in Embedded Space: A Perspective for Interpreting Deep Image Prior</summary>

- *Tatsuya Yokota, Hidekata Hontani, Qibin Zhao, Andrzej Cichocki*

- `1908.02995v2` - [abs](http://arxiv.org/abs/1908.02995v2) - [pdf](http://arxiv.org/pdf/1908.02995v2)

> Deep image prior (DIP), which utilizes a deep convolutional network (ConvNet) structure itself as an image prior, has attracted attentions in computer vision and machine learning communities. It empirically shows the effectiveness of ConvNet structure for various image restoration applications. However, why the DIP works so well is still unknown, and why convolution operation is useful for image reconstruction or enhancement is not very clear. In this study, we tackle these questions. The proposed approach is dividing the convolution into ``delay-embedding'' and ``transformation (\ie encoder-decoder)'', and proposing a simple, but essential, image/tensor modeling method which is closely related to dynamical systems and self-similarity. The proposed method named as manifold modeling in embedded space (MMES) is implemented by using a novel denoising-auto-encoder in combination with multi-way delay-embedding transform. In spite of its simplicity, the image/tensor completion, super-resolution, deconvolution, and denoising results of MMES are quite similar even competitive to DIP in our extensive experiments, and these results would help us for reinterpreting/characterizing the DIP from a perspective of ``low-dimensional patch-manifold prior''.

</details>

<details>

<summary>2020-01-21 09:18:03 - Towards Semantic Clone Detection via Probabilistic Software Modeling</summary>

- *Hannes Thaller, Lukas Linsbauer, Alexander Egyed*

- `2001.07399v1` - [abs](http://arxiv.org/abs/2001.07399v1) - [pdf](http://arxiv.org/pdf/2001.07399v1)

> Semantic clones are program components with similar behavior, but different textual representation. Semantic similarity is hard to detect, and semantic clone detection is still an open issue. We present semantic clone detection via Probabilistic Software Modeling (PSM) as a robust method for detecting semantically equivalent methods. PSM inspects the structure and runtime behavior of a program and synthesizes a network of Probabilistic Models (PMs). Each PM in the network represents a method in the program and is capable of generating and evaluating runtime events. We leverage these capabilities to accurately find semantic clones. Results show that the approach can detect semantic clones in the complete absence of syntactic similarity with high precision and low error rates.

</details>

<details>

<summary>2020-01-21 10:54:26 - Recognizing Images with at most one Spike per Neuron</summary>

- *Christoph Stöckl, Wolfgang Maass*

- `2001.01682v3` - [abs](http://arxiv.org/abs/2001.01682v3) - [pdf](http://arxiv.org/pdf/2001.01682v3)

> In order to port the performance of trained artificial neural networks (ANNs) to spiking neural networks (SNNs), which can be implemented in neuromorphic hardware with a drastically reduced energy consumption, an efficient ANN to SNN conversion is needed. Previous conversion schemes focused on the representation of the analog output of a rectified linear (ReLU) gate in the ANN by the firing rate of a spiking neuron. But this is not possible for other commonly used ANN gates, and it reduces the throughput even for ReLU gates. We introduce a new conversion method where a gate in the ANN, which can basically be of any type, is emulated by a small circuit of spiking neurons, with At Most One Spike (AMOS) per neuron. We show that this AMOS conversion improves the accuracy of SNNs for ImageNet from 74.60% to 80.97%, thereby bringing it within reach of the best available ANN accuracy (85.0%). The Top5 accuracy of SNNs is raised to 95.82%, getting even closer to the best Top5 performance of 97.2% for ANNs. In addition, AMOS conversion improves latency and throughput of spike-based image classification by several orders of magnitude. Hence these results suggest that SNNs provide a viable direction for developing highly energy efficient hardware for AI that combines high performance with versatility of applications.

</details>

<details>

<summary>2020-01-21 13:41:09 - Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems</summary>

- *Vevake Balaraman, Bernardo Magnini*

- `2001.07526v1` - [abs](http://arxiv.org/abs/2001.07526v1) - [pdf](http://arxiv.org/pdf/2001.07526v1)

> In task-oriented dialogue systems the dialogue state tracker (DST) component is responsible for predicting the state of the dialogue based on the dialogue history. Current DST approaches rely on a predefined domain ontology, a fact that limits their effective usage for large scale conversational agents, where the DST constantly needs to be interfaced with ever-increasing services and APIs. Focused towards overcoming this drawback, we propose a domain-aware dialogue state tracker, that is completely data-driven and it is modeled to predict for dynamic service schemas. The proposed model utilizes domain and slot information to extract both domain and slot specific representations for a given dialogue, and then uses such representations to predict the values of the corresponding slot. Integrating this mechanism with a pretrained language model (i.e. BERT), our approach can effectively learn semantic relations.

</details>

<details>

<summary>2020-01-21 14:39:20 - Generating Sense Embeddings for Syntactic and Semantic Analogy for Portuguese</summary>

- *Jessica Rodrigues da Silva, Helena de Medeiros Caseli*

- `2001.07574v1` - [abs](http://arxiv.org/abs/2001.07574v1) - [pdf](http://arxiv.org/pdf/2001.07574v1)

> Word embeddings are numerical vectors which can represent words or concepts in a low-dimensional continuous space. These vectors are able to capture useful syntactic and semantic information. The traditional approaches like Word2Vec, GloVe and FastText have a strict drawback: they produce a single vector representation per word ignoring the fact that ambiguous words can assume different meanings. In this paper we use techniques to generate sense embeddings and present the first experiments carried out for Portuguese. Our experiments show that sense vectors outperform traditional word vectors in syntactic and semantic analogy tasks, proving that the language resource generated here can improve the performance of NLP tasks in Portuguese.

</details>

<details>

<summary>2020-01-21 19:09:49 - Where New Words Are Born: Distributional Semantic Analysis of Neologisms and Their Semantic Neighborhoods</summary>

- *Maria Ryskina, Ella Rabinovich, Taylor Berg-Kirkpatrick, David R. Mortensen, Yulia Tsvetkov*

- `2001.07740v1` - [abs](http://arxiv.org/abs/2001.07740v1) - [pdf](http://arxiv.org/pdf/2001.07740v1)

> We perform statistical analysis of the phenomenon of neology, the process by which new words emerge in a language, using large diachronic corpora of English. We investigate the importance of two factors, semantic sparsity and frequency growth rates of semantic neighbors, formalized in the distributional semantics paradigm. We show that both factors are predictive of word emergence although we find more support for the latter hypothesis. Besides presenting a new linguistic application of distributional semantics, this study tackles the linguistic question of the role of language-internal factors (in our case, sparsity) in language change motivated by language-external factors (reflected in frequency growth).

</details>

<details>

<summary>2020-01-22 07:18:28 - An authentication protocol based on chaos and zero knowledge proof</summary>

- *Will Major, William J Buchanan, Jawad Ahmad*

- `2001.07897v1` - [abs](http://arxiv.org/abs/2001.07897v1) - [pdf](http://arxiv.org/pdf/2001.07897v1)

> Port Knocking is a method for authenticating clients through a closed stance firewall, and authorising their requested actions, enabling severs to offer services to authenticated clients, without opening ports on the firewall. Advances in port knocking have resulted in an increase in complexity in design, preventing port knocking solutions from realising their potential. This paper proposes a novel port knocking solution, named Crucible, which is a secure method of authentication, with high usability and features of stealth, allowing servers and services to remain hidden and protected. Crucible is a stateless solution, only requiring the client memorise a command, the server's IP and a chosen password. The solution is forwarded as a method for protecting servers against attacks ranging from port scans, to zero-day exploitation. To act as a random oracle for both client and server, cryptographic hashes were generated through chaotic systems.

</details>

<details>

<summary>2020-01-22 08:24:59 - Classifying Wikipedia in a fine-grained hierarchy: what graphs can contribute</summary>

- *Tiphaine Viard, Thomas McLachlan, Hamidreza Ghader, Satoshi Sekine*

- `2001.07558v2` - [abs](http://arxiv.org/abs/2001.07558v2) - [pdf](http://arxiv.org/pdf/2001.07558v2)

> Wikipedia is a huge opportunity for machine learning, being the largest semi-structured base of knowledge available. Because of this, many works examine its contents, and focus on structuring it in order to make it usable in learning tasks, for example by classifying it into an ontology. Beyond its textual contents, Wikipedia also displays a typical graph structure, where pages are linked together through citations. In this paper, we address the task of integrating graph (i.e. structure) information to classify Wikipedia into a fine-grained named entity ontology (NE), the Extended Named Entity hierarchy. To address this task, we first start by assessing the relevance of the graph structure for NE classification. We then explore two directions, one related to feature vectors using graph descriptors commonly used in large-scale network analysis, and one extending flat classification to a weighted model taking into account semantic similarity. We conduct at-scale practical experiments, on a manually labeled subset of 22,000 pages extracted from the Japanese Wikipedia. Our results show that integrating graph information succeeds at reducing sparsity of the input feature space, and yields classification results that are comparable or better than previous works.

</details>

<details>

<summary>2020-01-22 11:52:58 - Neural Networks-based Regularization for Large-Scale Medical Image Reconstruction</summary>

- *Andreas Kofler, Markus Haltmeier, Tobias Schaeffter, Marc Kachelrieß, Marc Dewey, Christian Wald, Christoph Kolbitsch*

- `1912.09395v2` - [abs](http://arxiv.org/abs/1912.09395v2) - [pdf](http://arxiv.org/pdf/1912.09395v2)

> In this paper we present a generalized Deep Learning-based approach for solving ill-posed large-scale inverse problems occuring in medical image reconstruction. Recently, Deep Learning methods using iterative neural networks and cascaded neural networks have been reported to achieve state-of-the-art results with respect to various quantitative quality measures as PSNR, NRMSE and SSIM across different imaging modalities. However, the fact that these approaches employ the forward and adjoint operators repeatedly in the network architecture requires the network to process the whole images or volumes at once, which for some applications is computationally infeasible. In this work, we follow a different reconstruction strategy by decoupling the regularization of the solution from ensuring consistency with the measured data. The regularization is given in the form of an image prior obtained by the output of a previously trained neural network which is used in a Tikhonov regularization framework. By doing so, more complex and sophisticated network architectures can be used for the removal of the artefacts or noise than it is usually the case in iterative networks. Due to the large scale of the considered problems and the resulting computational complexity of the employed networks, the priors are obtained by processing the images or volumes as patches or slices. We evaluated the method for the cases of 3D cone-beam low dose CT and undersampled 2D radial cine MRI and compared it to a total variation-minimization-based reconstruction algorithm as well as to a method with regularization based on learned overcomplete dictionaries. The proposed method outperformed all the reported methods with respect to all chosen quantitative measures and further accelerates the regularization step in the reconstruction by several orders of magnitude.

</details>

<details>

<summary>2020-01-22 13:49:14 - A Neural Architecture for Person Ontology population</summary>

- *Balaji Ganesan, Riddhiman Dasgupta, Akshay Parekh, Hima Patel, Berthold Reinwald*

- `2001.08013v1` - [abs](http://arxiv.org/abs/2001.08013v1) - [pdf](http://arxiv.org/pdf/2001.08013v1)

> A person ontology comprising concepts, attributes and relationships of people has a number of applications in data protection, didentification, population of knowledge graphs for business intelligence and fraud prevention. While artificial neural networks have led to improvements in Entity Recognition, Entity Classification, and Relation Extraction, creating an ontology largely remains a manual process, because it requires a fixed set of semantic relations between concepts. In this work, we present a system for automatically populating a person ontology graph from unstructured data using neural models for Entity Classification and Relation Extraction. We introduce a new dataset for these tasks and discuss our results.

</details>

<details>

<summary>2020-01-22 23:41:29 - Using a Generative Adversarial Network for CT Normalization and its Impact on Radiomic Features</summary>

- *Leihao Wei, Yannan Lin, William Hsu*

- `2001.08741v1` - [abs](http://arxiv.org/abs/2001.08741v1) - [pdf](http://arxiv.org/pdf/2001.08741v1)

> Computer-Aided-Diagnosis (CADx) systems assist radiologists with identifying and classifying potentially malignant pulmonary nodules on chest CT scans using morphology and texture-based (radiomic) features. However, radiomic features are sensitive to differences in acquisitions due to variations in dose levels and slice thickness. This study investigates the feasibility of generating a normalized scan from heterogeneous CT scans as input. We obtained projection data from 40 low-dose chest CT scans, simulating acquisitions at 10%, 25% and 50% dose and reconstructing the scans at 1.0mm and 2.0mm slice thickness. A 3D generative adversarial network (GAN) was used to simultaneously normalize reduced dose, thick slice (2.0mm) images to normal dose (100%), thinner slice (1.0mm) images. We evaluated the normalized image quality using peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS). Our GAN improved perceptual similarity by 35%, compared to a baseline CNN method. Our analysis also shows that the GAN-based approach led to a significantly smaller error (p-value < 0.05) in nine studied radiomic features. These results indicated that GANs could be used to normalize heterogeneous CT images and reduce the variability in radiomic feature values.

</details>

<details>

<summary>2020-01-23 05:44:11 - Adaptation of a deep learning malignancy model from full-field digital mammography to digital breast tomosynthesis</summary>

- *Sadanand Singh, Thomas Paul Matthews, Meet Shah, Brent Mombourquette, Trevor Tsue, Aaron Long, Ranya Almohsen, Stefano Pedemonte, Jason Su*

- `2001.08381v1` - [abs](http://arxiv.org/abs/2001.08381v1) - [pdf](http://arxiv.org/pdf/2001.08381v1)

> Mammography-based screening has helped reduce the breast cancer mortality rate, but has also been associated with potential harms due to low specificity, leading to unnecessary exams or procedures, and low sensitivity. Digital breast tomosynthesis (DBT) improves on conventional mammography by increasing both sensitivity and specificity and is becoming common in clinical settings. However, deep learning (DL) models have been developed mainly on conventional 2D full-field digital mammography (FFDM) or scanned film images. Due to a lack of large annotated DBT datasets, it is difficult to train a model on DBT from scratch. In this work, we present methods to generalize a model trained on FFDM images to DBT images. In particular, we use average histogram matching (HM) and DL fine-tuning methods to generalize a FFDM model to the 2D maximum intensity projection (MIP) of DBT images. In the proposed approach, the differences between the FFDM and DBT domains are reduced via HM and then the base model, which was trained on abundant FFDM images, is fine-tuned. When evaluating on image patches extracted around identified findings, we are able to achieve similar areas under the receiver operating characteristic curve (ROC AUC) of $\sim 0.9$ for FFDM and $\sim 0.85$ for MIP images, as compared to a ROC AUC of $\sim 0.75$ when tested directly on MIP images.

</details>

<details>

<summary>2020-01-23 16:09:15 - Automatic Malware Description via Attribute Tagging and Similarity Embedding</summary>

- *Felipe N. Ducau, Ethan M. Rudd, Tad M. Heppner, Alex Long, Konstantin Berlin*

- `1905.06262v3` - [abs](http://arxiv.org/abs/1905.06262v3) - [pdf](http://arxiv.org/pdf/1905.06262v3)

> With the rapid proliferation and increased sophistication of malicious software (malware), detection methods no longer rely only on manually generated signatures but have also incorporated more general approaches like machine learning detection. Although powerful for conviction of malicious artifacts, these methods do not produce any further information about the type of threat that has been detected neither allows for identifying relationships between malware samples. In this work, we address the information gap between machine learning and signature-based detection methods by learning a representation space for malware samples in which files with similar malicious behaviors appear close to each other. We do so by introducing a deep learning based tagging model trained to generate human-interpretable semantic descriptions of malicious software, which, at the same time provides potentially more useful and flexible information than malware family names.   We show that the malware descriptions generated with the proposed approach correctly identify more than 95% of eleven possible tag descriptions for a given sample, at a deployable false positive rate of 1% per tag. Furthermore, we use the learned representation space to introduce a similarity index between malware files, and empirically demonstrate using dynamic traces from files' execution, that is not only more effective at identifying samples from the same families, but also 32 times smaller than those based on raw feature vectors.

</details>

<details>

<summary>2020-01-23 17:19:46 - Integrating Lexical Knowledge in Word Embeddings using Sprinkling and Retrofitting</summary>

- *Aakash Srinivasan, Harshavardhan Kamarthi, Devi Ganesan, Sutanu Chakraborti*

- `1912.06889v2` - [abs](http://arxiv.org/abs/1912.06889v2) - [pdf](http://arxiv.org/pdf/1912.06889v2)

> Neural network based word embeddings, such as Word2Vec and GloVe, are purely data driven in that they capture the distributional information about words from the training corpus. Past works have attempted to improve these embeddings by incorporating semantic knowledge from lexical resources like WordNet. Some techniques like retrofitting modify word embeddings in the post-processing stage while some others use a joint learning approach by modifying the objective function of neural networks. In this paper, we discuss two novel approaches for incorporating semantic knowledge into word embeddings. In the first approach, we take advantage of Levy et al's work which showed that using SVD based methods on co-occurrence matrix provide similar performance to neural network based embeddings. We propose a 'sprinkling' technique to add semantic relations to the co-occurrence matrix directly before factorization. In the second approach, WordNet similarity scores are used to improve the retrofitting method. We evaluate the proposed methods in both intrinsic and extrinsic tasks and observe significant improvements over the baselines in many of the datasets.

</details>

<details>

<summary>2020-01-23 18:26:40 - One Homonym per Translation</summary>

- *Bradley Hauer, Grzegorz Kondrak*

- `1904.08533v2` - [abs](http://arxiv.org/abs/1904.08533v2) - [pdf](http://arxiv.org/pdf/1904.08533v2)

> The study of homonymy is vital to resolving fundamental problems in lexical semantics. In this paper, we propose four hypotheses that characterize the unique behavior of homonyms in the context of translations, discourses, collocations, and sense clusters. We present a new annotated homonym resource that allows us to test our hypotheses on existing WSD resources. The results of the experiments provide strong empirical evidence for the hypotheses. This study represents a step towards a computational method for distinguishing between homonymy and polysemy, and constructing a definitive inventory of coarse-grained senses.

</details>

<details>

<summary>2020-01-23 19:11:31 - Interventions for Ranking in the Presence of Implicit Bias</summary>

- *L. Elisa Celis, Anay Mehrotra, Nisheeth K. Vishnoi*

- `2001.08767v1` - [abs](http://arxiv.org/abs/2001.08767v1) - [pdf](http://arxiv.org/pdf/2001.08767v1)

> Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, (Kleinberg and Raghavan, 2018) considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection -- ranking -- that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in (Kleinberg and Raghavan, 2018). Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.

</details>

<details>

<summary>2020-01-23 19:37:20 - Deep Bayesian Network for Visual Question Generation</summary>

- *Badri N. Patro, Vinod K. Kurmi, Sandeep Kumar, Vinay P. Namboodiri*

- `2001.08779v1` - [abs](http://arxiv.org/abs/2001.08779v1) - [pdf](http://arxiv.org/pdf/2001.08779v1)

> Generating natural questions from an image is a semantic task that requires using vision and language modalities to learn multimodal representations. Images can have multiple visual and language cues such as places, captions, and tags. In this paper, we propose a principled deep Bayesian learning framework that combines these cues to produce natural questions. We observe that with the addition of more cues and by minimizing uncertainty in the among cues, the Bayesian network becomes more confident. We propose a Minimizing Uncertainty of Mixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues experts for generating probabilistic questions. This is a Bayesian framework and the results show a remarkable similarity to natural questions as validated by a human study. We observe that with the addition of more cues and by minimizing uncertainty among the cues, the Bayesian framework becomes more confident. Ablation studies of our model indicate that a subset of cues is inferior at this task and hence the principled fusion of cues is preferred. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE, and CIDEr). Here we provide project link for Deep Bayesian VQG \url{https://delta-lab-iitk.github.io/BVQG/}

</details>

<details>

<summary>2020-01-24 01:58:05 - Capturing Evolution in Word Usage: Just Add More Clusters?</summary>

- *Matej Martinc, Syrielle Montariol, Elaine Zosa, Lidia Pivovarova*

- `2001.06629v2` - [abs](http://arxiv.org/abs/2001.06629v2) - [pdf](http://arxiv.org/pdf/2001.06629v2)

> The way the words are used evolves through time, mirroring cultural or technological evolution of society. Semantic change detection is the task of detecting and analysing word evolution in textual data, even in short periods of time. In this paper we focus on a new set of methods relying on contextualised embeddings, a type of semantic modelling that revolutionised the NLP field recently. We leverage the ability of the transformer-based BERT model to generate contextualised embeddings capable of detecting semantic change of words across time. Several approaches are compared in a common setting in order to establish strengths and weaknesses for each of them. We also propose several ideas for improvements, managing to drastically improve the performance of existing approaches.

</details>

<details>

<summary>2020-01-24 04:48:39 - Advaita: Bug Duplicity Detection System</summary>

- *Amit Kumar, Manohar Madanu, Hari Prakash, Lalitha Jonnavithula, Srinivasa Rao Aravilli*

- `2001.10376v1` - [abs](http://arxiv.org/abs/2001.10376v1) - [pdf](http://arxiv.org/pdf/2001.10376v1)

> Bugs are prevalent in software development. To improve software quality, bugs are filed using a bug tracking system. Properties of a reported bug would consist of a headline, description, project, product, component that is affected by the bug and the severity of the bug. Duplicate bugs rate (% of duplicate bugs) are in the range from single digit (1 to 9%) to double digits (40%) based on the product maturity , size of the code and number of engineers working on the project. Duplicate bugs range are between 9% to 39% in some of the open source projects like Eclipse, Firefox etc. Detection of duplicity deals with identifying whether any two bugs convey the same meaning. This detection of duplicates helps in de-duplication. Detecting duplicate bugs help reduce triaging efforts and saves time for developers in fixing the issues. Traditional natural language processing techniques are less accurate in identifying similarity between sentences. Using the bug data present in a bug tracking system, various approaches were explored including several machine learning algorithms, to obtain a viable approach that can identify duplicate bugs, given a pair of sentences(i.e. the respective bug descriptions). This approach considers multiple sets of features viz. basic text statistical features, semantic features and contextual features. These features are extracted from the headline, description and component and are subsequently used to train a classification algorithm.

</details>

<details>

<summary>2020-01-24 05:12:45 - IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation</summary>

- *Zhijing Jin, Di Jin, Jonas Mueller, Nicholas Matthews, Enrico Santus*

- `1901.11333v4` - [abs](http://arxiv.org/abs/1901.11333v4) - [pdf](http://arxiv.org/pdf/1901.11333v4)

> Text attribute transfer aims to automatically rewrite sentences such that they possess certain linguistic attributes, while simultaneously preserving their semantic content. This task remains challenging due to a lack of supervised parallel data. Existing approaches try to explicitly disentangle content and attribute information, but this is difficult and often results in poor content-preservation and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching and Translation (IMaT), which: (1) constructs a pseudo-parallel corpus by aligning a subset of semantically similar sentences from the source and the target corpora; (2) applies a standard sequence-to-sequence model to learn the attribute transfer; (3) iteratively improves the learned transfer function by refining imperfections in the alignment. In sentiment modification and formality transfer tasks, our method outperforms complex state-of-the-art systems by a large margin. As an auxiliary contribution, we produce a publicly-available test set with human-generated transfer references.

</details>

<details>

<summary>2020-01-24 11:33:07 - The Homunculus Brain and Categorical Logic</summary>

- *Michael Heller*

- `1903.03424v2` - [abs](http://arxiv.org/abs/1903.03424v2) - [pdf](http://arxiv.org/pdf/1903.03424v2)

> The interaction between syntax (formal language) and its semantics (meanings of language) is one which has been well studied in categorical logic. The results of this particular study are employed to understand how the brain is able to create meanings. To emphasize the toy character of the proposed model, we prefer to speak of the homunculus brain rather than the brain per se. The homunculus brain consists of neurons, each of which is modeled by a category, and axons between neurons, which are modeled by functors between the corresponding neuron-categories. Each neuron (category) has its own program enabling its working, i.e. a theory of this neuron. In analogy to what is known from categorical logic, we postulate the existence of a pair of adjoint functors, called Lang and Syn, from a category, now called BRAIN, of categories, to a category, now called MIND, of theories. Our homunculus is a kind of ``mathematical robot'', the neuronal architecture of which is not important. Its only aim is to provide us with the opportunity to study how such a simple brain-like structure could ``create meanings'' and perform abstraction operations out of its purely syntactic program. The pair of adjoint functors Lang and Syn model the mutual dependencies between the syntactical structure of a given theory of MIND and the internal logic of its semantics given by a category of BRAIN. In this way, a formal language (syntax) and its meanings (semantics) are interwoven with each other in a manner corresponding to the adjointness of the functors Lang and Syn. Higher cognitive functions of abstraction and realization of concepts are also modelled by a corresponding pair of adjoint functors. The categories BRAIN and MIND interact with each other with their entire structures and, at the same time, these very structures are shaped by this interaction.

</details>

<details>

<summary>2020-01-24 18:58:50 - Learning to Catch Security Patches</summary>

- *Arthur D. Sawadogo, Tegawendé F. Bissyandé, Naouel Moha, Kevin Allix, Jacques Klein, Li Li, Yves Le Traon*

- `2001.09148v1` - [abs](http://arxiv.org/abs/2001.09148v1) - [pdf](http://arxiv.org/pdf/2001.09148v1)

> Timely patching is paramount to safeguard users and maintainers against dire consequences of malicious attacks. In practice, patching is prioritized following the nature of the code change that is committed in the code repository. When such a change is labeled as being security-relevant, i.e., as fixing a vulnerability, maintainers rapidly spread the change and users are notified about the need to update to a new version of the library or of the application. Unfortunately, oftentimes, some security-relevant changes go unnoticed as they represent silent fixes of vulnerabilities. In this paper, we propose a Co-Training-based approach to catch security patches as part of an automatic monitoring service of code repositories. Leveraging different classes of features, we empirically show that such automation is feasible and can yield a precision of over 90% in identifying security patches, with an unprecedented recall of over 80%. Beyond such a benchmarking with ground truth data which demonstrates an improvement over the state-of-the-art, we confirmed that our approach can help catch security patches that were not reported as such.

</details>

<details>

<summary>2020-01-24 20:12:32 - Learned Interpolation for 3D Generation</summary>

- *Austin Dill, Songwei Ge, Eunsu Kang, Chun-Liang Li, Barnabas Poczos*

- `1912.10787v2` - [abs](http://arxiv.org/abs/1912.10787v2) - [pdf](http://arxiv.org/pdf/1912.10787v2)

> In order to generate novel 3D shapes with machine learning, one must allow for interpolation. The typical approach for incorporating this creative process is to interpolate in a learned latent space so as to avoid the problem of generating unrealistic instances by exploiting the model's learned structure. The process of the interpolation is supposed to form a semantically smooth morphing. While this approach is sound for synthesizing realistic media such as lifelike portraits or new designs for everyday objects, it subjectively fails to directly model the unexpected, unrealistic, or creative. In this work, we present a method for learning how to interpolate point clouds. By encoding prior knowledge about real-world objects, the intermediate forms are both realistic and unlike any existing forms. We show not only how this method can be used to generate "creative" point clouds, but how the method can also be leveraged to generate 3D models suitable for sculpture.

</details>

<details>

<summary>2020-01-24 21:30:03 - Comparison of Syntactic and Semantic Representations of Programs in Neural Embeddings</summary>

- *Austin P. Wright, Herbert Wiklicky*

- `2001.09201v1` - [abs](http://arxiv.org/abs/2001.09201v1) - [pdf](http://arxiv.org/pdf/2001.09201v1)

> Neural approaches to program synthesis and understanding have proliferated widely in the last few years; at the same time graph based neural networks have become a promising new tool. This work aims to be the first empirical study comparing the effectiveness of natural language models and static analysis graph based models in representing programs in deep learning systems. It compares graph convolutional networks using different graph representations in the task of program embedding. It shows that the sparsity of control flow graphs and the implicit aggregation of graph convolutional networks cause these models to perform worse than naive models. Therefore it concludes that simply augmenting purely linguistic or statistical models with formal information does not perform well due to the nuanced nature of formal properties introducing more noise than structure for graph convolutional networks.

</details>

<details>

<summary>2020-01-24 21:48:13 - BioSentVec: creating sentence embeddings for biomedical texts</summary>

- *Qingyu Chen, Yifan Peng, Zhiyong Lu*

- `1810.09302v6` - [abs](http://arxiv.org/abs/1810.09302v6) - [pdf](http://arxiv.org/pdf/1810.09302v6)

> Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedical texts to date. In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMIC-III Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different text genres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings. BioSentVec is publicly available at https://github.com/ncbi-nlp/BioSentVec

</details>

<details>

<summary>2020-01-25 08:47:04 - Weakly Supervised Clustering by Exploiting Unique Class Count</summary>

- *Mustafa Umit Oner, Hwee Kuan Lee, Wing-Kin Sung*

- `1906.07647v2` - [abs](http://arxiv.org/abs/1906.07647v2) - [pdf](http://arxiv.org/pdf/1906.07647v2)

> A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count ($ucc$), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect $ucc$ classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based $ucc$ classifier and experimentally shown that the clustering performance of our framework with our weakly supervised $ucc$ classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model.

</details>

<details>

<summary>2020-01-25 15:12:01 - An Analysis of Word2Vec for the Italian Language</summary>

- *Giovanni Di Gennaro, Amedeo Buonanno, Antonio Di Girolamo, Armando Ospedale, Francesco A. N. Palmieri, Gianfranco Fedele*

- `2001.09332v1` - [abs](http://arxiv.org/abs/2001.09332v1) - [pdf](http://arxiv.org/pdf/2001.09332v1)

> Word representation is fundamental in NLP tasks, because it is precisely from the coding of semantic closeness between words that it is possible to think of teaching a machine to understand text. Despite the spread of word embedding concepts, still few are the achievements in linguistic contexts other than English. In this work, analysing the semantic capacity of the Word2Vec algorithm, an embedding for the Italian language is produced. Parameter setting such as the number of epochs, the size of the context window and the number of negatively backpropagated samples is explored.

</details>

<details>

<summary>2020-01-25 15:52:29 - Introduction of Quantification in Frame Semantics</summary>

- *Valentin D. Richard*

- `2002.00720v1` - [abs](http://arxiv.org/abs/2002.00720v1) - [pdf](http://arxiv.org/pdf/2002.00720v1)

> Feature Structures (FSs) are a widespread tool used for decompositional frameworks of Attribute-Value associations. Even though they thrive in simple systems, they lack a way of representing higher-order entities and relations. This is however needed in Frame Semantics, where semantic dependencies should be able to connect groups of individuals and their properties, especially to model quantification. To answer this issue, this master report introduces wrappings as a way to envelop a sub-FS and treat it as a node. Following the work of [Kallmeyer, Osswald 2013], we extend its syntax, semantics and some properties (translation to FOL, subsumption, unification). We can then expand the proposed pipeline. Lexical minimal model sets are generated from formulas. They unify by FS value equations obtained by LTAG parsing to an underspecified sentence representation. The syntactic approach of quantifiers allows us to use existing methods to produce any possible reading. Finally, we give a transcription to type-logical formulas to interact with the context in the view of dynamic semantics. Supported by ideas of Frame Types, this system provides a workable and tractable tool for higher-order relations with FS.

</details>

<details>

<summary>2020-01-26 05:59:57 - An Effective Automatic Image Annotation Model Via Attention Model and Data Equilibrium</summary>

- *Amir Vatani, Milad Taleby Ahvanooey, Mostafa Rahimi*

- `2001.10590v1` - [abs](http://arxiv.org/abs/2001.10590v1) - [pdf](http://arxiv.org/pdf/2001.10590v1)

> Nowadays, a huge number of images are available. However, retrieving a required image for an ordinary user is a challenging task in computer vision systems. During the past two decades, many types of research have been introduced to improve the performance of the automatic annotation of images, which are traditionally focused on content-based image retrieval. Although, recent research demonstrates that there is a semantic gap between content-based image retrieval and image semantics understandable by humans. As a result, existing research in this area has caused to bridge the semantic gap between low-level image features and high-level semantics. The conventional method of bridging the semantic gap is through the automatic image annotation (AIA) that extracts semantic features using machine learning techniques. In this paper, we propose a novel AIA model based on the deep learning feature extraction method. The proposed model has three phases, including a feature extractor, a tag generator, and an image annotator. First, the proposed model extracts automatically the high and low-level features based on dual-tree continues wavelet transform (DT-CWT), singular value decomposition, distribution of color ton, and the deep neural network. Moreover, the tag generator balances the dictionary of the annotated keywords by a new log-entropy auto-encoder (LEAE) and then describes these keywords by word embedding. Finally, the annotator works based on the long-short-term memory (LSTM) network in order to obtain the importance degree of specific features of the image. The experiments conducted on two benchmark datasets confirm that the superiority of the proposed model compared to the previous models in terms of performance criteria.

</details>

<details>

<summary>2020-01-26 15:42:43 - Information Credibility in the Social Web: Contexts, Approaches, and Open Issues</summary>

- *Gabriella Pasi, Marco Viviani*

- `2001.09473v1` - [abs](http://arxiv.org/abs/2001.09473v1) - [pdf](http://arxiv.org/pdf/2001.09473v1)

> In the Social Web scenario, large amounts of User-Generated Content (UGC) are diffused through social media often without almost any form of traditional trusted intermediaries. Therefore, the risk of running into misinformation is not negligible. For this reason, assessing and mining the credibility of online information constitutes nowadays a fundamental research issue. Credibility, also referred as believability, is a quality perceived by individuals, who are not always able to discern, with their own cognitive capacities, genuine information from fake one. Hence, in the last years, several approaches have been proposed to automatically assess credibility in social media. Many of them are based on data-driven models, i.e., they employ machine learning techniques to identify misinformation, but recently also model-driven approaches are emerging, as well as graph-based approaches focusing on credibility propagation, and knowledge-based ones exploiting Semantic Web technologies. Three of the main contexts in which the assessment of information credibility has been investigated concern: (i) the detection of opinion spam in review sites, (ii) the detection of fake news in microblogging, and (iii) the credibility assessment of online health-related information. In this article, the main issues connected to the evaluation of information credibility in the Social Web, which are shared by the above-mentioned contexts, are discussed. A concise survey of the approaches and methodologies that have been proposed in recent years to address these issues is also presented.

</details>

<details>

<summary>2020-01-26 21:30:21 - TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network</summary>

- *Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, Jiawei Han*

- `2001.09522v1` - [abs](http://arxiv.org/abs/2001.09522v1) - [pdf](http://arxiv.org/pdf/2001.09522v1)

> Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of <query concept, anchor concept> pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion.

</details>

<details>

<summary>2020-01-27 09:14:46 - Structural Information Learning Machinery: Learning from Observing, Associating, Optimizing, Decoding, and Abstracting</summary>

- *Angsheng Li*

- `2001.09637v1` - [abs](http://arxiv.org/abs/2001.09637v1) - [pdf](http://arxiv.org/pdf/2001.09637v1)

> In the present paper, we propose the model of {\it structural information learning machines} (SiLeM for short), leading to a mathematical definition of learning by merging the theories of computation and information. Our model shows that the essence of learning is {\it to gain information}, that to gain information is {\it to eliminate uncertainty} embedded in a data space, and that to eliminate uncertainty of a data space can be reduced to an optimization problem, that is, an {\it information optimization problem}, which can be realized by a general {\it encoding tree method}. The principle and criterion of the structural information learning machines are maximization of {\it decoding information} from the data points observed together with the relationships among the data points, and semantical {\it interpretation} of syntactical {\it essential structure}, respectively. A SiLeM machine learns the laws or rules of nature. It observes the data points of real world, builds the {\it connections} among the observed data and constructs a {\it data space}, for which the principle is to choose the way of connections of data points so that the {\it decoding information} of the data space is maximized, finds the {\it encoding tree} of the data space that minimizes the dynamical uncertainty of the data space, in which the encoding tree is hence referred to as a {\it decoder}, due to the fact that it has already eliminated the maximum amount of uncertainty embedded in the data space, interprets the {\it semantics} of the decoder, an encoding tree, to form a {\it knowledge tree}, extracts the {\it remarkable common features} for both semantical and syntactical features of the modules decoded by a decoder to construct {\it trees of abstractions}, providing the foundations for {\it intuitive reasoning} in the learning when new data are observed.

</details>

<details>

<summary>2020-01-27 20:39:32 - SemClinBr -- a multi institutional and multi specialty semantically annotated corpus for Portuguese clinical NLP tasks</summary>

- *Lucas Emanuel Silva e Oliveira, Ana Carolina Peters, Adalniza Moura Pucca da Silva, Caroline P. Gebeluca, Yohan Bonescki Gumiel, Lilian Mie Mukai Cintho, Deborah Ribeiro Carvalho, Sadid A. Hasan, Claudia Maria Cabral Moro*

- `2001.10071v1` - [abs](http://arxiv.org/abs/2001.10071v1) - [pdf](http://arxiv.org/pdf/2001.10071v1)

> The high volume of research focusing on extracting patient's information from electronic health records (EHR) has led to an increase in the demand for annotated corpora, which are a very valuable resource for both the development and evaluation of natural language processing (NLP) algorithms. The absence of a multi-purpose clinical corpus outside the scope of the English language, especially in Brazilian Portuguese, is glaring and severely impacts scientific progress in the biomedical NLP field. In this study, we developed a semantically annotated corpus using clinical texts from multiple medical specialties, document types, and institutions. We present the following: (1) a survey listing common aspects and lessons learned from previous research, (2) a fine-grained annotation schema which could be replicated and guide other annotation initiatives, (3) a web-based annotation tool focusing on an annotation suggestion feature, and (4) both intrinsic and extrinsic evaluation of the annotations. The result of this work is the SemClinBr, a corpus that has 1,000 clinical notes, labeled with 65,117 entities and 11,263 relations, and can support a variety of clinical NLP tasks and boost the EHR's secondary use for the Portuguese language.

</details>

<details>

<summary>2020-01-27 22:34:07 - Guiding Corpus-based Set Expansion by Auxiliary Sets Generation and Co-Expansion</summary>

- *Jiaxin Huang, Yiqing Xie, Yu Meng, Jiaming Shen, Yunyi Zhang, Jiawei Han*

- `2001.10106v1` - [abs](http://arxiv.org/abs/2001.10106v1) - [pdf](http://arxiv.org/pdf/2001.10106v1)

> Given a small set of seed entities (e.g., ``USA'', ``Russia''), corpus-based set expansion is to induce an extensive set of entities which share the same semantic class (Country in this example) from a given corpus. Set expansion benefits a wide range of downstream applications in knowledge discovery, such as web search, taxonomy construction, and query suggestion. Existing corpus-based set expansion algorithms typically bootstrap the given seeds by incorporating lexical patterns and distributional similarity. However, due to no negative sets provided explicitly, these methods suffer from semantic drift caused by expanding the seed set freely without guidance. We propose a new framework, Set-CoExpan, that automatically generates auxiliary sets as negative sets that are closely related to the target set of user's interest, and then performs multiple sets co-expansion that extracts discriminative features by comparing target set with auxiliary sets, to form multiple cohesive sets that are distinctive from one another, thus resolving the semantic drift issue. In this paper we demonstrate that by generating auxiliary sets, we can guide the expansion process of target set to avoid touching those ambiguous areas around the border with auxiliary sets, and we show that Set-CoExpan outperforms strong baseline methods significantly.

</details>

<details>

<summary>2020-01-28 04:13:05 - Bringing Stories Alive: Generating Interactive Fiction Worlds</summary>

- *Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, Mark O. Riedl*

- `2001.10161v1` - [abs](http://arxiv.org/abs/2001.10161v1) - [pdf](http://arxiv.org/pdf/2001.10161v1)

> World building forms the foundation of any task that requires narrative intelligence. In this work, we focus on procedurally generating interactive fiction worlds---text-based worlds that players "see" and "talk to" using natural language. Generating these worlds requires referencing everyday and thematic commonsense priors in addition to being semantically consistent, interesting, and coherent throughout. Using existing story plots as inspiration, we present a method that first extracts a partial knowledge graph encoding basic information regarding world structure such as locations and objects. This knowledge graph is then automatically completed utilizing thematic knowledge and used to guide a neural language generation model that fleshes out the rest of the world. We perform human participant-based evaluations, testing our neural model's ability to extract and fill-in a knowledge graph and to generate language conditioned on it against rule-based and human-made baselines. Our code is available at https://github.com/rajammanabrolu/WorldGeneration.

</details>

<details>

<summary>2020-01-28 06:32:21 - Robust Method for Semantic Segmentation of Whole-Slide Blood Cell Microscopic Image</summary>

- *Muhammad Shahzad, Arif Iqbal Umar, Muazzam A. Khan, Syed Hamad Shirazi, Zakir Khan, Waqas Yousaf*

- `2001.10188v1` - [abs](http://arxiv.org/abs/2001.10188v1) - [pdf](http://arxiv.org/pdf/2001.10188v1)

> Previous works on segmentation of SEM (scanning electron microscope) blood cell image ignore the semantic segmentation approach of whole-slide blood cell segmentation. In the proposed work, we address the problem of whole-slide blood cell segmentation using the semantic segmentation approach. We design a novel convolutional encoder-decoder framework along with VGG-16 as the pixel-level feature extraction model. -e proposed framework comprises 3 main steps: First, all the original images along with manually generated ground truth masks of each blood cell type are passed through the preprocessing stage. In the preprocessing stage, pixel-level labeling, RGB to grayscale conversion of masked image and pixel fusing, and unity mask generation are performed. After that, VGG16 is loaded into the system, which acts as a pretrained pixel-level feature extraction model. In the third step, the training process is initiated on the proposed model. We have evaluated our network performance on three evaluation metrics. We obtained outstanding results with respect to classwise, as well as global and mean accuracies. Our system achieved classwise accuracies of 97.45%, 93.34%, and 85.11% for RBCs, WBCs, and platelets, respectively, while global and mean accuracies remain 97.18% and 91.96%, respectively.

</details>

<details>

<summary>2020-01-28 09:07:47 - Structural-Aware Sentence Similarity with Recursive Optimal Transport</summary>

- *Zihao Wang, Yong Zhang, Hao Wu*

- `2002.00745v1` - [abs](http://arxiv.org/abs/2002.00745v1) - [pdf](http://arxiv.org/pdf/2002.00745v1)

> Measuring sentence similarity is a classic topic in natural language processing. Light-weighted similarities are still of particular practical significance even when deep learning models have succeeded in many other tasks. Some light-weighted similarities with more theoretical insights have been demonstrated to be even stronger than supervised deep learning approaches. However, the successful light-weighted models such as Word Mover's Distance [Kusner et al., 2015] or Smooth Inverse Frequency [Arora et al., 2017] failed to detect the difference from the structure of sentences, i.e. order of words. To address this issue, we present Recursive Optimal Transport (ROT) framework to incorporate the structural information with the classic OT. Moreover, we further develop Recursive Optimal Similarity (ROTS) for sentences with the valuable semantic insights from the connections between cosine similarity of weighted average of word vectors and optimal transport. ROTS is structural-aware and with low time complexity compared to optimal transport. Our experiments over 20 sentence textural similarity (STS) datasets show the clear advantage of ROTS over all weakly supervised approaches. Detailed ablation study demonstrate the effectiveness of ROT and the semantic insights.

</details>

<details>

<summary>2020-01-28 10:04:04 - Controlling generative models with continuous factors of variations</summary>

- *Antoine Plumerault, Hervé Le Borgne, Céline Hudelot*

- `2001.10238v1` - [abs](http://arxiv.org/abs/2001.10238v1) - [pdf](http://arxiv.org/pdf/2001.10238v1)

> Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders.

</details>

<details>

<summary>2020-01-28 13:40:53 - The POLAR Framework: Polar Opposites Enable Interpretability of Pre-Trained Word Embeddings</summary>

- *Binny Mathew, Sandipan Sikdar, Florian Lemmerich, Markus Strohmaier*

- `2001.09876v2` - [abs](http://arxiv.org/abs/2001.09876v2) - [pdf](http://arxiv.org/pdf/2001.09876v2)

> We introduce POLAR - a framework that adds interpretability to pre-trained word embeddings via the adoption of semantic differentials. Semantic differentials are a psychometric construct for measuring the semantics of a word by analysing its position on a scale between two polar opposites (e.g., cold -- hot, soft -- hard). The core idea of our approach is to transform existing, pre-trained word embeddings via semantic differentials to a new "polar" space with interpretable dimensions defined by such polar opposites. Our framework also allows for selecting the most discriminative dimensions from a set of polar dimensions provided by an oracle, i.e., an external source. We demonstrate the effectiveness of our framework by deploying it to various downstream tasks, in which our interpretable word embeddings achieve a performance that is comparable to the original word embeddings. We also show that the interpretable dimensions selected by our framework align with human judgement. Together, these results demonstrate that interpretability can be added to word embeddings without compromising performance. Our work is relevant for researchers and engineers interested in interpreting pre-trained word embeddings.

</details>

<details>

<summary>2020-01-28 17:15:02 - Incorporating Joint Embeddings into Goal-Oriented Dialogues with Multi-Task Learning</summary>

- *Firas Kassawat, Debanjan Chaudhuri, Jens Lehmann*

- `2001.10468v1` - [abs](http://arxiv.org/abs/2001.10468v1) - [pdf](http://arxiv.org/pdf/2001.10468v1)

> Attention-based encoder-decoder neural network models have recently shown promising results in goal-oriented dialogue systems. However, these models struggle to reason over and incorporate state-full knowledge while preserving their end-to-end text generation functionality. Since such models can greatly benefit from user intent and knowledge graph integration, in this paper we propose an RNN-based end-to-end encoder-decoder architecture which is trained with joint embeddings of the knowledge graph and the corpus as input. The model provides an additional integration of user intent along with text generation, trained with a multi-task learning paradigm along with an additional regularization technique to penalize generating the wrong entity as output. The model further incorporates a Knowledge Graph entity lookup during inference to guarantee the generated output is state-full based on the local knowledge graph provided. We finally evaluated the model using the BLEU score, empirical evaluation depicts that our proposed architecture can aid in the betterment of task-oriented dialogue system`s performance.

</details>

<details>

<summary>2020-01-28 19:12:37 - The KEEN Universe: An Ecosystem for Knowledge Graph Embeddings with a Focus on Reproducibility and Transferability</summary>

- *Mehdi Ali, Hajira Jabeen, Charles Tapley Hoyt, Jens Lehman*

- `2001.10560v1` - [abs](http://arxiv.org/abs/2001.10560v1) - [pdf](http://arxiv.org/pdf/2001.10560v1)

> There is an emerging trend of embedding knowledge graphs (KGs) in continuous vector spaces in order to use those for machine learning tasks. Recently, many knowledge graph embedding (KGE) models have been proposed that learn low dimensional representations while trying to maintain the structural properties of the KGs such as the similarity of nodes depending on their edges to other nodes. KGEs can be used to address tasks within KGs such as the prediction of novel links and the disambiguation of entities. They can also be used for downstream tasks like question answering and fact-checking. Overall, these tasks are relevant for the semantic web community. Despite their popularity, the reproducibility of KGE experiments and the transferability of proposed KGE models to research fields outside the machine learning community can be a major challenge. Therefore, we present the KEEN Universe, an ecosystem for knowledge graph embeddings that we have developed with a strong focus on reproducibility and transferability. The KEEN Universe currently consists of the Python packages PyKEEN (Python KnowlEdge EmbeddiNgs), BioKEEN (Biological KnowlEdge EmbeddiNgs), and the KEEN Model Zoo for sharing trained KGE models with the community.

</details>

<details>

<summary>2020-01-28 23:15:28 - UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</summary>

- *Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming Liang*

- `1912.05074v2` - [abs](http://arxiv.org/abs/1912.05074v2) - [pdf](http://arxiv.org/pdf/1912.05074v2)

> The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects -- an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.

</details>

<details>

<summary>2020-01-29 08:52:58 - Dolphin: A Spoken Language Proficiency Assessment System for Elementary Education</summary>

- *Wenbiao Ding, Guowei Xu, Tianqiao Liu, Weiping Fu, Yujia Song, Chaoyou Guo, Cong Kong, Songfan Yang, Gale Yan Huang, Zitao Liu*

- `1908.00358v3` - [abs](http://arxiv.org/abs/1908.00358v3) - [pdf](http://arxiv.org/pdf/1908.00358v3)

> Spoken language proficiency is critically important for children's growth and personal development. Due to the limited and imbalanced educational resources in China, elementary students barely have chances to improve their oral language skills in classes. Verbal fluency tasks (VFTs) were invented to let the students practice their spoken language proficiency after school. VFTs are simple but concrete math related questions that ask students to not only report answers but speak out the entire thinking process. In spite of the great success of VFTs, they bring a heavy grading burden to elementary teachers. To alleviate this problem, we develop Dolphin, a spoken language proficiency assessment system for Chinese elementary education. Dolphin is able to automatically evaluate both phonological fluency and semantic relevance of students' VFT answers. We conduct a wide range of offline and online experiments to demonstrate the effectiveness of Dolphin. In our offline experiments, we show that Dolphin improves both phonological fluency and semantic relevance evaluation performance when compared to state-of-the-art baselines on real-world educational data sets. In our online A/B experiments, we test Dolphin with 183 teachers from 2 major cities (Hangzhou and Xi'an) in China for 10 weeks and the results show that VFT assignments grading coverage is improved by 22\%.

</details>

<details>

<summary>2020-01-29 15:45:47 - Interventions and Counterfactuals in Tractable Probabilistic Models: Limitations of Contemporary Transformations</summary>

- *Ioannis Papantonis, Vaishak Belle*

- `2001.10905v1` - [abs](http://arxiv.org/abs/2001.10905v1) - [pdf](http://arxiv.org/pdf/2001.10905v1)

> In recent years, there has been an increasing interest in studying causality-related properties in machine learning models generally, and in generative models in particular. While that is well motivated, it inherits the fundamental computational hardness of probabilistic inference, making exact reasoning intractable. Probabilistic tractable models have also recently emerged, which guarantee that conditional marginals can be computed in time linear in the size of the model, where the model is usually learned from data. Although initially limited to low tree-width models, recent tractable models such as sum product networks (SPNs) and probabilistic sentential decision diagrams (PSDDs) exploit efficient function representations and also capture high tree-width models.   In this paper, we ask the following technical question: can we use the distributions represented or learned by these models to perform causal queries, such as reasoning about interventions and counterfactuals? By appealing to some existing ideas on transforming such models to Bayesian networks, we answer mostly in the negative. We show that when transforming SPNs to a causal graph interventional reasoning reduces to computing marginal distributions; in other words, only trivial causal reasoning is possible. For PSDDs the situation is only slightly better. We first provide an algorithm for constructing a causal graph from a PSDD, which introduces augmented variables. Intervening on the original variables, once again, reduces to marginal distributions, but when intervening on the augmented variables, a deterministic but nonetheless causal-semantics can be provided for PSDDs.

</details>

<details>

<summary>2020-01-29 18:13:12 - A4 : Evading Learning-based Adblockers</summary>

- *Shitong Zhu, Zhongjie Wang, Xun Chen, Shasha Li, Umar Iqbal, Zhiyun Qian, Kevin S. Chan, Srikanth V. Krishnamurthy, Zubair Shafiq*

- `2001.10999v1` - [abs](http://arxiv.org/abs/2001.10999v1) - [pdf](http://arxiv.org/pdf/2001.10999v1)

> Efforts by online ad publishers to circumvent traditional ad blockers towards regaining fiduciary benefits, have been demonstrably successful. As a result, there have recently emerged a set of adblockers that apply machine learning instead of manually curated rules and have been shown to be more robust in blocking ads on websites including social media sites such as Facebook. Among these, AdGraph is arguably the state-of-the-art learning-based adblocker. In this paper, we develop A4, a tool that intelligently crafts adversarial samples of ads to evade AdGraph. Unlike the popular research on adversarial samples against images or videos that are considered less- to un-restricted, the samples that A4 generates preserve application semantics of the web page, or are actionable. Through several experiments we show that A4 can bypass AdGraph about 60% of the time, which surpasses the state-of-the-art attack by a significant margin of 84.3%; in addition, changes to the visual layout of the web page due to these perturbations are imperceptible. We envision the algorithmic framework proposed in A4 is also promising in improving adversarial attacks against other learning-based web applications with similar requirements.

</details>

<details>

<summary>2020-01-29 21:30:53 - Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal Urban Neighborhood Embedding</summary>

- *Zhecheng Wang, Haoyuan Li, Ram Rajagopal*

- `2001.11101v1` - [abs](http://arxiv.org/abs/2001.11101v1) - [pdf](http://arxiv.org/pdf/2001.11101v1)

> Understanding intrinsic patterns and predicting spatiotemporal characteristics of cities require a comprehensive representation of urban neighborhoods. Existing works relied on either inter- or intra-region connectivities to generate neighborhood representations but failed to fully utilize the informative yet heterogeneous data within neighborhoods. In this work, we propose Urban2Vec, an unsupervised multi-modal framework which incorporates both street view imagery and point-of-interest (POI) data to learn neighborhood embeddings. Specifically, we use a convolutional neural network to extract visual features from street view images while preserving geospatial similarity. Furthermore, we model each POI as a bag-of-words containing its category, rating, and review information. Analog to document embedding in natural language processing, we establish the semantic similarity between neighborhood ("document") and the words from its surrounding POIs in the vector space. By jointly encoding visual, textual, and geospatial information into the neighborhood representation, Urban2Vec can achieve performances better than baseline models and comparable to fully-supervised methods in downstream prediction tasks. Extensive experiments on three U.S. metropolitan areas also demonstrate the model interpretability, generalization capability, and its value in neighborhood similarity analysis.

</details>

<details>

<summary>2020-01-30 07:10:39 - The Direction-Aware, Learnable, Additive Kernels and the Adversarial Network for Deep Floor Plan Recognition</summary>

- *Yuli Zhang, Yeyang He, Shaowen Zhu, Xinhan Di*

- `2001.11194v1` - [abs](http://arxiv.org/abs/2001.11194v1) - [pdf](http://arxiv.org/pdf/2001.11194v1)

> This paper presents a new approach for the recognition of elements in floor plan layouts. Besides of elements with common shapes, we aim to recognize elements with irregular shapes such as circular rooms and inclined walls. Furthermore, the reduction of noise in the semantic segmentation of the floor plan is on demand. To this end, we propose direction-aware, learnable, additive kernels in the application of both the context module and common convolutional blocks. We apply them for high performance of elements with both common and irregular shapes. Besides, an adversarial network with two discriminators is proposed to further improve the accuracy of the elements and to reduce the noise of the semantic segmentation. Experimental results demonstrate the superiority and effectiveness of the proposed network over the state-of-the-art methods.

</details>

<details>

<summary>2020-01-30 10:27:12 - An Automated Framework for the Extraction of Semantic Legal Metadata from Legal Texts</summary>

- *Amin Sleimi, Nicolas Sannier, Mehrdad Sabetzadeh, Lionel Briand, Marcello Ceci, John Dann*

- `2001.11245v1` - [abs](http://arxiv.org/abs/2001.11245v1) - [pdf](http://arxiv.org/pdf/2001.11245v1)

> Semantic legal metadata provides information that helps with understanding and interpreting legal provisions. Such metadata is therefore important for the systematic analysis of legal requirements. However, manually enhancing a large legal corpus with semantic metadata is prohibitively expensive. Our work is motivated by two observations: (1) the existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis; (2) automated support for the extraction of semantic legal metadata is scarce, and it does not exploit the full potential of artificial intelligence technologies, notably natural language processing (NLP) and machine learning (ML). Our objective is to take steps toward overcoming these limitations. To do so, we review and reconcile the semantic legal metadata types proposed in the RE literature. Subsequently, we devise an automated extraction approach for the identified metadata types using NLP and ML. We evaluate our approach through two case studies over the Luxembourgish legislation. Our results indicate a high accuracy in the generation of metadata annotations. In particular, in the two case studies, we were able to obtain precision scores of 97.2% and 82.4% and recall scores of 94.9% and 92.4%.

</details>

<details>

<summary>2020-01-30 17:11:00 - Don't Parse, Generate! A Sequence to Sequence Architecture for Task-Oriented Semantic Parsing</summary>

- *Subendhu Rongali, Luca Soldaini, Emilio Monti, Wael Hamza*

- `2001.11458v1` - [abs](http://arxiv.org/abs/2001.11458v1) - [pdf](http://arxiv.org/pdf/2001.11458v1)

> Virtual assistants such as Amazon Alexa, Apple Siri, and Google Assistant often rely on a semantic parsing component to understand which action(s) to execute for an utterance spoken by its users. Traditionally, rule-based or statistical slot-filling systems have been used to parse "simple" queries; that is, queries that contain a single action and can be decomposed into a set of non-overlapping entities. More recently, shift-reduce parsers have been proposed to process more complex utterances. These methods, while powerful, impose specific limitations on the type of queries that can be parsed; namely, they require a query to be representable as a parse tree.   In this work, we propose a unified architecture based on Sequence to Sequence models and Pointer Generator Network to handle both simple and complex queries. Unlike other works, our approach does not impose any restriction on the semantic parse schema. Furthermore, experiments show that it achieves state of the art performance on three publicly available datasets (ATIS, SNIPS, Facebook TOP), relatively improving between 3.3% and 7.7% in exact match accuracy over previous systems. Finally, we show the effectiveness of our approach on two internal datasets.

</details>

<details>

<summary>2020-01-31 06:31:39 - Augmenting Visual Question Answering with Semantic Frame Information in a Multitask Learning Approach</summary>

- *Mehrdad Alizadeh, Barbara Di Eugenio*

- `2001.11673v1` - [abs](http://arxiv.org/abs/2001.11673v1) - [pdf](http://arxiv.org/pdf/2001.11673v1)

> Visual Question Answering (VQA) concerns providing answers to Natural Language questions about images. Several deep neural network approaches have been proposed to model the task in an end-to-end fashion. Whereas the task is grounded in visual processing, if the question focuses on events described by verbs, the language understanding component becomes crucial. Our hypothesis is that models should be aware of verb semantics, as expressed via semantic role labels, argument types, and/or frame elements. Unfortunately, no VQA dataset exists that includes verb semantic information. Our first contribution is a new VQA dataset (imSituVQA) that we built by taking advantage of the imSitu annotations. The imSitu dataset consists of images manually labeled with semantic frame elements, mostly taken from FrameNet. Second, we propose a multitask CNN-LSTM VQA model that learns to classify the answers as well as the semantic frame elements. Our experiments show that semantic frame element classification helps the VQA system avoid inconsistent responses and improves performance.

</details>

<details>

<summary>2020-01-31 10:57:59 - Unsupervised deep clustering for predictive texture pattern discovery in medical images</summary>

- *Matthias Perkonigg, Daniel Sobotka, Ahmed Ba-Ssalamah, Georg Langs*

- `2002.03721v1` - [abs](http://arxiv.org/abs/2002.03721v1) - [pdf](http://arxiv.org/pdf/2002.03721v1)

> Predictive marker patterns in imaging data are a means to quantify disease and progression, but their identification is challenging, if the underlying biology is poorly understood. Here, we present a method to identify predictive texture patterns in medical images in an unsupervised way. Based on deep clustering networks, we simultaneously encode and cluster medical image patches in a low-dimensional latent space. The resulting clusters serve as features for disease staging, linking them to the underlying disease. We evaluate the method on 70 T1-weighted magnetic resonance images of patients with different stages of liver steatosis. The deep clustering approach is able to find predictive clusters with a stable ranking, differentiating between low and high steatosis with an F1-Score of 0.78.

</details>

<details>

<summary>2020-01-31 11:04:52 - Break It Down: A Question Understanding Benchmark</summary>

- *Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, Jonathan Berant*

- `2001.11770v1` - [abs](http://arxiv.org/abs/2001.11770v1) - [pdf](http://arxiv.org/pdf/2001.11770v1)

> Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.

</details>

<details>

<summary>2020-01-31 17:25:17 - Modeling Events and Events of Events in Software Engineering</summary>

- *Sabah Al-Fedaghi*

- `2001.11962v1` - [abs](http://arxiv.org/abs/2001.11962v1) - [pdf](http://arxiv.org/pdf/2001.11962v1)

> A model is a simplified representation of portion of reality that hides a system s nonessential characteristics. It provides a means for reducing complexity as well as visualization and communication and a basis for building it. Most models involve graphic languages during many of the software lifecycle phases. A new model, called thinging machine (TM), has recently been developed as an extension of the input-process-output framework. The paper focuses on events in a TM, offering a new perspective that captures a system s dynamic behaviors and a means of diagrammatically modeling events. The event notion is an important factor in giving semantics to specifications and providing a natural way to specify the interfaces and observable behavior of system components. Specifically, five generic TM event processes are analyzed: create, process, receive, release, and transfer. All events can be mapped (or reduced) to the events of these five event processes

</details>


## 2020-02

<details>

<summary>2020-02-01 12:10:27 - Self-supervised Adversarial Training</summary>

- *Kejiang Chen, Hang Zhou, Yuefeng Chen, Xiaofeng Mao, Yuhong Li, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu*

- `1911.06470v2` - [abs](http://arxiv.org/abs/1911.06470v2) - [pdf](http://arxiv.org/pdf/1911.06470v2)

> Recent work has demonstrated that neural networks are vulnerable to adversarial examples. To escape from the predicament, many works try to harden the model in various ways, in which adversarial training is an effective way which learns robust feature representation so as to resist adversarial attacks. Meanwhile, the self-supervised learning aims to learn robust and semantic embedding from data itself. With these views, we introduce self-supervised learning to against adversarial examples in this paper. Specifically, the self-supervised representation coupled with k-Nearest Neighbour is proposed for classification. To further strengthen the defense ability, self-supervised adversarial training is proposed, which maximizes the mutual information between the representations of original examples and the corresponding adversarial examples. Experimental results show that the self-supervised representation outperforms its supervised version in respect of robustness and self-supervised adversarial training can further improve the defense ability efficiently.

</details>

<details>

<summary>2020-02-01 13:24:03 - Novel Entity Discovery from Web Tables</summary>

- *Shuo Zhang, Edgar Meij, Krisztian Balog, Ridho Reinanda*

- `2002.00206v1` - [abs](http://arxiv.org/abs/2002.00206v1) - [pdf](http://arxiv.org/pdf/2002.00206v1)

> When working with any sort of knowledge base (KB) one has to make sure it is as complete and also as up-to-date as possible. Both tasks are non-trivial as they require recall-oriented efforts to determine which entities and relationships are missing from the KB. As such they require a significant amount of labor. Tables on the Web, on the other hand, are abundant and have the distinct potential to assist with these tasks. In particular, we can leverage the content in such tables to discover new entities, properties, and relationships. Because web tables typically only contain raw textual content we first need to determine which cells refer to which known entities---a task we dub table-to-KB matching. This first task aims to infer table semantics by linking table cells and heading columns to elements of a KB. Then second task builds upon these linked entities and properties to not only identify novel ones in the same table but also to bootstrap their type and additional relationships. We refer to this process as novel entity discovery and, to the best of our knowledge, it is the first endeavor on mining the unlinked cells in web tables. Our method identifies not only out-of-KB (``novel'') information but also novel aliases for in-KB (``known'') entities. When evaluated using three purpose-built test collections, we find that our proposed approaches obtain a marked improvement in terms of precision over our baselines whilst keeping recall stable.

</details>

<details>

<summary>2020-02-01 19:42:56 - A Simple Differentiable Programming Language</summary>

- *Martin Abadi, Gordon D. Plotkin*

- `1911.04523v4` - [abs](http://arxiv.org/abs/1911.04523v4) - [pdf](http://arxiv.org/pdf/1911.04523v4)

> Automatic differentiation plays a prominent role in scientific computing and in modern machine learning, often in the context of powerful programming systems. The relation of the various embodiments of automatic differentiation to the mathematical notion of derivative is not always entirely clear---discrepancies can arise, sometimes inadvertently. In order to study automatic differentiation in such programming contexts, we define a small but expressive programming language that includes a construct for reverse-mode differentiation. We give operational and denotational semantics for this language. The operational semantics employs popular implementation techniques, while the denotational semantics employs notions of differentiation familiar from real analysis. We establish that these semantics coincide.

</details>

<details>

<summary>2020-02-03 03:13:40 - Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring</summary>

- *Xiyao Ma, Qile Zhu, Yanlin Zhou, Xiaolin Li, Dapeng Wu*

- `1912.00879v3` - [abs](http://arxiv.org/abs/1912.00879v3) - [pdf](http://arxiv.org/pdf/1912.00879v3)

> Taking an answer and its context as input, sequence-to-sequence models have made considerable progress on question generation. However, we observe that these approaches often generate wrong question words or keywords and copy answer-irrelevant words from the input. We believe that lacking global question semantics and exploiting answer position-awareness not well are the key root causes. In this paper, we propose a neural question generation model with two concrete modules: sentence-level semantic matching and answer position inferring. Further, we enhance the initial state of the decoder by leveraging the answer-aware gated fusion mechanism. Experimental results demonstrate that our model outperforms the state-of-the-art (SOTA) models on SQuAD and MARCO datasets. Owing to its generality, our work also improves the existing models significantly.

</details>

<details>

<summary>2020-02-03 10:57:24 - Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View Residual Learning</summary>

- *Muhammad Usman, Byoung-Dai Lee, Shi Sub Byon, Sung Hyun Kim, Byung-ilLee*

- `1912.13335v2` - [abs](http://arxiv.org/abs/1912.13335v2) - [pdf](http://arxiv.org/pdf/1912.13335v2)

> Accurate quantification of pulmonary nodules can greatly assist the early diagnosis of lung cancer, which can enhance patient survival possibilities. A number of nodule segmentation techniques have been proposed, however, all of the existing techniques rely on radiologist 3-D volume of interest (VOI) input or use the constant region of interest (ROI) and only investigate the presence of nodule voxels within the given VOI. Such approaches restrain the solutions to investigate the nodule presence outside the given VOI and also include the redundant structures into VOI, which may lead to inaccurate nodule segmentation. In this work, a novel semi-automated approach for 3-D segmentation of nodule in volumetric computerized tomography (CT) lung scans has been proposed. The proposed technique can be segregated into two stages, at the first stage, it takes a 2-D ROI containing the nodule as input and it performs patch-wise investigation along the axial axis with a novel adaptive ROI strategy. The adaptive ROI algorithm enables the solution to dynamically select the ROI for the surrounding slices to investigate the presence of nodule using deep residual U-Net architecture. The first stage provides the initial estimation of nodule which is further utilized to extract the VOI. At the second stage, the extracted VOI is further investigated along the coronal and sagittal axis with two different networks and finally, all the estimated masks are fed into the consensus module to produce the final volumetric segmentation of nodule. The proposed approach has been rigorously evaluated on the LIDC dataset, which is the largest publicly available dataset. The result suggests that the approach is significantly robust and accurate as compared to the previous state of the art techniques.

</details>

<details>

<summary>2020-02-03 17:14:17 - Multi Sense Embeddings from Topic Models</summary>

- *Shobhit Jain, Sravan Babu Bodapati, Ramesh Nallapati, Anima Anandkumar*

- `1909.07746v2` - [abs](http://arxiv.org/abs/1909.07746v2) - [pdf](http://arxiv.org/pdf/1909.07746v2)

> Distributed word embeddings have yielded state-of-the-art performance in many NLP tasks, mainly due to their success in capturing useful semantic information. These representations assign only a single vector to each word whereas a large number of words are polysemous (i.e., have multiple meanings). In this work, we approach this critical problem in lexical semantics, namely that of representing various senses of polysemous words in vector spaces. We propose a topic modeling based skip-gram approach for learning multi-prototype word embeddings. We also introduce a method to prune the embeddings determined by the probabilistic representation of the word in each topic. We use our embeddings to show that they can capture the context and word similarity strongly and outperform various state-of-the-art implementations.

</details>

<details>

<summary>2020-02-03 22:08:57 - On Rational Entailment for Propositional Typicality Logic</summary>

- *Richard Booth, Giovanni Casini, Thomas Meyer, Ivan Varzinczak*

- `1809.10946v2` - [abs](http://arxiv.org/abs/1809.10946v2) - [pdf](http://arxiv.org/pdf/1809.10946v2)

> Propositional Typicality Logic (PTL) is a recently proposed logic, obtained by enriching classical propositional logic with a typicality operator capturing the most typical (alias normal or conventional) situations in which a given sentence holds. The semantics of PTL is in terms of ranked models as studied in the well-known KLM approach to preferential reasoning and therefore KLM-style rational consequence relations can be embedded in PTL. In spite of the non-monotonic features introduced by the semantics adopted for the typicality operator, the obvious Tarskian definition of entailment for PTL remains monotonic and is therefore not appropriate in many contexts. Our first important result is an impossibility theorem showing that a set of proposed postulates that at first all seem appropriate for a notion of entailment with regard to typicality cannot be satisfied simultaneously. Closer inspection reveals that this result is best interpreted as an argument for advocating the development of more than one type of PTL entailment. In the spirit of this interpretation, we investigate three different (semantic) versions of entailment for PTL, each one based on the definition of rational closure as introduced by Lehmann and Magidor for KLM-style conditionals, and constructed using different notions of minimality.

</details>

<details>

<summary>2020-02-04 09:43:22 - Semantics-aware BERT for Language Understanding</summary>

- *Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, Xiang Zhou*

- `1909.02209v3` - [abs](http://arxiv.org/abs/1909.02209v3) - [pdf](http://arxiv.org/pdf/1909.02209v3)

> The latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BERT precursor in a light fine-tuning way without substantial task-specific modifications. Compared with BERT, semantics-aware BERT is as simple in concept but more powerful. It obtains new state-of-the-art or substantially improves results on ten reading comprehension and language inference tasks.

</details>

<details>

<summary>2020-02-04 19:34:34 - Bicycle Attacks Considered Harmful: Quantifying the Damage of Widespread Password Length Leakage</summary>

- *Benjamin Harsha, Robert Morton, Jeremiah Blocki, John Springer, Melissa Dark*

- `2002.01513v1` - [abs](http://arxiv.org/abs/2002.01513v1) - [pdf](http://arxiv.org/pdf/2002.01513v1)

> We examine the issue of password length leakage via encrypted traffic i.e., bicycle attacks. We aim to quantify both the prevalence of password length leakage bugs as well as the potential harm to users. In an observational study, we find that {\em most} of the Alexa top 100 rates sites are vulnerable to bicycle attacks meaning that an eavesdropping attacker can infer the exact length of a password based on the length the encrypted packet containing the password. We discuss several ways in which an eavesdropping attacker could link this password length with a particular user account e.g., a targeted campaign against a smaller group of users or via DNS hijacking for larger scale campaigns. We next use a decision-theoretic model to quantify the extent to which password length leakage might help an attacker to crack user passwords. In our analysis, we consider three different levels of password attackers: hacker, criminal and nation-state. In all cases, we find that such an attacker who knows the length of each user password gains a significant advantage over one without knowing the password length. As part of this analysis, we also release a new differentially private password frequency dataset from the 2016 LinkedIn breach using a differentially private algorithm of Blocki et al. (NDSS 2016) to protect user accounts. The LinkedIn frequency corpus is based on over 170 million passwords making it the largest frequency corpus publicly available to password researchers. While the defense against bicycle attacks is straightforward (i.e., ensure that passwords are always padded before encryption), we discuss several practical challenges organizations may face when attempting to patch this vulnerability. We advocate for a new W3C standard on how password fields are handled which would effectively eliminate most instances of password length leakage.

</details>

<details>

<summary>2020-02-04 22:39:58 - DVNet: A Memory-Efficient Three-Dimensional CNN for Large-Scale Neurovascular Reconstruction</summary>

- *Leila Saadatifard, Aryan Mobiny, Pavel Govyadinov, Hien Nguyen, David Mayerich*

- `2002.01568v1` - [abs](http://arxiv.org/abs/2002.01568v1) - [pdf](http://arxiv.org/pdf/2002.01568v1)

> Maps of brain microarchitecture are important for understanding neurological function and behavior, including alterations caused by chronic conditions such as neurodegenerative disease. Techniques such as knife-edge scanning microscopy (KESM) provide the potential for whole organ imaging at sub-cellular resolution. However, multi-terabyte data sizes make manual annotation impractical and automatic segmentation challenging. Densely packed cells combined with interconnected microvascular networks are a challenge for current segmentation algorithms. The massive size of high-throughput microscopy data necessitates fast and largely unsupervised algorithms. In this paper, we investigate a fully-convolutional, deep, and densely-connected encoder-decoder for pixel-wise semantic segmentation. The excessive memory complexity often encountered with deep and dense networks is mitigated using skip connections, resulting in fewer parameters and enabling a significant performance increase over prior architectures. The proposed network provides superior performance for semantic segmentation problems applied to open-source benchmarks. We finally demonstrate our network for cellular and microvascular segmentation, enabling quantitative metrics for organ-scale neurovascular analysis.

</details>

<details>

<summary>2020-02-05 06:23:44 - Continuous Melody Generation via Disentangled Short-Term Representations and Structural Conditions</summary>

- *Ke Chen, Gus Xia, Shlomo Dubnov*

- `2002.02393v1` - [abs](http://arxiv.org/abs/2002.02393v1) - [pdf](http://arxiv.org/pdf/2002.02393v1)

> Automatic music generation is an interdisciplinary research topic that combines computational creativity and semantic analysis of music to create automatic machine improvisations. An important property of such a system is allowing the user to specify conditions and desired properties of the generated music. In this paper we designed a model for composing melodies given a user specified symbolic scenario combined with a previous music context. We add manual labeled vectors denoting external music quality in terms of chord function that provides a low dimensional representation of the harmonic tension and resolution. Our model is capable of generating long melodies by regarding 8-beat note sequences as basic units, and shares consistent rhythm pattern structure with another specific song. The model contains two stages and requires separate training where the first stage adopts a Conditional Variational Autoencoder (C-VAE) to build a bijection between note sequences and their latent representations, and the second stage adopts long short-term memory networks (LSTM) with structural conditions to continue writing future melodies. We further exploit the disentanglement technique via C-VAE to allow melody generation based on pitch contour information separately from conditioning on rhythm patterns. Finally, we evaluate the proposed model using quantitative analysis of rhythm and the subjective listening study. Results show that the music generated by our model tends to have salient repetition structures, rich motives, and stable rhythm patterns. The ability to generate longer and more structural phrases from disentangled representations combined with semantic scenario specification conditions shows a broad application of our model.

</details>

<details>

<summary>2020-02-05 07:16:02 - Language is Power: Representing States Using Natural Language in Reinforcement Learning</summary>

- *Erez Schwartz, Guy Tennenholtz, Chen Tessler, Shie Mannor*

- `1910.02789v2` - [abs](http://arxiv.org/abs/1910.02789v2) - [pdf](http://arxiv.org/pdf/1910.02789v2)

> Recent advances in reinforcement learning have shown its potential to tackle complex real-life tasks. However, as the dimensionality of the task increases, reinforcement learning methods tend to struggle. To overcome this, we explore methods for representing the semantic information embedded in the state. While previous methods focused on information in its raw form (e.g., raw visual input), we propose to represent the state using natural language. Language can represent complex scenarios and concepts, making it a favorable candidate for representation. Empirical evidence, within the domain of ViZDoom, suggests that natural language based agents are more robust, converge faster and perform better than vision based agents, showing the benefit of using natural language representations for reinforcement learning.

</details>

<details>

<summary>2020-02-05 11:47:56 - Sequence-to-sequence Automatic Speech Recognition with Word Embedding Regularization and Fused Decoding</summary>

- *Alexander H. Liu, Tzu-Wei Sung, Shun-Po Chuang, Hung-yi Lee, Lin-shan Lee*

- `1910.12740v2` - [abs](http://arxiv.org/abs/1910.12740v2) - [pdf](http://arxiv.org/pdf/1910.12740v2)

> In this paper, we investigate the benefit that off-the-shelf word embedding can bring to the sequence-to-sequence (seq-to-seq) automatic speech recognition (ASR). We first introduced the word embedding regularization by maximizing the cosine similarity between a transformed decoder feature and the target word embedding. Based on the regularized decoder, we further proposed the fused decoding mechanism. This allows the decoder to consider the semantic consistency during decoding by absorbing the information carried by the transformed decoder feature, which is learned to be close to the target word embedding. Initial results on LibriSpeech demonstrated that pre-trained word embedding can significantly lower ASR recognition error with a negligible cost, and the choice of word embedding algorithms among Skip-gram, CBOW and BERT is important.

</details>

<details>

<summary>2020-02-05 12:44:01 - Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and Manual Correction</summary>

- *Mingchen Li, Zili Zhou, Yanna Wang*

- `2002.01761v1` - [abs](http://arxiv.org/abs/2002.01761v1) - [pdf](http://arxiv.org/pdf/2002.01761v1)

> Princeton WordNet (PWN) is a lexicon-semantic network based on cognitive linguistics, which promotes the development of natural language processing. Based on PWN, five Chinese wordnets have been developed to solve the problems of syntax and semantics. They include: Northeastern University Chinese WordNet (NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese WordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet (COW). By using them, we found that these word networks have low accuracy and coverage, and cannot completely portray the semantic network of PWN. So we decided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW) to make up those shortcomings. The key idea is to extend the SEW with the help of Oxford bilingual dictionary and Xinhua bilingual dictionary, and then correct it. More specifically, we used machine learning and manual adjustment in our corrections. Two standards were formulated to help our work. We conducted experiments on three tasks including relatedness calculation, word similarity and word sense disambiguation for the comparison of lemma's accuracy, at the same time, coverage also was compared. The results indicate that MCW can benefit from coverage and accuracy via our method. However, it still has room for improvement, especially with lemmas. In the future, we will continue to enhance the accuracy of MCW and expand the concepts in it.

</details>

<details>

<summary>2020-02-05 20:43:14 - UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B</summary>

- *Sai Krishna Telukuntla, Aditya Kapri, Wlodek Zadrozny*

- `2002.01984v1` - [abs](http://arxiv.org/abs/2002.01984v1) - [pdf](http://arxiv.org/pdf/2002.01984v1)

> In this paper, we detail our submission to the 2019, 7th year, BioASQ competition. We present our approach for Task-7b, Phase B, Exact Answering Task. These Question Answering (QA) tasks include Factoid, Yes/No, List Type Question answering. Our system is based on a contextual word embedding model. We have used a Bidirectional Encoder Representations from Transformers(BERT) based system, fined tuned for biomedical question answering task using BioBERT. In the third test batch set, our system achieved the highest MRR score for Factoid Question Answering task. Also, for List type question answering task our system achieved the highest recall score in the fourth test batch set. Along with our detailed approach, we present the results for our submissions, and also highlight identified downsides for our current approach and ways to improve them in our future experiments.

</details>

<details>

<summary>2020-02-06 08:24:33 - A Neural Topical Expansion Framework for Unstructured Persona-oriented Dialogue Generation</summary>

- *Minghong Xu, Piji Li, Haoran Yang, Pengjie Ren, Zhaochun Ren, Zhumin Chen, Jun Ma*

- `2002.02153v1` - [abs](http://arxiv.org/abs/2002.02153v1) - [pdf](http://arxiv.org/pdf/2002.02153v1)

> Unstructured Persona-oriented Dialogue Systems (UPDS) has been demonstrated effective in generating persona consistent responses by utilizing predefined natural language user persona descriptions (e.g., "I am a vegan"). However, the predefined user persona descriptions are usually short and limited to only a few descriptive words, which makes it hard to correlate them with the dialogues. As a result, existing methods either fail to use the persona description or use them improperly when generating persona consistent responses. To address this, we propose a neural topical expansion framework, namely Persona Exploration and Exploitation (PEE), which is able to extend the predefined user persona description with semantically correlated content before utilizing them to generate dialogue responses. PEE consists of two main modules: persona exploration and persona exploitation. The former learns to extend the predefined user persona description by mining and correlating with existing dialogue corpus using a variational auto-encoder (VAE) based topic model. The latter learns to generate persona consistent responses by utilizing the predefined and extended user persona description. In order to make persona exploitation learn to utilize user persona description more properly, we also introduce two persona-oriented loss functions: Persona-oriented Matching (P-Match) loss and Persona-oriented Bag-of-Words (P-BoWs) loss which respectively supervise persona selection in encoder and decoder. Experimental results show that our approach outperforms state-of-the-art baselines, in terms of both automatic and human evaluations.

</details>

<details>

<summary>2020-02-06 13:11:46 - Towards Semantic Noise Cleansing of Categorical Data based on Semantic Infusion</summary>

- *Rishabh Gupta, Rajesh N Rao*

- `2002.02238v1` - [abs](http://arxiv.org/abs/2002.02238v1) - [pdf](http://arxiv.org/pdf/2002.02238v1)

> Semantic Noise affects text analytics activities for the domain-specific industries significantly. It impedes the text understanding which holds prime importance in the critical decision making tasks. In this work, we formalize semantic noise as a sequence of terms that do not contribute to the narrative of the text. We look beyond the notion of standard statistically-based stop words and consider the semantics of terms to exclude the semantic noise. We present a novel Semantic Infusion technique to associate meta-data with the categorical corpus text and demonstrate its near-lossless nature. Based on this technique, we propose an unsupervised text-preprocessing framework to filter the semantic noise using the context of the terms. Later we present the evaluation results of the proposed framework using a web forum dataset from the automobile-domain.

</details>

<details>

<summary>2020-02-06 16:57:27 - Conversational Structure Aware and Context Sensitive Topic Model for Online Discussions</summary>

- *Yingcheng Sun, Kenneth Loparo, Richard Kolacinski*

- `2002.02353v1` - [abs](http://arxiv.org/abs/2002.02353v1) - [pdf](http://arxiv.org/pdf/2002.02353v1)

> Millions of online discussions are generated everyday on social media platforms. Topic modelling is an efficient way of better understanding large text datasets at scale. Conventional topic models have had limited success in online discussions, and to overcome their limitations, we use the discussion thread tree structure and propose a "popularity" metric to quantify the number of replies to a comment to extend the frequency of word occurrences, and the "transitivity" concept to characterize topic dependency among nodes in a nested discussion thread. We build a Conversational Structure Aware Topic Model (CSATM) based on popularity and transitivity to infer topics and their assignments to comments. Experiments on real forum datasets are used to demonstrate improved performance for topic extraction with six different measurements of coherence and impressive accuracy for topic assignments.

</details>

<details>

<summary>2020-02-06 18:51:16 - Compositional Neural Machine Translation by Removing the Lexicon from Syntax</summary>

- *Tristan Thrush*

- `2002.08899v1` - [abs](http://arxiv.org/abs/2002.08899v1) - [pdf](http://arxiv.org/pdf/2002.08899v1)

> The meaning of a natural language utterance is largely determined from its syntax and words. Additionally, there is evidence that humans process an utterance by separating knowledge about the lexicon from syntax knowledge. Theories from semantics and neuroscience claim that complete word meanings are not encoded in the representation of syntax. In this paper, we propose neural units that can enforce this constraint over an LSTM encoder and decoder. We demonstrate that our model achieves competitive performance across a variety of domains including semantic parsing, syntactic parsing, and English to Mandarin Chinese translation. In these cases, our model outperforms the standard LSTM encoder and decoder architecture on many or all of our metrics. To demonstrate that our model achieves the desired separation between the lexicon and syntax, we analyze its weights and explore its behavior when different neural modules are damaged. When damaged, we find that the model displays the knowledge distortions that aphasics are evidenced to have.

</details>

<details>

<summary>2020-02-07 01:54:15 - Learning Hyperspectral Feature Extraction and Classification with ResNeXt Network</summary>

- *Divinah Nyasaka, Jing Wang, Haron Tinega*

- `2002.02585v1` - [abs](http://arxiv.org/abs/2002.02585v1) - [pdf](http://arxiv.org/pdf/2002.02585v1)

> The Hyperspectral image (HSI) classification is a standard remote sensing task, in which each image pixel is given a label indicating the physical land-cover on the earth's surface. The achievements of image semantic segmentation and deep learning approaches on ordinary images have accelerated the research on hyperspectral image classification. Moreover, the utilization of both the spectral and spatial cues in hyperspectral images has shown improved classification accuracy in hyperspectral image classification. The use of only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and spectral cues from Hyperspectral images results in an explosion of parameters hence high computational cost. We propose network architecture called the MixedSN that utilizes the 3D convolutions to modeling spectral-spatial information in the early layers of the architecture and the 2D convolutions at the top layers which majorly deal with semantic abstraction. We constrain our architecture to ResNeXt block because of their performance and simplicity. Our model drastically reduced the number of parameters and achieved comparable classification performance with state-of-the-art methods on Indian Pine (IP) scene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset, and Botswana (BW) dataset.

</details>

<details>

<summary>2020-02-07 05:34:58 - MA-DST: Multi-Attention Based Scalable Dialog State Tracking</summary>

- *Adarsh Kumar, Peter Ku, Anuj Kumar Goyal, Angeliki Metallinou, Dilek Hakkani-Tur*

- `2002.08898v1` - [abs](http://arxiv.org/abs/2002.08898v1) - [pdf](http://arxiv.org/pdf/2002.08898v1)

> Task oriented dialog agents provide a natural language interface for users to complete their goal. Dialog State Tracking (DST), which is often a core component of these systems, tracks the system's understanding of the user's goal throughout the conversation. To enable accurate multi-domain DST, the model needs to encode dependencies between past utterances and slot semantics and understand the dialog context, including long-range cross-domain references. We introduce a novel architecture for this task to encode the conversation history and slot semantics more robustly by using attention mechanisms at multiple granularities. In particular, we use cross-attention to model relationships between the context and slots at different semantic levels and self-attention to resolve cross-domain coreferences. In addition, our proposed architecture does not rely on knowing the domain ontologies beforehand and can also be used in a zero-shot setting for new domains or unseen slot values. Our model improves the joint goal accuracy by 5% (absolute) in the full-data setting and by up to 2% (absolute) in the zero-shot setting over the present state-of-the-art on the MultiWoZ 2.1 dataset.

</details>

<details>

<summary>2020-02-07 07:19:34 - What You See is What it Means! Semantic Representation Learning of Code based on Visualization and Transfer Learning</summary>

- *Patrick Keller, Laura Plein, Tegawendé F. Bissyandé, Jacques Klein, Yves Le Traon*

- `2002.02650v1` - [abs](http://arxiv.org/abs/2002.02650v1) - [pdf](http://arxiv.org/pdf/2002.02650v1)

> Recent successes in training word embeddings for NLP tasks have encouraged a wave of research on representation learning for source code, which builds on similar NLP methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the WYSIWIM ("What You See Is What It Means") approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the BigCloneBench (Java) and Open Judge (C) datasets that although simple, our WYSIWIM approach performs as effectively as state of the art approaches such as ASTNN or TBCNN. We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.

</details>

<details>

<summary>2020-02-07 09:43:40 - Machine Education: Designing semantically ordered and ontologically guided modular neural networks</summary>

- *Hussein A. Abbass, Sondoss Elsawah, Eleni Petraki, Robert Hunjet*

- `2002.03841v1` - [abs](http://arxiv.org/abs/2002.03841v1) - [pdf](http://arxiv.org/pdf/2002.03841v1)

> The literature on machine teaching, machine education, and curriculum design for machines is in its infancy with sparse papers on the topic primarily focusing on data and model engineering factors to improve machine learning. In this paper, we first discuss selected attempts to date on machine teaching and education. We then bring theories and methodologies together from human education to structure and mathematically define the core problems in lesson design for machine education and the modelling approaches required to support the steps for machine education. Last, but not least, we offer an ontology-based methodology to guide the development of lesson plans to produce transparent and explainable modular learning machines, including neural networks.

</details>

<details>

<summary>2020-02-07 10:42:22 - Overview of chemical ontologies</summary>

- *Christian Pachl, Nils Frank, Jan Breitbart, Stefan Bräse*

- `2002.03842v1` - [abs](http://arxiv.org/abs/2002.03842v1) - [pdf](http://arxiv.org/pdf/2002.03842v1)

> Ontologies order and interconnect knowledge of a certain field in a formal and semantic way so that they are machine-parsable. They try to define allwhere acceptable definition of concepts and objects, classify them, provide properties as well as interconnect them with relations (e.g. "A is a special case of B"). More precisely, Tom Gruber defines Ontologies as a "specification of a conceptualization; [...] a description (like a formal specification of a program) of the concepts and relationships that can exist for an agent or a community of agents." [1] An Ontology is made of Individuals which are organized in Classes. Both can have Attributes and Relations among themselves. Some complex Ontologies define Restrictions, Rules and Events which change attributes or relations. To be computer accessible they are written in certain ontology languages, like the OBO language or the more used Common Algebraic Specification Language. With the rising of a digitalized, interconnected and globalized world, where common standards have to be found, ontologies are of great interest. [2] Yet, the development of chemical ontologies is in the beginning. Indeed, some interesting basic approaches towards chemical ontologies can be found, but nevertheless they suffer from two main flaws. Firstly, we found that they are mostly only fragmentary completed or are still in an architecture state. Secondly, apparently no chemical ontology is widespread accepted. Therefore, we herein try to describe the major ontology-developments in the chemical related fields Ontologies about chemical analytical methods, Ontologies about name reactions and Ontologies about scientific units.

</details>

<details>

<summary>2020-02-07 12:26:41 - Incorporating Visual Semantics into Sentence Representations within a Grounded Space</summary>

- *Patrick Bordes, Eloi Zablocki, Laure Soulier, Benjamin Piwowarski, Patrick Gallinari*

- `2002.02734v1` - [abs](http://arxiv.org/abs/2002.02734v1) - [pdf](http://arxiv.org/pdf/2002.02734v1)

> Language grounding is an active field aiming at enriching textual representations with visual information. Generally, textual and visual elements are embedded in the same representation space, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations --- the focus of this paper --- as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer visual information to textual representations by learning an intermediate representation space: the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this model outperforms the previous state-of-the-art on classification and semantic relatedness tasks.

</details>

<details>

<summary>2020-02-07 17:03:50 - Ensembles of Locally Independent Prediction Models</summary>

- *Andrew Slavin Ross, Weiwei Pan, Leo Anthony Celi, Finale Doshi-Velez*

- `1911.01291v3` - [abs](http://arxiv.org/abs/1911.01291v3) - [pdf](http://arxiv.org/pdf/1911.01291v3)

> Ensembles depend on diversity for improved performance. Many ensemble training methods, therefore, attempt to optimize for diversity, which they almost always define in terms of differences in training set predictions. In this paper, however, we demonstrate the diversity of predictions on the training set does not necessarily imply diversity under mild covariate shift, which can harm generalization in practical settings. To address this issue, we introduce a new diversity metric and associated method of training ensembles of models that extrapolate differently on local patches of the data manifold. Across a variety of synthetic and real-world tasks, we find that our method improves generalization and diversity in qualitatively novel ways, especially under data limits and covariate shift.

</details>

<details>

<summary>2020-02-07 18:11:58 - AT-GAN: An Adversarial Generator Model for Non-constrained Adversarial Examples</summary>

- *Xiaosen Wang, Kun He, Chuanbiao Song, Liwei Wang, John E. Hopcroft*

- `1904.07793v4` - [abs](http://arxiv.org/abs/1904.07793v4) - [pdf](http://arxiv.org/pdf/1904.07793v4)

> Despite the rapid development of adversarial machine learning, most adversarial attack and defense researches mainly focus on the perturbation-based adversarial examples, which is constrained by the input images. In comparison with existing works, we propose non-constrained adversarial examples, which are generated entirely from scratch without any constraint on the input. Unlike perturbation-based attacks, or the so-called unrestricted adversarial attack which is still constrained by the input noise, we aim to learn the distribution of adversarial examples to generate non-constrained but semantically meaningful adversarial examples. Following this spirit, we propose a novel attack framework called AT-GAN (Adversarial Transfer on Generative Adversarial Net). Specifically, we first develop a normal GAN model to learn the distribution of benign data, and then transfer the pre-trained GAN model to estimate the distribution of adversarial examples for the target model. In this way, AT-GAN can learn the distribution of adversarial examples that is very close to the distribution of real data. To our knowledge, this is the first work of building an adversarial generator model that could produce adversarial examples directly from any input noise. Extensive experiments and visualizations show that the proposed AT-GAN can very efficiently generate diverse adversarial examples that are more realistic to human perception. In addition, AT-GAN yields higher attack success rates against adversarially trained models under white-box attack setting and exhibits moderate transferability against black-box models.

</details>

<details>

<summary>2020-02-07 19:05:15 - Automatic Inference of High-Level Network Intents by Mining Forwarding Patterns</summary>

- *Ali Kheradmand*

- `2002.02423v2` - [abs](http://arxiv.org/abs/2002.02423v2) - [pdf](http://arxiv.org/pdf/2002.02423v2)

> There is a semantic gap between the high-level intents of network operators and the low-level configurations that achieve the intents. Previous works tried to bridge the gap using verification or synthesis techniques, both requiring formal specifications of the intended behavior which are rarely available or even known in the real world. This paper discusses an alternative approach for bridging the gap, namely to infer the high-level intents from the low-level network behavior. Specifically, we provide Anime, a framework and a tool that given a set of observed forwarding behavior, automatically infers a set of possible intents that best describe all observations. Our results show that Anime can infer high-quality intents from the low-level forwarding behavior with acceptable performance.

</details>

<details>

<summary>2020-02-08 00:22:14 - MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences</summary>

- *Xingyu Liu, Mengyuan Yan, Jeannette Bohg*

- `1910.09165v2` - [abs](http://arxiv.org/abs/1910.09165v2) - [pdf](http://arxiv.org/pdf/1910.09165v2)

> Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called $MeteorNet$ for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.

</details>

<details>

<summary>2020-02-08 00:42:21 - autoNLP: NLP Feature Recommendations for Text Analytics Applications</summary>

- *Janardan Misra*

- `2002.03056v1` - [abs](http://arxiv.org/abs/2002.03056v1) - [pdf](http://arxiv.org/pdf/2002.03056v1)

> While designing machine learning based text analytics applications, often, NLP data scientists manually determine which NLP features to use based upon their knowledge and experience with related problems. This results in increased efforts during feature engineering process and renders automated reuse of features across semantically related applications inherently difficult. In this paper, we argue for standardization in feature specification by outlining structure of a language for specifying NLP features and present an approach for their reuse across applications to increase likelihood of identifying optimal features.

</details>

<details>

<summary>2020-02-08 01:40:57 - Generation of smoothly-varying infill configurations from a continuous menu of cell patterns and the asymptotic analysis of its mechanical behaviour</summary>

- *Dingchuan Xue, Yichao Zhu, Xu Guo*

- `2002.01894v2` - [abs](http://arxiv.org/abs/2002.01894v2) - [pdf](http://arxiv.org/pdf/2002.01894v2)

> We here introduce a novel scheme for generating smoothly-varying infill graded microstructural (IGM) configurations from a given menu of generating cells. The scheme was originally proposed for essentially improving the variety of describable configurations in a modified asymptotic homogenisation-based topology optimisation framework [1] for fast IGM design. But the proposed scheme, after modification, also demonstrates its unique values in two aspects of applications. First, it provides a fairly simple way of generating an IGM configuration continuously patching any given cell configurations. Second, it tenders a straightforward mean for decorating microstructures on a given manifold. We will further show that the form of topology description function given here effectively offers a platform for unifying most existing approaches for IGM generation. Fuelled by asymptotic analysis of the mechanical behaviour of the resulting IGM configurations, a topology optimisation scheme for compliance minimisation is introduced. We will finally show that, the use of the present scheme helps reduce the compliance value of an optimised structure by nearly a half, if compared with that from the original framework [1].

</details>

<details>

<summary>2020-02-08 01:59:06 - Learning from, Understanding, and Supporting DevOps Artifacts for Docker</summary>

- *Jordan Henkel, Christian Bird, Shuvendu K. Lahiri, Thomas Reps*

- `2002.03064v1` - [abs](http://arxiv.org/abs/2002.03064v1) - [pdf](http://arxiv.org/pdf/2002.03064v1)

> With the growing use of DevOps tools and frameworks, there is an increased need for tools and techniques that support more than code. The current state-of-the-art in static developer assistance for tools like Docker is limited to shallow syntactic validation. We identify three core challenges in the realm of learning from, understanding, and supporting developers writing DevOps artifacts: (i) nested languages in DevOps artifacts, (ii) rule mining, and (iii) the lack of semantic rule-based analysis. To address these challenges we introduce a toolset, binnacle, that enabled us to ingest 900,000 GitHub repositories.   Focusing on Docker, we extracted approximately 178,000 unique Dockerfiles, and also identified a Gold Set of Dockerfiles written by Docker experts. We addressed challenge (i) by reducing the number of effectively uninterpretable nodes in our ASTs by over 80% via a technique we call phased parsing. To address challenge (ii), we introduced a novel rule-mining technique capable of recovering two-thirds of the rules in a benchmark we curated. Through this automated mining, we were able to recover 16 new rules that were not found during manual rule collection. To address challenge (iii), we manually collected a set of rules for Dockerfiles from commits to the files in the Gold Set. These rules encapsulate best practices, avoid docker build failures, and improve image size and build latency. We created an analyzer that used these rules, and found that, on average, Dockerfiles on GitHub violated the rules five times more frequently than the Dockerfiles in our Gold Set. We also found that industrial Dockerfiles fared no better than those sourced from GitHub.   The learned rules and analyzer in binnacle can be used to aid developers in the IDE when creating Dockerfiles, and in a post-hoc fashion to identify issues in, and to improve, existing Dockerfiles.

</details>

<details>

<summary>2020-02-08 03:54:39 - Comprehensive and Efficient Data Labeling via Adaptive Model Scheduling</summary>

- *Mu Yuan, Lan Zhang, Xiang-Yang Li, Hui Xiong*

- `2002.05520v1` - [abs](http://arxiv.org/abs/2002.05520v1) - [pdf](http://arxiv.org/pdf/2002.05520v1)

> Labeling data (e.g., labeling the people, objects, actions and scene in images) comprehensively and efficiently is a widely needed but challenging task. Numerous models were proposed to label various data and many approaches were designed to enhance the ability of deep learning models or accelerate them. Unfortunately, a single machine-learning model is not powerful enough to extract various semantic information from data. Given certain applications, such as image retrieval platforms and photo album management apps, it is often required to execute a collection of models to obtain sufficient labels. With limited computing resources and stringent delay, given a data stream and a collection of applicable resource-hungry deep-learning models, we design a novel approach to adaptively schedule a subset of these models to execute on each data item, aiming to maximize the value of the model output (e.g., the number of high-confidence labels). Achieving this lofty goal is nontrivial since a model's output on any data item is content-dependent and unknown until we execute it. To tackle this, we propose an Adaptive Model Scheduling framework, consisting of 1) a deep reinforcement learning-based approach to predict the value of unexecuted models by mining semantic relationship among diverse models, and 2) two heuristic algorithms to adaptively schedule the model execution order under a deadline or deadline-memory constraints respectively. The proposed framework doesn't require any prior knowledge of the data, which works as a powerful complement to existing model optimization technologies. We conduct extensive evaluations on five diverse image datasets and 30 popular image labeling models to demonstrate the effectiveness of our design: our design could save around 53\% execution time without loss of any valuable labels.

</details>

<details>

<summary>2020-02-08 20:27:07 - Free-breathing Cardiovascular MRI Using a Plug-and-Play Method with Learned Denoiser</summary>

- *Sizhuo Liu, Edward Reehorst, Philip Schniter, Rizwan Ahmad*

- `2002.03226v1` - [abs](http://arxiv.org/abs/2002.03226v1) - [pdf](http://arxiv.org/pdf/2002.03226v1)

> Cardiac magnetic resonance imaging (CMR) is a noninvasive imaging modality that provides a comprehensive evaluation of the cardiovascular system. The clinical utility of CMR is hampered by long acquisition times, however. In this work, we propose and validate a plug-and-play (PnP) method for CMR reconstruction from undersampled multi-coil data. To fully exploit the rich image structure inherent in CMR, we pair the PnP framework with a deep learning (DL)-based denoiser that is trained using spatiotemporal patches from high-quality, breath-held cardiac cine images. The resulting "PnP-DL" method iterates over data consistency and denoising subroutines. We compare the reconstruction performance of PnP-DL to that of compressed sensing (CS) using eight breath-held and ten real-time (RT) free-breathing cardiac cine datasets. We find that, for breath-held datasets, PnP-DL offers more than one dB advantage over commonly used CS methods. For RT free-breathing datasets, where ground truth is not available, PnP-DL receives higher scores in qualitative evaluation. The results highlight the potential of PnP-DL to accelerate RT CMR.

</details>

<details>

<summary>2020-02-09 13:17:08 - Reinforced Bit Allocation under Task-Driven Semantic Distortion Metrics</summary>

- *Jun Shi, Zhibo Chen*

- `1910.07392v2` - [abs](http://arxiv.org/abs/1910.07392v2) - [pdf](http://arxiv.org/pdf/1910.07392v2)

> Rapid growing intelligent applications require optimized bit allocation in image/video coding to support specific task-driven scenarios such as detection, classification, segmentation, etc. Some learning-based frameworks have been proposed for this purpose due to their inherent end-to-end optimization mechanisms. However, it is still quite challenging to integrate these task-driven metrics seamlessly into traditional hybrid coding framework. To the best of our knowledge, this paper is the first work trying to solve this challenge based on reinforcement learning (RL) approach. Specifically, we formulate the bit allocation problem as a Markovian Decision Process (MDP) and train RL agents to automatically decide the quantization parameter (QP) of each coding tree unit (CTU) for HEVC intra coding, according to the task-driven semantic distortion metrics. This bit allocation scheme can maximize the semantic level fidelity of the task, such as classification accuracy, while minimizing the bit-rate. We also employ gradient class activation map (Grad-CAM) and Mask R-CNN tools to extract task-related importance maps to help the agents make decisions. Extensive experimental results demonstrate the superior performance of our approach by achieving 43.1% to 73.2% bit-rate saving over the anchor of HEVC under the equivalent task-related distortions.

</details>

<details>

<summary>2020-02-09 19:20:56 - Importance-Driven Deep Learning System Testing</summary>

- *Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, Alper Cakan*

- `2002.03433v1` - [abs](http://arxiv.org/abs/2002.03433v1) - [pdf](http://arxiv.org/pdf/2002.03433v1)

> Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.

</details>

<details>

<summary>2020-02-09 19:53:23 - Limits of Detecting Text Generated by Large-Scale Language Models</summary>

- *Lav R. Varshney, Nitish Shirish Keskar, Richard Socher*

- `2002.03438v1` - [abs](http://arxiv.org/abs/2002.03438v1) - [pdf](http://arxiv.org/pdf/2002.03438v1)

> Some consider large-scale language models that can generate long and coherent pieces of text as dangerous, since they may be used in misinformation campaigns. Here we formulate large-scale language model output detection as a hypothesis testing problem to classify text as genuine or generated. We show that error exponents for particular language models are bounded in terms of their perplexity, a standard measure of language generation performance. Under the assumption that human language is stationary and ergodic, the formulation is extended from considering specific language models to considering maximum likelihood language models, among the class of k-order Markov approximations; error probabilities are characterized. Some discussion of incorporating semantic side information is also given.

</details>

<details>

<summary>2020-02-10 08:20:19 - Droidetec: Android Malware Detection and Malicious Code Localization through Deep Learning</summary>

- *Zhuo Ma, Haoran Ge, Zhuzhu Wang, Yang Liu, Ximeng Liu*

- `2002.03594v1` - [abs](http://arxiv.org/abs/2002.03594v1) - [pdf](http://arxiv.org/pdf/2002.03594v1)

> Android malware detection is a critical step towards building a security credible system. Especially, manual search for the potential malicious code has plagued program analysts for a long time. In this paper, we propose Droidetec, a deep learning based method for android malware detection and malicious code localization, to model an application program as a natural language sequence. Droidetec adopts a novel feature extraction method to derive behavior sequences from Android applications. Based on that, the bi-directional Long Short Term Memory network is utilized for malware detection. Each unit in the extracted behavior sequence is inventively represented as a vector, which allows Droidetec to automatically analyze the semantics of sequence segments and eventually find out the malicious code. Experiments with 9616 malicious and 11982 benign programs show that Droidetec reaches an accuracy of 97.22% and an F1-score of 98.21%. In all, Droidetec has a hit rate of 91% to properly find out malicious code segments.

</details>

<details>

<summary>2020-02-10 08:41:03 - The Tensor Brain: Semantic Decoding for Perception and Memory</summary>

- *Volker Tresp, Sahand Sharifzadeh, Dario Konopatzki, Yunpu Ma*

- `2001.11027v3` - [abs](http://arxiv.org/abs/2001.11027v3) - [pdf](http://arxiv.org/pdf/2001.11027v3)

> We analyse perception and memory, using mathematical models for knowledge graphs and tensors, to gain insights into the corresponding functionalities of the human mind. Our discussion is based on the concept of propositional sentences consisting of \textit{subject-predicate-object} (SPO) triples for expressing elementary facts. SPO sentences are the basis for most natural languages but might also be important for explicit perception and declarative memories, as well as intra-brain communication and the ability to argue and reason. A set of SPO sentences can be described as a knowledge graph, which can be transformed into an adjacency tensor. We introduce tensor models, where concepts have dual representations as indices and associated embeddings, two constructs we believe are essential for the understanding of implicit and explicit perception and memory in the brain. We argue that a biological realization of perception and memory imposes constraints on information processing. In particular, we propose that explicit perception and declarative memories require a semantic decoder, which, in a simple realization, is based on four layers: First, a sensory memory layer, as a buffer for sensory input, second, an index layer representing concepts, third, a memoryless representation layer for the broadcasting of information ---the "blackboard", or the "canvas" of the brain--- and fourth, a working memory layer as a processing center and data buffer. We discuss the operations of the four layers and relate them to the global workspace theory. In a Bayesian brain interpretation, semantic memory defines the prior for observable triple statements. We propose that ---in evolution and during development--- semantic memory, episodic memory, and natural language evolved as emergent properties in agents' process to gain a deeper understanding of sensory information.

</details>

<details>

<summary>2020-02-10 14:47:20 - Unsupervised Adaptive Neural Network Regularization for Accelerated Radial Cine MRI</summary>

- *Andreas Kofler, Marc Dewey, Tobias Schaeffter, Christoph Kolbitsch, Markus Haltmeier*

- `2002.03820v1` - [abs](http://arxiv.org/abs/2002.03820v1) - [pdf](http://arxiv.org/pdf/2002.03820v1)

> In this work, we propose an iterative reconstruction scheme (ALONE - Adaptive Learning Of NEtworks) for 2D radial cine MRI based on ground truth-free unsupervised learning of shallow convolutional neural networks. The network is trained to approximate patches of the current estimate of the solution during the reconstruction. By imposing a shallow network topology and constraining the $L_2$-norm of the learned filters, the network's representation power is limited in order not to be able to recover noise. Therefore, the network can be interpreted to perform a low dimensional approximation of the patches for stabilizing the inversion process. We compare the proposed reconstruction scheme to two ground truth-free reconstruction methods, namely a well known Total Variation (TV) minimization and an unsupervised adaptive Dictionary Learning (DIC) method. The proposed method outperforms both methods with respect to all reported quantitative measures. Further, in contrast to DIC, where the sparse approximation of the patches involves the solution of a complex optimization problem, ALONE only requires a forward pass of all patches through the shallow network and therefore significantly accelerates the reconstruction.

</details>

<details>

<summary>2020-02-10 18:03:44 - Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation</summary>

- *Jun Liu, Xiangyue Wang, Xue-cheng Tai*

- `2002.03989v1` - [abs](http://arxiv.org/abs/2002.03989v1) - [pdf](http://arxiv.org/pdf/2002.03989v1)

> We use Deep Convolutional Neural Networks (DCNNs) for image segmentation problems. DCNNs can well extract the features from natural images. However, the classification functions in the existing network architecture of CNNs are simple and lack capabilities to handle important spatial information in a way that have been done for many well-known traditional variational models. Prior such as spatial regularity, volume prior and object shapes cannot be well handled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD) framework which can easily integrate many spatial priors of the classical variational models into the DCNNs for image segmentation. The novelty of our method is to interpret the softmax activation function as a dual variable in a variational problem, and thus many spatial priors can be imposed in the dual space. From this viewpoint, we can build a STD based framework which can enable the outputs of DCNNs to have many special priors such as spatial regularity, volume constraints and star-shape priori. The proposed method is a general mathematical framework and it can be applied to any semantic segmentation DCNNs. To show the efficiency and accuracy of our method, we applied it to the popular DeepLabV3+ image segmentation network, and the experiments results show that our method can work efficiently on data-driven image segmentation DCNNs.

</details>

<details>

<summary>2020-02-10 19:48:14 - Nested Multiple Instance Learning in Modelling of HTTP network traffic</summary>

- *Tomas Pevny, Marek Dedic*

- `2002.04059v1` - [abs](http://arxiv.org/abs/2002.04059v1) - [pdf](http://arxiv.org/pdf/2002.04059v1)

> In many interesting cases, the application of machine learning is hindered by data having a complicated structure stimulated by a structured file-formats like JSONs, XMLs, or ProtoBuffers, which is non-trivial to convert to a vector / matrix. Moreover, since the structure frequently carries a semantic meaning, reflecting it in the machine learning model should improve the accuracy but more importantly it facilitates the explanation of decisions and the model. This paper demonstrates on the identification of infected computers in the computer network from their HTTP traffic, how to achieve this reflection using recent progress in multiple-instance learning. The proposed model is compared to complementary approaches from the prior art, the first relying on human-designed features and the second on automatically learned features through convolution neural networks. In a challenging scenario measuring accuracy only on unseen domains/malware families, the proposed model is superior to the prior art while providing a valuable feedback to the security researchers. We believe that the proposed framework will found applications elsewhere even beyond the field of security.

</details>

<details>

<summary>2020-02-10 20:32:23 - Explanation by Progressive Exaggeration</summary>

- *Sumedha Singla, Brian Pollack, Junxiang Chen, Kayhan Batmanghelich*

- `1911.00483v3` - [abs](http://arxiv.org/abs/1911.00483v3) - [pdf](http://arxiv.org/pdf/1911.00483v3)

> As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical. Classical approaches that assess feature importance (e.g. saliency maps) do not explain how and why a particular region of an image is relevant to the prediction. We propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class. Given a query input to a classifier, our method produces a progressive set of plausible variations of that query, which gradually changes the posterior probability from its original class to its negation. These counter-factually generated samples preserve features unrelated to the classification decision, such that a user can employ our method as a "tuning knob" to traverse a data manifold while crossing the decision boundary. Our method is model agnostic and only requires the output value and gradient of the predictor with respect to its input.

</details>

<details>

<summary>2020-02-10 20:33:25 - Unsupervised Learning of Audio Perception for Robotics Applications: Learning to Project Data to T-SNE/UMAP space</summary>

- *Prateek Verma, Kenneth Salisbury*

- `2002.04076v1` - [abs](http://arxiv.org/abs/2002.04076v1) - [pdf](http://arxiv.org/pdf/2002.04076v1)

> Audio perception is a key to solving a variety of problems ranging from acoustic scene analysis, music meta-data extraction, recommendation, synthesis and analysis. It can potentially also augment computers in doing tasks that humans do effortlessly in day-to-day activities. This paper builds upon key ideas to build perception of touch sounds without access to any ground-truth data. We show how we can leverage ideas from classical signal processing to get large amounts of data of any sound of interest with a high precision. These sounds are then used, along with the images to map the sounds to a clustered space of the latent representation of these images. This approach, not only allows us to learn semantic representation of the possible sounds of interest, but also allows association of different modalities to the learned distinctions. The model trained to map sounds to this clustered representation, gives reasonable performance as opposed to expensive methods collecting a lot of human annotated data. Such approaches can be used to build a state of art perceptual model for any sound of interest described using a few signal processing features. Daisy chaining high precision sound event detectors using signal processing combined with neural architectures and high dimensional clustering of unlabelled data is a vastly powerful idea, and can be explored in a variety of ways in future.

</details>

<details>

<summary>2020-02-10 23:12:27 - Multimodal Learning For Classroom Activity Detection</summary>

- *Hang Li, Yu Kang, Wenbiao Ding, Song Yang, Songfan Yang, Gale Yan Huang, Zitao Liu*

- `1910.13799v3` - [abs](http://arxiv.org/abs/1910.13799v3) - [pdf](http://arxiv.org/pdf/1910.13799v3)

> Classroom activity detection (CAD) focuses on accurately classifying whether the teacher or student is speaking and recording both the length of individual utterances during a class. A CAD solution helps teachers get instant feedback on their pedagogical instructions. This greatly improves educators' teaching skills and hence leads to students' achievement. However, CAD is very challenging because (1) the CAD model needs to be generalized well enough for different teachers and students; (2) data from both vocal and language modalities has to be wisely fused so that they can be complementary; and (3) the solution shouldn't heavily rely on additional recording device. In this paper, we address the above challenges by using a novel attention based neural framework. Our framework not only extracts both speech and language information, but utilizes attention mechanism to capture long-term semantic dependence. Our framework is device-free and is able to take any classroom recording as input. The proposed CAD learning framework is evaluated in two real-world education applications. The experimental results demonstrate the benefits of our approach on learning attention based neural network from classroom data with different modalities, and show our approach is able to outperform state-of-the-art baselines in terms of various evaluation metrics.

</details>

<details>

<summary>2020-02-11 02:56:56 - Convolutional Prototype Learning for Zero-Shot Recognition</summary>

- *Zhizhe Liu, Xingxing Zhang, Zhenfeng Zhu, Shuai Zheng, Yao Zhao, Jian Cheng*

- `1910.09728v3` - [abs](http://arxiv.org/abs/1910.09728v3) - [pdf](http://arxiv.org/pdf/1910.09728v3)

> Zero-shot learning (ZSL) has received increasing attention in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning. The key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary class attribute vectors. However, the popularly learned projection functions in previous works cannot generalize well since they assume the distribution consistency between seen and unseen domains at sample-level.Besides, the provided non-visual and unique class attributes can significantly degrade the recognition performance in semantic space. In this paper, we propose a simple yet effective convolutional prototype learning (CPL) framework for zero-shot recognition. By assuming distribution consistency at task-level, our CPL is capable of transferring knowledge smoothly to recognize unseen samples.Furthermore, inside each task, discriminative visual prototypes are learned via a distance based training mechanism. Consequently, we can perform recognition in visual space, instead of semantic space. An extensive group of experiments are then carefully designed and presented, demonstrating that CPL obtains more favorable effectiveness, over currently available alternatives under various settings.

</details>

<details>

<summary>2020-02-11 05:49:15 - Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis</summary>

- *Ceyuan Yang, Yujun Shen, Bolei Zhou*

- `1911.09267v3` - [abs](http://arxiv.org/abs/1911.09267v3) - [pdf](http://arxiv.org/pdf/1911.09267v3)

> Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what generative models have learned inside the deep generative representations and how photo-realistic images are able to be composed of the layer-wise stochasticity introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges as variation factors from synthesizing scenes from the generative representations in state-of-the-art GAN models, like StyleGAN and BigGAN. By probing the layer-wise representations with a broad set of semantics at different abstraction levels, we are able to quantify the causality between the activations and semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results further suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation.

</details>

<details>

<summary>2020-02-11 10:05:20 - IntPhys: A Framework and Benchmark for Visual Intuitive Physics Reasoning</summary>

- *Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Véronique Izard, Emmanuel Dupoux*

- `1803.07616v3` - [abs](http://arxiv.org/abs/1803.07616v3) - [pdf](http://arxiv.org/pdf/1803.07616v3)

> In order to reach human performance on complexvisual tasks, artificial systems need to incorporate a sig-nificant amount of understanding of the world in termsof macroscopic objects, movements, forces, etc. Inspiredby work on intuitive physics in infants, we propose anevaluation benchmark which diagnoses how much a givensystem understands about physics by testing whether itcan tell apart well matched videos of possible versusimpossible events constructed with a game engine. Thetest requires systems to compute a physical plausibilityscore over an entire video. It is free of bias and cantest a range of basic physical reasoning concepts. Wethen describe two Deep Neural Networks systems aimedat learning intuitive physics in an unsupervised way,using only physically possible videos. The systems aretrained with a future semantic mask prediction objectiveand tested on the possible versus impossible discrimi-nation task. The analysis of their results compared tohuman data gives novel insights in the potentials andlimitations of next frame prediction architectures.

</details>

<details>

<summary>2020-02-11 10:36:32 - CROWN: Conversational Passage Ranking by Reasoning over Word Networks</summary>

- *Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum*

- `1911.02850v3` - [abs](http://arxiv.org/abs/1911.02850v3) - [pdf](http://arxiv.org/pdf/1911.02850v3)

> Information needs around a topic cannot be satisfied in a single turn; users typically ask follow-up questions referring to the same theme and a system must be capable of understanding the conversational context of a request to retrieve correct answers. In this paper, we present our submission to the TREC Conversational Assistance Track 2019, in which such a conversational setting is explored. We propose a simple unsupervised method for conversational passage ranking by formulating the passage score for a query as a combination of similarity and coherence. To be specific, passages are preferred that contain words semantically similar to the words used in the question, and where such words appear close by. We built a word-proximity network (WPN) from a large corpus, where words are nodes and there is an edge between two nodes if they co-occur in the same passages in a statistically significant way, within a context window. Our approach, named CROWN, improved nDCG scores over a provided Indri baseline on the CAsT training data. On the evaluation data for CAsT, our best run submission achieved above-average performance with respect to AP@5 and nDCG@1000.

</details>

<details>

<summary>2020-02-11 13:58:16 - Firearm Detection and Segmentation Using an Ensemble of Semantic Neural Networks</summary>

- *Alexander Egiazarov, Vasileios Mavroeidis, Fabio Massimo Zennaro, Kamer Vishi*

- `2003.00805v1` - [abs](http://arxiv.org/abs/2003.00805v1) - [pdf](http://arxiv.org/pdf/2003.00805v1)

> In recent years we have seen an upsurge in terror attacks around the world. Such attacks usually happen in public places with large crowds to cause the most damage possible and get the most attention. Even though surveillance cameras are assumed to be a powerful tool, their effect in preventing crime is far from clear due to either limitation in the ability of humans to vigilantly monitor video surveillance or for the simple reason that they are operating passively. In this paper, we present a weapon detection system based on an ensemble of semantic Convolutional Neural Networks that decomposes the problem of detecting and locating a weapon into a set of smaller problems concerned with the individual component parts of a weapon. This approach has computational and practical advantages: a set of simpler neural networks dedicated to specific tasks requires less computational resources and can be trained in parallel; the overall output of the system given by the aggregation of the outputs of individual networks can be tuned by a user to trade-off false positives and false negatives; finally, according to ensemble theory, the output of the overall system will be robust and reliable even in the presence of weak individual models. We evaluated our system running simulations aimed at assessing the accuracy of individual networks and the whole system. The results on synthetic data and real-world data are promising, and they suggest that our approach may have advantages compared to the monolithic approach based on a single deep convolutional neural network.

</details>

<details>

<summary>2020-02-11 15:07:30 - On Parameter Tuning in Meta-learning for Computer Vision</summary>

- *Farid Ghareh Mohammadi, M. Hadi Amini, Hamid R. Arabnia*

- `2003.00837v1` - [abs](http://arxiv.org/abs/2003.00837v1) - [pdf](http://arxiv.org/pdf/2003.00837v1)

> Learning to learn plays a pivotal role in meta-learning (MTL) to obtain an optimal learning model. In this paper, we investigate mage recognition for unseen categories of a given dataset with limited training information. We deploy a zero-shot learning (ZSL) algorithm to achieve this goal. We also explore the effect of parameter tuning on performance of semantic auto-encoder (SAE). We further address the parameter tuning problem for meta-learning, especially focusing on zero-shot learning. By combining different embedded parameters, we improved the accuracy of tuned-SAE. Advantages and disadvantages of parameter tuning and its application in image classification are also explored.

</details>

<details>

<summary>2020-02-11 23:11:58 - Eliminating Search Intent Bias in Learning to Rank</summary>

- *Yingcheng Sun, Richard Kolacinski, Kenneth Loparo*

- `2002.03203v2` - [abs](http://arxiv.org/abs/2002.03203v2) - [pdf](http://arxiv.org/pdf/2002.03203v2)

> Click-through data has proven to be a valuable resource for improving search-ranking quality. Search engines can easily collect click data, but biases introduced in the data can make it difficult to use the data effectively. In order to measure the effects of biases, many click models have been proposed in the literature. However, none of the models can explain the observation that users with different search intent (e.g., informational, navigational, etc.) have different click behaviors. In this paper, we study how differences in user search intent can influence click activities and determined that there exists a bias between user search intent and the relevance of the document relevance. Based on this observation, we propose a search intent bias hypothesis that can be applied to most existing click models to improve their ability to learn unbiased relevance. Experimental results demonstrate that after adopting the search intent hypothesis, click models can better interpret user clicks and substantially improve retrieval performance.

</details>

<details>

<summary>2020-02-12 02:16:11 - Inductive Relation Prediction by Subgraph Reasoning</summary>

- *Komal K. Teru, Etienne Denis, William L. Hamilton*

- `1911.06962v2` - [abs](http://arxiv.org/abs/1911.06962v2) - [pdf](http://arxiv.org/pdf/1911.06962v2)

> The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph, and they are limited to the transductive setting, where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework, GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics. Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can represent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.

</details>

<details>

<summary>2020-02-12 03:41:14 - Using Chinese Glyphs for Named Entity Recognition</summary>

- *Arijit Sehanobish, Chan Hee Song*

- `1909.09922v2` - [abs](http://arxiv.org/abs/1909.09922v2) - [pdf](http://arxiv.org/pdf/1909.09922v2)

> Most Named Entity Recognition (NER) systems use additional features like part-of-speech (POS) tags, shallow parsing, gazetteers, etc. Such kind of information requires external knowledge like unlabeled texts and trained taggers. Adding these features to NER systems have been shown to have a positive impact. However, sometimes creating gazetteers or taggers can take a lot of time and may require extensive data cleaning. In this paper for Chinese NER systems, we do not use these traditional features but we use lexicographic features of Chinese characters. Chinese characters are composed of graphical components called radicals and these components often have some semantic indicators. We propose CNN based models that incorporate this semantic information and use them for NER. Our models show an improvement over the baseline BERT-BiLSTM-CRF model. We set a new baseline score for Chinese OntoNotes v5.0 and show an improvement of +.64 F1 score. We present a state-of-the-art F1 score on Weibo dataset of 71.81 and show a competitive improvement of +0.72 over baseline on ResumeNER dataset.

</details>

<details>

<summary>2020-02-12 06:11:48 - Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference</summary>

- *Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, Tao Jiang*

- `2002.04815v1` - [abs](http://arxiv.org/abs/2002.04815v1) - [pdf](http://arxiv.org/pdf/2002.04815v1)

> Aspect based sentiment analysis aims to identify the sentimental tendency towards a given aspect in text. Fine-tuning of pretrained BERT performs excellent on this task and achieves state-of-the-art performances. Existing BERT-based works only utilize the last output layer of BERT and ignore the semantic knowledge in the intermediate layers. This paper explores the potential of utilizing BERT intermediate layers to enhance the performance of fine-tuning of BERT. To the best of our knowledge, no existing work has been done on this research. To show the generality, we also apply this approach to a natural language inference task. Experimental results demonstrate the effectiveness and generality of the proposed approach.

</details>

<details>

<summary>2020-02-12 10:22:08 - Word Embeddings for Entity-annotated Texts</summary>

- *Satya Almasian, Andreas Spitz, Michael Gertz*

- `1902.02078v3` - [abs](http://arxiv.org/abs/1902.02078v3) - [pdf](http://arxiv.org/pdf/1902.02078v3)

> Learned vector representations of words are useful tools for many information retrieval and natural language processing tasks due to their ability to capture lexical semantics. However, while many such tasks involve or even rely on named entities as central components, popular word embedding models have so far failed to include entities as first-class citizens. While it seems intuitive that annotating named entities in the training corpus should result in more intelligent word features for downstream tasks, performance issues arise when popular embedding approaches are naively applied to entity annotated corpora. Not only are the resulting entity embeddings less useful than expected, but one also finds that the performance of the non-entity word embeddings degrades in comparison to those trained on the raw, unannotated corpus. In this paper, we investigate approaches to jointly train word and entity embeddings on a large corpus with automatically annotated and linked entities. We discuss two distinct approaches to the generation of such embeddings, namely the training of state-of-the-art embeddings on raw-text and annotated versions of the corpus, as well as node embeddings of a co-occurrence graph representation of the annotated corpus. We compare the performance of annotated embeddings and classical word embeddings on a variety of word similarity, analogy, and clustering evaluation tasks, and investigate their performance in entity-specific tasks. Our findings show that it takes more than training popular word embedding models on an annotated corpus to create entity embeddings with acceptable performance on common test cases. Based on these results, we discuss how and when node embeddings of the co-occurrence graph representation of the text can restore the performance.

</details>

<details>

<summary>2020-02-12 10:50:12 - LeanConvNets: Low-cost Yet Effective Convolutional Neural Networks</summary>

- *Jonathan Ephrath, Moshe Eliasof, Lars Ruthotto, Eldad Haber, Eran Treister*

- `1910.13157v2` - [abs](http://arxiv.org/abs/1910.13157v2) - [pdf](http://arxiv.org/pdf/1910.13157v2)

> Convolutional Neural Networks (CNNs) have become indispensable for solving machine learning tasks in speech recognition, computer vision, and other areas that involve high-dimensional data. A CNN filters the input feature using a network containing spatial convolution operators with compactly supported stencils. In practice, the input data and the hidden features consist of a large number of channels, which in most CNNs are fully coupled by the convolution operators. This coupling leads to immense computational cost in the training and prediction phase. In this paper, we introduce LeanConvNets that are derived by sparsifying fully-coupled operators in existing CNNs. Our goal is to improve the efficiency of CNNs by reducing the number of weights, floating point operations and latency times, with minimal loss of accuracy. Our lean convolution operators involve tuning parameters that controls the trade-off between the network's accuracy and computational costs. These convolutions can be used in a wide range of existing networks, and we exemplify their use in residual networks (ResNets). Using a range of benchmark problems from image classification and semantic segmentation, we demonstrate that the resulting LeanConvNet's accuracy is close to state-of-the-art networks while being computationally less expensive. In our tests, the lean versions of ResNet in most cases outperform comparable reduced architectures such as MobileNets and ShuffleNets.

</details>

<details>

<summary>2020-02-12 18:57:14 - Learnable Bernoulli Dropout for Bayesian Deep Learning</summary>

- *Shahin Boluki, Randy Ardywibowo, Siamak Zamani Dadaneh, Mingyuan Zhou, Xiaoning Qian*

- `2002.05155v1` - [abs](http://arxiv.org/abs/2002.05155v1) - [pdf](http://arxiv.org/pdf/2002.05155v1)

> In this work, we propose learnable Bernoulli dropout (LBD), a new model-agnostic dropout scheme that considers the dropout rates as parameters jointly optimized with other model parameters. By probabilistic modeling of Bernoulli dropout, our method enables more robust prediction and uncertainty quantification in deep models. Especially, when combined with variational auto-encoders (VAEs), LBD enables flexible semi-implicit posterior representations, leading to new semi-implicit VAE~(SIVAE) models. We solve the optimization for training with respect to the dropout parameters using Augment-REINFORCE-Merge (ARM), an unbiased and low-variance gradient estimator. Our experiments on a range of tasks show the superior performance of our approach compared with other commonly used dropout schemes. Overall, LBD leads to improved accuracy and uncertainty estimates in image classification and semantic segmentation. Moreover, using SIVAE, we can achieve state-of-the-art performance on collaborative filtering for implicit feedback on several public datasets.

</details>

<details>

<summary>2020-02-12 21:09:15 - Image-to-Image Translation with Text Guidance</summary>

- *Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz*

- `2002.05235v1` - [abs](http://arxiv.org/abs/2002.05235v1) - [pdf](http://arxiv.org/pdf/2002.05235v1)

> The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions.

</details>

<details>

<summary>2020-02-12 23:24:28 - Retrain or not retrain? -- efficient pruning methods of deep CNN networks</summary>

- *Marcin Pietron, Maciej Wielgosz*

- `2002.07051v1` - [abs](http://arxiv.org/abs/2002.07051v1) - [pdf](http://arxiv.org/pdf/2002.07051v1)

> Convolutional neural networks (CNN) play a major role in image processing tasks like image classification, object detection, semantic segmentation. Very often CNN networks have from several to hundred stacked layers with several megabytes of weights. One of the possible methods to reduce complexity and memory footprint is pruning. Pruning is a process of removing weights which connect neurons from two adjacent layers in the network. The process of finding near optimal solution with specified drop in accuracy can be more sophisticated when DL model has higher number of convolutional layers. In the paper few approaches based on retraining and no retraining are described and compared together.

</details>

<details>

<summary>2020-02-13 08:59:31 - Hybrid Neural Tagging Model for Open Relation Extraction</summary>

- *Shengbin Jia, Yang Xiang*

- `1908.01761v3` - [abs](http://arxiv.org/abs/1908.01761v3) - [pdf](http://arxiv.org/pdf/1908.01761v3)

> Open relation extraction (ORE) remains a challenge to obtain a semantic representation by discovering arbitrary relation tuples from the unstructured text. Conventional methods heavily depend on feature engineering or syntactic parsing, they are inefficient or error-cascading. Recently, leveraging supervised deep learning structures to address the ORE task is an extraordinarily promising way. However, there are two main challenges: (1) The lack of enough labeled corpus to support supervised training; (2) The exploration of specific neural architecture that adapts to the characteristics of open relation extracting. In this paper, to overcome these difficulties, we build a large-scale, high-quality training corpus in a fully automated way, and design a tagging scheme to assist in transforming the ORE task into a sequence tagging processing. Furthermore, we propose a hybrid neural network model (HNN4ORT) for open relation tagging. The model employs the Ordered Neurons LSTM to encode potential syntactic information for capturing the associations among the arguments and relations. It also emerges a novel Dual Aware Mechanism, including Local-aware Attention and Global-aware Convolution. The dual aware nesses complement each other so that the model can take the sentence-level semantics as a global perspective, and at the same time implement salient local features to achieve sparse annotation. Experimental results on various testing sets show that our model can achieve state-of-the-art performances compared to the conventional methods or other neural models.

</details>

<details>

<summary>2020-02-13 09:48:31 - Keyphrase Extraction with Span-based Feature Representations</summary>

- *Funan Mu, Zhenting Yu, LiFeng Wang, Yequan Wang, Qingyu Yin, Yibo Sun, Liqun Liu, Teng Ma, Jing Tang, Xing Zhou*

- `2002.05407v1` - [abs](http://arxiv.org/abs/2002.05407v1) - [pdf](http://arxiv.org/pdf/2002.05407v1)

> Keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. Since keyphrase extraction is able to facilitate the management, categorization, and retrieval of information, it has received much attention in recent years. There are three approaches to address keyphrase extraction: (i) traditional two-step ranking method, (ii) sequence labeling and (iii) generation using neural networks. Two-step ranking approach is based on feature engineering, which is labor intensive and domain dependent. Sequence labeling is not able to tackle overlapping phrases. Generation methods (i.e., Sequence-to-sequence neural network models) overcome those shortcomings, so they have been widely studied and gain state-of-the-art performance. However, generation methods can not utilize context information effectively. In this paper, we propose a novelty Span Keyphrase Extraction model that extracts span-based feature representation of keyphrase directly from all the content tokens. In this way, our model obtains representation for each keyphrase and further learns to capture the interaction between keyphrases in one document to get better ranking results. In addition, with the help of tokens, our model is able to extract overlapped keyphrases. Experimental results on the benchmark datasets show that our proposed model outperforms the existing methods by a large margin.

</details>

<details>

<summary>2020-02-13 09:50:56 - Variational Template Machine for Data-to-Text Generation</summary>

- *Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, Lei Li*

- `2002.01127v2` - [abs](http://arxiv.org/abs/2002.01127v2) - [pdf](http://arxiv.org/pdf/2002.01127v2)

> How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations. Learning such templates is prohibitive since it often requires a large paired <table, description> corpus, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include: a) we carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b)we utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality.

</details>

<details>

<summary>2020-02-13 13:17:25 - End-to-end semantic segmentation of personalized deep brain structures for non-invasive brain stimulation</summary>

- *Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata*

- `2002.05487v1` - [abs](http://arxiv.org/abs/2002.05487v1) - [pdf](http://arxiv.org/pdf/2002.05487v1)

> Electro-stimulation or modulation of deep brain regions is commonly used in clinical procedures for the treatment of several nervous system disorders. In particular, transcranial direct current stimulation (tDCS) is widely used as an affordable clinical application that is applied through electrodes attached to the scalp. However, it is difficult to determine the amount and distribution of the electric field (EF) in the different brain regions due to anatomical complexity and high inter-subject variability. Personalized tDCS is an emerging clinical procedure that is used to tolerate electrode montage for accurate targeting. This procedure is guided by computational head models generated from anatomical images such as MRI. Distribution of the EF in segmented head models can be calculated through simulation studies. Therefore, fast, accurate, and feasible segmentation of different brain structures would lead to a better adjustment for customized tDCS studies. In this study, a single-encoder multi-decoders convolutional neural network is proposed for deep brain segmentation. The proposed architecture is trained to segment seven deep brain structures using T1-weighted MRI. Network generated models are compared with a reference model constructed using a semi-automatic method, and it presents a high matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate (DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution during tDCS in generated and reference models matched well each other, suggesting its potential usefulness in clinical practice.

</details>

<details>

<summary>2020-02-13 17:12:51 - Looking Enhances Listening: Recovering Missing Speech Using Images</summary>

- *Tejas Srinivasan, Ramon Sanabria, Florian Metze*

- `2002.05639v1` - [abs](http://arxiv.org/abs/2002.05639v1) - [pdf](http://arxiv.org/pdf/2002.05639v1)

> Speech is understood better by using visual context; for this reason, there have been many attempts to use images to adapt automatic speech recognition (ASR) systems. Current work, however, has shown that visually adapted ASR models only use images as a regularization signal, while completely ignoring their semantic content. In this paper, we present a set of experiments where we show the utility of the visual modality under noisy conditions. Our results show that multimodal ASR models can recover words which are masked in the input acoustic signal, by grounding its transcriptions using the visual representations. We observe that integrating visual context can result in up to 35% relative improvement in masked word recovery. These results demonstrate that end-to-end multimodal ASR systems can become more robust to noise by leveraging the visual context.

</details>

<details>

<summary>2020-02-13 18:49:29 - A Framework for End-to-End Learning on Semantic Tree-Structured Data</summary>

- *William Woof, Ke Chen*

- `2002.05707v1` - [abs](http://arxiv.org/abs/2002.05707v1) - [pdf](http://arxiv.org/pdf/2002.05707v1)

> While learning models are typically studied for inputs in the form of a fixed dimensional feature vector, real world data is rarely found in this form. In order to meet the basic requirement of traditional learning models, structural data generally have to be converted into fix-length vectors in a handcrafted manner, which is tedious and may even incur information loss. A common form of structured data is what we term "semantic tree-structures", corresponding to data where rich semantic information is encoded in a compositional manner, such as those expressed in JavaScript Object Notation (JSON) and eXtensible Markup Language (XML). For tree-structured data, several learning models have been studied to allow for working directly on raw tree-structure data, However such learning models are limited to either a specific tree-topology or a specific tree-structured data format, e.g., synthetic parse trees. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, such as data expressed in JSON, XML and so on. Motivated by the works in recursive and recurrent neural networks, we develop exemplar neural implementations of our framework for the JSON format. We evaluate our approach on several UCI benchmark datasets, including ablation and data-efficiency studies, and on a toy reinforcement learning task. Experimental results suggest that our framework yields comparable performance to use of standard models with dedicated feature-vectors in general, and even exceeds baseline performance in cases where compositional nature of the data is particularly important.   The source code for a JSON-based implementation of our framework along with experiments can be downloaded at https://github.com/EndingCredits/json2vec.

</details>

<details>

<summary>2020-02-13 21:26:32 - CBIR using features derived by Deep Learning</summary>

- *Subhadip Maji, Smarajit Bose*

- `2002.07877v1` - [abs](http://arxiv.org/abs/2002.07877v1) - [pdf](http://arxiv.org/pdf/2002.07877v1)

> In a Content Based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image, and retrieve images which have similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally the choice of these features play a very important role in the success of this system, and high level features are required to reduce the semantic gap.   In this paper, we propose to use features derived from pre-trained network models from a deep-learning convolution network trained for a large image classification problem. This approach appears to produce vastly superior results for a variety of databases, and it outperforms many contemporary CBIR systems. We analyse the retrieval time of the method, and also propose a pre-clustering of the database based on the above-mentioned features which yields comparable results in a much shorter time in most of the cases.

</details>

<details>

<summary>2020-02-13 23:51:23 - Tracing Information Flows Between Ad Exchanges Using Retargeted Ads</summary>

- *Muhammad Ahmad Bashir, Sajjad Arshad, William Robertson, Christo Wilson*

- `1811.00920v2` - [abs](http://arxiv.org/abs/1811.00920v2) - [pdf](http://arxiv.org/pdf/1811.00920v2)

> Numerous surveys have shown that Web users are concerned about the loss of privacy associated with online tracking. Alarmingly, these surveys also reveal that people are also unaware of the amount of data sharing that occurs between ad exchanges, and thus underestimate the privacy risks associated with online tracking.   In reality, the modern ad ecosystem is fueled by a flow of user data between trackers and ad exchanges. Although recent work has shown that ad exchanges routinely perform cookie matching with other exchanges, these studies are based on brittle heuristics that cannot detect all forms of information sharing, especially under adversarial conditions.   In this study, we develop a methodology that is able to detect client- and server-side flows of information between arbitrary ad exchanges. Our key insight is to leverage retargeted ads as a tool for identifying information flows. Intuitively, our methodology works because it relies on the semantics of how exchanges serve ads, rather than focusing on specific cookie matching mechanisms. Using crawled data on 35,448 ad impressions, we show that our methodology can successfully categorize four different kinds of information sharing behavior between ad exchanges, including cases where existing heuristic methods fail.   We conclude with a discussion of how our findings and methodologies can be leveraged to give users more control over what kind of ads they see and how their information is shared between ad exchanges.

</details>

<details>

<summary>2020-02-14 00:09:42 - GSANet: Semantic Segmentation with Global and Selective Attention</summary>

- *Qingfeng Liu, Mostafa El-Khamy, Dongwoon Bai, Jungwon Lee*

- `2003.00830v1` - [abs](http://arxiv.org/abs/2003.00830v1) - [pdf](http://arxiv.org/pdf/2003.00830v1)

> This paper proposes a novel deep learning architecture for semantic segmentation. The proposed Global and Selective Attention Network (GSANet) features Atrous Spatial Pyramid Pooling (ASPP) with a novel sparsemax global attention and a novel selective attention that deploys a condensation and diffusion mechanism to aggregate the multi-scale contextual information from the extracted deep features. A selective attention decoder is also proposed to process the GSA-ASPP outputs for optimizing the softmax volume. We are the first to benchmark the performance of semantic segmentation networks with the low-complexity feature extraction network (FXN) MobileNetEdge, that is optimized for low latency on edge devices. We show that GSANet can result in more accurate segmentation with MobileNetEdge, as well as with strong FXNs, such as Xception. GSANet improves the state-of-art semantic segmentation accuracy on both the ADE20k and the Cityscapes datasets.

</details>

<details>

<summary>2020-02-14 07:43:06 - Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering</summary>

- *Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong*

- `1911.10470v2` - [abs](http://arxiv.org/abs/1911.10470v2) - [pdf](http://arxiv.org/pdf/1911.10470v2)

> Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.

</details>

<details>

<summary>2020-02-14 07:58:08 - Fast Intent Classification for Spoken Language Understanding</summary>

- *Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, Bill Campbell*

- `1912.01728v2` - [abs](http://arxiv.org/abs/1912.01728v2) - [pdf](http://arxiv.org/pdf/1912.01728v2)

> Spoken Language Understanding (SLU) systems consist of several machine learning components operating together (e.g. intent classification, named entity recognition and resolution). Deep learning models have obtained state of the art results on several of these tasks, largely attributed to their better modeling capacity. However, an increase in modeling capacity comes with added costs of higher latency and energy usage, particularly when operating on low complexity devices. To address the latency and computational complexity issues, we explore a BranchyNet scheme on an intent classification scheme within SLU systems. The BranchyNet scheme when applied to a high complexity model, adds exit points at various stages in the model allowing early decision making for a set of queries to the SLU model. We conduct experiments on the Facebook Semantic Parsing dataset with two candidate model architectures for intent classification. Our experiments show that the BranchyNet scheme provides gains in terms of computational complexity without compromising model accuracy. We also conduct analytical studies regarding the improvements in the computational cost, distribution of utterances that egress from various exit points and the impact of adding more complexity to models with the BranchyNet scheme.

</details>

<details>

<summary>2020-02-14 10:24:42 - A Data Efficient End-To-End Spoken Language Understanding Architecture</summary>

- *Marco Dinarelli, Nikita Kapoor, Bassam Jabaian, Laurent Besacier*

- `2002.05955v1` - [abs](http://arxiv.org/abs/2002.05955v1) - [pdf](http://arxiv.org/pdf/2002.05955v1)

> End-to-end architectures have been recently proposed for spoken language understanding (SLU) and semantic parsing. Based on a large amount of data, those models learn jointly acoustic and linguistic-sequential features. Such architectures give very good results in the context of domain, intent and slot detection, their application in a more complex semantic chunking and tagging task is less easy. For that, in many cases, models are combined with an external language model to enhance their performance.   In this paper we introduce a data efficient system which is trained end-to-end, with no additional, pre-trained external module. One key feature of our approach is an incremental training procedure where acoustic, language and semantic models are trained sequentially one after the other. The proposed model has a reasonable size and achieves competitive results with respect to state-of-the-art while using a small training dataset. In particular, we reach 24.02% Concept Error Rate (CER) on MEDIA/test while training on MEDIA/train without any additional data.

</details>

<details>

<summary>2020-02-14 10:59:51 - Learning from Explanations with Neural Execution Tree</summary>

- *Ziqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, Xiang Ren*

- `1911.01352v3` - [abs](http://arxiv.org/abs/1911.01352v3) - [pdf](http://arxiv.org/pdf/1911.01352v3)

> While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.

</details>

<details>

<summary>2020-02-14 12:38:18 - A Dataset Independent Set of Baselines for Relation Prediction in Argument Mining</summary>

- *Oana Cocarascu, Elena Cabrio, Serena Villata, Francesca Toni*

- `2003.04970v1` - [abs](http://arxiv.org/abs/2003.04970v1) - [pdf](http://arxiv.org/pdf/2003.04970v1)

> Argument Mining is the research area which aims at extracting argument components and predicting argumentative relations (i.e.,support and attack) from text. In particular, numerous approaches have been proposed in the literature to predict the relations holding between the arguments, and application-specific annotated resources were built for this purpose. Despite the fact that these resources have been created to experiment on the same task, the definition of a single relation prediction method to be successfully applied to a significant portion of these datasets is an open research problem in Argument Mining. This means that none of the methods proposed in the literature can be easily ported from one resource to another. In this paper, we address this problem by proposing a set of dataset independent strong neural baselines which obtain homogeneous results on all the datasets proposed in the literature for the argumentative relation prediction task. Thus, our baselines can be employed by the Argument Mining community to compare more effectively how well a method performs on the argumentative relation prediction task.

</details>

<details>

<summary>2020-02-14 13:09:11 - Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems</summary>

- *Natalia Tomashenko, Christian Raymond, Antoine Caubriere, Renato De Mori, Yannick Esteve*

- `2002.06012v1` - [abs](http://arxiv.org/abs/2002.06012v1) - [pdf](http://arxiv.org/pdf/2002.06012v1)

> This work investigates the embeddings for representing dialog history in spoken language understanding (SLU) systems. We focus on the scenario when the semantic information is extracted directly from the speech signal by means of a single end-to-end neural network model. We proposed to integrate dialogue history into an end-to-end signal-to-concept SLU system. The dialog history is represented in the form of dialog history embedding vectors (so-called h-vectors) and is provided as an additional information to end-to-end SLU models in order to improve the system performance. Three following types of h-vectors are proposed and experimentally evaluated in this paper: (1) supervised-all embeddings predicting bag-of-concepts expected in the answer of the user from the last dialog system response; (2) supervised-freq embeddings focusing on predicting only a selected set of semantic concept (corresponding to the most frequent errors in our experiments); and (3) unsupervised embeddings. Experiments on the MEDIA corpus for the semantic slot filling task demonstrate that the proposed h-vectors improve the model performance.

</details>

<details>

<summary>2020-02-14 14:26:30 - Online Similarity Learning with Feedback for Invoice Line Item Matching</summary>

- *Chandresh Kumar Maurya, Neelamadhav Gantayat, Sampath Dechu, Tomas Horvath*

- `2001.00288v2` - [abs](http://arxiv.org/abs/2001.00288v2) - [pdf](http://arxiv.org/pdf/2001.00288v2)

> The procure to pay process (P2P) in large enterprises is a back-end business process which deals with the procurement of products and services for enterprise operations. Procurement is done by issuing purchase orders to impaneled vendors and invoices submitted by vendors are paid after they go through a rigorous validation process. Agents orchestrating P2P process often encounter the problem of matching a product or service descriptions in the invoice to those in purchase order and verify if the ordered items are what have been supplied or serviced. For example, the description in the invoice and purchase order could be TRES 739mL CD KER Smooth and TRES 0.739L CD KER Smth which look different at word level but refer to the same item. In a typical P2P process, agents are asked to manually select the products which are similar before invoices are posted for payment. This step in the business process is manual, repetitive, cumbersome, and costly. Since descriptions are not well-formed sentences, we cannot apply existing semantic and syntactic text similarity approaches directly. In this paper, we present two approaches to solve the above problem using various types of available agent's recorded feedback data. If the agent's feedback is in the form of a relative ranking between descriptions, we use similarity ranking algorithm. If the agent's feedback is absolute such as match or no-match, we use classification similarity algorithm. We also present the threats to the validity of our approach and present a possible remedy making use of product taxonomy and catalog. We showcase the comparative effectiveness and efficiency of the proposed approaches over many benchmarks and real-world data sets.

</details>

<details>

<summary>2020-02-14 16:32:19 - Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base</summary>

- *William W. Cohen, Haitian Sun, R. Alex Hofer, Matthew Siegler*

- `2002.06115v1` - [abs](http://arxiv.org/abs/2002.06115v1) - [pdf](http://arxiv.org/pdf/2002.06115v1)

> We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.

</details>

<details>

<summary>2020-02-14 18:10:14 - Generalization and Representational Limits of Graph Neural Networks</summary>

- *Vikas K. Garg, Stefanie Jegelka, Tommi Jaakkola*

- `2002.06157v1` - [abs](http://arxiv.org/abs/2002.06157v1) - [pdf](http://arxiv.org/pdf/2002.06157v1)

> We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.

</details>

<details>

<summary>2020-02-14 18:38:28 - Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech</summary>

- *David Harwath, Wei-Ning Hsu, James Glass*

- `1911.09602v2` - [abs](http://arxiv.org/abs/1911.09602v2) - [pdf](http://arxiv.org/pdf/1911.09602v2)

> In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3\% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.

</details>

<details>

<summary>2020-02-14 20:02:11 - Semantic Relatedness and Taxonomic Word Embeddings</summary>

- *Magdalena Kacmajor, John D. Kelleher, Filip Klubicka, Alfredo Maldonado*

- `2002.06235v1` - [abs](http://arxiv.org/abs/2002.06235v1) - [pdf](http://arxiv.org/pdf/2002.06235v1)

> This paper connects a series of papers dealing with taxonomic word embeddings. It begins by noting that there are different types of semantic relatedness and that different lexical representations encode different forms of relatedness. A particularly important distinction within semantic relatedness is that of thematic versus taxonomic relatedness. Next, we present a number of experiments that analyse taxonomic embeddings that have been trained on a synthetic corpus that has been generated via a random walk over a taxonomy. These experiments demonstrate how the properties of the synthetic corpus, such as the percentage of rare words, are affected by the shape of the knowledge graph the corpus is generated from. Finally, we explore the interactions between the relative sizes of natural and synthetic corpora on the performance of embeddings when taxonomic and thematic embeddings are combined.

</details>

<details>

<summary>2020-02-15 08:29:25 - Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning</summary>

- *Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee*

- `1910.05396v3` - [abs](http://arxiv.org/abs/1910.05396v3) - [pdf](http://arxiv.org/pdf/1910.05396v3)

> Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.

</details>

<details>

<summary>2020-02-15 10:01:40 - Prediction and optimization of NaV1.7 inhibitors based on machine learning methods</summary>

- *Weikaixin Kong, Xinyu Tu, Zhengwei Xie, Zhuo Huang*

- `1912.05903v2` - [abs](http://arxiv.org/abs/1912.05903v2) - [pdf](http://arxiv.org/pdf/1912.05903v2)

> We used machine learning methods to predict NaV1.7 inhibitors and found the model RF-CDK that performed best on the imbalanced dataset. Using the RF-CDK model for screening drugs, we got effective compounds K1. We use the cell patch clamp method to verify K1. However, because the model evaluation method in this article is not comprehensive enough, there is still a lot of research work to be performed, such as comparison with other existing methods.   The target protein has multiple active sites and requires our further research. We need more detailed models to consider this biological process and compare it with the current results, which is an error in this article.   So we want to withdraw this article.

</details>

<details>

<summary>2020-02-15 11:14:47 - Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral Segmentation of Rock micro-CT Images</summary>

- *Ying Da Wang, Mehdi Shabaninejad, Ryan T. Armstrong, Peyman Mostaghimi*

- `2002.05322v2` - [abs](http://arxiv.org/abs/2002.05322v2) - [pdf](http://arxiv.org/pdf/2002.05322v2)

> Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is essential for further Digital Rock Physics (DRP) analysis, however, conventional methods such as thresholding, watershed segmentation, and converging active contours are susceptible to user-bias. Deep Convolutional Neural Networks (CNNs) have produced accurate pixelwise semantic segmentation results with natural images and $\mu$CT rock images, however, physical accuracy is not well documented. The performance of 4 CNN architectures is tested for 2D and 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon Sandstone are treated as ground truth and used as training and validation data, with a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then used to validate physical accuracy. The topology of each segmented phase is calculated, and the absolute permeability and multiphase flow is modelled with direct simulation in single and mixed wetting cases. These physical measures of connectivity, and flow characteristics show high variance and uncertainty, with models that achieve 95\%+ in voxelwise accuracy possessing permeabilities and connectivities orders of magnitude off. A new network architecture is also introduced as a hybrid fusion of U-net and ResNet, combining short and long skip connections in a Network-in-Network configuration. The 3D implementation outperforms all other tested models in voxelwise and physical accuracy measures. The network architecture and the volume fraction in the dataset (and associated weighting), are factors that not only influence the accuracy trade-off in the voxelwise case, but is especially important in training a physically accurate model for segmentation.

</details>

<details>

<summary>2020-02-15 15:06:06 - Recommendation of Move Method Refactoring Using Path-Based Representation of Code</summary>

- *Zarina Kurbatova, Ivan Veselov, Yaroslav Golubev, Timofey Bryksin*

- `2002.06392v1` - [abs](http://arxiv.org/abs/2002.06392v1) - [pdf](http://arxiv.org/pdf/2002.06392v1)

> Software refactoring plays an important role in increasing code quality. One of the most popular refactoring types is the Move Method refactoring. It is usually applied when a method depends more on members of other classes than on its own original class. Several approaches have been proposed to recommend Move Method refactoring automatically. Most of them are based on heuristics and have certain limitations (e.g., they depend on the selection of metrics and manually-defined thresholds). In this paper, we propose an approach to recommend Move Method refactoring based on a path-based representation of code called code2vec that is able to capture the syntactic structure and semantic information of a code fragment. We use this code representation to train a machine learning classifier suggesting to move methods to more appropriate classes. We evaluate the approach on two publicly available datasets: a manually compiled dataset of well-known open-source projects and a synthetic dataset with automatically injected code smell instances. The results show that our approach is capable of recommending accurate refactoring opportunities and outperforms JDeodorant and JMove, which are state of the art tools in this field.

</details>

<details>

<summary>2020-02-15 19:03:36 - Undersensitivity in Neural Reading Comprehension</summary>

- *Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp, Sebastian Riedel*

- `2003.04808v1` - [abs](http://arxiv.org/abs/2003.04808v1) - [pdf](http://arxiv.org/pdf/2003.04808v1)

> Current reading comprehension models generalise well to in-distribution test sets, yet perform poorly on adversarially selected inputs. Most prior work on adversarial inputs studies oversensitivity: semantically invariant text perturbations that cause a model's prediction to change when it should not. In this work we focus on the complementary problem: excessive prediction undersensitivity, where input text is meaningfully changed but the model's prediction does not, even though it should. We formulate a noisy adversarial attack which searches among semantic variations of the question for which a model erroneously predicts the same answer, and with even higher probability. Despite comprising unanswerable questions, both SQuAD2.0 and NewsQA models are vulnerable to this attack. This indicates that although accurate, models tend to rely on spurious patterns and do not fully consider the information specified in a question. We experiment with data augmentation and adversarial training as defences, and find that both substantially decrease vulnerability to attacks on held out data, as well as held out attack spaces. Addressing undersensitivity also improves results on AddSent and AddOneSent, and models furthermore generalise better when facing train/evaluation distribution mismatch: they are less prone to overly rely on predictive cues present only in the training set, and outperform a conventional model by as much as 10.9% F1.

</details>

<details>

<summary>2020-02-16 04:38:41 - Semantic Discord: Finding Unusual Local Patterns for Time Series</summary>

- *Li Zhang, Yifeng Gao, Jessica Lin*

- `2001.11842v2` - [abs](http://arxiv.org/abs/2001.11842v2) - [pdf](http://arxiv.org/pdf/2001.11842v2)

> Finding anomalous subsequence in a long time series is a very important but difficult problem. Existing state-of-the-art methods have been focusing on searching for the subsequence that is the most dissimilar to the rest of the subsequences; however, they do not take into account the background patterns that contain the anomalous candidates. As a result, such approaches are likely to miss local anomalies. We introduce a new definition named \textit{semantic discord}, which incorporates the context information from larger subsequences containing the anomaly candidates. We propose an efficient algorithm with a derived lower bound that is up to 3 orders of magnitude faster than the brute force algorithm in real world data. We demonstrate that our method significantly outperforms the state-of-the-art methods in locating anomalies by extensive experiments. We further explain the interpretability of semantic discord.

</details>

<details>

<summary>2020-02-16 09:22:32 - Exploring Neural Models for Parsing Natural Language into First-Order Logic</summary>

- *Hrituraj Singh, Milan Aggrawal, Balaji Krishnamurthy*

- `2002.06544v1` - [abs](http://arxiv.org/abs/2002.06544v1) - [pdf](http://arxiv.org/pdf/2002.06544v1)

> Semantic parsing is the task of obtaining machine-interpretable representations from natural language text. We consider one such formal representation - First-Order Logic (FOL) and explore the capability of neural models in parsing English sentences to FOL. We model FOL parsing as a sequence to sequence mapping task where given a natural language sentence, it is encoded into an intermediate representation using an LSTM followed by a decoder which sequentially generates the predicates in the corresponding FOL formula. We improve the standard encoder-decoder model by introducing a variable alignment mechanism that enables it to align variables across predicates in the predicted FOL. We further show the effectiveness of predicting the category of FOL entity - Unary, Binary, Variables and Scoped Entities, at each decoder step as an auxiliary task on improving the consistency of generated FOL. We perform rigorous evaluations and extensive ablations. We also aim to release our code as well as large scale FOL dataset along with models to aid further research in logic-based parsing and inference in NLP.

</details>

<details>

<summary>2020-02-16 10:29:16 - Breast Cancer Histopathology Image Classification and Localization using Multiple Instance Learning</summary>

- *Abhijeet Patil, Dipesh Tamboli, Swati Meena, Deepak Anand, Amit Sethi*

- `2003.00823v1` - [abs](http://arxiv.org/abs/2003.00823v1) - [pdf](http://arxiv.org/pdf/2003.00823v1)

> Breast cancer has the highest mortality among cancers in women. Computer-aided pathology to analyze microscopic histopathology images for diagnosis with an increasing number of breast cancer patients can bring the cost and delays of diagnosis down. Deep learning in histopathology has attracted attention over the last decade of achieving state-of-the-art performance in classification and localization tasks. The convolutional neural network, a deep learning framework, provides remarkable results in tissue images analysis, but lacks in providing interpretation and reasoning behind the decisions. We aim to provide a better interpretation of classification results by providing localization on microscopic histopathology images. We frame the image classification problem as weakly supervised multiple instance learning problem where an image is collection of patches i.e. instances. Attention-based multiple instance learning (A-MIL) learns attention on the patches from the image to localize the malignant and normal regions in an image and use them to classify the image. We present classification and localization results on two publicly available BreakHIS and BACH dataset. The classification and visualization results are compared with other recent techniques. The proposed method achieves better localization results without compromising classification accuracy.

</details>

<details>

<summary>2020-02-16 17:42:37 - Block Annotation: Better Image Annotation for Semantic Segmentation with Sub-Image Decomposition</summary>

- *Hubert Lin, Paul Upchurch, Kavita Bala*

- `2002.06626v1` - [abs](http://arxiv.org/abs/2002.06626v1) - [pdf](http://arxiv.org/pdf/2002.06626v1)

> Image datasets with high-quality pixel-level annotations are valuable for semantic segmentation: labelling every pixel in an image ensures that rare classes and small objects are annotated. However, full-image annotations are expensive, with experts spending up to 90 minutes per image. We propose block sub-image annotation as a replacement for full-image annotation. Despite the attention cost of frequent task switching, we find that block annotations can be crowdsourced at higher quality compared to full-image annotation with equal monetary cost using existing annotation tools developed for full-image annotation. Surprisingly, we find that 50% pixels annotated with blocks allows semantic segmentation to achieve equivalent performance to 100% pixels annotated. Furthermore, as little as 12% of pixels annotated allows performance as high as 98% of the performance with dense annotation. In weakly-supervised settings, block annotation outperforms existing methods by 3-4% (absolute) given equivalent annotation time. To recover the necessary global structure for applications such as characterizing spatial context and affordance relationships, we propose an effective method to inpaint block-annotated images with high-quality labels without additional human effort. As such, fewer annotations can also be used for these applications compared to full-image annotation.

</details>

<details>

<summary>2020-02-16 18:17:51 - fastai: A Layered API for Deep Learning</summary>

- *Jeremy Howard, Sylvain Gugger*

- `2002.04688v2` - [abs](http://arxiv.org/abs/2002.04688v2) - [pdf](http://arxiv.org/pdf/2002.04688v2)

> fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at http://dev.fast.ai/

</details>

<details>

<summary>2020-02-16 21:11:03 - Video Question Generation via Cross-Modal Self-Attention Networks Learning</summary>

- *Yu-Siang Wang, Hung-Ting Su, Chen-Hsi Chang, Zhe-Yu Liu, Winston H. Hsu*

- `1907.03049v3` - [abs](http://arxiv.org/abs/1907.03049v3) - [pdf](http://arxiv.org/pdf/1907.03049v3)

> We introduce a novel task, Video Question Generation (Video QG). A Video QG model automatically generates questions given a video clip and its corresponding dialogues. Video QG requires a range of skills -- sentence comprehension, temporal relation, the interplay between vision and language, and the ability to ask meaningful questions. To address this, we propose a novel semantic rich cross-modal self-attention (SRCMSA) network to aggregate the multi-modal and diverse features. To be more precise, we enhance the video frames semantic by integrating the object-level information, and we jointly consider the cross-modal attention for the video question generation task. Excitingly, our proposed model remarkably improves the baseline from 7.58 to 14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a novel path toward understanding the challenging video input and we provide detailed analysis in terms of diversity, which ushers the avenues for future investigations.

</details>

<details>

<summary>2020-02-16 23:03:32 - Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO Framework</summary>

- *Chiranjib Sur*

- `2002.06701v1` - [abs](http://arxiv.org/abs/2002.06701v1) - [pdf](http://arxiv.org/pdf/2002.06701v1)

> In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF) for Better Semantic Selection for Indian regional language-based image captioning and introduced a procedure where we used the existing translation and English crowd-sourced sentences for training. We have shown that this architecture is a promising alternative source, where there is a crunch in resources. Our main contribution of this work is the development of deep learning architectures for the Bengali language (is the fifth widely spoken language in the world) with a completely different grammar and language attributes. We have shown that these are working well for complex applications like language generation from image contexts and can diversify the representation through introducing constraints, more extensive features, and unique feature spaces. We also established that we could achieve absolute precision and diversity when we use smoothened semantic tensor with the traditional LSTM and feature decomposition networks. With better learning architecture, we succeeded in establishing an automated algorithm and assessment procedure that can help in the evaluation of competent applications without the requirement for expertise and human intervention.

</details>

<details>

<summary>2020-02-17 18:23:56 - Handling Missing Annotations in Supervised Learning Data</summary>

- *Alaa E. Abdel-Hakim, Wael Deabes*

- `2002.07113v1` - [abs](http://arxiv.org/abs/2002.07113v1) - [pdf](http://arxiv.org/pdf/2002.07113v1)

> Data annotation is an essential stage in supervised learning. However, the annotation process is exhaustive and time consuming, specially for large datasets. Activities of Daily Living (ADL) recognition is an example of systems that exploit very large raw sensor data readings. In such systems, sensor readings are collected from activity-monitoring sensors in a 24/7 manner. The size of the generated dataset is so huge that it is almost impossible for a human annotator to give a certain label to every single instance in the dataset. This results in annotation gaps in the input data to the adopting supervised learning system. The performance of the recognition system is negatively affected by these gaps. In this work, we propose and investigate three different paradigms to handle these gaps. In the first paradigm, the gaps are taken out by dropping all unlabeled readings. A single "Unknown" or "Do-Nothing" label is given to the unlabeled readings within the operation of the second paradigm. The last paradigm handles these gaps by giving every one of them a unique label identifying the encapsulating deterministic labels. Also, we propose a semantic preprocessing method of annotation gaps by constructing a hybrid combination of some of these paradigms for further performance improvement. The performance of the proposed three paradigms and their hybrid combination is evaluated using an ADL benchmark dataset containing more than $2.5\times 10^6$ sensor readings that had been collected over more than nine months. The evaluation results emphasize the performance contrast under the operation of each paradigm and support a specific gap handling approach for better performance.

</details>

<details>

<summary>2020-02-18 00:39:49 - TensorShield: Tensor-based Defense Against Adversarial Attacks on Images</summary>

- *Negin Entezari, Evangelos E. Papalexakis*

- `2002.10252v1` - [abs](http://arxiv.org/abs/2002.10252v1) - [pdf](http://arxiv.org/pdf/2002.10252v1)

> Recent studies have demonstrated that machine learning approaches like deep neural networks (DNNs) are easily fooled by adversarial attacks. Subtle and imperceptible perturbations of the data are able to change the result of deep neural networks. Leveraging vulnerable machine learning methods raises many concerns especially in domains where security is an important factor. Therefore, it is crucial to design defense mechanisms against adversarial attacks. For the task of image classification, unnoticeable perturbations mostly occur in the high-frequency spectrum of the image. In this paper, we utilize tensor decomposition techniques as a preprocessing step to find a low-rank approximation of images which can significantly discard high-frequency perturbations. Recently a defense framework called Shield could "vaccinate" Convolutional Neural Networks (CNN) against adversarial examples by performing random-quality JPEG compressions on local patches of images on the ImageNet dataset. Our tensor-based defense mechanism outperforms the SLQ method from Shield by 14% against FastGradient Descent (FGSM) adversarial attacks, while maintaining comparable speed.

</details>

<details>

<summary>2020-02-18 00:50:26 - On the Matrix-Free Generation of Adversarial Perturbations for Black-Box Attacks</summary>

- *Hisaichi Shibata, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu Abe*

- `2002.07317v1` - [abs](http://arxiv.org/abs/2002.07317v1) - [pdf](http://arxiv.org/pdf/2002.07317v1)

> In general, adversarial perturbations superimposed on inputs are realistic threats for a deep neural network (DNN). In this paper, we propose a practical generation method of such adversarial perturbation to be applied to black-box attacks that demand access to an input-output relationship only. Thus, the attackers generate such perturbation without invoking inner functions and/or accessing the inner states of a DNN. Unlike the earlier studies, the algorithm to generate the perturbation presented in this study requires much fewer query trials. Moreover, to show the effectiveness of the adversarial perturbation extracted, we experiment with a DNN for semantic segmentation. The result shows that the network is easily deceived with the perturbation generated than using uniformly distributed random noise with the same magnitude.

</details>

<details>

<summary>2020-02-18 04:22:00 - Adaptive Cross-Modal Few-Shot Learning</summary>

- *Chen Xing, Negar Rostamzadeh, Boris N. Oreshkin, Pedro O. Pinheiro*

- `1902.07104v3` - [abs](http://arxiv.org/abs/1902.07104v3) - [pdf](http://arxiv.org/pdf/1902.07104v3)

> Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small.

</details>

<details>

<summary>2020-02-18 06:59:13 - Deflecting Adversarial Attacks</summary>

- *Yao Qin, Nicholas Frosst, Colin Raffel, Garrison Cottrell, Geoffrey Hinton*

- `2002.07405v1` - [abs](http://arxiv.org/abs/2002.07405v1) - [pdf](http://arxiv.org/pdf/2002.07405v1)

> There has been an ongoing cycle where stronger defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach towards ending this cycle where we "deflect'' adversarial attacks by causing the attacker to produce an input that semantically resembles the attack's target class. To this end, we first propose a stronger defense based on Capsule Networks that combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense often perceptually resemble the adversarial target class by performing a human study where participants are asked to label images produced by the attack. These attack images can no longer be called "adversarial'' because our network classifies them the same way as humans do.

</details>

<details>

<summary>2020-02-18 08:07:23 - Assessing the Memory Ability of Recurrent Neural Networks</summary>

- *Cheng Zhang, Qiuchi Li, Lingyu Hua, Dawei Song*

- `2002.07422v1` - [abs](http://arxiv.org/abs/2002.07422v1) - [pdf](http://arxiv.org/pdf/2002.07422v1)

> It is known that Recurrent Neural Networks (RNNs) can remember, in their hidden layers, part of the semantic information expressed by a sequence (e.g., a sentence) that is being processed. Different types of recurrent units have been designed to enable RNNs to remember information over longer time spans. However, the memory abilities of different recurrent units are still theoretically and empirically unclear, thus limiting the development of more effective and explainable RNNs. To tackle the problem, in this paper, we identify and analyze the internal and external factors that affect the memory ability of RNNs, and propose a Semantic Euclidean Space to represent the semantics expressed by a sequence. Based on the Semantic Euclidean Space, a series of evaluation indicators are defined to measure the memory abilities of different recurrent units and analyze their limitations. These evaluation indicators also provide a useful guidance to select suitable sequence lengths for different RNNs during training.

</details>

<details>

<summary>2020-02-18 09:34:59 - Generating random bigraphs with preferential attachment</summary>

- *Dominik Grzelak, Barbara Priwitzer, Uwe Aßmann*

- `2002.07448v1` - [abs](http://arxiv.org/abs/2002.07448v1) - [pdf](http://arxiv.org/pdf/2002.07448v1)

> The bigraph theory is a relatively young, yet formally rigorous, mathematical framework encompassing Robin Milner's previous work on process calculi, on the one hand, and provides a generic meta-model for complex systems such as multi-agent systems, on the other. A bigraph $F = \langle F^P, F^L\rangle$ is a superposition of two independent graph structures comprising a place graph $F^P$ (i.e., a forest) and a link graph $F^L$ (i.e., a hypergraph), sharing the same node set, to express locality and communication of processes independently from each other.   In this paper, we take some preparatory steps towards an algorithm for generating random bigraphs with preferential attachment feature w.r.t. $F^P$ and assortative (disassortative) linkage pattern w.r.t. $F^L$. We employ parameters allowing one to fine-tune the characteristics of the generated bigraph structures. To study the pattern formation properties of our algorithmic model, we analyze several metrics from graph theory based on artificially created bigraphs under different configurations.   Bigraphs provide a quite useful and expressive semantic for process calculi for mobile and global ubiquitous computing. So far, this subject has not received attention in the bigraph-related scientific literature. However, artificial models may be particularly useful for simulation and evaluation of real-world applications in ubiquitous systems necessitating random structures.

</details>

<details>

<summary>2020-02-18 11:11:02 - An Empirical Assessment of Security Risks of Global Android Banking Apps</summary>

- *Sen Chen, Lingling Fan, Guozhu Meng, Ting Su, Minhui Xue, Yinxing Xue, Yang Liu, Lihua Xu*

- `1805.05236v5` - [abs](http://arxiv.org/abs/1805.05236v5) - [pdf](http://arxiv.org/pdf/1805.05236v5)

> Mobile banking apps, belonging to the most security-critical app category, render massive and dynamic transactions susceptible to security risks. Given huge potential financial loss caused by vulnerabilities, existing research lacks a comprehensive empirical study on the security risks of global banking apps to provide useful insights and improve the security of banking apps.   Since data-related weaknesses in banking apps are critical and may directly cause serious financial loss, this paper first revisits the state-of-the-art available tools and finds that they have limited capability in identifying data-related security weaknesses of banking apps. To complement the capability of existing tools in data-related weakness detection, we propose a three-phase automated security risk assessment system, named AUSERA, which leverages static program analysis techniques and sensitive keyword identification. By leveraging AUSERA, we collect 2,157 weaknesses in 693 real-world banking apps across 83 countries, which we use as a basis to conduct a comprehensive empirical study from different aspects, such as global distribution and weakness evolution during version updates. We find that apps owned by subsidiary banks are always less secure than or equivalent to those owned by parent banks. In addition, we also track the patching of weaknesses and receive much positive feedback from banking entities so as to improve the security of banking apps in practice. To date, we highlight that 21 banks have confirmed the weaknesses we reported. We also exchange insights with 7 banks, such as HSBC in UK and OCBC in Singapore, via in-person or online meetings to help them improve their apps. We hope that the insights developed in this paper will inform the communities about the gaps among multiple stakeholders, including banks, academic researchers, and third-party security companies.

</details>

<details>

<summary>2020-02-19 03:42:32 - Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</summary>

- *Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Yubin Wang, Tingwen Liu, Bin Wang, Sujian Li*

- `1909.04273v3` - [abs](http://arxiv.org/abs/1909.04273v3) - [pdf](http://arxiv.org/pdf/1909.04273v3)

> Joint extraction of entities and relations aims to detect entity pairs along with their relations using a single model. Prior work typically solves this task in the extract-then-classify or unified labeling manner. However, these methods either suffer from the redundant entity pairs, or ignore the important inner structure in the process of extracting entities and relations. To address these limitations, in this paper, we first decompose the joint extraction task into two interrelated subtasks, namely HE extraction and TER extraction. The former subtask is to distinguish all head-entities that may be involved with target relations, and the latter is to identify corresponding tail-entities and relations for each extracted head-entity. Next, these two subtasks are further deconstructed into several sequence labeling problems based on our proposed span-based tagging scheme, which are conveniently solved by a hierarchical boundary tagger and a multi-span decoding algorithm. Owing to the reasonable decomposition strategy, our model can fully capture the semantic interdependency between different steps, as well as reduce noise from irrelevant entity pairs. Experimental results show that our method outperforms previous work by 5.2%, 5.9% and 21.5% (F1 score), achieving a new state-of-the-art on three public datasets

</details>

<details>

<summary>2020-02-19 14:28:35 - Ada-LISTA: Learned Solvers Adaptive to Varying Models</summary>

- *Aviad Aberdam, Alona Golts, Michael Elad*

- `2001.08456v2` - [abs](http://arxiv.org/abs/2001.08456v2) - [pdf](http://arxiv.org/pdf/2001.08456v2)

> Neural networks that are based on unfolding of an iterative solver, such as LISTA (learned iterative soft threshold algorithm), are widely used due to their accelerated performance. Nevertheless, as opposed to non-learned solvers, these networks are trained on a certain dictionary, and therefore they are inapplicable for varying model scenarios. This work introduces an adaptive learned solver, termed Ada-LISTA, which receives pairs of signals and their corresponding dictionaries as inputs, and learns a universal architecture to serve them all. We prove that this scheme is guaranteed to solve sparse coding in linear rate for varying models, including dictionary perturbations and permutations. We also provide an extensive numerical study demonstrating its practical adaptation capabilities. Finally, we deploy Ada-LISTA to natural image inpainting, where the patch-masks vary spatially, thus requiring such an adaptation.

</details>

<details>

<summary>2020-02-19 16:40:48 - Relation Embedding for Personalised POI Recommendation</summary>

- *Xianjing Wang, Flora D. Salim, Yongli Ren, Piotr Koniusz*

- `2002.03461v2` - [abs](http://arxiv.org/abs/2002.03461v2) - [pdf](http://arxiv.org/pdf/2002.03461v2)

> Point-of-Interest (POI) recommendation is one of the most important location-based services helping people discover interesting venues or services. However, the extreme user-POI matrix sparsity and the varying spatio-temporal context pose challenges for POI systems, which affects the quality of POI recommendations. To this end, we propose a translation-based relation embedding for POI recommendation. Our approach encodes the temporal and geographic information, as well as semantic contents effectively in a low-dimensional relation space by using Knowledge Graph Embedding techniques. To further alleviate the issue of user-POI matrix sparsity, a combined matrix factorization framework is built on a user-POI graph to enhance the inference of dynamic personal interests by exploiting the side-information. Experiments on two real-world datasets demonstrate the effectiveness of our proposed model.

</details>

<details>

<summary>2020-02-19 19:16:34 - MonoLayout: Amodal scene layout from a single image</summary>

- *Kaustubh Mani, Swapnil Daga, Shubhika Garg, N. Sai Shankar, Krishna Murthy Jatavallabhula, K. Madhava Krishna*

- `2002.08394v1` - [abs](http://arxiv.org/abs/2002.08394v1) - [pdf](http://arxiv.org/pdf/2002.08394v1)

> In this paper, we address the novel, highly challenging problem of estimating the layout of a complex urban driving scenario. Given a single color image captured from a driving platform, we aim to predict the bird's-eye view layout of the road and other traffic participants. The estimated layout should reason beyond what is visible in the image, and compensate for the loss of 3D information due to projection. We dub this problem amodal scene layout estimation, which involves "hallucinating" scene layout for even parts of the world that are occluded in the image. To this end, we present MonoLayout, a deep neural network for real-time amodal scene layout estimation from a single image. We represent scene layout as a multi-channel semantic occupancy grid, and leverage adversarial feature learning to hallucinate plausible completions for occluded image parts. Due to the lack of fair baseline methods, we extend several state-of-the-art approaches for road-layout estimation and vehicle occupancy estimation in bird's-eye view to the amodal setup for rigorous evaluation. By leveraging temporal sensor fusion to generate training labels, we significantly outperform current art over a number of datasets. On the KITTI and Argoverse datasets, we outperform all baselines by a significant margin. We also make all our annotations, and code publicly available. A video abstract of this paper is available https://www.youtube.com/watch?v=HcroGyo6yRQ .

</details>

<details>

<summary>2020-02-19 19:28:11 - A subtractive manufacturing constraint for level set topology optimization</summary>

- *Nigel Morris, Adrian Butscher, Francesco Iorio*

- `2002.10246v1` - [abs](http://arxiv.org/abs/2002.10246v1) - [pdf](http://arxiv.org/pdf/2002.10246v1)

> We present a method for enforcing manufacturability constraints in generated parts such that they will be automatically ready for fabrication using a subtractive approach. We primarily target multi-axis CNC milling approaches but the method should generalize to other subtractive methods as well. To this end, we take as user input: the radius of curvature of the tool bit, a coarse model of the tool head and optionally a set of milling directions. This allows us to enforce the following manufacturability conditions: 1) surface smoothness such that the radius of curvature of the part does not exceed the milling bit radius, 2) orientation such that every part of the surface to be milled is visible from at least one milling direction, 3) accessibility such that every surface patch can be reached by the tool bit without interference with the tool or head mount. We will show how to efficiently enforce the constraint during level set-based topology optimization modifying the advection velocity such that at each iteration the topology optimization maintains a descent optimization direction and does not violate any of the manufacturability conditions. This approach models the actual subtractive process by carving away material accessible to the machine at each iteration until a local optimum is achieved.

</details>

<details>

<summary>2020-02-19 21:12:49 - SD-GAN: Structural and Denoising GAN reveals facial parts under occlusion</summary>

- *Samik Banerjee, Sukhendu Das*

- `2002.08448v1` - [abs](http://arxiv.org/abs/2002.08448v1) - [pdf](http://arxiv.org/pdf/2002.08448v1)

> Certain facial parts are salient (unique) in appearance, which substantially contribute to the holistic recognition of a subject. Occlusion of these salient parts deteriorates the performance of face recognition algorithms. In this paper, we propose a generative model to reconstruct the missing parts of the face which are under occlusion. The proposed generative model (SD-GAN) reconstructs a face preserving the illumination variation and identity of the face. A novel adversarial training algorithm has been designed for a bimodal mutually exclusive Generative Adversarial Network (GAN) model, for faster convergence. A novel adversarial "structural" loss function is also proposed, comprising of two components: a holistic and a local loss, characterized by SSIM and patch-wise MSE. Ablation studies on real and synthetically occluded face datasets reveal that our proposed technique outperforms the competing methods by a considerable margin, even for boosting the performance of Face Recognition.

</details>

<details>

<summary>2020-02-19 21:35:51 - A Recurrent Neural Network Based Patch Recommender for Linux Kernel Bugs</summary>

- *Anusha Bableshwar, Arun Ravindran, Manoj Iyer*

- `2002.08454v1` - [abs](http://arxiv.org/abs/2002.08454v1) - [pdf](http://arxiv.org/pdf/2002.08454v1)

> Software bugs in a production environment have an undesirable impact on quality of service, unplanned system downtime, and disruption in good customer experience, resulting in loss of revenue and reputation. Existing approaches to automated software bug repair focuses on known bug templates detected using static code analysis tools and test suites, and in automatic generation of patch code for these bugs. We describe the typical bug fixing process employed in the Linux kernel, and motivate the need for a new automated tool flow to fix bugs. We present an initial design of such an automated tool that uses Recurrent Neural Network (RNN) based Natural Language Processing to generate patch recommendations from user generated bug reports. At the 50th percentile of the test bugs, the correct patch occurs within the top 11.5 patch recommendations output by the model. Further, we present a Linux kernel developer's assessment of the quality of patches recommended for new unresolved kernel bugs.

</details>

<details>

<summary>2020-02-20 10:18:37 - Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract Syntax Tree</summary>

- *Wenhan Wang, Ge Li, Bo Ma, Xin Xia, Zhi Jin*

- `2002.08653v1` - [abs](http://arxiv.org/abs/2002.08653v1) - [pdf](http://arxiv.org/pdf/2002.08653v1)

> Code clones are semantically similar code fragments pairs that are syntactically similar or different. Detection of code clones can help to reduce the cost of software maintenance and prevent bugs. Numerous approaches of detecting code clones have been proposed previously, but most of them focus on detecting syntactic clones and do not work well on semantic clones with different syntactic features. To detect semantic clones, researchers have tried to adopt deep learning for code clone detection to automatically learn latent semantic features from data. Especially, to leverage grammar information, several approaches used abstract syntax trees (AST) as input and achieved significant progress on code clone benchmarks in various programming languages. However, these AST-based approaches still can not fully leverage the structural information of code fragments, especially semantic information such as control flow and data flow. To leverage control and data flow information, in this paper, we build a graph representation of programs called flow-augmented abstract syntax tree (FA-AST). We construct FA-AST by augmenting original ASTs with explicit control and data flow edges. Then we apply two different types of graph neural networks (GNN) on FA-AST to measure the similarity of code pairs. As far as we have concerned, we are the first to apply graph neural networks on the domain of code clone detection.   We apply our FA-AST and graph neural networks on two Java datasets: Google Code Jam and BigCloneBench. Our approach outperforms the state-of-the-art approaches on both Google Code Jam and BigCloneBench tasks.

</details>

<details>

<summary>2020-02-20 11:18:29 - Semantic Web Environments for Multi-Agent Systems: Enabling agents to use Web of Things via semantic web</summary>

- *Alaa Daoud*

- `2003.02054v1` - [abs](http://arxiv.org/abs/2003.02054v1) - [pdf](http://arxiv.org/pdf/2003.02054v1)

> The Web is ubiquitous, increasingly populated with interconnected data, services, people, and objects. Semantic web technologies (SWT) promote uniformity of data formats, as well as modularization and reuse of specifications (e.g., ontologies), by allowing them to include and refer to information provided by other ontologies. In such a context, multi-agent system (MAS) technologies are the right abstraction for developing decentralized and open Web applications in which agents discover, reason and act on Web resources and cooperate with each other and with people. The aim of the project is to propose an approach to transform "Agent and artifact (A&A) meta-model" into a Web-readable format with ontologies in line with semantic web formats and to reuse already existing ontologies in order to provide uniform access for agents to things.

</details>

<details>

<summary>2020-02-20 12:07:44 - Performance Aware Convolutional Neural Network Channel Pruning for Embedded GPUs</summary>

- *Valentin Radu, Kuba Kaszyk, Yuan Wen, Jack Turner, Jose Cano, Elliot J. Crowley, Bjorn Franke, Amos Storkey, Michael O'Boyle*

- `2002.08697v1` - [abs](http://arxiv.org/abs/2002.08697v1) - [pdf](http://arxiv.org/pdf/2002.08697v1)

> Convolutional Neural Networks (CNN) are becoming a common presence in many applications and services, due to their superior recognition accuracy. They are increasingly being used on mobile devices, many times just by porting large models designed for server space, although several model compression techniques have been considered. One model compression technique intended to reduce computations is channel pruning. Mobile and embedded systems now have GPUs which are ideal for the parallel computations of neural networks and for their lower energy cost per operation. Specialized libraries perform these neural network computations through highly optimized routines. As we find in our experiments, these libraries are optimized for the most common network shapes, making uninstructed channel pruning inefficient. We evaluate higher level libraries, which analyze the input characteristics of a convolutional layer, based on which they produce optimized OpenCL (Arm Compute Library and TVM) and CUDA (cuDNN) code. However, in reality, these characteristics and subsequent choices intended for optimization can have the opposite effect. We show that a reduction in the number of convolutional channels, pruning 12% of the initial size, is in some cases detrimental to performance, leading to 2x slowdown. On the other hand, we also find examples where performance-aware pruning achieves the intended results, with performance speedups of 3x with cuDNN and above 10x with Arm Compute Library and TVM. Our findings expose the need for hardware-instructed neural network pruning.

</details>

<details>

<summary>2020-02-20 15:33:26 - Founded (Auto)Epistemic Equilibrium Logic Satisfies Epistemic Splitting</summary>

- *Jorge Fandinno*

- `1907.09247v2` - [abs](http://arxiv.org/abs/1907.09247v2) - [pdf](http://arxiv.org/pdf/1907.09247v2)

> In a recent line of research, two familiar concepts from logic programming semantics (unfounded sets and splitting) were extrapolated to the case of epistemic logic programs. The property of epistemic splitting provides a natural and modular way to understand programs without epistemic cycles but, surprisingly, was only fulfilled by Gelfond's original semantics (G91), among the many proposals in the literature. On the other hand, G91 may suffer from a kind of self-supported, unfounded derivations when epistemic cycles come into play. Recently, the absence of these derivations was also formalised as a property of epistemic semantics called foundedness. Moreover, a first semantics proved to satisfy foundedness was also proposed, the so-called Founded Autoepistemic Equilibrium Logic (FAEEL). In this paper, we prove that FAEEL also satisfies the epistemic splitting property something that, together with foundedness, was not fulfilled by any other approach up to date. To prove this result, we provide an alternative characterisation of FAEEL as a combination of G91 with a simpler logic we called Founded Epistemic Equilibrium Logic (FEEL), which is somehow an extrapolation of the stable model semantics to the modal logic S5. Under consideration for acceptance in TPLP.

</details>

<details>

<summary>2020-02-20 16:25:47 - Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach</summary>

- *Pierre Courtiol, Eric W. Tramel, Marc Sanselme, Gilles Wainrib*

- `1802.02212v2` - [abs](http://arxiv.org/abs/1802.02212v2) - [pdf](http://arxiv.org/pdf/1802.02212v2)

> Analysis of histopathology slides is a critical step for many diagnoses, and in particular in oncology where it defines the gold standard. In the case of digital histopathological analysis, highly trained pathologists must review vast whole-slide-images of extreme digital resolution ($100,000^2$ pixels) across multiple zoom levels in order to locate abnormal regions of cells, or in some cases single cells, out of millions. The application of deep learning to this problem is hampered not only by small sample sizes, as typical datasets contain only a few hundred samples, but also by the generation of ground-truth localized annotations for training interpretable classification and segmentation models. We propose a method for disease localization in the context of weakly supervised learning, where only image-level labels are available during training. Even without pixel-level annotations, we are able to demonstrate performance comparable with models trained with strong annotations on the Camelyon-16 lymph node metastases detection challenge. We accomplish this through the use of pre-trained deep convolutional networks, feature embedding, as well as learning via top instances and negative evidence, a multiple instance learning technique from the field of semantic segmentation and object detection.

</details>

<details>

<summary>2020-02-20 16:32:38 - Graph Embedding on Biomedical Networks: Methods, Applications, and Evaluations</summary>

- *Xiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab, Yungui Huang, Simon M. Lin, Wen Zhang, Ping Zhang, Huan Sun*

- `1906.05017v3` - [abs](http://arxiv.org/abs/1906.05017v3) - [pdf](http://arxiv.org/pdf/1906.05017v3)

> Graph embedding learning that aims to automatically learn low-dimensional node representations, has drawn increasing attention in recent years. To date, most recent graph embedding methods are evaluated on social and information networks and are not comprehensively studied on biomedical networks under systematic experiments and analyses. On the other hand, for a variety of biomedical network analysis tasks, traditional techniques such as matrix factorization (which can be seen as a type of graph embedding methods) have shown promising results, and hence there is a need to systematically evaluate the more recent graph embedding methods (e.g. random walk-based and neural network-based) in terms of their usability and potential to further the state-of-the-art.   We select 11 representative graph embedding methods and conduct a systematic comparison on 3 important biomedical link prediction tasks: drug-disease association (DDA) prediction, drug-drug interaction (DDI) prediction, protein-protein interaction (PPI) prediction; and 2 node classification tasks: medical term semantic type classification, protein function prediction. Our experimental results demonstrate that the recent graph embedding methods achieve promising results and deserve more attention in the future biomedical graph analysis. Compared with three state-of-the-art methods for DDAs, DDIs and protein function predictions, the recent graph embedding methods achieve competitive performance without using any biological features and the learned embeddings can be treated as complementary representations for the biological features. By summarizing the experimental results, we provide general guidelines for properly selecting graph embedding methods and setting their hyper-parameters for different biomedical tasks.

</details>

<details>

<summary>2020-02-20 17:57:06 - Deep Multi-Facial Patches Aggregation Network For Facial Expression Recognition</summary>

- *Ahmed Rachid Hazourli, Amine Djeghri, Hanan Salam, Alice Othmani*

- `2002.09298v1` - [abs](http://arxiv.org/abs/2002.09298v1) - [pdf](http://arxiv.org/pdf/2002.09298v1)

> In this paper, we propose an approach for Facial Expressions Recognition (FER) based on a deep multi-facial patches aggregation network. Deep features are learned from facial patches using deep sub-networks and aggregated within one deep architecture for expression classification . Several problems may affect the performance of deep-learning based FER approaches, in particular, the small size of existing FER datasets which might not be sufficient to train large deep learning networks. Moreover, it is extremely time-consuming to collect and annotate a large number of facial images. To account for this, we propose two data augmentation techniques for facial expression generation to expand FER labeled training datasets. We evaluate the proposed framework on three FER datasets. Results show that the proposed approach achieves state-of-art FER deep learning approaches performance when the model is trained and tested on images from the same dataset. Moreover, the proposed data augmentation techniques improve the expression recognition rate, and thus can be a solution for training deep learning FER models using small datasets. The accuracy degrades significantly when testing for dataset bias.

</details>

<details>

<summary>2020-02-20 18:36:05 - Towards Photographic Image Manipulation with Balanced Growing of Generative Autoencoders</summary>

- *Ari Heljakka, Arno Solin, Juho Kannala*

- `1904.06145v2` - [abs](http://arxiv.org/abs/1904.06145v2) - [pdf](http://arxiv.org/pdf/1904.06145v2)

> We present a generative autoencoder that provides fast encoding, faithful reconstructions (eg. retaining the identity of a face), sharp generated/reconstructed samples in high resolutions, and a well-structured latent space that supports semantic manipulation of the inputs. There are no current autoencoder or GAN models that satisfactorily achieve all of these. We build on the progressively growing autoencoder model PIONEER, for which we completely alter the training dynamics based on a careful analysis of recently introduced normalization schemes. We show significantly improved visual and quantitative results for face identity conservation in CelebAHQ. Our model achieves state-of-the-art disentanglement of latent space, both quantitatively and via realistic image attribute manipulations. On the LSUN Bedrooms dataset, we improve the disentanglement performance of the vanilla PIONEER, despite having a simpler model. Overall, our results indicate that the PIONEER networks provide a way towards photorealistic face manipulation.

</details>

<details>

<summary>2020-02-20 21:33:07 - Multi-label Sound Event Retrieval Using a Deep Learning-based Siamese Structure with a Pairwise Presence Matrix</summary>

- *Jianyu Fan, Eric Nichols, Daniel Tompkins, Ana Elisa Mendez Mendez, Benjamin Elizalde, Philippe Pasquier*

- `2002.09026v1` - [abs](http://arxiv.org/abs/2002.09026v1) - [pdf](http://arxiv.org/pdf/2002.09026v1)

> Realistic recordings of soundscapes often have multiple sound events co-occurring, such as car horns, engine and human voices. Sound event retrieval is a type of content-based search aiming at finding audio samples, similar to an audio query based on their acoustic or semantic content. State of the art sound event retrieval models have focused on single-label audio recordings, with only one sound event occurring, rather than on multi-label audio recordings (i.e., multiple sound events occur in one recording). To address this latter problem, we propose different Deep Learning architectures with a Siamese-structure and a Pairwise Presence Matrix. The networks are trained and evaluated using the SONYC-UST dataset containing both single- and multi-label soundscape recordings. The performance results show the effectiveness of our proposed model.

</details>

<details>

<summary>2020-02-20 21:40:15 - Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning</summary>

- *Akanksha Atrey, Kaleigh Clary, David Jensen*

- `1912.05743v2` - [abs](http://arxiv.org/abs/1912.05743v2) - [pdf](http://arxiv.org/pdf/1912.05743v2)

> Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.

</details>

<details>

<summary>2020-02-21 01:47:09 - On the impressive performance of randomly weighted encoders in summarization tasks</summary>

- *Jonathan Pilault, Jaehong Park, Christopher Pal*

- `2002.09084v1` - [abs](http://arxiv.org/abs/2002.09084v1) - [pdf](http://arxiv.org/pdf/2002.09084v1)

> In this work, we investigate the performance of untrained randomly initialized encoders in a general class of sequence to sequence models and compare their performance with that of fully-trained encoders on the task of abstractive summarization. We hypothesize that random projections of an input text have enough representational power to encode the hierarchical structure of sentences and semantics of documents. Using a trained decoder to produce abstractive text summaries, we empirically demonstrate that architectures with untrained randomly initialized encoders perform competitively with respect to the equivalent architectures with fully-trained encoders. We further find that the capacity of the encoder not only improves overall model generalization but also closes the performance gap between untrained randomly initialized and full-trained encoders. To our knowledge, it is the first time that general sequence to sequence models with attention are assessed for trained and randomly projected representations on abstractive summarization.

</details>

<details>

<summary>2020-02-21 12:46:41 - Semantics of negative sequential patterns</summary>

- *Thomas Guyet, Philippe Besnard*

- `2002.06920v2` - [abs](http://arxiv.org/abs/2002.06920v2) - [pdf](http://arxiv.org/pdf/2002.06920v2)

> In the field of pattern mining, a negative sequential pattern is specified by means of a sequence consisting of events to occur and of other events, called negative events, to be absent. For instance, containment of the pattern $\langle a\ \neg b\ c\rangle$ arises with an occurrence of a and a subsequent occurrence of c but no occurrence of b in between. This article is to shed light on the ambiguity of such a seemingly intuitive notation and we identify eight possible semantics for the containment relation between a pattern and a sequence. These semantics are illustrated and formally studied, in particular we propose dominance and equivalence relations between them. Also we prove that support is anti-monotonic for some of these semantics. Some of the results are discussed with the aim of developing algorithms to extract efficiently frequent negative patterns.

</details>

<details>

<summary>2020-02-21 19:33:30 - Introducing Fuzzy Layers for Deep Learning</summary>

- *Stanton R. Price, Steven R. Price, Derek T. Anderson*

- `2003.00880v1` - [abs](http://arxiv.org/abs/2003.00880v1) - [pdf](http://arxiv.org/pdf/2003.00880v1)

> Many state-of-the-art technologies developed in recent years have been influenced by machine learning to some extent. Most popular at the time of this writing are artificial intelligence methodologies that fall under the umbrella of deep learning. Deep learning has been shown across many applications to be extremely powerful and capable of handling problems that possess great complexity and difficulty. In this work, we introduce a new layer to deep learning: the fuzzy layer. Traditionally, the network architecture of neural networks is composed of an input layer, some combination of hidden layers, and an output layer. We propose the introduction of fuzzy layers into the deep learning architecture to exploit the powerful aggregation properties expressed through fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals. To date, fuzzy approaches taken to deep learning have been through the application of various fusion strategies at the decision level to aggregate outputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16, GoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown to improve accuracy performance for image classification tasks, none have explored the use of fuzzified intermediate, or hidden, layers. Herein, we present a new deep learning strategy that incorporates fuzzy strategies into the deep learning architecture focused on the application of semantic segmentation using per-pixel classification. Experiments are conducted on a benchmark data set as well as a data set collected via an unmanned aerial system at a U.S. Army test site for the task of automatic road segmentation, and preliminary results are promising.

</details>

<details>

<summary>2020-02-22 02:09:21 - A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation</summary>

- *Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H. Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W. Lee, Deog-Kyoon Jeong*

- `2002.10941v1` - [abs](http://arxiv.org/abs/2002.10941v1) - [pdf](http://arxiv.org/pdf/2002.10941v1)

> With the increasing computational demands of neural networks, many hardware accelerators for the neural networks have been proposed. Such existing neural network accelerators often focus on popular neural network types such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs); however, not much attention has been paid to attention mechanisms, an emerging neural network primitive that enables neural networks to retrieve most relevant information from a knowledge-base, external memory, or past states. The attention mechanism is widely adopted by many state-of-the-art neural networks for computer vision, natural language processing, and machine translation, and accounts for a large portion of total execution time. We observe today's practice of implementing this mechanism using matrix-vector multiplication is suboptimal as the attention mechanism is semantically a content-based search where a large portion of computations ends up not being used. Based on this observation, we design and architect A3, which accelerates attention mechanisms in neural networks with algorithmic approximation and hardware specialization. Our proposed accelerator achieves multiple orders of magnitude improvement in energy efficiency (performance/watt) as well as substantial speedup over the state-of-the-art conventional hardware.

</details>

<details>

<summary>2020-02-22 04:27:04 - Notes on neighborhood semantics for logics of unknown truths and false beliefs</summary>

- *Jie Fan*

- `2002.09622v1` - [abs](http://arxiv.org/abs/2002.09622v1) - [pdf](http://arxiv.org/pdf/2002.09622v1)

> In this article, we study logics of unknown truths and false beliefs under neighborhood semantics. We compare the relative expressivity of the two logics. It turns out that they are incomparable over various classes of neighborhood models, and the combination of the two logics are equally expressive as standard modal logic over any class of neighborhood models. We propose morphisms for each logic, which can help us explore the frame definability problem, show a general soundness and completeness result, and generalize some results in the literature. We axiomatize the two logics over various classes of neighborhood frames. Last but not least, we extend the results to the case of public announcements, which has good applications to Moore sentences and some others.

</details>

<details>

<summary>2020-02-22 05:25:07 - An Empirical Study of Android Security Bulletins in Different Vendors</summary>

- *Sadegh Farhang, Mehmet Bahadir Kirdan, Aron Laszka, Jens Grossklags*

- `2002.09629v1` - [abs](http://arxiv.org/abs/2002.09629v1) - [pdf](http://arxiv.org/pdf/2002.09629v1)

> Mobile devices encroach on almost every part of our lives, including work and leisure, and contain a wealth of personal and sensitive information. It is, therefore, imperative that these devices uphold high security standards. A key aspect is the security of the underlying operating system. In particular, Android plays a critical role due to being the most dominant platform in the mobile ecosystem with more than one billion active devices and due to its openness, which allows vendors to adopt and customize it. Similar to other platforms, Android maintains security by providing monthly security patches and announcing them via the Android security bulletin. To absorb this information successfully across the Android ecosystem, impeccable coordination by many different vendors is required.   In this paper, we perform a comprehensive study of 3,171 Android-related vulnerabilities and study to which degree they are reflected in the Android security bulletin, as well as in the security bulletins of three leading vendors: Samsung, LG, and Huawei. In our analysis, we focus on the metadata of these security bulletins (e.g., timing, affected layers, severity, and CWE data) to better understand the similarities and differences among vendors. We find that (i) the studied vendors in the Android ecosystem have adopted different structures for vulnerability reporting, (ii) vendors are less likely to react with delay for CVEs with Android Git repository references, (iii) vendors handle Qualcomm-related CVEs differently from the rest of external layer CVEs.

</details>

<details>

<summary>2020-02-22 10:06:37 - Incorporating Effective Global Information via Adaptive Gate Attention for Text Classification</summary>

- *Xianming Li, Zongxi Li, Yingbin Zhao, Haoran Xie, Qing Li*

- `2002.09673v1` - [abs](http://arxiv.org/abs/2002.09673v1) - [pdf](http://arxiv.org/pdf/2002.09673v1)

> The dominant text classification studies focus on training classifiers using textual instances only or introducing external knowledge (e.g., hand-craft features and domain expert knowledge). In contrast, some corpus-level statistical features, like word frequency and distribution, are not well exploited. Our work shows that such simple statistical information can enhance classification performance both efficiently and significantly compared with several baseline models. In this paper, we propose a classifier with gate mechanism named Adaptive Gate Attention model with Global Information (AGA+GI), in which the adaptive gate mechanism incorporates global statistical features into latent semantic features and the attention layer captures dependency relationship within the sentence. To alleviate the overfitting issue, we propose a novel Leaky Dropout mechanism to improve generalization ability and performance stability. Our experiments show that the proposed method can achieve better accuracy than CNN-based and RNN-based approaches without global information on several benchmarks.

</details>

<details>

<summary>2020-02-22 15:37:28 - An ASP semantics for Constraints involving Conditional Aggregates</summary>

- *Pedro Cabalar, Jorge Fandinno, Torsten Schaub, Philipp Wanko*

- `2002.06911v2` - [abs](http://arxiv.org/abs/2002.06911v2) - [pdf](http://arxiv.org/pdf/2002.06911v2)

> We elaborate upon the formal foundations of hybrid Answer Set Programming (ASP) and extend its underlying logical framework with aggregate functions over constraint values and variables. This is achieved by introducing the construct of conditional expressions, which allow for considering two alternatives while evaluating constraints. Which alternative is considered is interpretation-dependent and chosen according to an associated condition. We put some emphasis on logic programs with linear constraints and show how common ASP aggregates can be regarded as particular cases of so-called conditional linear constraints. Finally, we introduce a polynomial-size, modular and faithful translation from our framework into regular (condition-free) Constraint ASP, outlining an implementation of conditional aggregates on top of existing hybrid ASP solvers.

</details>

<details>

<summary>2020-02-23 03:35:45 - Assembling Semantically-Disentangled Representations for Predictive-Generative Models via Adaptation from Synthetic Domain</summary>

- *Burkay Donderici, Caleb New, Chenliang Xu*

- `2002.09818v1` - [abs](http://arxiv.org/abs/2002.09818v1) - [pdf](http://arxiv.org/pdf/2002.09818v1)

> Deep neural networks can form high-level hierarchical representations of input data. Various researchers have demonstrated that these representations can be used to enable a variety of useful applications. However, such representations are typically based on the statistics within the data, and may not conform with the semantic representation that may be necessitated by the application. Conditional models are typically used to overcome this challenge, but they require large annotated datasets which are difficult to come by and costly to create. In this paper, we show that semantically-aligned representations can be generated instead with the help of a physics based engine. This is accomplished by creating a synthetic dataset with decoupled attributes, learning an encoder for the synthetic dataset, and augmenting prescribed attributes from the synthetic domain with attributes from the real domain. It is shown that the proposed (SYNTH-VAE-GAN) method can construct a conditional predictive-generative model of human face attributes without relying on real data labels.

</details>

<details>

<summary>2020-02-23 12:32:08 - End-To-End Graph-based Deep Semi-Supervised Learning</summary>

- *Zihao Wang, Enmei Tu, Zhou Meng*

- `2002.09891v1` - [abs](http://arxiv.org/abs/2002.09891v1) - [pdf](http://arxiv.org/pdf/2002.09891v1)

> The quality of a graph is determined jointly by three key factors of the graph: nodes, edges and similarity measure (or edge weights), and is very crucial to the success of graph-based semi-supervised learning (SSL) approaches. Recently, dynamic graph, which means part/all its factors are dynamically updated during the training process, has demonstrated to be promising for graph-based semi-supervised learning. However, existing approaches only update part of the three factors and keep the rest manually specified during the learning stage. In this paper, we propose a novel graph-based semi-supervised learning approach to optimize all three factors simultaneously in an end-to-end learning fashion. To this end, we concatenate two neural networks (feature network and similarity network) together to learn the categorical label and semantic similarity, respectively, and train the networks to minimize a unified SSL objective function. We also introduce an extended graph Laplacian regularization term to increase training efficiency. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2020-02-23 23:58:04 - Deep Multimodal Image-Text Embeddings for Automatic Cross-Media Retrieval</summary>

- *Hadi Abdi Khojasteh, Ebrahim Ansari, Parvin Razzaghi, Akbar Karimi*

- `2002.10016v1` - [abs](http://arxiv.org/abs/2002.10016v1) - [pdf](http://arxiv.org/pdf/2002.10016v1)

> This paper considers the task of matching images and sentences by learning a visual-textual embedding space for cross-modal retrieval. Finding such a space is a challenging task since the features and representations of text and image are not comparable. In this work, we introduce an end-to-end deep multimodal convolutional-recurrent network for learning both vision and language representations simultaneously to infer image-text similarity. The model learns which pairs are a match (positive) and which ones are a mismatch (negative) using a hinge-based triplet ranking. To learn about the joint representations, we leverage our newly extracted collection of tweets from Twitter. The main characteristic of our dataset is that the images and tweets are not standardized the same as the benchmarks. Furthermore, there can be a higher semantic correlation between the pictures and tweets contrary to benchmarks in which the descriptions are well-organized. Experimental results on MS-COCO benchmark dataset show that our model outperforms certain methods presented previously and has competitive performance compared to the state-of-the-art. The code and dataset have been made available publicly.

</details>

<details>

<summary>2020-02-24 00:34:44 - Distributed Training of Embeddings using Graph Analytics</summary>

- *Gurbinder Gill, Roshan Dathathri, Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Olli Saarikivi*

- `1909.03359v2` - [abs](http://arxiv.org/abs/1909.03359v2) - [pdf](http://arxiv.org/pdf/1909.03359v2)

> Many applications today, such as NLP, network analysis, and code analysis, rely on semantically embedding objects into low-dimensional fixed-length vectors. Such embeddings naturally provide a way to perform useful downstream tasks, such as identifying relations among objects or predicting objects for a given context, etc. Unfortunately, the training necessary for accurate embeddings is usually computationally intensive and requires processing large amounts of data. Furthermore, distributing this training is challenging. Most embedding training uses stochastic gradient descent (SGD), an "inherently" sequential algorithm. Prior approaches to parallelizing SGD do not honor these dependencies and thus potentially suffer poor convergence.   This paper presents a distributed training framework for a class of applications that use Skip-gram-like models to generate embeddings. We call this class Any2Vec and it includes Word2Vec, DeepWalk, and Node2Vec among others. We first formulate Any2Vec training algorithm as a graph application and leverage the state-of-the-art distributed graph analytics framework, D-Galois. We adapt D-Galois to support dynamic graph generation and repartitioning, and incorporate novel communication optimizations. Finally, we introduce a novel way to combine gradients during distributed training to prevent accuracy loss. We show that our framework, called GraphAny2Vec, matches on a cluster of 32 hosts the accuracy of the state-of-the-art shared-memory implementations of Word2Vec and Vertex2Vec on 1 host, and gives a geo-mean speedup of 12x and 5x respectively. Furthermore, GraphAny2Vec is on average 2x faster than the state-of-the-art distributed Word2Vec implementation, DMTK, on 32 hosts. We also show the superiority of our Gradient Combiner independent of GraphAny2Vec by incorporating it in DMTK, which raises its accuracy by > 30%.

</details>

<details>

<summary>2020-02-24 05:47:08 - Utilizing a null class to restrict decision spaces and defend against neural network adversarial attacks</summary>

- *Matthew J. Roos*

- `2002.10084v1` - [abs](http://arxiv.org/abs/2002.10084v1) - [pdf](http://arxiv.org/pdf/2002.10084v1)

> Despite recent progress, deep neural networks generally continue to be vulnerable to so-called adversarial examples--input images with small perturbations that can result in changes in the output classifications, despite no such change in the semantic meaning to human viewers. This is true even for seemingly simple challenges such as the MNIST digit classification task. In part, this suggests that these networks are not relying on the same set of object features as humans use to make these classifications. In this paper we examine an additional, and largely unexplored, cause behind this phenomenon--namely, the use of the conventional training paradigm in which the entire input space is parcellated among the training classes. Owing to this paradigm, learned decision spaces for individual classes span excessively large regions of the input space and include images that have no semantic similarity to images in the training set. In this study, we train models that include a null class. That is, models may "opt-out" of classifying an input image as one of the digit classes. During training, null images are created through a variety of methods, in an attempt to create tighter and more semantically meaningful decision spaces for the digit classes. The best performing models classify nearly all adversarial examples as nulls, rather than mistaking them as a member of an incorrect digit class, while simultaneously maintaining high accuracy on the unperturbed test set. The use of a null class and the training paradigm presented herein may provide an effective defense against adversarial attacks for some applications. Code for replicating this study will be made available at https://github.com/mattroos/null_class_adversarial_defense .

</details>

<details>

<summary>2020-02-24 07:25:01 - Emosaic: Visualizing Affective Content of Text at Varying Granularity</summary>

- *Philipp Geuder, Marie Claire Leidinger, Martin von Lupin, Marian Dörk, Tobias Schröder*

- `2002.10096v1` - [abs](http://arxiv.org/abs/2002.10096v1) - [pdf](http://arxiv.org/pdf/2002.10096v1)

> This paper presents Emosaic, a tool for visualizing the emotional tone of text documents, considering multiple dimensions of emotion and varying levels of semantic granularity. Emosaic is grounded in psychological research on the relationship between language, affect, and color perception. We capitalize on an established three-dimensional model of human emotion: valence (good, nice vs. bad, awful), arousal (calm, passive vs. exciting, active) and dominance (weak, controlled vs. strong, in control). Previously, multi-dimensional models of emotion have been used rarely in visualizations of textual data, due to the perceptual challenges involved. Furthermore, until recently most text visualizations remained at a high level, precluding closer engagement with the deep semantic content of the text. Informed by empirical studies, we introduce a color mapping that translates any point in three-dimensional affective space into a unique color. Emosaic uses affective dictionaries of words annotated with the three emotional parameters of the valence-arousal-dominance model to extract emotional meanings from texts and then assigns to them corresponding color parameters of the hue-saturation-brightness color space. This approach of mapping emotion to color is aimed at helping readers to more easily grasp the emotional tone of the text. Several features of Emosaic allow readers to interactively explore the affective content of the text in more detail; e.g., in aggregated form as histograms, in sequential form following the order of text, and in detail embedded into the text display itself. Interaction techniques have been included to allow for filtering and navigating of text and visualizations.

</details>

<details>

<summary>2020-02-24 07:47:52 - Text Mining using Nonnegative Matrix Factorization and Latent Semantic Analysis</summary>

- *Ali Hassani, Amir Iranmanesh, Najme Mansouri*

- `1911.04705v3` - [abs](http://arxiv.org/abs/1911.04705v3) - [pdf](http://arxiv.org/pdf/1911.04705v3)

> Text clustering is arguably one of the most important topics in modern data mining. Nevertheless, text data require tokenization which usually yields a very large and highly sparse term-document matrix, which is usually difficult to process using conventional machine learning algorithms. Methods such as Latent Semantic Analysis have helped mitigate this issue, but are nevertheless not completely stable in practice. As a result, we propose a new feature agglomeration method based on Nonnegative Matrix Factorization, which is employed to separate the terms into groups, and then each group's term vectors are agglomerated into a new feature vector. Together, these feature vectors create a new feature space much more suitable for clustering. In addition, we propose a new deterministic initialization for spherical K-Means, which proves very useful for this specific type of data. In order to evaluate the proposed method, we compare it to some of the latest research done in this field, as well as some of the most practiced methods. In our experiments, we conclude that the proposed method either significantly improves clustering performance, or maintains the performance of other methods, while improving stability in results.

</details>

<details>

<summary>2020-02-24 10:11:49 - An Explicit Local and Global Representation Disentanglement Framework with Applications in Deep Clustering and Unsupervised Object Detection</summary>

- *Rujikorn Charakorn, Yuttapong Thawornwattana, Sirawaj Itthipuripat, Nick Pawlowski, Poramate Manoonpong, Nat Dilokthanakul*

- `2001.08957v2` - [abs](http://arxiv.org/abs/2001.08957v2) - [pdf](http://arxiv.org/pdf/2001.08957v2)

> Visual data can be understood at different levels of granularity, where global features correspond to semantic-level information and local features correspond to texture patterns. In this work, we propose a framework, called SPLIT, which allows us to disentangle local and global information into two separate sets of latent variables within the variational autoencoder (VAE) framework. Our framework adds generative assumption to the VAE by requiring a subset of the latent variables to generate an auxiliary set of observable data. This additional generative assumption primes the latent variables to local information and encourages the other latent variables to represent global information. We examine three different flavours of VAEs with different generative assumptions. We show that the framework can effectively disentangle local and global information within these models leads to improved representation, with better clustering and unsupervised object detection benchmarks. Finally, we establish connections between SPLIT and recent research in cognitive neuroscience regarding the disentanglement in human visual perception. The code for our experiments is at https://github.com/51616/split-vae .

</details>

<details>

<summary>2020-02-24 11:52:19 - Deep Multi-Facial patches Aggregation Network for Expression Classification from Face Images</summary>

- *Amine Djerghri, Ahmed Rachid Hazourli, Alice Othmani*

- `1909.10305v2` - [abs](http://arxiv.org/abs/1909.10305v2) - [pdf](http://arxiv.org/pdf/1909.10305v2)

> Emotional Intelligence in Human-Computer Interaction has attracted increasing attention from researchers in multidisciplinary research fields including psychology, computer vision, neuroscience, artificial intelligence, and related disciplines. Human prone to naturally interact with computers face-to-face. Human Expressions is an important key to better link human and computers. Thus, designing interfaces able to understand human expressions and emotions can improve Human-Computer Interaction (HCI) for better communication. In this paper, we investigate HCI via a deep multi-facial patches aggregation network for Face Expression Recognition (FER). Deep features are extracted from facial parts and aggregated for expression classification. Several problems may affect the performance of the proposed framework like the small size of FER datasets and the high number of parameters to learn. For That, two data augmentation techniques are proposed for facial expression generation to expand the labeled training. The proposed framework is evaluated on the extended Cohn-Konade dataset (CK+) and promising results are achieved.

</details>

<details>

<summary>2020-02-24 12:52:10 - Learning to Select Bi-Aspect Information for Document-Scale Text Content Manipulation</summary>

- *Xiaocheng Feng, Yawei Sun, Bing Qin, Heng Gong, Yibo Sun, Wei Bi, Xiaojiang Liu, Ting Liu*

- `2002.10210v1` - [abs](http://arxiv.org/abs/2002.10210v1) - [pdf](http://arxiv.org/pdf/2002.10210v1)

> In this paper, we focus on a new practical task, document-scale text content manipulation, which is the opposite of text style transfer and aims to preserve text styles while altering the content. In detail, the input is a set of structured records and a reference text for describing another recordset. The output is a summary that accurately describes the partial content in the source recordset with the same writing style of the reference. The task is unsupervised due to lack of parallel data, and is challenging to select suitable records and style words from bi-aspect inputs respectively and generate a high-fidelity long document. To tackle those problems, we first build a dataset based on a basketball game report corpus as our testbed, and present an unsupervised neural model with interactive attention mechanism, which is used for learning the semantic relationship between records and reference texts to achieve better content transfer and better style preservation. In addition, we also explore the effectiveness of the back-translation in our task for constructing some pseudo-training pairs. Empirical results show superiority of our approaches over competitive methods, and the models also yield a new state-of-the-art result on a sentence-level dataset.

</details>

<details>

<summary>2020-02-24 14:53:57 - Semantic, Efficient, and Secure Search over Encrypted Cloud Data</summary>

- *Fateh Boucenna*

- `2002.10294v1` - [abs](http://arxiv.org/abs/2002.10294v1) - [pdf](http://arxiv.org/pdf/2002.10294v1)

> Companies and individuals demand more and more storage space and computing power. For this purpose, several new technologies have been designed and implemented, such as the cloud computing. This technology provides its users with storage space and computing power according to their needs in a flexible and personalized way. However, the outsourced data such as emails, electronic health records, and company reports are sensitive and confidential. Therefore, It is primordial to protect the outsourced data against possible external attacks and the cloud server itself. That is why it is highly recommended to encrypt the sensitive data before being outsourced to a remote server. To perform searches over outsourced data, it is no longer possible to exploit traditional search engines given that these data are encrypted. Consequently, lots of searchable encryption (SE) schemes have been proposed in the literature. Three major research axes of searchable encryption area have been studied in the literature. The first axis consists in ensuring the security of the search approach. Indeed, the search process should be performed without decryption any data and without causing any sensitive information leakage. The second axis consists in studying the search performance. In fact, the encrypted indexes are less efficient than the plaintext indexes, which makes the searchable encryption schemes very slow in practice. More the approach is secure, less it is efficient, thus, the challenge consists in finding the best compromise between security and performance. Finally, the third research axis consists in the quality of the returned results in terms of relevance and recall. The problem is that the encryption of the index causes the degradation of the recall and the precision. Therefore, the goal is to propose a technique that is able to obtain almost the same result obtained in the traditional search.

</details>

<details>

<summary>2020-02-24 16:58:00 - Symbolic Learning and Reasoning with Noisy Data for Probabilistic Anchoring</summary>

- *Pedro Zuidberg Dos Martires, Nitesh Kumar, Andreas Persson, Amy Loutfi, Luc De Raedt*

- `2002.10373v1` - [abs](http://arxiv.org/abs/2002.10373v1) - [pdf](http://arxiv.org/pdf/2002.10373v1)

> Robotic agents should be able to learn from sub-symbolic sensor data, and at the same time, be able to reason about objects and communicate with humans on a symbolic level. This raises the question of how to overcome the gap between symbolic and sub-symbolic artificial intelligence. We propose a semantic world modeling approach based on bottom-up object anchoring using an object-centered representation of the world. Perceptual anchoring processes continuous perceptual sensor data and maintains a correspondence to a symbolic representation. We extend the definitions of anchoring to handle multi-modal probability distributions and we couple the resulting symbol anchoring system to a probabilistic logic reasoner for performing inference. Furthermore, we use statistical relational learning to enable the anchoring framework to learn symbolic knowledge in the form of a set of probabilistic logic rules of the world from noisy and sub-symbolic sensor input. The resulting framework, which combines perceptual anchoring and statistical relational learning, is able to maintain a semantic world model of all the objects that have been perceived over time, while still exploiting the expressiveness of logical rules to reason about the state of objects which are not directly observed through sensory input data. To validate our approach we demonstrate, on the one hand, the ability of our system to perform probabilistic reasoning over multi-modal probability distributions, and on the other hand, the learning of probabilistic logical rules from anchored objects produced by perceptual observations. The learned logical rules are, subsequently, used to assess our proposed probabilistic anchoring procedure. We demonstrate our system in a setting involving object interactions where object occlusions arise and where probabilistic inference is needed to correctly anchor objects.

</details>

<details>

<summary>2020-02-25 04:56:47 - Declarative Memory-based Structure for the Representation of Text Data</summary>

- *Sumant Pushp, Pragya Kashmira, Shyamanta M Hazarika*

- `2002.10665v1` - [abs](http://arxiv.org/abs/2002.10665v1) - [pdf](http://arxiv.org/pdf/2002.10665v1)

> In the era of intelligent computing, computational progress in text processing is an essential consideration. Many systems have been developed to process text over different languages. Though, there is considerable development, they still lack in understanding of the text, i.e., instead of keeping text as knowledge, many treat text as a data. In this work we introduce a text representation scheme which is influenced by human memory infrastructure. Since texts are declarative in nature, a structural organization would foster efficient computation over text. We exploit long term episodic memory to keep text information observed over time. This not only keep fragments of text in an organized fashion but also reduces redundancy and stores the temporal relation among them. Wordnet has been used to imitate semantic memory, which works at word level to facilitate the understanding about individual words within text. Experimental results of various operation performed over episodic memory and growth of knowledge infrastructure over time is reported.

</details>

<details>

<summary>2020-02-25 06:41:07 - Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge</summary>

- *Hung Le, Nancy F. Chen*

- `2002.10695v1` - [abs](http://arxiv.org/abs/2002.10695v1) - [pdf](http://arxiv.org/pdf/2002.10695v1)

> Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question Answering (QA) whereby the dialogue agent is required to generate natural language responses to address user queries and carry on conversations. This is a challenging task as it consists of video features of multiple modalities, including text, visual, and audio features. The agent also needs to learn semantic dependencies among user utterances and system responses to make coherent conversations with humans. In this work, we describe our submission to the AVSD track of the 8th Dialogue System Technology Challenge. We adopt dot-product attention to combine text and non-text features of input video. We further enhance the generation capability of the dialogue agent by adopting pointer networks to point to tokens from multiple source sequences in each generation step. Our systems achieve high performance in automatic metrics and obtain 5th and 6th place in human evaluation among all submissions.

</details>

<details>

<summary>2020-02-25 14:44:12 - Dividing the Ontology Alignment Task with Semantic Embeddings and Logic-based Modules</summary>

- *Ernesto Jiménez-Ruiz, Asan Agibetov, Jiaoyan Chen, Matthias Samwald, Valerie Cross*

- `2003.05370v1` - [abs](http://arxiv.org/abs/2003.05370v1) - [pdf](http://arxiv.org/pdf/2003.05370v1)

> Large ontologies still pose serious challenges to state-of-the-art ontology alignment systems. In this paper we present an approach that combines a neural embedding model and logic-based modules to accurately divide an input ontology matching task into smaller and more tractable matching (sub)tasks. We have conducted a comprehensive evaluation using the datasets of the Ontology Alignment Evaluation Initiative. The results are encouraging and suggest that the proposed method is adequate in practice and can be integrated within the workflow of systems unable to cope with very large ontologies.

</details>

<details>

<summary>2020-02-25 16:24:42 - Language-Independent Tokenisation Rivals Language-Specific Tokenisation for Word Similarity Prediction</summary>

- *Danushka Bollegala, Ryuichi Kiryo, Kosuke Tsujino, Haruki Yukawa*

- `2002.11004v1` - [abs](http://arxiv.org/abs/2002.11004v1) - [pdf](http://arxiv.org/pdf/2002.11004v1)

> Language-independent tokenisation (LIT) methods that do not require labelled language resources or lexicons have recently gained popularity because of their applicability in resource-poor languages. Moreover, they compactly represent a language using a fixed size vocabulary and can efficiently handle unseen or rare words. On the other hand, language-specific tokenisation (LST) methods have a long and established history, and are developed using carefully created lexicons and training resources. Unlike subtokens produced by LIT methods, LST methods produce valid morphological subwords. Despite the contrasting trade-offs between LIT vs. LST methods, their performance on downstream NLP tasks remain unclear. In this paper, we empirically compare the two approaches using semantic similarity measurement as an evaluation task across a diverse set of languages. Our experimental results covering eight languages show that LST consistently outperforms LIT when the vocabulary size is large, but LIT can produce comparable or better results than LST in many languages with comparatively smaller (i.e. less than 100K words) vocabulary sizes, encouraging the use of LIT when language-specific resources are unavailable, incomplete or a smaller model is required. Moreover, we find that smoothed inverse frequency (SIF) to be an accurate method to create word embeddings from subword embeddings for multilingual semantic similarity prediction tasks. Further analysis of the nearest neighbours of tokens show that semantically and syntactically related tokens are closely embedded in subword embedding spaces

</details>

<details>

<summary>2020-02-25 16:44:50 - Semantic Relatedness for Keyword Disambiguation: Exploiting Different Embeddings</summary>

- *María G. Buey, Carlos Bobed, Jorge Gracia, Eduardo Mena*

- `2002.11023v1` - [abs](http://arxiv.org/abs/2002.11023v1) - [pdf](http://arxiv.org/pdf/2002.11023v1)

> Understanding the meaning of words is crucial for many tasks that involve human-machine interaction. This has been tackled by research in Word Sense Disambiguation (WSD) in the Natural Language Processing (NLP) field. Recently, WSD and many other NLP tasks have taken advantage of embeddings-based representation of words, sentences, and documents. However, when it comes to WSD, most embeddings models suffer from ambiguity as they do not capture the different possible meanings of the words. Even when they do, the list of possible meanings for a word (sense inventory) has to be known in advance at training time to be included in the embeddings space. Unfortunately, there are situations in which such a sense inventory is not known in advance (e.g., an ontology selected at run-time), or it evolves with time and its status diverges from the one at training time. This hampers the use of embeddings models for WSD. Furthermore, traditional WSD techniques do not perform well in situations in which the available linguistic information is very scarce, such as the case of keyword-based queries. In this paper, we propose an approach to keyword disambiguation which grounds on a semantic relatedness between words and senses provided by an external inventory (ontology) that is not known at training time. Building on previous works, we present a semantic relatedness measure that uses word embeddings, and explore different disambiguation algorithms to also exploit both word and sentence representations. Experimental results show that this approach achieves results comparable with the state of the art when applied for WSD, without training for a particular domain.

</details>

<details>

<summary>2020-02-25 18:22:01 - A Novel Approach to Enhance the Performance of Semantic Search in Bengali using Neural Net and other Classification Techniques</summary>

- *Arijit Das, Diganta Saha*

- `1911.01256v2` - [abs](http://arxiv.org/abs/1911.01256v2) - [pdf](http://arxiv.org/pdf/1911.01256v2)

> Search has for a long time been an important tool for users to retrieve information. Syntactic search is matching documents or objects containing specific keywords like user-history, location, preference etc. to improve the results. However, it is often possible that the query and the best answer have no term or very less number of terms in common and syntactic search can not perform properly in such cases. Semantic search, on the other hand, resolves these issues but suffers from lack of annotation, absence of WordNet in case of low resource languages. In this work, we have demonstrated an end to end procedure to improve the performance of semantic search using semi-supervised and unsupervised learning algorithms. An available Bengali repository was chosen to have seven types of semantic properties primarily to develop the system. Performance has been tested using Support Vector Machine, Naive Bayes, Decision Tree and Artificial Neural Network (ANN). Our system has achieved the efficiency to predict the correct semantics using knowledge base over the time of learning. A repository containing around a million sentences, a product of TDIL project of Govt. of India, was used to test our system at first instance. Then the testing has been done for other languages. Being a cognitive system it may be very useful for improving user satisfaction in e-Governance or m-Governance in the multilingual environment and also for other applications.

</details>

<details>

<summary>2020-02-25 19:07:54 - End-to-End Entity Linking and Disambiguation leveraging Word and Knowledge Graph Embeddings</summary>

- *Rostislav Nedelchev, Debanjan Chaudhuri, Jens Lehmann, Asja Fischer*

- `2002.11143v1` - [abs](http://arxiv.org/abs/2002.11143v1) - [pdf](http://arxiv.org/pdf/2002.11143v1)

> Entity linking - connecting entity mentions in a natural language utterance to knowledge graph (KG) entities is a crucial step for question answering over KGs. It is often based on measuring the string similarity between the entity label and its mention in the question. The relation referred to in the question can help to disambiguate between entities with the same label. This can be misleading if an incorrect relation has been identified in the relation linking step. However, an incorrect relation may still be semantically similar to the relation in which the correct entity forms a triple within the KG; which could be captured by the similarity of their KG embeddings. Based on this idea, we propose the first end-to-end neural network approach that employs KG as well as word embeddings to perform joint relation and entity classification of simple questions while implicitly performing entity disambiguation with the help of a novel gating mechanism. An empirical evaluation shows that the proposed approach achieves a performance comparable to state-of-the-art entity linking while requiring less post-processing.

</details>

<details>

<summary>2020-02-26 03:13:59 - User Review-Based Change File Localization for Mobile Applications</summary>

- *Yu Zhou, Yanqi Su, Taolue Chen, Zhiqiu Huang, Harald Gall, Sebastiano Panichella*

- `1903.00894v3` - [abs](http://arxiv.org/abs/1903.00894v3) - [pdf](http://arxiv.org/pdf/1903.00894v3)

> In the current mobile app development, novel and emerging DevOps practices (e.g., Continuous Delivery, Integration, and user feedback analysis) and tools are becoming more widespread. For instance, the integration of user feedback (provided in the form of user reviews) in the software release cycle represents a valuable asset for the maintenance and evolution of mobile apps. To fully make use of these assets, it is highly desirable for developers to establish semantic links between the user reviews and the software artefacts to be changed (e.g., source code and documentation), and thus to localize the potential files to change for addressing the user feedback. In this paper, we propose RISING (Review Integration via claSsification, clusterIng, and linkiNG), an automated approach to support the continuous integration of user feedback via classification, clustering, and linking of user reviews. RISING leverages domain-specific constraint information and semi-supervised learning to group user reviews into multiple fine-grained clusters concerning similar users' requests. Then, by combining the textual information from both commit messages and source code, it automatically localizes potential change files to accommodate the users' requests. Our empirical studies demonstrate that the proposed approach outperforms the state-of-the-art baseline work in terms of clustering and localization accuracy, and thus produces more reliable results.

</details>

<details>

<summary>2020-02-26 04:58:15 - Semi-Supervised Semantic Image Segmentation with Self-correcting Networks</summary>

- *Mostafa S. Ibrahim, Arash Vahdat, Mani Ranjbar, William G. Macready*

- `1811.07073v3` - [abs](http://arxiv.org/abs/1811.07073v3) - [pdf](http://arxiv.org/pdf/1811.07073v3)

> Building a large image dataset with high-quality object masks for semantic segmentation is costly and time consuming. In this paper, we introduce a principled semi-supervised framework that only uses a small set of fully supervised images (having semantic segmentation labels and box labels) and a set of images with only object bounding box labels (we call it the weak set). Our framework trains the primary segmentation model with the aid of an ancillary model that generates initial segmentation labels for the weak set and a self-correction module that improves the generated labels during training using the increasingly accurate primary model. We introduce two variants of the self-correction module using either linear or convolutional functions. Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models trained with a small fully supervised set perform similar to, or better than, models trained with a large fully supervised set while requiring ~7x less annotation effort.

</details>

<details>

<summary>2020-02-26 08:49:52 - Type-2 Fuzzy Set based Hesitant Fuzzy Linguistic Term Sets for Linguistic Decision Making</summary>

- *Taniya Seth, Pranab K. Muhuri*

- `2002.11714v1` - [abs](http://arxiv.org/abs/2002.11714v1) - [pdf](http://arxiv.org/pdf/2002.11714v1)

> Approaches based on computing with words find good applicability in decision making systems. Predominantly finding their basis in type-1 fuzzy sets, computing with words approaches employ type-1 fuzzy sets as semantics of the linguistic terms. However, type-2 fuzzy sets have been proven to be scientifically more appropriate to represent linguistic information in practical systems. They take into account both the intra-uncertainty as well as the inter-uncertainty in cases where the linguistic information comes from a group of experts. Hence in this paper, we propose to introduce linguistic terms whose semantics are denoted by interval type-2 fuzzy sets within the hesitant fuzzy linguistic term set framework, resulting in type-2 fuzzy sets based hesitant fuzzy linguistic term sets. We also introduce a novel method of computing type-2 fuzzy envelopes out of multiple interval type-2 fuzzy sets with trapezoidal membership functions. Furthermore, the proposed framework with interval type-2 fuzzy sets is applied on a supplier performance evaluation scenario. Since humans are predominantly involved in the entire process of supply chain, their feedback is crucial while deciding many factors. Towards the end of the paper, we compare our presented model with various existing models and demonstrate the advantages of the former.

</details>

<details>

<summary>2020-02-26 12:32:40 - Towards Interpretable Semantic Segmentation via Gradient-weighted Class Activation Mapping</summary>

- *Kira Vinogradova, Alexandr Dibrov, Gene Myers*

- `2002.11434v1` - [abs](http://arxiv.org/abs/2002.11434v1) - [pdf](http://arxiv.org/pdf/2002.11434v1)

> Convolutional neural networks have become state-of-the-art in a wide range of image recognition tasks. The interpretation of their predictions, however, is an active area of research. Whereas various interpretation methods have been suggested for image classification, the interpretation of image segmentation still remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a gradient-based method for interpreting semantic segmentation. Our method is an extension of the widely-used Grad-CAM method, applied locally to produce heatmaps showing the relevance of individual pixels for semantic segmentation.

</details>

<details>

<summary>2020-02-26 15:34:52 - Object Relational Graph with Teacher-Recommended Learning for Video Captioning</summary>

- *Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, Zhengjun Zha*

- `2002.11566v1` - [abs](http://arxiv.org/abs/2002.11566v1) - [pdf](http://arxiv.org/pdf/2002.11566v1)

> Taking full advantage of the information from both vision and language is critical for the video captioning task. Existing models lack adequate visual representation due to the neglect of interaction between object, and sufficient training for content-related words due to long-tailed problems. In this paper, we propose a complete video captioning system including both a novel model and an effective training strategy. Specifically, we propose an object relational graph (ORG) based encoder, which captures more detailed interaction features to enrich visual representation. Meanwhile, we design a teacher-recommended learning (TRL) method to make full use of the successful external language model (ELM) to integrate the abundant linguistic knowledge into the caption model. The ELM generates more semantically similar word proposals which extend the ground-truth words used for training to deal with the long-tailed problem. Experimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the proposed ORG-TRL system achieves state-of-the-art performance. Extensive ablation studies and visualizations illustrate the effectiveness of our system.

</details>

<details>

<summary>2020-02-26 20:47:17 - A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</summary>

- *Duygu Ataman, Wilker Aziz, Alexandra Birch*

- `1910.13890v3` - [abs](http://arxiv.org/abs/1910.13890v3) - [pdf](http://arxiv.org/pdf/1910.13890v3)

> Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires rather deep architectures. In this paper, we propose to translate words by modeling word formation through a hierarchical latent variable model which mimics the process of morphological inflection. Our model generates words one character at a time by composing two latent representations: a continuous one, aimed at capturing the lexical semantics, and a set of (approximately) discrete features, aimed at capturing the morphosyntactic function, which are shared among different surface forms. Our model achieves better accuracy in translation into three morphologically-rich languages than conventional open-vocabulary NMT methods, while also demonstrating a better generalization capacity under low to mid-resource settings.

</details>

<details>

<summary>2020-02-27 12:44:54 - Chargrid-OCR: End-to-end Trainable Optical Character Recognition for Printed Documents using Instance Segmentation</summary>

- *Christian Reisswig, Anoop R Katti, Marco Spinaci, Johannes Höhne*

- `1909.04469v4` - [abs](http://arxiv.org/abs/1909.04469v4) - [pdf](http://arxiv.org/pdf/1909.04469v4)

> We present an end-to-end trainable approach for Optical Character Recognition (OCR) on printed documents. Specifically, we propose a model that predicts a) a two-dimensional character grid (\emph{chargrid}) representation of a document image as a semantic segmentation task and b) character boxes for delineating character instances as an object detection task. For training the model, we build two large-scale datasets without resorting to any manual annotation - synthetic documents with clean labels and real documents with noisy labels. We demonstrate experimentally that our method, trained on the combination of these datasets, (i) outperforms previous state-of-the-art approaches in accuracy (ii) is easily parallelizable on GPU and is, therefore, significantly faster and (iii) is easy to train and adapt to a new domain.

</details>

<details>

<summary>2020-02-27 16:57:04 - ZoomCount: A Zooming Mechanism for Crowd Counting in Static Images</summary>

- *Usman Sajid, Hasan Sajid, Hongcheng Wang, Guanghui Wang*

- `2002.12256v1` - [abs](http://arxiv.org/abs/2002.12256v1) - [pdf](http://arxiv.org/pdf/2002.12256v1)

> This paper proposes a novel approach for crowd counting in low to high density scenarios in static images. Current approaches cannot handle huge crowd diversity well and thus perform poorly in extreme cases, where the crowd density in different regions of an image is either too low or too high, leading to crowd underestimation or overestimation. The proposed solution is based on the observation that detecting and handling such extreme cases in a specialized way leads to better crowd estimation. Additionally, existing methods find it hard to differentiate between the actual crowd and the cluttered background regions, resulting in further count overestimation. To address these issues, we propose a simple yet effective modular approach, where an input image is first subdivided into fixed-size patches and then fed to a four-way classification module labeling each image patch as low, medium, high-dense or no-crowd. This module also provides a count for each label, which is then analyzed via a specifically devised novel decision module to decide whether the image belongs to any of the two extreme cases (very low or very high density) or a normal case. Images, specified as high- or low-density extreme or a normal case, pass through dedicated zooming or normal patch-making blocks respectively before routing to the regressor in the form of fixed-size patches for crowd estimate. Extensive experimental evaluations demonstrate that the proposed approach outperforms the state-of-the-art methods on four benchmarks under most of the evaluation criteria.

</details>

<details>

<summary>2020-02-27 18:40:10 - Semantically-Guided Representation Learning for Self-Supervised Monocular Depth</summary>

- *Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, Adrien Gaidon*

- `2002.12319v1` - [abs](http://arxiv.org/abs/2002.12319v1) - [pdf](http://arxiv.org/pdf/2002.12319v1)

> Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories.

</details>

<details>

<summary>2020-02-27 18:48:33 - Few-shot Natural Language Generation for Task-Oriented Dialog</summary>

- *Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, Jianfeng Gao*

- `2002.12328v1` - [abs](http://arxiv.org/abs/2002.12328v1) - [pdf](http://arxiv.org/pdf/2002.12328v1)

> As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains. Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications. To this end, we present FewShotWoz, the first NLG benchmark to simulate the few-shot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains. Experiments on FewShotWoz and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations.

</details>

<details>

<summary>2020-02-27 18:58:15 - Generating Followup Questions for Interpretable Multi-hop Question Answering</summary>

- *Christopher Malon, Bing Bai*

- `2002.12344v1` - [abs](http://arxiv.org/abs/2002.12344v1) - [pdf](http://arxiv.org/pdf/2002.12344v1)

> We propose a framework for answering open domain multi-hop questions in which partial information is read and used to generate followup questions, to finally be answered by a pretrained single-hop answer extractor. This framework makes each hop interpretable, and makes the retrieval associated with later hops as flexible and specific as for the first hop. As a first instantiation of this framework, we train a pointer-generator network to predict followup questions based on the question and partial information. This provides a novel application of a neural question generation network, which is applied to give weak ground truth single-hop followup questions based on the final answers and their supporting facts. Learning to generate followup questions that select the relevant answer spans against downstream supporting facts, while avoiding distracting premises, poses an exciting semantic challenge for text generation. We present an evaluation using the two-hop bridge questions of HotpotQA.

</details>

<details>

<summary>2020-02-27 21:20:38 - Bringing freedom in variable choice when searching counter-examples in floating point programs</summary>

- *Heytem Zitoun, Claude Michel, Laurent Michel, Michel Rueher*

- `2002.12447v1` - [abs](http://arxiv.org/abs/2002.12447v1) - [pdf](http://arxiv.org/pdf/2002.12447v1)

> Program verification techniques typically focus on finding counter-examples that violate properties of a program. Constraint programming offers a convenient way to verify programs by modeling their state transformations and specifying searches that seek counter-examples. Floating-point computations present additional challenges for verification given the semantic subtleties of floating point arithmetic. % This paper focuses on search strategies for CSPs using floating point numbers constraint systems and dedicated to program verification. It introduces a new search heuristic based on the global number of occurrences that outperforms state-of-the-art strategies. More importantly, it demonstrates that a new technique that only branches on input variables of the verified program improve performance. It composes with a diversification technique that prevents the selection of the same variable within a fixed horizon further improving performances and reduces disparities between various variable choice heuristics. The result is a robust methodology that can tailor the search strategy according to the sought properties of the counter example.

</details>

<details>

<summary>2020-02-27 21:44:41 - Comment Ranking Diversification in Forum Discussions</summary>

- *Curtis G. Northcutt, Kimberly A. Leon, Naichun Chen*

- `2002.12457v1` - [abs](http://arxiv.org/abs/2002.12457v1) - [pdf](http://arxiv.org/pdf/2002.12457v1)

> Viewing consumption of discussion forums with hundreds or more comments depends on ranking because most users only view top-ranked comments. When comments are ranked by an ordered score (e.g. number of replies or up-votes) without adjusting for semantic similarity of near-ranked comments, top-ranked comments are more likely to emphasize the majority opinion and incur redundancy. In this paper, we propose a top K comment diversification re-ranking model using Maximal Marginal Relevance (MMR) and evaluate its impact in three categories: (1) semantic diversity, (2) inclusion of the semantics of lower-ranked comments, and (3) redundancy, within the context of a HarvardX course discussion forum. We conducted a double-blind, small-scale evaluation experiment requiring subjects to select between the top 5 comments of a diversified ranking and a baseline ranking ordered by score. For three subjects, across 100 trials, subjects selected the diversified (75% score, 25% diversification) ranking as significantly (1) more diverse, (2) more inclusive, and (3) less redundant. Within each category, inter-rater reliability showed moderate consistency, with typical Cohen-Kappa scores near 0.2. Our findings suggest that our model improves (1) diversification, (2) inclusion, and (3) redundancy, among top K ranked comments in online discussion forums.

</details>

<details>

<summary>2020-02-28 04:32:16 - UKARA 1.0 Challenge Track 1: Automatic Short-Answer Scoring in Bahasa Indonesia</summary>

- *Ali Akbar Septiandri, Yosef Ardhito Winatmoko*

- `2002.12540v1` - [abs](http://arxiv.org/abs/2002.12540v1) - [pdf](http://arxiv.org/pdf/2002.12540v1)

> We describe our third-place solution to the UKARA 1.0 challenge on automated essay scoring. The task consists of a binary classification problem on two datasets | answers from two different questions. We ended up using two different models for the two datasets. For task A, we applied a random forest algorithm on features extracted using unigram with latent semantic analysis (LSA). On the other hand, for task B, we only used logistic regression on TF-IDF features. Our model results in F1 score of 0.812.

</details>

<details>

<summary>2020-02-28 07:57:56 - Regional Registration of Whole Slide Image Stacks Containing Highly Deformed Artefacts</summary>

- *Mahsa Paknezhad, Sheng Yang Michael Loh, Yukti Choudhury, Valerie Koh Cui Koh, TimothyTay Kwang Yong, Hui Shan Tan, Ravindran Kanesvaran, Puay Hoon Tan, John Yuen Shyi Peng, Weimiao Yu, Yongcheng Benjamin Tan, Yong Zhen Loy, Min-Han Tan, Hwee Kuan Lee*

- `2002.12588v1` - [abs](http://arxiv.org/abs/2002.12588v1) - [pdf](http://arxiv.org/pdf/2002.12588v1)

> Motivation: High resolution 2D whole slide imaging provides rich information about the tissue structure. This information can be a lot richer if these 2D images can be stacked into a 3D tissue volume. A 3D analysis, however, requires accurate reconstruction of the tissue volume from the 2D image stack. This task is not trivial due to the distortions that each individual tissue slice experiences while cutting and mounting the tissue on the glass slide. Performing registration for the whole tissue slices may be adversely affected by the deformed tissue regions. Consequently, regional registration is found to be more effective. In this paper, we propose an accurate and robust regional registration algorithm for whole slide images which incrementally focuses registration on the area around the region of interest. Results: Using mean similarity index as the metric, the proposed algorithm (mean $\pm$ std: $0.84 \pm 0.11$) followed by a fine registration algorithm ($0.86 \pm 0.08$) outperformed the state-of-the-art linear whole tissue registration algorithm ($0.74 \pm 0.19$) and the regional version of this algorithm ($0.81 \pm 0.15$). The proposed algorithm also outperforms the state-of-the-art nonlinear registration algorithm (original : $0.82 \pm 0.12$, regional : $0.77 \pm 0.22$) for whole slide images and a recently proposed patch-based registration algorithm (patch size 256: $0.79 \pm 0.16$ , patch size 512: $0.77 \pm 0.16$) for medical images. Availability: The C++ implementation code is available online at the github repository: https://github.com/MahsaPaknezhad/WSIRegistration

</details>

<details>

<summary>2020-02-28 09:16:35 - Challenges in Building Intelligent Open-domain Dialog Systems</summary>

- *Minlie Huang, Xiaoyan Zhu, Jianfeng Gao*

- `1905.05709v3` - [abs](http://arxiv.org/abs/1905.05709v3) - [pdf](http://arxiv.org/pdf/1905.05709v3)

> There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This paper reviews the recent works on neural approaches that are devoted to addressing three challenges in developing such systems: semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify user's social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users trust and gain their long-term confidence. Interactiveness refers to the system's ability to generate interpersonal responses to achieve particular social goals such as entertainment, conforming, and task completion. The works we select to present here is based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent dialog systems.

</details>

<details>

<summary>2020-02-28 10:12:15 - Multi-task Generative Adversarial Learning on Geometrical Shape Reconstruction from EEG Brain Signals</summary>

- *Xiang Zhang, Xiaocong Chen, Manqing Dong, Huan Liu, Chang Ge, Lina Yao*

- `1907.13351v2` - [abs](http://arxiv.org/abs/1907.13351v2) - [pdf](http://arxiv.org/pdf/1907.13351v2)

> Synthesizing geometrical shapes from human brain activities is an interesting and meaningful but very challenging topic. Recently, the advancements of deep generative models like Generative Adversarial Networks (GANs) have supported the object generation from neurological signals. However, the Electroencephalograph (EEG)-based shape generation still suffer from the low realism problem. In particular, the generated geometrical shapes lack clear edges and fail to contain necessary details. In light of this, we propose a novel multi-task generative adversarial network to convert the individual's EEG signals evoked by geometrical shapes to the original geometry. First, we adopt a Convolutional Neural Network (CNN) to learn highly informative latent representation for the raw EEG signals, which is vital for the subsequent shape reconstruction. Next, we build the discriminator based on multi-task learning to distinguish and classify fake samples simultaneously, where the mutual promotion between different tasks improves the quality of the recovered shapes. Then, we propose a semantic alignment constraint in order to force the synthesized samples to approach the real ones in pixel-level, thus producing more compelling shapes. The proposed approach is evaluated over a local dataset and the results show that our model outperforms the competitive state-of-the-art baselines.

</details>

<details>

<summary>2020-02-28 13:20:14 - Semantrix: A Compressed Semantic Matrix</summary>

- *Nieves R. Brisaboa, Antonio Fariña, Gonzalo Navarro, Tirso V. Rodeiro*

- `2002.12050v2` - [abs](http://arxiv.org/abs/2002.12050v2) - [pdf](http://arxiv.org/pdf/2002.12050v2)

> We present a compact data structure to represent both the duration and length of homogeneous segments of trajectories from moving objects in a way that, as a data warehouse, it allows us to efficiently answer cumulative queries. The division of trajectories into relevant segments has been studied in the literature under the topic of Trajectory Segmentation. In this paper, we design a data structure to compactly represent them and the algorithms to answer the more relevant queries. We experimentally evaluate our proposal in the real context of an enterprise with mobile workers (truck drivers) where we aim at analyzing the time they spend in different activities. To test our proposal under higher stress conditions we generated a huge amount of synthetic realistic trajectories and evaluated our system with those data to have a good idea about its space needs and its efficiency when answering different types of queries.

</details>

<details>

<summary>2020-02-28 14:12:02 - Light-weight Calibrator: a Separable Component for Unsupervised Domain Adaptation</summary>

- *Shaokai Ye, Kailu Wu, Mu Zhou, Yunfei Yang, Sia huat Tan, Kaidi Xu, Jiebo Song, Chenglong Bao, Kaisheng Ma*

- `1911.12796v2` - [abs](http://arxiv.org/abs/1911.12796v2) - [pdf](http://arxiv.org/pdf/1911.12796v2)

> Existing domain adaptation methods aim at learning features that can be generalized among domains. These methods commonly require to update source classifier to adapt to the target domain and do not properly handle the trade off between the source domain and the target domain. In this work, instead of training a classifier to adapt to the target domain, we use a separable component called data calibrator to help the fixed source classifier recover discrimination power in the target domain, while preserving the source domain's performance. When the difference between two domains is small, the source classifier's representation is sufficient to perform well in the target domain and outperforms GAN-based methods in digits. Otherwise, the proposed method can leverage synthetic images generated by GANs to boost performance and achieve state-of-the-art performance in digits datasets and driving scene semantic segmentation. Our method empirically reveals that certain intriguing hints, which can be mitigated by adversarial attack to domain discriminators, are one of the sources for performance degradation under the domain shift.

</details>

<details>

<summary>2020-02-28 14:23:26 - Exploring the Robustness of NMT Systems to Nonsensical Inputs</summary>

- *Akshay Chaturvedi, Abijith KP, Utpal Garain*

- `1908.01165v3` - [abs](http://arxiv.org/abs/1908.01165v3) - [pdf](http://arxiv.org/pdf/1908.01165v3)

> Neural machine translation (NMT) systems have been shown to give undesirable translation when a small change is made in the source sentence. In this paper, we study the behaviour of NMT systems when multiple changes are made to the source sentence. In particular, we ask the following question "Is it possible for an NMT system to predict same translation even when multiple words in the source sentence have been replaced?". To this end, we propose a soft-attention based technique to make the aforementioned word replacements. The experiments are conducted on two language pairs: English-German (en-de) and English-French (en-fr) and two state-of-the-art NMT systems: BLSTM-based encoder-decoder with attention and Transformer. The proposed soft-attention based technique achieves high success rate and outperforms existing methods like HotFlip by a significant margin for all the conducted experiments. The results demonstrate that state-of-the-art NMT systems are unable to capture the semantics of the source language. The proposed soft-attention based technique is an invariance-based adversarial attack on NMT systems. To better evaluate such attacks, we propose an alternate metric and argue its benefits in comparison with success rate.

</details>

<details>

<summary>2020-02-29 02:04:35 - Automated Regression Unit Test Generation for Program Merges</summary>

- *Tao Ji, Liqian Chen, Xiaoguang Mao, Xin Yi, Jiahong Jiang*

- `2003.00154v1` - [abs](http://arxiv.org/abs/2003.00154v1) - [pdf](http://arxiv.org/pdf/2003.00154v1)

> Merging other branches into the current working branch is common in collaborative software development. However, developers still heavily rely on the textual merge tools to handle the complicated merge tasks. The latent semantic merge conflicts may fail to be detected and degrade the software quality. Regression testing is able to prevent regression faults and has been widely used in real-world software development. However, the merged software may fail to be well examined by rerunning the existing whole test suite. Intuitively, if the test suite fails to cover the changes of different branches at the same time, the merge conflicts would fail to be detected. Recently, it has been proposed to conduct verification on 3-way merges, but this approach does not support even some common cases such as different changes made to different parts of the program. In this paper, we propose an approach of regression unit test generation specifically for checking program merges according to our proposed test oracles. And our general test oracles support us to examine not only 3-way merges, but also 2-way and octopus merges. Considering the conflicts may arise in other locations besides changed methods of the project, we design an algorithm to select UUTs based on the dependency analysis of the whole project. On this basis, we implement a tool called TOM to generate unit tests for Java program merges. We also design the benchmark MCon4J consisting of 389 conflict 3-way merges and 389 conflict octopus merges to facilitate further studies on this topic. The experimental results show that TOM finds 45 conflict 3- way merges and 87 conflicts octopus merges, while the verification based tool fails to work on MCon4J.

</details>

<details>

<summary>2020-02-29 15:02:16 - Causal Learning by a Robot with Semantic-Episodic Memory in an Aesop's Fable Experiment</summary>

- *Ajaz A. Bhat, Vishwanathan Mohan*

- `2003.00274v1` - [abs](http://arxiv.org/abs/2003.00274v1) - [pdf](http://arxiv.org/pdf/2003.00274v1)

> Corvids, apes, and children solve The Crow and The Pitcher task (from Aesop's Fables) indicating a causal understanding of the task. By cumulatively interacting with different objects, how can cognitive agents abstract the underlying cause-effect relations to predict affordances of novel objects? We address this question by re-enacting the Aesop's Fable task on a robot and present a) a brain-guided neural model of semantic-episodic memory; with b) four task-agnostic learning rules that compare expectations from recalled past episodes with the current scenario to progressively extract the hidden causal relations. The ensuing robot behaviours illustrate causal learning; and predictions for novel objects converge to Archimedes' principle, independent of both the objects explored during learning and the order of their cumulative exploration.

</details>

<details>

<summary>2020-02-29 20:33:48 - An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data: Experience and Practice</summary>

- *Ruwan Wickramarachchi, Cory Henson, Amit Sheth*

- `2003.00344v1` - [abs](http://arxiv.org/abs/2003.00344v1) - [pdf](http://arxiv.org/pdf/2003.00344v1)

> The autonomous driving (AD) industry is exploring the use of knowledge graphs (KGs) to manage the vast amount of heterogeneous data generated from vehicular sensors. The various types of equipped sensors include video, LIDAR and RADAR. Scene understanding is an important topic in AD which requires consideration of various aspects of a scene, such as detected objects, events, time and location. Recent work on knowledge graph embeddings (KGEs) - an approach that facilitates neuro-symbolic fusion - has shown to improve the predictive performance of machine learning models. With the expectation that neuro-symbolic fusion through KGEs will improve scene understanding, this research explores the generation and evaluation of KGEs for autonomous driving data. We also present an investigation of the relationship between the level of informational detail in a KG and the quality of its derivative embeddings. By systematically evaluating KGEs along four dimensions -- i.e. quality metrics, KG informational detail, algorithms, and datasets -- we show that (1) higher levels of informational detail in KGs lead to higher quality embeddings, (2) type and relation semantics are better captured by the semantic transitional distance-based TransE algorithm, and (3) some metrics, such as coherence measure, may not be suitable for intrinsically evaluating KGEs in this domain. Additionally, we also present an (early) investigation of the usefulness of KGEs for two use-cases in the AD domain.

</details>

<details>

<summary>2020-02-29 22:15:15 - Clinical Text Summarization with Syntax-Based Negation and Semantic Concept Identification</summary>

- *Wei-Hung Weng, Yu-An Chung, Schrasing Tong*

- `2003.00353v1` - [abs](http://arxiv.org/abs/2003.00353v1) - [pdf](http://arxiv.org/pdf/2003.00353v1)

> In the era of clinical information explosion, a good strategy for clinical text summarization is helpful to improve the clinical workflow. The ideal summarization strategy can preserve important information in the informative but less organized, ill-structured clinical narrative texts. Instead of using pure statistical learning approaches, which are difficult to interpret and explain, we utilized knowledge of computational linguistics with human experts-curated biomedical knowledge base to achieve the interpretable and meaningful clinical text summarization. Our research objective is to use the biomedical ontology with semantic information, and take the advantage from the language hierarchical structure, the constituency tree, in order to identify the correct clinical concepts and the corresponding negation information, which is critical for summarizing clinical concepts from narrative text. We achieved the clinically acceptable performance for both negation detection and concept identification, and the clinical concepts with common negated patterns can be identified and negated by the proposed method.

</details>


## 2020-03

<details>

<summary>2020-03-01 00:38:46 - MLIR: A Compiler Infrastructure for the End of Moore's Law</summary>

- *Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, Oleksandr Zinenko*

- `2002.11054v2` - [abs](http://arxiv.org/abs/2002.11054v2) - [pdf](http://arxiv.org/pdf/2002.11054v2)

> This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.

</details>

<details>

<summary>2020-03-01 03:34:07 - Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs</summary>

- *Shizhe Chen, Qin Jin, Peng Wang, Qi Wu*

- `2003.00387v1` - [abs](http://arxiv.org/abs/2003.00387v1) - [pdf](http://arxiv.org/pdf/2003.00387v1)

> Humans are able to describe image contents with coarse to fine details as they wish. However, most image captioning models are intention-agnostic which can not generate diverse descriptions according to different user intentions initiatively. In this work, we propose the Abstract Scene Graph (ASG) structure to represent user intention in fine-grained level and control what and how detailed the generated description should be. The ASG is a directed graph consisting of three types of \textbf{abstract nodes} (object, attribute, relationship) grounded in the image without any concrete semantic labels. Thus it is easy to obtain either manually or automatically. From the ASG, we propose a novel ASG2Caption model, which is able to recognise user intentions and semantics in the graph, and therefore generate desired captions according to the graph structure. Our model achieves better controllability conditioning on ASGs than carefully designed baselines on both VisualGenome and MSCOCO datasets. It also significantly improves the caption diversity via automatically sampling diverse ASGs as control signals.

</details>

<details>

<summary>2020-03-01 03:44:19 - Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning</summary>

- *Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu*

- `2003.00392v1` - [abs](http://arxiv.org/abs/2003.00392v1) - [pdf](http://arxiv.org/pdf/2003.00392v1)

> Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences.

</details>

<details>

<summary>2020-03-02 11:18:30 - On the Existence of Characterization Logics and Fundamental Properties of Argumentation Semantics</summary>

- *Ringo Baumann*

- `2003.00767v1` - [abs](http://arxiv.org/abs/2003.00767v1) - [pdf](http://arxiv.org/pdf/2003.00767v1)

> Given the large variety of existing logical formalisms it is of utmost importance to select the most adequate one for a specific purpose, e.g. for representing the knowledge relevant for a particular application or for using the formalism as a modeling tool for problem solving. Awareness of the nature of a logical formalism, in other words, of its fundamental intrinsic properties, is indispensable and provides the basis of an informed choice. In this treatise we consider the existence characterization logics as well as properties like existence and uniqueness, expressibility, replaceability and verifiability in the realm of abstract argumentation

</details>

<details>

<summary>2020-03-02 12:12:43 - Data-Free Adversarial Distillation</summary>

- *Gongfan Fang, Jie Song, Chengchao Shen, Xinchao Wang, Da Chen, Mingli Song*

- `1912.11006v3` - [abs](http://arxiv.org/abs/1912.11006v3) - [pdf](http://arxiv.org/pdf/1912.11006v3)

> Knowledge Distillation (KD) has made remarkable progress in the last few years and become a popular paradigm for model compression and knowledge transfer. However, almost all existing KD algorithms are data-driven, i.e., relying on a large amount of original training data or alternative data, which is usually unavailable in real-world scenarios. In this paper, we devote ourselves to this challenging problem and propose a novel adversarial distillation mechanism to craft a compact student model without any real-world data. We introduce a model discrepancy to quantificationally measure the difference between student and teacher models and construct an optimizable upper bound. In our work, the student and the teacher jointly act the role of the discriminator to reduce this discrepancy, when a generator adversarially produces some "hard samples" to enlarge it. Extensive experiments demonstrate that the proposed data-free method yields comparable performance to existing data-driven methods. More strikingly, our approach can be directly extended to semantic segmentation, which is more complicated than classification, and our approach achieves state-of-the-art results. Code and pretrained models are available at https://github.com/VainF/Data-Free-Adversarial-Distillation.

</details>

<details>

<summary>2020-03-02 15:46:24 - Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation</summary>

- *Umberto Michieli, Matteo Biasetton, Gianluca Agresti, Pietro Zanuttigh*

- `1909.00781v2` - [abs](http://arxiv.org/abs/1909.00781v2) - [pdf](http://arxiv.org/pdf/1909.00781v2)

> Deep learning techniques have been widely used in autonomous driving systems for the semantic understanding of urban scenes. However, they need a huge amount of labeled data for training, which is difficult and expensive to acquire. A recently proposed workaround is to train deep networks using synthetic data, but the domain shift between real world and synthetic representations limits the performance. In this work, a novel Unsupervised Domain Adaptation (UDA) strategy is introduced to solve this issue. The proposed learning strategy is driven by three components: a standard supervised learning loss on labeled synthetic data; an adversarial learning module that exploits both labeled synthetic data and unlabeled real data; finally, a self-teaching strategy applied to unlabeled data. The last component exploits a region growing framework guided by the segmentation confidence. Furthermore, we weighted this component on the basis of the class frequencies to enhance the performance on less common classes. Experimental results prove the effectiveness of the proposed strategy in adapting a segmentation network trained on synthetic datasets, like GTA5 and SYNTHIA, to real world datasets like Cityscapes and Mapillary.

</details>

<details>

<summary>2020-03-03 02:56:36 - Transfer Learning for Context-Aware Spoken Language Understanding</summary>

- *Qian Chen, Zhu Zhuo, Wen Wang, Qiuyun Xu*

- `2003.01305v1` - [abs](http://arxiv.org/abs/2003.01305v1) - [pdf](http://arxiv.org/pdf/2003.01305v1)

> Spoken language understanding (SLU) is a key component of task-oriented dialogue systems. SLU parses natural language user utterances into semantic frames. Previous work has shown that incorporating context information significantly improves SLU performance for multi-turn dialogues. However, collecting a large-scale human-labeled multi-turn dialogue corpus for the target domains is complex and costly. To reduce dependency on the collection and annotation effort, we propose a Context Encoding Language Transformer (CELT) model facilitating exploiting various context information for SLU. We explore different transfer learning approaches to reduce dependency on data collection and annotation. In addition to unsupervised pre-training using large-scale general purpose unlabeled corpora, such as Wikipedia, we explore unsupervised and supervised adaptive training approaches for transfer learning to benefit from other in-domain and out-of-domain dialogue corpora. Experimental results demonstrate that the proposed model with the proposed transfer learning approaches achieves significant improvement on the SLU performance over state-of-the-art models on two large-scale single-turn dialogue benchmarks and one large-scale multi-turn dialogue benchmark.

</details>

<details>

<summary>2020-03-03 13:32:42 - GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception</summary>

- *Laura Bostan, Evgeny Kim, Roman Klinger*

- `1912.03184v3` - [abs](http://arxiv.org/abs/1912.03184v3) - [pdf](http://arxiv.org/pdf/1912.03184v3)

> Most research on emotion analysis from text focuses on the task of emotion classification or emotion intensity regression. Fewer works address emotions as a phenomenon to be tackled with structured learning, which can be explained by the lack of relevant datasets. We fill this gap by releasing a dataset of 5000 English news headlines annotated via crowdsourcing with their associated emotions, the corresponding emotion experiencers and textual cues, related emotion causes and targets, as well as the reader's perception of the emotion of the headline. This annotation task is comparably challenging, given the large number of classes and roles to be identified. We therefore propose a multiphase annotation procedure in which we first find relevant instances with emotional content and then annotate the more fine-grained aspects. Finally, we develop a baseline for the task of automatic prediction of semantic role structures and discuss the results. The corpus we release enables further research on emotion classification, emotion intensity prediction, emotion cause detection, and supports further qualitative studies.

</details>

<details>

<summary>2020-03-03 14:08:07 - word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement</summary>

- *Aliakbar Panahi, Seyran Saeedi, Tom Arodz*

- `1911.04975v3` - [abs](http://arxiv.org/abs/1911.04975v3) - [pdf](http://arxiv.org/pdf/1911.04975v3)

> Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, {\em word2ket} and {\em word2ketXS}, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.

</details>

<details>

<summary>2020-03-03 20:35:25 - Security of Deep Learning based Lane Keeping System under Physical-World Adversarial Attack</summary>

- *Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi Alfred Chen*

- `2003.01782v1` - [abs](http://arxiv.org/abs/2003.01782v1) - [pdf](http://arxiv.org/pdf/2003.01782v1)

> Lane-Keeping Assistance System (LKAS) is convenient and widely available today, but also extremely security and safety critical. In this work, we design and implement the first systematic approach to attack real-world DNN-based LKASes. We identify dirty road patches as a novel and domain-specific threat model for practicality and stealthiness. We formulate the attack as an optimization problem, and address the challenge from the inter-dependencies among attacks on consecutive camera frames. We evaluate our approach on a state-of-the-art LKAS and our preliminary results show that our attack can successfully cause it to drive off lane boundaries within as short as 1.3 seconds.

</details>

<details>

<summary>2020-03-03 21:07:13 - Textual analysis of artificial intelligence manuscripts reveals features associated with peer review outcome</summary>

- *Philippe Vincent-Lamarre, Vincent Larivière*

- `1911.02648v2` - [abs](http://arxiv.org/abs/1911.02648v2) - [pdf](http://arxiv.org/pdf/1911.02648v2)

> We analysed a dataset of scientific manuscripts that were submitted to various conferences in artificial intelligence. We performed a combination of semantic, lexical and psycholinguistic analyses of the full text of the manuscripts and compared them with the outcome of the peer review process. We found that accepted manuscripts scored lower than rejected manuscripts on two indicators of readability, and that they also used more scientific and artificial intelligence jargon. We also found that accepted manuscripts were written with words that are less frequent, that are acquired at an older age, and that are more abstract than rejected manuscripts. The analysis of references included in the manuscripts revealed that the subset of accepted submissions were more likely to cite the same publications. This finding was echoed by pairwise comparisons of the word content of the manuscripts (i.e. an indicator or semantic similarity), which were more similar in the subset of accepted manuscripts. Finally, we predicted the peer review outcome of manuscripts with their word content, with words related to machine learning and neural networks positively related with acceptance, whereas words related to logic, symbolic processing and knowledge-based systems negatively related with acceptance.

</details>

<details>

<summary>2020-03-04 02:00:57 - SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text Classification</summary>

- *Changzeng Fu, Chaoran Liu, Carlos Toshinori Ishi, Yuichiro Yoshikawa, Hiroshi Ishiguro*

- `2003.01857v1` - [abs](http://arxiv.org/abs/2003.01857v1) - [pdf](http://arxiv.org/pdf/2003.01857v1)

> Text categorization is the task of assigning labels to documents written in a natural language, and it has numerous real-world applications including sentiment analysis as well as traditional topic assignment tasks. In this paper, we propose 5 different configurations for the semantic matrix-based memory neural network with end-to-end learning manner and evaluate our proposed method on two corpora of news articles (AG news, Sogou news). The best performance of our proposed method outperforms the baseline VDCNN models on the text classification task and gives a faster speed for learning semantics. Moreover, we also evaluate our model on small scale datasets. The results show that our proposed method can still achieve better results in comparison to VDCNN on the small scale dataset. This paper is to appear in the Proceedings of the 2020 IEEE 14th International Conference on Semantic Computing (ICSC 2020), San Diego, California, 2020.

</details>

<details>

<summary>2020-03-04 03:20:21 - Weighted Encoding Based Image Interpolation With Nonlocal Linear Regression Model</summary>

- *Junchao Zhang*

- `2003.04811v1` - [abs](http://arxiv.org/abs/2003.04811v1) - [pdf](http://arxiv.org/pdf/2003.04811v1)

> Image interpolation is a special case of image super-resolution, where the low-resolution image is directly down-sampled from its high-resolution counterpart without blurring and noise. Therefore, assumptions adopted in super-resolution models are not valid for image interpolation. To address this problem, we propose a novel image interpolation model based on sparse representation. Two widely used priors including sparsity and nonlocal self-similarity are used as the regularization terms to enhance the stability of interpolation model. Meanwhile, we incorporate the nonlocal linear regression into this model since nonlocal similar patches could provide a better approximation to a given patch. Moreover, we propose a new approach to learn adaptive sub-dictionary online instead of clustering. For each patch, similar patches are grouped to learn adaptive sub-dictionary, generating a more sparse and accurate representation. Finally, the weighted encoding is introduced to suppress tailing of fitting residuals in data fidelity. Abundant experimental results demonstrate that our proposed method outperforms several state-of-the-art methods in terms of quantitative measures and visual quality.

</details>

<details>

<summary>2020-03-04 03:39:54 - UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation</summary>

- *Junjiao Tian, Wesley Cheung, Nathan Glaser, Yen-Cheng Liu, Zsolt Kira*

- `1911.05611v2` - [abs](http://arxiv.org/abs/1911.05611v2) - [pdf](http://arxiv.org/pdf/1911.05611v2)

> The fusion of multiple sensor modalities, especially through deep learning architectures, has been an active area of study. However, an under-explored aspect of such work is whether the methods can be robust to degradations across their input modalities, especially when they must generalize to degradations not seen during training. In this work, we propose an uncertainty-aware fusion scheme to effectively fuse inputs that might suffer from a range of known and unknown degradations. Specifically, we analyze a number of uncertainty measures, each of which captures a different aspect of uncertainty, and we propose a novel way to fuse degraded inputs by scaling modality-specific output softmax probabilities. We additionally propose a novel data-dependent spatial temperature scaling method to complement these existing uncertainty measures. Finally, we integrate the uncertainty-scaled output from each modality using a probabilistic noisy-or fusion method. In a photo-realistic simulation environment (AirSim), we show that our method achieves significantly better results on a semantic segmentation task, compared to state-of-art fusion architectures, on a range of degradations (e.g. fog, snow, frost, and various other types of noise), some of which are unknown during training. We specifically improve upon the state-of-art[1] by 28% in mean IoU on various degradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard. Self-Supervised Model Adaptation for Multimodal Semantic Segmentation. In: arXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv: 1808.03833 [cs.CV].

</details>

<details>

<summary>2020-03-04 04:49:45 - Efficient Sentence Embedding via Semantic Subspace Analysis</summary>

- *Bin Wang, Fenxiao Chen, Yuncheng Wang, C. -C. Jay Kuo*

- `2002.09620v2` - [abs](http://arxiv.org/abs/2002.09620v2) - [pdf](http://arxiv.org/pdf/2002.09620v2)

> A novel sentence embedding method built upon semantic subspace analysis, called semantic subspace sentence embedding (S3E), is proposed in this work. Given the fact that word embeddings can capture semantic relationship while semantically similar words tend to form semantic groups in a high-dimensional embedding space, we develop a sentence representation scheme by analyzing semantic subspaces of its constituent words. Specifically, we construct a sentence model from two aspects. First, we represent words that lie in the same semantic group using the intra-group descriptor. Second, we characterize the interaction between multiple semantic groups with the inter-group descriptor. The proposed S3E method is evaluated on both textual similarity tasks and supervised tasks. Experimental results show that it offers comparable or better performance than the state-of-the-art. The complexity of our S3E method is also much lower than other parameterized models.

</details>

<details>

<summary>2020-03-04 07:44:55 - GraphTTS: graph-to-sequence modelling in neural text-to-speech</summary>

- *Aolan Sun, Jianzong Wang, Ning Cheng, Huayi Peng, Zhen Zeng, Jing Xiao*

- `2003.01924v1` - [abs](http://arxiv.org/abs/2003.01924v1) - [pdf](http://arxiv.org/pdf/2003.01924v1)

> This paper leverages the graph-to-sequence method in neural text-to-speech (GraphTTS), which maps the graph embedding of the input sequence to spectrograms. The graphical inputs consist of node and edge representations constructed from input texts. The encoding of these graphical inputs incorporates syntax information by a GNN encoder module. Besides, applying the encoder of GraphTTS as a graph auxiliary encoder (GAE) can analyse prosody information from the semantic structure of texts. This can remove the manual selection of reference audios process and makes prosody modelling an end-to-end procedure. Experimental analysis shows that GraphTTS outperforms the state-of-the-art sequence-to-sequence models by 0.24 in Mean Opinion Score (MOS). GAE can adjust the pause, ventilation and tones of synthesised audios automatically. This experimental conclusion may give some inspiration to researchers working on improving speech synthesis prosody.

</details>

<details>

<summary>2020-03-05 07:57:53 - Structured Compression by Weight Encryption for Unstructured Pruning and Quantization</summary>

- *Se Jung Kwon, Dongsoo Lee, Byeongwook Kim, Parichay Kapoor, Baeseong Park, Gu-Yeon Wei*

- `1905.10138v2` - [abs](http://arxiv.org/abs/1905.10138v2) - [pdf](http://arxiv.org/pdf/1905.10138v2)

> Model compression techniques, such as pruning and quantization, are becoming increasingly important to reduce the memory footprints and the amount of computations. Despite model size reduction, achieving performance enhancement on devices is, however, still challenging mainly due to the irregular representations of sparse matrix formats. This paper proposes a new weight representation scheme for Sparse Quantized Neural Networks, specifically achieved by fine-grained and unstructured pruning method. The representation is encrypted in a structured regular format, which can be efficiently decoded through XOR-gate network during inference in a parallel manner. We demonstrate various deep learning models that can be compressed and represented by our proposed format with fixed and high compression ratio. For example, for fully-connected layers of AlexNet on ImageNet dataset, we can represent the sparse weights by only 0.28 bits/weight for 1-bit quantization and 91% pruning rate with a fixed decoding rate and full memory bandwidth usage. Decoding through XOR-gate network can be performed without any model accuracy degradation with additional patch data associated with small overhead.

</details>

<details>

<summary>2020-03-05 12:47:29 - DANTE: A framework for mining and monitoring darknet traffic</summary>

- *Dvir Cohen, Yisroel Mirsky, Yuval Elovici, Rami Puzis, Manuel Kamp, Tobias Martin, Asaf Shabtai*

- `2003.02575v1` - [abs](http://arxiv.org/abs/2003.02575v1) - [pdf](http://arxiv.org/pdf/2003.02575v1)

> Trillions of network packets are sent over the Internet to destinations which do not exist. This 'darknet' traffic captures the activity of botnets and other malicious campaigns aiming to discover and compromise devices around the world. In order to mine threat intelligence from this data, one must be able to handle large streams of logs and represent the traffic patterns in a meaningful way. However, by observing how network ports (services) are used, it is possible to capture the intent of each transmission. In this paper, we present DANTE: a framework and algorithm for mining darknet traffic. DANTE learns the meaning of targeted network ports by applying Word2Vec to observed port sequences. Then, when a host sends a new sequence, DANTE represents the transmission as the average embedding of the ports found that sequence. Finally, DANTE uses a novel and incremental time-series cluster tracking algorithm on observed sequences to detect recurring behaviors and new emerging threats. To evaluate the system, we ran DANTE on a full year of darknet traffic (over three Tera-Bytes) collected by the largest telecommunications provider in Europe, Deutsche Telekom and analyzed the results. DANTE discovered 1,177 new emerging threats and was able to track malicious campaigns over time. We also compared DANTE to the current best approach and found DANTE to be more practical and effective at detecting darknet traffic patterns.

</details>

<details>

<summary>2020-03-05 17:29:39 - Leveraging Contextual Embeddings for Detecting Diachronic Semantic Shift</summary>

- *Matej Martinc, Petra Kralj Novak, Senja Pollak*

- `1912.01072v2` - [abs](http://arxiv.org/abs/1912.01072v2) - [pdf](http://arxiv.org/pdf/1912.01072v2)

> We propose a new method that leverages contextual embeddings for the task of diachronic semantic shift detection by generating time specific word representations from BERT embeddings. The results of our experiments in the domain specific LiverpoolFC corpus suggest that the proposed method has performance comparable to the current state-of-the-art without requiring any time consuming domain adaptation on large corpora. The results on the newly created Brexit news corpus suggest that the method can be successfully used for the detection of a short-term yearly semantic shift. And lastly, the model also shows promising results in a multilingual settings, where the task was to detect differences and similarities between diachronic semantic shifts in different languages.

</details>

<details>

<summary>2020-03-05 19:07:37 - The Prolog debugger and declarative programming</summary>

- *Włodzimierz Drabent*

- `1906.04765v4` - [abs](http://arxiv.org/abs/1906.04765v4) - [pdf](http://arxiv.org/pdf/1906.04765v4)

> Logic programming is a declarative programming paradigm. Programming language Prolog makes logic programming possible, at least to a substantial extent. However the Prolog debugger works solely in terms of the operational semantics. So it is incompatible with declarative programming. This report discusses this issue and tries to find how the debugger may be used from the declarative point of view. The results are rather not encouraging.   Also, the box model of Byrd, used by the debugger, is explained in terms of SLD-resolution.

</details>

<details>

<summary>2020-03-05 20:07:48 - Segmentation of Satellite Imagery using U-Net Models for Land Cover Classification</summary>

- *Priit Ulmas, Innar Liiv*

- `2003.02899v1` - [abs](http://arxiv.org/abs/2003.02899v1) - [pdf](http://arxiv.org/pdf/2003.02899v1)

> The focus of this paper is using a convolutional machine learning model with a modified U-Net structure for creating land cover classification mapping based on satellite imagery. The aim of the research is to train and test convolutional models for automatic land cover mapping and to assess their usability in increasing land cover mapping accuracy and change detection. To solve these tasks, authors prepared a dataset and trained machine learning models for land cover classification and semantic segmentation from satellite images. The results were analysed on three different land classification levels. BigEarthNet satellite image archive was selected for the research as one of two main datasets. This novel and recent dataset was published in 2019 and includes Sentinel-2 satellite photos from 10 European countries made in 2017 and 2018. As a second dataset the authors composed an original set containing a Sentinel-2 image and a CORINE land cover map of Estonia. The developed classification model shows a high overall F\textsubscript{1} score of 0.749 on multiclass land cover classification with 43 possible image labels. The model also highlights noisy data in the BigEarthNet dataset, where images seem to have incorrect labels. The segmentation models offer a solution for generating automatic land cover mappings based on Sentinel-2 satellite images and show a high IoU score for land cover classes such as forests, inland waters and arable land. The models show a capability of increasing the accuracy of existing land classification maps and in land cover change detection.

</details>

<details>

<summary>2020-03-06 09:44:42 - Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation</summary>

- *Jinghui Qin, Zheng Ye, Jianheng Tang, Xiaodan Liang*

- `2002.01196v2` - [abs](http://arxiv.org/abs/2002.01196v2) - [pdf](http://arxiv.org/pdf/2002.01196v2)

> Target-guided open-domain conversation aims to proactively and naturally guide a dialogue agent or human to achieve specific goals, topics or keywords during open-ended conversations. Existing methods mainly rely on single-turn datadriven learning and simple target-guided strategy without considering semantic or factual knowledge relations among candidate topics/keywords. This results in poor transition smoothness and low success rate. In this work, we adopt a structured approach that controls the intended content of system responses by introducing coarse-grained keywords, attains smooth conversation transition through turn-level supervised learning and knowledge relations between candidate keywords, and drives an conversation towards an specified target with discourse-level guiding strategy. Specially, we propose a novel dynamic knowledge routing network (DKRN) which considers semantic knowledge relations among candidate keywords for accurate next topic prediction of next discourse. With the help of more accurate keyword prediction, our keyword-augmented response retrieval module can achieve better retrieval performance and more meaningful conversations. Besides, we also propose a novel dual discourse-level target-guided strategy to guide conversations to reach their goals smoothly with higher success rate. Furthermore, to push the research boundary of target-guided open-domain conversation to match real-world scenarios better, we introduce a new large-scale Chinese target-guided open-domain conversation dataset (more than 900K conversations) crawled from Sina Weibo. Quantitative and human evaluations show our method can produce meaningful and effective target-guided conversations, significantly improving over other state-of-the-art methods by more than 20% in success rate and more than 0.6 in average smoothness score.

</details>

<details>

<summary>2020-03-06 11:09:44 - Knowledge graph based methods for record linkage</summary>

- *B. Gautam, O. Ramos Terrades, J. M. Pujades, M. Valls*

- `2003.03136v1` - [abs](http://arxiv.org/abs/2003.03136v1) - [pdf](http://arxiv.org/pdf/2003.03136v1)

> Nowadays, it is common in Historical Demography the use of individual-level data as a consequence of a predominant life-course approach for the understanding of the demographic behaviour, family transition, mobility, etc. Record linkage advance is key in these disciplines since it allows to increase the volume and the data complexity to be analyzed. However, current methods are constrained to link data coming from the same kind of sources. Knowledge graph are flexible semantic representations, which allow to encode data variability and semantic relations in a structured manner.   In this paper we propose the knowledge graph use to tackle record linkage task. The proposed method, named {\bf WERL}, takes advantage of the main knowledge graph properties and learns embedding vectors to encode census information. These embeddings are properly weighted to maximize the record linkage performance. We have evaluated this method on benchmark data sets and we have compared it to related methods with stimulating and satisfactory results.

</details>

<details>

<summary>2020-03-06 12:15:20 - An Ontology-based Context Model in Intelligent Environments</summary>

- *Tao Gu, Xiao Hang Wang, Hung Keng Pung, Da Qing Zhang*

- `2003.05055v1` - [abs](http://arxiv.org/abs/2003.05055v1) - [pdf](http://arxiv.org/pdf/2003.05055v1)

> Computing becomes increasingly mobile and pervasive today; these changes imply that applications and services must be aware of and adapt to their changing contexts in highly dynamic environments. Today, building context-aware systems is a complex task due to lack of an appropriate infrastructure support in intelligent environments. A context-aware infrastructure requires an appropriate context model to represent, manipulate and access context information. In this paper, we propose a formal context model based on ontology using OWL to address issues including semantic context representation, context reasoning and knowledge sharing, context classification, context dependency and quality of context. The main benefit of this model is the ability to reason about various contexts. Based on our context model, we also present a Service-Oriented Context-Aware Middleware (SOCAM) architecture for building of context-aware services.

</details>

<details>

<summary>2020-03-06 14:02:34 - Me Love (SYN-)Cookies: SYN Flood Mitigation in Programmable Data Planes</summary>

- *Dominik Scholz, Sebastian Gallenmüller, Henning Stubbe, Bassam Jaber, Minoo Rouhi, Georg Carle*

- `2003.03221v1` - [abs](http://arxiv.org/abs/2003.03221v1) - [pdf](http://arxiv.org/pdf/2003.03221v1)

> The SYN flood attack is a common attack strategy on the Internet, which tries to overload services with requests leading to a Denial-of-Service (DoS). Highly asymmetric costs for connection setup - putting the main burden on the attackee - make SYN flooding an efficient and popular DoS attack strategy. Abusing the widely used TCP as an attack vector complicates the detection of malicious traffic and its prevention utilizing naive connection blocking strategies. Modern programmable data plane devices are capable of handling traffic in the 10 Gbit/s range without overloading. We discuss how we can harness their performance to defend entire networks against SYN flood attacks. Therefore, we analyze different defense strategies, SYN authentication and SYN cookie, and discuss implementation difficulties when ported to different target data planes: software, network processors, and FPGAs. We provide prototype implementations and performance figures for all three platforms. Further, we fully disclose the artifacts leading to the experiments described in this work.

</details>

<details>

<summary>2020-03-06 15:26:59 - ÆTHEL: Automatically Extracted Typelogical Derivations for Dutch</summary>

- *Konstantinos Kogkalidis, Michael Moortgat, Richard Moot*

- `1912.12635v2` - [abs](http://arxiv.org/abs/1912.12635v2) - [pdf](http://arxiv.org/pdf/1912.12635v2)

> We present {\AE}THEL, a semantic compositionality dataset for written Dutch. {\AE}THEL consists of two parts. First, it contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, {\AE}THEL further provides 72 192 validated derivations, presented in four formats: natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs (lambda terms) for meaning composition. {\AE}THEL's types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of LASSY Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how `virtual elements' in the original LASSY annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with {\AE}THEL: the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.

</details>

<details>

<summary>2020-03-06 18:27:39 - Distributional semantic modeling: a revised technique to train term/word vector space models applying the ontology-related approach</summary>

- *Oleksandr Palagin, Vitalii Velychko, Kyrylo Malakhov, Oleksandr Shchurov*

- `2003.03350v1` - [abs](http://arxiv.org/abs/2003.03350v1) - [pdf](http://arxiv.org/pdf/2003.03350v1)

> We design a new technique for the distributional semantic modeling with a neural network-based approach to learn distributed term representations (or term embeddings) - term vector space models as a result, inspired by the recent ontology-related approach (using different types of contextual knowledge such as syntactic knowledge, terminological knowledge, semantic knowledge, etc.) to the identification of terms (term extraction) and relations between them (relation extraction) called semantic pre-processing technology - SPT. Our method relies on automatic term extraction from the natural language texts and subsequent formation of the problem-oriented or application-oriented (also deeply annotated) text corpora where the fundamental entity is the term (includes non-compositional and compositional terms). This gives us an opportunity to changeover from distributed word representations (or word embeddings) to distributed term representations (or term embeddings). This transition will allow to generate more accurate semantic maps of different subject domains (also, of relations between input terms - it is useful to explore clusters and oppositions, or to test your hypotheses about them). The semantic map can be represented as a graph using Vec2graph - a Python library for visualizing word embeddings (term embeddings in our case) as dynamic and interactive graphs. The Vec2graph library coupled with term embeddings will not only improve accuracy in solving standard NLP tasks, but also update the conventional concept of automated ontology development. The main practical result of our work is the development kit (set of toolkits represented as web service APIs and web application), which provides all necessary routines for the basic linguistic pre-processing and the semantic pre-processing of the natural language texts in Ukrainian for future training of term vector space models.

</details>

<details>

<summary>2020-03-06 19:09:42 - Scalable Uncertainty for Computer Vision with Functional Variational Inference</summary>

- *Eduardo D C Carvalho, Ronald Clark, Andrea Nicastro, Paul H J Kelly*

- `2003.03396v1` - [abs](http://arxiv.org/abs/2003.03396v1) - [pdf](http://arxiv.org/pdf/2003.03396v1)

> As Deep Learning continues to yield successful applications in Computer Vision, the ability to quantify all forms of uncertainty is a paramount requirement for its safe and reliable deployment in the real-world. In this work, we leverage the formulation of variational inference in function space, where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and variational family. Since GPs are fully determined by their mean and covariance functions, we are able to obtain predictive uncertainty estimates at the cost of a single forward pass through any chosen CNN architecture and for any supervised learning task. By leveraging the structure of the induced covariance matrices, we propose numerically efficient algorithms which enable fast training in the context of high-dimensional tasks such as depth estimation and semantic segmentation. Additionally, we provide sufficient conditions for constructing regression loss functions whose probabilistic counterparts are compatible with aleatoric uncertainty quantification.

</details>

<details>

<summary>2020-03-07 02:22:19 - Semantic Change Pattern Analysis</summary>

- *Wensheng Cheng, Yan Zhang, Xu Lei, Wen Yang, Guisong Xia*

- `2003.03492v1` - [abs](http://arxiv.org/abs/2003.03492v1) - [pdf](http://arxiv.org/pdf/2003.03492v1)

> Change detection is an important problem in vision field, especially for aerial images. However, most works focus on traditional change detection, i.e., where changes happen, without considering the change type information, i.e., what changes happen. Although a few works have tried to apply semantic information to traditional change detection, they either only give the label of emerging objects without taking the change type into consideration, or set some kinds of change subjectively without specifying semantic information. To make use of semantic information and analyze change types comprehensively, we propose a new task called semantic change pattern analysis for aerial images. Given a pair of co-registered aerial images, the task requires a result including both where and what changes happen. We then describe the metric adopted for the task, which is clean and interpretable. We further provide the first well-annotated aerial image dataset for this task. Extensive baseline experiments are conducted as reference for following works. The aim of this work is to explore high-level information based on change detection and facilitate the development of this field with the publicly available dataset.

</details>

<details>

<summary>2020-03-07 17:22:52 - Frozen Binomials on the Web: Word Ordering and Language Conventions in Online Text</summary>

- *Katherine Van Koevering, Austin R. Benson, Jon Kleinberg*

- `2003.03612v1` - [abs](http://arxiv.org/abs/2003.03612v1) - [pdf](http://arxiv.org/pdf/2003.03612v1)

> There is inherent information captured in the order in which we write words in a list. The orderings of binomials --- lists of two words separated by `and' or `or' --- has been studied for more than a century. These binomials are common across many areas of speech, in both formal and informal text. In the last century, numerous explanations have been given to describe what order people use for these binomials, from differences in semantics to differences in phonology. These rules describe primarily `frozen' binomials that exist in exactly one ordering and have lacked large-scale trials to determine efficacy.   Online text provides a unique opportunity to study these lists in the context of informal text at a very large scale. In this work, we expand the view of binomials to include a large-scale analysis of both frozen and non-frozen binomials in a quantitative way. Using this data, we then demonstrate that most previously proposed rules are ineffective at predicting binomial ordering. By tracking the order of these binomials across time and communities we are able to establish additional, unexplored dimensions central to these predictions.   Expanding beyond the question of individual binomials, we also explore the global structure of binomials in various communities, establishing a new model for these lists and analyzing this structure for non-frozen and frozen binomials. Additionally, novel analysis of trinomials --- lists of length three --- suggests that none of the binomials analysis applies in these cases. Finally, we demonstrate how large data sets gleaned from the web can be used in conjunction with older theories to expand and improve on old questions.

</details>

<details>

<summary>2020-03-07 17:25:21 - AlphaNet: An Attention Guided Deep Network for Automatic Image Matting</summary>

- *Rishab Sharma, Rahul Deora, Anirudha Vishvakarma*

- `2003.03613v1` - [abs](http://arxiv.org/abs/2003.03613v1) - [pdf](http://arxiv.org/pdf/2003.03613v1)

> In this paper, we propose an end to end solution for image matting i.e high-precision extraction of foreground objects from natural images. Image matting and background detection can be achieved easily through chroma keying in a studio setting when the background is either pure green or blue. Nonetheless, image matting in natural scenes with complex and uneven depth backgrounds remains a tedious task that requires human intervention. To achieve complete automatic foreground extraction in natural scenes, we propose a method that assimilates semantic segmentation and deep image matting processes into a single network to generate detailed semantic mattes for image composition task. The contribution of our proposed method is two-fold, firstly it can be interpreted as a fully automated semantic image matting method and secondly as a refinement of existing semantic segmentation models. We propose a novel model architecture as a combination of segmentation and matting that unifies the function of upsampling and downsampling operators with the notion of attention. As shown in our work, attention guided downsampling and upsampling can extract high-quality boundary details, unlike other normal downsampling and upsampling techniques. For achieving the same, we utilized an attention guided encoder-decoder framework which does unsupervised learning for generating an attention map adaptively from the data to serve and direct the upsampling and downsampling operators. We also construct a fashion e-commerce focused dataset with high-quality alpha mattes to facilitate the training and evaluation for image matting.

</details>

<details>

<summary>2020-03-07 20:14:16 - ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation</summary>

- *Nora Hollenstein, Marius Troendle, Ce Zhang, Nicolas Langer*

- `1912.00903v3` - [abs](http://arxiv.org/abs/1912.00903v3) - [pdf](http://arxiv.org/pdf/1912.00903v3)

> We recorded and preprocessed ZuCo 2.0, a new dataset of simultaneous eye-tracking and electroencephalography during natural reading and during annotation. This corpus contains gaze and brain activity data of 739 sentences, 349 in a normal reading paradigm and 390 in a task-specific paradigm, in which the 18 participants actively search for a semantic relation type in the given sentences as a linguistic annotation task. This new dataset complements ZuCo 1.0 by providing experiments designed to analyze the differences in cognitive processing between natural reading and annotation. The data is freely available here: https://osf.io/2urht/.

</details>

<details>

<summary>2020-03-08 07:15:48 - DeepLENS: Deep Learning for Entity Summarization</summary>

- *Qingxia Liu, Gong Cheng, Yuzhong Qu*

- `2003.03736v1` - [abs](http://arxiv.org/abs/2003.03736v1) - [pdf](http://arxiv.org/pdf/2003.03736v1)

> Entity summarization has been a prominent task over knowledge graphs. While existing methods are mainly unsupervised, we present DeepLENS, a simple yet effective deep learning model where we exploit textual semantics for encoding triples and we score each candidate triple based on its interdependence on other triples. DeepLENS significantly outperformed existing methods on a public benchmark.

</details>

<details>

<summary>2020-03-08 07:52:40 - Utilizing Deep Learning to Identify Drug Use on Twitter Data</summary>

- *Joseph Tassone, Peizhi Yan, Mackenzie Simpson, Chetan Mendhe, Vijay Mago, Salimur Choudhury*

- `2003.11522v1` - [abs](http://arxiv.org/abs/2003.11522v1) - [pdf](http://arxiv.org/pdf/2003.11522v1)

> The collection and examination of social media has become a useful mechanism for studying the mental activity and behavior tendencies of users. Through the analysis of collected Twitter data, models were developed for classifying drug-related tweets. Using topic pertaining keywords, such as slang and methods of drug consumption, a set of tweets was generated. Potential candidates were then preprocessed resulting in a dataset of 3,696,150 rows. The classification power of multiple methods was compared including support vector machines (SVM), XGBoost, and convolutional neural network (CNN) based classifiers. Rather than simple feature or attribute analysis, a deep learning approach was implemented to screen and analyze the tweets' semantic meaning. The two CNN-based classifiers presented the best result when compared against other methodologies. The first was trained with 2,661 manually labeled samples, while the other included synthetically generated tweets culminating in 12,142 samples. The accuracy scores were 76.35% and 82.31%, with an AUC of 0.90 and 0.91. Additionally, association rule mining showed that commonly mentioned drugs had a level of correspondence with frequently used illicit substances, proving the practical usefulness of the system. Lastly, the synthetically generated set provided increased scores, improving the classification capability and proving the worth of this methodology.

</details>

<details>

<summary>2020-03-08 14:04:23 - Dependently Typed Knowledge Graphs</summary>

- *Zhangsheng Lai, Aik Beng Ng, Liang Ze Wong, Simon See, Shaowei Lin*

- `2003.03785v1` - [abs](http://arxiv.org/abs/2003.03785v1) - [pdf](http://arxiv.org/pdf/2003.03785v1)

> Reasoning over knowledge graphs is traditionally built upon a hierarchy of languages in the Semantic Web Stack. Starting from the Resource Description Framework (RDF) for knowledge graphs, more advanced constructs have been introduced through various syntax extensions to add reasoning capabilities to knowledge graphs. In this paper, we show how standardized semantic web technologies (RDF and its query language SPARQL) can be reproduced in a unified manner with dependent type theory. In addition to providing the basic functionalities of knowledge graphs, dependent types add expressiveness in encoding both entities and queries, explainability in answers to queries through witnesses, and compositionality and automation in the construction of witnesses. Using the Coq proof assistant, we demonstrate how to build and query dependently typed knowledge graphs as a proof of concept for future works in this direction.

</details>

<details>

<summary>2020-03-09 10:58:38 - Sentence Analogies: Exploring Linguistic Relationships and Regularities in Sentence Embeddings</summary>

- *Xunjie Zhu, Gerard de Melo*

- `2003.04036v1` - [abs](http://arxiv.org/abs/2003.04036v1) - [pdf](http://arxiv.org/pdf/2003.04036v1)

> While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different models differ substantially in their ability to reflect such regularities.

</details>

<details>

<summary>2020-03-09 12:57:10 - KGvec2go -- Knowledge Graph Embeddings as a Service</summary>

- *Jan Portisch, Michael Hladik, Heiko Paulheim*

- `2003.05809v1` - [abs](http://arxiv.org/abs/2003.05809v1) - [pdf](http://arxiv.org/pdf/2003.05809v1)

> In this paper, we present KGvec2go, a Web API for accessing and consuming graph embeddings in a light-weight fashion in downstream applications. Currently, we serve pre-trained embeddings for four knowledge graphs. We introduce the service and its usage, and we show further that the trained models have semantic value by evaluating them on multiple semantic benchmarks. The evaluation also reveals that the combination of multiple models can lead to a better outcome than the best individual model.

</details>

<details>

<summary>2020-03-09 15:04:07 - Neuro-symbolic Architectures for Context Understanding</summary>

- *Alessandro Oltramari, Jonathan Francis, Cory Henson, Kaixin Ma, Ruwan Wickramarachchi*

- `2003.04707v1` - [abs](http://arxiv.org/abs/2003.04707v1) - [pdf](http://arxiv.org/pdf/2003.04707v1)

> Computational context understanding refers to an agent's ability to fuse disparate sources of information for decision-making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (AI). Data-driven and knowledge-driven methods are two classical techniques in the pursuit of such machine sense-making capability. However, while data-driven methods seek to model the statistical regularities of events by making observations in the real-world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge. Conversely, knowledge-driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference. To combat these issues, we propose the use of hybrid AI methodology as a general framework for combining the strengths of both approaches. Specifically, we inherit the concept of neuro-symbolism as a way of using knowledge-bases to guide the learning progress of deep neural networks. We further ground our discussion in two applications of neuro-symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state-of-the-art.

</details>

<details>

<summary>2020-03-09 16:12:39 - Z-Net: an Anisotropic 3D DCNN for Medical CT Volume Segmentation</summary>

- *Peichao Li, Xiao-Yun Zhou, Zhao-Yang Wang, Guang-Zhong Yang*

- `1909.07480v2` - [abs](http://arxiv.org/abs/1909.07480v2) - [pdf](http://arxiv.org/pdf/1909.07480v2)

> Accurate volume segmentation from the Computed Tomography (CT) scan is a common prerequisite for pre-operative planning, intra-operative guidance and quantitative assessment of therapeutic outcomes in robot-assisted Minimally Invasive Surgery (MIS). 3D Deep Convolutional Neural Network (DCNN) is a viable solution for this task, but is memory intensive. Small isotropic patches are cropped from the original and large CT volume to mitigate this issue in practice, but it may cause discontinuities between the adjacent patches and severe class-imbalances within individual sub-volumes. This paper presents a new 3D DCNN framework, namely Z-Net, to tackle the discontinuity and class-imbalance issue by preserving a full field-of-view of the objects in the XY planes using anisotropic spatial separable convolutions. The proposed Z-Net can be seamlessly integrated into existing 3D DCNNs with isotropic convolutions such as 3D U-Net and V-Net, with improved volume segmentation Intersection over Union (IoU) - up to $12.6\%$. Detailed validation of Z-Net is provided for CT aortic, liver and lung segmentation, demonstrating the effectiveness and practical value of Z-Net for intra-operative 3D navigation in robot-assisted MIS.

</details>

<details>

<summary>2020-03-09 20:33:30 - FusionLane: Multi-Sensor Fusion for Lane Marking Semantic Segmentation Using Deep Neural Networks</summary>

- *Ruochen Yin, Biao Yu, Huapeng Wu, Yutao Song, Runxin Niu*

- `2003.04404v1` - [abs](http://arxiv.org/abs/2003.04404v1) - [pdf](http://arxiv.org/pdf/2003.04404v1)

> It is a crucial step to achieve effective semantic segmentation of lane marking during the construction of the lane level high-precision map. In recent years, many image semantic segmentation methods have been proposed. These methods mainly focus on the image from camera, due to the limitation of the sensor itself, the accurate three-dimensional spatial position of the lane marking cannot be obtained, so the demand for the lane level high-precision map construction cannot be met. This paper proposes a lane marking semantic segmentation method based on LIDAR and camera fusion deep neural network. Different from other methods, in order to obtain accurate position information of the segmentation results, the semantic segmentation object of this paper is a bird's eye view converted from a LIDAR points cloud instead of an image captured by a camera. This method first uses the deeplabv3+ [\ref{ref:1}] network to segment the image captured by the camera, and the segmentation result is merged with the point clouds collected by the LIDAR as the input of the proposed network. In this neural network, we also add a long short-term memory (LSTM) structure to assist the network for semantic segmentation of lane markings by using the the time series information. The experiments on more than 14,000 image datasets which we have manually labeled and expanded have shown the proposed method has better performance on the semantic segmentation of the points cloud bird's eye view. Therefore, the automation of high-precision map construction can be significantly improved. Our code is available at https://github.com/rolandying/FusionLane.

</details>

<details>

<summary>2020-03-10 00:19:49 - An abstract semantics of speculative execution for reasoning about security vulnerabilities</summary>

- *Robert J. Colvin, Kirsten Winter*

- `2004.00577v1` - [abs](http://arxiv.org/abs/2004.00577v1) - [pdf](http://arxiv.org/pdf/2004.00577v1)

> Reasoning about correctness and security of software is increasingly difficult due to the complexity of modern microarchitectural features such as out-of-order execution. A class of security vulnerabilities termed Spectre that exploits side effects of speculative, out-of-order execution was announced in 2018 and has since drawn much attention. In this paper we formalise speculative execution and its side effects with the intention of allowing speculation to be reasoned about abstractly at the program level, limiting the exposure to processor-specific or low-level semantics. To this end we encode and expose speculative execution explicitly in the programming language, rather than solely in the operational semantics; as a result the effects of speculative execution are captured by redefining the meaning of a conditional statement, and introducing novel language constructs that model transient execution of an alternative branch. We add an abstract cache to the global state of the system, and derive some general refinement rules that expose cache side effects due to speculative loads. Underlying this extension is a semantic model that is based on instruction-level parallelism. The rules are encoded in a simulation tool, which we use to analyse an abstract specification of a Spectre attack and vulnerable code fragments.

</details>

<details>

<summary>2020-03-10 11:36:42 - HeatNet: Bridging the Day-Night Domain Gap in Semantic Segmentation with Thermal Images</summary>

- *Johan Vertens, Jannik Zürn, Wolfram Burgard*

- `2003.04645v1` - [abs](http://arxiv.org/abs/2003.04645v1) - [pdf](http://arxiv.org/pdf/2003.04645v1)

> The majority of learning-based semantic segmentation methods are optimized for daytime scenarios and favorable lighting conditions. Real-world driving scenarios, however, entail adverse environmental conditions such as nighttime illumination or glare which remain a challenge for existing approaches. In this work, we propose a multimodal semantic segmentation model that can be applied during daytime and nighttime. To this end, besides RGB images, we leverage thermal images, making our network significantly more robust. We avoid the expensive annotation of nighttime images by leveraging an existing daytime RGB-dataset and propose a teacher-student training approach that transfers the dataset's knowledge to the nighttime domain. We further employ a domain adaptation method to align the learned feature spaces across the domains and propose a novel two-stage training scheme. Furthermore, due to a lack of thermal data for autonomous driving, we present a new dataset comprising over 20,000 time-synchronized and aligned RGB-thermal image pairs. In this context, we also present a novel target-less calibration method that allows for automatic robust extrinsic and intrinsic thermal camera calibration. Among others, we employ our new dataset to show state-of-the-art results for nighttime semantic segmentation.

</details>

<details>

<summary>2020-03-10 13:10:26 - Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog</summary>

- *Shen Gao, Xiuying Chen, Chang Liu, Li Liu, Dongyan Zhao, Rui Yan*

- `2003.04679v1` - [abs](http://arxiv.org/abs/2003.04679v1) - [pdf](http://arxiv.org/pdf/2003.04679v1)

> Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching text labels of stickers with previous utterances. However, due to their large quantities, it is impractical to require text labels for the all stickers. Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context history without any external labels. Two main challenges are confronted in this task. One is to learn semantic meaning of stickers without corresponding text labels. Another challenge is to jointly model the candidate sticker with the multi-turn dialog context. To tackle these challenges, we propose a sticker response selector (SRS) model. Specifically, SRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker with each utterance in the dialog history. SRS then learns the short-term and long-term dependency between all interaction results by a fusion network to output the the final matching score. To evaluate our proposed method, we collect a large-scale real-world dialog dataset with stickers from one of the most popular online chatting platform. Extensive experiments conducted on this dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics. Experiments also verify the effectiveness of each component of SRS. To facilitate further research in sticker selection field, we release this dataset of 340K multi-turn dialog and sticker pairs.

</details>

<details>

<summary>2020-03-10 13:21:57 - Continual Local Replacement for Few-shot Learning</summary>

- *Canyu Le, Zhonggui Chen, Xihan Wei, Biao Wang, Lei Zhang*

- `2001.08366v2` - [abs](http://arxiv.org/abs/2001.08366v2) - [pdf](http://arxiv.org/pdf/2001.08366v2)

> The goal of few-shot learning is to learn a model that can recognize novel classes based on one or few training data. It is challenging mainly due to two aspects: (1) it lacks good feature representation of novel classes; (2) a few of labeled data could not accurately represent the true data distribution and thus it's hard to learn a good decision function for classification. In this work, we use a sophisticated network architecture to learn better feature representation and focus on the second issue. A novel continual local replacement strategy is proposed to address the data deficiency problem. It takes advantage of the content in unlabeled images to continually enhance labeled ones. Specifically, a pseudo labeling method is adopted to constantly select semantically similar images on the fly. Original labeled images will be locally replaced by the selected images for the next epoch training. In this way, the model can directly learn new semantic information from unlabeled images and the capacity of supervised signals in the embedding space can be significantly enlarged. This allows the model to improve generalization and learn a better decision boundary for classification. Our method is conceptually simple and easy to implement. Extensive experiments demonstrate that it can achieve state-of-the-art results on various few-shot image recognition benchmarks.

</details>

<details>

<summary>2020-03-10 17:17:01 - Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity</summary>

- *Ivan Vulić, Simon Baker, Edoardo Maria Ponti, Ulla Petti, Ira Leviant, Kelly Wing, Olga Majewska, Eden Bar, Matt Malone, Thierry Poibeau, Roi Reichart, Anna Korhonen*

- `2003.04866v1` - [abs](http://arxiv.org/abs/2003.04866v1) - [pdf](http://arxiv.org/pdf/2003.04866v1)

> We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering datasets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 cross-lingual semantic similarity datasets. Due to its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and cross-lingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and cross-lingual representation models, including static and contextualized word embeddings (such as fastText, M-BERT and XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised cross-lingual word embeddings. We also present a step-by-step dataset creation protocol for creating consistent, Multi-Simlex-style resources for additional languages. We make these contributions -- the public release of Multi-SimLex datasets, their creation protocol, strong baseline results, and in-depth analyses which can be be helpful in guiding future developments in multilingual lexical semantics and representation learning -- available via a website which will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.

</details>

<details>

<summary>2020-03-10 22:07:34 - Text classification with word embedding regularization and soft similarity measure</summary>

- *Vít Novotný, Eniafe Festus Ayetiran, Michal Štefánik, Petr Sojka*

- `2003.05019v1` - [abs](http://arxiv.org/abs/2003.05019v1) - [pdf](http://arxiv.org/pdf/2003.05019v1)

> Since the seminal work of Mikolov et al., word embeddings have become the preferred word representations for many natural language processing tasks. Document similarity measures extracted from word embeddings, such as the soft cosine measure (SCM) and the Word Mover's Distance (WMD), were reported to achieve state-of-the-art performance on semantic text similarity and text classification.   Despite the strong performance of the WMD on text classification and semantic text similarity, its super-cubic average time complexity is impractical. The SCM has quadratic worst-case time complexity, but its performance on text classification has never been compared with the WMD. Recently, two word embedding regularization techniques were shown to reduce storage and memory costs, and to improve training speed, document processing speed, and task performance on word analogy, word similarity, and semantic text similarity. However, the effect of these techniques on text classification has not yet been studied.   In our work, we investigate the individual and joint effect of the two word embedding regularization techniques on the document processing speed and the task performance of the SCM and the WMD on text classification. For evaluation, we use the $k$NN classifier and six standard datasets: BBCSPORT, TWITTER, OHSUMED, REUTERS-21578, AMAZON, and 20NEWS.   We show 39% average $k$NN test error reduction with regularized word embeddings compared to non-regularized word embeddings. We describe a practical procedure for deriving such regularized embeddings through Cholesky factorization. We also show that the SCM with regularized word embeddings significantly outperforms the WMD on text classification and is over 10,000 times faster.

</details>

<details>

<summary>2020-03-11 07:48:46 - Entity Extraction from Wikipedia List Pages</summary>

- *Nicolas Heist, Heiko Paulheim*

- `2003.05146v1` - [abs](http://arxiv.org/abs/2003.05146v1) - [pdf](http://arxiv.org/pdf/2003.05146v1)

> When it comes to factual knowledge about a wide range of domains, Wikipedia is often the prime source of information on the web. DBpedia and YAGO, as large cross-domain knowledge graphs, encode a subset of that knowledge by creating an entity for each page in Wikipedia, and connecting them through edges. It is well known, however, that Wikipedia-based knowledge graphs are far from complete. Especially, as Wikipedia's policies permit pages about subjects only if they have a certain popularity, such graphs tend to lack information about less well-known entities. Information about these entities is oftentimes available in the encyclopedia, but not represented as an individual page. In this paper, we present a two-phased approach for the extraction of entities from Wikipedia's list pages, which have proven to serve as a valuable source of information. In the first phase, we build a large taxonomy from categories and list pages with DBpedia as a backbone. With distant supervision, we extract training data for the identification of new entities in list pages that we use in the second phase to train a classification model. With this approach we extract over 700k new entities and extend DBpedia with 7.5M new type statements and 3.8M new facts of high precision.

</details>

<details>

<summary>2020-03-11 09:42:39 - Scan Correlation -- Revealing distributed scan campaigns</summary>

- *Steffen Haas, Florian Wilkens, Mathias Fischer*

- `2003.05188v1` - [abs](http://arxiv.org/abs/2003.05188v1) - [pdf](http://arxiv.org/pdf/2003.05188v1)

> Public networks are exposed to port scans from the Internet. Attackers search for vulnerable services they can exploit. In large scan campaigns, attackers often utilize different machines to perform distributed scans, which impedes their detection and might also camouflage the actual goal of the scanning campaign. In this paper, we present a correlation algorithm to detect scans, identify potential relations among them, and reassemble them to larger campaigns. We evaluate our approach on real-world Internet traffic and our results indicate that it can summarize and characterize standalone and distributed scan campaigns based on their tools and intention.

</details>

<details>

<summary>2020-03-11 10:18:32 - Keyword-Attentive Deep Semantic Matching</summary>

- *Changyu Miao, Zhen Cao, Yik-Cheung Tam*

- `2003.11516v1` - [abs](http://arxiv.org/abs/2003.11516v1) - [pdf](http://arxiv.org/pdf/2003.11516v1)

> Deep Semantic Matching is a crucial component in various natural language processing applications such as question and answering (QA), where an input query is compared to each candidate question in a QA corpus in terms of relevance. Measuring similarities between a query-question pair in an open domain scenario can be challenging due to diverse word tokens in the queryquestion pair. We propose a keyword-attentive approach to improve deep semantic matching. We first leverage domain tags from a large corpus to generate a domain-enhanced keyword dictionary. Built upon BERT, we stack a keyword-attentive transformer layer to highlight the importance of keywords in the query-question pair. During model training, we propose a new negative sampling approach based on keyword coverage between the input pair. We evaluate our approach on a Chinese QA corpus using various metrics, including precision of retrieval candidates and accuracy of semantic matching. Experiments show that our approach outperforms existing strong baselines. Our approach is general and can be applied to other text matching tasks with little adaptation.

</details>

<details>

<summary>2020-03-11 12:35:52 - Expressiveness and machine processability of Knowledge Organization Systems (KOS): An analysis of concepts and relations</summary>

- *Manolis Peponakis, Anna Mastora, Sarantos Kapidakis, Martin Doerr*

- `2003.05258v1` - [abs](http://arxiv.org/abs/2003.05258v1) - [pdf](http://arxiv.org/pdf/2003.05258v1)

> This study considers the expressiveness (that is the expressive power or expressivity) of different types of Knowledge Organization Systems (KOS) and discusses its potential to be machine-processable in the context of the Semantic Web. For this purpose, the theoretical foundations of KOS are reviewed based on conceptualizations introduced by the Functional Requirements for Subject Authority Data (FRSAD) and the Simple Knowledge Organization System (SKOS); natural language processing techniques are also implemented. Applying a comparative analysis, the dataset comprises a thesaurus (Eurovoc), a subject headings system (LCSH) and a classification scheme (DDC). These are compared with an ontology (CIDOC-CRM) by focusing on how they define and handle concepts and relations. It was observed that LCSH and DDC focus on the formalism of character strings (nomens) rather than on the modelling of semantics; their definition of what constitutes a concept is quite fuzzy, and they comprise a large number of complex concepts. By contrast, thesauri have a coherent definition of what constitutes a concept, and apply a systematic approach to the modelling of relations. Ontologies explicitly define diverse types of relations, and are by their nature machine-processable. The paper concludes that the potential of both the expressiveness and machine processability of each KOS is extensively regulated by its structural rules. It is harder to represent subject headings and classification schemes as semantic networks with nodes and arcs, while thesauri are more suitable for such a representation. In addition, a paradigm shift is revealed which focuses on the modelling of relations between concepts, rather than the concepts themselves.

</details>

<details>

<summary>2020-03-11 13:27:45 - Deep learning networks for selection of persistent scatterer pixels in multi-temporal SAR interferometric processing</summary>

- *Ashutosh Tiwari, Avadh Bihari Narayan, Onkar Dikshit*

- `1909.01868v3` - [abs](http://arxiv.org/abs/1909.01868v3) - [pdf](http://arxiv.org/pdf/1909.01868v3)

> In multi-temporal SAR interferometry (MT-InSAR), persistent scatterer (PS) pixels are used to estimate geophysical parameters, essentially deformation. Conventionally, PS pixels are selected on the basis of the estimated noise present in the spatially uncorrelated phase component along with look-angle error in a temporal interferometric stack. In this study, two deep learning architectures, namely convolutional neural network for interferometric semantic segmentation (CNN-ISS) and convolutional long short term memory network for interferometric semantic segmentation (CLSTM-ISS), based on learning spatial and spatio-temporal behaviour respectively, were proposed for selection of PS pixels. These networks were trained to relate the interferometric phase history to its classification into phase stable (PS) and phase unstable (non-PS) measurement pixels using ~10,000 real world interferometric images of different study sites containing man-made objects, forests, vegetation, uncropped land, water bodies, and areas affected by lengthening, foreshortening, layover and shadowing. The networks were trained using training labels obtained from the Stanford method for Persistent Scatterer Interferometry (StaMPS) algorithm. However, pixel selection results, when compared to a combination of R-index and a classified image of the test dataset, reveal that CLSTM-ISS estimates improved the classification of PS and non-PS pixels compared to those of StaMPS and CNN-ISS. The predicted results show that CLSTM-ISS reached an accuracy of 93.50%, higher than that of CNN-ISS (89.21%). CLSTM-ISS also improved the density of reliable PS pixels compared to StaMPS and CNN-ISS and outperformed StaMPS and other conventional MT-InSAR methods in terms of computational efficiency.

</details>

<details>

<summary>2020-03-11 21:04:49 - Semantic Holism and Word Representations in Artificial Neural Networks</summary>

- *Tomáš Musil*

- `2003.05522v1` - [abs](http://arxiv.org/abs/2003.05522v1) - [pdf](http://arxiv.org/pdf/2003.05522v1)

> Artificial neural networks are a state-of-the-art solution for many problems in natural language processing. What can we learn about language and meaning from the way artificial neural networks represent it? Word representations obtained from the Skip-gram variant of the word2vec model exhibit interesting semantic properties. This is usually explained by referring to the general distributional hypothesis, which states that the meaning of the word is given by the contexts where it occurs. We propose a more specific approach based on Frege's holistic and functional approach to meaning. Taking Tugendhat's formal reinterpretation of Frege's work as a starting point, we demonstrate that it is analogical to the process of training the Skip-gram model and offers a possible explanation of its semantic properties.

</details>

<details>

<summary>2020-03-12 02:54:11 - Learning Conceptual-Contextual Embeddings for Medical Text</summary>

- *Xiao Zhang, Dejing Dou, Ji Wu*

- `1908.06203v3` - [abs](http://arxiv.org/abs/1908.06203v3) - [pdf](http://arxiv.org/pdf/1908.06203v3)

> External knowledge is often useful for natural language understanding tasks. We introduce a contextual text representation model called Conceptual-Contextual (CC) embeddings, which incorporates structured knowledge into text representations. Unlike entity embedding methods, our approach encodes a knowledge graph into a context model. CC embeddings can be easily reused for a wide range of tasks just like pre-trained language models. Our model effectively encodes the huge UMLS database by leveraging semantic generalizability. Experiments on electronic health records (EHRs) and medical text processing benchmarks showed our model gives a major boost to the performance of supervised medical NLP tasks.

</details>

<details>

<summary>2020-03-12 05:03:46 - CC2Vec: Distributed Representations of Code Changes</summary>

- *Thong Hoang, Hong Jin Kang, Julia Lawall, David Lo*

- `2003.05620v1` - [abs](http://arxiv.org/abs/2003.05620v1) - [pdf](http://arxiv.org/pdf/2003.05620v1)

> Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.   To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.

</details>

<details>

<summary>2020-03-12 07:04:28 - Interpretable CNNs for Object Classification</summary>

- *Quanshi Zhang, Xin Wang, Ying Nian Wu, Huilin Zhou, Song-Chun Zhu*

- `1901.02413v2` - [abs](http://arxiv.org/abs/1901.02413v2) - [pdf](http://arxiv.org/pdf/1901.02413v2)

> This paper proposes a generic method to learn interpretable convolutional filters in a deep convolutional neural network (CNN) for object classification, where each interpretable filter encodes features of a specific object part. Our method does not require additional annotations of object parts or textures for supervision. Instead, we use the same training data as traditional CNNs. Our method automatically assigns each interpretable filter in a high conv-layer with an object part of a certain category during the learning process. Such explicit knowledge representations in conv-layers of CNN help people clarify the logic encoded in the CNN, i.e., answering what patterns the CNN extracts from an input image and uses for prediction. We have tested our method using different benchmark CNNs with various structures to demonstrate the broad applicability of our method. Experiments have shown that our interpretable filters are much more semantically meaningful than traditional filters.

</details>

<details>

<summary>2020-03-12 10:22:43 - Unsupervised Domain Adaptation for Mobile Semantic Segmentation based on Cycle Consistency and Feature Alignment</summary>

- *Marco Toldo, Umberto Michieli, Gianluca Agresti, Pietro Zanuttigh*

- `2001.04692v2` - [abs](http://arxiv.org/abs/2001.04692v2) - [pdf](http://arxiv.org/pdf/2001.04692v2)

> The supervised training of deep networks for semantic segmentation requires a huge amount of labeled real world data. To solve this issue, a commonly exploited workaround is to use synthetic data for training, but deep networks show a critical performance drop when analyzing data with slightly different statistical properties with respect to the training set. In this work, we propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the domain shift issue between real world and synthetic representations. An adversarial model, based on the cycle consistency framework, performs the mapping between the synthetic and real domain. The data is then fed to a MobileNet-v2 architecture that performs the semantic segmentation task. An additional couple of discriminators, working at the feature level of the MobileNet-v2, allows to better align the features of the two domain distributions and to further improve the performance. Finally, the consistency of the semantic maps is exploited. After an initial supervised training on synthetic data, the whole UDA architecture is trained end-to-end considering all its components at once. Experimental results show how the proposed strategy is able to obtain impressive performance in adapting a segmentation network trained on synthetic data to real world scenarios. The usage of the lightweight MobileNet-v2 architecture allows its deployment on devices with limited computational resources as the ones employed in autonomous vehicles.

</details>

<details>

<summary>2020-03-12 10:31:25 - Knowledge Graphs on the Web -- an Overview</summary>

- *Nicolas Heist, Sven Hertling, Daniel Ringler, Heiko Paulheim*

- `2003.00719v3` - [abs](http://arxiv.org/abs/2003.00719v3) - [pdf](http://arxiv.org/pdf/2003.00719v3)

> Knowledge Graphs are an emerging form of knowledge representation. While Google coined the term Knowledge Graph first and promoted it as a means to improve their search results, they are used in many applications today. In a knowledge graph, entities in the real world and/or a business domain (e.g., people, places, or events) are represented as nodes, which are connected by edges representing the relations between those entities. While companies such as Google, Microsoft, and Facebook have their own, non-public knowledge graphs, there is also a larger body of publicly available knowledge graphs, such as DBpedia or Wikidata. In this chapter, we provide an overview and comparison of those publicly available knowledge graphs, and give insights into their contents, size, coverage, and overlap.

</details>

<details>

<summary>2020-03-12 18:04:20 - Post-Estimation Smoothing: A Simple Baseline for Learning with Side Information</summary>

- *Esther Rolf, Michael I. Jordan, Benjamin Recht*

- `2003.05955v1` - [abs](http://arxiv.org/abs/2003.05955v1) - [pdf](http://arxiv.org/pdf/2003.05955v1)

> Observational data are often accompanied by natural structural indices, such as time stamps or geographic locations, which are meaningful to prediction tasks but are often discarded. We leverage semantically meaningful indexing data while ensuring robustness to potentially uninformative or misleading indices. We propose a post-estimation smoothing operator as a fast and effective method for incorporating structural index data into prediction. Because the smoothing step is separate from the original predictor, it applies to a broad class of machine learning tasks, with no need to retrain models. Our theoretical analysis details simple conditions under which post-estimation smoothing will improve accuracy over that of the original predictor. Our experiments on large scale spatial and temporal datasets highlight the speed and accuracy of post-estimation smoothing in practice. Together, these results illuminate a novel way to consider and incorporate the natural structure of index variables in machine learning.

</details>

<details>

<summary>2020-03-12 18:52:43 - Securing Vehicle-to-Everything (V2X) Communication Platforms</summary>

- *Monowar Hasan, Sibin Mohan, Takayuki Shimizu, Hongsheng Lu*

- `2003.07191v1` - [abs](http://arxiv.org/abs/2003.07191v1) - [pdf](http://arxiv.org/pdf/2003.07191v1)

> Modern vehicular wireless technology enables vehicles to exchange information at any time, from any place, to any network -- forms the vehicle-to-everything (V2X) communication platforms. Despite benefits, V2X applications also face great challenges to security and privacy -- a very valid concern since breaches are not uncommon in automotive communication networks and applications. In this survey, we provide an extensive overview of V2X ecosystem. We also review main security/privacy issues, current standardization activities and existing defense mechanisms proposed within the V2X domain. We then identified semantic gaps of existing security solutions and outline possible open issues.

</details>

<details>

<summary>2020-03-12 22:26:11 - Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition</summary>

- *Zhigang Dai, Jinhua Fu, Qile Zhu, Hengbin Cui, Xiaolong li, Yuan Qi*

- `2003.06044v1` - [abs](http://arxiv.org/abs/2003.06044v1) - [pdf](http://arxiv.org/pdf/2003.06044v1)

> Dialogue act recognition is a fundamental task for an intelligent dialogue system. Previous work models the whole dialog to predict dialog acts, which may bring the noise from unrelated sentences. In this work, we design a hierarchical model based on self-attention to capture intra-sentence and inter-sentence information. We revise the attention distribution to focus on the local and contextual semantic information by incorporating the relative position information between utterances. Based on the found that the length of dialog affects the performance, we introduce a new dialog segmentation mechanism to analyze the effect of dialog length and context padding length under online and offline settings. The experiment shows that our method achieves promising performance on two datasets: Switchboard Dialogue Act and DailyDialog with the accuracy of 80.34\% and 85.81\% respectively. Visualization of the attention weights shows that our method can learn the context dependency between utterances explicitly.

</details>

<details>

<summary>2020-03-13 04:32:11 - BigGAN-based Bayesian reconstruction of natural images from human brain activity</summary>

- *Kai Qiao, Jian Chen, Linyuan Wang, Chi Zhang, Li Tong, Bin Yan*

- `2003.06105v1` - [abs](http://arxiv.org/abs/2003.06105v1) - [pdf](http://arxiv.org/pdf/2003.06105v1)

> In the visual decoding domain, visually reconstructing presented images given the corresponding human brain activity monitored by functional magnetic resonance imaging (fMRI) is difficult, especially when reconstructing viewed natural images. Visual reconstruction is a conditional image generation on fMRI data and thus generative adversarial network (GAN) for natural image generation is recently introduced for this task. Although GAN-based methods have greatly improved, the fidelity and naturalness of reconstruction are still unsatisfactory due to the small number of fMRI data samples and the instability of GAN training. In this study, we proposed a new GAN-based Bayesian visual reconstruction method (GAN-BVRM) that includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluator to evaluate generated images. GAN-BVRM employs the pre-trained generator of the prevailing BigGAN to generate masses of natural images, and selects the images that best matches with the corresponding brain activity through the encoding models as the reconstruction of the image stimuli. In this process, the semantic and detailed contents of reconstruction are controlled by decoded categories and encoding models, respectively. GAN-BVRM used the Bayesian manner to avoid contradiction between naturalness and fidelity from current GAN-based methods and thus can improve the advantages of GAN. Experimental results revealed that GAN-BVRM improves the fidelity and naturalness, that is, the reconstruction is natural and similar to the presented image stimuli.

</details>

<details>

<summary>2020-03-13 12:42:54 - A Uniform Treatment of Aggregates and Constraints in Hybrid ASP</summary>

- *Pedro Cabalar, Jorge Fandinno, Torsten Schaub, Philipp Wanko*

- `2003.04176v2` - [abs](http://arxiv.org/abs/2003.04176v2) - [pdf](http://arxiv.org/pdf/2003.04176v2)

> Characterizing hybrid ASP solving in a generic way is difficult since one needs to abstract from specific theories. Inspired by lazy SMT solving, this is usually addressed by treating theory atoms as opaque. Unlike this, we propose a slightly more transparent approach that includes an abstract notion of a term. Rather than imposing a syntax on terms, we keep them abstract by stipulating only some basic properties. With this, we further develop a semantic framework for hybrid ASP solving and provide aggregate functions for theory variables that adhere to different semantic principles, show that they generalize existing aggregate semantics in ASP and how we can rely on off-the-shelf hybrid solvers for implementation.

</details>

<details>

<summary>2020-03-13 13:11:35 - Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems</summary>

- *Patrick Knöbelreiter, Christian Sormann, Alexander Shekhovtsov, Friedrich Fraundorfer, Thomas Pock*

- `2003.06258v1` - [abs](http://arxiv.org/abs/2003.06258v1) - [pdf](http://arxiv.org/pdf/2003.06258v1)

> It has been proposed by many researchers that combining deep neural networks with graphical models can create more efficient and better regularized composite models. The main difficulties in implementing this in practice are associated with a discrepancy in suitable learning objectives as well as with the necessity of approximations for the inference. In this work we take one of the simplest inference methods, a truncated max-product Belief Propagation, and add what is necessary to make it a proper component of a deep learning model: We connect it to learning formulations with losses on marginals and compute the backprop operation. This BP-Layer can be used as the final or an intermediate block in convolutional neural networks (CNNs), allowing us to design a hierarchical model composing BP inference and CNNs at different scale levels. The model is applicable to a range of dense prediction problems, is well-trainable and provides parameter-efficient and robust solutions in stereo, optical flow and semantic segmentation.

</details>

<details>

<summary>2020-03-13 20:05:38 - ASR Error Correction and Domain Adaptation Using Machine Translation</summary>

- *Anirudh Mani, Shruti Palaskar, Nimshi Venkat Meripo, Sandeep Konam, Florian Metze*

- `2003.07692v1` - [abs](http://arxiv.org/abs/2003.07692v1) - [pdf](http://arxiv.org/pdf/2003.07692v1)

> Off-the-shelf pre-trained Automatic Speech Recognition (ASR) systems are an increasingly viable service for companies of any size building speech-based products. While these ASR systems are trained on large amounts of data, domain mismatch is still an issue for many such parties that want to use this service as-is leading to not so optimal results for their task. We propose a simple technique to perform domain adaptation for ASR error correction via machine translation. The machine translation model is a strong candidate to learn a mapping from out-of-domain ASR errors to in-domain terms in the corresponding reference files. We use two off-the-shelf ASR systems in this work: Google ASR (commercial) and the ASPIRE model (open-source). We observe 7% absolute improvement in word error rate and 4 point absolute improvement in BLEU score in Google ASR output via our proposed method. We also evaluate ASR error correction via a downstream task of Speaker Diarization that captures speaker style, syntax, structure and semantic improvements we obtain via ASR correction.

</details>

<details>

<summary>2020-03-13 22:24:14 - LSCP: Enhanced Large Scale Colloquial Persian Language Understanding</summary>

- *Hadi Abdi Khojasteh, Ebrahim Ansari, Mahdi Bohlouli*

- `2003.06499v1` - [abs](http://arxiv.org/abs/2003.06499v1) - [pdf](http://arxiv.org/pdf/2003.06499v1)

> Language recognition has been significantly advanced in recent years by means of modern machine learning methods such as deep learning and benchmarks with rich annotations. However, research is still limited in low-resource formal languages. This consists of a significant gap in describing the colloquial language especially for low-resourced ones such as Persian. In order to target this gap for low resource languages, we propose a "Large Scale Colloquial Persian Dataset" (LSCP). LSCP is hierarchically organized in a semantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. This encompasses the recognition of multiple semantic aspects in the human-level sentences, which naturally captures from the real-world sentences. We believe that further investigations and processing, as well as the application of novel algorithms and methods, can strengthen enriching computerized understanding and processing of low resource languages. The proposed corpus consists of 120M sentences resulted from 27M tweets annotated with parsing tree, part-of-speech tags, sentiment polarity and translation in five different languages.

</details>

<details>

<summary>2020-03-14 01:25:46 - Boundary Guidance Hierarchical Network for Real-Time Tongue Segmentation</summary>

- *Xinyi Zeng, Qian Zhang, Jia Chen, Guixu Zhang, Aimin Zhou, Yiqin Wang*

- `2003.06529v1` - [abs](http://arxiv.org/abs/2003.06529v1) - [pdf](http://arxiv.org/pdf/2003.06529v1)

> Automated tongue image segmentation in tongue images is a challenging task for two reasons: 1) there are many pathological details on the tongue surface, which affect the extraction of the boundary; 2) the shapes of the tongues captured from various persons (with different diseases) are quite different. To deal with the challenge, a novel end-to-end Boundary Guidance Hierarchical Network (BGHNet) with a new hybrid loss is proposed in this paper. In the new approach, firstly Context Feature Encoder Module (CFEM) is built upon the bottomup pathway to confront with the shrinkage of the receptive field. Secondly, a novel hierarchical recurrent feature fusion module (HRFFM) is adopt to progressively and hierarchically refine object maps to recover image details by integrating local context information. Finally, the proposed hybrid loss in a four hierarchy-pixel, patch, map and boundary guides the network to effectively segment the tongue regions and accurate tongue boundaries. BGHNet is applied to a set of tongue images. The experimental results suggest that the proposed approach can achieve the latest tongue segmentation performance. And in the meantime, the lightweight network contains only 15.45M parameters and performs only 11.22GFLOPS.

</details>

<details>

<summary>2020-03-14 06:16:30 - Semantically-Enriched Search Engine for Geoportals: A Case Study with ArcGIS Online</summary>

- *Gengchen Mai, Krzysztof Janowicz, Sathya Prasad, Meilin Shi, Ling Cai, Rui Zhu, Blake Regalia, Ni Lao*

- `2003.06561v1` - [abs](http://arxiv.org/abs/2003.06561v1) - [pdf](http://arxiv.org/pdf/2003.06561v1)

> Many geoportals such as ArcGIS Online are established with the goal of improving geospatial data reusability and achieving intelligent knowledge discovery. However, according to previous research, most of the existing geoportals adopt Lucene-based techniques to achieve their core search functionality, which has a limited ability to capture the user's search intentions. To better understand a user's search intention, query expansion can be used to enrich the user's query by adding semantically similar terms. In the context of geoportals and geographic information retrieval, we advocate the idea of semantically enriching a user's query from both geospatial and thematic perspectives. In the geospatial aspect, we propose to enrich a query by using both place partonomy and distance decay. In terms of the thematic aspect, concept expansion and embedding-based document similarity are used to infer the implicit information hidden in a user's query. This semantic query expansion 1 2 G. Mai et al. framework is implemented as a semantically-enriched search engine using ArcGIS Online as a case study. A benchmark dataset is constructed to evaluate the proposed framework. Our evaluation results show that the proposed semantic query expansion framework is very effective in capturing a user's search intention and significantly outperforms a well-established baseline-Lucene's practical scoring function-with more than 3.0 increments in DCG@K (K=3,5,10).

</details>

<details>

<summary>2020-03-14 19:44:44 - Shorter Labels for Routing in Trees</summary>

- *Paweł Gawrychowski, Wojciech Janczewski, Jakub Łopuszański*

- `2003.06691v1` - [abs](http://arxiv.org/abs/2003.06691v1) - [pdf](http://arxiv.org/pdf/2003.06691v1)

> A routing labeling scheme assigns a binary string, called a label, to each node in a network, and chooses a distinct port number from $\{1,\ldots,d\}$ for every edge outgoing from a node of degree $d$. Then, given the labels of $u$ and $w$ and no other information about the network, it should be possible to determine the port number corresponding to the first edge on the shortest path from $u$ to $w$. In their seminal paper, Thorup and Zwick [SPAA 2001] designed several routing methods for general weighted networks. An important technical ingredient in their paper that according to the authors ``may be of independent practical and theoretical interest'' is a routing labeling scheme for trees of arbitrary degrees. For a tree on $n$ nodes, their scheme constructs labels consisting of $(1+o(1))\log n$ bits such that the sought port number can be computed in constant time. Looking closer at their construction, the labels consist of $\log n + O(\log n\cdot \log\log\log n / \log\log n)$ bits. Given that the only known lower bound is $\log n+\Omega(\log\log n)$, a natural question that has been asked for other labeling problems in trees is to determine the asymptotics of the smaller-order term.   We make the first (and significant) progress in 19 years on determining the correct second-order term for the length of a label in a routing labeling scheme for trees on $n$ nodes. We design such a scheme with labels of length $\log n+O((\log\log n)^{2})$. Furthermore, we modify the scheme to allow for computing the port number in constant time at the expense of slightly increasing the length to $\log n+O((\log\log n)^{3})$.

</details>

<details>

<summary>2020-03-15 11:53:51 - Exploring and Distilling Cross-Modal Information for Image Captioning</summary>

- *Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Kai Lei, Xu Sun*

- `2002.12585v2` - [abs](http://arxiv.org/abs/2002.12585v2) - [pdf](http://arxiv.org/pdf/2002.12585v2)

> Recently, attention-based encoder-decoder models have been used extensively in image captioning. Yet there is still great difficulty for the current methods to achieve deep image understanding. In this work, we argue that such understanding requires visual attention to correlated image regions and semantic attention to coherent attributes of interest. Based on the Transformer, to perform effective attention, we explore image captioning from a cross-modal perspective and propose the Global-and-Local Information Exploring-and-Distilling approach that explores and distills the source information in vision and language. It globally provides the aspect vector, a spatial and relational representation of images based on caption contexts, through the extraction of salient region groupings and attribute collocations, and locally extracts the fine-grained regions and attributes in reference to the aspect vector for word selection. Our Transformer-based model achieves a CIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with remarkable efficiency in terms of accuracy, speed, and parameter budget.

</details>

<details>

<summary>2020-03-15 14:01:54 - Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the WASABI Song Corpus with Lyrics Annotations</summary>

- *Michael Fell, Elena Cabrio, Elmahdi Korfed, Michel Buffa, Fabien Gandon*

- `1912.02477v2` - [abs](http://arxiv.org/abs/1912.02477v2) - [pdf](http://arxiv.org/pdf/1912.02477v2)

> We present the WASABI Song Corpus, a large corpus of songs enriched with metadata extracted from music databases on the Web, and resulting from the processing of song lyrics and from audio analysis. More specifically, given that lyrics encode an important part of the semantics of a song, we focus here on the description of the methods we proposed to extract relevant information from the lyrics, such as their structure segmentation, their topics, the explicitness of the lyrics content, the salient passages of a song and the emotions conveyed. The creation of the resource is still ongoing: so far, the corpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at different levels with the output of the above mentioned methods. Such corpus labels and the provided methods can be exploited by music search engines and music professionals (e.g. journalists, radio presenters) to better handle large collections of lyrics, allowing an intelligent browsing, categorization and segmentation recommendation of songs.

</details>

<details>

<summary>2020-03-15 14:06:45 - The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes</summary>

- *Nicholas Heller, Niranjan Sathianathen, Arveen Kalapara, Edward Walczak, Keenan Moore, Heather Kaluzniak, Joel Rosenberg, Paul Blake, Zachary Rengel, Makinna Oestreich, Joshua Dean, Michael Tradewell, Aneri Shah, Resha Tejpaul, Zachary Edgerton, Matthew Peterson, Shaneabbas Raza, Subodh Regmi, Nikolaos Papanikolopoulos, Christopher Weight*

- `1904.00445v2` - [abs](http://arxiv.org/abs/1904.00445v2) - [pdf](http://arxiv.org/pdf/1904.00445v2)

> The morphometry of a kidney tumor revealed by contrast-enhanced Computed Tomography (CT) imaging is an important factor in clinical decision making surrounding the lesion's diagnosis and treatment. Quantitative study of the relationship between kidney tumor morphology and clinical outcomes is difficult due to data scarcity and the laborious nature of manually quantifying imaging predictors. Automatic semantic segmentation of kidneys and kidney tumors is a promising tool towards automatically quantifying a wide array of morphometric features, but no sizeable annotated dataset is currently available to train models for this task. We present the KiTS19 challenge dataset: A collection of multi-phase CT imaging, segmentation masks, and comprehensive clinical outcomes for 300 patients who underwent nephrectomy for kidney tumors at our center between 2010 and 2018. 210 (70%) of these patients were selected at random as the training set for the 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge and have been released publicly. With the presence of clinical context and surgical outcomes, this data can serve not only for benchmarking semantic segmentation models, but also for developing and studying biomarkers which make use of the imaging and semantic segmentation masks.

</details>

<details>

<summary>2020-03-15 15:55:23 - Metrics and methods for robustness evaluation of neural networks with generative models</summary>

- *Igor Buzhinsky, Arseny Nerinovsky, Stavros Tripakis*

- `2003.01993v2` - [abs](http://arxiv.org/abs/2003.01993v2) - [pdf](http://arxiv.org/pdf/2003.01993v2)

> Recent studies have shown that modern deep neural network classifiers are easy to fool, assuming that an adversary is able to slightly modify their inputs. Many papers have proposed adversarial attacks, defenses and methods to measure robustness to such adversarial perturbations. However, most commonly considered adversarial examples are based on $\ell_p$-bounded perturbations in the input space of the neural network, which are unlikely to arise naturally. Recently, especially in computer vision, researchers discovered "natural" or "semantic" perturbations, such as rotations, changes of brightness, or more high-level changes, but these perturbations have not yet been systematically utilized to measure the performance of classifiers. In this paper, we propose several metrics to measure robustness of classifiers to natural adversarial examples, and methods to evaluate them. These metrics, called latent space performance metrics, are based on the ability of generative models to capture probability distributions, and are defined in their latent spaces. On three image classification case studies, we evaluate the proposed metrics for several classifiers, including ones trained in conventional and robust ways. We find that the latent counterparts of adversarial robustness are associated with the accuracy of the classifier rather than its conventional adversarial robustness, but the latter is still reflected on the properties of found latent perturbations. In addition, our novel method of finding latent adversarial perturbations demonstrates that these perturbations are often perceptually small.

</details>

<details>

<summary>2020-03-16 02:41:44 - Semantic Mask for Transformer based End-to-End Speech Recognition</summary>

- *Chengyi Wang, Yu Wu, Yujiao Du, Jinyu Li, Shujie Liu, Liang Lu, Shuo Ren, Guoli Ye, Sheng Zhao, Ming Zhou*

- `1912.03010v2` - [abs](http://arxiv.org/abs/1912.03010v2) - [pdf](http://arxiv.org/pdf/1912.03010v2)

> Attention-based encoder-decoder model has achieved impressive results for both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. This approach takes advantage of the memorization capacity of neural networks to learn the mapping from the input sequence to the output sequence from scratch, without the assumption of prior knowledge such as the alignments. However, this model is prone to overfitting, especially when the amount of training data is limited. Inspired by SpecAugment and BERT, in this paper, we propose a semantic mask based regularization for training such kind of end-to-end (E2E) model. The idea is to mask the input features corresponding to a particular output token, e.g., a word or a word-piece, in order to encourage the model to fill the token based on the contextual information. While this approach is applicable to the encoder-decoder framework with any type of neural network architecture, we study the transformer-based model for ASR in this work. We perform experiments on Librispeech 960h and TedLium2 data sets, and achieve the state-of-the-art performance on the test set in the scope of E2E models.

</details>

<details>

<summary>2020-03-16 03:43:59 - A Generative Learning Approach for Spatio-temporal Modeling in Connected Vehicular Network</summary>

- *Rong Xia, Yong Xiao, Yingyu Li, Marwan Krunz, Dusit Niyato*

- `2003.07004v1` - [abs](http://arxiv.org/abs/2003.07004v1) - [pdf](http://arxiv.org/pdf/2003.07004v1)

> Spatio-temporal modeling of wireless access latency is of great importance for connected-vehicular systems. The quality of the molded results rely heavily on the number and quality of samples which can vary significantly due to the sensor deployment density as well as traffic volume and density. This paper proposes LaMI (Latency Model Inpainting), a novel framework to generate a comprehensive spatio-temporal of wireless access latency of a connected vehicles across a wide geographical area. LaMI adopts the idea from image inpainting and synthesizing and can reconstruct the missing latency samples by a two-step procedure. In particular, it first discovers the spatial correlation between samples collected in various regions using a patching-based approach and then feeds the original and highly correlated samples into a Variational Autoencoder (VAE), a deep generative model, to create latency samples with similar probability distribution with the original samples. Finally, LaMI establishes the empirical PDF of latency performance and maps the PDFs into the confidence levels of different vehicular service requirements. Extensive performance evaluation has been conducted using the real traces collected in a commercial LTE network in a university campus. Simulation results show that our proposed model can significantly improve the accuracy of latency modeling especially compared to existing popular solutions such as interpolation and nearest neighbor-based methods.

</details>

<details>

<summary>2020-03-16 13:43:41 - solc-verify: A Modular Verifier for Solidity Smart Contracts</summary>

- *Ákos Hajdu, Dejan Jovanović*

- `1907.04262v2` - [abs](http://arxiv.org/abs/1907.04262v2) - [pdf](http://arxiv.org/pdf/1907.04262v2)

> We present solc-verify, a source-level verification tool for Ethereum smart contracts. Solc-verify takes smart contracts written in Solidity and discharges verification conditions using modular program analysis and SMT solvers. Built on top of the Solidity compiler, solc-verify reasons at the level of the contract source code, as opposed to the more common approaches that operate at the level of Ethereum bytecode. This enables solc-verify to effectively reason about high-level contract properties while modeling low-level language semantics precisely. The contract properties, such as contract invariants, loop invariants, and function pre- and post-conditions, can be provided as annotations in the code by the developer. This enables automated, yet user-friendly formal verification for smart contracts. We demonstrate solc-verify by examining real-world examples where our tool can effectively find bugs and prove correctness of non-trivial properties with minimal user effort.

</details>

<details>

<summary>2020-03-16 14:31:49 - Semantic Pyramid for Image Generation</summary>

- *Assaf Shocher, Yossi Gandelsman, Inbar Mosseri, Michal Yarom, Michal Irani, William T. Freeman, Tali Dekel*

- `2003.06221v2` - [abs](http://arxiv.org/abs/2003.06221v2) - [pdf](http://arxiv.org/pdf/2003.06221v2)

> We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid -- a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled inpainting and compositing; all achieved with the same model, with no further training.

</details>

<details>

<summary>2020-03-16 17:37:25 - Deep Adaptive Semantic Logic (DASL): Compiling Declarative Knowledge into Deep Neural Networks</summary>

- *Karan Sikka, Andrew Silberfarb, John Byrnes, Indranil Sur, Ed Chow, Ajay Divakaran, Richard Rohwer*

- `2003.07344v1` - [abs](http://arxiv.org/abs/2003.07344v1) - [pdf](http://arxiv.org/pdf/2003.07344v1)

> We introduce Deep Adaptive Semantic Logic (DASL), a novel framework for automating the generation of deep neural networks that incorporates user-provided formal knowledge to improve learning from data. We provide formal semantics that demonstrate that our knowledge representation captures all of first order logic and that finite sampling from infinite domains converges to correct truth values. DASL's representation improves on prior neural-symbolic work by avoiding vanishing gradients, allowing deeper logical structure, and enabling richer interactions between the knowledge and learning components. We illustrate DASL through a toy problem in which we add structure to an image classification problem and demonstrate that knowledge of that structure reduces data requirements by a factor of $1000$. We then evaluate DASL on a visual relationship detection task and demonstrate that the addition of commonsense knowledge improves performance by $10.7\%$ in a data scarce setting.

</details>

<details>

<summary>2020-03-16 18:08:52 - A Formal Analysis of Multimodal Referring Strategies Under Common Ground</summary>

- *Nikhil Krishnaswamy, James Pustejovsky*

- `2003.07385v1` - [abs](http://arxiv.org/abs/2003.07385v1) - [pdf](http://arxiv.org/pdf/2003.07385v1)

> In this paper, we present an analysis of computationally generated mixed-modality definite referring expressions using combinations of gesture and linguistic descriptions. In doing so, we expose some striking formal semantic properties of the interactions between gesture and language, conditioned on the introduction of content into the common ground between the (computational) speaker and (human) viewer, and demonstrate how these formal features can contribute to training better models to predict viewer judgment of referring expressions, and potentially to the generation of more natural and informative referring expressions.

</details>

<details>

<summary>2020-03-17 04:18:48 - Foundations of Explainable Knowledge-Enabled Systems</summary>

- *Shruthi Chari, Daniel M. Gruen, Oshani Seneviratne, Deborah L. McGuinness*

- `2003.07520v1` - [abs](http://arxiv.org/abs/2003.07520v1) - [pdf](http://arxiv.org/pdf/2003.07520v1)

> Explainability has been an important goal since the early days of Artificial Intelligence. Several approaches for producing explanations have been developed. However, many of these approaches were tightly coupled with the capabilities of the artificial intelligence systems at the time. With the proliferation of AI-enabled systems in sometimes critical settings, there is a need for them to be explainable to end-users and decision-makers. We present a historical overview of explainable artificial intelligence systems, with a focus on knowledge-enabled systems, spanning the expert systems, cognitive assistants, semantic applications, and machine learning domains. Additionally, borrowing from the strengths of past approaches and identifying gaps needed to make explanations user- and context-focused, we propose new definitions for explanations and explainable knowledge-enabled systems.

</details>

<details>

<summary>2020-03-17 04:28:27 - FRAGE: Frequency-Agnostic Word Representation</summary>

- *Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu*

- `1809.06858v2` - [abs](http://arxiv.org/abs/1809.06858v2) - [pdf](http://arxiv.org/pdf/1809.06858v2)

> Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn \emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.

</details>

<details>

<summary>2020-03-17 04:34:29 - Directions for Explainable Knowledge-Enabled Systems</summary>

- *Shruthi Chari, Daniel M. Gruen, Oshani Seneviratne, Deborah L. McGuinness*

- `2003.07523v1` - [abs](http://arxiv.org/abs/2003.07523v1) - [pdf](http://arxiv.org/pdf/2003.07523v1)

> Interest in the field of Explainable Artificial Intelligence has been growing for decades and has accelerated recently. As Artificial Intelligence models have become more complex, and often more opaque, with the incorporation of complex machine learning techniques, explainability has become more critical. Recently, researchers have been investigating and tackling explainability with a user-centric focus, looking for explanations to consider trustworthiness, comprehensibility, explicit provenance, and context-awareness. In this chapter, we leverage our survey of explanation literature in Artificial Intelligence and closely related fields and use these past efforts to generate a set of explanation types that we feel reflect the expanded needs of explanation for today's artificial intelligence applications. We define each type and provide an example question that would motivate the need for this style of explanation. We believe this set of explanation types will help future system designers in their generation and prioritization of requirements and further help generate explanations that are better aligned to users' and situational needs.

</details>

<details>

<summary>2020-03-18 01:18:09 - Realistic Re-evaluation of Knowledge Graph Completion Methods: An Experimental Study</summary>

- *Farahnaz Akrami, Mohammed Samiul Saeef, Qingheng Zhang, Wei Hu, Chengkai Li*

- `2003.08001v1` - [abs](http://arxiv.org/abs/2003.08001v1) - [pdf](http://arxiv.org/pdf/2003.08001v1)

> In the active research area of employing embedding models for knowledge graph completion, particularly for the task of link prediction, most prior studies used two benchmark datasets FB15k and WN18 in evaluating such models. Most triples in these and other datasets in such studies belong to reverse and duplicate relations which exhibit high data redundancy due to semantic duplication, correlation or data incompleteness. This is a case of excessive data leakage---a model is trained using features that otherwise would not be available when the model needs to be applied for real prediction. There are also Cartesian product relations for which every triple formed by the Cartesian product of applicable subjects and objects is a true fact. Link prediction on the aforementioned relations is easy and can be achieved with even better accuracy using straightforward rules instead of sophisticated embedding models. A more fundamental defect of these models is that the link prediction scenario, given such data, is non-existent in the real-world. This paper is the first systematic study with the main objective of assessing the true effectiveness of embedding models when the unrealistic triples are removed. Our experiment results show these models are much less accurate than what we used to perceive. Their poor accuracy renders link prediction a task without truly effective automated solution. Hence, we call for re-investigation of possible effective approaches.

</details>

<details>

<summary>2020-03-18 01:30:02 - Selective Attention Encoders by Syntactic Graph Convolutional Networks for Document Summarization</summary>

- *Haiyang Xu, Yun Wang, Kun Han, Baochang Ma, Junwen Chen, Xiangang Li*

- `2003.08004v1` - [abs](http://arxiv.org/abs/2003.08004v1) - [pdf](http://arxiv.org/pdf/2003.08004v1)

> Abstractive text summarization is a challenging task, and one need to design a mechanism to effectively extract salient information from the source text and then generate a summary. A parsing process of the source text contains critical syntactic or semantic structures, which is useful to generate more accurate summary. However, modeling a parsing tree for text summarization is not trivial due to its non-linear structure and it is harder to deal with a document that includes multiple sentences and their parsing trees. In this paper, we propose to use a graph to connect the parsing trees from the sentences in a document and utilize the stacked graph convolutional networks (GCNs) to learn the syntactic representation for a document. The selective attention mechanism is used to extract salient information in semantic and structural aspect and generate an abstractive summary. We evaluate our approach on the CNN/Daily Mail text summarization dataset. The experimental results show that the proposed GCNs based selective attention approach outperforms the baselines and achieves the state-of-the-art performance on the dataset.

</details>

<details>

<summary>2020-03-18 07:24:37 - OpenGAN: Open Set Generative Adversarial Networks</summary>

- *Luke Ditria, Benjamin J. Meyer, Tom Drummond*

- `2003.08074v1` - [abs](http://arxiv.org/abs/2003.08074v1) - [pdf](http://arxiv.org/pdf/2003.08074v1)

> Many existing conditional Generative Adversarial Networks (cGANs) are limited to conditioning on pre-defined and fixed class-level semantic labels or attributes. We propose an open set GAN architecture (OpenGAN) that is conditioned per-input sample with a feature embedding drawn from a metric space. Using a state-of-the-art metric learning model that encodes both class-level and fine-grained semantic information, we are able to generate samples that are semantically similar to a given source image. The semantic information extracted by the metric learning model transfers to out-of-distribution novel classes, allowing the generative model to produce samples that are outside of the training distribution. We show that our proposed method is able to generate 256$\times$256 resolution images from novel classes that are of similar visual quality to those from the training classes. In lieu of a source image, we demonstrate that random sampling of the metric space also results in high-quality samples. We show that interpolation in the feature space and latent space results in semantically and visually plausible transformations in the image space. Finally, the usefulness of the generated samples to the downstream task of data augmentation is demonstrated. We show that classifier performance can be significantly improved by augmenting the training data with OpenGAN samples on classes that are outside of the GAN training distribution.

</details>

<details>

<summary>2020-03-18 09:28:40 - Automated synthesis of local time requirement for service composition</summary>

- *Étienne André, Tian Huat Tan, Manman Chen, Shuang Liu, Jun Sun, Yang Liu, Jin Song Dong*

- `2003.08116v1` - [abs](http://arxiv.org/abs/2003.08116v1) - [pdf](http://arxiv.org/pdf/2003.08116v1)

> Service composition aims at achieving a business goal by composing existing service-based applications or components. The response time of a service is crucial especially in time critical business environments, which is often stated as a clause in service level agreements between service providers and service users. To meet the guaranteed response time requirement of a composite service, it is important to select a feasible set of component services such that their response time will collectively satisfy the response time requirement of the composite service. In this work, we use the BPEL modeling language, that aims at specifying Web services. We extend it with timing parameters, and equip it with a formal semantics. Then, we propose a fully automated approach to synthesize the response time requirement of component services modeled using BPEL, in the form of a constraint on the local response times. The synthesized requirement will guarantee the satisfaction of the global response time requirement, statically or dynamically. We implemented our work into a tool, Selamat, and performed several experiments to evaluate the validity of our approach.

</details>

<details>

<summary>2020-03-18 09:42:02 - Deep Image Spatial Transformation for Person Image Generation</summary>

- *Yurui Ren, Xiaoming Yu, Junming Chen, Thomas H. Li, Ge Li*

- `2003.00696v2` - [abs](http://arxiv.org/abs/2003.00696v2) - [pdf](http://arxiv.org/pdf/2003.00696v2)

> Pose-guided person image generation is to transform a source person image to a target pose. This task requires spatial manipulations of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. Specifically, our model first calculates the global correlations between sources and targets to predict flow fields. Then, the flowed local patch pairs are extracted from the feature maps to calculate the local attention coefficients. Finally, we warp the source features using a content-aware sampling method with the obtained local attention coefficients. The results of both subjective and objective experiments demonstrate the superiority of our model. Besides, additional results in video animation and view synthesis show that our model is applicable to other tasks requiring spatial transformation. Our source code is available at https://github.com/RenYurui/Global-Flow-Local-Attention.

</details>

<details>

<summary>2020-03-18 13:39:23 - Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media</summary>

- *Massimo Stella*

- `2003.08835v1` - [abs](http://arxiv.org/abs/2003.08835v1) - [pdf](http://arxiv.org/pdf/2003.08835v1)

> Mindset reconstruction maps how individuals structure and perceive knowledge, a map unfolded here by investigating language and its cognitive reflection in the human mind, i.e. the mental lexicon. Textual forma mentis networks (TFMN) are glass boxes introduced for extracting, representing and understanding mindsets' structure, in Latin "forma mentis", from textual data. Combining network science, psycholinguistics and Big Data, TFMNs successfully identified relevant concepts, without supervision, in benchmark texts. Once validated, TFMNs were applied to the case study of the gender gap in science, which was strongly linked to distorted mindsets by recent studies. Focusing over social media perception and online discourse, this work analysed 10,000 relevant tweets. "Gender" and "gap" elicited a mostly positive perception, with a trustful/joyous emotional profile and semantic associates that: celebrated successful female scientists, related gender gap to wage differences, and hoped for a future resolution. The perception of "woman" highlighted discussion about sexual harassment and stereotype threat (a form of implicit cognitive bias) relative to women in science "sacrificing personal skills for success". The reconstructed perception of "man" highlighted social users' awareness of the myth of male superiority in science. No anger was detected around "person", suggesting that gap-focused discourse got less tense around genderless terms. No stereotypical perception of "scientist" was identified online, differently from real-world surveys. The overall analysis identified the online discourse as promoting a mostly stereotype-free, positive/trustful perception of gender disparity, aware of implicit/explicit biases and projected to closing the gap. TFMNs opened new ways for investigating perceptions in different groups, offering detailed data-informed grounding for policy making.

</details>

<details>

<summary>2020-03-18 14:18:59 - CAFENet: Class-Agnostic Few-Shot Edge Detection Network</summary>

- *Young-Hyun Park, Jun Seo, Jaekyun Moon*

- `2003.08235v1` - [abs](http://arxiv.org/abs/2003.08235v1) - [pdf](http://arxiv.org/pdf/2003.08235v1)

> We tackle a novel few-shot learning challenge, which we call few-shot semantic edge detection, aiming to localize crisp boundaries of novel categories using only a few labeled samples. We also present a Class-Agnostic Few-shot Edge detection Network (CAFENet) based on meta-learning strategy. CAFENet employs a semantic segmentation module in small-scale to compensate for lack of semantic information in edge labels. The predicted segmentation mask is used to generate an attention map to highlight the target object region, and make the decoder module concentrate on that region. We also propose a new regularization method based on multi-split matching. In meta-training, the metric-learning problem with high-dimensional vectors are divided into small subproblems with low-dimensional sub-vectors. Since there is no existing dataset for few-shot semantic edge detection, we construct two new datasets, FSE-1000 and SBD-$5^i$, and evaluate the performance of the proposed CAFENet on them. Extensive simulation results confirm the performance merits of the techniques adopted in CAFENet.

</details>

<details>

<summary>2020-03-18 15:57:10 - A Survey on Deep Learning for Named Entity Recognition</summary>

- *Jing Li, Aixin Sun, Jianglei Han, Chenliang Li*

- `1812.09449v3` - [abs](http://arxiv.org/abs/1812.09449v3) - [pdf](http://arxiv.org/pdf/1812.09449v3)

> Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.

</details>

<details>

<summary>2020-03-18 16:09:19 - Distributional Semantics and Linguistic Theory</summary>

- *Gemma Boleda*

- `1905.01896v4` - [abs](http://arxiv.org/abs/1905.01896v4) - [pdf](http://arxiv.org/pdf/1905.01896v4)

> Distributional semantics provides multi-dimensional, graded, empirically induced word representations that successfully capture many aspects of meaning in natural languages, as shown in a large body of work in computational linguistics; yet, its impact in theoretical linguistics has so far been limited. This review provides a critical discussion of the literature on distributional semantics, with an emphasis on methods and results that are of relevance for theoretical linguistics, in three areas: semantic change, polysemy and composition, and the grammar-semantics interface (specifically, the interface of semantics with syntax and with derivational morphology). The review aims at fostering greater cross-fertilization of theoretical and computational approaches to language, as a means to advance our collective knowledge of how it works.

</details>

<details>

<summary>2020-03-18 17:28:39 - Constraint Solving with Deep Learning for Symbolic Execution</summary>

- *Junye Wen, Mujahid Khan, Meiru Che, Yan Yan, Guowei Yang*

- `2003.08350v1` - [abs](http://arxiv.org/abs/2003.08350v1) - [pdf](http://arxiv.org/pdf/2003.08350v1)

> Symbolic execution is a powerful systematic software analysis technique, but suffers from the high cost of constraint solving, which is the key supporting technology that affects the effectiveness of symbolic execution. Techniques like Green and GreenTrie reuse constraint solutions to speed up constraint solving for symbolic execution; however, these reuse techniques require syntactic/semantic equivalence or implication relationship between constraints. This paper introduces DeepSover, a novel approach to constraint solving with deep learning for symbolic execution. Our key insight is to utilize the collective knowledge of a set of constraint solutions to train a deep neural network, which is then used to classify path conditions for their satisfiability during symbolic execution. Experimental evaluation shows DeepSolver is highly accurate in classifying path conditions, is more efficient than state-of-the-art constraint solving and constraint solution reuse techniques, and can well support symbolic execution tasks.

</details>

<details>

<summary>2020-03-18 18:00:31 - ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation</summary>

- *Yawar Siddiqui, Julien Valentin, Matthias Nießner*

- `1911.11789v2` - [abs](http://arxiv.org/abs/1911.11789v2) - [pdf](http://arxiv.org/pdf/1911.11789v2)

> We propose ViewAL, a novel active learning strategy for semantic segmentation that exploits viewpoint consistency in multi-view datasets. Our core idea is that inconsistencies in model predictions across viewpoints provide a very reliable measure of uncertainty and encourage the model to perform well irrespective of the viewpoint under which objects are observed. To incorporate this uncertainty measure, we introduce a new viewpoint entropy formulation, which is the basis of our active learning strategy. In addition, we propose uncertainty computations on a superpixel level, which exploits inherently localized signal in the segmentation task, directly lowering the annotation costs. This combination of viewpoint entropy and the use of superpixels allows to efficiently select samples that are highly informative for improving the network. We demonstrate that our proposed active learning strategy not only yields the best-performing models for the same amount of required labeled data, but also significantly reduces labeling effort. For instance, our method achieves 95% of maximum achievable network performance using only 7%, 17%, and 24% labeled data on SceneNet-RGBD, ScanNet, and Matterport3D, respectively. On these datasets, the best state-of-the-art method achieves the same performance with 14%, 27% and 33% labeled data. Finally, we demonstrate that labeling using superpixels yields the same quality of ground-truth compared to labeling whole images, but requires 25% less time.

</details>

<details>

<summary>2020-03-18 18:49:40 - Post-Training Piecewise Linear Quantization for Deep Neural Networks</summary>

- *Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, Joseph Hassoun*

- `2002.00104v2` - [abs](http://arxiv.org/abs/2002.00104v2) - [pdf](http://arxiv.org/pdf/2002.00104v2)

> Quantization plays an important role in the energy-efficient deployment of deep neural networks on resource-limited devices. Post-training quantization is highly desirable since it does not require retraining or access to the full training dataset. The well-established uniform scheme for post-training quantization achieves satisfactory results by converting neural networks from full-precision to 8-bit fixed-point integers. However, it suffers from significant performance degradation when quantizing to lower bit-widths. In this paper, we propose a piecewise linear quantization (PWLQ) scheme to enable accurate approximation for tensor values that have bell-shaped distributions with long tails. Our approach breaks the entire quantization range into non-overlapping regions for each tensor, with each region being assigned an equal number of quantization levels. Optimal breakpoints that divide the entire range are found by minimizing the quantization error. Compared to state-of-the-art post-training quantization methods, experimental results show that our proposed method achieves superior performance on image classification, semantic segmentation, and object detection with minor overhead.

</details>

<details>

<summary>2020-03-18 18:58:28 - Improved Embeddings with Easy Positive Triplet Mining</summary>

- *Hong Xuan, Abby Stylianou, Robert Pless*

- `1904.04370v2` - [abs](http://arxiv.org/abs/1904.04370v2) - [pdf](http://arxiv.org/pdf/1904.04370v2)

> Deep metric learning seeks to define an embedding where semantically similar images are embedded to nearby locations, and semantically dissimilar images are embedded to distant locations. Substantial work has focused on loss functions and strategies to learn these embeddings by pushing images from the same class as close together in the embedding space as possible. In this paper, we propose an alternative, loosened embedding strategy that requires the embedding function only map each training image to the most similar examples from the same class, an approach we call "Easy Positive" mining. We provide a collection of experiments and visualizations that highlight that this Easy Positive mining leads to embeddings that are more flexible and generalize better to new unseen data. This simple mining strategy yields recall performance that exceeds state of the art approaches (including those with complicated loss functions and ensemble methods) on image retrieval datasets including CUB, Stanford Online Products, In-Shop Clothes and Hotels-50K.

</details>

<details>

<summary>2020-03-18 18:59:55 - A Corpus of Adpositional Supersenses for Mandarin Chinese</summary>

- *Siyao Peng, Yang Liu, Yilun Zhu, Austin Blodgett, Yushi Zhao, Nathan Schneider*

- `2003.08437v1` - [abs](http://arxiv.org/abs/2003.08437v1) - [pdf](http://arxiv.org/pdf/2003.08437v1)

> Adpositions are frequent markers of semantic relations, but they are highly ambiguous and vary significantly from language to language. Moreover, there is a dearth of annotated corpora for investigating the cross-linguistic variation of adposition semantics, or for building multilingual disambiguation systems. This paper presents a corpus in which all adpositions have been semantically annotated in Mandarin Chinese; to the best of our knowledge, this is the first Chinese corpus to be broadly annotated with adposition semantics. Our approach adapts a framework that defined a general set of supersenses according to ostensibly language-independent semantic criteria, though its development focused primarily on English prepositions (Schneider et al., 2018). We find that the supersense categories are well-suited to Chinese adpositions despite syntactic differences from English. On a Mandarin translation of The Little Prince, we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext.

</details>

<details>

<summary>2020-03-19 04:13:08 - Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis</summary>

- *Mang Tik Chiu, Xingqian Xu, Yunchao Wei, Zilong Huang, Alexander Schwing, Robert Brunner, Hrant Khachatrian, Hovnatan Karapetyan, Ivan Dozier, Greg Rose, David Wilson, Adrian Tudor, Naira Hovakimyan, Thomas S. Huang, Honghui Shi*

- `2001.01306v2` - [abs](http://arxiv.org/abs/2001.01306v2) - [pdf](http://arxiv.org/pdf/2001.01306v2)

> The success of deep learning in visual recognition tasks has driven advancements in multiple fields of research. Particularly, increasing attention has been drawn towards its application in agriculture. Nevertheless, while visual pattern recognition on farmlands carries enormous economic values, little progress has been made to merge computer vision and crop sciences due to the lack of suitable agricultural image datasets. Meanwhile, problems in agriculture also pose new challenges in computer vision. For example, semantic segmentation of aerial farmland images requires inference over extremely large-size images with extreme annotation sparsity. These challenges are not present in most of the common object datasets, and we show that they are more challenging than many other aerial image datasets. To encourage research in computer vision for agriculture, we present Agriculture-Vision: a large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. We collected 94,986 high-quality aerial images from 3,432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. We annotate nine types of field anomaly patterns that are most important to farmers. As a pilot study of aerial agricultural semantic segmentation, we perform comprehensive experiments using popular semantic segmentation models; we also propose an effective model designed for aerial agricultural pattern recognition. Our experiments demonstrate several challenges Agriculture-Vision poses to both the computer vision and agriculture communities. Future versions of this dataset will include even more aerial images, anomaly patterns and image channels. More information at https://www.agriculture-vision.com.

</details>

<details>

<summary>2020-03-19 07:49:07 - PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting</summary>

- *Weiwei Cai, Zhanguo Wei*

- `1912.01834v2` - [abs](http://arxiv.org/abs/1912.01834v2) - [pdf](http://arxiv.org/pdf/1912.01834v2)

> The latest methods based on deep learning have achieved amazing results regarding the complex work of inpainting large missing areas in an image. But this type of method generally attempts to generate one single "optimal" result, ignoring many other plausible results. Considering the uncertainty of the inpainting task, one sole result can hardly be regarded as a desired regeneration of the missing area. In view of this weakness, which is related to the design of the previous algorithms, we propose a novel deep generative model equipped with a brand new style extractor which can extract the style feature (latent vector) from the ground truth. Once obtained, the extracted style feature and the ground truth are both input into the generator. We also craft a consistency loss that guides the generated image to approximate the ground truth. After iterations, our generator is able to learn the mapping of styles corresponding to multiple sets of vectors. The proposed model can generate a large number of results consistent with the context semantics of the image. Moreover, we evaluated the effectiveness of our model on three datasets, i.e., CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting methods, this model is able to offer desirable inpainting results with both better quality and higher diversity. The code and model will be made available on https://github.com/vivitsai/PiiGAN.

</details>

<details>

<summary>2020-03-19 11:02:36 - Zero-shot Learning for Audio-based Music Classification and Tagging</summary>

- *Jeong Choi, Jongpil Lee, Jiyoung Park, Juhan Nam*

- `1907.02670v2` - [abs](http://arxiv.org/abs/1907.02670v2) - [pdf](http://arxiv.org/pdf/1907.02670v2)

> Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where side information about the labels is used to unveil the relationship between each other. In this work, we investigate the zero-shot learning in the music domain and organize two different setups of side information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations. Considering a music track is usually multi-labeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning. Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in the music domain.

</details>

<details>

<summary>2020-03-19 14:23:12 - Temporal Embeddings and Transformer Models for Narrative Text Understanding</summary>

- *Vani K, Simone Mellace, Alessandro Antonucci*

- `2003.08811v1` - [abs](http://arxiv.org/abs/2003.08811v1) - [pdf](http://arxiv.org/pdf/2003.08811v1)

> We present two deep learning approaches to narrative text understanding for character relationship modelling. The temporal evolution of these relations is described by dynamic word embeddings, that are designed to learn semantic changes over time. An empirical analysis of the corresponding character trajectories shows that such approaches are effective in depicting dynamic evolution. A supervised learning approach based on the state-of-the-art transformer model BERT is used instead to detect static relations between characters. The empirical validation shows that such events (e.g., two characters belonging to the same family) might be spotted with good accuracy, even when using automatically annotated data. This provides a deeper understanding of narrative plots based on the identification of key facts. Standard clustering techniques are finally used for character de-aliasing, a necessary pre-processing step for both approaches. Overall, deep learning models appear to be suitable for narrative text understanding, while also providing a challenging and unexploited benchmark for general natural language understanding.

</details>

<details>

<summary>2020-03-19 15:55:21 - Beheshti-NER: Persian Named Entity Recognition Using BERT</summary>

- *Ehsan Taher, Seyed Abbas Hoseini, Mehrnoush Shamsfard*

- `2003.08875v1` - [abs](http://arxiv.org/abs/2003.08875v1) - [pdf](http://arxiv.org/pdf/2003.08875v1)

> Named entity recognition is a natural language processing task to recognize and extract spans of text associated with named entities and classify them in semantic Categories.   Google BERT is a deep bidirectional language model, pre-trained on large corpora that can be fine-tuned to solve many NLP tasks such as question answering, named entity recognition, part of speech tagging and etc. In this paper, we use the pre-trained deep bidirectional network, BERT, to make a model for named entity recognition in Persian.   We also compare the results of our model with the previous state of the art results achieved on Persian NER. Our evaluation metric is CONLL 2003 score in two levels of word and phrase. This model achieved second place in NSURL-2019 task 7 competition which associated with NER for the Persian language. our results in this competition are 83.5 and 88.4 f1 CONLL score respectively in phrase and word level evaluation.

</details>

<details>

<summary>2020-03-19 17:59:44 - Breaking certified defenses: Semantic adversarial examples with spoofed robustness certificates</summary>

- *Amin Ghiasi, Ali Shafahi, Tom Goldstein*

- `2003.08937v1` - [abs](http://arxiv.org/abs/2003.08937v1) - [pdf](http://arxiv.org/pdf/2003.08937v1)

> To deflect adversarial attacks, a range of "certified" classifiers have been proposed. In addition to labeling an image, certified classifiers produce (when possible) a certificate guaranteeing that the input image is not an $\ell_p$-bounded adversarial example. We present a new attack that exploits not only the labelling function of a classifier, but also the certificate generator. The proposed method applies large perturbations that place images far from a class boundary while maintaining the imperceptibility property of adversarial examples. The proposed "Shadow Attack" causes certifiably robust networks to mislabel an image and simultaneously produce a "spoofed" certificate of robustness.

</details>

<details>

<summary>2020-03-19 18:58:13 - Local Implicit Grid Representations for 3D Scenes</summary>

- *Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, Thomas Funkhouser*

- `2003.08981v1` - [abs](http://arxiv.org/abs/2003.08981v1) - [pdf](http://arxiv.org/pdf/2003.08981v1)

> Shape priors learned from data are commonly used to reconstruct 3D objects from partial or noisy data. Yet no such shape priors are available for indoor scenes, since typical 3D autoencoders cannot handle their scale, complexity, or diversity. In this paper, we introduce Local Implicit Grid Representations, a new 3D shape representation designed for scalability and generality. The motivating idea is that most 3D surfaces share geometric details at some scale -- i.e., at a scale smaller than an entire object and larger than a small patch. We train an autoencoder to learn an embedding of local crops of 3D shapes at that size. Then, we use the decoder as a component in a shape optimization that solves for a set of latent codes on a regular grid of overlapping crops such that an interpolation of the decoded local shapes matches a partial or noisy observation. We demonstrate the value of this proposed approach for 3D surface reconstruction from sparse point observations, showing significantly better results than alternative approaches.

</details>

<details>

<summary>2020-03-20 08:41:46 - Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task</summary>

- *Aritra Bhowmik, Stefan Gumhold, Carsten Rother, Eric Brachmann*

- `1912.00623v2` - [abs](http://arxiv.org/abs/1912.00623v2) - [pdf](http://arxiv.org/pdf/1912.00623v2)

> We address a core problem of computer vision: Detection and description of 2D feature points for image matching. For a long time, hand-crafted designs, like the seminal SIFT algorithm, were unsurpassed in accuracy and efficiency. Recently, learned feature detectors emerged that implement detection and description using neural networks. Training these networks usually resorts to optimizing low-level matching scores, often pre-defining sets of image patches which should or should not match, or which should or should not contain key points. Unfortunately, increased accuracy for these low-level matching scores does not necessarily translate to better performance in high-level vision tasks. We propose a new training methodology which embeds the feature detector in a complete vision pipeline, and where the learnable parameters are trained in an end-to-end fashion. We overcome the discrete nature of key point selection and descriptor matching using principles from reinforcement learning. As an example, we address the task of relative pose estimation between a pair of images. We demonstrate that the accuracy of a state-of-the-art learning-based feature detector can be increased when trained for the task it is supposed to solve at test time. Our training methodology poses little restrictions on the task to learn, and works for any architecture which predicts key point heat maps, and descriptors for key point locations.

</details>

<details>

<summary>2020-03-20 12:48:39 - Coronavirus (COVID-19) Classification using CT Images by Machine Learning Methods</summary>

- *Mucahid Barstugan, Umut Ozkaya, Saban Ozturk*

- `2003.09424v1` - [abs](http://arxiv.org/abs/2003.09424v1) - [pdf](http://arxiv.org/pdf/2003.09424v1)

> This study presents early phase detection of Coronavirus (COVID-19), which is named by World Health Organization (WHO), by machine learning methods. The detection process was implemented on abdominal Computed Tomography (CT) images. The expert radiologists detected from CT images that COVID-19 shows different behaviours from other viral pneumonia. Therefore, the clinical experts specify that COV\.ID-19 virus needs to be diagnosed in early phase. For detection of the COVID-19, four different datasets were formed by taking patches sized as 16x16, 32x32, 48x48, 64x64 from 150 CT images. The feature extraction process was applied to patches to increase the classification performance. Grey Level Co-occurrence Matrix (GLCM), Local Directional Pattern (LDP), Grey Level Run Length Matrix (GLRLM), Grey-Level Size Zone Matrix (GLSZM), and Discrete Wavelet Transform (DWT) algorithms were used as feature extraction methods. Support Vector Machines (SVM) classified the extracted features. 2-fold, 5-fold and 10-fold cross-validations were implemented during the classification process. Sensitivity, specificity, accuracy, precision, and F-score metrics were used to evaluate the classification performance. The best classification accuracy was obtained as 99.68% with 10-fold cross-validation and GLSZM feature extraction method.

</details>

<details>

<summary>2020-03-21 00:16:14 - Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring</summary>

- *Du Su, Ali Yekkehkhany, Yi Lu, Wenmiao Lu*

- `2003.10838v1` - [abs](http://arxiv.org/abs/2003.10838v1) - [pdf](http://arxiv.org/pdf/2003.10838v1)

> We propose a new application of embedding techniques for problem retrieval in adaptive tutoring. The objective is to retrieve problems whose mathematical concepts are similar. There are two challenges: First, like sentences, problems helpful to tutoring are never exactly the same in terms of the underlying concepts. Instead, good problems mix concepts in innovative ways, while still displaying continuity in their relationships. Second, it is difficult for humans to determine a similarity score that is consistent across a large enough training set. We propose a hierarchical problem embedding algorithm, called Prob2Vec, that consists of abstraction and embedding steps. Prob2Vec achieves 96.88\% accuracy on a problem similarity test, in contrast to 75\% from directly applying state-of-the-art sentence embedding methods. It is interesting that Prob2Vec is able to distinguish very fine-grained differences among problems, an ability humans need time and effort to acquire. In addition, the sub-problem of concept labeling with imbalanced training data set is interesting in its own right. It is a multi-label problem suffering from dimensionality explosion, which we propose ways to ameliorate. We propose the novel negative pre-training algorithm that dramatically reduces false negative and positive ratios for classification, using an imbalanced training data set.

</details>

<details>

<summary>2020-03-21 15:47:22 - Intrinsic dimension estimation for locally undersampled data</summary>

- *Vittorio Erba, Marco Gherardi, Pietro Rotondo*

- `1906.07670v2` - [abs](http://arxiv.org/abs/1906.07670v2) - [pdf](http://arxiv.org/pdf/1906.07670v2)

> High-dimensional data are ubiquitous in contemporary science and finding methods to compress them is one of the primary goals of machine learning. Given a dataset lying in a high-dimensional space (in principle hundreds to several thousands of dimensions), it is often useful to project it onto a lower-dimensional manifold, without loss of information. Identifying the minimal dimension of such manifold is a challenging problem known in the literature as intrinsic dimension estimation (IDE). Traditionally, most IDE algorithms are either based on multiscale principal component analysis (PCA) or on the notion of correlation dimension (and more in general on k-nearest-neighbors distances). These methods are affected, in different ways, by a severe curse of dimensionality. In particular, none of the existing algorithms can provide accurate ID estimates in the extreme locally undersampled regime, i.e. in the limit where the number of samples in any local patch of the manifold is less than (or of the same order of) the ID of the dataset. Here we introduce a new ID estimator that leverages on simple properties of the tangent space of a manifold to overcome these shortcomings. The method is based on the full correlation integral, going beyond the limit of small radius used for the estimation of the correlation dimension. Our estimator alleviates the extreme undersampling problem, intractable with other methods. Based on this insight, we explore a multiscale generalization of the algorithm. We show that it is capable of (i) identifying multiple dimensionalities in a dataset, and (ii) providing accurate estimates of the ID of extremely curved manifolds. In particular, we test the method on manifolds generated from global transformations of high-contrast images, relevant for invariant object recognition and considered a challenge for state-of-the-art ID estimators.

</details>

<details>

<summary>2020-03-21 16:51:02 - Large-scale Ontological Reasoning via Datalog</summary>

- *Mario Alviano, Marco Manna*

- `2003.09698v1` - [abs](http://arxiv.org/abs/2003.09698v1) - [pdf](http://arxiv.org/pdf/2003.09698v1)

> Reasoning over OWL 2 is a very expensive task in general, and therefore the W3C identified tractable profiles exhibiting good computational properties. Ontological reasoning for many fragments of OWL 2 can be reduced to the evaluation of Datalog queries. This paper surveys some of these compilations, and in particular the one addressing queries over Horn-$\mathcal{SHIQ}$ knowledge bases and its implementation in DLV2 enanched by a new version of the Magic Sets algorithm.

</details>

<details>

<summary>2020-03-22 01:12:23 - DiversityGAN: Diversity-Aware Vehicle Motion Prediction via Latent Semantic Sampling</summary>

- *Xin Huang, Stephen G. McGill, Jonathan A. DeCastro, Luke Fletcher, John J. Leonard, Brian C. Williams, Guy Rosman*

- `1911.12736v2` - [abs](http://arxiv.org/abs/1911.12736v2) - [pdf](http://arxiv.org/pdf/1911.12736v2)

> Vehicle trajectory prediction is crucial for autonomous driving and advanced driver assistant systems. While existing approaches may sample from a predicted distribution of vehicle trajectories, they lack the ability to explore it -- a key ability for evaluating safety from a planning and verification perspective. In this work, we devise a novel approach for generating realistic and diverse vehicle trajectories. We extend the generative adversarial network (GAN) framework with a low-dimensional approximate semantic space, and shape that space to capture semantics such as merging and turning. We sample from this space in a way that mimics the predicted distribution, but allows us to control coverage of semantically distinct outcomes. We validate our approach on a publicly available dataset and show results that achieve state-of-the-art prediction performance, while providing improved coverage of the space of predicted trajectory semantics.

</details>

<details>

<summary>2020-03-22 07:27:07 - Prior Knowledge Driven Label Embedding for Slot Filling in Natural Language Understanding</summary>

- *Su Zhu, Zijian Zhao, Rao Ma, Kai Yu*

- `2003.09831v1` - [abs](http://arxiv.org/abs/2003.09831v1) - [pdf](http://arxiv.org/pdf/2003.09831v1)

> Traditional slot filling in natural language understanding (NLU) predicts a one-hot vector for each word. This form of label representation lacks semantic correlation modelling, which leads to severe data sparsity problem, especially when adapting an NLU model to a new domain. To address this issue, a novel label embedding based slot filling framework is proposed in this paper. Here, distributed label embedding is constructed for each slot using prior knowledge. Three encoding methods are investigated to incorporate different kinds of prior knowledge about slots: atomic concepts, slot descriptions, and slot exemplars. The proposed label embeddings tend to share text patterns and reuses data with different slot labels. This makes it useful for adaptive NLU with limited data. Also, since label embedding is independent of NLU model, it is compatible with almost all deep learning based slot filling models. The proposed approaches are evaluated on three datasets. Experiments on single domain and domain adaptation tasks show that label embedding achieves significant performance improvement over traditional one-hot label representation as well as advanced zero-shot approaches.

</details>

<details>

<summary>2020-03-22 12:52:56 - Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles</summary>

- *Malte Ostendorff, Terry Ruas, Moritz Schubotz, Georg Rehm, Bela Gipp*

- `2003.09881v1` - [abs](http://arxiv.org/abs/2003.09881v1) - [pdf](http://arxiv.org/pdf/2003.09881v1)

> Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph-Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of recommender systems based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another.

</details>

<details>

<summary>2020-03-22 23:17:19 - Interpretable machine learning models: a physics-based view</summary>

- *Ion Matei, Johan de Kleer, Christoforos Somarakis, Rahul Rai, John S. Baras*

- `2003.10025v1` - [abs](http://arxiv.org/abs/2003.10025v1) - [pdf](http://arxiv.org/pdf/2003.10025v1)

> To understand changes in physical systems and facilitate decisions, explaining how model predictions are made is crucial. We use model-based interpretability, where models of physical systems are constructed by composing basic constructs that explain locally how energy is exchanged and transformed. We use the port Hamiltonian (p-H) formalism to describe the basic constructs that contain physically interpretable processes commonly found in the behavior of physical systems. We describe how we can build models out of the p-H constructs and how we can train them. In addition we show how we can impose physical properties such as dissipativity that ensure numerical stability of the training process. We give examples on how to build and train models for describing the behavior of two physical systems: the inverted pendulum and swarm dynamics.

</details>

<details>

<summary>2020-03-23 01:26:36 - Understanding the robustness of deep neural network classifiers for breast cancer screening</summary>

- *Witold Oleszkiewicz, Taro Makino, Stanisław Jastrzębski, Tomasz Trzciński, Linda Moy, Kyunghyun Cho, Laura Heacock, Krzysztof J. Geras*

- `2003.10041v1` - [abs](http://arxiv.org/abs/2003.10041v1) - [pdf](http://arxiv.org/pdf/2003.10041v1)

> Deep neural networks (DNNs) show promise in breast cancer screening, but their robustness to input perturbations must be better understood before they can be clinically implemented. There exists extensive literature on this subject in the context of natural images that can potentially be built upon. However, it cannot be assumed that conclusions about robustness will transfer from natural images to mammogram images, due to significant differences between the two image modalities. In order to determine whether conclusions will transfer, we measure the sensitivity of a radiologist-level screening mammogram image classifier to four commonly studied input perturbations that natural image classifiers are sensitive to. We find that mammogram image classifiers are also sensitive to these perturbations, which suggests that we can build on the existing literature. We also perform a detailed analysis on the effects of low-pass filtering, and find that it degrades the visibility of clinically meaningful features called microcalcifications. Since low-pass filtering removes semantically meaningful information that is predictive of breast cancer, we argue that it is undesirable for mammogram image classifiers to be invariant to it. This is in contrast to natural images, where we do not want DNNs to be sensitive to low-pass filtering due to its tendency to remove information that is human-incomprehensible.

</details>

<details>

<summary>2020-03-23 06:46:28 - E2EET: From Pipeline to End-to-end Entity Typing via Transformer-Based Embeddings</summary>

- *Michael Stewart, Wei Liu*

- `2003.10097v1` - [abs](http://arxiv.org/abs/2003.10097v1) - [pdf](http://arxiv.org/pdf/2003.10097v1)

> Entity Typing (ET) is the process of identifying the semantic types of every entity within a corpus. In contrast to Named Entity Recognition, where each token in a sentence is labelled with zero or one class label, ET involves labelling each entity mention with one or more class labels. Existing entity typing models, which operate at the mention level, are limited by two key factors: they do not make use of recently-proposed context-dependent embeddings, and are trained on fixed context windows. They are therefore sensitive to window size selection and are unable to incorporate the context of the entire document. In light of these drawbacks we propose to incorporate context using transformer-based embeddings for a mention-level model, and an end-to-end model using a Bi-GRU to remove the dependency on window size. An extensive ablative study demonstrates the effectiveness of contextualised embeddings for mention-level models and the competitiveness of our end-to-end model for entity typing.

</details>

<details>

<summary>2020-03-23 18:23:18 - Modeling Contrary-to-Duty with CP-nets</summary>

- *Roberta Calegari, Andrea Loreggia, Emiliano Lorini, Francesca Rossi, Giovanni Sartor*

- `2003.10480v1` - [abs](http://arxiv.org/abs/2003.10480v1) - [pdf](http://arxiv.org/pdf/2003.10480v1)

> In a ceteris-paribus semantics for deontic logic, a state of affairs where a larger set of prescriptions is respected is preferable to a state of affairs where some of them are violated. Conditional preference nets (CP-nets) are a compact formalism to express and analyse ceteris paribus preferences, which nice computational properties. This paper shows how deontic concepts can be captured through conditional preference models. A restricted deontic logic will be defined, and mapped into conditional preference nets. We shall also show how to model contrary to duties obligations in CP-nets and how to capture in this formalism the distinction between strong and weak permission.

</details>

<details>

<summary>2020-03-23 20:27:00 - ProGraML: Graph-based Deep Learning for Program Optimization and Analysis</summary>

- *Chris Cummins, Zacharias V. Fisches, Tal Ben-Nun, Torsten Hoefler, Hugh Leather*

- `2003.10536v1` - [abs](http://arxiv.org/abs/2003.10536v1) - [pdf](http://arxiv.org/pdf/2003.10536v1)

> The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation.   We introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks.   ProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.

</details>

<details>

<summary>2020-03-23 23:57:41 - Task-Aware Feature Generation for Zero-Shot Compositional Learning</summary>

- *Xin Wang, Fisher Yu, Trevor Darrell, Joseph E. Gonzalez*

- `1906.04854v2` - [abs](http://arxiv.org/abs/1906.04854v2) - [pdf](http://arxiv.org/pdf/1906.04854v2)

> Visual concepts (e.g., red apple, big elephant) are often semantically compositional and each element of the compositions can be reused to construct novel concepts (e.g., red elephant). Compositional feature synthesis, which generates image feature distributions exploiting the semantic compositionality, is a promising approach to sample-efficient model generalization. In this work, we propose a task-aware feature generation (TFG) framework for compositional learning, which generates features of novel visual concepts by transferring knowledge from previously seen concepts. These synthetic features are then used to train a classifier to recognize novel concepts in a zero-shot manner. Our novel TFG design injects task-conditioned noise layer-by-layer, producing task-relevant variation at each level. We find the proposed generator design improves classification accuracy and sample efficiency. Our model establishes a new state of the art on three zero-shot compositional learning (ZSCL) benchmarks, outperforming the previous discriminative models by a large margin. Our model improves the performance of the prior arts by over 2x in the generalized ZSCL setting.

</details>

<details>

<summary>2020-03-24 01:31:14 - Video Object Grounding using Semantic Roles in Language Description</summary>

- *Arka Sadhu, Kan Chen, Ram Nevatia*

- `2003.10606v1` - [abs](http://arxiv.org/abs/2003.10606v1) - [pdf](http://arxiv.org/pdf/2003.10606v1)

> We explore the task of Video Object Grounding (VOG), which grounds objects in videos referred to in natural language descriptions. Previous methods apply image grounding based algorithms to address VOG, fail to explore the object relation information and suffer from limited generalization. Here, we investigate the role of object relations in VOG and propose a novel framework VOGNet to encode multi-modal object relations via self-attention with relative position encoding. To evaluate VOGNet, we propose novel contrasting sampling methods to generate more challenging grounding input samples, and construct a new dataset called ActivityNet-SRL (ASRL) based on existing caption and grounding datasets. Experiments on ASRL validate the need of encoding object relations in VOG, and our VOGNet outperforms competitive baselines by a significant margin.

</details>

<details>

<summary>2020-03-24 11:17:20 - Generating Chinese Poetry from Images via Concrete and Abstract Information</summary>

- *Yusen Liu, Dayiheng Liu, Jiancheng Lv, Yongsheng Sang*

- `2003.10773v1` - [abs](http://arxiv.org/abs/2003.10773v1) - [pdf](http://arxiv.org/pdf/2003.10773v1)

> In recent years, the automatic generation of classical Chinese poetry has made great progress. Besides focusing on improving the quality of the generated poetry, there is a new topic about generating poetry from an image. However, the existing methods for this topic still have the problem of topic drift and semantic inconsistency, and the image-poem pairs dataset is hard to be built when training these models. In this paper, we extract and integrate the Concrete and Abstract information from images to address those issues. We proposed an infilling-based Chinese poetry generation model which can infill the Concrete keywords into each line of poems in an explicit way, and an abstract information embedding to integrate the Abstract information into generated poems. In addition, we use non-parallel data during training and construct separate image datasets and poem datasets to train the different components in our framework. Both automatic and human evaluation results show that our approach can generate poems which have better consistency with images without losing the quality.

</details>

<details>

<summary>2020-03-24 12:35:23 - Self-Assignment Flows for Unsupervised Data Labeling on Graphs</summary>

- *Matthias Zisler, Artjom Zern, Stefania Petra, Christoph Schnörr*

- `1911.03472v2` - [abs](http://arxiv.org/abs/1911.03472v2) - [pdf](http://arxiv.org/pdf/1911.03472v2)

> This paper extends the recently introduced assignment flow approach for supervised image labeling to unsupervised scenarios where no labels are given. The resulting self-assignment flow takes a pairwise data affinity matrix as input data and maximizes the correlation with a low-rank matrix that is parametrized by the variables of the assignment flow, which entails an assignment of the data to themselves through the formation of latent labels (feature prototypes). A single user parameter, the neighborhood size for the geometric regularization of assignments, drives the entire process. By smooth geodesic interpolation between different normalizations of self-assignment matrices on the positive definite matrix manifold, a one-parameter family of self-assignment flows is defined. Accordingly, our approach can be characterized from different viewpoints, e.g. as performing spatially regularized, rank-constrained discrete optimal transport, or as computing spatially regularized normalized spectral cuts. Regarding combinatorial optimization, our approach successfully determines completely positive factorizations of self-assignments in large-scale scenarios, subject to spatial regularization. Various experiments including the unsupervised learning of patch dictionaries using a locally invariant distance function, illustrate the properties of the approach.

</details>

<details>

<summary>2020-03-24 14:27:25 - Scalable Deployment of AI Time-series Models for IoT</summary>

- *Bradley Eck, Francesco Fusco, Robert Gormally, Mark Purcell, Seshu Tirupathi*

- `2003.12141v1` - [abs](http://arxiv.org/abs/2003.12141v1) - [pdf](http://arxiv.org/pdf/2003.12141v1)

> IBM Research Castor, a cloud-native system for managing and deploying large numbers of AI time-series models in IoT applications, is described. Modelling code templates, in Python and R, following a typical machine-learning workflow are supported. A knowledge-based approach to managing model and time-series data allows the use of general semantic concepts for expressing feature engineering tasks. Model templates can be programmatically deployed against specific instances of semantic concepts, thus supporting model reuse and automated replication as the IoT application grows. Deployed models are automatically executed in parallel leveraging a serverless cloud computing framework. The complete history of trained model versions and rolling-horizon predictions is persisted, thus enabling full model lineage and traceability. Results from deployments in real-world smart-grid live forecasting applications are reported. Scalability of executing up to tens of thousands of AI modelling tasks is also evaluated.

</details>

<details>

<summary>2020-03-24 19:18:34 - Can Embeddings Adequately Represent Medical Terminology? New Large-Scale Medical Term Similarity Datasets Have the Answer!</summary>

- *Claudia Schulz, Damir Juric*

- `2003.11082v1` - [abs](http://arxiv.org/abs/2003.11082v1) - [pdf](http://arxiv.org/pdf/2003.11082v1)

> A large number of embeddings trained on medical data have emerged, but it remains unclear how well they represent medical terminology, in particular whether the close relationship of semantically similar medical terms is encoded in these embeddings. To date, only small datasets for testing medical term similarity are available, not allowing to draw conclusions about the generalisability of embeddings to the enormous amount of medical terms used by doctors. We present multiple automatically created large-scale medical term similarity datasets and confirm their high quality in an annotation study with doctors. We evaluate state-of-the-art word and contextual embeddings on our new datasets, comparing multiple vector similarity metrics and word vector aggregation techniques. Our results show that current embeddings are limited in their ability to adequately encode medical terms. The novel datasets thus form a challenging new benchmark for the development of medical embeddings able to accurately represent the whole medical terminology.

</details>

<details>

<summary>2020-03-24 21:19:14 - Context-Aware Parse Trees</summary>

- *Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Paul Petersen, Jesmin Jahan Tithi, Tim Mattson, Tim Kraska, Pradeep Dubey, Vivek Sarkar, Justin Gottschlich*

- `2003.11118v1` - [abs](http://arxiv.org/abs/2003.11118v1) - [pdf](http://arxiv.org/pdf/2003.11118v1)

> The simplified parse tree (SPT) presented in Aroma, a state-of-the-art code recommendation system, is a tree-structured representation used to infer code semantics by capturing program \emph{structure} rather than program \emph{syntax}. This is a departure from the classical abstract syntax tree, which is principally driven by programming language syntax. While we believe a semantics-driven representation is desirable, the specifics of an SPT's construction can impact its performance. We analyze these nuances and present a new tree structure, heavily influenced by Aroma's SPT, called a \emph{context-aware parse tree} (CAPT). CAPT enhances SPT by providing a richer level of semantic representation. Specifically, CAPT provides additional binding support for language-specific techniques for adding semantically-salient features, and language-agnostic techniques for removing syntactically-present but semantically-irrelevant features. Our research quantitatively demonstrates the value of our proposed semantically-salient features, enabling a specific CAPT configuration to be 39\% more accurate than SPT across the 48,610 programs we analyzed.

</details>

<details>

<summary>2020-03-24 23:59:59 - Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</summary>

- *Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach*

- `1911.06258v3` - [abs](http://arxiv.org/abs/1911.06258v3) - [pdf](http://arxiv.org/pdf/1911.06258v3)

> Many visual scenes contain text that carries crucial information, and it is thus essential to understand text in images for downstream reasoning tasks. For example, a deep water label on a warning sign warns people about the danger in the scene. Recent work has explored the TextVQA task that requires reading and understanding text in images to answer a question. However, existing approaches for TextVQA are mostly based on custom pairwise fusion mechanisms between a pair of two modalities and are restricted to a single prediction step by casting TextVQA as a classification task. In this work, we propose a novel model for the TextVQA task based on a multimodal transformer architecture accompanied by a rich representation for text in images. Our model naturally fuses different modalities homogeneously by embedding them into a common semantic space where self-attention is applied to model inter- and intra- modality context. Furthermore, it enables iterative answer decoding with a dynamic pointer network, allowing the model to form an answer through multi-step prediction instead of one-step classification. Our model outperforms existing approaches on three benchmark datasets for the TextVQA task by a large margin.

</details>

<details>

<summary>2020-03-25 01:07:06 - Norms and Sanctions as a Basis for Promoting Cybersecurity Practices</summary>

- *Nirav Ajmeri, Shubham Goyal, Munindar P. Singh*

- `2003.11170v1` - [abs](http://arxiv.org/abs/2003.11170v1) - [pdf](http://arxiv.org/pdf/2003.11170v1)

> Many cybersecurity breaches occur due to users not following good cybersecurity practices, chief among them being regulations for applying software patches to operating systems, updating applications, and maintaining strong passwords.   We capture cybersecurity expectations on users as norms. We empirically investigate sanctioning mechanisms in promoting compliance with those norms as well as the detrimental effect of sanctions on the ability of users to complete their work. We realize these ideas in a game that emulates the decision making of workers in a research lab.   Through a human-subject study, we find that whereas individual sanctions are more effective than group sanctions in achieving compliance and less detrimental on the ability of users to complete their work, individual sanctions offer significantly lower resilience especially for organizations comprising risk seekers. Our findings have implications for workforce training in cybersecurity.

</details>

<details>

<summary>2020-03-25 01:29:38 - Learning Syntactic and Dynamic Selective Encoding for Document Summarization</summary>

- *Haiyang Xu, Yahao He, Kun Han, Junwen Chen, Xiangang Li*

- `2003.11173v1` - [abs](http://arxiv.org/abs/2003.11173v1) - [pdf](http://arxiv.org/pdf/2003.11173v1)

> Text summarization aims to generate a headline or a short summary consisting of the major information of the source text. Recent studies employ the sequence-to-sequence framework to encode the input with a neural network and generate abstractive summary. However, most studies feed the encoder with the semantic word embedding but ignore the syntactic information of the text. Further, although previous studies proposed the selective gate to control the information flow from the encoder to the decoder, it is static during the decoding and cannot differentiate the information based on the decoder states. In this paper, we propose a novel neural architecture for document summarization. Our approach has the following contributions: first, we incorporate syntactic information such as constituency parsing trees into the encoding sequence to learn both the semantic and syntactic information from the document, resulting in more accurate summary; second, we propose a dynamic gate network to select the salient information based on the context of the decoder state, which is essential to document summarization. The proposed model has been evaluated on CNN/Daily Mail summarization datasets and the experimental results show that the proposed approach outperforms baseline approaches.

</details>

<details>

<summary>2020-03-25 04:27:01 - A New Multiple Max-pooling Integration Module and Cross Multiscale Deconvolution Network Based on Image Semantic Segmentation</summary>

- *Hongfeng You, Shengwei Tian, Long Yu, Xiang Ma, Yan Xing, Ning Xin*

- `2003.11213v1` - [abs](http://arxiv.org/abs/2003.11213v1) - [pdf](http://arxiv.org/pdf/2003.11213v1)

> To better retain the deep features of an image and solve the sparsity problem of the end-to-end segmentation model, we propose a new deep convolutional network model for medical image pixel segmentation, called MC-Net. The core of this network model consists of four parts, namely, an encoder network, a multiple max-pooling integration module, a cross multiscale deconvolution decoder network and a pixel-level classification layer. In the network structure of the encoder, we use multiscale convolution instead of the traditional single-channel convolution. The multiple max-pooling integration module first integrates the output features of each submodule of the encoder network and reduces the number of parameters by convolution using a kernel size of 1. At the same time, each max-pooling layer (the pooling size of each layer is different) is spliced after each convolution to achieve the translation invariance of the feature maps of each submodule. We use the output feature maps from the multiple max-pooling integration module as the input of the decoder network; the multiscale convolution of each submodule in the decoder network is cross-fused with the feature maps generated by the corresponding multiscale convolution in the encoder network. Using the above feature map processing methods solves the sparsity problem after the max-pooling layer-generating matrix and enhances the robustness of the classification. We compare our proposed model with the well-known Fully Convolutional Networks for Semantic Segmentation (FCNs), DecovNet, PSPNet, U-net, SgeNet and other state-of-the-art segmentation networks such as HyperDenseNet, MS-Dual, Espnetv2, Denseaspp using one binary Kaggle 2018 data science bowl dataset and two multiclass dataset and obtain encouraging experimental results.

</details>

<details>

<summary>2020-03-25 09:33:57 - Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations</summary>

- *Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timothy Mann, Pushmeet Kohli*

- `1912.03192v2` - [abs](http://arxiv.org/abs/1912.03192v2) - [pdf](http://arxiv.org/pdf/1912.03192v2)

> Recent research has made the surprising finding that state-of-the-art deep learning models sometimes fail to generalize to small variations of the input. Adversarial training has been shown to be an effective approach to overcome this problem. However, its application has been limited to enforcing invariance to analytically defined transformations like $\ell_p$-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input (such as a change in lighting conditions). In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input. The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to define different factors of variations, and (2) generating new input images by adversarially composing the representations of different images. We use a StyleGAN model to demonstrate the efficacy of this framework. Specifically, we leverage the disentangled latent representations computed by a StyleGAN model to generate perturbations of an image that are similar to real-world variations (like adding make-up, or changing the skin-tone of a person) and train models to be invariant to these perturbations. Extensive experiments show that our method improves generalization and reduces the effect of spurious correlations (reducing the error rate of a "smile" detector by 21% for example).

</details>

<details>

<summary>2020-03-25 21:34:58 - Overview of the TREC 2019 Fair Ranking Track</summary>

- *Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier*

- `2003.11650v1` - [abs](http://arxiv.org/abs/2003.11650v1) - [pdf](http://arxiv.org/pdf/2003.11650v1)

> The goal of the TREC Fair Ranking track was to develop a benchmark for evaluating retrieval systems in terms of fairness to different content providers in addition to classic notions of relevance. As part of the benchmark, we defined standardized fairness metrics with evaluation protocols and released a dataset for the fair ranking problem. The 2019 task focused on reranking academic paper abstracts given a query. The objective was to fairly represent relevant authors from several groups that were unknown at the system submission time. Thus, the track emphasized the development of systems which have robust performance across a variety of group definitions. Participants were provided with querylog data (queries, documents, and relevance) from Semantic Scholar. This paper presents an overview of the track, including the task definition, descriptions of the data and the annotation process, as well as a comparison of the performance of submitted systems.

</details>

<details>

<summary>2020-03-25 22:40:29 - Patch Quality and Diversity of Invariant-Guided Search-Based Program Repair</summary>

- *Zhen Yu Ding*

- `2003.11667v1` - [abs](http://arxiv.org/abs/2003.11667v1) - [pdf](http://arxiv.org/pdf/2003.11667v1)

> Most automatic program repair techniques rely on test cases to specify correct program behavior. Due to test cases' frequently incomplete coverage of desired behavior, however, patches often overfit and fail to generalize to broader requirements. Moreover, in the absence of perfectly correct outputs, methods to ensure higher patch quality, such as merging together several patches or a human evaluating patch recommendations, benefit from having access to a diverse set of patches, making patch diversity a potentially useful trait. We evaluate the correctness and diversity of patches generated by GenProg and an invariant-based diversity-enhancing extension described in our prior work. We find no evidence that promoting diversity changes the correctness of patches in a positive or negative direction. Using invariant- and test case generation-driven metrics for measuring semantic diversity, we find no observed semantic differences between patches for most bugs, regardless of the repair technique used.

</details>

<details>

<summary>2020-03-26 08:47:21 - Neural encoding and interpretation for high-level visual cortices based on fMRI using image caption features</summary>

- *Kai Qiao, Chi Zhang, Jian Chen, Linyuan Wang, Li Tong, Bin Yan*

- `2003.11797v1` - [abs](http://arxiv.org/abs/2003.11797v1) - [pdf](http://arxiv.org/pdf/2003.11797v1)

> On basis of functional magnetic resonance imaging (fMRI), researchers are devoted to designing visual encoding models to predict the neuron activity of human in response to presented image stimuli and analyze inner mechanism of human visual cortices. Deep network structure composed of hierarchical processing layers forms deep network models by learning features of data on specific task through big dataset. Deep network models have powerful and hierarchical representation of data, and have brought about breakthroughs for visual encoding, while revealing hierarchical structural similarity with the manner of information processing in human visual cortices. However, previous studies almost used image features of those deep network models pre-trained on classification task to construct visual encoding models. Except for deep network structure, the task or corresponding big dataset is also important for deep network models, but neglected by previous studies. Because image classification is a relatively fundamental task, it is difficult to guide deep network models to master high-level semantic representations of data, which causes into that encoding performance for high-level visual cortices is limited. In this study, we introduced one higher-level vision task: image caption (IC) task and proposed the visual encoding model based on IC features (ICFVEM) to encode voxels of high-level visual cortices. Experiment demonstrated that ICFVEM obtained better encoding performance than previous deep network models pre-trained on classification task. In addition, the interpretation of voxels was realized to explore the detailed characteristics of voxels based on the visualization of semantic words, and comparative analysis implied that high-level visual cortices behaved the correlative representation of image content.

</details>

<details>

<summary>2020-03-26 09:04:45 - Hybrid Attention-Based Transformer Block Model for Distant Supervision Relation Extraction</summary>

- *Yan Xiao, Yaochu Jin, Ran Cheng, Kuangrong Hao*

- `2003.11518v2` - [abs](http://arxiv.org/abs/2003.11518v2) - [pdf](http://arxiv.org/pdf/2003.11518v2)

> With an exponential explosive growth of various digital text information, it is challenging to efficiently obtain specific knowledge from massive unstructured text information. As one basic task for natural language processing (NLP), relation extraction aims to extract the semantic relation between entity pairs based on the given text. To avoid manual labeling of datasets, distant supervision relation extraction (DSRE) has been widely used, aiming to utilize knowledge base to automatically annotate datasets. Unfortunately, this method heavily suffers from wrong labelling due to the underlying strong assumptions. To address this issue, we propose a new framework using hybrid attention-based Transformer block with multi-instance learning to perform the DSRE task. More specifically, the Transformer block is firstly used as the sentence encoder to capture syntactic information of sentences, which mainly utilizes multi-head self-attention to extract features from word level. Then, a more concise sentence-level attention mechanism is adopted to constitute the bag representation, aiming to incorporate valid information of each sentence to effectively represent the bag. Experimental results on the public dataset New York Times (NYT) demonstrate that the proposed approach can outperform the state-of-the-art algorithms on the evaluation dataset, which verifies the effectiveness of our model for the DSRE task.

</details>

<details>

<summary>2020-03-26 09:53:41 - Modern RESTful API DLs and frameworks for RESTful web services API schema modeling, documenting, visualizing</summary>

- *Kyrylo Malakhov, Oleksandr Kurgaev, Vitalii Velychko*

- `1811.04659v3` - [abs](http://arxiv.org/abs/1811.04659v3) - [pdf](http://arxiv.org/pdf/1811.04659v3)

> The given paper presents an overview of modern RESTful API description languages (belongs to interface description languages set) - OpenAPI, RAML, WADL, Slate - designed to provide a structured description of a RESTful web APIs (that is useful both to a human and for automated machine processing), with related RESTful web API modeling frameworks. We propose an example of the schema model of web API of the service for pre-trained distributional semantic models (word embeddings) processing. This service is a part of the Personal Research Information System services ecosystem - the Research and Development Workstation Environment class system for supporting research in the field of ontology engineering: the automated building of applied ontology in an arbitrary domain area as a main feature; scientific and technical creativity: the automated preparation of application documents for patenting inventions in Ukraine. It also presents a quick look at the relationship of Service-Oriented Architecture and Web services as well as REST fundamentals and RESTful web services; RESTful API creation process.

</details>

<details>

<summary>2020-03-26 11:04:49 - Weak Supervision in Convolutional Neural Network for Semantic Segmentation of Diffuse Lung Diseases Using Partially Annotated Dataset</summary>

- *Yuki Suzuki, Kazuki Yamagata, Yanagawa Masahiro, Shoji Kido, Noriyuki Tomiyama*

- `2002.11936v2` - [abs](http://arxiv.org/abs/2002.11936v2) - [pdf](http://arxiv.org/pdf/2002.11936v2)

> Computer-aided diagnosis system for diffuse lung diseases (DLDs) is necessary for the objective assessment of the lung diseases. In this paper, we develop semantic segmentation model for 5 kinds of DLDs. DLDs considered in this work are consolidation, ground glass opacity, honeycombing, emphysema, and normal. Convolutional neural network (CNN) is one of the most promising technique for semantic segmentation among machine learning algorithms. While creating annotated dataset for semantic segmentation is laborious and time consuming, creating partially annotated dataset, in which only one chosen class is annotated for each image, is easier since annotators only need to focus on one class at a time during the annotation task. In this paper, we propose a new weak supervision technique that effectively utilizes partially annotated dataset. The experiments using partially annotated dataset composed 372 CT images demonstrated that our proposed technique significantly improved segmentation accuracy.

</details>

<details>

<summary>2020-03-26 15:34:32 - Founded Semantics and Constraint Semantics of Logic Rules</summary>

- *Yanhong A. Liu, Scott D. Stoller*

- `1606.06269v4` - [abs](http://arxiv.org/abs/1606.06269v4) - [pdf](http://arxiv.org/pdf/1606.06269v4)

> Logic rules and inference are fundamental in computer science and have been studied extensively. However, prior semantics of logic languages can have subtle implications and can disagree significantly, on even very simple programs, including in attempting to solve the well-known Russell's paradox. These semantics are often non-intuitive and hard-to-understand when unrestricted negation is used in recursion.   This paper describes a simple new semantics for logic rules, founded semantics, and its straightforward extension to another simple new semantics, constraint semantics, that unify the core of different prior semantics. The new semantics support unrestricted negation, as well as unrestricted existential and universal quantifications. They are uniquely expressive and intuitive by allowing assumptions about the predicates, rules, and reasoning to be specified explicitly, as simple and precise binary choices. They are completely declarative and relate cleanly to prior semantics. In addition, founded semantics can be computed in linear time in the size of the ground program.

</details>

<details>

<summary>2020-03-26 17:59:38 - Memory Enhanced Global-Local Aggregation for Video Object Detection</summary>

- *Yihong Chen, Yue Cao, Han Hu, Liwei Wang*

- `2003.12063v1` - [abs](http://arxiv.org/abs/2003.12063v1) - [pdf](http://arxiv.org/pdf/2003.12063v1)

> How do humans recognize an object in a piece of video? Due to the deteriorated quality of single frame, it may be hard for people to identify an occluded object in this frame by just utilizing information within one image. We argue that there are two important cues for humans to recognize objects in videos: the global semantic information and the local localization information. Recently, plenty of methods adopt the self-attention mechanisms to enhance the features in key frame with either global semantic information or local localization information. In this paper we introduce memory enhanced global-local aggregation (MEGA) network, which is among the first trials that takes full consideration of both global and local information. Furthermore, empowered by a novel and carefully-designed Long Range Memory (LRM) module, our proposed MEGA could enable the key frame to get access to much more content than any previous methods. Enhanced by these two sources of information, our method achieves state-of-the-art performance on ImageNet VID dataset. Code is available at \url{https://github.com/Scalsol/mega.pytorch}.

</details>

<details>

<summary>2020-03-27 07:37:15 - Unsupervised Cross-Modal Audio Representation Learning from Unstructured Multilingual Text</summary>

- *Alexander Schindler, Sergiu Gordea, Peter Knees*

- `2003.12265v1` - [abs](http://arxiv.org/abs/2003.12265v1) - [pdf](http://arxiv.org/pdf/2003.12265v1)

> We present an approach to unsupervised audio representation learning. Based on a triplet neural network architecture, we harnesses semantically related cross-modal information to estimate audio track-relatedness. By applying Latent Semantic Indexing (LSI) we embed corresponding textual information into a latent vector space from which we derive track relatedness for online triplet selection. This LSI topic modelling facilitates fine-grained selection of similar and dissimilar audio-track pairs to learn the audio representation using a Convolution Recurrent Neural Network (CRNN). By this we directly project the semantic context of the unstructured text modality onto the learned representation space of the audio modality without deriving structured ground-truth annotations from it. We evaluate our approach on the Europeana Sounds collection and show how to improve search in digital audio libraries by harnessing the multilingual meta-data provided by numerous European digital libraries. We show that our approach is invariant to the variety of annotation styles as well as to the different languages of this collection. The learned representations perform comparable to the baseline of handcrafted features, respectively exceeding this baseline in similarity retrieval precision at higher cut-offs with only 15\% of the baseline's feature vector length.

</details>

<details>

<summary>2020-03-27 09:16:33 - Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models</summary>

- *Pranav Agarwal, Alejandro Betancourt, Vana Panagiotou, Natalia Díaz-Rodríguez*

- `2003.11743v2` - [abs](http://arxiv.org/abs/2003.11743v2) - [pdf](http://arxiv.org/pdf/2003.11743v2)

> Image captioning models have been able to generate grammatically correct and human understandable sentences. However most of the captions convey limited information as the model used is trained on datasets that do not caption all possible objects existing in everyday life. Due to this lack of prior information most of the captions are biased to only a few objects present in the scene, hence limiting their usage in daily life. In this paper, we attempt to show the biased nature of the currently existing image captioning models and present a new image captioning dataset, Egoshots, consisting of 978 real life images with no captions. We further exploit the state of the art pre-trained image captioning and object recognition networks to annotate our images and show the limitations of existing works. Furthermore, in order to evaluate the quality of the generated captions, we propose a new image captioning metric, object based Semantic Fidelity (SF). Existing image captioning metrics can evaluate a caption only in the presence of their corresponding annotations; however, SF allows evaluating captions generated for images without annotations, making it highly useful for real life generated captions.

</details>

<details>

<summary>2020-03-27 14:30:19 - Splitting Epistemic Logic Programs</summary>

- *Pedro Cabalar, Jorge Fandinno, Luis Fariñas del Cerro*

- `1812.08763v2` - [abs](http://arxiv.org/abs/1812.08763v2) - [pdf](http://arxiv.org/pdf/1812.08763v2)

> Epistemic logic programs constitute an extension of the stable models semantics to deal with new constructs called subjective literals. Informally speaking, a subjective literal allows checking whether some regular literal is true in all stable models or in some stable model. As it can be imagined, the associated semantics has proved to be non-trivial, as the truth of the subjective literal may interfere with the set of stable models it is supposed to query. As a consequence, no clear agreement has been reached and different semantic proposals have been made in the literature. Unfortunately, comparison among these proposals has been limited to a study of their effect on individual examples, rather than identifying general properties to be checked. In this paper, we propose an extension of the well-known splitting property for logic programs to the epistemic case. To this aim, we formally define when an arbitrary semantics satisfies the epistemic splitting property and examine some of the consequences that can be derived from that, including its relation to conformant planning and to epistemic constraints. Interestingly, we prove (through counterexamples) that most of the existing proposals fail to fulfill the epistemic splitting property, except the original semantics proposed by Gelfond in 1991.

</details>

<details>

<summary>2020-03-27 14:52:55 - Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment Classification</summary>

- *Wuraola Fisayo Oyewusi, Olubayo Adekanmbi, Olalekan Akinsande*

- `2003.12450v1` - [abs](http://arxiv.org/abs/2003.12450v1) - [pdf](http://arxiv.org/pdf/2003.12450v1)

> Nigerian English adaptation, Pidgin, has evolved over the years through multi-language code switching, code mixing and linguistic adaptation. While Pidgin preserves many of the words in the normal English language corpus, both in spelling and pronunciation, the fundamental meaning of these words have changed significantly. For example,'ginger' is not a plant but an expression of motivation and 'tank' is not a container but an expression of gratitude. The implication is that the current approach of using direct English sentiment analysis of social media text from Nigeria is sub-optimal, as it will not be able to capture the semantic variation and contextual evolution in the contemporary meaning of these words. In practice, while many words in Nigerian Pidgin adaptation are the same as the standard English, the full English language based sentiment analysis models are not designed to capture the full intent of the Nigerian pidgin when used alone or code-mixed. By augmenting scarce human labelled code-changed text with ample synthetic code-reformatted text and meaning, we achieve significant improvements in sentiment scoring. Our research explores how to understand sentiment in an intrasentential code mixing and switching context where there has been significant word localization.This work presents a 300 VADER lexicon compatible Nigerian Pidgin sentiment tokens and their scores and a 14,000 gold standard Nigerian Pidgin tweets and their sentiments labels.

</details>

<details>

<summary>2020-03-27 15:46:47 - Quantum Semantic Learning by Reverse Annealing an Adiabatic Quantum Computer</summary>

- *Lorenzo Rocutto, Claudio Destri, Enrico Prati*

- `2003.11945v2` - [abs](http://arxiv.org/abs/2003.11945v2) - [pdf](http://arxiv.org/pdf/2003.11945v2)

> Boltzmann Machines constitute a class of neural networks with applications to image reconstruction, pattern classification and unsupervised learning in general. Their most common variants, called Restricted Boltzmann Machines (RBMs) exhibit a good trade-off between computability on existing silicon-based hardware and generality of possible applications.   Still, the diffusion of RBMs is quite limited, since their training process proves to be hard. The advent of commercial Adiabatic Quantum Computers (AQCs) raised the expectation that the implementations of RBMs on such quantum devices could increase the training speed with respect to conventional hardware. To date, however, the implementation of RBM networks on AQCs has been limited by the low qubit connectivity when each qubit acts as a node of the neural network.   Here we demonstrate the feasibility of a complete RBM on AQCs, thanks to an embedding that associates its nodes to virtual qubits, thus outperforming previous implementations based on incomplete graphs.   Moreover, to accelerate the learning, we implement a semantic quantum search which, contrary to previous proposals, takes the input data as initial boundary conditions to start each learning step of the RBM, thanks to a reverse annealing schedule. Such an approach, unlike the more conventional forward annealing schedule, allows sampling configurations in a meaningful neighborhood of the training data, mimicking the behavior of the classical Gibbs sampling algorithm.   We show that the learning based on reverse annealing quickly raises the sampling probability of a meaningful subset of the set of the configurations. Even without a proper optimization of the annealing schedule, the RBM semantically trained by reverse annealing achieves better scores on reconstruction tasks.

</details>

<details>

<summary>2020-03-27 18:21:02 - Hierarchically Robust Representation Learning</summary>

- *Qi Qian, Juhua Hu, Hao Li*

- `1911.04047v2` - [abs](http://arxiv.org/abs/1911.04047v2) - [pdf](http://arxiv.org/pdf/1911.04047v2)

> With the tremendous success of deep learning in visual tasks, the representations extracted from intermediate layers of learned models, that is, deep features, attract much attention of researchers. Previous empirical analysis shows that those features can contain appropriate semantic information. Therefore, with a model trained on a large-scale benchmark data set (e.g., ImageNet), the extracted features can work well on other tasks. In this work, we investigate this phenomenon and demonstrate that deep features can be suboptimal due to the fact that they are learned by minimizing the empirical risk. When the data distribution of the target task is different from that of the benchmark data set, the performance of deep features can degrade. Hence, we propose a hierarchically robust optimization method to learn more generic features. Considering the example-level and concept-level robustness simultaneously, we formulate the problem as a distributionally robust optimization problem with Wasserstein ambiguity set constraints, and an efficient algorithm with the conventional training pipeline is proposed. Experiments on benchmark data sets demonstrate the effectiveness of the robust deep representations.

</details>

<details>

<summary>2020-03-27 20:33:52 - MCFlow: Monte Carlo Flow Models for Data Imputation</summary>

- *Trevor W. Richardson, Wencheng Wu, Lei Lin, Beilei Xu, Edgar A. Bernal*

- `2003.12628v1` - [abs](http://arxiv.org/abs/2003.12628v1) - [pdf](http://arxiv.org/pdf/2003.12628v1)

> We consider the topic of data imputation, a foundational task in machine learning that addresses issues with missing data. To that end, we propose MCFlow, a deep framework for imputation that leverages normalizing flow generative models and Monte Carlo sampling. We address the causality dilemma that arises when training models with incomplete data by introducing an iterative learning scheme which alternately updates the density estimate and the values of the missing entries in the training data. We provide extensive empirical validation of the effectiveness of the proposed method on standard multivariate and image datasets, and benchmark its performance against state-of-the-art alternatives. We demonstrate that MCFlow is superior to competing methods in terms of the quality of the imputed data, as well as with regards to its ability to preserve the semantic structure of the data.

</details>

<details>

<summary>2020-03-28 07:48:02 - Variational Transformers for Diverse Response Generation</summary>

- *Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, Pascale Fung*

- `2003.12738v1` - [abs](http://arxiv.org/abs/2003.12738v1) - [pdf](http://arxiv.org/pdf/2003.12738v1)

> Despite the great promise of Transformers in many sequence modeling tasks (e.g., machine translation), their deterministic nature hinders them from generalizing to high entropy tasks such as dialogue response generation. Previous work proposes to capture the variability of dialogue responses with a recurrent neural network (RNN)-based conditional variational autoencoder (CVAE). However, the autoregressive computation of the RNN limits the training efficiency. Therefore, we propose the Variational Transformer (VT), a variational self-attentive feed-forward sequence model. The VT combines the parallelizability and global receptive field of the Transformer with the variational nature of the CVAE by incorporating stochastic latent variables into Transformers. We explore two types of the VT: 1) modeling the discourse-level diversity with a global latent variable; and 2) augmenting the Transformer decoder with a sequence of fine-grained latent variables. Then, the proposed models are evaluated on three conversational datasets with both automatic metric and human evaluation. The experimental results show that our models improve standard Transformers and other baselines in terms of diversity, semantic relevance, and human judgment.

</details>

<details>

<summary>2020-03-28 16:52:02 - Towards Learning Structure via Consensus for Face Segmentation and Parsing</summary>

- *Iacopo Masi, Joe Mathai, Wael AbdAlmageed*

- `1911.00957v3` - [abs](http://arxiv.org/abs/1911.00957v3) - [pdf](http://arxiv.org/pdf/1911.00957v3)

> Face segmentation is the task of densely labeling pixels on the face according to their semantics. While current methods place an emphasis on developing sophisticated architectures, use conditional random fields for smoothness, or rather employ adversarial training, we follow an alternative path towards robust face segmentation and parsing. Occlusions, along with other parts of the face, have a proper structure that needs to be propagated in the model during training. Unlike state-of-the-art methods that treat face segmentation as an independent pixel prediction problem, we argue instead that it should hold highly correlated outputs within the same object pixels. We thereby offer a novel learning mechanism to enforce structure in the prediction via consensus, guided by a robust loss function that forces pixel objects to be consistent with each other. Our face parser is trained by transferring knowledge from another model, yet it encourages spatial consistency while fitting the labels. Different than current practice, our method enjoys pixel-wise predictions, yet paves the way for fewer artifacts, less sparse masks, and spatially coherent outputs.

</details>

<details>

<summary>2020-03-28 17:09:11 - Cross-Lingual Adaptation Using Universal Dependencies</summary>

- *Nasrin Taghizadeh, Heshaam Faili*

- `2003.10816v2` - [abs](http://arxiv.org/abs/2003.10816v2) - [pdf](http://arxiv.org/pdf/2003.10816v2)

> We describe a cross-lingual adaptation method based on syntactic parse trees obtained from the Universal Dependencies (UD), which are consistent across languages, to develop classifiers in low-resource languages. The idea of UD parsing is to capture similarities as well as idiosyncrasies among typologically different languages. In this paper, we show that models trained using UD parse trees for complex NLP tasks can characterize very different languages. We study two tasks of paraphrase identification and semantic relation extraction as case studies. Based on UD parse trees, we develop several models using tree kernels and show that these models trained on the English dataset can correctly classify data of other languages e.g. French, Farsi, and Arabic. The proposed approach opens up avenues for exploiting UD parsing in solving similar cross-lingual tasks, which is very useful for languages that no labeled data is available for them.

</details>

<details>

<summary>2020-03-28 18:11:37 - LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval</summary>

- *Reuben Tan, Huijuan Xu, Kate Saenko, Bryan A. Plummer*

- `1909.13784v2` - [abs](http://arxiv.org/abs/1909.13784v2) - [pdf](http://arxiv.org/pdf/1909.13784v2)

> The goal of weakly-supervised video moment retrieval is to localize the video segment most relevant to the given natural language query without access to temporal annotations during training. Prior strongly- and weakly-supervised approaches often leverage co-attention mechanisms to learn visual-semantic representations for localization. However, while such approaches tend to focus on identifying relationships between elements of the video and language modalities, there is less emphasis on modeling relational context between video frames given the semantic context of the query. Consequently, the above-mentioned visual-semantic representations, built upon local frame features, do not contain much contextual information. To address this limitation, we propose a Latent Graph Co-Attention Network (LoGAN) that exploits fine-grained frame-by-word interactions to reason about correspondences between all possible pairs of frames, given the semantic context of the query. Comprehensive experiments across two datasets, DiDeMo and Charades-Sta, demonstrate the effectiveness of our proposed latent co-attention model where it outperforms current state-of-the-art (SOTA) weakly-supervised approaches by a significant margin. Notably, it even achieves a 11% improvement to Recall@1 accuracy over strongly-supervised SOTA methods on DiDeMo.

</details>

<details>

<summary>2020-03-28 21:38:21 - Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yorùbá and Twi</summary>

- *Jesujoba O. Alabi, Kwabena Amponsah-Kaakyire, David I. Adelani, Cristina España-Bonet*

- `1912.02481v2` - [abs](http://arxiv.org/abs/1912.02481v2) - [pdf](http://arxiv.org/pdf/1912.02481v2)

> The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as Wikipedia has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yor\`ub\'a and Twi, and compare the word embeddings obtained in this way, with word embeddings obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and characters to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor\`ub\'a and Twi. As output of the work, we provide corpora, embeddings and the test suits for both languages.

</details>

<details>

<summary>2020-03-28 22:10:48 - Orchestrating NLP Services for the Legal Domain</summary>

- *Julián Moreno-Schneider, Georg Rehm, Elena Montiel-Ponsoda, Víctor Rodriguez-Doncel, Artem Revenko, Sotirios Karampatakis, Maria Khvalchik, Christian Sageder, Jorge Gracia, Filippo Maganza*

- `2003.12900v1` - [abs](http://arxiv.org/abs/2003.12900v1) - [pdf](http://arxiv.org/pdf/2003.12900v1)

> Legal technology is currently receiving a lot of attention from various angles. In this contribution we describe the main technical components of a system that is currently under development in the European innovation project Lynx, which includes partners from industry and research. The key contribution of this paper is a workflow manager that enables the flexible orchestration of workflows based on a portfolio of Natural Language Processing and Content Curation services as well as a Multilingual Legal Knowledge Graph that contains semantic information and meaningful references to legal documents. We also describe different use cases with which we experiment and develop prototypical solutions.

</details>

<details>

<summary>2020-03-28 23:46:08 - A Dataset of Dockerfiles</summary>

- *Jordan Henkel, Christian Bird, Shuvendu K. Lahiri, Thomas Reps*

- `2003.12912v1` - [abs](http://arxiv.org/abs/2003.12912v1) - [pdf](http://arxiv.org/pdf/2003.12912v1)

> Dockerfiles are one of the most prevalent kinds of DevOps artifacts used in industry. Despite their prevalence, there is a lack of sophisticated semantics-aware static analysis of Dockerfiles. In this paper, we introduce a dataset of approximately 178,000 unique Dockerfiles collected from GitHub. To enhance the usability of this data, we describe five representations we have devised for working with, mining from, and analyzing these Dockerfiles. Each Dockerfile representation builds upon the previous ones, and the final representation, created by three levels of nested parsing and abstraction, makes tasks such as mining and static checking tractable. The Dockerfiles, in each of the five representations, along with metadata and the tools used to shepard the data from one representation to the next are all available at: https://doi.org/10.5281/zenodo.3628771.

</details>

<details>

<summary>2020-03-29 02:22:54 - Unsupervised Attributed Multiplex Network Embedding</summary>

- *Chanyoung Park, Donghyun Kim, Jiawei Han, Hwanjo Yu*

- `1911.06750v2` - [abs](http://arxiv.org/abs/1911.06750v2) - [pdf](http://arxiv.org/pdf/1911.06750v2)

> Nodes in a multiplex network are connected by multiple types of relations. However, most existing network embedding methods assume that only a single type of relation exists between nodes. Even for those that consider the multiplexity of a network, they overlook node attributes, resort to node labels for training, and fail to model the global properties of a graph. We present a simple yet effective unsupervised network embedding method for attributed multiplex network called DMGI, inspired by Deep Graph Infomax (DGI) that maximizes the mutual information between local patches of a graph, and the global representation of the entire graph. We devise a systematic way to jointly integrate the node embeddings from multiple graphs by introducing 1) the consensus regularization framework that minimizes the disagreements among the relation-type specific node embeddings, and 2) the universal discriminator that discriminates true samples regardless of the relation types. We also show that the attention mechanism infers the importance of each relation type, and thus can be useful for filtering unnecessary relation types as a preprocessing step. Extensive experiments on various downstream tasks demonstrate that DMGI outperforms the state-of-the-art methods, even though DMGI is fully unsupervised.

</details>

<details>

<summary>2020-03-29 06:00:41 - Open Compound Domain Adaptation</summary>

- *Ziwei Liu, Zhongqi Miao, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella X. Yu, Boqing Gong*

- `1909.03403v2` - [abs](http://arxiv.org/abs/1909.03403v2) - [pdf](http://arxiv.org/pdf/1909.03403v2)

> A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2020-03-29 13:20:43 - A Dataset of German Legal Documents for Named Entity Recognition</summary>

- *Elena Leitner, Georg Rehm, Julián Moreno-Schneider*

- `2003.13016v1` - [abs](http://arxiv.org/abs/2003.13016v1) - [pdf](http://arxiv.org/pdf/2003.13016v1)

> We describe a dataset developed for Named Entity Recognition in German federal court decisions. It consists of approx. 67,000 sentences with over 2 million tokens. The resource contains 54,000 manually annotated entities, mapped to 19 fine-grained semantic classes: person, judge, lawyer, country, city, street, landscape, organization, company, institution, court, brand, law, ordinance, European legal norm, regulation, contract, court decision, and legal literature. The legal documents were, furthermore, automatically annotated with more than 35,000 TimeML-based time expressions. The dataset, which is available under a CC-BY 4.0 license in the CoNNL-2002 format, was developed for training an NER service for German legal documents in the EU project Lynx.

</details>

<details>

<summary>2020-03-29 17:40:04 - Best Practices for Implementing FAIR Vocabularies and Ontologies on the Web</summary>

- *Daniel Garijo, María Poveda-Villalón*

- `2003.13084v1` - [abs](http://arxiv.org/abs/2003.13084v1) - [pdf](http://arxiv.org/pdf/2003.13084v1)

> With the adoption of Semantic Web technologies, an increasing number of vocabularies and ontologies have been developed in different domains, ranging from Biology to Agronomy or Geosciences. However, many of these ontologies are still difficult to find, access and understand by researchers due to a lack of documentation, URI resolving issues, versioning problems, etc. In this chapter we describe guidelines and best practices for creating accessible, understandable and reusable ontologies on the Web, using standard practices and pointing to existing tools and frameworks developed by the Semantic Web community. We illustrate our guidelines with concrete examples, in order to help researchers implement these practices in their future vocabularies.

</details>

<details>

<summary>2020-03-29 21:02:09 - Topological Data Analysis in Text Classification: Extracting Features with Additive Information</summary>

- *Shafie Gholizadeh, Ketki Savle, Armin Seyeditabari, Wlodek Zadrozny*

- `2003.13138v1` - [abs](http://arxiv.org/abs/2003.13138v1) - [pdf](http://arxiv.org/pdf/2003.13138v1)

> While the strength of Topological Data Analysis has been explored in many studies on high dimensional numeric data, it is still a challenging task to apply it to text. As the primary goal in topological data analysis is to define and quantify the shapes in numeric data, defining shapes in the text is much more challenging, even though the geometries of vector spaces and conceptual spaces are clearly relevant for information retrieval and semantics. In this paper, we examine two different methods of extraction of topological features from text, using as the underlying representations of words the two most popular methods, namely word embeddings and TF-IDF vectors. To extract topological features from the word embedding space, we interpret the embedding of a text document as high dimensional time series, and we analyze the topology of the underlying graph where the vertices correspond to different embedding dimensions. For topological data analysis with the TF-IDF representations, we analyze the topology of the graph whose vertices come from the TF-IDF vectors of different blocks in the textual document. In both cases, we apply homological persistence to reveal the geometric structures under different distance resolutions. Our results show that these topological features carry some exclusive information that is not captured by conventional text mining methods. In our experiments we observe adding topological features to the conventional features in ensemble models improves the classification results (up to 5\%). On the other hand, as expected, topological features by themselves may be not sufficient for effective classification. It is an open problem to see whether TDA features from word embeddings might be sufficient, as they seem to perform within a range of few points from top results obtained with a linear support vector classifier.

</details>

<details>

<summary>2020-03-30 00:52:54 - Adversarial Domain Adaptation Being Aware of Class Relationships</summary>

- *Zeya Wang, Baoyu Jing, Yang Ni, Nanqing Dong, Pengtao Xie, Eric P. Xing*

- `1905.11931v2` - [abs](http://arxiv.org/abs/1905.11931v2) - [pdf](http://arxiv.org/pdf/1905.11931v2)

> Adversarial training is a useful approach to promote the learning of transferable representations across the source and target domains, which has been widely applied for domain adaptation (DA) tasks based on deep neural networks. Until very recently, existing adversarial domain adaptation (ADA) methods ignore the useful information from the label space, which is an important factor accountable for the complicated data distributions associated with different semantic classes. Especially, the inter-class semantic relationships have been rarely considered and discussed in the current work of transfer learning. In this paper, we propose a novel relationship-aware adversarial domain adaptation (RADA) algorithm, which first utilizes a single multi-class domain discriminator to enforce the learning of inter-class dependency structure during domain-adversarial training and then aligns this structure with the inter-class dependencies that are characterized from training the label predictor on source domain. Specifically, we impose a regularization term to penalize the structure discrepancy between the inter-class dependencies respectively estimated from domain discriminator and label predictor. Through this alignment, our proposed method makes the adversarial domain adaptation aware of the class relationships. Empirical studies show that the incorporation of class relationships significantly improves the performance on benchmark datasets.

</details>

<details>

<summary>2020-03-30 02:17:10 - Gold Seeker: Information Gain from Policy Distributions for Goal-oriented Vision-and-Langauge Reasoning</summary>

- *Ehsan Abbasnejad, Iman Abbasnejad, Qi Wu, Javen Shi, Anton van den Hengel*

- `1812.06398v3` - [abs](http://arxiv.org/abs/1812.06398v3) - [pdf](http://arxiv.org/pdf/1812.06398v3)

> As Computer Vision moves from a passive analysis of pixels to active analysis of semantics, the breadth of information algorithms need to reason over has expanded significantly. One of the key challenges in this vein is the ability to identify the information required to make a decision, and select an action that will recover it. We propose a reinforcement-learning approach that maintains a distribution over its internal information, thus explicitly representing the ambiguity in what it knows, and needs to know, towards achieving its goal. Potential actions are then generated according to this distribution. For each potential action a distribution of the expected outcomes is calculated, and the value of the potential information gain assessed. The action taken is that which maximizes the potential information gain. We demonstrate this approach applied to two vision-and-language problems that have attracted significant recent interest, visual dialog and visual query generation. In both cases, the method actively selects actions that will best reduce its internal uncertainty and outperforms its competitors in achieving the goal of the challenge.

</details>

<details>

<summary>2020-03-30 05:42:03 - AliCoCo: Alibaba E-commerce Cognitive Concept Net</summary>

- *Xusheng Luo, Luxin Liu, Yonghua Yang, Le Bo, Yuanpeng Cao, Jinhang Wu, Qiang Li, Keping Yang, Kenny Q. Zhu*

- `2003.13230v1` - [abs](http://arxiv.org/abs/2003.13230v1) - [pdf](http://arxiv.org/pdf/2003.13230v1)

> One of the ultimate goals of e-commerce platforms is to satisfy various shopping needs for their customers. Much efforts are devoted to creating taxonomies or ontologies in e-commerce towards this goal. However, user needs in e-commerce are still not well defined, and none of the existing ontologies has the enough depth and breadth for universal user needs understanding. The semantic gap in-between prevents shopping experience from being more intelligent. In this paper, we propose to construct a large-scale e-commerce cognitive concept net named "AliCoCo", which is practiced in Alibaba, the largest Chinese e-commerce platform in the world. We formally define user needs in e-commerce, then conceptualize them as nodes in the net. We present details on how AliCoCo is constructed semi-automatically and its successful, ongoing and potential applications in e-commerce.

</details>

<details>

<summary>2020-03-30 11:51:32 - MetNet: A Neural Weather Model for Precipitation Forecasting</summary>

- *Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner*

- `2003.12140v2` - [abs](http://arxiv.org/abs/2003.12140v2) - [pdf](http://arxiv.org/pdf/2003.12140v2)

> Weather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km$^2$ and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.

</details>

<details>

<summary>2020-03-30 13:53:26 - A Context-Aware Loss Function for Action Spotting in Soccer Videos</summary>

- *Anthony Cioppa, Adrien Deliège, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck, Rikke Gade, Thomas B. Moeslund*

- `1912.01326v3` - [abs](http://arxiv.org/abs/1912.01326v3) - [pdf](http://arxiv.org/pdf/1912.01326v3)

> In video understanding, action spotting consists in temporally localizing human-induced events annotated with single timestamps. In this paper, we propose a novel loss function that specifically considers the temporal context naturally present around each action, rather than focusing on the single annotated frame to spot. We benchmark our loss on a large dataset of soccer videos, SoccerNet, and achieve an improvement of 12.8% over the baseline. We show the generalization capability of our loss for generic activity proposals and detection on ActivityNet, by spotting the beginning and the end of each activity. Furthermore, we provide an extended ablation study and display challenging cases for action spotting in soccer videos. Finally, we qualitatively illustrate how our loss induces a precise temporal understanding of actions and show how such semantic knowledge can be used for automatic highlights generation.

</details>

<details>

<summary>2020-03-30 17:19:26 - A CNN-RNN Framework for Image Annotation from Visual Cues and Social Network Metadata</summary>

- *Tobia Tesan, Pasquale Coscia, Lamberto Ballan*

- `1910.05770v2` - [abs](http://arxiv.org/abs/1910.05770v2) - [pdf](http://arxiv.org/pdf/1910.05770v2)

> Images represent a commonly used form of visual communication among people. Nevertheless, image classification may be a challenging task when dealing with unclear or non-common images needing more context to be correctly annotated. Metadata accompanying images on social-media represent an ideal source of additional information for retrieving proper neighborhoods easing image annotation task. To this end, we blend visual features extracted from neighbors and their metadata to jointly leverage context and visual cues. Our models use multiple semantic embeddings to achieve the dual objective of being robust to vocabulary changes between train and test sets and decoupling the architecture from the low-level metadata representation. Convolutional and recurrent neural networks (CNNs-RNNs) are jointly adopted to infer similarity among neighbors and query images. We perform comprehensive experiments on the NUS-WIDE dataset showing that our models outperform state-of-the-art architectures based on images and metadata, and decrease both sensory and semantic gaps to better annotate images.

</details>

<details>

<summary>2020-03-30 17:51:57 - Semantic Private Information Retrieval</summary>

- *Sajani Vithana, Karim Banawan, Sennur Ulukus*

- `2003.13667v1` - [abs](http://arxiv.org/abs/2003.13667v1) - [pdf](http://arxiv.org/pdf/2003.13667v1)

> We investigate the problem of semantic private information retrieval (semantic PIR). In semantic PIR, a user retrieves a message out of $K$ independent messages stored in $N$ replicated and non-colluding databases without revealing the identity of the desired message to any individual database. The messages come with \emph{different semantics}, i.e., the messages are allowed to have \emph{non-uniform a priori probabilities} denoted by $(p_i>0,\: i \in [K])$, which are a proxy for their respective popularity of retrieval, and \emph{arbitrary message sizes} $(L_i,\: i \in [K])$. This is a generalization of the classical private information retrieval (PIR) problem, where messages are assumed to have equal a priori probabilities and equal message sizes. We derive the semantic PIR capacity for general $K$, $N$. The results show that the semantic PIR capacity depends on the number of databases $N$, the number of messages $K$, the a priori probability distribution of messages $p_i$, and the message sizes $L_i$. We present two achievable semantic PIR schemes: The first one is a deterministic scheme which is based on message asymmetry. This scheme employs non-uniform subpacketization. The second scheme is probabilistic and is based on choosing one query set out of multiple options at random to retrieve the required message without the need for exponential subpacketization. We derive necessary and sufficient conditions for the semantic PIR capacity to exceed the classical PIR capacity with equal priors and sizes. Our results show that the semantic PIR capacity can be larger than the classical PIR capacity when longer messages have higher popularities. However, when messages are equal-length, the non-uniform priors cannot be exploited to improve the retrieval rate over the classical PIR capacity.

</details>

<details>

<summary>2020-03-30 17:57:09 - Deep reinforcement learning for large-scale epidemic control</summary>

- *Pieter Libin, Arno Moonens, Timothy Verstraeten, Fabian Perez-Sanjines, Niel Hens, Philippe Lemey, Ann Nowé*

- `2003.13676v1` - [abs](http://arxiv.org/abs/2003.13676v1) - [pdf](http://arxiv.org/pdf/2003.13676v1)

> Epidemics of infectious diseases are an important threat to public health and global economies. Yet, the development of prevention strategies remains a challenging process, as epidemics are non-linear and complex processes. For this reason, we investigate a deep reinforcement learning approach to automatically learn prevention strategies in the context of pandemic influenza. Firstly, we construct a new epidemiological meta-population model, with 379 patches (one for each administrative district in Great Britain), that adequately captures the infection process of pandemic influenza. Our model balances complexity and computational efficiency such that the use of reinforcement learning techniques becomes attainable. Secondly, we set up a ground truth such that we can evaluate the performance of the 'Proximal Policy Optimization' algorithm to learn in a single district of this epidemiological model. Finally, we consider a large-scale problem, by conducting an experiment where we aim to learn a joint policy to control the districts in a community of 11 tightly coupled districts, for which no ground truth can be established. This experiment shows that deep reinforcement learning can be used to learn mitigation policies in complex epidemiological models with a large state space. Moreover, through this experiment, we demonstrate that there can be an advantage to consider collaboration between districts when designing prevention strategies.

</details>

<details>

<summary>2020-03-30 19:42:35 - ManiGAN: Text-Guided Image Manipulation</summary>

- *Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr*

- `1912.06203v2` - [abs](http://arxiv.org/abs/1912.06203v2) - [pdf](http://arxiv.org/pdf/1912.06203v2)

> The goal of our paper is to semantically edit parts of an image matching a given text that describes desired attributes (e.g., texture, colour, and background), while preserving other contents that are irrelevant to the text. To achieve this, we propose a novel generative adversarial network (ManiGAN), which contains two key components: text-image affine combination module (ACM) and detail correction module (DCM). The ACM selects image regions relevant to the given text and then correlates the regions with corresponding semantic words for effective manipulation. Meanwhile, it encodes original image features to help reconstruct text-irrelevant contents. The DCM rectifies mismatched attributes and completes missing contents of the synthetic image. Finally, we suggest a new metric for evaluating image manipulation results, in terms of both the generation of new attributes and the reconstruction of text-irrelevant contents. Extensive experiments on the CUB and COCO datasets demonstrate the superior performance of the proposed method. Code is available at https://github.com/mrlibw/ManiGAN.

</details>

<details>

<summary>2020-03-31 01:31:12 - Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation</summary>

- *Hao Tang, Dan Xu, Yan Yan, Philip H. S. Torr, Nicu Sebe*

- `1912.12215v3` - [abs](http://arxiv.org/abs/1912.12215v3) - [pdf](http://arxiv.org/pdf/1912.12215v3)

> In this paper, we address the task of semantic-guided scene generation. One open challenge in scene generation is the difficulty of the generation of small objects and detailed local texture, which has been widely observed in global image-level generation methods. To tackle this issue, in this work we consider learning the scene generation in a local context, and correspondingly design a local class-specific generative network with semantic maps as a guidance, which separately constructs and learns sub-generators concentrating on the generation of different classes, and is able to provide more scene details. To learn more discriminative class-specific feature representations for the local generation, a novel classification module is also proposed. To combine the advantage of both the global image-level and the local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Extensive experiments on two scene image generation tasks show superior generation performance of the proposed model. The state-of-the-art results are established by large margins on both tasks and on challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.

</details>

<details>

<summary>2020-03-31 02:05:27 - Proxy Anchor Loss for Deep Metric Learning</summary>

- *Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak*

- `2003.13911v1` - [abs](http://arxiv.org/abs/2003.13911v1) - [pdf](http://arxiv.org/pdf/2003.13911v1)

> Existing metric learning losses can be categorized into two classes: pair-based and proxy-based losses. The former class can leverage fine-grained semantic relations between data points, but slows convergence in general due to its high training complexity. In contrast, the latter class enables fast and reliable convergence, but cannot consider the rich data-to-data relations. This paper presents a new proxy-based loss that takes advantages of both pair- and proxy-based methods and overcomes their limitations. Thanks to the use of proxies, our loss boosts the speed of convergence and is robust against noisy labels and outliers. At the same time, it allows embedding vectors of data to interact with each other in its gradients to exploit data-to-data relations. Our method is evaluated on four public benchmarks, where a standard network trained with our loss achieves state-of-the-art performance and most quickly converges.

</details>

<details>

<summary>2020-03-31 02:07:33 - Y-net: Multi-scale feature aggregation network with wavelet structure similarity loss function for single image dehazing</summary>

- *Hao-Hsiang Yang, Chao-Han Huck Yang, Yi-Chang James Tsai*

- `2003.13912v1` - [abs](http://arxiv.org/abs/2003.13912v1) - [pdf](http://arxiv.org/pdf/2003.13912v1)

> Single image dehazing is the ill-posed two-dimensional signal reconstruction problem. Recently, deep convolutional neural networks (CNN) have been successfully used in many computer vision problems. In this paper, we propose a Y-net that is named for its structure. This network reconstructs clear images by aggregating multi-scale features maps. Additionally, we propose a Wavelet Structure SIMilarity (W-SSIM) loss function in the training step. In the proposed loss function, discrete wavelet transforms are applied repeatedly to divide the image into differently sized patches with different frequencies and scales. The proposed loss function is the accumulation of SSIM loss of various patches with respective ratios. Extensive experimental results demonstrate that the proposed Y-net with the W-SSIM loss function restores high-quality clear images and outperforms state-of-the-art algorithms. Code and models are available at https://github.com/dectrfov/Y-net.

</details>

<details>

<summary>2020-03-31 03:51:52 - MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</summary>

- *Xinyu Fu, Jiani Zhang, Ziqiao Meng, Irwin King*

- `2002.01680v2` - [abs](http://arxiv.org/abs/2002.01680v2) - [pdf](http://arxiv.org/pdf/2002.01680v2)

> A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.

</details>

<details>

<summary>2020-03-31 05:12:31 - SPARQA: Skeleton-based Semantic Parsing for Complex Questions over Knowledge Bases</summary>

- *Yawei Sun, Lingling Zhang, Gong Cheng, Yuzhong Qu*

- `2003.13956v1` - [abs](http://arxiv.org/abs/2003.13956v1) - [pdf](http://arxiv.org/pdf/2003.13956v1)

> Semantic parsing transforms a natural language question into a formal query over a knowledge base. Many existing methods rely on syntactic parsing like dependencies. However, the accuracy of producing such expressive formalisms is not satisfying on long complex questions. In this paper, we propose a novel skeleton grammar to represent the high-level structure of a complex question. This dedicated coarse-grained formalism with a BERT-based parsing algorithm helps to improve the accuracy of the downstream fine-grained semantic parsing. Besides, to align the structure of a question with the structure of a knowledge base, our multi-strategy method combines sentence-level and word-level semantics. Our approach shows promising performance on several datasets.

</details>

<details>

<summary>2020-03-31 07:00:59 - Deep semantic gaze embedding and scanpath comparison for expertise classification during OPT viewing</summary>

- *Nora Castner, Thomas Kübler, Katharina Scheiter, Juilane Richter, Thérése Eder, Fabian Hüttig, Constanze Keutel, Enkelejda Kasneci*

- `2003.13987v1` - [abs](http://arxiv.org/abs/2003.13987v1) - [pdf](http://arxiv.org/pdf/2003.13987v1)

> Modeling eye movement indicative of expertise behavior is decisive in user evaluation. However, it is indisputable that task semantics affect gaze behavior. We present a novel approach to gaze scanpath comparison that incorporates convolutional neural networks (CNN) to process scene information at the fixation level. Image patches linked to respective fixations are used as input for a CNN and the resulting feature vectors provide the temporal and spatial gaze information necessary for scanpath similarity comparison.We evaluated our proposed approach on gaze data from expert and novice dentists interpreting dental radiographs using a local alignment similarity score. Our approach was capable of distinguishing experts from novices with 93% accuracy while incorporating the image semantics. Moreover, our scanpath comparison using image patch features has the potential to incorporate task semantics from a variety of tasks

</details>

<details>

<summary>2020-03-31 08:52:13 - Distilled Semantics for Comprehensive Scene Understanding from Videos</summary>

- *Fabio Tosi, Filippo Aleotti, Pierluigi Zama Ramirez, Matteo Poggi, Samuele Salti, Luigi Di Stefano, Stefano Mattoccia*

- `2003.14030v1` - [abs](http://arxiv.org/abs/2003.14030v1) - [pdf](http://arxiv.org/pdf/2003.14030v1)

> Whole understanding of the surroundings is paramount to autonomous systems. Recent works have shown that deep neural networks can learn geometry (depth) and motion (optical flow) from a monocular video without any explicit supervision from ground truth annotations, particularly hard to source for these two tasks. In this paper, we take an additional step toward holistic scene understanding with monocular cameras by learning depth and motion alongside with semantics, with supervision for the latter provided by a pre-trained network distilling proxy ground truth images. We address the three tasks jointly by a) a novel training protocol based on knowledge distillation and self-supervision and b) a compact network architecture which enables efficient scene understanding on both power hungry GPUs and low-power embedded platforms. We thoroughly assess the performance of our framework and show that it yields state-of-the-art results for monocular depth estimation, optical flow and motion segmentation.

</details>

<details>

<summary>2020-03-31 15:27:31 - Graph Enhanced Representation Learning for News Recommendation</summary>

- *Suyu Ge, Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang*

- `2003.14292v1` - [abs](http://arxiv.org/abs/2003.14292v1) - [pdf](http://arxiv.org/pdf/2003.14292v1)

> With the explosion of online news, personalized news recommendation becomes increasingly important for online news platforms to help their users find interesting information. Existing news recommendation methods achieve personalization by building accurate news representations from news content and user representations from their direct interactions with news (e.g., click), while ignoring the high-order relatedness between users and news. Here we propose a news recommendation method which can enhance the representation learning of users and news by modeling their relatedness in a graph setting. In our method, users and news are both viewed as nodes in a bipartite graph constructed from historical user click behaviors. For news representations, a transformer architecture is first exploited to build news semantic representations. Then we combine it with the information from neighbor news in the graph via a graph attention network. For user representations, we not only represent users from their historically clicked news, but also attentively incorporate the representations of their neighbor users in the graph. Improved performances on a large-scale real-world dataset validate the effectiveness of our proposed method.

</details>

<details>

<summary>2020-03-31 15:42:21 - RESTORE: Retrospective Fault Localization Enhancing Automated Program Repair</summary>

- *Tongtong Xu, Liushan Chen, Yu Pei, Tian Zhang, Minxue Pan, Carlo A. Furia*

- `1906.01778v3` - [abs](http://arxiv.org/abs/1906.01778v3) - [pdf](http://arxiv.org/pdf/1906.01778v3)

> Fault localization is a crucial step of automated program repair, because accurately identifying program locations that are most closely implicated with a fault greatly affects the effectiveness of the patching process. An ideal fault localization technique would provide precise information while requiring moderate computational resources---to best support an efficient search for correct fixes. In contrast, most automated program repair tools use standard fault localization techniques---which are not tightly integrated with the overall program repair process, and hence deliver only subpar efficiency.   In this paper, we present retrospective fault localization: a novel fault localization technique geared to the requirements of automated program repair. A key idea of retrospective fault localization is to reuse the outcome of failed patch validation to support mutation-based dynamic analysis---providing accurate fault localization information without incurring onerous computational costs.   We implemented retrospective fault localization in a tool called RESTORE---based on the JAID Java program repair system. Experiments involving faults from the Defects4J standard benchmark indicate that retrospective fault localization can boost automated program repair: RESTORE efficiently explores a large fix space, delivering state-of-the-art effectiveness (41 Defects4J bugs correctly fixed, 8 more than any other automated repair tools for Java) while simultaneously boosting performance (speedup over 3 compared to JAID). Retrospective fault localization is applicable to any automated program repair techniques that rely on fault localization and dynamic validation of patches.

</details>

<details>

<summary>2020-03-31 16:47:20 - Binary Neural Networks: A Survey</summary>

- *Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu Sebe*

- `2004.03333v1` - [abs](http://arxiv.org/abs/2004.03333v1) - [pdf](http://arxiv.org/pdf/2004.03333v1)

> The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.

</details>

<details>

<summary>2020-03-31 18:13:34 - Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data</summary>

- *Yen-Chang Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira*

- `2002.11297v2` - [abs](http://arxiv.org/abs/2002.11297v2) - [pdf](http://arxiv.org/pdf/2002.11297v2)

> Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OoD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OoD benchmarks consisting of small image datasets. However, many recent methods based on neural networks rely on training or tuning with both in-distribution and out-of-distribution data. The latter is generally hard to define a-priori, and its selection can easily bias the learning. We base our work on a popular method ODIN, proposing two strategies for freeing it from the needs of tuning with OoD data, while improving its OoD detection performance. We specifically propose to decompose confidence scoring as well as a modified input pre-processing method. We show that both of these significantly help in detection performance. Our further analysis on a larger scale image dataset shows that the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem, providing an analysis of when ODIN-like strategies do or do not work.

</details>

<details>

<summary>2020-03-31 20:22:10 - Automatic Extraction of Bengali Root Verbs using Paninian Grammar</summary>

- *Arijit Das, Tapas Halder, Diganta Saha*

- `2004.00089v1` - [abs](http://arxiv.org/abs/2004.00089v1) - [pdf](http://arxiv.org/pdf/2004.00089v1)

> In this research work, we have proposed an algorithm based on supervised learning methodology to extract the root forms of the Bengali verbs using the grammatical rules proposed by Panini [1] in Ashtadhyayi. This methodology can be applied for the languages which are derived from Sanskrit. The proposed system has been developed based on tense, person and morphological inflections of the verbs to find their root forms. The work has been executed in two phases: first, the surface level forms or inflected forms of the verbs have been classified into a certain number of groups of similar tense and person. For this task, a standard pattern, available in Bengali language has been used. Next, a set of rules have been applied to extract the root form from the surface level forms of a verb. The system has been tested on 10000 verbs collected from the Bengali text corpus developed in the TDIL project of the Govt. of India. The accuracy of the output has been achieved 98% which is verified by a linguistic expert. Root verb identification is a key step in semantic searching, multi-sentence search query processing, understanding the meaning of a language, disambiguation of word sense, classification of the sentences etc.

</details>

<details>

<summary>2020-03-31 21:22:05 - EOLO: Embedded Object Segmentation only Look Once</summary>

- *Longfei Zeng, Mohammed Sabah*

- `2004.00123v1` - [abs](http://arxiv.org/abs/2004.00123v1) - [pdf](http://arxiv.org/pdf/2004.00123v1)

> In this paper, we introduce an anchor-free and single-shot instance segmentation method, which is conceptually simple with 3 independent branches, fully convolutional and can be used by easily embedding it into mobile and embedded devices.   Our method, refer as EOLO, reformulates the instance segmentation problem as predicting semantic segmentation and distinguishing overlapping objects problem, through instance center classification and 4D distance regression on each pixel. Moreover, we propose one effective loss function to deal with sampling a high-quality center of gravity examples and optimization for 4D distance regression, which can significantly improve the mAP performance. Without any bells and whistles, EOLO achieves 27.7$\%$ in mask mAP under IoU50 and reaches 30 FPS on 1080Ti GPU, with a single-model and single-scale training/testing on the challenging COCO2017 dataset.   For the first time, we show the different comprehension of instance segmentation in recent methods, in terms of both up-bottom, down-up, and direct-predict paradigms. Then we illustrate our model and present related experiments and results. We hope that the proposed EOLO framework can serve as a fundamental baseline for a single-shot instance segmentation task in Real-time Industrial Scenarios.

</details>


## 2020-04

<details>

<summary>2020-04-01 02:51:57 - Ontology-based Interpretable Machine Learning for Textual Data</summary>

- *Phung Lai, NhatHai Phan, Han Hu, Anuja Badeti, David Newman, Dejing Dou*

- `2004.00204v1` - [abs](http://arxiv.org/abs/2004.00204v1) - [pdf](http://arxiv.org/pdf/2004.00204v1)

> In this paper, we introduce a novel interpreting framework that learns an interpretable model based on an ontology-based sampling technique to explain agnostic prediction models. Different from existing approaches, our algorithm considers contextual correlation among words, described in domain knowledge ontologies, to generate semantic explanations. To narrow down the search space for explanations, which is a major problem of long and complicated text data, we design a learnable anchor algorithm, to better extract explanations locally. A set of regulations is further introduced, regarding combining learned interpretable representations with anchors to generate comprehensible semantic explanations. An extensive experiment conducted on two real-world datasets shows that our approach generates more precise and insightful explanations compared with baseline approaches.

</details>

<details>

<summary>2020-04-01 03:14:54 - KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm</summary>

- *Han Xiao, Minlie Huang, Xiaoyan Zhu*

- `1608.07685v8` - [abs](http://arxiv.org/abs/1608.07685v8) - [pdf](http://arxiv.org/pdf/1608.07685v8)

> Knowledge representation is a long-history topic in AI, which is very important. A variety of models have been proposed for knowledge graph embedding, which projects symbolic entities and relations into continuous vector space. However, most related methods merely focus on the data-fitting of knowledge graph, and ignore the interpretable semantic expression. Thus, traditional embedding methods are not friendly for applications that require semantic analysis, such as question answering and entity retrieval. To this end, this paper proposes a semantic representation method for knowledge graph \textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments show that our model outperforms other state-of-the-art baselines substantially.

</details>

<details>

<summary>2020-04-01 05:34:29 - MaskGAN: Towards Diverse and Interactive Facial Image Manipulation</summary>

- *Cheng-Han Lee, Ziwei Liu, Lingyun Wu, Ping Luo*

- `1907.11922v2` - [abs](http://arxiv.org/abs/1907.11922v2) - [pdf](http://arxiv.org/pdf/1907.11922v2)

> Facial image manipulation has achieved great progress in recent years. However, previous methods either operate on a predefined set of face attributes or leave users little freedom to interactively manipulate images. To overcome these drawbacks, we propose a novel framework termed MaskGAN, enabling diverse and interactive face manipulation. Our key insight is that semantic masks serve as a suitable intermediate representation for flexible face manipulation with fidelity preservation. MaskGAN has two main components: 1) Dense Mapping Network (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically, DMN learns style mapping between a free-form user modified mask and a target image, enabling diverse generation results. EBST models the user editing behavior on the source mask, making the overall framework more robust to various manipulated inputs. Specifically, it introduces dual-editing consistency as the auxiliary supervision signal. To facilitate extensive studies, we construct a large-scale high-resolution face dataset with fine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively evaluated on two challenging tasks: attribute transfer and style copy, demonstrating superior performance over other state-of-the-art methods. The code, models, and dataset are available at https://github.com/switchablenorms/CelebAMask-HQ.

</details>

<details>

<summary>2020-04-01 07:10:08 - DSTC8-AVSD: Multimodal Semantic Transformer Network with Retrieval Style Word Generator</summary>

- *Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Kyomin Jung*

- `2004.08299v1` - [abs](http://arxiv.org/abs/2004.08299v1) - [pdf](http://arxiv.org/pdf/2004.08299v1)

> Audio Visual Scene-aware Dialog (AVSD) is the task of generating a response for a question with a given scene, video, audio, and the history of previous turns in the dialog. Existing systems for this task employ the transformers or recurrent neural network-based architecture with the encoder-decoder framework. Even though these techniques show superior performance for this task, they have significant limitations: the model easily overfits only to memorize the grammatical patterns; the model follows the prior distribution of the vocabularies in a dataset. To alleviate the problems, we propose a Multimodal Semantic Transformer Network. It employs a transformer-based architecture with an attention-based word embedding layer that generates words by querying word embeddings. With this design, our model keeps considering the meaning of the words at the generation stage. The empirical results demonstrate the superiority of our proposed model that outperforms most of the previous works for the AVSD task.

</details>

<details>

<summary>2020-04-01 08:49:39 - Collective Entity Alignment via Adaptive Features</summary>

- *Weixin Zeng, Xiang Zhao, Jiuyang Tang, Xuemin Lin*

- `1912.08404v3` - [abs](http://arxiv.org/abs/1912.08404v3) - [pdf](http://arxiv.org/pdf/1912.08404v3)

> Entity alignment (EA) identifies entities that refer to the same real-world object but locate in different knowledge graphs (KGs), and has been harnessed for KG construction and integration. When generating EA results, current solutions treat entities independently and fail to take into account the interdependence between entities. To fill this gap, we propose a collective EA framework. We first employ three representative features, i.e., structural, semantic and string signals, which are adapted to capture different aspects of the similarity between entities in heterogeneous KGs. In order to make collective EA decisions, we formulate EA as the classical stable matching problem, which is further effectively solved by deferred acceptance algorithm. Our proposal is evaluated on both cross-lingual and mono-lingual EA benchmarks against state-of-the-art solutions, and the empirical results verify its effectiveness and superiority.

</details>

<details>

<summary>2020-04-01 08:58:25 - Impact of Semantic Granularity on Geographic Information Search Support</summary>

- *Noemi Mauro, Liliana Ardissono, Laura Di Rocco, Michela Bertolotto, Giovanna Guerrini*

- `2004.00293v1` - [abs](http://arxiv.org/abs/2004.00293v1) - [pdf](http://arxiv.org/pdf/2004.00293v1)

> The Information Retrieval research has used semantics to provide accurate search results, but the analysis of conceptual abstraction has mainly focused on information integration. We consider session-based query expansion in Geographical Information Retrieval, and investigate the impact of semantic granularity (i.e., specificity of concepts representation) on the suggestion of relevant types of information to search for. We study how different levels of detail in knowledge representation influence the capability of guiding the user in the exploration of a complex information space. A comparative analysis of the performance of a query expansion model, using three spatial ontologies defined at different semantic granularity levels, reveals that a fine-grained representation enhances recall. However, precision depends on how closely the ontologies match the way people conceptualize and verbally describe the geographic space.

</details>

<details>

<summary>2020-04-01 13:31:19 - Semantic Drift Compensation for Class-Incremental Learning</summary>

- *Lu Yu, Bartłomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, Joost van de Weijer*

- `2004.00440v1` - [abs](http://arxiv.org/abs/2004.00440v1) - [pdf](http://arxiv.org/pdf/2004.00440v1)

> Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results.

</details>

<details>

<summary>2020-04-01 15:14:40 - Tree bark re-identification using a deep-learning feature descriptor</summary>

- *Martin Robert, Patrick Dallaire, Philippe Giguère*

- `1912.03221v2` - [abs](http://arxiv.org/abs/1912.03221v2) - [pdf](http://arxiv.org/pdf/1912.03221v2)

> The ability to visually re-identify objects is a fundamental capability in vision systems. Oftentimes, it relies on collections of visual signatures based on descriptors, such as SIFT or SURF. However, these traditional descriptors were designed for a certain domain of surface appearances and geometries (limited relief). Consequently, highly-textured surfaces such as tree bark pose a challenge to them. In turn, this makes it more difficult to use trees as identifiable landmarks for navigational purposes (robotics) or to track felled lumber along a supply chain (logistics). We thus propose to use data-driven descriptors trained on bark images for tree surface re-identification. To this effect, we collected a large dataset containing 2,400 bark images with strong illumination changes, annotated by surface and with the ability to pixel-align them. We used this dataset to sample from more than 2 million 64x64 pixel patches to train our novel local descriptors DeepBark and SqueezeBark. Our DeepBark method has shown a clear advantage against the hand-crafted descriptors SIFT and SURF. For instance, we demonstrated that DeepBark can reach a mAP of 87.2% when retrieving 11 relevant bark images, i.e. corresponding to the same physical surface, to a bark query against 7,900 images. Our work thus suggests that re-identifying tree surfaces in a challenging illuminations context is possible. We also make public our dataset, which can be used to benchmark surface re-identification techniques.

</details>

<details>

<summary>2020-04-01 18:29:28 - Self-Supervised Learning of Video-Induced Visual Invariances</summary>

- *Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Xiaohua Zhai, Neil Houlsby, Sylvain Gelly, Mario Lucic*

- `1912.02783v2` - [abs](http://arxiv.org/abs/1912.02783v2) - [pdf](http://arxiv.org/pdf/1912.02783v2)

> We propose a general framework for self-supervised learning of transferable visual representations based on Video-Induced Visual Invariances (VIVI). We consider the implicit hierarchy present in the videos and make use of (i) frame-level invariances (e.g. stability to color and contrast perturbations), (ii) shot/clip-level invariances (e.g. robustness to changes in object orientation and lighting conditions), and (iii) video-level invariances (semantic relationships of scenes across shots/clips), to define a holistic self-supervised loss. Training models using different variants of the proposed framework on videos from the YouTube-8M (YT8M) data set, we obtain state-of-the-art self-supervised transfer learning results on the 19 diverse downstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only 1000 labels per task. We then show how to co-train our models jointly with labeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points with 10x fewer labeled images, as well as the previous best supervised model by 3.7 points using the full ImageNet data set.

</details>

<details>

<summary>2020-04-01 23:14:52 - On adversarial patches: real-world attack on ArcFace-100 face recognition system</summary>

- *Mikhail Pautov, Grigorii Melnikov, Edgar Kaziakhmedov, Klim Kireev, Aleksandr Petiushko*

- `1910.07067v3` - [abs](http://arxiv.org/abs/1910.07067v3) - [pdf](http://arxiv.org/pdf/1910.07067v3)

> Recent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.

</details>

<details>

<summary>2020-04-02 08:09:06 - Semantic Image Search for Robotic Applications</summary>

- *Tomas Kulvicius, Irene Markelic, Minija Tamosiunaite, Florentin Wörgötter*

- `2004.02607v1` - [abs](http://arxiv.org/abs/2004.02607v1) - [pdf](http://arxiv.org/pdf/2004.02607v1)

> Generalization in robotics is one of the most important problems. New generalization approaches use internet databases in order to solve new tasks. Modern search engines can return a large amount of information according to a query within milliseconds. However, not all of the returned information is task relevant, partly due to the problem of polysemes. Here we specifically address the problem of object generalization by using image search. We suggest a bi-modal solution, combining visual and textual information, based on the observation that humans use additional linguistic cues to demarcate intended word meaning. We evaluate the quality of our approach by comparing it to human labelled data and find that, on average, our approach leads to improved results in comparison to Google searches, and that it can treat the problem of polysemes.

</details>

<details>

<summary>2020-04-02 11:22:26 - CORSICA: Cross-Origin Web Service Identification</summary>

- *Christian Dresen, Fabian Ising, Damian Poddebniak, Tobias Kappert, Thorsten Holz, Sebastian Schinzel*

- `2004.00939v1` - [abs](http://arxiv.org/abs/2004.00939v1) - [pdf](http://arxiv.org/pdf/2004.00939v1)

> Vulnerabilities in private networks are difficult to detect for attackers outside of the network. While there are known methods for port scanning internal hosts that work by luring unwitting internal users to an external web page that hosts malicious JavaScript code, no such method for detailed and precise service identification is known. The reason is that the Same Origin Policy (SOP) prevents access to HTTP responses of other origins by default. We perform a structured analysis of loopholes in the SOP that can be used to identify web applications across network boundaries. For this, we analyze HTML5, CSS, and JavaScript features of standard-compliant web browsers that may leak sensitive information about cross-origin content. The results reveal several novel techniques, including leaking JavaScript function names or styles of cross-origin requests that are available in all common browsers. We implement and test these techniques in a tool called CORSICA. It can successfully identify 31 of 42 (74%) of web services running on different IoT devices as well as the version numbers of the four most widely used content management systems WordPress, Drupal, Joomla, and TYPO3. CORSICA can also determine the patch level on average down to three versions (WordPress), six versions (Drupal), two versions (Joomla), and four versions (TYPO3) with only ten requests on average. Furthermore, CORSICA is able to identify 48 WordPress plugins containing 65 vulnerabilities. Finally, we analyze mitigation strategies and show that the proposed but not yet implemented strategies Cross-Origin Resource Policy (CORP)} and Sec-Metadata would prevent our identification techniques.

</details>

<details>

<summary>2020-04-02 16:21:14 - An Attention-Based Deep Learning Model for Multiple Pedestrian Attributes Recognition</summary>

- *Ehsan Yaghoubi, Diana Borza, João Neves, Aruna Kumar, Hugo Proença*

- `2004.01110v1` - [abs](http://arxiv.org/abs/2004.01110v1) - [pdf](http://arxiv.org/pdf/2004.01110v1)

> The automatic characterization of pedestrians in surveillance footage is a tough challenge, particularly when the data is extremely diverse with cluttered backgrounds, and subjects are captured from varying distances, under multiple poses, with partial occlusion. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with two-fold contributions: 1) considering the strong semantic correlation between the different full-body attributes, we propose a multi-task deep model that uses an element-wise multiplication layer to extract more comprehensive feature representations. In practice, this layer serves as a filter to remove irrelevant background features, and is particularly important to handle complex, cluttered data; and 2) we introduce a weighted-sum term to the loss function that not only relativizes the contribution of each task (kind of attributed) but also is crucial for performance improvement in multiple-attribute inference settings. Our experiments were performed on two well-known datasets (RAP and PETA) and point for the superiority of the proposed method with respect to the state-of-the-art. The code is available at https://github.com/Ehsan-Yaghoubi/MAN-PAR-.

</details>

<details>

<summary>2020-04-03 08:42:24 - Batch-Shaping for Learning Conditional Channel Gated Networks</summary>

- *Babak Ehteshami Bejnordi, Tijmen Blankevoort, Max Welling*

- `1907.06627v4` - [abs](http://arxiv.org/abs/1907.06627v4) - [pdf](http://arxiv.org/pdf/1907.06627v4)

> We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool $batch$-$shaping$ that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.

</details>

<details>

<summary>2020-04-03 08:49:40 - Modeling Rare Interactions in Time Series Data Through Qualitative Change: Application to Outcome Prediction in Intensive Care Units</summary>

- *Zina Ibrahim, Honghan Wu, Richard Dobson*

- `2004.01431v1` - [abs](http://arxiv.org/abs/2004.01431v1) - [pdf](http://arxiv.org/pdf/2004.01431v1)

> Many areas of research are characterised by the deluge of large-scale highly-dimensional time-series data. However, using the data available for prediction and decision making is hampered by the current lag in our ability to uncover and quantify true interactions that explain the outcomes.We are interested in areas such as intensive care medicine, which are characterised by i) continuous monitoring of multivariate variables and non-uniform sampling of data streams, ii) the outcomes are generally governed by interactions between a small set of rare events, iii) these interactions are not necessarily definable by specific values (or value ranges) of a given group of variables, but rather, by the deviations of these values from the normal state recorded over time, iv) the need to explain the predictions made by the model. Here, while numerous data mining models have been formulated for outcome prediction, they are unable to explain their predictions.   We present a model for uncovering interactions with the highest likelihood of generating the outcomes seen from highly-dimensional time series data. Interactions among variables are represented by a relational graph structure, which relies on qualitative abstractions to overcome non-uniform sampling and to capture the semantics of the interactions corresponding to the changes and deviations from normality of variables of interest over time. Using the assumption that similar templates of small interactions are responsible for the outcomes (as prevalent in the medical domains), we reformulate the discovery task to retrieve the most-likely templates from the data.

</details>

<details>

<summary>2020-04-03 23:08:01 - Real-Time Panoptic Segmentation from Dense Detections</summary>

- *Rui Hou, Jie Li, Arjun Bhargava, Allan Raventos, Vitor Guizilini, Chao Fang, Jerome Lynch, Adrien Gaidon*

- `1912.01202v3` - [abs](http://arxiv.org/abs/1912.01202v3) - [pdf](http://arxiv.org/pdf/1912.01202v3)

> Panoptic segmentation is a complex full scene parsing task requiring simultaneous instance and semantic segmentation at high resolution. Current state-of-the-art approaches cannot run in real-time, and simplifying these architectures to improve efficiency severely degrades their accuracy. In this paper, we propose a new single-shot panoptic segmentation network that leverages dense detections and a global self-attention mechanism to operate in real-time with performance approaching the state of the art. We introduce a novel parameter-free mask construction method that substantially reduces computational complexity by efficiently reusing information from the object detection and semantic segmentation sub-tasks. The resulting network has a simple data flow that does not require feature map re-sampling or clustering post-processing, enabling significant hardware acceleration. Our experiments on the Cityscapes and COCO benchmarks show that our network works at 30 FPS on 1024x2048 resolution, trading a 3% relative performance degradation from the current state of the art for up to 440% faster inference.

</details>

<details>

<summary>2020-04-04 09:03:04 - Evaluating Multimodal Representations on Visual Semantic Textual Similarity</summary>

- *Oier Lopez de Lacalle, Ander Salaberria, Aitor Soroa, Gorka Azkune, Eneko Agirre*

- `2004.01894v1` - [abs](http://arxiv.org/abs/2004.01894v1) - [pdf](http://arxiv.org/pdf/2004.01894v1)

> The combination of visual and textual representations has produced excellent results in tasks such as image captioning and visual question answering, but the inference capabilities of multimodal representations are largely untested. In the case of textual representations, inference tasks such as Textual Entailment and Semantic Textual Similarity have been often used to benchmark the quality of textual representations. The long term goal of our research is to devise multimodal representation techniques that improve current inference capabilities. We thus present a novel task, Visual Semantic Textual Similarity (vSTS), where such inference ability can be tested directly. Given two items comprised each by an image and its accompanying caption, vSTS systems need to assess the degree to which the captions in context are semantically equivalent to each other. Our experiments using simple multimodal representations show that the addition of image representations produces better inference, compared to text-only representations. The improvement is observed both when directly computing the similarity between the representations of the two items, and when learning a siamese network based on vSTS training data. Our work shows, for the first time, the successful contribution of visual information to textual inference, with ample room for benchmarking more complex multimodal representation options.

</details>

<details>

<summary>2020-04-04 20:15:03 - Convolutional Neural Networks based automated segmentation and labelling of the lumbar spine X-ray</summary>

- *Sandor Konya, Sai Natarajan T R, Hassan Allouch, Kais Abu Nahleh, Omneya Yakout Dogheim, Heinrich Boehm*

- `2004.03364v1` - [abs](http://arxiv.org/abs/2004.03364v1) - [pdf](http://arxiv.org/pdf/2004.03364v1)

> The aim of this study is to investigate the segmentation accuracies of different segmentation networks trained on 730 manually annotated lateral lumbar spine X-rays. Instance segmentation networks were compared to semantic segmentation networks. The study cohort comprised diseased spines and postoperative images with metallic implants. The average mean accuracy and mean intersection over union (IoU) was up to 3 percent better for the best performing instance segmentation model, the average pixel accuracy and weighted IoU were slightly better for the best performing semantic segmentation model. Moreover, the inferences of the instance segmentation models are easier to implement for further processing pipelines in clinical decision support.

</details>

<details>

<summary>2020-04-04 21:01:01 - The Prolog Debugger and Declarative Programming. Examples</summary>

- *Włodzimierz Drabent*

- `2003.01422v2` - [abs](http://arxiv.org/abs/2003.01422v2) - [pdf](http://arxiv.org/pdf/2003.01422v2)

> This paper contains examples for a companion paper "The Prolog Debugger and Declarative Programming", which discusses (in)adequacy of the Prolog debugger for declarative programming.   Logic programming is a declarative programming paradigm. Programming language Prolog makes logic programming possible, at least to a substantial extent. However the Prolog debugger works solely in terms of the operational semantics. So it is incompatible with declarative programming. The companion paper tries to find methods of using it from the declarative point of view. Here we provide examples of applying them.

</details>

<details>

<summary>2020-04-05 04:28:03 - How Can BERT Help Lexical Semantics Tasks?</summary>

- *Yile Wang, Leyang Cui, Yue Zhang*

- `1911.02929v2` - [abs](http://arxiv.org/abs/1911.02929v2) - [pdf](http://arxiv.org/pdf/1911.02929v2)

> Contextualized embeddings such as BERT can serve as strong input representations to NLP tasks, outperforming their static embeddings counterparts such as skip-gram, CBOW and GloVe. However, such embeddings are dynamic, calculated according to a sentence-level context, which limits their use in lexical semantics tasks. We address this issue by making use of dynamic embeddings as word representations in training static embeddings, thereby leveraging their strong representation power for disambiguating context information. Results show that this method leads to improvements over traditional static embeddings on a range of lexical semantics tasks, obtaining the best reported results on seven datasets.

</details>

<details>

<summary>2020-04-05 14:59:22 - CLAREL: Classification via retrieval loss for zero-shot learning</summary>

- *Boris N. Oreshkin, Negar Rostamzadeh, Pedro O. Pinheiro, Christopher Pal*

- `1906.11892v3` - [abs](http://arxiv.org/abs/1906.11892v3) - [pdf](http://arxiv.org/pdf/1906.11892v3)

> We address the problem of learning fine-grained cross-modal representations. We propose an instance-based deep metric learning approach in joint visual and textual space. The key novelty of this paper is that it shows that using per-image semantic supervision leads to substantial improvement in zero-shot performance over using class-only supervision. On top of that, we provide a probabilistic justification for a metric rescaling approach that solves a very common problem in the generalized zero-shot learning setting, i.e., classifying test images from unseen classes as one of the classes seen during training. We evaluate our approach on two fine-grained zero-shot learning datasets: CUB and FLOWERS. We find that on the generalized zero-shot classification task CLAREL consistently outperforms the existing approaches on both datasets.

</details>

<details>

<summary>2020-04-05 17:43:13 - PrivFL: Practical Privacy-preserving Federated Regressions on High-dimensional Data over Mobile Networks</summary>

- *Kalikinkar Mandal, Guang Gong*

- `2004.02264v1` - [abs](http://arxiv.org/abs/2004.02264v1) - [pdf](http://arxiv.org/pdf/2004.02264v1)

> Federated Learning (FL) enables a large number of users to jointly learn a shared machine learning (ML) model, coordinated by a centralized server, where the data is distributed across multiple devices. This approach enables the server or users to train and learn an ML model using gradient descent, while keeping all the training data on users' devices. We consider training an ML model over a mobile network where user dropout is a common phenomenon. Although federated learning was aimed at reducing data privacy risks, the ML model privacy has not received much attention.   In this work, we present PrivFL, a privacy-preserving system for training (predictive) linear and logistic regression models and oblivious predictions in the federated setting, while guaranteeing data and model privacy as well as ensuring robustness to users dropping out in the network. We design two privacy-preserving protocols for training linear and logistic regression models based on an additive homomorphic encryption (HE) scheme and an aggregation protocol. Exploiting the training algorithm of federated learning, at the core of our training protocols is a secure multiparty global gradient computation on alive users' data. We analyze the security of our training protocols against semi-honest adversaries. As long as the aggregation protocol is secure under the aggregation privacy game and the additive HE scheme is semantically secure, PrivFL guarantees the users' data privacy against the server, and the server's regression model privacy against the users. We demonstrate the performance of PrivFL on real-world datasets and show its applicability in the federated learning system.

</details>

<details>

<summary>2020-04-05 18:02:07 - MNEW: Multi-domain Neighborhood Embedding and Weighting for Sparse Point Clouds Segmentation</summary>

- *Yang Zheng, Izzat H. Izzat, Sanling Song*

- `2004.03401v1` - [abs](http://arxiv.org/abs/2004.03401v1) - [pdf](http://arxiv.org/pdf/2004.03401v1)

> Point clouds have been widely adopted in 3D semantic scene understanding. However, point clouds for typical tasks such as 3D shape segmentation or indoor scenario parsing are much denser than outdoor LiDAR sweeps for the application of autonomous driving perception. Due to the spatial property disparity, many successful methods designed for dense point clouds behave depreciated effectiveness on the sparse data. In this paper, we focus on the semantic segmentation task of sparse outdoor point clouds. We propose a new method called MNEW, including multi-domain neighborhood embedding, and attention weighting based on their geometry distance, feature similarity, and neighborhood sparsity. The network architecture inherits PointNet which directly process point clouds to capture pointwise details and global semantics, and is improved by involving multi-scale local neighborhoods in static geometry domain and dynamic feature space. The distance/similarity attention and sparsity-adapted weighting mechanism of MNEW enable its capability for a wide range of data sparsity distribution. With experiments conducted on virtual and real KITTI semantic datasets, MNEW achieves the top performance for sparse point clouds, which is important to the application of LiDAR-based automated driving perception.

</details>

<details>

<summary>2020-04-06 13:46:54 - SHOP-VRB: A Visual Reasoning Benchmark for Object Perception</summary>

- *Michal Nazarczuk, Krystian Mikolajczyk*

- `2004.02673v1` - [abs](http://arxiv.org/abs/2004.02673v1) - [pdf](http://arxiv.org/pdf/2004.02673v1)

> In this paper we present an approach and a benchmark for visual reasoning in robotics applications, in particular small object grasping and manipulation. The approach and benchmark are focused on inferring object properties from visual and text data. It concerns small household objects with their properties, functionality, natural language descriptions as well as question-answer pairs for visual reasoning queries along with their corresponding scene semantic representations. We also present a method for generating synthetic data which allows to extend the benchmark to other objects or scenes and propose an evaluation protocol that is more challenging than in the existing datasets. We propose a reasoning system based on symbolic program execution. A disentangled representation of the visual and textual inputs is obtained and used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We perform a set of experiments on the proposed benchmark and compare to results for the state of the art methods. These results expose the shortcomings of the existing benchmarks that may lead to misleading conclusions on the actual performance of the visual reasoning systems.

</details>

<details>

<summary>2020-04-06 15:09:25 - Representational Rényi heterogeneity</summary>

- *Abraham Nunes, Martin Alda, Timothy Bardouille, Thomas Trappenberg*

- `1912.05031v3` - [abs](http://arxiv.org/abs/1912.05031v3) - [pdf](http://arxiv.org/pdf/1912.05031v3)

> A discrete system's heterogeneity is measured by the R\'enyi heterogeneity family of indices (also known as Hill numbers or Hannah--Kay indices), whose units are {the numbers equivalent}. Unfortunately, numbers equivalent heterogeneity measures for non-categorical data require {a priori} (A) categorical partitioning and (B) pairwise distance measurement on the observable data space, thereby precluding application to problems with ill-defined categories or where semantically relevant features must be learned as abstractions from some data. We thus introduce representational R\'enyi heterogeneity (RRH), which transforms an observable domain onto a latent space upon which the R\'enyi heterogeneity is both tractable and semantically relevant. This method requires neither {a priori} binning nor definition of a distance function on the observable space. We show that RRH can generalize existing biodiversity and economic equality indices. Compared with existing indices on a beta-mixture distribution, we show that RRH responds more appropriately to changes in mixture component separation and weighting. Finally, we demonstrate the measurement of RRH in a set of natural images, with respect to abstract representations learned by a deep neural network. The RRH approach will further enable heterogeneity measurement in disciplines whose data do not easily conform to the assumptions of existing indices.

</details>

<details>

<summary>2020-04-06 17:59:15 - DualSDF: Semantic Shape Manipulation using a Two-Level Representation</summary>

- *Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie*

- `2004.02869v1` - [abs](http://arxiv.org/abs/2004.02869v1) - [pdf](http://arxiv.org/pdf/2004.02869v1)

> We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.

</details>

<details>

<summary>2020-04-06 19:05:18 - Embedding Java Classes with code2vec: Improvements from Variable Obfuscation</summary>

- *Rhys Compton, Eibe Frank, Panos Patros, Abigail Koay*

- `2004.02942v1` - [abs](http://arxiv.org/abs/2004.02942v1) - [pdf](http://arxiv.org/pdf/2004.02942v1)

> Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning (ML). However, many standard ML approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable ML, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during the training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared for further ML research on source code.

</details>

<details>

<summary>2020-04-06 20:24:33 - Zero-Shot Learning of Text Adventure Games with Sentence-Level Semantics</summary>

- *Xusen Yin, Jonathan May*

- `2004.02986v1` - [abs](http://arxiv.org/abs/2004.02986v1) - [pdf](http://arxiv.org/pdf/2004.02986v1)

> Reinforcement learning algorithms such as Q-learning have shown great promise in training models to learn the optimal action to take for a given system state; a goal in applications with an exploratory or adversarial nature such as task-oriented dialogues or games. However, models that do not have direct access to their state are harder to train; when the only state access is via the medium of language, this can be particularly pronounced. We introduce a new model amenable to deep Q-learning that incorporates a Siamese neural network architecture and a novel refactoring of the Q-value function in order to better represent system state given its approximation over a language channel. We evaluate the model in the context of zero-shot text-based adventure game learning. Extrinsically, our model reaches the baseline's convergence performance point needing only 15% of its iterations, reaches a convergence performance point 15% higher than the baseline's, and is able to play unseen, unrelated games with no fine-tuning. We probe our new model's representation space to determine that intrinsically, this is due to the appropriate clustering of different linguistic mediation into the same state.

</details>

<details>

<summary>2020-04-06 22:35:55 - Learning Generative Models of Shape Handles</summary>

- *Matheus Gadelha, Giorgio Gori, Duygu Ceylan, Radomir Mech, Nathan Carr, Tamy Boubekeur, Rui Wang, Subhransu Maji*

- `2004.03028v1` - [abs](http://arxiv.org/abs/2004.03028v1) - [pdf](http://arxiv.org/pdf/2004.03028v1)

> We present a generative model to synthesize 3D shapes as sets of handles -- lightweight proxies that approximate the original 3D shape -- for applications in interactive editing, shape parsing, and building compact 3D representations. Our model can generate handle sets with varying cardinality and different types of handles (Figure 1). Key to our approach is a deep architecture that predicts both the parameters and existence of shape handles, and a novel similarity measure that can easily accommodate different types of handles, such as cuboids or sphere-meshes. We leverage the recent advances in semantic 3D annotation as well as automatic shape summarizing techniques to supervise our approach. We show that the resulting shape representations are intuitive and achieve superior quality than previous state-of-the-art. Finally, we demonstrate how our method can be used in applications such as interactive shape editing, completion, and interpolation, leveraging the latent space learned by our model to guide these tasks. Project page: http://mgadelha.me/shapehandles.

</details>

<details>

<summary>2020-04-07 00:44:51 - Temporally Distributed Networks for Fast Video Semantic Segmentation</summary>

- *Ping Hu, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Stan Sclaroff, Federico Perazzi*

- `2004.01800v2` - [abs](http://arxiv.org/abs/2004.01800v2) - [pdf](http://arxiv.org/pdf/2004.01800v2)

> We present TDNet, a temporally distributed network designed for fast and accurate video semantic segmentation. We observe that features extracted from a certain high-level layer of a deep CNN can be approximated by composing features extracted from several shallower sub-networks. Leveraging the inherent temporal continuity in videos, we distribute these sub-networks over sequential frames. Therefore, at each time step, we only need to perform a lightweight computation to extract a sub-features group from a single sub-network. The full features used for segmentation are then recomposed by application of a novel attention propagation module that compensates for geometry deformation between frames. A grouped knowledge distillation loss is also introduced to further improve the representation power at both full and sub-feature levels. Experiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method achieves state-of-the-art accuracy with significantly faster speed and lower latency.

</details>

<details>

<summary>2020-04-07 02:38:56 - Salience Estimation with Multi-Attention Learning for Abstractive Text Summarization</summary>

- *Piji Li, Lidong Bing, Zhongyu Wei, Wai Lam*

- `2004.03589v1` - [abs](http://arxiv.org/abs/2004.03589v1) - [pdf](http://arxiv.org/pdf/2004.03589v1)

> Attention mechanism plays a dominant role in the sequence generation models and has been used to improve the performance of machine translation and abstractive text summarization. Different from neural machine translation, in the task of text summarization, salience estimation for words, phrases or sentences is a critical component, since the output summary is a distillation of the input text. Although the typical attention mechanism can conduct text fragment selection from the input text conditioned on the decoder states, there is still a gap to conduct direct and effective salience detection. To bring back direct salience estimation for summarization with neural networks, we propose a Multi-Attention Learning framework which contains two new attention learning components for salience estimation: supervised attention learning and unsupervised attention learning. We regard the attention weights as the salience information, which means that the semantic units with large attention value will be more important. The context information obtained based on the estimated salience is incorporated with the typical attention mechanism in the decoder to conduct summary generation. Extensive experiments on some benchmark datasets in different languages demonstrate the effectiveness of the proposed framework for the task of abstractive summarization.

</details>

<details>

<summary>2020-04-07 02:57:35 - Automatically Assessing Quality of Online Health Articles</summary>

- *Fariha Afsana, Muhammad Ashad Kabir, Naeemul Hassan, Manoranjan Paul*

- `2004.05113v1` - [abs](http://arxiv.org/abs/2004.05113v1) - [pdf](http://arxiv.org/pdf/2004.05113v1)

> The information ecosystem today is overwhelmed by an unprecedented quantity of data on versatile topics are with varied quality. However, the quality of information disseminated in the field of medicine has been questioned as the negative health consequences of health misinformation can be life-threatening. There is currently no generic automated tool for evaluating the quality of online health information spanned over a broad range. To address this gap, in this paper, we applied a data mining approach to automatically assess the quality of online health articles based on 10 quality criteria. We have prepared a labeled dataset with 53012 features and applied different feature selection methods to identify the best feature subset with which our trained classifier achieved an accuracy of 84%-90% varied over 10 criteria. Our semantic analysis of features shows the underpinning associations between the selected features & assessment criteria and further rationalize our assessment approach. Our findings will help in identifying high-quality health articles and thus aiding users in shaping their opinion to make the right choice while picking health-related help from online.

</details>

<details>

<summary>2020-04-07 03:03:00 - Towards Non-task-specific Distillation of BERT via Sentence Representation Approximation</summary>

- *Bowen Wu, Huan Zhang, Mengyuan Li, Zongsheng Wang, Qihang Feng, Junhong Huang, Baoxun Wang*

- `2004.03097v1` - [abs](http://arxiv.org/abs/2004.03097v1) - [pdf](http://arxiv.org/pdf/2004.03097v1)

> Recently, BERT has become an essential ingredient of various NLP deep models due to its effectiveness and universal-usability. However, the online deployment of BERT is often blocked by its large-scale parameters and high computational cost. There are plenty of studies showing that the knowledge distillation is efficient in transferring the knowledge from BERT into the model with a smaller size of parameters. Nevertheless, current BERT distillation approaches mainly focus on task-specified distillation, such methodologies lead to the loss of the general semantic knowledge of BERT for universal-usability. In this paper, we propose a sentence representation approximating oriented distillation framework that can distill the pre-trained BERT into a simple LSTM based model without specifying tasks. Consistent with BERT, our distilled model is able to perform transfer learning via fine-tuning to adapt to any sentence-level downstream task. Besides, our model can further cooperate with task-specific distillation procedures. The experimental results on multiple NLP tasks from the GLUE benchmark show that our approach outperforms other task-specific distillation methods or even much larger models, i.e., ELMO, with efficiency well-improved.

</details>

<details>

<summary>2020-04-07 11:45:22 - A Corpus Study and Annotation Schema for Named Entity Recognition and Relation Extraction of Business Products</summary>

- *Saskia Schön, Veselina Mironova, Aleksandra Gabryszak, Leonhard Hennig*

- `2004.03287v1` - [abs](http://arxiv.org/abs/2004.03287v1) - [pdf](http://arxiv.org/pdf/2004.03287v1)

> Recognizing non-standard entity types and relations, such as B2B products, product classes and their producers, in news and forum texts is important in application areas such as supply chain monitoring and market research. However, there is a decided lack of annotated corpora and annotation guidelines in this domain. In this work, we present a corpus study, an annotation schema and associated guidelines, for the annotation of product entity and company-product relation mentions. We find that although product mentions are often realized as noun phrases, defining their exact extent is difficult due to high boundary ambiguity and the broad syntactic and semantic variety of their surface realizations. We also describe our ongoing annotation effort, and present a preliminary corpus of English web and social media documents annotated according to the proposed guidelines.

</details>

<details>

<summary>2020-04-07 17:58:49 - Morpheus: A Deep Learning Framework For Pixel-Level Analysis of Astronomical Image Data</summary>

- *Ryan Hausen, Brant Robertson*

- `1906.11248v2` - [abs](http://arxiv.org/abs/1906.11248v2) - [pdf](http://arxiv.org/pdf/1906.11248v2)

> We present Morpheus, a new model for generating pixel-level morphological classifications of astronomical sources. Morpheus leverages advances in deep learning to perform source detection, source segmentation, and morphological classification pixel-by-pixel via a semantic segmentation algorithm adopted from the field of computer vision. By utilizing morphological information about the flux of real astronomical sources during object detection, Morpheus shows resiliency to false-positive identifications of sources. We evaluate Morpheus by performing source detection, source segmentation, morphological classification on the Hubble Space Telescope data in the five CANDELS fields with a focus on the GOODS South field, and demonstrate a high completeness in recovering known GOODS South 3D-HST sources with H < 26 AB. We release the code publicly, provide online demonstrations, and present an interactive visualization of the Morpheus results in GOODS South.

</details>

<details>

<summary>2020-04-07 18:01:26 - PatchVAE: Learning Local Latent Codes for Recognition</summary>

- *Kamal Gupta, Saurabh Singh, Abhinav Shrivastava*

- `2004.03623v1` - [abs](http://arxiv.org/abs/2004.03623v1) - [pdf](http://arxiv.org/pdf/2004.03623v1)

> Unsupervised representation learning holds the promise of exploiting large amounts of unlabeled data to learn general representations. A promising technique for unsupervised learning is the framework of Variational Auto-encoders (VAEs). However, unsupervised representations learned by VAEs are significantly outperformed by those learned by supervised learning for recognition. Our hypothesis is that to learn useful representations for recognition the model needs to be encouraged to learn about repeating and consistent patterns in data. Drawing inspiration from the mid-level representation discovery work, we propose PatchVAE, that reasons about images at patch level. Our key contribution is a bottleneck formulation that encourages mid-level style representations in the VAE framework. Our experiments demonstrate that representations learned by our method perform much better on the recognition tasks compared to those learned by vanilla VAEs.

</details>

<details>

<summary>2020-04-07 18:21:47 - Efficient long-distance relation extraction with DG-SpanBERT</summary>

- *Jun Chen, Robert Hoehndorf, Mohamed Elhoseiny, Xiangliang Zhang*

- `2004.03636v1` - [abs](http://arxiv.org/abs/2004.03636v1) - [pdf](http://arxiv.org/pdf/2004.03636v1)

> In natural language processing, relation extraction seeks to rationally understand unstructured text. Here, we propose a novel SpanBERT-based graph convolutional network (DG-SpanBERT) that extracts semantic features from a raw sentence using the pre-trained language model SpanBERT and a graph convolutional network to pool latent features. Our DG-SpanBERT model inherits the advantage of SpanBERT on learning rich lexical features from large-scale corpus. It also has the ability to capture long-range relations between entities due to the usage of GCN on dependency tree. The experimental results show that our model outperforms other existing dependency-based and sequence-based models and achieves a state-of-the-art performance on the TACRED dataset.

</details>

<details>

<summary>2020-04-07 19:36:40 - Neural Graph Embedding Methods for Natural Language Processing</summary>

- *Shikhar Vashishth*

- `1911.03042v3` - [abs](http://arxiv.org/abs/1911.03042v3) - [pdf](http://arxiv.org/pdf/1911.03042v3)

> Knowledge graphs are structured representations of facts in a graph, where nodes represent entities and edges represent relationships between them. Recent research has resulted in the development of several large KGs. However, all of them tend to be sparse with very few facts per entity. In the first part of the thesis, we propose two solutions to alleviate this problem: (1) KG Canonicalization, i.e., identifying and merging duplicate entities in a KG, (2) Relation Extraction which involves automating the process of extracting semantic relationships between entities from unstructured text. Traditional Neural Networks like CNNs and RNNs are constrained to handle Euclidean data. However, graphs in Natural Language Processing (NLP) are prominent. Recently, Graph Convolutional Networks (GCNs) have been proposed to address this shortcoming and have been successfully applied for several problems. In the second part of the thesis, we utilize GCNs for Document Timestamping problem and for learning word embeddings using dependency context of a word instead of sequential context. In this third part of the thesis, we address two limitations of existing GCN models, i.e., (1) The standard neighborhood aggregation scheme puts no constraints on the number of nodes that can influence the representation of a target node. This leads to a noisy representation of hub-nodes which coves almost the entire graph in a few hops. (2) Most of the existing GCN models are limited to handle undirected graphs. However, a more general and pervasive class of graphs are relational graphs where each edge has a label and direction associated with it. Existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representation of nodes only.

</details>

<details>

<summary>2020-04-07 20:43:44 - Coronavirus (COVID-19) Classification using Deep Features Fusion and Ranking Technique</summary>

- *Umut Ozkaya, Saban Ozturk, Mucahid Barstugan*

- `2004.03698v1` - [abs](http://arxiv.org/abs/2004.03698v1) - [pdf](http://arxiv.org/pdf/2004.03698v1)

> Coronavirus (COVID-19) emerged towards the end of 2019. World Health Organization (WHO) was identified it as a global epidemic. Consensus occurred in the opinion that using Computerized Tomography (CT) techniques for early diagnosis of pandemic disease gives both fast and accurate results. It was stated by expert radiologists that COVID-19 displays different behaviours in CT images. In this study, a novel method was proposed as fusing and ranking deep features to detect COVID-19 in early phase. 16x16 (Subset-1) and 32x32 (Subset-2) patches were obtained from 150 CT images to generate sub-datasets. Within the scope of the proposed method, 3000 patch images have been labelled as CoVID-19 and No finding for using in training and testing phase. Feature fusion and ranking method have been applied in order to increase the performance of the proposed method. Then, the processed data was classified with a Support Vector Machine (SVM). According to other pre-trained Convolutional Neural Network (CNN) models used in transfer learning, the proposed method shows high performance on Subset-2 with 98.27% accuracy, 98.93% sensitivity, 97.60% specificity, 97.63% precision, 98.28% F1-score and 96.54% Matthews Correlation Coefficient (MCC) metrics.

</details>

<details>

<summary>2020-04-08 03:22:21 - Satirical News Detection with Semantic Feature Extraction and Game-theoretic Rough Sets</summary>

- *Yue Zhou, Yan Zhang, JingTao Yao*

- `2004.03788v1` - [abs](http://arxiv.org/abs/2004.03788v1) - [pdf](http://arxiv.org/pdf/2004.03788v1)

> Satirical news detection is an important yet challenging task to prevent spread of misinformation. Many feature based and end-to-end neural nets based satirical news detection systems have been proposed and delivered promising results. Existing approaches explore comprehensive word features from satirical news articles, but lack semantic metrics using word vectors for tweet form satirical news. Moreover, the vagueness of satire and news parody determines that a news tweet can hardly be classified with a binary decision, that is, satirical or legitimate. To address these issues, we collect satirical and legitimate news tweets, and propose a semantic feature based approach. Features are extracted by exploring inconsistencies in phrases, entities, and between main and relative clauses. We apply game-theoretic rough set model to detect satirical news, in which probabilistic thresholds are derived by game equilibrium and repetition learning mechanism. Experimental results on the collected dataset show the robustness and improvement of the proposed approach compared with Pawlak rough set model and SVM.

</details>

<details>

<summary>2020-04-08 08:44:48 - Deep learning approaches in food recognition</summary>

- *Chairi Kiourt, George Pavlidis, Stella Markantonatou*

- `2004.03357v2` - [abs](http://arxiv.org/abs/2004.03357v2) - [pdf](http://arxiv.org/pdf/2004.03357v2)

> Automatic image-based food recognition is a particularly challenging task. Traditional image analysis approaches have achieved low classification accuracy in the past, whereas deep learning approaches enabled the identification of food types and their ingredients. The contents of food dishes are typically deformable objects, usually including complex semantics, which makes the task of defining their structure very difficult. Deep learning methods have already shown very promising results in such challenges, so this chapter focuses on the presentation of some popular approaches and techniques applied in image-based food recognition. The three main lines of solutions, namely the design from scratch, the transfer learning and the platform-based approaches, are outlined, particularly for the task at hand, and are tested and compared to reveal the inherent strengths and weaknesses. The chapter is complemented with basic background material, a section devoted to the relevant datasets that are crucial in light of the empirical approaches adopted, and some concluding remarks that underline the future directions.

</details>

<details>

<summary>2020-04-08 13:18:05 - GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph Generation</summary>

- *Nikhil Goyal, Harsh Vardhan Jain, Sayan Ranu*

- `2001.08184v2` - [abs](http://arxiv.org/abs/2001.08184v2) - [pdf](http://arxiv.org/pdf/2001.08184v2)

> Graph generative models have been extensively studied in the data mining literature. While traditional techniques are based on generating structures that adhere to a pre-decided distribution, recent techniques have shifted towards learning this distribution directly from the data. While learning-based approaches have imparted significant improvement in quality, some limitations remain to be addressed. First, learning graph distributions introduces additional computational overhead, which limits their scalability to large graph databases. Second, many techniques only learn the structure and do not address the need to also learn node and edge labels, which encode important semantic information and influence the structure itself. Third, existing techniques often incorporate domain-specific rules and lack generalizability. Fourth, the experimentation of existing techniques is not comprehensive enough due to either using weak evaluation metrics or focusing primarily on synthetic or small datasets. In this work, we develop a domain-agnostic technique called GraphGen to overcome all of these limitations. GraphGen converts graphs to sequences using minimum DFS codes. Minimum DFS codes are canonical labels and capture the graph structure precisely along with the label information. The complex joint distributions between structure and semantic labels are learned through a novel LSTM architecture. Extensive experiments on million-sized, real graph datasets show GraphGen to be 4 times faster on average than state-of-the-art techniques while being significantly better in quality across a comprehensive set of 11 different metrics. Our code is released at https://github.com/idea-iitd/graphgen.

</details>

<details>

<summary>2020-04-08 14:34:18 - Shoestring: Graph-Based Semi-Supervised Learning with Severely Limited Labeled Data</summary>

- *Wanyu Lin, Zhaolin Gao, Baochun Li*

- `1910.12976v2` - [abs](http://arxiv.org/abs/1910.12976v2) - [pdf](http://arxiv.org/pdf/1910.12976v2)

> Graph-based semi-supervised learning has been shown to be one of the most effective approaches for classification tasks from a wide range of domains, such as image classification and text classification, as they can exploit the connectivity patterns between labeled and unlabeled samples to improve learning performance. In this work, we advance this effective learning paradigm towards a scenario where labeled data are severely limited. More specifically, we address the problem of graph-based semi-supervised learning in the presence of severely limited labeled samples, and propose a new framework, called {\em Shoestring}, that improves the learning performance through semantic transfer from these very few labeled samples to large numbers of unlabeled samples.   In particular, our framework learns a metric space in which classification can be performed by computing the similarity to centroid embedding of each class. {\em Shoestring} is trained in an end-to-end fashion to learn to leverage the semantic knowledge of limited labeled samples as well as their connectivity patterns with large numbers of unlabeled samples simultaneously. By combining {\em Shoestring} with graph convolutional networks, label propagation and their recent label-efficient variations (IGCN and GLP), we are able to achieve state-of-the-art node classification performance in the presence of very few labeled samples. In addition, we demonstrate the effectiveness of our framework on image classification tasks in the few-shot learning regime, with significant gains on miniImageNet ($2.57\%\sim3.59\%$) and tieredImageNet ($1.05\%\sim2.70\%$).

</details>

<details>

<summary>2020-04-08 23:10:10 - Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</summary>

- *Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits*

- `1907.11932v6` - [abs](http://arxiv.org/abs/1907.11932v6) - [pdf](http://arxiv.org/pdf/1907.11932v6)

> Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.

</details>

<details>

<summary>2020-04-08 23:46:24 - Automated Content Grading Using Machine Learning</summary>

- *Rahul Kr Chauhan, Ravinder Saharan, Siddhartha Singh, Priti Sharma*

- `2004.04300v1` - [abs](http://arxiv.org/abs/2004.04300v1) - [pdf](http://arxiv.org/pdf/2004.04300v1)

> Grading of examination papers is a hectic, time-labor intensive task and is often subjected to inefficiency and bias in checking. This research project is a primitive experiment in the automation of grading of theoretical answers written in exams by students in technical courses which yet had continued to be human graded. In this paper, we show how the algorithmic approach in machine learning can be used to automatically examine and grade theoretical content in exam answer papers. Bag of words, their vectors & centroids, and a few semantic and lexical text features have been used overall. Machine learning models have been implemented on datasets manually built from exams given by graduating students enrolled in technical courses. These models have been compared to show the effectiveness of each model.

</details>

<details>

<summary>2020-04-09 04:14:14 - Networked Multi-Agent Reinforcement Learning with Emergent Communication</summary>

- *Shubham Gupta, Rishi Hazra, Ambedkar Dukkipati*

- `2004.02780v2` - [abs](http://arxiv.org/abs/2004.02780v2) - [pdf](http://arxiv.org/pdf/2004.02780v2)

> Multi-Agent Reinforcement Learning (MARL) methods find optimal policies for agents that operate in the presence of other learning agents. Central to achieving this is how the agents coordinate. One way to coordinate is by learning to communicate with each other. Can the agents develop a language while learning to perform a common task? In this paper, we formulate and study a MARL problem where cooperative agents are connected to each other via a fixed underlying network. These agents can communicate along the edges of this network by exchanging discrete symbols. However, the semantics of these symbols are not predefined and, during training, the agents are required to develop a language that helps them in accomplishing their goals. We propose a method for training these agents using emergent communication. We demonstrate the applicability of the proposed framework by applying it to the problem of managing traffic controllers, where we achieve state-of-the-art performance as compared to a number of strong baselines. More importantly, we perform a detailed analysis of the emergent communication to show, for instance, that the developed language is grounded and demonstrate its relationship with the underlying network topology. To the best of our knowledge, this is the only work that performs an in depth analysis of emergent communication in a networked MARL setting while being applicable to a broad class of problems.

</details>

<details>

<summary>2020-04-09 09:26:42 - Improving Readability for Automatic Speech Recognition Transcription</summary>

- *Junwei Liao, Sefik Emre Eskimez, Liyang Lu, Yu Shi, Ming Gong, Linjun Shou, Hong Qu, Michael Zeng*

- `2004.04438v1` - [abs](http://arxiv.org/abs/2004.04438v1) - [pdf](http://arxiv.org/pdf/2004.04438v1)

> Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other errata common in spoken communication. Many downstream tasks and human readers rely on the output of the ASR system; therefore, errors introduced by the speaker and ASR system alike will be propagated to the next task in the pipeline. In this work, we propose a novel NLP task called ASR post-processing for readability (APR) that aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of the speaker. In addition, we describe a method to address the lack of task-specific data by synthesizing examples for the APR task using the datasets collected for Grammatical Error Correction (GEC) followed by text-to-speech (TTS) and ASR. Furthermore, we propose metrics borrowed from similar tasks to evaluate performance on the APR task. We compare fine-tuned models based on several open-sourced and adapted pre-trained models with the traditional pipeline method. Our results suggest that finetuned models improve the performance on the APR task significantly, hinting at the potential benefits of using APR systems. We hope that the read, understand, and rewrite approach of our work can serve as a basis that many NLP tasks and human readers can benefit from.

</details>

<details>

<summary>2020-04-09 11:24:00 - Multi-Granularity Canonical Appearance Pooling for Remote Sensing Scene Classification</summary>

- *S. Wang, Y. Guan, L. Shao*

- `2004.04491v1` - [abs](http://arxiv.org/abs/2004.04491v1) - [pdf](http://arxiv.org/pdf/2004.04491v1)

> Recognising remote sensing scene images remains challenging due to large visual-semantic discrepancies. These mainly arise due to the lack of detailed annotations that can be employed to align pixel-level representations with high-level semantic labels. As the tagging process is labour-intensive and subjective, we hereby propose a novel Multi-Granularity Canonical Appearance Pooling (MG-CAP) to automatically capture the latent ontological structure of remote sensing datasets. We design a granular framework that allows progressively cropping the input image to learn multi-grained features. For each specific granularity, we discover the canonical appearance from a set of pre-defined transformations and learn the corresponding CNN features through a maxout-based Siamese style architecture. Then, we replace the standard CNN features with Gaussian covariance matrices and adopt the proper matrix normalisations for improving the discriminative power of features. Besides, we provide a stable solution for training the eigenvalue-decomposition function (EIG) in a GPU and demonstrate the corresponding back-propagation using matrix calculus. Extensive experiments have shown that our framework can achieve promising results in public remote sensing scene datasets.

</details>

<details>

<summary>2020-04-09 20:00:12 - Two halves of a meaningful text are statistically different</summary>

- *Weibing Deng, R. Xie, S. Deng, Armen E. Allahverdyan*

- `2004.06474v1` - [abs](http://arxiv.org/abs/2004.06474v1) - [pdf](http://arxiv.org/pdf/2004.06474v1)

> Which statistical features distinguish a meaningful text (possibly written in an unknown system) from a meaningless set of symbols? Here we answer this question by comparing features of the first half of a text to its second half. This comparison can uncover hidden effects, because the halves have the same values of many parameters (style, genre {\it etc}). We found that the first half has more different words and more rare words than the second half. Also, words in the first half are distributed less homogeneously over the text in the sense of of the difference between the frequency and the inverse spatial period. These differences hold for the significant majority of several hundred relatively short texts we studied. The statistical significance is confirmed via the Wilcoxon test. Differences disappear after random permutation of words that destroys the linear structure of the text. The differences reveal a temporal asymmetry in meaningful texts, which is confirmed by showing that texts are much better compressible in their natural way (i.e. along the narrative) than in the word-inverted form. We conjecture that these results connect the semantic organization of a text (defined by the flow of its narrative) to its statistical features.

</details>

<details>

<summary>2020-04-09 21:13:23 - Very high resolution Airborne PolSAR Image Classification using Convolutional Neural Networks</summary>

- *Minh-Tan Pham, Sébastien Lefèvre*

- `1910.14578v2` - [abs](http://arxiv.org/abs/1910.14578v2) - [pdf](http://arxiv.org/pdf/1910.14578v2)

> In this work, we exploit convolutional neural networks (CNNs) for the classification of very high resolution (VHR) polarimetric SAR (PolSAR) data. Due to the significant appearance of heterogeneous textures within these data, not only polarimetric features but also structural tensors are exploited to feed CNN models. For deep networks, we use the SegNet model for semantic segmentation, which corresponds to pixelwise classification in remote sensing. Our experiments on the airborne F-SAR data show that for VHR PolSAR images, SegNet could provide high accuracy for the classification task; and introducing structural tensors together with polarimetric features as inputs could help the network to focus more on geometrical information to significantly improve the classification performance.

</details>

<details>

<summary>2020-04-09 23:20:30 - Spatial Priming for Detecting Human-Object Interactions</summary>

- *Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama Chellappa*

- `2004.04851v1` - [abs](http://arxiv.org/abs/2004.04851v1) - [pdf](http://arxiv.org/pdf/2004.04851v1)

> The relative spatial layout of a human and an object is an important cue for determining how they interact. However, until now, spatial layout has been used just as side-information for detecting human-object interactions (HOIs). In this paper, we present a method for exploiting this spatial layout information for detecting HOIs in images. The proposed method consists of a layout module which primes a visual module to predict the type of interaction between a human and an object. The visual and layout modules share information through lateral connections at several stages. The model uses predictions from the layout module as a prior to the visual module and the prediction from the visual module is given as the final output. It also incorporates semantic information about the object using word2vec vectors. The proposed model reaches an mAP of 24.79% for HICO-Det dataset which is about 2.8% absolute points higher than the current state-of-the-art.

</details>

<details>

<summary>2020-04-10 00:05:48 - Decentralized Differentially Private Segmentation with PATE</summary>

- *Dominik Fay, Jens Sjölund, Tobias J. Oechtering*

- `2004.06567v1` - [abs](http://arxiv.org/abs/2004.06567v1) - [pdf](http://arxiv.org/pdf/2004.06567v1)

> When it comes to preserving privacy in medical machine learning, two important considerations are (1) keeping data local to the institution and (2) avoiding inference of sensitive information from the trained model. These are often addressed using federated learning and differential privacy, respectively. However, the commonly used Federated Averaging algorithm requires a high degree of synchronization between participating institutions. For this reason, we turn our attention to Private Aggregation of Teacher Ensembles (PATE), where all local models can be trained independently without inter-institutional communication. The purpose of this paper is thus to explore how PATE -- originally designed for classification -- can best be adapted for semantic segmentation. To this end, we build low-dimensional representations of segmentation masks which the student can obtain through low-sensitivity queries to the private aggregator. On the Brain Tumor Segmentation (BraTS 2019) dataset, an Autoencoder-based PATE variant achieves a higher Dice coefficient for the same privacy guarantee than prior work based on noisy Federated Averaging.

</details>

<details>

<summary>2020-04-10 08:23:59 - State-Relabeling Adversarial Active Learning</summary>

- *Beichen Zhang, Liang Li, Shijie Yang, Shuhui Wang, Zheng-Jun Zha, Qingming Huang*

- `2004.04943v1` - [abs](http://arxiv.org/abs/2004.04943v1) - [pdf](http://arxiv.org/pdf/2004.04943v1)

> Active learning is to design label-efficient algorithms by sampling the most representative samples to be labeled by an oracle. In this paper, we propose a state relabeling adversarial active learning model (SRAAL), that leverages both the annotation and the labeled/unlabeled state information for deriving the most informative unlabeled samples. The SRAAL consists of a representation generator and a state discriminator. The generator uses the complementary annotation information with traditional reconstruction information to generate the unified representation of samples, which embeds the semantic into the whole data representation. Then, we design an online uncertainty indicator in the discriminator, which endues unlabeled samples with different importance. As a result, we can select the most informative samples based on the discriminator's predicted state. We also design an algorithm to initialize the labeled pool, which makes subsequent sampling more efficient. The experiments conducted on various datasets show that our model outperforms the previous state-of-art active learning methods and our initially sampling algorithm achieves better performance.

</details>

<details>

<summary>2020-04-10 09:11:02 - Boosting Semantic Human Matting with Coarse Annotations</summary>

- *Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, Xian-sheng Hua*

- `2004.04955v1` - [abs](http://arxiv.org/abs/2004.04955v1) - [pdf](http://arxiv.org/pdf/2004.04955v1)

> Semantic human matting aims to estimate the per-pixel opacity of the foreground human regions. It is quite challenging and usually requires user interactive trimaps and plenty of high quality annotated data. Annotating such kind of data is labor intensive and requires great skills beyond normal users, especially considering the very detailed hair part of humans. In contrast, coarse annotated human dataset is much easier to acquire and collect from the public dataset. In this paper, we propose to use coarse annotated data coupled with fine annotated data to boost end-to-end semantic human matting without trimaps as extra input. Specifically, we train a mask prediction network to estimate the coarse semantic mask using the hybrid data, and then propose a quality unification network to unify the quality of the previous coarse mask outputs. A matting refinement network takes in the unified mask and the input image to predict the final alpha matte. The collected coarse annotated dataset enriches our dataset significantly, allows generating high quality alpha matte for real images. Experimental results show that the proposed method performs comparably against state-of-the-art methods. Moreover, the proposed method can be used for refining coarse annotated public dataset, as well as semantic segmentation methods, which reduces the cost of annotating high quality human data to a great extent.

</details>

<details>

<summary>2020-04-10 19:14:39 - ARCH: Animatable Reconstruction of Clothed Humans</summary>

- *Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung*

- `2004.04572v2` - [abs](http://arxiv.org/abs/2004.04572v2) - [pdf](http://arxiv.org/pdf/2004.04572v2)

> In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.

</details>

<details>

<summary>2020-04-11 08:59:27 - Early Forecasting of Text Classification Accuracy and F-Measure with Active Learning</summary>

- *Thomas Orth, Michael Bloodgood*

- `2001.10337v2` - [abs](http://arxiv.org/abs/2001.10337v2) - [pdf](http://arxiv.org/pdf/2001.10337v2)

> When creating text classification systems, one of the major bottlenecks is the annotation of training data. Active learning has been proposed to address this bottleneck using stopping methods to minimize the cost of data annotation. An important capability for improving the utility of stopping methods is to effectively forecast the performance of the text classification models. Forecasting can be done through the use of logarithmic models regressed on some portion of the data as learning is progressing. A critical unexplored question is what portion of the data is needed for accurate forecasting. There is a tension, where it is desirable to use less data so that the forecast can be made earlier, which is more useful, versus it being desirable to use more data, so that the forecast can be more accurate. We find that when using active learning it is even more important to generate forecasts earlier so as to make them more useful and not waste annotation effort. We investigate the difference in forecasting difficulty when using accuracy and F-measure as the text classification system performance metrics and we find that F-measure is more difficult to forecast. We conduct experiments on seven text classification datasets in different semantic domains with different characteristics and with three different base machine learning algorithms. We find that forecasting is easiest for decision tree learning, moderate for Support Vector Machines, and most difficult for neural networks.

</details>

<details>

<summary>2020-04-11 12:22:27 - Trajectory annotation using sequences of spatial perception</summary>

- *Sebastian Feld, Steffen Illium, Andreas Sedlmeier, Lenz Belzner*

- `2004.05383v1` - [abs](http://arxiv.org/abs/2004.05383v1) - [pdf](http://arxiv.org/pdf/2004.05383v1)

> In the near future, more and more machines will perform tasks in the vicinity of human spaces or support them directly in their spatially bound activities. In order to simplify the verbal communication and the interaction between robotic units and/or humans, reliable and robust systems w.r.t. noise and processing results are needed. This work builds a foundation to address this task. By using a continuous representation of spatial perception in interiors learned from trajectory data, our approach clusters movement in dependency to its spatial context. We propose an unsupervised learning approach based on a neural autoencoding that learns semantically meaningful continuous encodings of spatio-temporal trajectory data. This learned encoding can be used to form prototypical representations. We present promising results that clear the path for future applications.

</details>

<details>

<summary>2020-04-11 17:51:39 - Learning Representations For Images With Hierarchical Labels</summary>

- *Ankit Dhall*

- `2004.00909v2` - [abs](http://arxiv.org/abs/2004.00909v2) - [pdf](http://arxiv.org/pdf/2004.00909v2)

> Image classification has been studied extensively but there has been limited work in the direction of using non-conventional, external guidance other than traditional image-label pairs to train such models. In this thesis we present a set of methods to leverage information about the semantic hierarchy induced by class labels. In the first part of the thesis, we inject label-hierarchy knowledge to an arbitrary classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions by using order-preserving embedding-based models, prevalent in natural language, and tailor them to the domain of computer vision to perform image classification. Although, contrasting in nature, both the CNN-classifiers injected with hierarchical information, and the embedding-based models outperform a hierarchy-agnostic model on the newly presented, real-world ETH Entomological Collection image dataset https://www.research-collection.ethz.ch/handle/20.500.11850/365379.

</details>

<details>

<summary>2020-04-11 20:51:11 - LAReQA: Language-agnostic answer retrieval from a multilingual pool</summary>

- *Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips, Yinfei Yang*

- `2004.05484v1` - [abs](http://arxiv.org/abs/2004.05484v1) - [pdf](http://arxiv.org/pdf/2004.05484v1)

> We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for "strong" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, the embedding baseline that performs the best on LAReQA falls short of competing baselines on zero-shot variants of our task that only target "weak" alignment. This finding underscores our claim that languageagnostic retrieval is a substantively new kind of cross-lingual evaluation.

</details>

<details>

<summary>2020-04-12 09:25:36 - YouMakeup VQA Challenge: Towards Fine-grained Action Understanding in Domain-Specific Videos</summary>

- *Shizhe Chen, Weiying Wang, Ludan Ruan, Linli Yao, Qin Jin*

- `2004.05573v1` - [abs](http://arxiv.org/abs/2004.05573v1) - [pdf](http://arxiv.org/pdf/2004.05573v1)

> The goal of the YouMakeup VQA Challenge 2020 is to provide a common benchmark for fine-grained action understanding in domain-specific videos e.g. makeup instructional videos. We propose two novel question-answering tasks to evaluate models' fine-grained action understanding abilities. The first task is \textbf{Facial Image Ordering}, which aims to understand visual effects of different actions expressed in natural language to the facial object. The second task is \textbf{Step Ordering}, which aims to measure cross-modal semantic alignments between untrimmed videos and multi-sentence texts. In this paper, we present the challenge guidelines, the dataset used, and performances of baseline models on the two proposed tasks. The baseline codes and models are released at \url{https://github.com/AIM3-RUC/YouMakeup_Baseline}.

</details>

<details>

<summary>2020-04-12 13:39:52 - Bayesian Hierarchical Words Representation Learning</summary>

- *Oren Barkan, Idan Rejwan, Avi Caciularu, Noam Koenigstein*

- `2004.07126v1` - [abs](http://arxiv.org/abs/2004.07126v1) - [pdf](http://arxiv.org/pdf/2004.07126v1)

> This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words.

</details>

<details>

<summary>2020-04-12 20:48:50 - Measuring spatial uniformity with the hypersphere chord length distribution</summary>

- *Panagiotis Sidiropoulos*

- `2004.05692v1` - [abs](http://arxiv.org/abs/2004.05692v1) - [pdf](http://arxiv.org/pdf/2004.05692v1)

> Data uniformity is a concept associated with several semantic data characteristics such as lack of features, correlation and sample bias. This article introduces a novel measure to assess data uniformity and detect uniform pointsets on high-dimensional Euclidean spaces. Spatial uniformity measure builds upon the isomorphism between hyperspherical chords and L2-normalised data Euclidean distances, which is implied by the fact that, in Euclidean spaces, L2-normalised data can be geometrically defined as points on a hypersphere. The imposed connection between the distance distribution of uniformly selected points and the hyperspherical chord length distribution is employed to quantify uniformity. More specifically,, the closed-form expression of hypersphere chord length distribution is revisited extended, before examining a few qualitative and quantitative characteristics of this distribution that can be rather straightforwardly linked to data uniformity. The experimental section includes validation in four distinct setups, thus substantiating the potential of the new uniformity measure on practical data-science applications.

</details>

<details>

<summary>2020-04-13 08:55:36 - Generator evaluator-selector net for panoptic image segmentation and splitting unfamiliar objects into parts</summary>

- *Sagi Eppel, Alan Aspuru-Guzik*

- `1908.09108v4` - [abs](http://arxiv.org/abs/1908.09108v4) - [pdf](http://arxiv.org/pdf/1908.09108v4)

> In machine learning and other fields, suggesting a good solution to a problem is usually a harder task than evaluating the quality of such a solution. This asymmetry is the basis for a large number of selection oriented methods that use a generator system to guess a set of solutions and an evaluator system to rank and select the best solutions. This work examines the use of this approach to the problem of panoptic image segmentation and class agnostic parts segmentation. The generator/evaluator approach for this case consists of two independent convolutional neural nets: a generator net that suggests variety segments corresponding to objects, stuff and parts regions in the image, and an evaluator net that chooses the best segments to be merged into the segmentation map. The result is a trial and error evolutionary approach in which a generator that guesses segments with low average accuracy, but with wide variability, can still produce good results when coupled with an accurate evaluator. The generator consists of a Pointer net that receives an image and a point in the image, and predicts the region of the segment containing the point. Generating and evaluating each segment separately is essential in this case since it demands exponentially fewer guesses compared to a system that guesses and evaluates the full segmentation map in each try. The classification of the selected segments is done by an independent region-specific classification net. This allows the segmentation to be class agnostic and hence, capable of segmenting unfamiliar categories that were not part of the training set. The method was examined on the COCO Panoptic segmentation benchmark and gave results comparable to those of the basic semantic segmentation and Mask-RCNN methods. In addition, the system was used for the task of splitting objects of unseen classes (that did not appear in the training set) into parts.

</details>

<details>

<summary>2020-04-13 14:25:43 - Non-clairvoyant Scheduling of Coflows</summary>

- *Akhil Bhimaraju, Debanuj Nayak, Rahul Vaze*

- `2004.05961v1` - [abs](http://arxiv.org/abs/2004.05961v1) - [pdf](http://arxiv.org/pdf/2004.05961v1)

> The coflow scheduling problem is considered: given an input/output switch with each port having a fixed capacity, find a scheduling algorithm that minimizes the weighted sum of the coflow completion times respecting the port capacities, where each flow of a coflow has a demand per input/output port, and coflow completion time is the finishing time of the last flow of the coflow. The objective of this paper is to present theoretical guarantees on approximating the sum of coflow completion time in the non-clairvoyant setting, where on a coflow arrival, only the number of flows, and their input-output port is revealed, while the critical demand volumes for each flow on the respective input-output port is unknown. The main result of this paper is to show that the proposed BlindFlow algorithm is $8p$-approximate, where $p$ is the largest number of input-output port pairs that a coflow uses. This result holds even in the online case, where coflows arrive over time and the scheduler has to use only causal information. Simulations reveal that the experimental performance of BlindFlow is far better than the theoretical guarantee.

</details>

<details>

<summary>2020-04-13 15:46:47 - Compass-aligned Distributional Embeddings for Studying Semantic Differences across Corpora</summary>

- *Federico Bianchi, Valerio Di Carlo, Paolo Nicoli, Matteo Palmonari*

- `2004.06519v1` - [abs](http://arxiv.org/abs/2004.06519v1) - [pdf](http://arxiv.org/pdf/2004.06519v1)

> Word2vec is one of the most used algorithms to generate word embeddings because of a good mix of efficiency, quality of the generated representations and cognitive grounding. However, word meaning is not static and depends on the context in which words are used. Differences in word meaning that depends on time, location, topic, and other factors, can be studied by analyzing embeddings generated from different corpora in collections that are representative of these factors. For example, language evolution can be studied using a collection of news articles published in different time periods. In this paper, we present a general framework to support cross-corpora language studies with word embeddings, where embeddings generated from different corpora can be compared to find correspondences and differences in meaning across the corpora. CADE is the core component of our framework and solves the key problem of aligning the embeddings generated from different corpora. In particular, we focus on providing solid evidence about the effectiveness, generality, and robustness of CADE. To this end, we conduct quantitative and qualitative experiments in different domains, from temporal word embeddings to language localization and topical analysis. The results of our experiments suggest that CADE achieves state-of-the-art or superior performance on tasks where several competing approaches are available, yet providing a general method that can be used in a variety of domains. Finally, our experiments shed light on the conditions under which the alignment is reliable, which substantially depends on the degree of cross-corpora vocabulary overlap.

</details>

<details>

<summary>2020-04-13 22:01:38 - Robustly Pre-trained Neural Model for Direct Temporal Relation Extraction</summary>

- *Hong Guan, Jianfu Li, Hua Xu, Murthy Devarakonda*

- `2004.06216v1` - [abs](http://arxiv.org/abs/2004.06216v1) - [pdf](http://arxiv.org/pdf/2004.06216v1)

> Background: Identifying relationships between clinical events and temporal expressions is a key challenge in meaningfully analyzing clinical text for use in advanced AI applications. While previous studies exist, the state-of-the-art performance has significant room for improvement.   Methods: We studied several variants of BERT (Bidirectional Encoder Representations using Transformers) some involving clinical domain customization and the others involving improved architecture and/or training strategies. We evaluated these methods using a direct temporal relations dataset which is a semantically focused subset of the 2012 i2b2 temporal relations challenge dataset.   Results: Our results show that RoBERTa, which employs better pre-training strategies including using 10x larger corpus, has improved overall F measure by 0.0864 absolute score (on the 1.00 scale) and thus reducing the error rate by 24% relative to the previous state-of-the-art performance achieved with an SVM (support vector machine) model.   Conclusion: Modern contextual language modeling neural networks, pre-trained on a large corpus, achieve impressive performance even on highly-nuanced clinical temporal relation tasks.

</details>

<details>

<summary>2020-04-14 02:32:10 - Bidirectional Graph Reasoning Network for Panoptic Segmentation</summary>

- *Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang, Liang Lin*

- `2004.06272v1` - [abs](http://arxiv.org/abs/2004.06272v1) - [pdf](http://arxiv.org/pdf/2004.06272v1)

> Recent researches on panoptic segmentation resort to a single end-to-end network to combine the tasks of instance segmentation and semantic segmentation. However, prior models only unified the two related tasks at the architectural level via a multi-branch scheme or revealed the underlying correlation between them by unidirectional feature fusion, which disregards the explicit semantic and co-occurrence relations among objects and background. Inspired by the fact that context information is critical to recognize and localize the objects, and inclusive object details are significant to parse the background scene, we thus investigate on explicitly modeling the correlations between object and background to achieve a holistic understanding of an image in the panoptic segmentation task. We introduce a Bidirectional Graph Reasoning Network (BGRNet), which incorporates graph structure into the conventional panoptic segmentation network to mine the intra-modular and intermodular relations within and between foreground things and background stuff classes. In particular, BGRNet first constructs image-specific graphs in both instance and semantic segmentation branches that enable flexible reasoning at the proposal level and class level, respectively. To establish the correlations between separate branches and fully leverage the complementary relations between things and stuff, we propose a Bidirectional Graph Connection Module to diffuse information across branches in a learnable fashion. Experimental results demonstrate the superiority of our BGRNet that achieves the new state-of-the-art performance on challenging COCO and ADE20K panoptic segmentation benchmarks.

</details>

<details>

<summary>2020-04-14 04:53:14 - Robust Modelling of Reflectance Pulse Oximetry for SpO$_2$ Estimation</summary>

- *Sricharan Vijayarangan, Prithvi Suresh, Preejith SP, Jayaraj Joseph, Mohansankar Sivaprakasam*

- `2004.06301v1` - [abs](http://arxiv.org/abs/2004.06301v1) - [pdf](http://arxiv.org/pdf/2004.06301v1)

> Continuous monitoring of blood oxygen saturation levels is vital for patients with pulmonary disorders. Traditionally, SpO$_2$ monitoring has been carried out using transmittance pulse oximeters due to its dependability. However, SpO$_2$ measurement from transmittance pulse oximeters is limited to peripheral regions. This becomes a disadvantage at very low temperatures as blood perfusion to the peripherals decreases. On the other hand, reflectance pulse oximeters can be used at various sites like finger, wrist, chest and forehead. Additionally, reflectance pulse oximeters can be scaled down to affordable patches that do not interfere with the user's diurnal activities. However, accurate SpO$_2$ estimation from reflectance pulse oximeters is challenging due to its patient dependent, subjective nature of measurement. Recently, a Machine Learning (ML) method was used to model reflectance waveforms onto SpO$_2$ obtained from transmittance waveforms. However, the generalizability of the model to new patients was not tested. In light of this, the current work implemented multiple ML based approaches which were subsequently found to be incapable of generalizing to new patients. Furthermore, a minimally calibrated data driven approach was utilized in order to obtain SpO$_2$ from reflectance PPG waveforms. The proposed solution produces an average mean absolute error of 1.81\% on unseen patients which is well within the clinically permissible error of 2\%. Two statistical tests were conducted to establish the effectiveness of the proposed method.

</details>

<details>

<summary>2020-04-14 06:45:07 - A2D2: Audi Autonomous Driving Dataset</summary>

- *Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh Durgesh, Andrew S. Chung, Lorenz Hauswald, Viet Hoang Pham, Maximilian Mühlegg, Sebastian Dorn, Tiffany Fernandez, Martin Jänicke, Sudesh Mirashi, Chiragkumar Savani, Martin Sturm, Oleksandr Vorobiov, Martin Oelker, Sebastian Garreis, Peter Schuberth*

- `2004.06320v1` - [abs](http://arxiv.org/abs/2004.06320v1) - [pdf](http://arxiv.org/pdf/2004.06320v1)

> Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.

</details>

<details>

<summary>2020-04-14 08:33:42 - Knowledge Elicitation using Deep Metric Learning and Psychometric Testing</summary>

- *Lu Yin, Vlado Menkovski, Mykola Pechenizkiy*

- `2004.06353v1` - [abs](http://arxiv.org/abs/2004.06353v1) - [pdf](http://arxiv.org/pdf/2004.06353v1)

> Knowledge present in a domain is well expressed as relationships between corresponding concepts. For example, in zoology, animal species form complex hierarchies; in genomics, the different (parts of) molecules are organized in groups and subgroups based on their functions; plants, molecules, and astronomical objects all form complex taxonomies. Nevertheless, when applying supervised machine learning (ML) in such domains, we commonly reduce the complex and rich knowledge to a fixed set of labels, and induce a model shows good generalization performance with respect to these labels. The main reason for such a reductionist approach is the difficulty in eliciting the domain knowledge from the experts. Developing a label structure with sufficient fidelity and providing comprehensive multi-label annotation can be exceedingly labor-intensive in many real-world applications. In this paper, we provide a method for efficient hierarchical knowledge elicitation (HKE) from experts working with high-dimensional data such as images or videos. Our method is based on psychometric testing and active deep metric learning. The developed models embed the high-dimensional data in a metric space where distances are semantically meaningful, and the data can be organized in a hierarchical structure. We provide empirical evidence with a series of experiments on a synthetically generated dataset of simple shapes, and Cifar 10 and Fashion-MNIST benchmarks that our method is indeed successful in uncovering hierarchical structures.

</details>

<details>

<summary>2020-04-14 10:16:50 - StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of Very High Resolution Satellite Images by Data Standardization</summary>

- *Onur Tasar, Yuliya Tarabalka, Alain Giros, Pierre Alliez, Sébastien Clerc*

- `2004.06402v1` - [abs](http://arxiv.org/abs/2004.06402v1) - [pdf](http://arxiv.org/pdf/2004.06402v1)

> Domain adaptation for semantic segmentation has recently been actively studied to increase the generalization capabilities of deep learning models. The vast majority of the domain adaptation methods tackle single-source case, where the model trained on a single source domain is adapted to a target domain. However, these methods have limited practical real world applications, since usually one has multiple source domains with different data distributions. In this work, we deal with the multi-source domain adaptation problem. Our method, namely StandardGAN, standardizes each source and target domains so that all the data have similar data distributions. We then use the standardized source domains to train a classifier and segment the standardized target domain. We conduct extensive experiments on two remote sensing data sets, in which the first one consists of multiple cities from a single country, and the other one contains multiple cities from different countries. Our experimental results show that the standardized data generated by StandardGAN allow the classifiers to generate significantly better segmentation.

</details>

<details>

<summary>2020-04-14 13:44:30 - Unsupervised Multimodal Video-to-Video Translation via Self-Supervised Learning</summary>

- *Kangning Liu, Shuhang Gu, Andres Romero, Radu Timofte*

- `2004.06502v1` - [abs](http://arxiv.org/abs/2004.06502v1) - [pdf](http://arxiv.org/pdf/2004.06502v1)

> Existing unsupervised video-to-video translation methods fail to produce translated videos which are frame-wise realistic, semantic information preserving and video-level consistent. In this work, we propose UVIT, a novel unsupervised video-to-video translation model. Our model decomposes the style and the content, uses the specialized encoder-decoder structure and propagates the inter-frame information through bidirectional recurrent neural network (RNN) units. The style-content decomposition mechanism enables us to achieve style consistent video translation results as well as provides us with a good interface for modality flexible translation. In addition, by changing the input frames and style codes incorporated in our translation, we propose a video interpolation loss, which captures temporal information within the sequence to train our building blocks in a self-supervised manner. Our model can produce photo-realistic, spatio-temporal consistent translated videos in a multimodal way. Subjective and objective experimental results validate the superiority of our model over existing methods. More details can be found on our project website: https://uvit.netlify.com

</details>

<details>

<summary>2020-04-14 13:50:37 - Machine Translation with Cross-lingual Word Embeddings</summary>

- *Marco Berlot, Evan Kaplan*

- `1912.10167v2` - [abs](http://arxiv.org/abs/1912.10167v2) - [pdf](http://arxiv.org/pdf/1912.10167v2)

> Learning word embeddings using distributional information is a task that has been studied by many researchers, and a lot of studies are reported in the literature. On the contrary, less studies were done for the case of multiple languages. The idea is to focus on a single representation for a pair of languages such that semantically similar words are closer to one another in the induced representation irrespective of the language. In this way, when data are missing for a particular language, classifiers from another language can be used.

</details>

<details>

<summary>2020-04-14 14:38:41 - Multi-Ontology Refined Embeddings (MORE): A Hybrid Multi-Ontology and Corpus-based Semantic Representation for Biomedical Concepts</summary>

- *Steven Jiang, Weiyi Wu, Naofumi Tomita, Craig Ganoe, Saeed Hassanpour*

- `2004.06555v1` - [abs](http://arxiv.org/abs/2004.06555v1) - [pdf](http://arxiv.org/pdf/2004.06555v1)

> Objective: Currently, a major limitation for natural language processing (NLP) analyses in clinical applications is that a concept can be referenced in various forms across different texts. This paper introduces Multi-Ontology Refined Embeddings (MORE), a novel hybrid framework for incorporating domain knowledge from multiple ontologies into a distributional semantic model, learned from a corpus of clinical text.   Materials and Methods: We use the RadCore and MIMIC-III free-text datasets for the corpus-based component of MORE. For the ontology-based part, we use the Medical Subject Headings (MeSH) ontology and three state-of-the-art ontology-based similarity measures. In our approach, we propose a new learning objective, modified from the Sigmoid cross-entropy objective function.   Results and Discussion: We evaluate the quality of the generated word embeddings using two established datasets of semantic similarities among biomedical concept pairs. On the first dataset with 29 concept pairs, with the similarity scores established by physicians and medical coders, MORE's similarity scores have the highest combined correlation (0.633), which is 5.0% higher than that of the baseline model and 12.4% higher than that of the best ontology-based similarity measure.On the second dataset with 449 concept pairs, MORE's similarity scores have a correlation of 0.481, with the average of four medical residents' similarity ratings, and that outperforms the skip-gram model by 8.1% and the best ontology measure by 6.9%.

</details>

<details>

<summary>2020-04-14 20:05:28 - DALES: A Large-scale Aerial LiDAR Data Set for Semantic Segmentation</summary>

- *Nina Varney, Vijayan K. Asari, Quinn Graehling*

- `2004.11985v1` - [abs](http://arxiv.org/abs/2004.11985v1) - [pdf](http://arxiv.org/pdf/2004.11985v1)

> We present the Dayton Annotated LiDAR Earth Scan (DALES) data set, a new large-scale aerial LiDAR data set with over a half-billion hand-labeled points spanning 10 square kilometers of area and eight object categories. Large annotated point cloud data sets have become the standard for evaluating deep learning methods. However, most of the existing data sets focus on data collected from a mobile or terrestrial scanner with few focusing on aerial data. Point cloud data collected from an Aerial Laser Scanner (ALS) presents a new set of challenges and applications in areas such as 3D urban modeling and large-scale surveillance. DALES is the most extensive publicly available ALS data set with over 400 times the number of points and six times the resolution of other currently available annotated aerial point cloud data sets. This data set gives a critical number of expert verified hand-labeled points for the evaluation of new 3D deep learning algorithms, helping to expand the focus of current algorithms to aerial data. We describe the nature of our data, annotation workflow, and provide a benchmark of current state-of-the-art algorithm performance on the DALES data set.

</details>

<details>

<summary>2020-04-14 21:21:01 - Res-CR-Net, a residual network with a novel architecture optimized for the semantic segmentation of microscopy images</summary>

- *Hassan Abdallah, Asiri Liyanaarachchi, Maranda Saigh, Samantha Silvers, Suzan Arslanturk, Douglas J. Taatjes, Lars Larsson, Bhanu P. Jena, Domenico L. Gatti*

- `2004.08246v1` - [abs](http://arxiv.org/abs/2004.08246v1) - [pdf](http://arxiv.org/pdf/2004.08246v1)

> Deep Neural Networks (DNN) have been widely used to carry out segmentation tasks in both electron and light microscopy. Most DNNs developed for this purpose are based on some variation of the encoder-decoder type U-Net architecture, in combination with residual blocks to increase ease of training and resilience to gradient degradation. Here we introduce Res-CR-Net, a type of DNN that features residual blocks with either a bundle of separable atrous convolutions with different dilation rates or a convolutional LSTM. The number of filters used in each residual block and the number of blocks are the only hyperparameters that need to be modified in order to optimize the network training for a variety of different microscopy images.

</details>

<details>

<summary>2020-04-15 11:00:07 - Exploring Probabilistic Soft Logic as a framework for integrating top-down and bottom-up processing of language in a task context</summary>

- *Johannes Dellert*

- `2004.07000v1` - [abs](http://arxiv.org/abs/2004.07000v1) - [pdf](http://arxiv.org/pdf/2004.07000v1)

> This technical report describes a new prototype architecture designed to integrate top-down and bottom-up analysis of non-standard linguistic input, where a semantic model of the context of an utterance is used to guide the analysis of the non-standard surface forms, including their automated normalization in context. While the architecture is generally applicable, as a concrete use case of the architecture we target the generation of semantically-informed target hypotheses for answers written by German learners in response to reading comprehension questions, where the reading context and possible target answers are given.   The architecture integrates existing NLP components to produce candidate analyses on eight levels of linguistic modeling, all of which are broken down into atomic statements and connected into a large graphical model using Probabilistic Soft Logic (PSL) as a framework. Maximum a posteriori inference on the resulting graphical model then assigns a belief distribution to candidate target hypotheses. The current version of the architecture builds on Universal Dependencies (UD) as its representation formalism on the form level and on Abstract Meaning Representations (AMRs) to represent semantic analyses of learner answers and the context information provided by the target answers. These general choices will make it comparatively straightforward to apply the architecture to other tasks and other languages.

</details>

<details>

<summary>2020-04-15 18:01:36 - Learning Structured Embeddings of Knowledge Graphs with Adversarial Learning Framework</summary>

- *Jiehang Zeng, Lu Liu, Xiaoqing Zheng*

- `2004.07265v1` - [abs](http://arxiv.org/abs/2004.07265v1) - [pdf](http://arxiv.org/pdf/2004.07265v1)

> Many large-scale knowledge graphs are now available and ready to provide semantically structured information that is regarded as an important resource for question answering and decision support tasks. However, they are built on rigid symbolic frameworks which makes them hard to be used in other intelligent systems. We present a learning method using generative adversarial architecture designed to embed the entities and relations of the knowledge graphs into a continuous vector space. A generative network (GN) takes two elements of a (subject, predicate, object) triple as input and generates the vector representation of the missing element. A discriminative network (DN) scores a triple to distinguish a positive triple from those generated by GN. The training goal for GN is to deceive DN to make wrong classification. When arriving at a convergence, GN recovers the training data and can be used for knowledge graph completion, while DN is trained to be a good triple classifier. Unlike few previous studies based on generative adversarial architectures, our GN is able to generate unseen instances while they just use GN to better choose negative samples (already existed) for DN. Experiments demonstrate our method can improve classical relational learning models (e.g.TransE) with a significant margin on both the link prediction and triple classification tasks.

</details>

<details>

<summary>2020-04-16 00:48:32 - Radiologist-Level COVID-19 Detection Using CT Scans with Detail-Oriented Capsule Networks</summary>

- *Aryan Mobiny, Pietro Antonio Cicalese, Samira Zare, Pengyu Yuan, Mohammadsajad Abavisani, Carol C. Wu, Jitesh Ahuja, Patricia M. de Groot, Hien Van Nguyen*

- `2004.07407v1` - [abs](http://arxiv.org/abs/2004.07407v1) - [pdf](http://arxiv.org/pdf/2004.07407v1)

> Radiographic images offer an alternative method for the rapid screening and monitoring of Coronavirus Disease 2019 (COVID-19) patients. This approach is limited by the shortage of radiology experts who can provide a timely interpretation of these images. Motivated by this challenge, our paper proposes a novel learning architecture, called Detail-Oriented Capsule Networks (DECAPS), for the automatic diagnosis of COVID-19 from Computed Tomography (CT) scans. Our network combines the strength of Capsule Networks with several architecture improvements meant to boost classification accuracies. First, DECAPS uses an Inverted Dynamic Routing mechanism which increases model stability by preventing the passage of information from non-descriptive regions. Second, DECAPS employs a Peekaboo training procedure which uses a two-stage patch crop and drop strategy to encourage the network to generate activation maps for every target concept. The network then uses the activation maps to focus on regions of interest and combines both coarse and fine-grained representations of the data. Finally, we use a data augmentation method based on conditional generative adversarial networks to deal with the issue of data scarcity. Our model achieves 84.3% precision, 91.5% recall, and 96.1% area under the ROC curve, significantly outperforming state-of-the-art methods. We compare the performance of the DECAPS model with three experienced, well-trained thoracic radiologists and show that the architecture significantly outperforms them. While further studies on larger datasets are required to confirm this finding, our results imply that architectures like DECAPS can be used to assist radiologists in the CT scan mediated diagnosis of COVID-19.

</details>

<details>

<summary>2020-04-16 06:46:25 - Generating Emotionally Aligned Responses in Dialogues using Affect Control Theory</summary>

- *Nabiha Asghar, Ivan Kobyzev, Jesse Hoey, Pascal Poupart, Muhammad Bilal Sheikh*

- `2003.03645v2` - [abs](http://arxiv.org/abs/2003.03645v2) - [pdf](http://arxiv.org/pdf/2003.03645v2)

> State-of-the-art neural dialogue systems excel at syntactic and semantic modelling of language, but often have a hard time establishing emotional alignment with the human interactant during a conversation. In this work, we bring Affect Control Theory (ACT), a socio-mathematical model of emotions for human-human interactions, to the neural dialogue generation setting. ACT makes predictions about how humans respond to emotional stimuli in social situations. Due to this property, ACT and its derivative probabilistic models have been successfully deployed in several applications of Human-Computer Interaction, including empathetic tutoring systems, assistive healthcare devices and two-person social dilemma games. We investigate how ACT can be used to develop affect-aware neural conversational agents, which produce emotionally aligned responses to prompts and take into consideration the affective identities of the interactants.

</details>

<details>

<summary>2020-04-16 07:32:00 - COSTRA 1.0: A Dataset of Complex Sentence Transformations</summary>

- *Petra Barancikova, Ondrej Bojar*

- `1912.01673v2` - [abs](http://arxiv.org/abs/1912.01673v2) - [pdf](http://arxiv.org/pdf/1912.01673v2)

> We present COSTRA 1.0, a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. This first version of the dataset is limited to sentences in Czech but the construction method is universal and we plan to use it also for other languages. The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation. The hope is that with this dataset, we should be able to test semantic properties of sentence embeddings and perhaps even to find some topologically interesting 'skeleton' in the sentence embedding space. A preliminary analysis using LASER, multi-purpose multi-lingual sentence embeddings suggests that the LASER space does not exhibit the desired properties.

</details>

<details>

<summary>2020-04-16 11:38:14 - Toward Efficient Web Publishing with Provenance of Information Using Trusty URIs: Applying the proposed model with the Quran</summary>

- *Khalid S. Aloufi, Abdulrahman A. Alsewari*

- `2004.07609v1` - [abs](http://arxiv.org/abs/2004.07609v1) - [pdf](http://arxiv.org/pdf/2004.07609v1)

> This research presents a methodology for trusting the provenance of data on the web. The implication is that data does not change after publication and the source of the data is stable. There are different data that should not change over time, such as published information in books and similar documents as well as news or events reported on the web. If the data change after publication on the web, the web pages that reference the unstable data will lose points of interest or link to different resources. With the current move to linked data and the semantic web, this is becoming a greater obstacle to be solved. This research presents a methodology for establishing trusted information using an encoded reference of the data embedded in its URI, which creates a stable reference of the data and a method for ensuring its provenance stability. After applying the methodology, the results showed that the methodology is highly applicable and has no overhead cost over the loading time. The novel solution can be applied directly to any data portals or web content management systems.

</details>

<details>

<summary>2020-04-16 16:24:56 - CO.ME.T.A. -- covid-19 media textual analysis. A dashboard for media monitoring</summary>

- *Emma Zavarrone, Maria Gabriella Grassia, Marina Marino, Rasanna Cataldo, Rocco Mazza, Nicola Canestrari*

- `2004.07742v1` - [abs](http://arxiv.org/abs/2004.07742v1) - [pdf](http://arxiv.org/pdf/2004.07742v1)

> The focus of this paper is to trace how mass media, particularly newspapers, have addressed the issues about the containment of contagion or the explanation of epidemiological evolution. We propose an interactive dashboard: CO.ME.T.A.. During crises it is important to shape the best communication strategies in order to respond to critical situations. In this regard, it is important to monitor the information that mass media and social platforms convey. The dashboard allows to explore the mining of contents extracted and study the lexical structure that links the main discussion topics. The dashboard merges together four methods: text mining, sentiment analysis, textual network analysis and latent topic models. Results obtained on a subset of documents show not only a health-related semantic dimension, but it also extends to social-economic dimensions.

</details>

<details>

<summary>2020-04-16 20:42:51 - Cityscapes-Panoptic-Parts and PASCAL-Panoptic-Parts datasets for Scene Understanding</summary>

- *Panagiotis Meletis, Xiaoxiao Wen, Chenyang Lu, Daan de Geus, Gijs Dubbelman*

- `2004.07944v1` - [abs](http://arxiv.org/abs/2004.07944v1) - [pdf](http://arxiv.org/pdf/2004.07944v1)

> In this technical report, we present two novel datasets for image scene understanding. Both datasets have annotations compatible with panoptic segmentation and additionally they have part-level labels for selected semantic classes. This report describes the format of the two datasets, the annotation protocols, the merging strategies, and presents the datasets statistics. The datasets labels together with code for processing and visualization will be published at https://github.com/tue-mps/panoptic_parts.

</details>

<details>

<summary>2020-04-17 01:56:52 - SECRET: Semantically Enhanced Classification of Real-world Tasks</summary>

- *Ayten Ozge Akmandor, Jorge Ortiz, Irene Manotas, Bongjun Ko, Niraj K. Jha*

- `1905.12356v3` - [abs](http://arxiv.org/abs/1905.12356v3) - [pdf](http://arxiv.org/pdf/1905.12356v3)

> Supervised machine learning (ML) algorithms are aimed at maximizing classification performance under available energy and storage constraints. They try to map the training data to the corresponding labels while ensuring generalizability to unseen data. However, they do not integrate meaning-based relationships among labels in the decision process. On the other hand, natural language processing (NLP) algorithms emphasize the importance of semantic information. In this paper, we synthesize the complementary advantages of supervised ML and NLP algorithms into one method that we refer to as SECRET (Semantically Enhanced Classification of REal-world Tasks). SECRET performs classifications by fusing the semantic information of the labels with the available data: it combines the feature space of the supervised algorithms with the semantic space of the NLP algorithms and predicts labels based on this joint space. Experimental results indicate that, compared to traditional supervised learning, SECRET achieves up to 14.0% accuracy and 13.1% F1 score improvements. Moreover, compared to ensemble methods, SECRET achieves up to 12.7% accuracy and 13.3% F1 score improvements. This points to a new research direction for supervised classification based on incorporation of semantic information.

</details>

<details>

<summary>2020-04-17 05:18:51 - Defining Smart Contract Defects on Ethereum</summary>

- *Jiachi Chen, Xin Xia, David Lo, John Grundy, Daniel Xiapu Luo, Ting Chen*

- `1905.01467v3` - [abs](http://arxiv.org/abs/1905.01467v3) - [pdf](http://arxiv.org/pdf/1905.01467v3)

> Smart contracts are programs running on a blockchain. They are immutable to change, and hence can not be patched for bugs once deployed. Thus it is critical to ensure they are bug-free and well-designed before deployment. A Contract defect is an error, flaw or fault in a smart contract that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. The detection of contract defects is a method to avoid potential bugs and improve the design of existing code. Since smart contracts contain numerous distinctive features, such as the gas system. decentralized, it is important to find smart contract specified defects. To fill this gap, we collected smart-contract-related posts from Ethereum StackExchange, as well as real-world smart contracts. We manually analyzed these posts and contracts; using them to define 20 kinds of contract defects. We categorized them into indicating potential security, availability, performance, maintainability and reusability problems. To validate if practitioners consider these contract as harmful, we created an online survey and received 138 responses from 32 different countries. Feedback showed these contract defects are harmful and removing them would improve the quality and robustness of smart contracts. We manually identified our defined contract defects in 587 real world smart contract and publicly released our dataset. Finally, we summarized 5 impacts caused by contract defects. These help developers better understand the symptoms of the defects and removal priority.

</details>

<details>

<summary>2020-04-17 05:26:47 - Knowledge-guided Deep Reinforcement Learning for Interactive Recommendation</summary>

- *Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wei Liu, Wenjie Zhang*

- `2004.08068v1` - [abs](http://arxiv.org/abs/2004.08068v1) - [pdf](http://arxiv.org/pdf/2004.08068v1)

> Interactive recommendation aims to learn from dynamic interactions between items and users to achieve responsiveness and accuracy. Reinforcement learning is inherently advantageous for coping with dynamic environments and thus has attracted increasing attention in interactive recommendation research. Inspired by knowledge-aware recommendation, we proposed Knowledge-Guided deep Reinforcement learning (KGRL) to harness the advantages of both reinforcement learning and knowledge graphs for interactive recommendation. This model is implemented upon the actor-critic network framework. It maintains a local knowledge network to guide decision-making and employs the attention mechanism to capture long-term semantics between items. We have conducted comprehensive experiments in a simulated online environment with six public real-world datasets and demonstrated the superiority of our model over several state-of-the-art methods.

</details>

<details>

<summary>2020-04-17 06:46:36 - Knowledge Fusion and Semantic Knowledge Ranking for Open Domain Question Answering</summary>

- *Pratyay Banerjee, Chitta Baral*

- `2004.03101v2` - [abs](http://arxiv.org/abs/2004.03101v2) - [pdf](http://arxiv.org/pdf/2004.03101v2)

> Open Domain Question Answering requires systems to retrieve external knowledge and perform multi-hop reasoning by composing knowledge spread over multiple sentences. In the recently introduced open domain question answering challenge datasets, QASC and OpenBookQA, we need to perform retrieval of facts and compose facts to correctly answer questions. In our work, we learn a semantic knowledge ranking model to re-rank knowledge retrieved through Lucene based information retrieval systems. We further propose a "knowledge fusion model" which leverages knowledge in BERT-based language models with externally retrieved knowledge and improves the knowledge understanding of the BERT-based language models. On both OpenBookQA and QASC datasets, the knowledge fusion model with semantically re-ranked knowledge outperforms previous attempts.

</details>

<details>

<summary>2020-04-17 07:43:38 - Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning</summary>

- *Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung*

- `2004.08097v1` - [abs](http://arxiv.org/abs/2004.08097v1) - [pdf](http://arxiv.org/pdf/2004.08097v1)

> Even though BERT achieves successful performance improvements in various supervised learning tasks, applying BERT for unsupervised tasks still holds a limitation that it requires repetitive inference for computing contextual language representations. To resolve the limitation, we propose a novel deep bidirectional language model called Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and has benefits of the deep bidirectional architecture like BERT. In run-time experiments on CPU environments, the proposed T-TA performs over six times faster than the BERT-based model in the reranking task and twelve times faster in the semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks.

</details>

<details>

<summary>2020-04-17 13:09:17 - SMT-Friendly Formalization of the Solidity Memory Model</summary>

- *Ákos Hajdu, Dejan Jovanović*

- `2001.03256v2` - [abs](http://arxiv.org/abs/2001.03256v2) - [pdf](http://arxiv.org/pdf/2001.03256v2)

> Solidity is the dominant programming language for Ethereum smart contracts. This paper presents a high-level formalization of the Solidity language with a focus on the memory model. The presented formalization covers all features of the language related to managing state and memory. In addition, the formalization we provide is effective: all but few features can be encoded in the quantifier-free fragment of standard SMT theories. This enables precise and efficient reasoning about the state of smart contracts written in Solidity. The formalization is implemented in the solc-verify verifier and we provide an extensive set of tests that covers the breadth of the required semantics. We also provide an evaluation on the test set that validates the semantics and shows the novelty of the approach compared to other Solidity-level contract analysis tools.

</details>

<details>

<summary>2020-04-17 16:39:26 - Representation Learning of Histopathology Images using Graph Neural Networks</summary>

- *Mohammed Adnan, Shivam Kalra, Hamid R. Tizhoosh*

- `2004.07399v2` - [abs](http://arxiv.org/abs/2004.07399v2) - [pdf](http://arxiv.org/pdf/2004.07399v2)

> Representation learning for Whole Slide Images (WSIs) is pivotal in developing image-based systems to achieve higher precision in diagnostic pathology. We propose a two-stage framework for WSI representation learning. We sample relevant patches using a color-based method and use graph neural networks to learn relations among sampled patches to aggregate the image information into a single vector representation. We introduce attention via graph pooling to automatically infer patches with higher relevance. We demonstrate the performance of our approach for discriminating two sub-types of lung cancers, Lung Adenocarcinoma (LUAD) & Lung Squamous Cell Carcinoma (LUSC). We collected 1,026 lung cancer WSIs with the 40$\times$ magnification from The Cancer Genome Atlas (TCGA) dataset, the largest public repository of histopathology images and achieved state-of-the-art accuracy of 88.8% and AUC of 0.89 on lung cancer sub-type classification by extracting features from a pre-trained DenseNet

</details>

<details>

<summary>2020-04-17 17:49:45 - Exploring the Combination of Contextual Word Embeddings and Knowledge Graph Embeddings</summary>

- *Lea Dieudonat, Kelvin Han, Phyllicia Leavitt, Esteban Marquer*

- `2004.08371v1` - [abs](http://arxiv.org/abs/2004.08371v1) - [pdf](http://arxiv.org/pdf/2004.08371v1)

> ``Classical'' word embeddings, such as Word2Vec, have been shown to capture the semantics of words based on their distributional properties. However, their ability to represent the different meanings that a word may have is limited. Such approaches also do not explicitly encode relations between entities, as denoted by words. Embeddings of knowledge bases (KB) capture the explicit relations between entities denoted by words, but are not able to directly capture the syntagmatic properties of these words. To our knowledge, recent research have focused on representation learning that augment the strengths of one with the other. In this work, we begin exploring another approach using contextual and KB embeddings jointly at the same level and propose two tasks -- an entity typing and a relation typing task -- that evaluate the performance of contextual and KB embeddings. We also evaluated a concatenated model of contextual and KB embeddings with these two tasks, and obtain conclusive results on the first task. We hope our work may contribute as a basis for models and datasets that develop in the direction of this approach.

</details>

<details>

<summary>2020-04-18 05:14:18 - Effect of Text Color on Word Embeddings</summary>

- *Masaya Ikoma, Brian Kenji Iwana, Seiichi Uchida*

- `2004.08526v1` - [abs](http://arxiv.org/abs/2004.08526v1) - [pdf](http://arxiv.org/pdf/2004.08526v1)

> In natural scenes and documents, we can find the correlation between a text and its color. For instance, the word, "hot", is often printed in red, while "cold" is often in blue. This correlation can be thought of as a feature that represents the semantic difference between the words. Based on this observation, we propose the idea of using text color for word embeddings. While text-only word embeddings (e.g. word2vec) have been extremely successful, they often represent antonyms as similar since they are often interchangeable in sentences. In this paper, we try two tasks to verify the usefulness of text color in understanding the meanings of words, especially in identifying synonyms and antonyms. First, we quantify the color distribution of words from the book cover images and analyze the correlation between the color and meaning of the word. Second, we try to retrain word embeddings with the color distribution of words as a constraint. By observing the changes in the word embeddings of synonyms and antonyms before and after re-training, we aim to understand the kind of words that have positive or negative effects in their word embeddings when incorporating text color information.

</details>

<details>

<summary>2020-04-18 08:59:41 - sFuzz: An Efficient Adaptive Fuzzer for Solidity Smart Contracts</summary>

- *Tai D. Nguyen, Long H. Pham, Jun Sun, Yun Lin, Quang Tran Minh*

- `2004.08563v1` - [abs](http://arxiv.org/abs/2004.08563v1) - [pdf](http://arxiv.org/pdf/2004.08563v1)

> Smart contracts are Turing-complete programs that execute on the infrastructure of the blockchain, which often manage valuable digital assets. Solidity is one of the most popular programming languages for writing smart contracts on the Ethereum platform. Like traditional programs, smart contracts may contain vulnerabilities. Unlike traditional programs, smart contracts cannot be easily patched once they are deployed. It is thus important that smart contracts are tested thoroughly before deployment. In this work, we present an adaptive fuzzer for smart contracts on the Ethereum platform called sFuzz. Compared to existing Solidity fuzzers, sFuzz combines the strategy in the AFL fuzzer and an efficient lightweight multi-objective adaptive strategy targeting those hard-to-cover branches. sFuzz has been applied to more than 4 thousand smart contracts and the experimental results show that (1) sFuzz is efficient, e.g., two orders of magnitude faster than state-of-the-art tools; (2) sFuzz is effective in achieving high code coverage and discovering vulnerabilities; and (3) the different fuzzing strategies in sFuzz complement each other.

</details>

<details>

<summary>2020-04-18 17:54:55 - A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention</summary>

- *Maria Mihaela Trusca, Daan Wassenberg, Flavius Frasincar, Rommert Dekker*

- `2004.08673v1` - [abs](http://arxiv.org/abs/2004.08673v1) - [pdf](http://arxiv.org/pdf/2004.08673v1)

> The Web has become the main platform where people express their opinions about entities of interest and their associated aspects. Aspect-Based Sentiment Analysis (ABSA) aims to automatically compute the sentiment towards these aspects from opinionated text. In this paper we extend the state-of-the-art Hybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two directions. First we replace the non-contextual word embeddings with deep contextual word embeddings in order to better cope with the word semantics in a given text. Second, we use hierarchical attention by adding an extra attention layer to the HAABSA high-level representations in order to increase the method flexibility in modeling the input data. Using two standard datasets (SemEval 2015 and SemEval 2016) we show that the proposed extensions improve the accuracy of the built model for ABSA.

</details>

<details>

<summary>2020-04-18 19:39:58 - Identifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study</summary>

- *Navedanjum Ansari, Rajesh Sharma*

- `2004.11694v1` - [abs](http://arxiv.org/abs/2004.11694v1) - [pdf](http://arxiv.org/pdf/2004.11694v1)

> Identifying semantically identical questions on, Question and Answering social media platforms like Quora is exceptionally significant to ensure that the quality and the quantity of content are presented to users, based on the intent of the question and thus enriching overall user experience. Detecting duplicate questions is a challenging problem because natural language is very expressive, and a unique intent can be conveyed using different words, phrases, and sentence structuring. Machine learning and deep learning methods are known to have accomplished superior results over traditional natural language processing techniques in identifying similar texts. In this paper, taking Quora for our case study, we explored and applied different machine learning and deep learning techniques on the task of identifying duplicate questions on Quora's dataset. By using feature engineering, feature importance techniques, and experimenting with seven selected machine learning classifiers, we demonstrated that our models outperformed previous studies on this task. Xgboost model with character level term frequency and inverse term frequency is our best machine learning model that has also outperformed a few of the Deep learning baseline models. We applied deep learning techniques to model four different deep neural networks of multiple layers consisting of Glove embeddings, Long Short Term Memory, Convolution, Max pooling, Dense, Batch Normalization, Activation functions, and model merge. Our deep learning models achieved better accuracy than machine learning models. Three out of four proposed architectures outperformed the accuracy from previous machine learning and deep learning research work, two out of four models outperformed accuracy from previous deep learning study on Quora's question pair dataset, and our best model achieved accuracy of 85.82% which is close to Quora state of the art accuracy.

</details>

<details>

<summary>2020-04-19 08:05:59 - UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</summary>

- *Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, Jian Wu*

- `2004.08790v1` - [abs](http://arxiv.org/abs/2004.08790v1) - [pdf](http://arxiv.org/pdf/2004.08790v1)

> Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets. The code is available at: github.com/ZJUGiveLab/UNet-Version

</details>

<details>

<summary>2020-04-19 08:13:13 - Pattern Learning for Detecting Defect Reports and Improvement Requests in App Reviews</summary>

- *Gino V. H. Mangnoesing, Maria Mihaela Trusca, Flavius Frasincar*

- `2004.08793v1` - [abs](http://arxiv.org/abs/2004.08793v1) - [pdf](http://arxiv.org/pdf/2004.08793v1)

> Online reviews are an important source of feedback for understanding customers. In this study, we follow novel approaches that target this absence of actionable insights by classifying reviews as defect reports and requests for improvement. Unlike traditional classification methods based on expert rules, we reduce the manual labour by employing a supervised system that is capable of learning lexico-semantic patterns through genetic programming. Additionally, we experiment with a distantly-supervised SVM that makes use of noisy labels generated by patterns. Using a real-world dataset of app reviews, we show that the automatically learned patterns outperform the manually created ones, to be generated. Also the distantly-supervised SVM models are not far behind the pattern-based solutions, showing the usefulness of this approach when the amount of annotated data is limited.

</details>

<details>

<summary>2020-04-19 08:27:57 - Extractive Summarization as Text Matching</summary>

- *Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang*

- `2004.08795v1` - [abs](http://arxiv.org/abs/2004.08795v1) - [pdf](http://arxiv.org/pdf/2004.08795v1)

> This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset.   Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/MatchSum.

</details>

<details>

<summary>2020-04-19 08:41:12 - Knowledge-graph based Proactive Dialogue Generation with Improved Meta-Learning</summary>

- *Hongcai Xu, Junpeng Bao, Junqing Wang*

- `2004.08798v1` - [abs](http://arxiv.org/abs/2004.08798v1) - [pdf](http://arxiv.org/pdf/2004.08798v1)

> Knowledge graph-based dialogue systems can narrow down knowledge candidates for generating informative and diverse responses with the use of prior information, e.g., triple attributes or graph paths. However, most current knowledge graph (KG) cover incomplete domain-specific knowledge. To overcome this drawback, we propose a knowledge graph based proactive dialogue generation model (KgDg) with three components, improved model-agnostic meta-learning algorithm (MAML), knowledge selection in knowledge triplets embedding, and knowledge aware proactive response generator. For knowledge triplets embedding and selection, we formulate it as a problem of sentence embedding to better capture semantic information. Our improved MAML algorithm is capable of learning general features from a limited number of knowledge graphs, which can also quickly adapt to dialogue generation with unseen knowledge triplets. Extensive experiments are conducted on a knowledge aware dialogue dataset (DuConv). The results show that KgDg adapts both fast and well to knowledge graph-based dialogue generation and outperforms state-of-the-art baseline.

</details>

<details>

<summary>2020-04-19 11:00:30 - Graph-Structured Referring Expression Reasoning in The Wild</summary>

- *Sibei Yang, Guanbin Li, Yizhou Yu*

- `2004.08814v1` - [abs](http://arxiv.org/abs/2004.08814v1) - [pdf](http://arxiv.org/pdf/2004.08814v1)

> Grounding referring expressions aims to locate in an image an object referred to by a natural language expression. The linguistic structure of a referring expression provides a layout of reasoning over the visual contents, and it is often crucial to align and jointly understand the image and the referring expression. In this paper, we propose a scene graph guided modular network (SGMN), which performs reasoning over a semantic graph and a scene graph with neural modules under the guidance of the linguistic structure of the expression. In particular, we model the image as a structured semantic graph, and parse the expression into a language scene graph. The language scene graph not only decodes the linguistic structure of the expression, but also has a consistent representation with the image semantic graph. In addition to exploring structured solutions to grounding referring expressions, we also propose Ref-Reasoning, a large-scale real-world dataset for structured referring expression reasoning. We automatically generate referring expressions over the scene graphs of images using diverse expression templates and functional programs. This dataset is equipped with real-world visual contents as well as semantically rich expressions with different reasoning layouts. Experimental results show that our SGMN not only significantly outperforms existing state-of-the-art algorithms on the new Ref-Reasoning dataset, but also surpasses state-of-the-art structured methods on commonly used benchmark datasets. It can also provide interpretable visual evidences of reasoning. Data and code are available at https://github.com/sibeiyang/sgmn

</details>

<details>

<summary>2020-04-19 11:04:35 - Relationship-Embedded Representation Learning for Grounding Referring Expressions</summary>

- *Sibei Yang, Guanbin Li, Yizhou Yu*

- `1906.04464v3` - [abs](http://arxiv.org/abs/1906.04464v3) - [pdf](http://arxiv.org/pdf/1906.04464v3)

> Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a Cross-Modal Relationship Extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods. Code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models

</details>

<details>

<summary>2020-04-19 12:25:11 - Train, Learn, Expand, Repeat</summary>

- *Abhijeet Parida, Aadhithya Sankar, Rami Eisawy, Tom Finck, Benedikt Wiestler, Franz Pfister, Julia Moosbauer*

- `2003.08469v2` - [abs](http://arxiv.org/abs/2003.08469v2) - [pdf](http://arxiv.org/pdf/2003.08469v2)

> High-quality labeled data is essential to successfully train supervised machine learning models. Although a large amount of unlabeled data is present in the medical domain, labeling poses a major challenge: medical professionals who can expertly label the data are a scarce and expensive resource. Making matters worse, voxel-wise delineation of data (e.g. for segmentation tasks) is tedious and suffers from high inter-rater variance, thus dramatically limiting available training data. We propose a recursive training strategy to perform the task of semantic segmentation given only very few training samples with pixel-level annotations. We expand on this small training set having cheaper image-level annotations using a recursive training strategy. We apply this technique on the segmentation of intracranial hemorrhage (ICH) in CT (computed tomography) scans of the brain, where typically few annotated data is available.

</details>

<details>

<summary>2020-04-19 14:52:22 - Superkernel Neural Architecture Search for Image Denoising</summary>

- *Marcin Możejko, Tomasz Latkowski, Łukasz Treszczotko, Michał Szafraniuk, Krzysztof Trojanowski*

- `2004.08870v1` - [abs](http://arxiv.org/abs/2004.08870v1) - [pdf](http://arxiv.org/pdf/2004.08870v1)

> Recent advancements in Neural Architecture Search(NAS) resulted in finding new state-of-the-art Artificial Neural Network (ANN) solutions for tasks like image classification, object detection, or semantic segmentation without substantial human supervision. In this paper, we focus on exploring NAS for a dense prediction task that is image denoising. Due to a costly training procedure, most NAS solutions for image enhancement rely on reinforcement learning or evolutionary algorithm exploration, which usually take weeks (or even months) to train. Therefore, we introduce a new efficient implementation of various superkernel techniques that enable fast (6-8 RTX2080 GPU hours) single-shot training of models for dense predictions. We demonstrate the effectiveness of our method on the SIDD+ benchmark for image denoising.

</details>

<details>

<summary>2020-04-19 17:21:30 - Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network</summary>

- *Md. Rezaul Karim, Bharathi Raja Chakravarthi, John P. McCrae, Michael Cochez*

- `2004.07807v2` - [abs](http://arxiv.org/abs/2004.07807v2) - [pdf](http://arxiv.org/pdf/2004.07807v2)

> Exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices but also enables people to express anti-social behaviour like online harassment, cyberbullying, and hate speech. Numerous works have been proposed to utilize these data for social and anti-social behaviours analysis, document characterization, and sentiment analysis by predicting the contexts mostly for highly resourced languages such as English. However, there are languages that are under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese, Telugu that lack of computational resources for the NLP tasks. In this paper, we provide several classification benchmarks for Bengali, an under-resourced language. We prepared three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis, respectively. We built the largest Bengali word embedding models to date based on 250 million articles, which we call BengFastText. We perform three different experiments, covering document classification, sentiment analysis, and hate speech detection. We incorporate word embeddings into a Multichannel Convolutional-LSTM (MConv-LSTM) network for predicting different types of hate speech, document classification, and sentiment analysis. Experiments demonstrate that BengFastText can capture the semantics of words from respective contexts correctly. Evaluations against several baseline embedding models, e.g., Word2Vec and GloVe yield up to 92.30%, 82.25%, and 90.45% F1-scores in case of document classification, sentiment analysis, and hate speech detection, respectively during 5-fold cross-validation tests.

</details>

<details>

<summary>2020-04-19 17:56:05 - Reverse engineering of CAD models via clustering and approximate implicitization</summary>

- *Andrea Raffo, Oliver J. D. Barrowclough, Georg Muntingh*

- `1810.07451v3` - [abs](http://arxiv.org/abs/1810.07451v3) - [pdf](http://arxiv.org/pdf/1810.07451v3)

> In applications like computer aided design, geometric models are often represented numerically as polynomial splines or NURBS, even when they originate from primitive geometry. For purposes such as redesign and isogeometric analysis, it is of interest to extract information about the underlying geometry through reverse engineering. In this work we develop a novel method to determine these primitive shapes by combining clustering analysis with approximate implicitization. The proposed method is automatic and can recover algebraic hypersurfaces of any degree in any dimension. In exact arithmetic, the algorithm returns exact results. All the required parameters, such as the implicit degree of the patches and the number of clusters of the model, are inferred using numerical approaches in order to obtain an algorithm that requires as little manual input as possible. The effectiveness, efficiency and robustness of the method are shown both in a theoretical analysis and in numerical examples implemented in Python.

</details>

<details>

<summary>2020-04-19 19:07:28 - Generative Adversarial Network Rooms in Generative Graph Grammar Dungeons for The Legend of Zelda</summary>

- *Jake Gutierrez, Jacob Schrum*

- `2001.05065v2` - [abs](http://arxiv.org/abs/2001.05065v2) - [pdf](http://arxiv.org/pdf/2001.05065v2)

> Generative Adversarial Networks (GANs) have demonstrated their ability to learn patterns in data and produce new exemplars similar to, but different from, their training set in several domains, including video games. However, GANs have a fixed output size, so creating levels of arbitrary size for a dungeon crawling game is difficult. GANs also have trouble encoding semantic requirements that make levels interesting and playable. This paper combines a GAN approach to generating individual rooms with a graph grammar approach to combining rooms into a dungeon. The GAN captures design principles of individual rooms, but the graph grammar organizes rooms into a global layout with a sequence of obstacles determined by a designer. Room data from The Legend of Zelda is used to train the GAN. This approach is validated by a user study, showing that GAN dungeons are as enjoyable to play as a level from the original game, and levels generated with a graph grammar alone. However, GAN dungeons have rooms considered more complex, and plain graph grammar's dungeons are considered least complex and challenging. Only the GAN approach creates an extensive supply of both layouts and rooms, where rooms span across the spectrum of those seen in the training set to new creations merging design principles from multiple rooms.

</details>

<details>

<summary>2020-04-19 19:18:51 - Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing</summary>

- *Md. Rezaul Karim, Michael Cochez, Achille Zappa, Ratnesh Sahay, Oya Beyan, Dietrich-Rebholz Schuhmann, Stefan Decker*

- `1805.12218v2` - [abs](http://arxiv.org/abs/1805.12218v2) - [pdf](http://arxiv.org/pdf/1805.12218v2)

> The study of genetic variants can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how patients react to drugs. Machine learning algorithms are increasingly being applied to identify interacting GVs to understand their complex phenotypic traits. Since the performance of a learning algorithm not only depends on the size and nature of the data but also on the quality of underlying representation, deep neural networks can learn non-linear mappings that allow transforming GVs data into more clustering and classification friendly representations than manual feature selection. In this paper, we proposed convolutional embedded networks in which we combine two DNN architectures called convolutional embedded clustering and convolutional autoencoder classifier for clustering individuals and predicting geographic ethnicity based on GVs, respectively. We employed CAE-based representation learning on 95 million GVs from the 1000 genomes and Simons genome diversity projects. Quantitative and qualitative analyses with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted population groups in 22 hours with an adjusted rand index of 0.915, the normalized mutual information of 0.92, and the clustering accuracy of 89%. Contrarily, the CAE classifier can predict the geographic ethnicity of unknown samples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and 0.8245, respectively. To provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees(GBT) and SHAP. Overall, our approach is transparent and faster than the baseline methods, and scalable for 5% to 100% of the full human genome.

</details>

<details>

<summary>2020-04-19 21:40:00 - An overview of Intrusion Detection and Prevention Systems</summary>

- *Keturahlee Coulibaly*

- `2004.08967v1` - [abs](http://arxiv.org/abs/2004.08967v1) - [pdf](http://arxiv.org/pdf/2004.08967v1)

> Cyber threats are increasing not only in their volume but also in their sophistication and difficulty to detect. Attacks have become a national/global threat as they have targeted private and public, as well as government sectors over the years. This is a growing issue and organisations are taking steps to reduce, detect and prevent threats. To do this they need to use systems that are equipped with the capabilities to do either of those steps and develop them for the type of networks they use, for instance wired or wireless. One of these systems are Intrusion Detection Systems (IDS), which can be used as the first defence mechanism or a secondary defence mechanism of a threat or an attack. There are different types of attacks that can occur in a network, such as Denial of service (DoS)/Distributed Denial of Service (DDoS), port scanning, malware or ransomware and so forth that IDSs have a capability of detecting. Assisting in the mitigation of such attacks, there are also Intrusion Prevention Systems (IPS) whose role has a different purpose than that of IDSs. Unlike IDSs they not only detect threats but prevent them from disrupting the network, IPSs can be used in conjunction with IDSs to double the defences. This paper provides an overview of IDS and their classifications and IPS. It will detail typical benefits and limitations to using IDSs, IPSs and the hybrids (such as Intrusions Detection Prevention Systems (IDPSs and more)) which will be discussed further. It will also outline developments in the making using ML and how it is used to improve these systems and the dilemmas they produce and possible ways to counter act them.

</details>

<details>

<summary>2020-04-19 23:42:19 - Learning to Determine the Quality of News Headlines</summary>

- *Amin Omidvar, Hossein Poormodheji, Aijun An, Gordon Edall*

- `1911.11139v2` - [abs](http://arxiv.org/abs/1911.11139v2) - [pdf](http://arxiv.org/pdf/1911.11139v2)

> Today, most newsreaders read the online version of news articles rather than traditional paper-based newspapers. Also, news media publishers rely heavily on the income generated from subscriptions and website visits made by newsreaders. Thus, online user engagement is a very important issue for online newspapers. Much effort has been spent on writing interesting headlines to catch the attention of online users. On the other hand, headlines should not be misleading (e.g., clickbaits); otherwise, readers would be disappointed when reading the content. In this paper, we propose four indicators to determine the quality of published news headlines based on their click count and dwell time, which are obtained by website log analysis. Then, we use soft target distribution of the calculated quality indicators to train our proposed deep learning model which can predict the quality of unpublished news headlines. The proposed model not only processes the latent features of both headline and body of the article to predict its headline quality but also considers the semantic relation between headline and body as well. To evaluate our model, we use a real dataset from a major Canadian newspaper. Results show our proposed model outperforms other state-of-the-art NLP models.

</details>

<details>

<summary>2020-04-20 02:02:19 - Convolutional Neural Networks for Image-based Corn Kernel Detection and Counting</summary>

- *Saeed Khaki, Hieu Pham, Ye Han, Andy Kuhl, Wade Kent, Lizhi Wang*

- `2003.12025v2` - [abs](http://arxiv.org/abs/2003.12025v2) - [pdf](http://arxiv.org/pdf/2003.12025v2)

> Precise in-season corn grain yield estimates enable farmers to make real-time accurate harvest and grain marketing decisions minimizing possible losses of profitability. A well developed corn ear can have up to 800 kernels, but manually counting the kernels on an ear of corn is labor-intensive, time consuming and prone to human error. From an algorithmic perspective, the detection of the kernels from a single corn ear image is challenging due to the large number of kernels at different angles and very small distance among the kernels. In this paper, we propose a kernel detection and counting method based on a sliding window approach. The proposed method detect and counts all corn kernels in a single corn ear image taken in uncontrolled lighting conditions. The sliding window approach uses a convolutional neural network (CNN) for kernel detection. Then, a non-maximum suppression (NMS) is applied to remove overlapping detections. Finally, windows that are classified as kernel are passed to another CNN regression model for finding the (x,y) coordinates of the center of kernel image patches. Our experiments indicate that the proposed method can successfully detect the corn kernels with a low detection error and is also able to detect kernels on a batch of corn ears positioned at different angles.

</details>

<details>

<summary>2020-04-20 11:48:24 - A Practical Guide to Studying Emergent Communication through Grounded Language Games</summary>

- *Jens Nevens, Paul Van Eecke, Katrien Beuls*

- `2004.09218v1` - [abs](http://arxiv.org/abs/2004.09218v1) - [pdf](http://arxiv.org/pdf/2004.09218v1)

> The question of how an effective and efficient communication system can emerge in a population of agents that need to solve a particular task attracts more and more attention from researchers in many fields, including artificial intelligence, linguistics and statistical physics. A common methodology for studying this question consists of carrying out multi-agent experiments in which a population of agents takes part in a series of scripted and task-oriented communicative interactions, called 'language games'. While each individual language game is typically played by two agents in the population, a large series of games allows the population to converge on a shared communication system. Setting up an experiment in which a rich system for communicating about the real world emerges is a major enterprise, as it requires a variety of software components for running multi-agent experiments, for interacting with sensors and actuators, for conceptualising and interpreting semantic structures, and for mapping between these semantic structures and linguistic utterances. The aim of this paper is twofold. On the one hand, it introduces a high-level robot interface that extends the Babel software system, presenting for the first time a toolkit that provides flexible modules for dealing with each subtask involved in running advanced grounded language game experiments. On the other hand, it provides a practical guide to using the toolkit for implementing such experiments, taking a grounded colour naming game experiment as a didactic example.

</details>

<details>

<summary>2020-04-20 15:31:20 - Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity</summary>

- *Anne Lauscher, Ivan Vulić, Edoardo Maria Ponti, Anna Korhonen, Goran Glavaš*

- `1909.02339v2` - [abs](http://arxiv.org/abs/1909.02339v2) - [pdf](http://arxiv.org/pdf/1909.02339v2)

> Unsupervised pretraining models have been shown to facilitate a wide range of downstream NLP applications. These models, however, retain some of the limitations of traditional static word embeddings. In particular, they encode only the distributional knowledge available in raw text corpora, incorporated through language modeling objectives. In this work, we complement such distributional knowledge with external lexical knowledge, that is, we integrate the discrete knowledge on word-level semantic similarity into pretraining. To this end, we generalize the standard BERT model to a multi-task learning setting where we couple BERT's masked language modeling and next sentence prediction objectives with an auxiliary task of binary word relation classification. Our experiments suggest that our "Lexically Informed" BERT (LIBERT), specialized for the word-level semantic similarity, yields better performance than the lexically blind "vanilla" BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount.

</details>

<details>

<summary>2020-04-20 17:53:46 - Music Gesture for Visual Sound Separation</summary>

- *Chuang Gan, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio Torralba*

- `2004.09476v1` - [abs](http://arxiv.org/abs/2004.09476v1) - [pdf](http://arxiv.org/pdf/2004.09476v1)

> Recent deep learning approaches have achieved impressive performance on visual sound separation tasks. However, these approaches are mostly built on appearance and optical flow like motion feature representations, which exhibit limited abilities to find the correlations between audio signals and visual points, especially when separating multiple instruments of the same types, such as multiple violins in a scene. To address this, we propose "Music Gesture," a keypoint-based structured representation to explicitly model the body and finger movements of musicians when they perform music. We first adopt a context-aware graph network to integrate visual semantic context with body dynamics, and then apply an audio-visual fusion model to associate body movements with the corresponding audio signals. Experimental results on three music performance datasets show: 1) strong improvements upon benchmark metrics for hetero-musical separation tasks (i.e. different instruments); 2) new ability for effective homo-musical separation for piano, flute, and trumpet duets, which to our best knowledge has never been achieved with alternative methods. Project page: http://music-gesture.csail.mit.edu.

</details>

<details>

<summary>2020-04-21 07:38:15 - Inpainting via Generative Adversarial Networks for CMB data analysis</summary>

- *Alireza Vafaei Sadr, Farida Farsian*

- `2004.04177v2` - [abs](http://arxiv.org/abs/2004.04177v2) - [pdf](http://arxiv.org/pdf/2004.04177v2)

> In this work, we propose a new method to inpaint the CMB signal in regions masked out following a point source extraction process. We adopt a modified Generative Adversarial Network (GAN) and compare different combinations of internal (hyper-)parameters and training strategies. We study the performance using a suitable $\mathcal{C}_r$ variable in order to estimate the performance regarding the CMB power spectrum recovery. We consider a test set where one point source is masked out in each sky patch with a 1.83 $\times$ 1.83 squared degree extension, which, in our gridding, corresponds to 64 $\times$ 64 pixels. The GAN is optimized for estimating performance on Planck 2018 total intensity simulations. The training makes the GAN effective in reconstructing a masking corresponding to about 1500 pixels with $1\%$ error down to angular scales corresponding to about 5 arcminutes.

</details>

<details>

<summary>2020-04-21 08:33:29 - Instance Segmentation of Biomedical Images with an Object-aware Embedding Learned with Local Constraints</summary>

- *Long Chen, Martin Strauch, Dorit Merhof*

- `2004.09821v1` - [abs](http://arxiv.org/abs/2004.09821v1) - [pdf](http://arxiv.org/pdf/2004.09821v1)

> Automatic instance segmentation is a problem that occurs in many biomedical applications. State-of-the-art approaches either perform semantic segmentation or refine object bounding boxes obtained from detection methods. Both suffer from crowded objects to varying degrees, merging adjacent objects or suppressing a valid object. In this work, we assign an embedding vector to each pixel through a deep neural network. The network is trained to output embedding vectors of similar directions for pixels from the same object, while adjacent objects are orthogonal in the embedding space, which effectively avoids the fusion of objects in a crowd. Our method yields state-of-the-art results even with a light-weighted backbone network on a cell segmentation (BBBC006 + DSB2018) and a leaf segmentation data set (CVPPP2017). The code and model weights are public available.

</details>

<details>

<summary>2020-04-21 08:55:55 - MixNet: Multi-modality Mix Network for Brain Segmentation</summary>

- *Long Chen, Dorit Merhof*

- `2004.09832v1` - [abs](http://arxiv.org/abs/2004.09832v1) - [pdf](http://arxiv.org/pdf/2004.09832v1)

> Automated brain structure segmentation is important to many clinical quantitative analysis and diagnoses. In this work, we introduce MixNet, a 2D semantic-wise deep convolutional neural network to segment brain structure in multi-modality MRI images. The network is composed of our modified deep residual learning units. In the unit, we replace the traditional convolution layer with the dilated convolutional layer, which avoids the use of pooling layers and deconvolutional layers, reducing the number of network parameters. Final predictions are made by aggregating information from multiple scales and modalities. A pyramid pooling module is used to capture spatial information of the anatomical structures at the output end. In addition, we test three architectures (MixNetv1, MixNetv2 and MixNetv3) which fuse the modalities differently to see the effect on the results. Our network achieves the state-of-the-art performance. MixNetv2 was submitted to the MRBrainS challenge at MICCAI 2018 and won the 3rd place in the 3-label task. On the MRBrainS2018 dataset, which includes subjects with a variety of pathologies, the overall DSC (Dice Coefficient) of 84.7% (gray matter), 87.3% (white matter) and 83.4% (cerebrospinal fluid) were obtained with only 7 subjects as training data.

</details>

<details>

<summary>2020-04-21 10:37:44 - Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling</summary>

- *David Harbecke, Christoph Alt*

- `2004.09890v1` - [abs](http://arxiv.org/abs/2004.09890v1) - [pdf](http://arxiv.org/pdf/2004.09890v1)

> Recently, state-of-the-art NLP models gained an increasing syntactic and semantic understanding of language, and explanation methods are crucial to understand their decisions. Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model's decision. We argue that current occlusion-based methods often produce invalid or syntactically incorrect language data, neglecting the improved abilities of recent NLP models. Furthermore, gradient-based explanation methods disregard the discrete distribution of data in NLP. Thus, we propose OLM: a novel explanation method that combines occlusion and language models to sample valid and syntactically correct replacements with high likelihood, given the context of the original input. We lay out a theoretical foundation that alleviates these weaknesses of other explanation methods in NLP and provide results that underline the importance of considering data likelihood in occlusion-based explanation.

</details>

<details>

<summary>2020-04-21 11:55:17 - PaStaNet: Toward Human Activity Knowledge Engine</summary>

- *Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu*

- `2004.00945v2` - [abs](http://arxiv.org/abs/2004.00945v2) - [pdf](http://arxiv.org/pdf/2004.00945v2)

> Existing image-based activity understanding methods mainly adopt direct mapping, i.e. from image to activity concepts, which may encounter performance bottleneck since the huge gap. In light of this, we propose a new path: infer human part states first and then reason out the activities based on part-level semantics. Human Body Part States (PaSta) are fine-grained action semantic tokens, e.g. <hand, hold, something>, which can compose the activities and help us step toward human activity knowledge engine. To fully utilize the power of PaSta, we build a large-scale knowledge base PaStaNet, which contains 7M+ PaSta annotations. And two corresponding models are proposed: first, we design a model named Activity2Vec to extract PaSta features, which aim to be general representations for various activities. Second, we use a PaSta-based Reasoning method to infer activities. Promoted by PaStaNet, our method achieves significant improvements, e.g. 6.4 and 13.9 mAP on full and one-shot sets of HICO in supervised learning, and 3.2 and 4.2 mAP on V-COCO and images-based AVA in transfer learning. Code and data are available at http://hake-mvig.cn/.

</details>

<details>

<summary>2020-04-21 13:51:03 - Adaptive Interaction Fusion Networks for Fake News Detection</summary>

- *Lianwei Wu, Yuan Rao*

- `2004.10009v1` - [abs](http://arxiv.org/abs/2004.10009v1) - [pdf](http://arxiv.org/pdf/2004.10009v1)

> The majority of existing methods for fake news detection universally focus on learning and fusing various features for detection. However, the learning of various features is independent, which leads to a lack of cross-interaction fusion between features on social media, especially between posts and comments. Generally, in fake news, there are emotional associations and semantic conflicts between posts and comments. How to represent and fuse the cross-interaction between both is a key challenge. In this paper, we propose Adaptive Interaction Fusion Networks (AIFN) to fulfill cross-interaction fusion among features for fake news detection. In AIFN, to discover semantic conflicts, we design gated adaptive interaction networks (GAIN) to capture adaptively similar semantics and conflicting semantics between posts and comments. To establish feature associations, we devise semantic-level fusion self-attention networks (SFSN) to enhance semantic correlations and fusion among features. Extensive experiments on two real-world datasets, i.e., RumourEval and PHEME, demonstrate that AIFN achieves the state-of-the-art performance and boosts accuracy by more than 2.05% and 1.90%, respectively.

</details>

<details>

<summary>2020-04-21 14:13:33 - Leveraging Cognitive Search Patterns to Enhance Automated Natural Language Retrieval Performance</summary>

- *Bhawani Selvaretnam, Mohammed Belkhatir*

- `2004.10035v1` - [abs](http://arxiv.org/abs/2004.10035v1) - [pdf](http://arxiv.org/pdf/2004.10035v1)

> The search of information in large text repositories has been plagued by the so-called document-query vocabulary gap, i.e. the semantic discordance between the contents in the stored document entities on the one hand and the human query on the other hand. Over the past two decades, a significant body of works has advanced technical retrieval prowess while several studies have shed light on issues pertaining to human search behavior. We believe that these efforts should be conjoined, in the sense that automated retrieval systems have to fully emulate human search behavior and thus consider the procedure according to which users incrementally enhance their initial query. To this end, cognitive reformulation patterns that mimic user search behaviour are highlighted and enhancement terms which are statistically collocated with or lexical-semantically related to the original terms adopted in the retrieval process. We formalize the application of these patterns by considering a query conceptual representation and introducing a set of operations allowing to operate modifications on the initial query. A genetic algorithm-based weighting process allows placing emphasis on terms according to their conceptual role-type. An experimental evaluation on real-world datasets against relevance, language, conceptual and knowledge-based models is conducted. We also show, when compared to language and relevance models, a better performance in terms of mean average precision than a word embedding-based model instantiation.

</details>

<details>

<summary>2020-04-21 15:09:48 - TAPAS: Weakly Supervised Table Parsing via Pre-training</summary>

- *Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, Julian Martin Eisenschlos*

- `2004.02349v2` - [abs](http://arxiv.org/abs/2004.02349v2) - [pdf](http://arxiv.org/pdf/2004.02349v2)

> Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.

</details>

<details>

<summary>2020-04-21 15:12:07 - Curriculum Pre-training for End-to-End Speech Translation</summary>

- *Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang*

- `2004.10093v1` - [abs](http://arxiv.org/abs/2004.10093v1) - [pdf](http://arxiv.org/pdf/2004.10093v1)

> End-to-end speech translation poses a heavy burden on the encoder, because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.

</details>

<details>

<summary>2020-04-21 20:46:32 - ParkPredict: Motion and Intent Prediction of Vehicles in Parking Lots</summary>

- *Xu Shen, Ivo Batkovic, Vijay Govindarajan, Paolo Falcone, Trevor Darrell, Francesco Borrelli*

- `2004.10293v1` - [abs](http://arxiv.org/abs/2004.10293v1) - [pdf](http://arxiv.org/pdf/2004.10293v1)

> We investigate the problem of predicting driver behavior in parking lots, an environment which is less structured than typical road networks and features complex, interactive maneuvers in a compact space. Using the CARLA simulator, we develop a parking lot environment and collect a dataset of human parking maneuvers. We then study the impact of model complexity and feature information by comparing a multi-modal Long Short-Term Memory (LSTM) prediction model and a Convolution Neural Network LSTM (CNN-LSTM) to a physics-based Extended Kalman Filter (EKF) baseline. Our results show that 1) intent can be estimated well (roughly 85% top-1 accuracy and nearly 100% top-3 accuracy with the LSTM and CNN-LSTM model); 2) knowledge of the human driver's intended parking spot has a major impact on predicting parking trajectory; and 3) the semantic representation of the environment improves long term predictions.

</details>

<details>

<summary>2020-04-21 23:58:16 - Textual Visual Semantic Dataset for Text Spotting</summary>

- *Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró*

- `2004.10349v1` - [abs](http://arxiv.org/abs/2004.10349v1) - [pdf](http://arxiv.org/pdf/2004.10349v1)

> Text Spotting in the wild consists of detecting and recognizing text appearing in images (e.g. signboards, traffic signals or brands in clothing or objects). This is a challenging problem due to the complexity of the context where texts appear (uneven backgrounds, shading, occlusions, perspective distortions, etc.). Only a few approaches try to exploit the relation between text and its surrounding environment to better recognize text in the scene. In this paper, we propose a visual context dataset for Text Spotting in the wild, where the publicly available dataset COCO-text [Veit et al. 2016] has been extended with information about the scene (such as objects and places appearing in the image) to enable researchers to include semantic relations between texts and scene in their Text Spotting systems, and to offer a common framework for such approaches. For each text in an image, we extract three kinds of context information: objects in the scene, image location label and a textual image description (caption). We use state-of-the-art out-of-the-box available tools to extract this additional information. Since this information has textual form, it can be used to leverage text similarity or semantic relation methods into Text Spotting systems, either as a post-processing or in an end-to-end training strategy. Our data is publicly available at https://git.io/JeZTb.

</details>

<details>

<summary>2020-04-22 03:00:45 - TransEdge: Translating Relation-contextualized Embeddings for Knowledge Graphs</summary>

- *Zequn Sun, Jiacheng Huang, Wei Hu, Muchao Chen, Lingbing Guo, Yuzhong Qu*

- `2004.13579v1` - [abs](http://arxiv.org/abs/2004.13579v1) - [pdf](http://arxiv.org/pdf/2004.13579v1)

> Learning knowledge graph (KG) embeddings has received increasing attention in recent years. Most embedding models in literature interpret relations as linear or bilinear mapping functions to operate on entity embeddings. However, we find that such relation-level modeling cannot capture the diverse relational structures of KGs well. In this paper, we propose a novel edge-centric embedding model TransEdge, which contextualizes relation representations in terms of specific head-tail entity pairs. We refer to such contextualized representations of a relation as edge embeddings and interpret them as translations between entity embeddings. TransEdge achieves promising performance on different prediction tasks. Our experiments on benchmark datasets indicate that it obtains the state-of-the-art results on embedding-based entity alignment. We also show that TransEdge is complementary with conventional entity alignment methods. Moreover, it shows very competitive performance on link prediction.

</details>

<details>

<summary>2020-04-22 03:21:38 - A Data-driven Approach for Constructing Multilayer Network-based Service Ecosystem Models</summary>

- *Mingyi Liu, Zhiying Tu, Xiaofei Xu, Zhongjie Wang*

- `2004.10383v1` - [abs](http://arxiv.org/abs/2004.10383v1) - [pdf](http://arxiv.org/pdf/2004.10383v1)

> Services are flourishing drastically both on the Internet and in the real world. Additionally, services have become much more interconnected to facilitate transboundary business collaboration to create and deliver distinct new values to customers. Various service ecosystems have become a focus in both research and practice. However, due to the lack of widely recognized service ecosystem models and sufficient data for constructing such models, existing studies on service ecosystems are limited to very narrow scope and cannot effectively guide the design, optimization, and evolution of service ecosystems. We propose a Multilayer network-based Service Ecosystem Model, which covers a variety of service-related elements, including stakeholders, channels, functional and nonfunctional features, and domains, and especially, structural and evolutionary relations between them. "Events" are introduced to describe the triggers of service ecosystem evolution. We propose a data-driven approach for constructing MSEM from public media news and external data sources. Qualitative comparison with state-of-the-art models shows that MSEM has a higher coverage degree of fine-grained elements/relations in service ecosystems and richer semantics for higher interpretability. Experiments conducted on real news corpora show that compared with other approaches, our approach can construct large-scale models for real-world service ecosystems with lower cost and higher efficiency.

</details>

<details>

<summary>2020-04-22 09:41:15 - DeepSubQE: Quality estimation for subtitle translations</summary>

- *Prabhakar Gupta, Anil Nelakanti*

- `2004.13828v1` - [abs](http://arxiv.org/abs/2004.13828v1) - [pdf](http://arxiv.org/pdf/2004.13828v1)

> Quality estimation (QE) for tasks involving language data is hard owing to numerous aspects of natural language like variations in paraphrasing, style, grammar, etc. There can be multiple answers with varying levels of acceptability depending on the application at hand. In this work, we look at estimating quality of translations for video subtitles. We show how existing QE methods are inadequate and propose our method DeepSubQE as a system to estimate quality of translation given subtitles data for a pair of languages. We rely on various data augmentation strategies for automated labelling and synthesis for training. We create a hybrid network which learns semantic and syntactic features of bilingual data and compare it with only-LSTM and only-CNN networks. Our proposed network outperforms them by significant margin.

</details>

<details>

<summary>2020-04-22 12:13:25 - Human and Machine Action Prediction Independent of Object Information</summary>

- *Fatemeh Ziaeetabar, Jennifer Pomp, Stefan Pfeiffer, Nadiya El-Sourani, Ricarda I. Schubotz, Minija Tamosiunaite, Florentin Wörgötter*

- `2004.10518v1` - [abs](http://arxiv.org/abs/2004.10518v1) - [pdf](http://arxiv.org/pdf/2004.10518v1)

> Predicting other people's action is key to successful social interactions, enabling us to adjust our own behavior to the consequence of the others' future actions. Studies on action recognition have focused on the importance of individual visual features of objects involved in an action and its context. Humans, however, recognize actions on unknown objects or even when objects are imagined (pantomime). Other cues must thus compensate the lack of recognizable visual object features. Here, we focus on the role of inter-object relations that change during an action. We designed a virtual reality setup and tested recognition speed for 10 different manipulation actions on 50 subjects. All objects were abstracted by emulated cubes so the actions could not be inferred using object information. Instead, subjects had to rely only on the information that comes from the changes in the spatial relations that occur between those cubes. In spite of these constraints, our results show the subjects were able to predict actions in, on average, less than 64% of the action's duration. We employed a computational model -an enriched Semantic Event Chain (eSEC)- incorporating the information of spatial relations, specifically (a) objects' touching/untouching, (b) static spatial relations between objects and (c) dynamic spatial relations between objects. Trained on the same actions as those observed by subjects, the model successfully predicted actions even better than humans. Information theoretical analysis shows that eSECs optimally use individual cues, whereas humans presumably mostly rely on a mixed-cue strategy, which takes longer until recognition. Providing a better cognitive basis of action recognition may, on one hand improve our understanding of related human pathologies and, on the other hand, also help to build robots for conflict-free human-robot cooperation. Our results open new avenues here.

</details>

<details>

<summary>2020-04-22 13:14:48 - Defending Adversarial Attacks via Semantic Feature Manipulation</summary>

- *Shuo Wang, Tianle Chen, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen*

- `2002.02007v2` - [abs](http://arxiv.org/abs/2002.02007v2) - [pdf](http://arxiv.org/pdf/2002.02007v2)

> Machine learning models have demonstrated vulnerability to adversarial attacks, more specifically misclassification of adversarial examples. In this paper, we propose a one-off and attack-agnostic Feature Manipulation (FM)-Defense to detect and purify adversarial examples in an interpretable and efficient manner. The intuition is that the classification result of a normal image is generally resistant to non-significant intrinsic feature changes, e.g., varying thickness of handwritten digits. In contrast, adversarial examples are sensitive to such changes since the perturbation lacks transferability. To enable manipulation of features, a combo-variational autoencoder is applied to learn disentangled latent codes that reveal semantic features. The resistance to classification change over the morphs, derived by varying and reconstructing latent codes, is used to detect suspicious inputs. Further, combo-VAE is enhanced to purify the adversarial examples with good quality by considering both class-shared and class-unique features. We empirically demonstrate the effectiveness of detection and the quality of purified instance. Our experiments on three datasets show that FM-Defense can detect nearly $100\%$ of adversarial examples produced by different state-of-the-art adversarial attacks. It achieves more than $99\%$ overall purification accuracy on the suspicious instances that close the manifold of normal examples.

</details>

<details>

<summary>2020-04-22 15:34:11 - Semantic Entity Enrichment by Leveraging Multilingual Descriptions for Link Prediction</summary>

- *Genet Asefa Gesese, Mehwish Alam, Harald Sack*

- `2004.10640v1` - [abs](http://arxiv.org/abs/2004.10640v1) - [pdf](http://arxiv.org/pdf/2004.10640v1)

> Most Knowledge Graphs (KGs) contain textual descriptions of entities in various natural languages. These descriptions of entities provide valuable information that may not be explicitly represented in the structured part of the KG. Based on this fact, some link prediction methods which make use of the information presented in the textual descriptions of entities have been proposed to learn representations of (monolingual) KGs. However, these methods use entity descriptions in only one language and ignore the fact that descriptions given in different languages may provide complementary information and thereby also additional semantics. In this position paper, the problem of effectively leveraging multilingual entity descriptions for the purpose of link prediction in KGs will be discussed along with potential solutions to the problem.

</details>

<details>

<summary>2020-04-22 16:50:17 - Deep Generation of Coq Lemma Names Using Elaborated Terms</summary>

- *Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric*

- `2004.07761v2` - [abs](http://arxiv.org/abs/2004.07761v2) - [pdf](http://arxiv.org/pdf/2004.07761v2)

> Coding conventions for naming, spacing, and other essentially stylistic properties are necessary for developers to effectively understand, review, and modify source code in large software projects. Consistent conventions in verification projects based on proof assistants, such as Coq, increase in importance as projects grow in size and scope. While conventions can be documented and enforced manually at high cost, emerging approaches automatically learn and suggest idiomatic names in Java-like languages by applying statistical language models on large code corpora. However, due to its powerful language extension facilities and fusion of type checking and computation, Coq is a challenging target for automated learning techniques. We present novel generation models for learning and suggesting lemma names for Coq projects. Our models, based on multi-input neural networks, are the first to leverage syntactic and semantic information from Coq's lexer (tokens in lemma statements), parser (syntax trees), and kernel (elaborated terms) for naming; the key insight is that learning from elaborated terms can substantially boost model performance. We implemented our models in a toolchain, dubbed Roosterize, and applied it on a large corpus of code derived from the Mathematical Components family of projects, known for its stringent coding conventions. Our results show that Roosterize substantially outperforms baselines for suggesting lemma names, highlighting the importance of using multi-input models and elaborated terms.

</details>

<details>

<summary>2020-04-22 17:28:42 - Per-pixel Classification Rebar Exposures in Bridge Eye-inspection</summary>

- *Takato Yasuno, Nakajima Michihiro, Noda Kazuhiro*

- `2004.12805v1` - [abs](http://arxiv.org/abs/2004.12805v1) - [pdf](http://arxiv.org/pdf/2004.12805v1)

> Efficient inspection and accurate diagnosis are required for civil infrastructures with 50 years since completion. Especially in municipalities, the shortage of technical staff and budget constraints on repair expenses have become a critical problem. If we can detect damaged photos automatically per-pixels from the record of the inspection record in addition to the 5-step judgment and countermeasure classification of eye-inspection vision, then it is possible that countermeasure information can be provided more flexibly, whether we need to repair and how large the expose of damage interest. A piece of damage photo is often sparse as long as it is not zoomed around damage, exactly the range where the detection target is photographed, is at most only 1%. Generally speaking, rebar exposure is frequently occurred, and there are many opportunities to judge repair measure. In this paper, we propose three damage detection methods of transfer learning which enables semantic segmentation in an image with low pixels using damaged photos of human eye-inspection. Also, we tried to create a deep convolutional network from scratch with the preprocessing that random crops with rotations are generated. In fact, we show the results applied this method using the 208 rebar exposed images on the 106 real-world bridges. Finally, future tasks of damage detection modeling are mentioned.

</details>

<details>

<summary>2020-04-22 23:42:59 - Qd-tree: Learning Data Layouts for Big Data Analytics</summary>

- *Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar Farooq Minhas, Per-Åke Larson, Donald Kossmann, Rajeev Acharya*

- `2004.10898v1` - [abs](http://arxiv.org/abs/2004.10898v1) - [pdf](http://arxiv.org/pdf/2004.10898v1)

> Corporations today collect data at an unprecedented and accelerating scale, making the need to run queries on large datasets increasingly important. Technologies such as columnar block-based data organization and compression have become standard practice in most commercial database systems. However, the problem of best assigning records to data blocks on storage is still open. For example, today's systems usually partition data by arrival time into row groups, or range/hash partition the data based on selected fields. For a given workload, however, such techniques are unable to optimize for the important metric of the number of blocks accessed by a query. This metric directly relates to the I/O cost, and therefore performance, of most analytical queries. Further, they are unable to exploit additional available storage to drive this metric down further.   In this paper, we propose a new framework called a query-data routing tree, or qd-tree, to address this problem, and propose two algorithms for their construction based on greedy and deep reinforcement learning techniques. Experiments over benchmark and real workloads show that a qd-tree can provide physical speedups of more than an order of magnitude compared to current blocking schemes, and can reach within 2X of the lower bound for data skipping based on selectivity, while providing complete semantic descriptions of created blocks.

</details>

<details>

<summary>2020-04-23 01:02:15 - TCNN: Triple Convolutional Neural Network Models for Retrieval-based Question Answering System in E-commerce</summary>

- *Shuangyong Song, Chao Wang*

- `2004.10919v1` - [abs](http://arxiv.org/abs/2004.10919v1) - [pdf](http://arxiv.org/pdf/2004.10919v1)

> Automatic question-answering (QA) systems have boomed during last few years, and commonly used techniques can be roughly categorized into Information Retrieval (IR)-based and generation-based. A key solution to the IR based models is to retrieve the most similar knowledge entries of a given query from a QA knowledge base, and then rerank those knowledge entries with semantic matching models. In this paper, we aim to improve an IR based e-commerce QA system-AliMe with proposed text matching models, including a basic Triple Convolutional Neural Network (TCNN) model and two Attention-based TCNN (ATCNN) models. Experimental results show their effect.

</details>

<details>

<summary>2020-04-23 03:07:44 - Securing Organization's Data: A Role-Based Authorized Keyword Search Scheme with Efficient Decryption</summary>

- *Nazatul Haque Sultan, Maryline Laurent, Vijay Varadharajan*

- `2004.10952v1` - [abs](http://arxiv.org/abs/2004.10952v1) - [pdf](http://arxiv.org/pdf/2004.10952v1)

> For better data availability and accessibility while ensuring data secrecy, organizations often tend to outsource their encrypted data to the cloud storage servers, thus bringing the challenge of keyword search over encrypted data. In this paper, we propose a novel authorized keyword search scheme using Role-Based Encryption (RBE) technique in a cloud environment. The contributions of this paper are multi-fold. First, it presents a keyword search scheme which enables only the authorized users, having proper assigned roles, to delegate keyword-based data search capabilities over encrypted data to the cloud providers without disclosing any sensitive information. Second, it supports a multi-organization cloud environment, where the users can be associated with more than one organization. Third, the proposed scheme provides efficient decryption, conjunctive keyword search and revocation mechanisms. Fourth, the proposed scheme outsources expensive cryptographic operations in decryption to the cloud in a secure manner. Fifth, we have provided a formal security analysis to prove that the proposed scheme is semantically secure against Chosen Plaintext and Chosen Keyword Attacks. Finally, our performance analysis shows that the proposed scheme is suitable for practical applications.

</details>

<details>

<summary>2020-04-23 06:13:11 - Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning</summary>

- *Byungsoo Ko, Geonmo Gu*

- `2003.02546v3` - [abs](http://arxiv.org/abs/2003.02546v3) - [pdf](http://arxiv.org/pdf/2003.02546v3)

> Learning the distance metric between pairs of samples has been studied for image retrieval and clustering. With the remarkable success of pair-based metric learning losses, recent works have proposed the use of generated synthetic points on metric learning losses for augmentation and generalization. However, these methods require additional generative networks along with the main network, which can lead to a larger model size, slower training speed, and harder optimization. Meanwhile, post-processing techniques, such as query expansion and database augmentation, have proposed the combination of feature points to obtain additional semantic information. In this paper, inspired by query expansion and database augmentation, we propose an augmentation method in an embedding space for pair-based metric learning losses, called embedding expansion. The proposed method generates synthetic points containing augmented information by a combination of feature points and performs hard negative pair mining to learn with the most informative feature representations. Because of its simplicity and flexibility, it can be used for existing metric learning losses without affecting model size, training speed, or optimization difficulty. Finally, the combination of embedding expansion and representative metric learning losses outperforms the state-of-the-art losses and previous sample generation methods in both image retrieval and clustering tasks. The implementation is publicly available.

</details>

<details>

<summary>2020-04-23 08:28:24 - tax2vec: Constructing Interpretable Features from Taxonomies for Short Text Classification</summary>

- *Blaž Škrlj, Matej Martinc, Jan Kralj, Nada Lavrač, Senja Pollak*

- `1902.00438v3` - [abs](http://arxiv.org/abs/1902.00438v3) - [pdf](http://arxiv.org/pdf/1902.00438v3)

> The use of background knowledge is largely unexploited in text classification tasks. This paper explores word taxonomies as means for constructing new semantic features, which may improve the performance and robustness of the learned classifiers. We propose tax2vec, a parallel algorithm for constructing taxonomy-based features, and demonstrate its use on six short text classification problems: prediction of gender, personality type, age, news topics, drug side effects and drug effectiveness. The constructed semantic features, in combination with fast linear classifiers, tested against strong baselines such as hierarchical attention neural networks, achieves comparable classification results on short text documents. The algorithm's performance is also tested in a few-shot learning setting, indicating that the inclusion of semantic features can improve the performance in data-scarce situations. The tax2vec capability to extract corpus-specific semantic keywords is also demonstrated. Finally, we investigate the semantic space of potential features, where we observe a similarity with the well known Zipf's law.

</details>

<details>

<summary>2020-04-23 11:21:30 - Coupling semantic and statistical techniques for dynamically enriching web ontologies</summary>

- *Mohammed Maree, Mohammed Belkhatir*

- `2004.11081v1` - [abs](http://arxiv.org/abs/2004.11081v1) - [pdf](http://arxiv.org/pdf/2004.11081v1)

> With the development of the Semantic Web technology, the use of ontologies to store and retrieve information covering several domains has increased. However, very few ontologies are able to cope with the ever-growing need of frequently updated semantic information or specific user requirements in specialized domains. As a result, a critical issue is related to the unavailability of relational information between concepts, also coined missing background knowledge. One solution to address this issue relies on the manual enrichment of ontologies by domain experts which is however a time consuming and costly process, hence the need for dynamic ontology enrichment. In this paper we present an automatic coupled statistical/semantic framework for dynamically enriching large-scale generic ontologies from the World Wide Web. Using the massive amount of information encoded in texts on the Web as a corpus, missing background knowledge can therefore be discovered through a combination of semantic relatedness measures and pattern acquisition techniques and subsequently exploited. The benefits of our approach are: (i) proposing the dynamic enrichment of large-scale generic ontologies with missing background knowledge, and thus, enabling the reuse of such knowledge, (ii) dealing with the issue of costly ontological manual enrichment by domain experts. Experimental results in a precision-based evaluation setting demonstrate the effectiveness of the proposed techniques.

</details>

<details>

<summary>2020-04-23 11:22:38 - Coupled intrinsic and extrinsic human language resource-based query expansion</summary>

- *Bhawani Selvaretnam, Mohammed Belkhatir*

- `2004.11083v1` - [abs](http://arxiv.org/abs/2004.11083v1) - [pdf](http://arxiv.org/pdf/2004.11083v1)

> Poor information retrieval performance has often been attributed to the query-document vocabulary mismatch problem which is defined as the difficulty for human users to formulate precise natural language queries that are in line with the vocabulary of the documents deemed relevant to a specific search goal. To alleviate this problem, query expansion processes are applied in order to spawn and integrate additional terms to an initial query. This requires accurate identification of main query concepts to ensure the intended search goal is duly emphasized and relevant expansion concepts are extracted and included in the enriched query. Natural language queries have intrinsic linguistic properties such as parts-of-speech labels and grammatical relations which can be utilized in determining the intended search goal. Additionally, extrinsic language-based resources such as ontologies are needed to suggest expansion concepts semantically coherent with the query content. We present here a query expansion framework which capitalizes on both linguistic characteristics of user queries and ontology resources for query constituent encoding, expansion concept extraction and concept weighting. A thorough empirical evaluation on real-world datasets validates our approach against unigram language model, relevance model and a sequential dependence based technique.

</details>

<details>

<summary>2020-04-23 11:39:07 - Natural language technology and query expansion: issues, state-of-the-art and perspectives</summary>

- *Bhawani Selvaretnam, Mohammed Belkhatir*

- `2004.11093v1` - [abs](http://arxiv.org/abs/2004.11093v1) - [pdf](http://arxiv.org/pdf/2004.11093v1)

> The availability of an abundance of knowledge sources has spurred a large amount of effort in the development and enhancement of Information Retrieval techniques. Users information needs are expressed in natural language and successful retrieval is very much dependent on the effective communication of the intended purpose. Natural language queries consist of multiple linguistic features which serve to represent the intended search goal. Linguistic characteristics that cause semantic ambiguity and misinterpretation of queries as well as additional factors such as the lack of familiarity with the search environment affect the users ability to accurately represent their information needs, coined by the concept intention gap. The latter directly affects the relevance of the returned search results which may not be to the users satisfaction and therefore is a major issue impacting the effectiveness of information retrieval systems. Central to our discussion is the identification of the significant constituents that characterize the query intent and their enrichment through the addition of meaningful terms, phrases or even latent representations, either manually or automatically to capture their intended meaning. Specifically, we discuss techniques to achieve the enrichment and in particular those utilizing the information gathered from statistical processing of term dependencies within a document corpus or from external knowledge sources such as ontologies. We lay down the anatomy of a generic linguistic based query expansion framework and propose its module-based decomposition, covering topical issues from query processing, information retrieval, computational linguistics and ontology engineering. For each of the modules we review state-of-the-art solutions in the literature categorized and analyzed under the light of the techniques used.

</details>

<details>

<summary>2020-04-23 13:46:11 - On Adversarial Examples for Biomedical NLP Tasks</summary>

- *Vladimir Araujo, Andres Carvallo, Carlos Aspillaga, Denis Parra*

- `2004.11157v1` - [abs](http://arxiv.org/abs/2004.11157v1) - [pdf](http://arxiv.org/pdf/2004.11157v1)

> The success of pre-trained word embeddings has motivated its use in tasks in the biomedical domain. The BERT language model has shown remarkable results on standard performance metrics in tasks such as Named Entity Recognition (NER) and Semantic Textual Similarity (STS), which has brought significant progress in the field of NLP. However, it is unclear whether these systems work seemingly well in critical domains, such as legal or medical. For that reason, in this work, we propose an adversarial evaluation scheme on two well-known datasets for medical NER and STS. We propose two types of attacks inspired by natural spelling errors and typos made by humans. We also propose another type of attack that uses synonyms of medical terms. Under these adversarial settings, the accuracy of the models drops significantly, and we quantify the extent of this performance loss. We also show that we can significantly improve the robustness of the models by training them with adversarial examples. We hope our work will motivate the use of adversarial examples to evaluate and develop models with increased robustness for medical tasks.

</details>

<details>

<summary>2020-04-23 16:15:31 - Reasoning about Typicality and Probabilities in Preferential Description Logics</summary>

- *Laura Giordano, Valentina Gliozzi, Antonio Lieto, Nicola Olivetti, Gian Luca Pozzato*

- `2004.09507v2` - [abs](http://arxiv.org/abs/2004.09507v2) - [pdf](http://arxiv.org/pdf/2004.09507v2)

> In this work we describe preferential Description Logics of typicality, a nonmonotonic extension of standard Description Logics by means of a typicality operator T allowing to extend a knowledge base with inclusions of the form T(C) v D, whose intuitive meaning is that normally/typically Cs are also Ds. This extension is based on a minimal model semantics corresponding to a notion of rational closure, built upon preferential models. We recall the basic concepts underlying preferential Description Logics. We also present two extensions of the preferential semantics: on the one hand, we consider probabilistic extensions, based on a distributed semantics that is suitable for tackling the problem of commonsense concept combination, on the other hand, we consider other strengthening of the rational closure semantics and construction to avoid the so-called blocking of property inheritance problem.

</details>

<details>

<summary>2020-04-23 17:24:31 - The 1st Agriculture-Vision Challenge: Methods and Results</summary>

- *Mang Tik Chiu, Xingqian Xu, Kai Wang, Jennifer Hobbs, Naira Hovakimyan, Thomas S. Huang, Honghui Shi, Yunchao Wei, Zilong Huang, Alexander Schwing, Robert Brunner, Ivan Dozier, Wyatt Dozier, Karen Ghandilyan, David Wilson, Hyunseong Park, Junhee Kim, Sungho Kim, Qinghui Liu, Michael C. Kampffmeyer, Robert Jenssen, Arnt B. Salberg, Alexandre Barbosa, Rodrigo Trevisan, Bingchen Zhao, Shaozuo Yu, Siwei Yang, Yin Wang, Hao Sheng, Xiao Chen, Jingyi Su, Ram Rajagopal, Andrew Ng, Van Thong Huynh, Soo-Hyung Kim, In-Seop Na, Ujjwal Baid, Shubham Innani, Prasad Dutande, Bhakti Baheti, Sanjay Talbar, Jianyu Tang*

- `2004.09754v2` - [abs](http://arxiv.org/abs/2004.09754v2) - [pdf](http://arxiv.org/pdf/2004.09754v2)

> The first Agriculture-Vision Challenge aims to encourage research in developing novel and effective algorithms for agricultural pattern recognition from aerial images, especially for the semantic segmentation task associated with our challenge dataset. Around 57 participating teams from various countries compete to achieve state-of-the-art in aerial agriculture semantic segmentation. The Agriculture-Vision Challenge Dataset was employed, which comprises of 21,061 aerial and multi-spectral farmland images. This paper provides a summary of notable methods and results in the challenge. Our submission server and leaderboard will continue to open for researchers that are interested in this challenge dataset and task; the link can be found here.

</details>

<details>

<summary>2020-04-23 18:56:04 - MolTrans: Molecular Interaction Transformer for Drug Target Interaction Prediction</summary>

- *Kexin Huang, Cao Xiao, Lucas Glass, Jimeng Sun*

- `2004.11424v1` - [abs](http://arxiv.org/abs/2004.11424v1) - [pdf](http://arxiv.org/pdf/2004.11424v1)

> Drug target interaction (DTI) prediction is a foundational task for in silico drug discovery, which is costly and time-consuming due to the need of experimental search over large drug compound space. Recent years have witnessed promising progress for deep learning in DTI predictions. However, the following challenges are still open: (1) the sole data-driven molecular representation learning approaches ignore the sub-structural nature of DTI, thus produce results that are less accurate and difficult to explain; (2) existing methods focus on limited labeled data while ignoring the value of massive unlabelled molecular data. We propose a Molecular Interaction Transformer (MolTrans) to address these limitations via: (1) knowledge inspired sub-structural pattern mining algorithm and interaction modeling module for more accurate and interpretable DTI prediction; (2) an augmented transformer encoder to better extract and capture the semantic relations among substructures extracted from massive unlabeled biomedical data. We evaluate MolTrans on real world data and show it improved DTI prediction performance compared to state-of-the-art baselines.

</details>

<details>

<summary>2020-04-23 20:24:02 - Social Bias Frames: Reasoning about Social and Power Implications of Language</summary>

- *Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, Yejin Choi*

- `1911.03891v3` - [abs](http://arxiv.org/abs/1911.03891v3) - [pdf](http://arxiv.org/pdf/1911.03891v3)

> Warning: this paper contains content that may be offensive or upsetting.   Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that "we shouldn't lower our standards to hire more women," most listeners will infer the implicature intended by the speaker -- that "women (candidates) are less qualified." Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language.   We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.   We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.

</details>

<details>

<summary>2020-04-24 00:57:05 - What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation</summary>

- *Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, Xiaowei Xu*

- `2004.11500v1` - [abs](http://arxiv.org/abs/2004.11500v1) - [pdf](http://arxiv.org/pdf/2004.11500v1)

> Unsupervised domain adaptation has attracted growing research attention on semantic segmentation. However, 1) most existing models cannot be directly applied into lesions transfer of medical images, due to the diverse appearances of same lesion among different datasets; 2) equal attention has been paid into all semantic representations instead of neglecting irrelevant knowledge, which leads to negative transfer of untransferable knowledge. To address these challenges, we develop a new unsupervised semantic transfer model including two complementary modules (i.e., T_D and T_F ) for endoscopic lesions segmentation, which can alternatively determine where and how to explore transferable domain-invariant knowledge between labeled source lesions dataset (e.g., gastroscope) and unlabeled target diseases dataset (e.g., enteroscopy). Specifically, T_D focuses on where to translate transferable visual information of medical lesions via residual transferability-aware bottleneck, while neglecting untransferable visual characterizations. Furthermore, T_F highlights how to augment transferable semantic features of various lesions and automatically ignore untransferable representations, which explores domain-invariant knowledge and in return improves the performance of T_D. To the end, theoretical analysis and extensive experiments on medical endoscopic dataset and several non-medical public datasets well demonstrate the superiority of our proposed model.

</details>

<details>

<summary>2020-04-24 01:44:57 - On the Importance of Delexicalization for Fact Verification</summary>

- *Sandeep Suntwal, Mithun Paul, Rebecca Sharp, Mihai Surdeanu*

- `1909.09868v2` - [abs](http://arxiv.org/abs/1909.09868v2) - [pdf](http://arxiv.org/pdf/1909.09868v2)

> In this work we aim to understand and estimate the importance that a neural network assigns to various aspects of the data while learning and making predictions. Here we focus on the recognizing textual entailment (RTE) task and its application to fact verification. In this context, the contributions of this work are as follows. We investigate the attention weights a state of the art RTE method assigns to input tokens in the RTE component of fact verification systems, and confirm that most of the weight is assigned to POS tags of nouns (e.g., NN, NNP etc.) or their phrases. To verify that these lexicalized models transfer poorly, we implement a domain transfer experiment where a RTE component is trained on the FEVER data, and tested on the Fake News Challenge (FNC) dataset. As expected, even though this method achieves high accuracy when evaluated in the same domain, the performance in the target domain is poor, marginally above chance.To mitigate this dependence on lexicalized information, we experiment with several strategies for masking out names by replacing them with their semantic category, coupled with a unique identifier to mark that the same or new entities are referenced between claim and evidence. The results show that, while the performance on the FEVER dataset remains at par with that of the model trained on lexicalized data, it improves significantly when tested in the FNC dataset. Thus our experiments demonstrate that our strategy is successful in mitigating the dependency on lexical information.

</details>

<details>

<summary>2020-04-24 02:23:52 - Reinforcing Short-Length Hashing</summary>

- *Xingbo Liu, Xiushan Nie, Qi Dai, Yupan Huang, Yilong Yin*

- `2004.11511v1` - [abs](http://arxiv.org/abs/2004.11511v1) - [pdf](http://arxiv.org/pdf/2004.11511v1)

> Due to the compelling efficiency in retrieval and storage, similarity-preserving hashing has been widely applied to approximate nearest neighbor search in large-scale image retrieval. However, existing methods have poor performance in retrieval using an extremely short-length hash code due to weak ability of classification and poor distribution of hash bit. To address this issue, in this study, we propose a novel reinforcing short-length hashing (RSLH). In this proposed RSLH, mutual reconstruction between the hash representation and semantic labels is performed to preserve the semantic information. Furthermore, to enhance the accuracy of hash representation, a pairwise similarity matrix is designed to make a balance between accuracy and training expenditure on memory. In addition, a parameter boosting strategy is integrated to reinforce the precision with hash bits fusion. Extensive experiments on three large-scale image benchmarks demonstrate the superior performance of RSLH under various short-length hashing scenarios.

</details>

<details>

<summary>2020-04-24 13:18:35 - Predicting Vulnerability In Large Codebases With Deep Code Representation</summary>

- *Anshul Tanwar, Krishna Sundaresan, Parmesh Ashwath, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi*

- `2004.12783v1` - [abs](http://arxiv.org/abs/2004.12783v1) - [pdf](http://arxiv.org/pdf/2004.12783v1)

> Currently, while software engineers write code for various modules, quite often, various types of errors - coding, logic, semantic, and others (most of which are not caught by compilation and other tools) get introduced. Some of these bugs might be found in the later stage of testing, and many times it is reported by customers on production code. Companies have to spend many resources, both money and time in finding and fixing the bugs which would have been avoided if coding was done right. Also, concealed flaws in software can lead to security vulnerabilities that potentially allow attackers to compromise systems and applications. Interestingly, same or similar issues/bugs, which were fixed in the past (although in different modules), tend to get introduced in production code again.   We developed a novel AI-based system which uses the deep representation of Abstract Syntax Tree (AST) created from the source code and also the active feedback loop to identify and alert the potential bugs that could be caused at the time of development itself i.e. as the developer is writing new code (logic and/or function). This tool integrated with IDE as a plugin would work in the background, point out existing similar functions/code-segments and any associated bugs in those functions. The tool would enable the developer to incorporate suggestions right at the time of development, rather than waiting for UT/QA/customer to raise a defect.   We assessed our tool on both open-source code and also on Cisco codebase for C and C++ programing language. Our results confirm that deep representation of source code and the active feedback loop is an assuring approach for predicting security and other vulnerabilities present in the code.

</details>

<details>

<summary>2020-04-24 17:28:24 - CQE in Description Logics Through Instance Indistinguishability (extended version)</summary>

- *Gianluca Cima, Domenico Lembo, Riccardo Rosati, Domenico Fabio Savo*

- `2004.11870v1` - [abs](http://arxiv.org/abs/2004.11870v1) - [pdf](http://arxiv.org/pdf/2004.11870v1)

> We study privacy-preserving query answering in Description Logics (DLs). Specifically, we consider the approach of controlled query evaluation (CQE) based on the notion of instance indistinguishability. We derive data complexity results for query answering over DL-Lite$_{\mathcal{R}}$ ontologies, through a comparison with an alternative, existing confidentiality-preserving approach to CQE. Finally, we identify a semantically well-founded notion of approximated query answering for CQE, and prove that, for DL-Lite$_{\mathcal{R}}$ ontologies, this form of CQE is tractable with respect to data complexity and is first-order rewritable, i.e., it is always reducible to the evaluation of a first-order query over the data instance.

</details>

<details>

<summary>2020-04-24 19:52:40 - The Inception Team at NSURL-2019 Task 8: Semantic Question Similarity in Arabic</summary>

- *Hana Al-Theiabat, Aisha Al-Sadi*

- `2004.11964v1` - [abs](http://arxiv.org/abs/2004.11964v1) - [pdf](http://arxiv.org/pdf/2004.11964v1)

> This paper describes our method for the task of Semantic Question Similarity in Arabic in the workshop on NLP Solutions for Under-Resourced Languages (NSURL). The aim is to build a model that is able to detect similar semantic questions in the Arabic language for the provided dataset. Different methods of determining questions similarity are explored in this work. The proposed models achieved high F1-scores, which range from (88% to 96%). Our official best result is produced from the ensemble model of using a pre-trained multilingual BERT model with different random seeds with 95.924% F1-Score, which ranks the first among nine participants teams.

</details>

<details>

<summary>2020-04-25 00:22:46 - Combining Word Embeddings and N-grams for Unsupervised Document Summarization</summary>

- *Zhuolin Jiang, Manaj Srivastava, Sanjay Krishna, David Akodes, Richard Schwartz*

- `2004.14119v1` - [abs](http://arxiv.org/abs/2004.14119v1) - [pdf](http://arxiv.org/pdf/2004.14119v1)

> Graph-based extractive document summarization relies on the quality of the sentence similarity graph. Bag-of-words or tf-idf based sentence similarity uses exact word matching, but fails to measure the semantic similarity between individual words or to consider the semantic structure of sentences. In order to improve the similarity measure between sentences, we employ off-the-shelf deep embedding features and tf-idf features, and introduce a new text similarity metric. An improved sentence similarity graph is built and used in a submodular objective function for extractive summarization, which consists of a weighted coverage term and a diversity term. A Transformer based compression model is developed for sentence compression to aid in document summarization. Our summarization approach is extractive and unsupervised. Experiments demonstrate that our approach can outperform the tf-idf based approach and achieve state-of-the-art performance on the DUC04 dataset, and comparable performance to the fully supervised learning methods on the CNN/DM and NYT datasets.

</details>

<details>

<summary>2020-04-25 01:25:35 - Analysis & Shortcomings of E-Recruitment Systems: Towards a Semantics-based Approach Addressing Knowledge Incompleteness and Limited Domain Coverage</summary>

- *M. Maree, A. Kmail, M. Belkhatir*

- `2004.12034v1` - [abs](http://arxiv.org/abs/2004.12034v1) - [pdf](http://arxiv.org/pdf/2004.12034v1)

> The rapid development of the Internet has led to introducing new methods for e-recruitment and human resources management. These methods aim to systematically address the limitations of conventional recruitment procedures through incorporating natural language processing tools and semantics-based methods. In this context, for a given job post, applicant resumes (usually uploaded as free-text unstructured documents in different formats such as .pdf, .doc, or .rtf) are matched/screened out using the conventional keyword-based model enriched by additional resources such as occupational categories and semantics-based techniques. Employing these techniques has proved to be effective in reducing the cost, time, and efforts required in traditional recruitment and candidate selection methods. However, the skill gap, i.e. the propensity to precisely detect and extract relevant skills in applicant resumes and job posts, and the hidden semantic dimensions encoded in applicant resumes still form a major obstacle for e-recruitment systems. This is due to the fact that resources exploited by current e-recruitment systems are obtained from generic domain-independent sources, therefore resulting in knowledge incompleteness and the lack of domain coverage. In this paper, we review state-of-the-art e-recruitment approaches and highlight recent advancements in this domain. An e-recruitment framework addressing current shortcomings through the use of multiple cooperative semantic resources, feature extraction techniques and skill relatedness measures is detailed. An instantiation of the proposed framework is proposed and an experimental validation using a real-world recruitment dataset from two employment portals demonstrates the effectiveness of the proposed approach.

</details>

<details>

<summary>2020-04-25 01:43:00 - A Linguistically Driven Framework for Query Expansion via Grammatical Constituent Highlighting and Role-Based Concept Weighting</summary>

- *Bhawani Selvaretnam, Mohammed Belkhatir*

- `2004.13481v1` - [abs](http://arxiv.org/abs/2004.13481v1) - [pdf](http://arxiv.org/pdf/2004.13481v1)

> In this paper, we propose a linguistically-motivated query expansion framework that recognizes and en-codes significant query constituents that characterize query intent in order to improve retrieval performance. Concepts-of-Interest are recognized as the core concepts that represent the gist of the search goal whilst the remaining query constituents which serve to specify the search goal and complete the query structure are classified as descriptive, relational or structural. Acknowledging the need to form semantically-associated base pairs for the purpose of extracting related potential expansion concepts, an algorithm which capitalizes on syntactical dependencies to capture relationships between adjacent and non-adjacent query concepts is proposed. Lastly, a robust weighting scheme that duly emphasizes the importance of query constituents based on their linguistic role within the expanded query is presented. We demonstrate improvements in retrieval effectiveness in terms of increased mean average precision (MAP) garnered by the proposed linguistic-based query expansion framework through experimentation on the TREC ad hoc test collections.

</details>

<details>

<summary>2020-04-25 03:13:03 - Implicit Semantic Data Augmentation for Deep Networks</summary>

- *Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Cheng Wu, Gao Huang*

- `1909.12220v5` - [abs](http://arxiv.org/abs/1909.12220v5) - [pdf](http://arxiv.org/pdf/1909.12220v5)

> In this paper, we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping, translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds. As a consequence, translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently, we first perform an online estimate of the covariance matrix of deep features for each class, which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly, instead of augmenting the samples explicitly, we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set, leading to a highly efficient algorithm. In fact, we show that the proposed ISDA amounts to minimizing a novel robust CE loss, which adds negligible extra computational cost to a normal training procedure. Although being simple, ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet. Code for reproducing our results is available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.

</details>

<details>

<summary>2020-04-25 06:30:13 - Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks</summary>

- *Yuanzhi Li, Colin Wei, Tengyu Ma*

- `1907.04595v2` - [abs](http://arxiv.org/abs/1907.04595v2) - [pdf](http://arxiv.org/pdf/1907.04595v2)

> Stochastic gradient descent with a large initial learning rate is widely used for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes easy-to-generalize, hard-to-fit patterns, it generalizes worse on hard-to-generalize, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on.

</details>

<details>

<summary>2020-04-25 06:48:06 - Reasoning Over Semantic-Level Graph for Fact Checking</summary>

- *Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin*

- `1909.03745v3` - [abs](http://arxiv.org/abs/1909.03745v3) - [pdf](http://arxiv.org/pdf/1909.03745v3)

> Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.

</details>

<details>

<summary>2020-04-25 12:56:07 - Hierarchical Image Classification using Entailment Cone Embeddings</summary>

- *Ankit Dhall, Anastasia Makarova, Octavian Ganea, Dario Pavllo, Michael Greeff, Andreas Krause*

- `2004.03459v2` - [abs](http://arxiv.org/abs/2004.03459v2) - [pdf](http://arxiv.org/pdf/2004.03459v2)

> Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.

</details>

<details>

<summary>2020-04-25 15:39:14 - Algebraic Approach to Directed Rough Sets</summary>

- *Mani A, Sandor Radeleczki*

- `2004.12171v1` - [abs](http://arxiv.org/abs/2004.12171v1) - [pdf](http://arxiv.org/pdf/2004.12171v1)

> In relational approach to general rough sets, ideas of directed relations are supplemented with additional conditions for multiple algebraic approaches in this research paper. The relations are also specialized to representations of general parthood that are upper-directed, reflexive and antisymmetric for a better behaved groupoidal semantics over the set of roughly equivalent objects by the first author. Another distinct algebraic semantics over the set of approximations, and a new knowledge interpretation are also invented in this research by her. Because of minimal conditions imposed on the relations, neighborhood granulations are used in the construction of all approximations (granular and pointwise). Necessary and sufficient conditions for the lattice of local upper approximations to be completely distributive are proved by the second author. These results are related to formal concept analysis. Applications to student centered learning and decision making are also outlined.

</details>

<details>

<summary>2020-04-25 16:37:26 - A Named Entity Based Approach to Model Recipes</summary>

- *Nirav Diwan, Devansh Batra, Ganesh Bagler*

- `2004.12184v1` - [abs](http://arxiv.org/abs/2004.12184v1) - [pdf](http://arxiv.org/pdf/2004.12184v1)

> Traditional cooking recipes follow a structure which can be modelled very well if the rules and semantics of the different sections of the recipe text are analyzed and represented accurately. We propose a structure that can accurately represent the recipe as well as a pipeline to infer the best representation of the recipe in this uniform structure. The Ingredients section in a recipe typically lists down the ingredients required and corresponding attributes such as quantity, temperature, and processing state. This can be modelled by defining these attributes and their values. The physical entities which make up a recipe can be broadly classified into utensils, ingredients and their combinations that are related by cooking techniques. The instruction section lists down a series of events in which a cooking technique or process is applied upon these utensils and ingredients. We model these relationships in the form of tuples. Thus, using a combination of these methods we model cooking recipe in the dataset RecipeDB to show the efficacy of our method. This mined information model can have several applications which include translating recipes between languages, determining similarity between recipes, generation of novel recipes and estimation of the nutritional profile of recipes. For the purpose of recognition of ingredient attributes, we train the Named Entity Relationship (NER) models and analyze the inferences with the help of K-Means clustering. Our model presented with an F1 score of 0.95 across all datasets. We use a similar NER tagging model for labelling cooking techniques (F1 score = 0.88) and utensils (F1 score = 0.90) within the instructions section. Finally, we determine the temporal sequence of relationships between ingredients, utensils and cooking techniques for modeling the instruction steps.

</details>

<details>

<summary>2020-04-25 17:09:56 - Towards Discourse Parsing-inspired Semantic Storytelling</summary>

- *Georg Rehm, Karolina Zaczynska, Julián Moreno-Schneider, Malte Ostendorff, Peter Bourgonje, Maria Berger, Jens Rauenbusch, André Schmidt, Mikka Wild*

- `2004.12190v1` - [abs](http://arxiv.org/abs/2004.12190v1) - [pdf](http://arxiv.org/pdf/2004.12190v1)

> Previous work of ours on Semantic Storytelling uses text analytics procedures including Named Entity Recognition and Event Detection. In this paper, we outline our longer-term vision on Semantic Storytelling and describe the current conceptual and technical approach. In the project that drives our research we develop AI-based technologies that are verified by partners from industry. One long-term goal is the development of an approach for Semantic Storytelling that has broad coverage and that is, furthermore, robust. We provide first results on experiments that involve discourse parsing, applied to a concrete use case, "Explore the Neighbourhood!", which is based on a semi-automatically collected data set with documents about noteworthy people in one of Berlin's districts. Though automatically obtaining annotations for coherence relations from plain text is a non-trivial challenge, our preliminary results are promising. We envision our approach to be combined with additional features (NER, coreference resolution, knowledge graphs

</details>

<details>

<summary>2020-04-25 17:46:31 - SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic Question Answering and Spatial Semantic Lifting</summary>

- *Gengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo Yan, Meilin Shi, Ni Lao*

- `2004.14171v1` - [abs](http://arxiv.org/abs/2004.14171v1) - [pdf](http://arxiv.org/pdf/2004.14171v1)

> Learning knowledge graph (KG) embeddings is an emerging technique for a variety of downstream tasks such as summarization, link prediction, information retrieval, and question answering. However, most existing KG embedding models neglect space and, therefore, do not perform well when applied to (geo)spatial data and tasks. For those models that consider space, most of them primarily rely on some notions of distance. These models suffer from higher computational complexity during training while still losing information beyond the relative distance between entities. In this work, we propose a location-aware KG embedding model called SE-KGE. It directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space. The resulting model is capable of handling different types of spatial reasoning. We also construct a geographic knowledge graph as well as a set of geographic query-answer pairs called DBGeo to evaluate the performance of SE-KGE in comparison to multiple baselines. Evaluation results show that SE-KGE outperforms these baselines on the DBGeo dataset for geographic logic query answering task. This demonstrates the effectiveness of our spatially-explicit model and the importance of considering the scale of different geographic entities. Finally, we introduce a novel downstream task called spatial semantic lifting which links an arbitrary location in the study area to entities in the KG via some relations. Evaluation on DBGeo shows that our model outperforms the baseline by a substantial margin.

</details>

<details>

<summary>2020-04-25 18:14:49 - Explainable Deep CNNs for MRI-Based Diagnosis of Alzheimer's Disease</summary>

- *Eduardo Nigri, Nivio Ziviani, Fabio Cappabianco, Augusto Antunes, Adriano Veloso*

- `2004.12204v1` - [abs](http://arxiv.org/abs/2004.12204v1) - [pdf](http://arxiv.org/pdf/2004.12204v1)

> Deep Convolutional Neural Networks (CNNs) are becoming prominent models for semi-automated diagnosis of Alzheimer's Disease (AD) using brain Magnetic Resonance Imaging (MRI). Although being highly accurate, deep CNN models lack transparency and interpretability, precluding adequate clinical reasoning and not complying with most current regulatory demands. One popular choice for explaining deep image models is occluding regions of the image to isolate their influence on the prediction. However, existing methods for occluding patches of brain scans generate images outside the distribution to which the model was trained for, thus leading to unreliable explanations. In this paper, we propose an alternative explanation method that is specifically designed for the brain scan task. Our method, which we refer to as Swap Test, produces heatmaps that depict the areas of the brain that are most indicative of AD, providing interpretability for the model's decisions in a format understandable to clinicians. Experimental results using an axiomatic evaluation show that the proposed method is more suitable for explaining the diagnosis of AD using MRI while the opposite trend was observed when using a typical occlusion test. Therefore, we believe our method may address the inherent black-box nature of deep neural networks that are capable of diagnosing AD.

</details>

<details>

<summary>2020-04-26 07:07:22 - Mining Implicit Entity Preference from User-Item Interaction Data for Knowledge Graph Completion via Adversarial Learning</summary>

- *Gaole He, Junyi Li, Wayne Xin Zhao, Peiju Liu, Ji-Rong Wen*

- `2003.12718v3` - [abs](http://arxiv.org/abs/2003.12718v3) - [pdf](http://arxiv.org/pdf/2003.12718v3)

> The task of Knowledge Graph Completion (KGC) aims to automatically infer the missing fact information in Knowledge Graph (KG). In this paper, we take a new perspective that aims to leverage rich user-item interaction data (user interaction data for short) for improving the KGC task. Our work is inspired by the observation that many KG entities correspond to online items in application systems. However, the two kinds of data sources have very different intrinsic characteristics, and it is likely to hurt the original performance using simple fusion strategy. To address this challenge, we propose a novel adversarial learning approach by leveraging user interaction data for the KGC task. Our generator is isolated from user interaction data, and serves to improve the performance of the discriminator. The discriminator takes the learned useful information from user interaction data as input, and gradually enhances the evaluation capacity in order to identify the fake samples generated by the generator. To discover implicit entity preference of users, we design an elaborate collaborative learning algorithms based on graph neural networks, which will be jointly optimized with the discriminator. Such an approach is effective to alleviate the issues about data heterogeneity and semantic complexity for the KGC task. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our approach on the KGC task.

</details>

<details>

<summary>2020-04-26 07:43:30 - Challenge Closed-book Science Exam: A Meta-learning Based Question Answering System</summary>

- *Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi*

- `2004.12303v1` - [abs](http://arxiv.org/abs/2004.12303v1) - [pdf](http://arxiv.org/pdf/2004.12303v1)

> Prior work in standardized science exams requires support from large text corpus, such as targeted science corpus fromWikipedia or SimpleWikipedia. However, retrieving knowledge from the large corpus is time-consuming and questions embedded in complex semantic representation may interfere with retrieval. Inspired by the dual process theory in cognitive science, we propose a MetaQA framework, where system 1 is an intuitive meta-classifier and system 2 is a reasoning module. Specifically, our method based on meta-learning method and large language model BERT, which can efficiently solve science problems by learning from related example questions without relying on external knowledge bases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the experimental results show that meta-classifier yields considerable classification performance on emerging question types. The information provided by meta-classifier significantly improves the accuracy of reasoning module from 46.6% to 64.2%, which has a competitive advantage over retrieval-based QA methods.

</details>

<details>

<summary>2020-04-26 09:41:17 - Neural Topic Modeling with Bidirectional Adversarial Training</summary>

- *Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye, Haiyang Xu*

- `2004.12331v1` - [abs](http://arxiv.org/abs/2004.12331v1) - [pdf](http://arxiv.org/pdf/2004.12331v1)

> Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6\% is observed in accuracy.

</details>

<details>

<summary>2020-04-26 09:50:15 - Structure Preserving Compressive Sensing MRI Reconstruction using Generative Adversarial Networks</summary>

- *Puneesh Deora, Bhavya Vasudeva, Saumik Bhattacharya, Pyari Mohan Pradhan*

- `1910.06067v2` - [abs](http://arxiv.org/abs/1910.06067v2) - [pdf](http://arxiv.org/pdf/1910.06067v2)

> Compressive sensing magnetic resonance imaging (CS-MRI) accelerates the acquisition of MR images by breaking the Nyquist sampling limit. In this work, a novel generative adversarial network (GAN) based framework for CS-MRI reconstruction is proposed. Leveraging a combination of patch-based discriminator and structural similarity index based loss, our model focuses on preserving high frequency content as well as fine textural details in the reconstructed image. Dense and residual connections have been incorporated in a U-net based generator architecture to allow easier transfer of information as well as variable network length. We show that our algorithm outperforms state-of-the-art methods in terms of quality of reconstruction and robustness to noise. Also, the reconstruction time, which is of the order of milliseconds, makes it highly suitable for real-time clinical use.

</details>

<details>

<summary>2020-04-26 12:21:17 - Multi-Domain Dialogue Acts and Response Co-Generation</summary>

- *Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu*

- `2004.12363v1` - [abs](http://arxiv.org/abs/2004.12363v1) - [pdf](http://arxiv.org/pdf/2004.12363v1)

> Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently. Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed. We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively. Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.

</details>

<details>

<summary>2020-04-26 13:34:57 - Towards Runtime Verification of Programmable Switches</summary>

- *Apoorv Shukla, Kevin Hudemann, Zsolt Vági, Lily Hügerich, Georgios Smaragdakis, Stefan Schmid, Artur Hecker, Anja Feldmann*

- `2004.10887v2` - [abs](http://arxiv.org/abs/2004.10887v2) - [pdf](http://arxiv.org/pdf/2004.10887v2)

> Is it possible to patch software bugs in P4 programs without human involvement? We show that this is partially possible in many cases due to advances in software testing and the structure of P4 programs. Our insight is that runtime verification can detect bugs, even those that are not detected at compile-time, with machine learning-guided fuzzing. This enables a more automated and real-time localization of bugs in P4 programs using software testing techniques like Tarantula. Once the bug in a P4 program is localized, the faulty code can be patched due to the programmable nature of P4. In addition, platform-dependent bugs can be detected. From P4_14 to P4_16 (latest version), our observation is that as the programmable blocks increase, the patchability of P4 programs increases accordingly. To this end, we design, develop, and evaluate P6 that (a) detects, (b) localizes, and (c) patches bugs in P4 programs with minimal human interaction. P6 tests P4 switch non-intrusively, i.e., requires no modification to the P4 program for detecting and localizing bugs. We used a P6 prototype to detect and patch seven existing bugs in eight publicly available P4 application programs deployed on two different switch platforms: behavioral model (bmv2) and Tofino. Our evaluation shows that P6 significantly outperforms bug detection baselines while generating fewer packets and patches bugs in P4 programs such as switch.p4 without triggering any regressions.

</details>

<details>

<summary>2020-04-26 14:38:11 - Heterogeneous Graph Neural Networks for Extractive Document Summarization</summary>

- *Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang*

- `2004.12393v1` - [abs](http://arxiv.org/abs/2004.12393v1) - [pdf](http://arxiv.org/pdf/2004.12393v1)

> As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HeterSumGraph), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github

</details>

<details>

<summary>2020-04-26 16:29:06 - Towards Multimodal Response Generation with Exemplar Augmentation and Curriculum Optimization</summary>

- *Zeyang Lei, Zekang Li, Jinchao Zhang, Fandong Meng, Yang Feng, Yujiu Yang, Cheng Niu, Jie Zhou*

- `2004.12429v1` - [abs](http://arxiv.org/abs/2004.12429v1) - [pdf](http://arxiv.org/pdf/2004.12429v1)

> Recently, variational auto-encoder (VAE) based approaches have made impressive progress on improving the diversity of generated responses. However, these methods usually suffer the cost of decreased relevance accompanied by diversity improvements. In this paper, we propose a novel multimodal response generation framework with exemplar augmentation and curriculum optimization to enhance relevance and diversity of generated responses. First, unlike existing VAE-based models that usually approximate a simple Gaussian posterior distribution, we present a Gaussian mixture posterior distribution (i.e, multimodal) to further boost response diversity, which helps capture complex semantics of responses. Then, to ensure that relevance does not decrease while diversity increases, we fully exploit similar examples (exemplars) retrieved from the training data into posterior distribution modeling to augment response relevance. Furthermore, to facilitate the convergence of Gaussian mixture prior and posterior distributions, we devise a curriculum optimization strategy to progressively train the model under multiple training criteria from easy to hard. Experimental results on widely used SwitchBoard and DailyDialog datasets demonstrate that our model achieves significant improvements compared to strong baselines in terms of diversity and relevance.

</details>

<details>

<summary>2020-04-26 18:27:59 - Towards Geocoding Spatial Expressions</summary>

- *Hussein S. Al-Olimat, Valerie L. Shalin, Krishnaprasad Thirunarayan, Joy Prakash Sain*

- `1906.04960v2` - [abs](http://arxiv.org/abs/1906.04960v2) - [pdf](http://arxiv.org/pdf/1906.04960v2)

> Imprecise composite location references formed using ad hoc spatial expressions in English text makes the geocoding task challenging for both inference and evaluation. Typically such spatial expressions fill in unestablished areas with new toponyms for finer spatial referents. For example, the spatial extent of the ad hoc spatial expression "north of" or "50 minutes away from" in relation to the toponym "Dayton, OH" refers to an ambiguous, imprecise area, requiring translation from this qualitative representation to a quantitative one with precise semantics using systems such as WGS84. Here we highlight the challenges of geocoding such referents and propose a formal representation that employs background knowledge, semantic approximations and rules, and fuzzy linguistic variables. We also discuss an appropriate evaluation technique for the task that is based on human contextualized and subjective judgment.

</details>

<details>

<summary>2020-04-26 19:25:09 - Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models</summary>

- *Hussein S. Al-Olimat, Krishnaprasad Thirunarayan, Valerie Shalin, Amit Sheth*

- `1708.03105v3` - [abs](http://arxiv.org/abs/1708.03105v3) - [pdf](http://arxiv.org/pdf/1708.03105v3)

> Extracting location names from informal and unstructured social media data requires the identification of referent boundaries and partitioning compound names. Variability, particularly systematic variability in location names (Carroll, 1983), challenges the identification task. Some of this variability can be anticipated as operations within a statistical language model, in this case drawn from gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia. This permits evaluation of an observed n-gram in Twitter targeted text as a legitimate location name variant from the same location-context. Using n-gram statistics and location-related dictionaries, our Location Name Extraction tool (LNEx) handles abbreviations and automatically filters and augments the location names in gazetteers (handling name contractions and auxiliary contents) to help detect the boundaries of multi-word location names and thereby delimit them in texts.   We evaluated our approach on 4,500 event-specific tweets from three targeted streams to compare the performance of LNEx against that of ten state-of-the-art taggers that rely on standard semantic, syntactic and/or orthographic features. LNEx improved the average F-Score by 33-179%, outperforming all taggers. Further, LNEx is capable of stream processing.

</details>

<details>

<summary>2020-04-26 23:17:22 - Probabilistic Bias Mitigation in Word Embeddings</summary>

- *Hailey Joren, David Alvarez-Melis*

- `1910.14497v2` - [abs](http://arxiv.org/abs/1910.14497v2) - [pdf](http://arxiv.org/pdf/1910.14497v2)

> It has been shown that word embeddings derived from large corpora tend to incorporate biases present in their training data. Various methods for mitigating these biases have been proposed, but recent work has demonstrated that these methods hide but fail to truly remove the biases, which can still be observed in word nearest-neighbor statistics. In this work we propose a probabilistic view of word embedding bias. We leverage this framework to present a novel method for mitigating bias which relies on probabilistic observations to yield a more robust bias mitigation algorithm. We demonstrate that this method effectively reduces bias according to three separate measures of bias while maintaining embedding quality across various popular benchmark semantic tasks

</details>

<details>

<summary>2020-04-27 08:34:57 - Interactive Patch Filtering as Debugging Aid</summary>

- *Jingjing Liang, Ruyi Ji, Jiajun Jiang, Yiling Lou, Yingfei Xiong, Gang Huang*

- `2004.08746v2` - [abs](http://arxiv.org/abs/2004.08746v2) - [pdf](http://arxiv.org/pdf/2004.08746v2)

> It is widely recognized that program repair tools need to have a high precision to be useful, i.e., the generated patches need to have a high probability to be correct. However, it is fundamentally difficult to ensure the correctness of the patches, and many tools compromise other aspects of repair performance such as recall for an acceptable precision.   In this paper we ask a question: can a repair tool with a low precision be still useful? To explore this question, we propose an interactive filtering approach to patch review, which filters out incorrect patches by asking questions to the developers. Our intuition is that incorrect patches can still help understand the bug. With proper tool support, the benefit outweighs the cost even if there are many incorrect patches.   We implemented the approach as an Eclipse plugin tool, InPaFer, and evaluated it with a simulated experiment and a user study with 30 developers. The results show that our approach improve the repair performance of developers, with 62.5% more successfully repaired bugs and 25.3% less debugging time in average. In particular, even if the generated patches are all incorrect, the performance of the developers would not be significantly reduced, and could be improved when some patches provide useful information for repairing, such as the faulty location and a partial fix.

</details>

<details>

<summary>2020-04-27 10:52:52 - Semantic Graphs for Generating Deep Questions</summary>

- *Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan*

- `2004.12704v1` - [abs](http://arxiv.org/abs/2004.12704v1) - [pdf](http://arxiv.org/pdf/2004.12704v1)

> This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information of the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework which first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.

</details>

<details>

<summary>2020-04-27 11:48:03 - Unsupervised Domain Adaptation with Multiple Domain Discriminators and Adaptive Self-Training</summary>

- *Teo Spadotto, Marco Toldo, Umberto Michieli, Pietro Zanuttigh*

- `2004.12724v1` - [abs](http://arxiv.org/abs/2004.12724v1) - [pdf](http://arxiv.org/pdf/2004.12724v1)

> Unsupervised Domain Adaptation (UDA) aims at improving the generalization capability of a model trained on a source domain to perform well on a target domain for which no labeled data is available. In this paper, we consider the semantic segmentation of urban scenes and we propose an approach to adapt a deep neural network trained on synthetic data to real scenes addressing the domain shift between the two different data distributions. We introduce a novel UDA framework where a standard supervised loss on labeled synthetic data is supported by an adversarial module and a self-training strategy aiming at aligning the two domain distributions. The adversarial module is driven by a couple of fully convolutional discriminators dealing with different domains: the first discriminates between ground truth and generated maps, while the second between segmentation maps coming from synthetic or real world data. The self-training module exploits the confidence estimated by the discriminators on unlabeled data to select the regions used to reinforce the learning process. Furthermore, the confidence is thresholded with an adaptive mechanism based on the per-class overall confidence. Experimental results prove the effectiveness of the proposed strategy in adapting a segmentation network trained on synthetic datasets like GTA5 and SYNTHIA, to real world datasets like Cityscapes and Mapillary.

</details>

<details>

<summary>2020-04-27 14:13:56 - Station-to-User Transfer Learning: Towards Explainable User Clustering Through Latent Trip Signatures Using Tidal-Regularized Non-Negative Matrix Factorization</summary>

- *Liming Zhang, Andreas Züfle, Dieter Pfoser*

- `2004.12828v1` - [abs](http://arxiv.org/abs/2004.12828v1) - [pdf](http://arxiv.org/pdf/2004.12828v1)

> Urban areas provide us with a treasure trove of available data capturing almost every aspect of a population's life. This work focuses on mobility data and how it will help improve our understanding of urban mobility patterns. Readily available and sizable farecard data captures trips in a public transportation network. However, such data typically lacks temporal modalities and as such the task of inferring trip semantic, station function, and user profile is quite challenging. As existing approaches either focus on station-level or user-level signals, they are prone to overfitting and generate less credible and insightful results. To properly learn such characteristics from trip data, we propose a Collective Learning Framework through Latent Representation, which augments user-level learning with collective patterns learned from station-level signals. This framework uses a novel, so-called Tidal-Regularized Non-negative Matrix Factorization method, which incorporates domain knowledge in the form of temporal passenger flow patterns in generic Non-negative Matrix Factorization. To evaluate our model performance, a user stability test based on the classical Rand Index is introduced as a metric to benchmark different unsupervised learning models. We provide a qualitative analysis of the station functions and user profiles for the Washington D.C. metro and show how our method supports spatiotemporal intra-city mobility exploration.

</details>

<details>

<summary>2020-04-27 20:54:08 - Understanding Convolutional Neural Networks for Text Classification</summary>

- *Alon Jacovi, Oren Sar Shalom, Yoav Goldberg*

- `1809.08037v3` - [abs](http://arxiv.org/abs/1809.08037v3) - [pdf](http://arxiv.org/pdf/1809.08037v3)

> We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions). Code implementation is available online at github.com/sayaendo/interpreting-cnn-for-text.

</details>

<details>

<summary>2020-04-28 02:43:36 - Conversational Word Embedding for Retrieval-Based Dialog System</summary>

- *Wentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang, Guoping Hu*

- `2004.13249v1` - [abs](http://arxiv.org/abs/2004.13249v1) - [pdf](http://arxiv.org/pdf/2004.13249v1)

> Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs $ \left\langle{post, reply} \right\rangle$ to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply. To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level. We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems. The experiment results show that PR-Embedding can improve the quality of the selected response. PR-Embedding source code is available at https://github.com/wtma/PR-Embedding

</details>

<details>

<summary>2020-04-28 03:44:34 - Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models</summary>

- *Shilin He, Xing Wang, Shuming Shi, Michael R. Lyu, Zhaopeng Tu*

- `2004.13270v1` - [abs](http://arxiv.org/abs/2004.13270v1) - [pdf](http://arxiv.org/pdf/2004.13270v1)

> Machine translation (MT) systems translate text between different languages by automatically learning in-depth knowledge of bilingual lexicons, grammar and semantics from the training examples. Although neural machine translation (NMT) has led the field of MT, we have a poor understanding on how and why it works. In this paper, we bridge the gap by assessing the bilingual knowledge learned by NMT models with phrase table -- an interpretable table of bilingual lexicons. We extract the phrase table from the training examples that an NMT model correctly predicts. Extensive experiments on widely-used datasets show that the phrase table is reasonable and consistent against language pairs and random seeds. Equipped with the interpretable phrase table, we find that NMT models learn patterns from simple to complex and distill essential bilingual knowledge from the training examples. We also revisit some advances that potentially affect the learning of bilingual knowledge (e.g., back-translation), and report some interesting findings. We believe this work opens a new angle to interpret NMT with statistic models, and provides empirical supports for recent advances in improving NMT models.

</details>

<details>

<summary>2020-04-28 07:24:43 - Semantics-Aware Inferential Network for Natural Language Understanding</summary>

- *Shuailiang Zhang, Hai Zhao, Junru Zhou*

- `2004.13338v1` - [abs](http://arxiv.org/abs/2004.13338v1) - [pdf](http://arxiv.org/pdf/2004.13338v1)

> For natural language understanding tasks, either machine reading comprehension or natural language inference, both semantics-aware and inference are favorable features of the concerned modeling for better understanding performance. Thus we propose a Semantics-Aware Inferential Network (SAIN) to meet such a motivation. Taking explicit contextualized semantics as a complementary input, the inferential module of SAIN enables a series of reasoning steps over semantic clues through an attention mechanism. By stringing these steps, the inferential network effectively learns to perform iterative reasoning which incorporates both explicit semantics and contextualized representations. In terms of well pre-trained language models as front-end encoder, our model achieves significant improvement on 11 tasks including machine reading comprehension and natural language inference.

</details>

<details>

<summary>2020-04-28 12:16:16 - MuonTrap: Preventing Cross-Domain Spectre-Like Attacks by Capturing Speculative State</summary>

- *Sam Ainsworth, Timothy M. Jones*

- `1911.08384v2` - [abs](http://arxiv.org/abs/1911.08384v2) - [pdf](http://arxiv.org/pdf/1911.08384v2)

> The disclosure of the Spectre speculative-execution attacks in January 2018 has left a severe vulnerability that systems are still struggling with how to patch. The solutions that currently exist tend to have incomplete coverage, perform badly, or have highly undesirable edge cases that cause application domains to break.   MuonTrap allows processors to continue to speculate, avoiding significant reductions in performance, without impacting security. We instead prevent the propagation of any state based on speculative execution, by placing the results of speculative cache accesses into a small, fast L0 filter cache, that is non-inclusive, non-exclusive with the rest of the cache hierarchy. This isolates all parts of the system that can't be quickly cleared on any change in threat domain.   MuonTrap uses these speculative filter caches, which are cleared on context and protection-domain switches, along with a series of extensions to the cache coherence protocol and prefetcher. This renders systems immune to cross-domain information leakage via Spectre and a host of similar attacks based on speculative execution, with low performance impact and few changes to the CPU design.

</details>

<details>

<summary>2020-04-28 15:06:24 - Unifying Neural Learning and Symbolic Reasoning for Spinal Medical Report Generation</summary>

- *Zhongyi Han, Benzheng Wei, Yilong Yin, Shuo Li*

- `2004.13577v1` - [abs](http://arxiv.org/abs/2004.13577v1) - [pdf](http://arxiv.org/pdf/2004.13577v1)

> Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When it employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation as well as show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.

</details>

<details>

<summary>2020-04-28 17:04:19 - LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network</summary>

- *Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin*

- `2004.13659v1` - [abs](http://arxiv.org/abs/2004.13659v1) - [pdf](http://arxiv.org/pdf/2004.13659v1)

> Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc. In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking. It achieves the state-of-the-art performance on TABFACT, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables. This is achieved by a graph module network built upon the Transformer-based architecture. With a textual statement and a table as the input, LogicalFactChecker automatically derives a program (a.k.a. logical form) of the statement in a semantic parsing manner. A heterogeneous graph is then constructed to capture not only the structures of the table and the program, but also the connections between inputs with different modalities. Such a graph reveals the related contexts of each word in the statement, the table and the program. The graph is used to obtain graph-enhanced contextual representations of words in Transformer-based architecture. After that, a program-driven module network is further introduced to exploit the hierarchical structure of the program, where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules. Ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results.

</details>

<details>

<summary>2020-04-28 17:58:14 - Autoencoding Word Representations through Time for Semantic Change Detection</summary>

- *Adam Tsakalidis, Maria Liakata*

- `2004.13703v1` - [abs](http://arxiv.org/abs/2004.13703v1) - [pdf](http://arxiv.org/pdf/2004.13703v1)

> Semantic change detection concerns the task of identifying words whose meaning has changed over time. The current state-of-the-art detects the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time, in a temporally sensitive manner. Through extensive experimentation under various settings with both synthetic and real data we showcase the importance of sequential modelling of word vectors through time for detecting the words whose semantics have changed the most. Finally, we take a step towards comparing different approaches in a quantitative manner, demonstrating that the temporal modelling of word representations yields a clear-cut advantage in performance.

</details>

<details>

<summary>2020-04-28 20:11:18 - Minority Reports Defense: Defending Against Adversarial Patches</summary>

- *Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu, David Wagner*

- `2004.13799v1` - [abs](http://arxiv.org/abs/2004.13799v1) - [pdf](http://arxiv.org/pdf/2004.13799v1)

> Deep learning image classification is vulnerable to adversarial attack, even if the attacker changes just a small patch of the image. We propose a defense against patch attacks based on partially occluding the image around each candidate patch location, so that a few occlusions each completely hide the patch. We demonstrate on CIFAR-10, Fashion MNIST, and MNIST that our defense provides certified security against patch attacks of a certain size.

</details>

<details>

<summary>2020-04-28 20:17:51 - An Unsupervised Semantic Sentence Ranking Scheme for Text Documents</summary>

- *Hao Zhang, Jie Wang*

- `2005.02158v1` - [abs](http://arxiv.org/abs/2005.02158v1) - [pdf](http://arxiv.org/pdf/2005.02158v1)

> This paper presents Semantic SentenceRank (SSR), an unsupervised scheme for automatically ranking sentences in a single document according to their relative importance. In particular, SSR extracts essential words and phrases from a text document, and uses semantic measures to construct, respectively, a semantic phrase graph over phrases and words, and a semantic sentence graph over sentences. It applies two variants of article-structure-biased PageRank to score phrases and words on the first graph and sentences on the second graph. It then combines these scores to generate the final score for each sentence. Finally, SSR solves a multi-objective optimization problem for ranking sentences based on their final scores and topic diversity through semantic subtopic clustering. An implementation of SSR that runs in quadratic time is presented, and it outperforms, on the SummBank benchmarks, each individual judge's ranking and compares favorably with the combined ranking of all judges.

</details>

<details>

<summary>2020-04-29 03:20:22 - Revisiting Round-Trip Translation for Quality Estimation</summary>

- *Jihyung Moon, Hyunchang Cho, Eunjeong L. Park*

- `2004.13937v1` - [abs](http://arxiv.org/abs/2004.13937v1) - [pdf](http://arxiv.org/pdf/2004.13937v1)

> Quality estimation (QE) is the task of automatically evaluating the quality of translations without human-translated references. Calculating BLEU between the input sentence and round-trip translation (RTT) was once considered as a metric for QE, however, it was found to be a poor predictor of translation quality. Recently, various pre-trained language models have made breakthroughs in NLP tasks by providing semantically meaningful word and sentence embeddings. In this paper, we employ semantic embeddings to RTT-based QE. Our method achieves the highest correlations with human judgments, compared to previous WMT 2019 quality estimation metric task submissions. While backward translation models can be a drawback when using RTT, we observe that with semantic-level metrics, RTT-based QE is robust to the choice of the backward translation system. Additionally, the proposed method shows consistent performance for both SMT and NMT forward translation systems, implying the method does not penalize a certain type of model.

</details>

<details>

<summary>2020-04-29 04:29:33 - Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention</summary>

- *Yanzeng Li, Bowen Yu, Mengge Xue, Tingwen Liu*

- `1911.02821v2` - [abs](http://arxiv.org/abs/1911.02821v2) - [pdf](http://arxiv.org/pdf/1911.02821v2)

> Most Chinese pre-trained models take character as the basic unit and learn representation according to character's external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models. Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion. As a result, word and character information are explicitly integrated at the fine-tuning procedure. Experimental results on five Chinese NLP benchmark tasks demonstrate that our model could bring another significant gain over several pre-trained models.

</details>

<details>

<summary>2020-04-29 06:12:03 - Domain Adaptive Transfer Attack (DATA)-based Segmentation Networks for Building Extraction from Aerial Images</summary>

- *Younghwan Na, Jun Hee Kim, Kyungsu Lee, Juhum Park, Jae Youn Hwang, Jihwan P. Choi*

- `2004.11819v2` - [abs](http://arxiv.org/abs/2004.11819v2) - [pdf](http://arxiv.org/pdf/2004.11819v2)

> Semantic segmentation models based on convolutional neural networks (CNNs) have gained much attention in relation to remote sensing and have achieved remarkable performance for the extraction of buildings from high-resolution aerial images. However, the issue of limited generalization for unseen images remains. When there is a domain gap between the training and test datasets, CNN-based segmentation models trained by a training dataset fail to segment buildings for the test dataset. In this paper, we propose segmentation networks based on a domain adaptive transfer attack (DATA) scheme for building extraction from aerial images. The proposed system combines the domain transfer and adversarial attack concepts. Based on the DATA scheme, the distribution of the input images can be shifted to that of the target images while turning images into adversarial examples against a target network. Defending adversarial examples adapted to the target domain can overcome the performance degradation due to the domain gap and increase the robustness of the segmentation model. Cross-dataset experiments and the ablation study are conducted for the three different datasets: the Inria aerial image labeling dataset, the Massachusetts building dataset, and the WHU East Asia dataset. Compared to the performance of the segmentation network without the DATA scheme, the proposed method shows improvements in the overall IoU. Moreover, it is verified that the proposed method outperforms even when compared to feature adaptation (FA) and output space adaptation (OSA).

</details>

<details>

<summary>2020-04-29 10:06:00 - Topic Propagation in Conversational Search</summary>

- *I. Mele, C. I. Muntean, F. M. Nardini, R. Perego, N. Tonellotto, O. Frieder*

- `2004.14054v1` - [abs](http://arxiv.org/abs/2004.14054v1) - [pdf](http://arxiv.org/pdf/2004.14054v1)

> In a conversational context, a user expresses her multi-faceted information need as a sequence of natural-language questions, i.e., utterances. Starting from a given topic, the conversation evolves through user utterances and system replies. The retrieval of documents relevant to a given utterance in a conversation is challenging due to ambiguity of natural language and to the difficulty of detecting possible topic shifts and semantic relationships among utterances. We adopt the 2019 TREC Conversational Assistant Track (CAsT) framework to experiment with a modular architecture performing: (i) topic-aware utterance rewriting, (ii) retrieval of candidate passages for the rewritten utterances, and (iii) neural-based re-ranking of candidate passages. We present a comprehensive experimental evaluation of the architecture assessed in terms of traditional IR metrics at small cutoffs. Experimental results show the effectiveness of our techniques that achieve an improvement up to 0.28 (+93%) for P@1 and 0.19 (+89.9%) for nDCG@3 w.r.t. the CAsT baseline.

</details>

<details>

<summary>2020-04-29 12:03:32 - Informative Scene Decomposition for Crowd Analysis, Comparison and Simulation Guidance</summary>

- *Feixiang He, Yuanhang Xiang, Xi Zhao, He Wang*

- `2004.14107v1` - [abs](http://arxiv.org/abs/2004.14107v1) - [pdf](http://arxiv.org/pdf/2004.14107v1)

> Crowd simulation is a central topic in several fields including graphics. To achieve high-fidelity simulations, data has been increasingly relied upon for analysis and simulation guidance. However, the information in real-world data is often noisy, mixed and unstructured, making it difficult for effective analysis, therefore has not been fully utilized. With the fast-growing volume of crowd data, such a bottleneck needs to be addressed. In this paper, we propose a new framework which comprehensively tackles this problem. It centers at an unsupervised method for analysis. The method takes as input raw and noisy data with highly mixed multi-dimensional (space, time and dynamics) information, and automatically structure it by learning the correlations among these dimensions. The dimensions together with their correlations fully describe the scene semantics which consists of recurring activity patterns in a scene, manifested as space flows with temporal and dynamics profiles. The effectiveness and robustness of the analysis have been tested on datasets with great variations in volume, duration, environment and crowd dynamics. Based on the analysis, new methods for data visualization, simulation evaluation and simulation guidance are also proposed. Together, our framework establishes a highly automated pipeline from raw data to crowd analysis, comparison and simulation guidance. Extensive experiments and evaluations have been conducted to show the flexibility, versatility and intuitiveness of our framework.

</details>

<details>

<summary>2020-04-29 12:18:14 - Analysing Lexical Semantic Change with Contextualised Word Representations</summary>

- *Mario Giulianelli, Marco Del Tredici, Raquel Fernández*

- `2004.14118v1` - [abs](http://arxiv.org/abs/2004.14118v1) - [pdf](http://arxiv.org/pdf/2004.14118v1)

> This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.

</details>

<details>

<summary>2020-04-29 15:25:28 - Exploring the Suitability of Semantic Spaces as Word Association Models for the Extraction of Semantic Relationships</summary>

- *Epaminondas Kapetanios, Vijayan Sugumaran, Anastassia Angelopoulou*

- `2004.14265v1` - [abs](http://arxiv.org/abs/2004.14265v1) - [pdf](http://arxiv.org/pdf/2004.14265v1)

> Given the recent advances and progress in Natural Language Processing (NLP), extraction of semantic relationships has been at the top of the research agenda in the last few years. This work has been mainly motivated by the fact that building knowledge graphs (KG) and bases (KB), as a key ingredient of intelligent applications, is a never-ending challenge, since new knowledge needs to be harvested while old knowledge needs to be revised. Currently, approaches towards relation extraction from text are dominated by neural models practicing some sort of distant (weak) supervision in machine learning from large corpora, with or without consulting external knowledge sources. In this paper, we empirically study and explore the potential of a novel idea of using classical semantic spaces and models, e.g., Word Embedding, generated for extracting word association, in conjunction with relation extraction approaches. The goal is to use these word association models to reinforce current relation extraction approaches. We believe that this is a first attempt of this kind and the results of the study should shed some light on the extent to which these word association models can be used as well as the most promising types of relationships to be considered for extraction.

</details>

<details>

<summary>2020-04-29 18:29:49 - FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation</summary>

- *Matias Tassano, Julie Delon, Thomas Veit*

- `1907.01361v2` - [abs](http://arxiv.org/abs/1907.01361v2) - [pdf](http://arxiv.org/pdf/1907.01361v2)

> In this paper, we propose a state-of-the-art video denoising algorithm based on a convolutional neural network architecture. Until recently, video denoising with neural networks had been a largely under explored domain, and existing methods could not compete with the performance of the best patch-based methods. The approach we introduce in this paper, called FastDVDnet, shows similar or better performance than other state-of-the-art competitors with significantly lower computing times. In contrast to other existing neural network denoisers, our algorithm exhibits several desirable properties such as fast runtimes, and the ability to handle a wide range of noise levels with a single network model. The characteristics of its architecture make it possible to avoid using a costly motion compensation stage while achieving excellent performance. The combination between its denoising performance and lower computational load makes this algorithm attractive for practical denoising applications. We compare our method with different state-of-art algorithms, both visually and with respect to objective quality metrics.

</details>

<details>

<summary>2020-04-29 19:24:15 - A Joint Model for Definition Extraction with Syntactic Connection and Semantic Consistency</summary>

- *Amir Pouran Ben Veyseh, Franck Dernoncourt, Dejing Dou, Thien Huu Nguyen*

- `1911.01678v4` - [abs](http://arxiv.org/abs/1911.01678v4) - [pdf](http://arxiv.org/pdf/1911.01678v4)

> Definition Extraction (DE) is one of the well-known topics in Information Extraction that aims to identify terms and their corresponding definitions in unstructured texts. This task can be formalized either as a sentence classification task (i.e., containing term-definition pairs or not) or a sequential labeling task (i.e., identifying the boundaries of the terms and definitions). The previous works for DE have only focused on one of the two approaches, failing to model the inter-dependencies between the two tasks. In this work, we propose a novel model for DE that simultaneously performs the two tasks in a single framework to benefit from their inter-dependencies. Our model features deep learning architectures to exploit the global structures of the input sentences as well as the semantic consistencies between the terms and the definitions, thereby improving the quality of the representation vectors for DE. Besides the joint inference between sentence classification and sequential labeling, the proposed model is fundamentally different from the prior work for DE in that the prior work has only employed the local structures of the input sentences (i.e., word-to-word relations), and not yet considered the semantic consistencies between terms and definitions. In order to implement these novel ideas, our model presents a multi-task learning framework that employs graph convolutional neural networks and predicts the dependency paths between the terms and the definitions. We also seek to enforce the consistency between the representations of the terms and definitions both globally (i.e., increasing semantic consistency between the representations of the entire sentences and the terms/definitions) and locally (i.e., promoting the similarity between the representations of the terms and the definitions).

</details>

<details>

<summary>2020-04-29 20:12:20 - "The Boating Store Had Its Best Sail Ever": Pronunciation-attentive Contextualized Pun Recognition</summary>

- *Yichao Zhou, Jyun-Yu Jiang, Jieyu Zhao, Kai-Wei Chang, Wei Wang*

- `2004.14457v1` - [abs](http://arxiv.org/abs/2004.14457v1) - [pdf](http://arxiv.org/pdf/2004.14457v1)

> Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR.

</details>

<details>

<summary>2020-04-30 02:53:24 - Dissipative SymODEN: Encoding Hamiltonian Dynamics with Dissipation and Control into Deep Learning</summary>

- *Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty*

- `2002.08860v3` - [abs](http://arxiv.org/abs/2002.08860v3) - [pdf](http://arxiv.org/pdf/2002.08860v3)

> In this work, we introduce Dissipative SymODEN, a deep learning architecture which can infer the dynamics of a physical system with dissipation from observed state trajectories. To improve prediction accuracy while reducing network size, Dissipative SymODEN encodes the port-Hamiltonian dynamics with energy dissipation and external input into the design of its computation graph and learns the dynamics in a structured way. The learned model, by revealing key aspects of the system, such as the inertia, dissipation, and potential energy, paves the way for energy-based controllers.

</details>

<details>

<summary>2020-04-30 04:12:38 - Facet-Aware Evaluation for Extractive Summarization</summary>

- *Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, Jiawei Han*

- `1908.10383v2` - [abs](http://arxiv.org/abs/1908.10383v2) - [pdf](http://arxiv.org/pdf/1908.10383v2)

> Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. Specifically, we treat each sentence in the reference summary as a \textit{facet}, identify the sentences in the document that express the semantics of each facet as \textit{support sentences} of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary. To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods. Data can be found at https://github.com/morningmoni/FAR.

</details>

<details>

<summary>2020-04-30 05:03:50 - On the Merging of Domain-Specific Heterogeneous Ontologies using Wordnet and Web Pattern-based Queries</summary>

- *M. Maree, M. Belkhatir*

- `2005.00158v1` - [abs](http://arxiv.org/abs/2005.00158v1) - [pdf](http://arxiv.org/pdf/2005.00158v1)

> Ontologies form the basic interest in various computer science disciplines such as semantic web, information retrieval, database design, etc. They aim at providing a formal, explicit and shared conceptualization and understanding of common domains between different communities. In addition, they allow for concepts and their constraints of a specific domain to be explicitly defined. However, the distributed nature of ontology development and the differences in viewpoints of the ontology engineers have resulted in the so called "semantic heterogeneity" between ontologies. Semantic heterogeneity constitutes the major obstacle against achieving interoperability between ontologies. To overcome this obstacle, we present a multi-purpose framework which exploits the WordNet generic knowledge base for: i) Discovering and correcting the incorrect semantic relations between the concepts of the ontology in a specific domain. This step is a primary step of ontology merging. ii) Merging domain-specific ontologies through computing semantic relations between their concepts. iii) Handling the issue of missing concepts in WordNet through the acquisition of statistical information on the Web. And iv) Enriching WordNet with these missing concepts. An experimental instantiation of the framework and comparisons with state-of-the-art syntactic and semantic-based systems validate our proposal.

</details>

<details>

<summary>2020-04-30 10:04:12 - Normalizing Compositional Structures Across Graphbanks</summary>

- *Lucia Donatelli, Jonas Groschwitz, Alexander Koller, Matthias Lindemann, Pia Weißenhorn*

- `2004.14236v2` - [abs](http://arxiv.org/abs/2004.14236v2) - [pdf](http://arxiv.org/pdf/2004.14236v2)

> The emergence of a variety of graph-based meaning representations (MRs) has sparked an important conversation about how to adequately represent semantic structure. These MRs exhibit structural differences that reflect different theoretical and design considerations, presenting challenges to uniform linguistic analysis and cross-framework semantic parsing. Here, we ask the question of which design differences between MRs are meaningful and semantically-rooted, and which are superficial. We present a methodology for normalizing discrepancies between MRs at the compositional level (Lindemann et al., 2019), finding that we can normalize the majority of divergent phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, demonstrating the usefulness of careful MR design analysis and comparison.

</details>

<details>

<summary>2020-04-30 11:19:04 - Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders</summary>

- *Yanbin Zhao, Lu Chen, Zhi Chen, Kai Yu*

- `2004.14693v1` - [abs](http://arxiv.org/abs/2004.14693v1) - [pdf](http://arxiv.org/pdf/2004.14693v1)

> Text simplification (TS) rephrases long sentences into simplified variants while preserving inherent semantics. Traditional sequence-to-sequence models heavily rely on the quantity and quality of parallel sentences, which limits their applicability in different languages and domains. This work investigates how to leverage large amounts of unpaired corpora in TS task. We adopt the back-translation architecture in unsupervised machine translation (NMT), including denoising autoencoders for language modeling and automatic generation of parallel data by iterative back-translation. However, it is non-trivial to generate appropriate complex-simple pair if we directly treat the set of simple and complex corpora as two different languages, since the two types of sentences are quite similar and it is hard for the model to capture the characteristics in different types of sentences. To tackle this problem, we propose asymmetric denoising methods for sentences with separate complexity. When modeling simple and complex sentences with autoencoders, we introduce different types of noise into the training process. Such a method can significantly improve the simplification performance. Our model can be trained in both unsupervised and semi-supervised manner. Automatic and human evaluations show that our unsupervised model outperforms the previous systems, and with limited supervision, our model can perform competitively with multiple state-of-the-art simplification systems.

</details>

<details>

<summary>2020-04-30 11:33:36 - Dual Supervised Learning for Natural Language Understanding and Generation</summary>

- *Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen*

- `1905.06196v4` - [abs](http://arxiv.org/abs/1905.06196v4) - [pdf](http://arxiv.org/pdf/1905.06196v4)

> Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP field. Natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics. However, such dual relationship has not been investigated in the literature. This paper proposes a new learning framework for language understanding and generation on top of dual supervised learning, providing a way to exploit the duality. The preliminary experiments show that the proposed approach boosts the performance for both tasks.

</details>

<details>

<summary>2020-04-30 12:02:33 - Towards Unsupervised Language Understanding and Generation by Joint Dual Learning</summary>

- *Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen*

- `2004.14710v1` - [abs](http://arxiv.org/abs/2004.14710v1) - [pdf](http://arxiv.org/pdf/2004.14710v1)

> In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner, instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG.

</details>

<details>

<summary>2020-04-30 14:08:10 - A Novel Perspective to Zero-shot Learning: Towards an Alignment of Manifold Structures via Semantic Feature Expansion</summary>

- *Jingcai Guo, Song Guo*

- `2004.14795v1` - [abs](http://arxiv.org/abs/2004.14795v1) - [pdf](http://arxiv.org/pdf/2004.14795v1)

> Zero-shot learning aims at recognizing unseen classes (no training example) with knowledge transferred from seen classes. This is typically achieved by exploiting a semantic feature space shared by both seen and unseen classes, i.e., attribute or word vector, as the bridge. One common practice in zero-shot learning is to train a projection between the visual and semantic feature spaces with labeled seen classes examples. When inferring, this learned projection is applied to unseen classes and recognizes the class labels by some metrics. However, the visual and semantic feature spaces are mutually independent and have quite different manifold structures. Under such a paradigm, most existing methods easily suffer from the domain shift problem and weaken the performance of zero-shot recognition. To address this issue, we propose a novel model called AMS-SFE. It considers the alignment of manifold structures by semantic feature expansion. Specifically, we build upon an autoencoder-based model to expand the semantic features from the visual inputs. Additionally, the expansion is jointly guided by an embedded manifold extracted from the visual feature space of the data. Our model is the first attempt to align both feature spaces by expanding semantic features and derives two benefits: first, we expand some auxiliary features that enhance the semantic feature space; second and more importantly, we implicitly align the manifold structures between the visual and semantic feature spaces; thus, the projection can be better trained and mitigate the domain shift problem. Extensive experiments show significant performance improvement, which verifies the effectiveness of our model.

</details>

<details>

<summary>2020-04-30 14:25:58 - Reinforcement learning of minimalist grammars</summary>

- *Peter beim Graben, Ronald Römer, Werner Meyer, Markus Huber, Matthias Wolff*

- `2005.00359v1` - [abs](http://arxiv.org/abs/2005.00359v1) - [pdf](http://arxiv.org/pdf/2005.00359v1)

> Speech-controlled user interfaces facilitate the operation of devices and household functions to laymen. State-of-the-art language technology scans the acoustically analyzed speech signal for relevant keywords that are subsequently inserted into semantic slots to interpret the user's intent. In order to develop proper cognitive information and communication technologies, simple slot-filling should be replaced by utterance meaning transducers (UMT) that are based on semantic parsers and a mental lexicon, comprising syntactic, phonetic and semantic features of the language under consideration. This lexicon must be acquired by a cognitive agent during interaction with its users. We outline a reinforcement learning algorithm for the acquisition of syntax and semantics of English utterances, based on minimalist grammar (MG), a recent computational implementation of generative linguistics. English declarative sentences are presented to the agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as formulas of predicate logic. Since MG codifies universal linguistic competence through inference rules, thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach unifies generative grammar and reinforcement learning, hence potentially resolving the still pending Chomsky-Skinner controversy.

</details>

<details>

<summary>2020-04-30 14:55:09 - Knowledge Graph Embeddings and Explainable AI</summary>

- *Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo Palmonari, Pasquale Minervini*

- `2004.14843v1` - [abs](http://arxiv.org/abs/2004.14843v1) - [pdf](http://arxiv.org/pdf/2004.14843v1)

> Knowledge graph embeddings are now a widely adopted approach to knowledge representation in which entities and relationships are embedded in vector spaces. In this chapter, we introduce the reader to the concept of knowledge graph embeddings by explaining what they are, how they can be generated and how they can be evaluated. We summarize the state-of-the-art in this field by describing the approaches that have been introduced to represent knowledge in the vector space. In relation to knowledge representation, we consider the problem of explainability, and discuss models and methods for explaining predictions obtained via knowledge graph embeddings.

</details>

<details>

<summary>2020-04-30 16:35:01 - Perceptual reasoning based solution methodology for linguistic optimization problems</summary>

- *Prashant K Gupta, Pranab K. Muhuri*

- `2004.14933v1` - [abs](http://arxiv.org/abs/2004.14933v1) - [pdf](http://arxiv.org/pdf/2004.14933v1)

> Decision making in real-life scenarios may often be modeled as an optimization problem. It requires the consideration of various attributes like human preferences and thinking, which constrain achieving the optimal value of the problem objectives. The value of the objectives may be maximized or minimized, depending on the situation. Numerous times, the values of these problem parameters are in linguistic form, as human beings naturally understand and express themselves using words. These problems are therefore termed as linguistic optimization problems (LOPs), and are of two types, namely single objective linguistic optimization problems (SOLOPs) and multi-objective linguistic optimization problems (MOLOPs). In these LOPs, the value of the objective function(s) may not be known at all points of the decision space, and therefore, the objective function(s) as well as problem constraints are linked by the if-then rules. Tsukamoto inference method has been used to solve these LOPs; however, it suffers from drawbacks. As, the use of linguistic information inevitably calls for the utilization of computing with words (CWW), and therefore, 2-tuple linguistic model based solution methodologies were proposed for LOPs. However, we found that 2-tuple linguistic model based solution methodologies represent the semantics of the linguistic information using a combination of type-1 fuzzy sets and ordinal term sets. As, the semantics of linguistic information are best modeled using the interval type-2 fuzzy sets, hence we propose solution methodologies for LOPs based on CWW approach of perceptual computing, in this paper. The perceptual computing based solution methodologies use a novel design of CWW engine, called the perceptual reasoning (PR). PR in the current form is suitable for solving SOLOPs and, hence, we have also extended it to the MOLOPs.

</details>

<details>

<summary>2020-04-30 17:04:49 - Parallel processor scheduling: formulation as multi-objective linguistic optimization and solution using Perceptual Reasoning based methodology</summary>

- *Prashant K Gupta, Pranab K. Muhuri*

- `2004.14955v1` - [abs](http://arxiv.org/abs/2004.14955v1) - [pdf](http://arxiv.org/pdf/2004.14955v1)

> In the era of Industry 4.0, the focus is on the minimization of human element and maximizing the automation in almost all the industrial and manufacturing establishments. These establishments contain numerous processing systems, which can execute a number of tasks, in parallel with minimum number of human beings. This parallel execution of tasks is done in accordance to a scheduling policy. However, the minimization of human element beyond a certain point is difficult. In fact, the expertise and experience of a group of humans, called the experts, becomes imminent to design a fruitful scheduling policy. The aim of the scheduling policy is to achieve the optimal value of an objective, like production time, cost, etc. In real-life situations, there are more often than not, multiple objectives in any parallel processing scenario. Furthermore, the experts generally provide their opinions, about various scheduling criteria (pertaining to the scheduling policies) in linguistic terms or words. Word semantics are best modeled using fuzzy sets (FSs). Thus, all these factors have motivated us to model the parallel processing scenario as a multi-objective linguistic optimization problem (MOLOP) and use the novel perceptual reasoning (PR) based methodology for solving it. We have also compared the results of the PR based solution methodology with those obtained from the 2-tuple based solution methodology. PR based solution methodology offers three main advantages viz., it generates unique recommendations, here the linguistic recommendations match a codebook word, and also the word model comes before the word. 2-tuple based solution methodology fails to give all these advantages. Thus, we feel that our work is novel and will provide directions for the future research.

</details>

<details>

<summary>2020-04-30 17:09:51 - Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies</summary>

- *Maryam Aminian, Mohammad Sadegh Rasooli, Mona Diab*

- `2004.14961v1` - [abs](http://arxiv.org/abs/2004.14961v1) - [pdf](http://arxiv.org/pdf/2004.14961v1)

> We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with an annotation projection method. We transfer supervised semantic dependency parse annotations from a rich-resource language to a low-resource language through parallel data, and train a semantic parser on projected data. We make use of supervised syntactic parsing as an auxiliary task in a multitask learning framework, and show that with different multitask learning settings, we consistently improve over the single-task baseline. In the setting in which English is the source, and Czech is the target language, our best multitask model improves the labeled F1 score over the single-task baseline by 1.8 in the in-domain SemEval data (Oepen et al., 2015), as well as 2.5 in the out-of-domain test set. Moreover, we observe that syntactic and semantic dependency direction match is an important factor in improving the results.

</details>

<details>

<summary>2020-04-30 17:45:16 - A Matter of Framing: The Impact of Linguistic Formalism on Probing Results</summary>

- *Ilia Kuznetsov, Iryna Gurevych*

- `2004.14999v1` - [abs](http://arxiv.org/abs/2004.14999v1) - [pdf](http://arxiv.org/pdf/2004.14999v1)

> Deep pre-trained contextualized encoders like BERT (Delvin et al., 2019) demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.

</details>

<details>

<summary>2020-04-30 17:58:07 - GCN-RL Circuit Designer: Transferable Transistor Sizing with Graph Neural Networks and Reinforcement Learning</summary>

- *Hanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao Shen, Nan Sun, Hae-Seung Lee, Song Han*

- `2005.00406v1` - [abs](http://arxiv.org/abs/2005.00406v1) - [pdf](http://arxiv.org/pdf/2005.00406v1)

> Automatic transistor sizing is a challenging problem in circuit design due to the large design space, complex performance trade-offs, and fast technological advancements. Although there has been plenty of work on transistor sizing targeting on one circuit, limited research has been done on transferring the knowledge from one circuit to another to reduce the re-design overhead. In this paper, we present GCN-RL Circuit Designer, leveraging reinforcement learning (RL) to transfer the knowledge between different technology nodes and topologies. Moreover, inspired by the simple fact that circuit is a graph, we learn on the circuit topology representation with graph convolutional neural networks (GCN). The GCN-RL agent extracts features of the topology graph whose vertices are transistors, edges are wires. Our learning-based optimization consistently achieves the highest Figures of Merit (FoM) on four different circuits compared with conventional black-box optimization methods (Bayesian Optimization, Evolutionary Algorithms), random search, and human expert designs. Experiments on transfer learning between five technology nodes and two circuit topologies demonstrate that RL with transfer learning can achieve much higher FoMs than methods without knowledge transfer. Our transferable optimization method makes transistor sizing and design porting more effective and efficient.

</details>

<details>

<summary>2020-04-30 18:39:25 - Context based Text-generation using LSTM networks</summary>

- *Sivasurya Santhanam*

- `2005.00048v1` - [abs](http://arxiv.org/abs/2005.00048v1) - [pdf](http://arxiv.org/pdf/2005.00048v1)

> Long short-term memory(LSTM) units on sequence-based models are being used in translation, question-answering systems, classification tasks due to their capability of learning long-term dependencies. In Natural language generation, LSTM networks are providing impressive results on text generation models by learning language models with grammatically stable syntaxes. But the downside is that the network does not learn about the context. The network only learns the input-output function and generates text given a set of input words irrespective of pragmatics. As the model is trained without any such context, there is no semantic consistency among the generated sentences. The proposed model is trained to generate text for a given set of input words along with a context vector. A context vector is similar to a paragraph vector that grasps the semantic meaning(context) of the sentence. Several methods of extracting the context vectors are proposed in this work. While training a language model, in addition to the input-output sequences, context vectors are also trained along with the inputs. Due to this structure, the model learns the relation among the input words, context vector and the target word. Given a set of context terms, a well trained model will generate text around the provided context. Based on the nature of computing context vectors, the model has been tried out with two variations (word importance and word clustering). In the word clustering method, the suitable embeddings among various domains are also explored. The results are evaluated based on the semantic closeness of the generated text to the given context.

</details>

<details>

<summary>2020-04-30 23:01:12 - Contextual Text Style Transfer</summary>

- *Yu Cheng, Zhe Gan, Yizhe Zhang, Oussama Elachqar, Dianqi Li, Jingjing Liu*

- `2005.00136v1` - [abs](http://arxiv.org/abs/2005.00136v1) - [pdf](http://arxiv.org/pdf/2005.00136v1)

> We introduce a new task, Contextual Text Style Transfer - translating a sentence into a desired style with its surrounding context taken into account. This brings two key challenges to existing style transfer approaches: ($i$) how to preserve the semantic meaning of target sentence and its consistency with surrounding context during transfer; ($ii$) how to train a robust model with limited labeled data accompanied with context. To realize high-quality style transfer with natural context preservation, we propose a Context-Aware Style Transfer (CAST) model, which uses two separate encoders for each input sentence and its surrounding context. A classifier is further trained to ensure contextual consistency of the generated sentence. To compensate for the lack of parallel data, additional self-reconstruction and back-translation losses are introduced to leverage non-parallel data in a semi-supervised fashion. Two new benchmarks, Enron-Context and Reddit-Context, are introduced for formality and offensiveness style transfer. Experimental results on these datasets demonstrate the effectiveness of the proposed CAST model over state-of-the-art methods across style accuracy, content preservation and contextual consistency metrics.

</details>


## 2020-05

<details>

<summary>2020-05-01 21:23:20 - Multi-Dimensional Gender Bias Classification</summary>

- *Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, Adina Williams*

- `2005.00614v1` - [abs](http://arxiv.org/abs/2005.00614v1) - [pdf](http://arxiv.org/pdf/2005.00614v1)

> Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.

</details>

<details>

<summary>2020-05-01 21:29:02 - RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</summary>

- *Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, Andrew Markham*

- `1911.11236v3` - [abs](http://arxiv.org/abs/1911.11236v3) - [pdf](http://arxiv.org/pdf/1911.11236v3)

> We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.

</details>

<details>

<summary>2020-05-01 22:04:58 - From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers</summary>

- *Anne Lauscher, Vinit Ravishankar, Ivan Vulić, Goran Glavaš*

- `2005.00633v1` - [abs](http://arxiv.org/abs/2005.00633v1) - [pdf](http://arxiv.org/pdf/2005.00633v1)

> Massively multilingual transformers pretrained with language modeling objectives (e.g., mBERT, XLM-R) have become a de facto default transfer paradigm for zero-shot cross-lingual transfer in NLP, offering unmatched transfer performance. Current downstream evaluations, however, verify their efficacy predominantly in transfer settings involving languages with sufficient amounts of pretraining data, and with lexically and typologically close languages. In this work, we analyze their limitations and show that cross-lingual transfer via massively multilingual transformers, much like transfer via cross-lingual word embeddings, is substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER), as well as two high-level semantic tasks (NLI, QA), empirically correlate transfer performance with linguistic similarity between the source and target languages, but also with the size of pretraining corpora of target languages. We also demonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e., fine-tuning on a few target-language instances after fine-tuning in the source) across the board. This suggests that additional research efforts should be invested to reach beyond the limiting zero-shot conditions.

</details>

<details>

<summary>2020-05-01 22:05:28 - Deciding Differential Privacy for Programs with Finite Inputs and Outputs</summary>

- *Gilles Barthe, Rohit Chadha, Vishal Jagannath, A. Prasad Sistla, Mahesh Viswanathan*

- `1910.04137v2` - [abs](http://arxiv.org/abs/1910.04137v2) - [pdf](http://arxiv.org/pdf/1910.04137v2)

> Differential privacy is a de facto standard for statistical computations over databases that contain private data. The strength of differential privacy lies in a rigorous mathematical definition that guarantees individual privacy and yet allows for accurate statistical results. Thanks to its mathematical definition, differential privacy is also a natural target for formal analysis. A broad line of work uses logical methods for proving privacy. However, these methods are not complete, and only partially automated. A recent and complementary line of work uses statistical methods for finding privacy violations. However, the methods only provide statistical guarantees (but no proofs).   We propose the first decision procedure for checking the differential privacy of a non-trivial class of probabilistic computations. Our procedure takes as input a program P parametrized by a privacy budget $\epsilon$, and either proves differential privacy for all possible values of $\epsilon$ or generates a counterexample. In addition, our procedure applies both to $\epsilon$-differential privacy and $(\epsilon,\delta)$-differential privacy. Technically, the decision procedure is based on a novel and judicious encoding of the semantics of programs in our class into a decidable fragment of the first-order theory of the reals with exponentiation. We implement our procedure and use it for (dis)proving privacy bounds for many well-known examples, including randomized response, histogram, report noisy max and sparse vector.

</details>

<details>

<summary>2020-05-01 23:05:55 - Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic Parsing</summary>

- *Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Minjoon Seo*

- `2005.00644v1` - [abs](http://arxiv.org/abs/2005.00644v1) - [pdf](http://arxiv.org/pdf/2005.00644v1)

> Deep learning approaches to semantic parsing require a large amount of labeled data, but annotating complex logical forms is costly. Here, we propose Syntactic Question Abstraction and Retrieval (SQAR), a method to build a neural semantic parser that translates a natural language (NL) query to a SQL logical form (LF) with less than 1,000 annotated examples. SQAR first retrieves a logical pattern from the train data by computing the similarity between NL queries and then grounds a lexical information on the retrieved pattern in order to generate the final LF. We validate SQAR by training models using various small subsets of WikiSQL train data achieving up to 4.9% higher LF accuracy compared to the previous state-of-the-art models on WikiSQL test set. We also show that by using query-similarity to retrieve logical pattern, SQAR can leverage a paraphrasing dataset achieving up to 5.9% higher LF accuracy compared to the case where SQAR is trained by using only WikiSQL data. In contrast to a simple pattern classification approach, SQAR can generate unseen logical patterns upon the addition of new examples without re-training the model. We also discuss an ideal way to create cost efficient and robust train datasets when the data distribution can be approximated under a data-hungry setting.

</details>

<details>

<summary>2020-05-01 23:50:37 - Jacks of All Trades, Masters Of None: Addressing Distributional Shift and Obtrusiveness via Transparent Patch Attacks</summary>

- *Neil Fendley, Max Lennon, I-Jeng Wang, Philippe Burlina, Nathan Drenkow*

- `2005.00656v1` - [abs](http://arxiv.org/abs/2005.00656v1) - [pdf](http://arxiv.org/pdf/2005.00656v1)

> We focus on the development of effective adversarial patch attacks and -- for the first time -- jointly address the antagonistic objectives of attack success and obtrusiveness via the design of novel semi-transparent patches. This work is motivated by our pursuit of a systematic performance analysis of patch attack robustness with regard to geometric transformations. Specifically, we first elucidate a) key factors underpinning patch attack success and b) the impact of distributional shift between training and testing/deployment when cast under the Expectation over Transformation (EoT) formalism. By focusing our analysis on three principal classes of transformations (rotation, scale, and location), our findings provide quantifiable insights into the design of effective patch attacks and demonstrate that scale, among all factors, significantly impacts patch attack success. Working from these findings, we then focus on addressing how to overcome the principal limitations of scale for the deployment of attacks in real physical settings: namely the obtrusiveness of large patches. Our strategy is to turn to the novel design of irregularly-shaped, semi-transparent partial patches which we construct via a new optimization process that jointly addresses the antagonistic goals of mitigating obtrusiveness and maximizing effectiveness. Our study -- we hope -- will help encourage more focus in the community on the issues of obtrusiveness, scale, and success in patch attacks.

</details>

<details>

<summary>2020-05-02 00:08:42 - GenericsKB: A Knowledge Base of Generic Statements</summary>

- *Sumithra Bhakthavatsalam, Chloe Anastasiades, Peter Clark*

- `2005.00660v1` - [abs](http://arxiv.org/abs/2005.00660v1) - [pdf](http://arxiv.org/pdf/2005.00660v1)

> We present a new resource for the NLP community, namely a large (3.5M+ sentence) knowledge base of *generic statements*, e.g., "Trees remove carbon dioxide from the atmosphere", collected from multiple corpora. This is the first large resource to contain *naturally occurring* generic sentences, as opposed to extracted or crowdsourced triples, and thus is rich in high-quality, general, semantically complete statements. All GenericsKB sentences are annotated with their topical term, surrounding context (sentences), and a (learned) confidence. We also release GenericsKB-Best (1M+ sentences), containing the best-quality generics in GenericsKB augmented with selected, synthesized generics from WordNet and ConceptNet. In tests on two existing datasets requiring multihop reasoning (OBQA and QASC), we find using GenericsKB can result in higher scores and better explanations than using a much larger corpus. This demonstrates that GenericsKB can be a useful resource for NLP applications, as well as providing data for linguistic studies of generics and their semantics. GenericsKB is available at https://allenai.org/data/genericskb.

</details>

<details>

<summary>2020-05-02 01:29:09 - PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data</summary>

- *Zheng Tang, Milind Naphade, Stan Birchfield, Jonathan Tremblay, William Hodge, Ratnesh Kumar, Shuo Wang, Xiaodong Yang*

- `2005.00673v1` - [abs](http://arxiv.org/abs/2005.00673v1) - [pdf](http://arxiv.org/pdf/2005.00673v1)

> In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID. Code and models are available at https://github.com/NVlabs/PAMTRI.

</details>

<details>

<summary>2020-05-02 04:34:37 - Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer</summary>

- *Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, Ahmed Hassan Awadallah*

- `2005.00699v1` - [abs](http://arxiv.org/abs/2005.00699v1) - [pdf](http://arxiv.org/pdf/2005.00699v1)

> Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.

</details>

<details>

<summary>2020-05-02 05:00:16 - AVA: an Automatic eValuation Approach to Question Answering Systems</summary>

- *Thuy Vu, Alessandro Moschitti*

- `2005.00705v1` - [abs](http://arxiv.org/abs/2005.00705v1) - [pdf](http://arxiv.org/pdf/2005.00705v1)

> We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers, can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference text. This allows for effectively measuring the similarity between the reference and an automatic answer, biased towards the question semantics. To design, train and test AVA, we built multiple large training, development, and test sets on both public and industrial benchmarks. Our innovative solutions achieve up to 74.7% in F1 score in predicting human judgement for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an RMSE, ranging from 0.02 to 0.09, depending on the availability of multiple references.

</details>

<details>

<summary>2020-05-02 12:33:08 - SS3D: Single Shot 3D Object Detector</summary>

- *Aniket Limaye, Manu Mathew, Soyeb Nagori, Pramod Kumar Swami, Debapriya Maji, Kumar Desappan*

- `2004.14674v2` - [abs](http://arxiv.org/abs/2004.14674v2) - [pdf](http://arxiv.org/pdf/2004.14674v2)

> Single stage deep learning algorithm for 2D object detection was made popular by Single Shot MultiBox Detector (SSD) and it was heavily adopted in several embedded applications. PointPillars is a state of the art 3D object detection algorithm that uses a Single Shot Detector adapted for 3D object detection. The main downside of PointPillars is that it has a two stage approach with learned input representation based on fully connected layers followed by the Single Shot Detector for 3D detection. In this paper we present Single Shot 3D Object Detection (SS3D) - a single stage 3D object detection algorithm which combines straight forward, statistically computed input representation and a Single Shot Detector (based on PointPillars). Computing the input representation is straight forward, does not involve learning and does not have much computational cost. We also extend our method to stereo input and show that, aided by additional semantic segmentation input; our method produces similar accuracy as state of the art stereo based detectors. Achieving the accuracy of two stage detectors using a single stage approach is important as single stage approaches are simpler to implement in embedded, real-time applications. With LiDAR as well as stereo input, our method outperforms PointPillars. When using LiDAR input, our input representation is able to improve the AP3D of Cars objects in the moderate category from 74.99 to 76.84. When using stereo input, our input representation is able to improve the AP3D of Cars objects in the moderate category from 38.13 to 45.13. Our results are also better than other popular 3D object detectors such as AVOD and F-PointNet.

</details>

<details>

<summary>2020-05-02 14:03:14 - Universal Decompositional Semantic Parsing</summary>

- *Elias Stengel-Eskin, Aaron Steven White, Sheng Zhang, Benjamin Van Durme*

- `1910.10138v3` - [abs](http://arxiv.org/abs/1910.10138v3) - [pdf](http://arxiv.org/pdf/1910.10138v3)

> We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.

</details>

<details>

<summary>2020-05-03 02:33:20 - Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation</summary>

- *Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann, Vicente Ordonez, Caiming Xiong*

- `2005.00965v1` - [abs](http://arxiv.org/abs/2005.00965v1) - [pdf](http://arxiv.org/pdf/2005.00965v1)

> Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.

</details>

<details>

<summary>2020-05-03 04:18:44 - How Does Selective Mechanism Improve Self-Attention Networks?</summary>

- *Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu*

- `2005.00979v1` - [abs](http://arxiv.org/abs/2005.00979v1) - [pdf](http://arxiv.org/pdf/2005.00979v1)

> Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN.

</details>

<details>

<summary>2020-05-03 12:06:06 - A Two-Stage Masked LM Method for Term Set Expansion</summary>

- *Guy Kushilevitz, Shaul Markovitch, Yoav Goldberg*

- `2005.01063v1` - [abs](http://arxiv.org/abs/2005.01063v1) - [pdf](http://arxiv.org/pdf/2005.01063v1)

> We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class. The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. Previous approaches to the TSE task can be characterized as either distributional or pattern-based. We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches. Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM. The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns. Our method outperforms state-of-the-art TSE algorithms. Implementation is available at: https://github.com/ guykush/TermSetExpansion-MPB/

</details>

<details>

<summary>2020-05-03 18:23:06 - Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward</summary>

- *Luyang Huang, Lingfei Wu, Lu Wang*

- `2005.01159v1` - [abs](http://arxiv.org/abs/2005.01159v1) - [pdf](http://arxiv.org/pdf/2005.01159v1)

> Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders---a sequential document encoder and a graph-structured encoder---to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.

</details>

<details>

<summary>2020-05-03 20:07:51 - Generalizing Natural Language Analysis through Span-relation Representations</summary>

- *Zhengbao Jiang, Wei Xu, Jun Araki, Graham Neubig*

- `1911.03822v2` - [abs](http://arxiv.org/abs/1911.03822v2) - [pdf](http://arxiv.org/pdf/1911.03822v2)

> Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.

</details>

<details>

<summary>2020-05-03 23:32:36 - Obtaining Basic Algebra Formulas with Genetic Programming and Functional Rewriting</summary>

- *Edwin Camilo Cubides, Jonatan Gomez*

- `2005.01207v1` - [abs](http://arxiv.org/abs/2005.01207v1) - [pdf](http://arxiv.org/pdf/2005.01207v1)

> In this paper, we develop a set of genetic programming operators and an initialization population process based on concepts of functional programming rewriting for boosting inductive genetic programming. Such genetic operators are used within a hybrid adaptive evolutionary algorithm that evolves operator rates at the same time it evolves the solution. Solutions are represented using recursive functions where genome is encoded as an ordered list of trees and phenotype is written in a simple functional programming language that uses rewriting as operational semantic (computational model). The fitness is the number of examples successfully deduced over the cardinal of the set of examples. Parents are selected following a tournament selection mechanism and the next population is obtained following a steady-state strategy. The evolutionary process can use some previous functions (programs) induced as background knowledge. We compare the performance of our technique in a set of hard problems (for classical genetic programming). In particular, we take as test-bed the problem of obtaining equivalent algebraic expressions of some notable products (such as square of a binomial, and cube of a binomial), and the recursive formulas of sum of the first n and squares of the first n natural numbers.

</details>

<details>

<summary>2020-05-04 02:46:31 - Visual Question Answering with Prior Class Semantics</summary>

- *Violetta Shevchenko, Damien Teney, Anthony Dick, Anton van den Hengel*

- `2005.01239v1` - [abs](http://arxiv.org/abs/2005.01239v1) - [pdf](http://arxiv.org/pdf/2005.01239v1)

> We present a novel mechanism to embed prior knowledge in a model for visual question answering. The open-set nature of the task is at odds with the ubiquitous approach of training of a fixed classifier. We show how to exploit additional information pertaining to the semantics of candidate answers. We extend the answer prediction process with a regression objective in a semantic space, in which we project candidate answers using prior knowledge derived from word embeddings. We perform an extensive study of learned representations with the GQA dataset, revealing that important semantic information is captured in the relations between embeddings in the answer space. Our method brings improvements in consistency and accuracy over a range of question types. Experiments with novel answers, unseen during training, indicate the method's potential for open-set prediction.

</details>

<details>

<summary>2020-05-04 05:45:13 - Improving Adversarial Text Generation by Modeling the Distant Future</summary>

- *Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen, Lawrence Carin*

- `2005.01279v1` - [abs](http://arxiv.org/abs/2005.01279v1) - [pdf](http://arxiv.org/pdf/2005.01279v1)

> Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance.

</details>

<details>

<summary>2020-05-04 12:18:32 - Enabling Deletion in Append-Only Blockchains (Short Summary / Work in Progress)</summary>

- *Michael Kuperberg*

- `2005.06026v1` - [abs](http://arxiv.org/abs/2005.06026v1) - [pdf](http://arxiv.org/pdf/2005.06026v1)

> Conventional blockchain implementations with append-only semantics do not support deleting or overwriting data in confirmed blocks. However, many industry-relevant use cases require the ability to delete data, especially when personally identifiable information is stored or when data growth has to be constrained. Existing attempts to reconcile these contradictions compromise on core qualities of the blockchain paradigm, as they include backdoor-like approaches such as central authorities with elevated rights or usage of specialized chameleon hash algorithms in chaining of the blocks. In this technical report, we outline a novel architecture for the blockchain ledger and consensus, which uses a tree of context chains with simultaneous validity. A context chain captures the transactions of a closed group of entities and persons, thus structuring blocks in a precisely defined way. The resulting context isolation enables consensus-steered deletion of an entire context without side effects to other contextes. This architecture opens the possibility of truncation, data rollover and separation of concerns, and can help to fulfill the GDPR regulations.

</details>

<details>

<summary>2020-05-04 19:01:55 - Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction</summary>

- *Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, Katja Markert*

- `2005.01791v1` - [abs](http://arxiv.org/abs/2005.01791v1) - [pdf](http://arxiv.org/pdf/2005.01791v1)

> Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.

</details>

<details>

<summary>2020-05-04 19:19:32 - Mind the Gap: On Bridging the Semantic Gap between Machine Learning and Information Security</summary>

- *Michael R. Smith, Nicholas T. Johnson, Joe B. Ingram, Armida J. Carbajal, Ramyaa Ramyaa, Evelyn Domschot, Christopher C. Lamb, Stephen J. Verzi, W. Philip Kegelmeyer*

- `2005.01800v1` - [abs](http://arxiv.org/abs/2005.01800v1) - [pdf](http://arxiv.org/pdf/2005.01800v1)

> Despite the potential of Machine learning (ML) to learn the behavior of malware, detect novel malware samples, and significantly improve information security (InfoSec) we see few, if any, high-impact ML techniques in deployed systems, notwithstanding multiple reported successes in open literature. We hypothesize that the failure of ML in making high-impacts in InfoSec are rooted in a disconnect between the two communities as evidenced by a semantic gap---a difference in how executables are described (e.g. the data and features extracted from the data). Specifically, current datasets and representations used by ML are not suitable for learning the behaviors of an executable and differ significantly from those used by the InfoSec community. In this paper, we survey existing datasets used for classifying malware by ML algorithms and the features that are extracted from the data. We observe that: 1) the current set of extracted features are primarily syntactic, not behavioral, 2) datasets generally contain extreme exemplars producing a dataset in which it is easy to discriminate classes, and 3) the datasets provide significantly different representations of the data encountered in real-world systems. For ML to make more of an impact in the InfoSec community requires a change in the data (including the features and labels) that is used to bridge the current semantic gap. As a first step in enabling more behavioral analyses, we label existing malware datasets with behavioral features using open-source threat reports associated with malware families. This behavioral labeling alters the analysis from identifying intent (e.g. good vs bad) or malware family membership to an analysis of which behaviors are exhibited by an executable. We offer the annotations with the hope of inspiring future improvements in the data that will further bridge the semantic gap between the ML and InfoSec communities.

</details>

<details>

<summary>2020-05-04 19:26:14 - Semi-supervised lung nodule retrieval</summary>

- *Mark Loyman, Hayit Greenspan*

- `2005.01805v1` - [abs](http://arxiv.org/abs/2005.01805v1) - [pdf](http://arxiv.org/pdf/2005.01805v1)

> Content based image retrieval (CBIR) provides the clinician with visual information that can support, and hopefully improve, his or her decision making process. Given an input query image, a CBIR system provides as its output a set of images, ranked by similarity to the query image. Retrieved images may come with relevant information, such as biopsy-based malignancy labeling, or categorization. Ground truth on similarity between dataset elements (e.g. between nodules) is not readily available, thus greatly challenging machine learning methods. Such annotations are particularly difficult to obtain, due to the subjective nature of the task, with high inter-observer variability requiring multiple expert annotators. Consequently, past approaches have focused on manual feature extraction, while current approaches use auxiliary tasks, such as a binary classification task (e.g. malignancy), for which ground-true is more readily accessible. However, in a previous study, we have shown that binary auxiliary tasks are inferior to the usage of a rough similarity estimate that are derived from data annotations. The current study suggests a semi-supervised approach that involves two steps: 1) Automatic annotation of a given partially labeled dataset; 2) Learning a semantic similarity metric space based on the predicated annotations. The proposed system is demonstrated in lung nodule retrieval using the LIDC dataset, and shows that it is feasible to learn embedding from predicted ratings. The semi-supervised approach has demonstrated a significantly higher discriminative ability than the fully-unsupervised reference.

</details>

<details>

<summary>2020-05-04 20:03:21 - Neural Subdivision</summary>

- *Hsueh-Ti Derek Liu, Vladimir G. Kim, Siddhartha Chaudhuri, Noam Aigerman, Alec Jacobson*

- `2005.01819v1` - [abs](http://arxiv.org/abs/2005.01819v1) - [pdf](http://arxiv.org/pdf/2005.01819v1)

> This paper introduces Neural Subdivision, a novel framework for data-driven coarse-to-fine geometry modeling. During inference, our method takes a coarse triangle mesh as input and recursively subdivides it to a finer geometry by applying the fixed topological updates of Loop Subdivision, but predicting vertex positions using a neural network conditioned on the local geometry of a patch. This approach enables us to learn complex non-linear subdivision schemes, beyond simple linear averaging used in classical techniques. One of our key contributions is a novel self-supervised training setup that only requires a set of high-resolution meshes for learning network weights. For any training shape, we stochastically generate diverse low-resolution discretizations of coarse counterparts, while maintaining a bijective mapping that prescribes the exact target position of every new vertex during the subdivision process. This leads to a very efficient and accurate loss function for conditional mesh generation, and enables us to train a method that generalizes across discretizations and favors preserving the manifold structure of the output. During training we optimize for the same set of network weights across all local mesh patches, thus providing an architecture that is not constrained to a specific input mesh, fixed genus, or category. Our network encodes patch geometry in a local frame in a rotation- and translation-invariant manner. Jointly, these design choices enable our method to generalize well, and we demonstrate that even when trained on a single high-resolution mesh our method generates reasonable subdivisions for novel shapes.

</details>

<details>

<summary>2020-05-05 04:34:54 - Head-Driven Phrase Structure Grammar Parsing on Penn Treebank</summary>

- *Junru Zhou, Hai Zhao*

- `1907.02684v4` - [abs](http://arxiv.org/abs/1907.02684v4) - [pdf](http://arxiv.org/pdf/1907.02684v4)

> Head-driven phrase structure grammar (HPSG) enjoys a uniform formalism representing rich contextual syntactic and even semantic meanings. This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure. Then two parsing algorithms are respectively proposed for two converted tree representations, division span and joint span. As HPSG encodes both constituent and dependency structure information, the proposed HPSG parsers may be regarded as a sort of joint decoder for both types of structures and thus are evaluated in terms of extracted or converted constituent and dependency parsing trees. Our parser achieves new state-of-the-art performance for both parsing tasks on Penn Treebank (PTB) and Chinese Penn Treebank, verifying the effectiveness of joint learning constituent and dependency structures. In details, we report 96.33 F1 of constituent parsing and 97.20\% UAS of dependency parsing on PTB.

</details>

<details>

<summary>2020-05-05 05:03:50 - CPU and GPU Accelerated Fully Homomorphic Encryption</summary>

- *Toufique Morshed, Md Momin Al Aziz, Noman Mohammed*

- `2005.01945v1` - [abs](http://arxiv.org/abs/2005.01945v1) - [pdf](http://arxiv.org/pdf/2005.01945v1)

> Fully Homomorphic Encryption (FHE) is one of the most promising technologies for privacy protection as it allows an arbitrary number of function computations over encrypted data. However, the computational cost of these FHE systems limits their widespread applications. In this paper, our objective is to improve the performance of FHE schemes by designing efficient parallel frameworks. In particular, we choose Torus Fully Homomorphic Encryption (TFHE) as it offers exact results for an infinite number of boolean gate (e.g., AND, XOR) evaluations. We first extend the gate operations to algebraic circuits such as addition, multiplication, and their vector and matrix equivalents. Secondly, we consider the multi-core CPUs to improve the efficiency of both the gate and the arithmetic operations. Finally, we port the TFHE to the Graphics Processing Units (GPU) and device novel optimizations for boolean and arithmetic circuits employing the multitude of cores. We also experimentally analyze both the CPU and GPU parallel frameworks for different numeric representations (16 to 32-bit). Our GPU implementation outperforms the existing technique, and it achieves a speedup of 20x for any 32-bit boolean operation and 14.5x for multiplications.

</details>

<details>

<summary>2020-05-05 07:39:17 - Structured Tuning for Semantic Role Labeling</summary>

- *Tao Li, Parth Anand Jawale, Martha Palmer, Vivek Srikumar*

- `2005.00496v2` - [abs](http://arxiv.org/abs/2005.00496v2) - [pdf](http://arxiv.org/pdf/2005.00496v2)

> Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the benefits of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time. Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components. We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.

</details>

<details>

<summary>2020-05-05 15:35:08 - Discriminative Pattern Mining for Breast Cancer Histopathology Image Classification via Fully Convolutional Autoencoder</summary>

- *Xingyu Li, Marko Radulovic, Ksenija Kanjer, Konstantinos N. Plataniotis*

- `1902.08670v3` - [abs](http://arxiv.org/abs/1902.08670v3) - [pdf](http://arxiv.org/pdf/1902.08670v3)

> Accurate diagnosis of breast cancer in histopathology images is challenging due to the heterogeneity of cancer cell growth as well as of a variety of benign breast tissue proliferative lesions. In this paper, we propose a practical and self-interpretable invasive cancer diagnosis solution. With minimum annotation information, the proposed method mines contrast patterns between normal and malignant images in unsupervised manner and generates a probability map of abnormalities to verify its reasoning. Particularly, a fully convolutional autoencoder is used to learn the dominant structural patterns among normal image patches. Patches that do not share the characteristics of this normal population are detected and analyzed by one-class support vector machine and 1-layer neural network. We apply the proposed method to a public breast cancer image set. Our results, in consultation with a senior pathologist, demonstrate that the proposed method outperforms existing methods. The obtained probability map could benefit the pathology practice by providing visualized verification data and potentially leads to a better understanding of data-driven diagnosis solutions.

</details>

<details>

<summary>2020-05-05 16:07:25 - Deep Learning COVID-19 Features on CXR using Limited Training Data Sets</summary>

- *Yujin Oh, Sangjoon Park, Jong Chul Ye*

- `2004.05758v2` - [abs](http://arxiv.org/abs/2004.05758v2) - [pdf](http://arxiv.org/pdf/2004.05758v2)

> Under the global pandemic of COVID-19, the use of artificial intelligence to analyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is becoming important. Unfortunately, due to the emergent nature of the COVID-19 pandemic, a systematic collection of the CXR data set for deep neural network training is difficult. To address this problem, here we propose a patch-based convolutional neural network approach with a relatively small number of trainable parameters for COVID-19 diagnosis. The proposed method is inspired by our statistical analysis of the potential imaging biomarkers of the CXR radiographs. Experimental results show that our method achieves state-of-the-art performance and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage.

</details>

<details>

<summary>2020-05-05 18:12:41 - Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs</summary>

- *Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu*

- `1911.02707v3` - [abs](http://arxiv.org/abs/1911.02707v3) - [pdf](http://arxiv.org/pdf/1911.02707v3)

> Human conversations naturally evolve around related concepts and scatter to multi-hop concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.

</details>

<details>

<summary>2020-05-05 18:13:19 - SPARQL as a Foreign Language</summary>

- *Tommaso Soru, Edgard Marx, Diego Moussallem, Gustavo Publio, André Valdestilhas, Diego Esteves, Ciro Baron Neto*

- `1708.07624v2` - [abs](http://arxiv.org/abs/1708.07624v2) - [pdf](http://arxiv.org/pdf/1708.07624v2)

> In the last years, the Linked Data Cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains. However, accessing this information has been significantly challenging for lay users. Approaches to problems such as Question Answering on Linked Data and Link Discovery have notably played a role in increasing information access. These approaches are often based on handcrafted and/or statistical models derived from data observation. Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve state-of-the-art results at translating sequences into sequences. In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition.

</details>

<details>

<summary>2020-05-05 20:21:53 - Cross-media Structured Common Space for Multimedia Event Extraction</summary>

- *Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang*

- `2005.02472v1` - [abs](http://arxiv.org/abs/2005.02472v1) - [pdf](http://arxiv.org/pdf/2005.02472v1)

> We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.

</details>

<details>

<summary>2020-05-06 00:36:16 - An Annotated Dataset of Stack Overflow Post Edits</summary>

- *Sebastian Baltes, Markus Wagner*

- `2004.08193v2` - [abs](http://arxiv.org/abs/2004.08193v2) - [pdf](http://arxiv.org/pdf/2004.08193v2)

> To improve software engineering, software repositories have been mined for code snippets and bug fixes. Typically, this mining takes place at the level of files or commits. To be able to dig deeper and to extract insights at a higher resolution, we hereby present an annotated dataset that contains over 7 million edits of code and text on Stack Overflow. Our preliminary study indicates that these edits might be a treasure trove for mining information about fine-grained patches, e.g., for the optimisation of non-functional properties.

</details>

<details>

<summary>2020-05-06 02:17:07 - Synthesis of Parallel Synchronous Software</summary>

- *Pantea Kiaei, Patrick Schaumont*

- `2005.02562v1` - [abs](http://arxiv.org/abs/2005.02562v1) - [pdf](http://arxiv.org/pdf/2005.02562v1)

> In typical embedded applications, the precise execution time of the program does not matter, and it is sufficient to meet a real-time deadline. However, modern applications in information security have become much more time-sensitive, due to the risk of timing side-channel leakage. The timing of such programs needs to be data-independent and precise. We describe a parallel synchronous software model, which executes as N parallel threads on a processor with word-length N. Each thread is a single-bit synchronous machine with precise, contention-free timing, while each of the N threads still executes as an independent machine. The resulting software supports fine-grained parallel execution. In contrast to earlier work to obtain precise and repeatable timing in software, our solution does not require modifications to the processor architecture nor specialized instruction scheduling techniques. In addition, all threads run in parallel and without contention, which eliminates the problem of thread scheduling. We use hardware (HDL) semantics to describe a thread as a single-bit synchronous machine. Using logic synthesis and code generation, we derive a parallel synchronous implementation of this design. We illustrate the synchronous parallel programming model with practical examples from cryptography and other applications with precise timing requirements.

</details>

<details>

<summary>2020-05-06 03:18:11 - Probing the Natural Language Inference Task with Automated Reasoning Tools</summary>

- *Zaid Marji, Animesh Nighojkar, John Licato*

- `2005.02573v1` - [abs](http://arxiv.org/abs/2005.02573v1) - [pdf](http://arxiv.org/pdf/2005.02573v1)

> The Natural Language Inference (NLI) task is an important task in modern NLP, as it asks a broad question to which many other tasks may be reducible: Given a pair of sentences, does the first entail the second? Although the state-of-the-art on current benchmark datasets for NLI are deep learning-based, it is worthwhile to use other techniques to examine the logical structure of the NLI task. We do so by testing how well a machine-oriented controlled natural language (Attempto Controlled English) can be used to parse NLI sentences, and how well automated theorem provers can reason over the resulting formulae. To improve performance, we develop a set of syntactic and semantic transformation rules. We report their performance, and discuss implications for NLI and logic-based NLP.

</details>

<details>

<summary>2020-05-06 04:46:11 - A Multi-Perspective Architecture for Semantic Code Search</summary>

- *Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, Julia Hockenmaier*

- `2005.06980v1` - [abs](http://arxiv.org/abs/2005.06980v1) - [pdf](http://arxiv.org/pdf/2005.06980v1)

> The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code--text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.

</details>

<details>

<summary>2020-05-06 06:46:27 - Piveau: A Large-scale Open Data Management Platform based on Semantic Web Technologies</summary>

- *Fabian Kirstein, Kyriakos Stefanidis, Benjamin Dittwald, Simon Dutkowski, Sebastian Urbanek, Manfred Hauswirth*

- `2005.02614v1` - [abs](http://arxiv.org/abs/2005.02614v1) - [pdf](http://arxiv.org/pdf/2005.02614v1)

> The publication and (re)utilization of Open Data is still facing multiple barriers on technical, organizational and legal levels. This includes limitations in interfaces, search capabilities, provision of quality information and the lack of definite standards and implementation guidelines. Many Semantic Web specifications and technologies are specifically designed to address the publication of data on the web. In addition, many official publication bodies encourage and foster the development of Open Data standards based on Semantic Web principles. However, no existing solution for managing Open Data takes full advantage of these possibilities and benfits. In this paper, we present our solution "Piveau", a fully-fledged Open Data management solution, based on Semantic Web technologies. It harnesses a variety of standards, like RDF, DCAT, DQV, and SKOS, to overcome the barriers in Open Data publication. The solution puts a strong focus on assuring data quality and scalability. We give a detailed description of the underlying, highly scalable, service-oriented architecture, how we integrated the aforementioned standards, and used a triplestore as our primary database. We have evaluated our work in a comprehensive feature comparison to established solutions and through a practical application in a production environment, the European Data Portal. Our solution is available as Open Source.

</details>

<details>

<summary>2020-05-06 08:38:20 - Evaluating text coherence based on the graph of the consistency of phrases to identify symptoms of schizophrenia</summary>

- *Artem Kramov*

- `2005.03008v1` - [abs](http://arxiv.org/abs/2005.03008v1) - [pdf](http://arxiv.org/pdf/2005.03008v1)

> Different state-of-the-art methods of the detection of schizophrenia symptoms based on the estimation of text coherence have been analyzed. The analysis of a text at the level of phrases has been suggested. The method based on the graph of the consistency of phrases has been proposed to evaluate the semantic coherence and the cohesion of a text. The semantic coherence, cohesion, and other linguistic features (lexical diversity, lexical density) have been taken into account to form feature vectors for the training of a model-classifier. The training of the classifier has been performed on the set of English-language interviews. According to the retrieved results, the impact of each feature on the output of the model has been analyzed. The results obtained can indicate that the proposed method based on the graph of the consistency of phrases may be used in the different tasks of the detection of mental illness.

</details>

<details>

<summary>2020-05-06 08:45:00 - Towards Building Knowledge by Merging Multiple Ontologies with CoMerger: A Partitioning-based Approach</summary>

- *Samira Babalou, Birgitta König-Ries*

- `2005.02659v1` - [abs](http://arxiv.org/abs/2005.02659v1) - [pdf](http://arxiv.org/pdf/2005.02659v1)

> Ontologies are the prime way of organizing data in the Semantic Web. Often, it is necessary to combine several, independently developed ontologies to obtain a knowledge graph fully representing a domain of interest. The complementarity of existing ontologies can be leveraged by merging them. Existing approaches for ontology merging mostly implement a binary merge. However, with the growing number and size of relevant ontologies across domains, scalability becomes a central challenge. A multi-ontology merging technique offers a potential solution to this problem. We present CoMerger, a scalable multiple ontologies merging method. For efficient processing, rather than successively merging complete ontologies pairwise, we group related concepts across ontologies into partitions and merge first within and then across those partitions. The experimental results on well-known datasets confirm the feasibility of our approach and demonstrate its superiority over binary strategies. A prototypical implementation is freely accessible through a live web portal.

</details>

<details>

<summary>2020-05-06 17:17:18 - PointPainting: Sequential Fusion for 3D Object Detection</summary>

- *Sourabh Vora, Alex H. Lang, Bassam Helou, Oscar Beijbom*

- `1911.10150v2` - [abs](http://arxiv.org/abs/1911.10150v2) - [pdf](http://arxiv.org/pdf/1911.10150v2)

> Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor-fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to fill this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidar-only method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets. The painted version of PointRCNN represents a new state of the art on the KITTI leaderboard for the bird's-eye view detection task. In ablation, we study how the effects of Painting depends on the quality and format of the semantic segmentation output, and demonstrate how latency can be minimized through pipelining.

</details>

<details>

<summary>2020-05-06 17:36:16 - What are the Goals of Distributional Semantics?</summary>

- *Guy Emerson*

- `2005.02982v1` - [abs](http://arxiv.org/abs/2005.02982v1) - [pdf](http://arxiv.org/pdf/2005.02982v1)

> Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks. However, assessing long-term progress requires explicit long-term goals. In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges. Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them. I conclude that, while linguistic insights can guide the design of model architectures, future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.

</details>

<details>

<summary>2020-05-06 19:24:33 - Diagnosing the Environment Bias in Vision-and-Language Navigation</summary>

- *Yubo Zhang, Hao Tan, Mohit Bansal*

- `2005.03086v1` - [abs](http://arxiv.org/abs/2005.03086v1) - [pdf](http://arxiv.org/pdf/2005.03086v1)

> Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions, explore the given environments, and reach the desired target locations. These step-by-step navigational instructions are crucial when the agent is navigating new environments about which it has no prior knowledge. Most recent works that study VLN observe a significant performance drop when tested on unseen environments (i.e., environments not used in training), indicating that the neural agent models are highly biased towards training environments. Although this issue is considered as one of the major challenges in VLN research, it is still under-studied and needs a clearer explanation. In this work, we design novel diagnosis experiments via environment re-splitting and feature replacement, looking into possible reasons for this environment bias. We observe that neither the language nor the underlying navigational graph, but the low-level visual appearance conveyed by ResNet features directly affects the agent model and contributes to this environment bias in results. According to this observation, we explore several kinds of semantic representations that contain less low-level visual information, hence the agent learned with these features could be better generalized to unseen testing environments. Without modifying the baseline agent model and its training method, our explored semantic features significantly decrease the performance gaps between seen and unseen on multiple datasets (i.e. R2R, R4R, and CVDN) and achieve competitive unseen results to previous state-of-the-art models. Our code and features are available at: https://github.com/zhangybzbo/EnvBiasVLN

</details>

<details>

<summary>2020-05-06 20:11:46 - Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting</summary>

- *Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann*

- `2005.03119v1` - [abs](http://arxiv.org/abs/2005.03119v1) - [pdf](http://arxiv.org/pdf/2005.03119v1)

> Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when the images are not available at the testing time.

</details>

<details>

<summary>2020-05-07 03:43:42 - Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus</summary>

- *Hao Fei, Meishan Zhang, Donghong Ji*

- `2004.06295v2` - [abs](http://arxiv.org/abs/2004.06295v2) - [pdf](http://arxiv.org/pdf/2004.06295v2)

> Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.

</details>

<details>

<summary>2020-05-07 06:04:02 - Synthetic Image Augmentation for Damage Region Segmentation using Conditional GAN with Structure Edge</summary>

- *Takato Yasuno, Michihiro Nakajima, Tomoharu Sekiguchi, Kazuhiro Noda, Kiyoshi Aoyanagi, Sakura Kato*

- `2005.08628v1` - [abs](http://arxiv.org/abs/2005.08628v1) - [pdf](http://arxiv.org/pdf/2005.08628v1)

> Recently, social infrastructure is aging, and its predictive maintenance has become important issue. To monitor the state of infrastructures, bridge inspection is performed by human eye or bay drone. For diagnosis, primary damage region are recognized for repair targets. But, the degradation at worse level has rarely occurred, and the damage regions of interest are often narrow, so their ratio per image is extremely small pixel count, as experienced 0.6 to 1.5 percent. The both scarcity and imbalance property on the damage region of interest influences limited performance to detect damage. If additional data set of damaged images can be generated, it may enable to improve accuracy in damage region segmentation algorithm. We propose a synthetic augmentation procedure to generate damaged images using the image-to-image translation mapping from the tri-categorical label that consists the both semantic label and structure edge to the real damage image. We use the Sobel gradient operator to enhance structure edge. Actually, in case of bridge inspection, we apply the RC concrete structure with the number of 208 eye-inspection photos that rebar exposure have occurred, which are prepared 840 block images with size 224 by 224. We applied popular per-pixel segmentation algorithms such as the FCN-8s, SegNet, and DeepLabv3+Xception-v2. We demonstrates that re-training a data set added with synthetic augmentation procedure make higher accuracy based on indices the mean IoU, damage region of interest IoU, precision, recall, BF score when we predict test images.

</details>

<details>

<summary>2020-05-07 07:29:49 - YANG2UML: Bijective Transformation and Simplification of YANG to UML</summary>

- *Mario Golling, Robert Koch, Peter Hillmann, Rick Hofstede, Frank Tietze*

- `2005.03292v1` - [abs](http://arxiv.org/abs/2005.03292v1) - [pdf](http://arxiv.org/pdf/2005.03292v1)

> Software Defined Networking is currently revolutionizing computer networking by decoupling the network control (control plane) from the forwarding functions (data plane) enabling the network control to become directly programmable and the underlying infrastructure to be abstracted for applications and network services. Next to the well-known OpenFlow protocol, the XML-based NETCONF protocol is also an important means for exchanging configuration information from a management platform and is nowadays even part of OpenFlow. In combination with NETCONF, YANG is the corresponding protocol that defines the associated data structures supporting virtually all network configuration protocols. YANG itself is a semantically rich language, which -- in order to facilitate familiarization with the relevant subject -- is often visualized to involve other experts or developers and to support them by their daily work (writing applications which make use of YANG). In order to support this process, this paper presents an novel approach to optimize and simplify YANG data models to assist further discussions with the management and implementations (especially of interfaces) to reduce complexity. Therefore, we have defined a bidirectional mapping of YANG to UML and developed a tool that renders the created UML diagrams. This combines the benefits to use the formal language YANG with automatically maintained UML diagrams to involve other experts or developers, closing the gap between technically improved data models and their human readability.

</details>

<details>

<summary>2020-05-07 11:43:16 - Revisiting Semantics of Interactions for Trace Validity Analysis</summary>

- *Erwan Mahe, Christophe Gaston, Pascale Le Gall*

- `1911.03094v2` - [abs](http://arxiv.org/abs/1911.03094v2) - [pdf](http://arxiv.org/pdf/1911.03094v2)

> Interaction languages such as MSC are often associated with formal semantics by means of translations into distinct behavioral formalisms such as automatas or Petri nets. In contrast to translational approaches we propose an operational approach. Its principle is to identify which elementary communication actions can be immediately executed, and then to compute, for every such action, a new interaction representing the possible continuations to its execution. We also define an algorithm for checking the validity of execution traces (i.e. whether or not they belong to an interaction's semantics). Algorithms for semantic computation and trace validity are analyzed by means of experiments.

</details>

<details>

<summary>2020-05-07 13:48:26 - Bundle Recommendation with Graph Convolutional Networks</summary>

- *Jianxin Chang, Chen Gao, Xiangnan He, Yong Li, Depeng Jin*

- `2005.03475v1` - [abs](http://arxiv.org/abs/2005.03475v1) - [pdf](http://arxiv.org/pdf/2005.03475v1)

> Bundle recommendation aims to recommend a bundle of items for a user to consume as a whole. Existing solutions integrate user-item interaction modeling into bundle recommendation by sharing model parameters or learning in a multi-task manner, which cannot explicitly model the affiliation between items and bundles, and fail to explore the decision-making when a user chooses bundles. In this work, we propose a graph neural network model named BGCN (short for \textit{\textBF{B}undle \textBF{G}raph \textBF{C}onvolutional \textBF{N}etwork}) for bundle recommendation. BGCN unifies user-item interaction, user-bundle interaction and bundle-item affiliation into a heterogeneous graph. With item nodes as the bridge, graph convolutional propagation between user and bundle nodes makes the learned representations capture the item level semantics. Through training based on hard-negative sampler, the user's fine-grained preferences for similar bundles are further distinguished. Empirical results on two real-world datasets demonstrate the strong performance gains of BGCN, which outperforms the state-of-the-art baselines by 10.77\% to 23.18\%.

</details>

<details>

<summary>2020-05-07 17:42:13 - Where is Linked Data in Question Answering over Linked Data?</summary>

- *Tommaso Soru, Edgard Marx, André Valdestilhas, Diego Moussallem, Gustavo Publio, Muhammad Saleem*

- `2005.03640v1` - [abs](http://arxiv.org/abs/2005.03640v1) - [pdf](http://arxiv.org/pdf/2005.03640v1)

> We argue that "Question Answering with Knowledge Base" and "Question Answering over Linked Data" are currently two instances of the same problem, despite one explicitly declares to deal with Linked Data. We point out the lack of existing methods to evaluate question answering on datasets which exploit external links to the rest of the cloud or share common schema. To this end, we propose the creation of new evaluation settings to leverage the advantages of the Semantic Web to achieve AI-complete question answering.

</details>

<details>

<summary>2020-05-07 19:54:24 - SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization</summary>

- *Yang Gao, Wei Zhao, Steffen Eger*

- `2005.03724v1` - [abs](http://arxiv.org/abs/2005.03724v1) - [pdf](http://arxiv.org/pdf/2005.03724v1)

> We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.

</details>

<details>

<summary>2020-05-08 01:17:59 - Using Taint Analysis and Reinforcement Learning (TARL) to Repair Autonomous Robot Software</summary>

- *D. M. Lyons, S. Zahra*

- `2005.03813v1` - [abs](http://arxiv.org/abs/2005.03813v1) - [pdf](http://arxiv.org/pdf/2005.03813v1)

> It is important to be able to establish formal performance bounds for autonomous systems. However, formal verification techniques require a model of the environment in which the system operates; a challenge for autonomous systems, especially those expected to operate over longer timescales. This paper describes work in progress to automate the monitor and repair of ROS-based autonomous robot software written for an a-priori partially known and possibly incorrect environment model. A taint analysis method is used to automatically extract the data-flow sequence from input topic to publish topic, and instrument that code. A unique reinforcement learning approximation of MDP utility is calculated, an empirical and non-invasive characterization of the inherent objectives of the software designers. By comparing off-line (a-priori) utility with on-line (deployed system) utility, we show, using a small but real ROS example, that it's possible to monitor a performance criterion and relate violations of the criterion to parts of the software. The software is then patched using automated software repair techniques and evaluated against the original off-line utility.

</details>

<details>

<summary>2020-05-08 03:17:23 - BasConv: Aggregating Heterogeneous Interactions for Basket Recommendation with Graph Convolutional Neural Network</summary>

- *Zhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, Philip S. Yu*

- `2001.09900v2` - [abs](http://arxiv.org/abs/2001.09900v2) - [pdf](http://arxiv.org/pdf/2001.09900v2)

> Within-basket recommendation reduces the exploration time of users, where the user's intention of the basket matters. The intent of a shopping basket can be retrieved from both user-item collaborative filtering signals and multi-item correlations. By defining a basket entity to represent the basket intent, we can model this problem as a basket-item link prediction task in the User-Basket-Item~(UBI) graph. Previous work solves the problem by leveraging user-item interactions and item-item interactions simultaneously. However, collectivity and heterogeneity characteristics are hardly investigated before. Collectivity defines the semantics of each node which should be aggregated from both directly and indirectly connected neighbors. Heterogeneity comes from multi-type interactions as well as multi-type nodes in the UBI graph. To this end, we propose a new framework named \textbf{BasConv}, which is based on the graph convolutional neural network. Our BasConv model has three types of aggregators specifically designed for three types of nodes. They collectively learn node embeddings from both neighborhood and high-order context. Additionally, the interactive layers in the aggregators can distinguish different types of interactions. Extensive experiments on two real-world datasets prove the effectiveness of BasConv. Our code is available online at https://github.com/JimLiu96/basConv.

</details>

<details>

<summary>2020-05-08 06:53:05 - OpenEDS2020: Open Eyes Dataset</summary>

- *Cristina Palmero, Abhishek Sharma, Karsten Behrendt, Kapil Krishnakumar, Oleg V. Komogortsev, Sachin S. Talathi*

- `2005.03876v1` - [abs](http://arxiv.org/abs/2005.03876v1) - [pdf](http://arxiv.org/pdf/2005.03876v1)

> We present the second edition of OpenEDS dataset, OpenEDS2020, a novel dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display mounted with two synchronized eye-facing cameras. The dataset, which is anonymized to remove any personally identifiable information on participants, consists of 80 participants of varied appearance performing several gaze-elicited tasks, and is divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560 sequences containing 550,400 eye-images and respective gaze vectors, created to foster research in spatio-temporal gaze estimation and prediction approaches; and 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz, with up to 29,500 images, of which 5% contain a semantic segmentation label, devised to encourage the use of temporal information to propagate labels to contiguous frames. Baseline experiments have been evaluated on OpenEDS2020, one for each task, with average angular error of 5.37 degrees when performing gaze prediction on 1 to 5 frames into the future, and a mean intersection over union score of 84.1% for semantic segmentation. As its predecessor, OpenEDS dataset, we anticipate that this new dataset will continue creating opportunities to researchers in eye tracking, machine learning and computer vision communities, to advance the state of the art for virtual reality applications. The dataset is available for download upon request at http://research.fb.com/programs/openeds-2020-challenge/.

</details>

<details>

<summary>2020-05-08 09:00:06 - Relational Model for Parameter Description in Automatic Semantic Web Service Composition</summary>

- *Paul Diac, Liana Ţucăr, Andrei Netedu*

- `2005.05046v1` - [abs](http://arxiv.org/abs/2005.05046v1) - [pdf](http://arxiv.org/pdf/2005.05046v1)

> Automatic Service Composition is a research direction aimed at facilitating the usage of atomic web services. Particularly, the goal is to build workflows of services that solve specific queries, which cannot be resolved by any single service from a known repository. Each of these services is described independently by their providers that can have no interaction with each other, therefore some common standards have been developed, such as WSDL, BPEL, OWL-S. Our proposal is to use such standards together with JSON-LD to model a next level of semantics, mainly based on binary relations between parameters of services. Services relate to a public ontology to describe their functionality. Binary relations can be specified between input and/or output parameters in service definition. The ontology includes some relations and inference rules that help to deduce new relations between parameters of services. To our knowledge, it is for the first time that parameters are matched not only based on their type, but on a more meaningful semantic context considering such type of relations. This enables the automation of a large part of the reasoning that a human person would do when manually building a composition. Moreover, the proposed model and the composition algorithm can work with multiple objects of the same type, a fundamental feature that was not possible before. We believe that the poor model expressiveness is what is keeping service composition from reaching large-scale application in practice.

</details>

<details>

<summary>2020-05-08 09:15:17 - Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts</summary>

- *Faisal Alshargi, Saeedeh Shekarpour, Tommaso Soru, Amit Sheth*

- `1803.04488v3` - [abs](http://arxiv.org/abs/1803.04488v3) - [pdf](http://arxiv.org/pdf/1803.04488v3)

> Although there is an emerging trend towards generating embeddings for primarily unstructured data and, recently, for structured data, no systematic suite for measuring the quality of embeddings has been proposed yet. This deficiency is further sensed with respect to embeddings generated for structured data because there are no concrete evaluation metrics measuring the quality of the encoded structure as well as semantic patterns in the embedding space. In this paper, we introduce a framework containing three distinct tasks concerned with the individual aspects of ontological concepts: (i) the categorization aspect, (ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the scope of each task, a number of intrinsic metrics are proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this framework, multiple experimental studies were run to compare the quality of the available embedding models. Employing this framework in future research can reduce misjudgment and provide greater insight about quality comparisons of embeddings for ontological concepts. We positioned our sampled data and code at https://github.com/alshargi/Concept2vec under GNU General Public License v3.0.

</details>

<details>

<summary>2020-05-08 10:44:23 - TransSent: Towards Generation of Structured Sentences with Discourse Marker</summary>

- *Xing Wu, Dongjun Wei, Liangjun Zang, Jizhong Han, Songlin Hu*

- `1909.05364v3` - [abs](http://arxiv.org/abs/1909.05364v3) - [pdf](http://arxiv.org/pdf/1909.05364v3)

> Structured sentences are important expressions in human writings and dialogues. Previous works on neural text generation fused semantic and structural information by encoding the entire sentence into a mixed hidden representation. However, when a generated sentence becomes complicated, the structure is difficult to be properly maintained. To alleviate this problem, we explicitly separate the modeling process of semantic and structural information. Intuitively, humans generate structured sentences by directly connecting discourses with discourse markers (such as and, but, etc.). Therefore, we propose a task that mimics this process, called discourse transfer. This task represents a structured sentence as (head discourse, discourse marker, tail discourse), and aims at tail discourse generation based on head discourse and discourse marker. We also propose a corresponding model called TransSent, which interprets the relationship between two discourses as a translation1 from the head discourse to the tail discourse in the embedding space. We experiment TransSent not only in discourse transfer task but also in free text generation and dialogue generation tasks. Automatic and human evaluation results show that TransSent can generate structured sentences with high quality, and has certain scalability in different tasks.

</details>

<details>

<summary>2020-05-08 13:47:58 - Literature Triage on Genomic Variation Publications by Knowledge-enhanced Multi-channel CNN</summary>

- *Chenhui Lv, Qian Lu, Xiang Zhang*

- `2005.04044v1` - [abs](http://arxiv.org/abs/2005.04044v1) - [pdf](http://arxiv.org/pdf/2005.04044v1)

> Background: To investigate the correlation between genomic variation and certain diseases or phenotypes, the fundamental task is to screen out the concerning publications from massive literature, which is called literature triage. Some knowledge bases, including UniProtKB/Swiss-Prot and NHGRI-EBI GWAS Catalog are created for collecting concerning publications. These publications are manually curated by experts, which is time-consuming. Moreover, the manual curation of information from literature is not scalable due to the rapidly increasing amount of publications. In order to cut down the cost of literature triage, machine-learning models were adopted to automatically identify biomedical publications. Methods: Comparing to previous studies utilizing machine-learning models for literature triage, we adopt a multi-channel convolutional network to utilize rich textual information and meanwhile bridge the semantic gaps from different corpora. In addition, knowledge embeddings learned from UMLS is also used to provide extra medical knowledge beyond textual features in the process of triage. Results: We demonstrate that our model outperforms the state-of-the-art models over 5 datasets with the help of knowledge embedding and multiple channels. Our model improves the accuracy of biomedical literature triage results. Conclusions: Multiple channels and knowledge embeddings enhance the performance of the CNN model in the task of biomedical literature triage. Keywords: Literature Triage; Knowledge Embedding; Multi-channel Convolutional Network

</details>

<details>

<summary>2020-05-08 14:40:36 - What do you mean, BERT? Assessing BERT as a Distributional Semantics Model</summary>

- *Timothee Mickus, Denis Paperno, Mathieu Constant, Kees van Deemter*

- `1911.05758v2` - [abs](http://arxiv.org/abs/1911.05758v2) - [pdf](http://arxiv.org/pdf/1911.05758v2)

> Contextualized word embeddings, i.e. vector representations for words in context, are naturally seen as an extension of previous noncontextual distributional semantic models. In this work, we focus on BERT, a deep neural network that produces contextualized embeddings and has set the state-of-the-art in several semantic tasks, and study the semantic coherence of its embedding space. While showing a tendency towards coherence, BERT does not fully live up to the natural expectations for a semantic vector space. In particular, we find that the position of the sentence in which a word occurs, while having no meaning correlates, leaves a noticeable trace on the word embeddings and disturbs similarity relationships.

</details>

<details>

<summary>2020-05-08 14:54:13 - A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View</summary>

- *Lennart Reiher, Bastian Lampe, Lutz Eckstein*

- `2005.04078v1` - [abs](http://arxiv.org/abs/2005.04078v1) - [pdf](http://arxiv.org/pdf/2005.04078v1)

> Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360{\deg} BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV

</details>

<details>

<summary>2020-05-08 18:51:10 - Speeding up Word Mover's Distance and its variants via properties of distances between embeddings</summary>

- *Matheus Werner, Eduardo Laber*

- `1912.00509v2` - [abs](http://arxiv.org/abs/1912.00509v2) - [pdf](http://arxiv.org/pdf/1912.00509v2)

> The Word Mover's Distance (WMD) proposed by Kusner et al. is a distance between documents that takes advantage of semantic relations among words that are captured by their embeddings. This distance proved to be quite effective, obtaining state-of-art error rates for classification tasks, but is also impracticable for large collections/documents due to its computational complexity. For circumventing this problem, variants of WMD have been proposed. Among them, Relaxed Word Mover's Distance (RWMD) is one of the most successful due to its simplicity, effectiveness, and also because of its fast implementations.   Relying on assumptions that are supported by empirical properties of the distances between embeddings, we propose an approach to speed up both WMD and RWMD. Experiments over 10 datasets suggest that our approach leads to a significant speed-up in document classification tasks while maintaining the same error rates.

</details>

<details>

<summary>2020-05-08 23:00:40 - Constant-Time Foundations for the New Spectre Era</summary>

- *Sunjay Cauligi, Craig Disselkoen, Klaus v. Gleissenthall, Dean Tullsen, Deian Stefan, Tamara Rezk, Gilles Barthe*

- `1910.01755v3` - [abs](http://arxiv.org/abs/1910.01755v3) - [pdf](http://arxiv.org/pdf/1910.01755v3)

> The constant-time discipline is a software-based countermeasure used for protecting high assurance cryptographic implementations against timing side-channel attacks. Constant-time is effective (it protects against many known attacks), rigorous (it can be formalized using program semantics), and amenable to automated verification. Yet, the advent of micro-architectural attacks makes constant-time as it exists today far less useful.   This paper lays foundations for constant-time programming in the presence of speculative and out-of-order execution. We present an operational semantics and a formal definition of constant-time programs in this extended setting. Our semantics eschews formalization of microarchitectural features (that are instead assumed under adversary control), and yields a notion of constant-time that retains the elegance and tractability of the usual notion. We demonstrate the relevance of our semantics in two ways: First, by contrasting existing Spectre-like attacks with our definition of constant-time. Second, by implementing a static analysis tool, Pitchfork, which detects violations of our extended constant-time property in real world cryptographic libraries.

</details>

<details>

<summary>2020-05-08 23:56:26 - A Showcase of the Use of Autoencoders in Feature Learning Applications</summary>

- *David Charte, Francisco Charte, María J. del Jesus, Francisco Herrera*

- `2005.04321v1` - [abs](http://arxiv.org/abs/2005.04321v1) - [pdf](http://arxiv.org/pdf/2005.04321v1)

> Autoencoders are techniques for data representation learning based on artificial neural networks. Differently to other feature learning methods which may be focused on finding specific transformations of the feature space, they can be adapted to fulfill many purposes, such as data visualization, denoising, anomaly detection and semantic hashing. This work presents these applications and provides details on how autoencoders can perform them, including code samples making use of an R package with an easy-to-use interface for autoencoder design and training, \texttt{ruta}. Along the way, the explanations on how each learning task has been achieved are provided with the aim to help the reader design their own autoencoders for these or other objectives.

</details>

<details>

<summary>2020-05-09 01:36:58 - Multi-Sentence Argument Linking</summary>

- *Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, Benjamin Van Durme*

- `1911.03766v3` - [abs](http://arxiv.org/abs/1911.03766v3) - [pdf](http://arxiv.org/pdf/1911.03766v3)

> We present a novel document-level model for finding argument spans that fill an event's roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.

</details>

<details>

<summary>2020-05-09 04:01:43 - It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations</summary>

- *Samson Tan, Shafiq Joty, Min-Yen Kan, Richard Socher*

- `2005.04364v1` - [abs](http://arxiv.org/abs/2005.04364v1) - [pdf](http://arxiv.org/pdf/2005.04364v1)

> Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.

</details>

<details>

<summary>2020-05-09 05:19:33 - Building and Maintaining a Third-Party Library Supply Chain for Productive and Secure SGX Enclave Development</summary>

- *Pei Wang, Yu Ding, Mingshen Sun, Huibo Wang, Tongxin Li, Rundong Zhou, Zhaofeng Chen, Yiming Jing*

- `2005.04367v1` - [abs](http://arxiv.org/abs/2005.04367v1) - [pdf](http://arxiv.org/pdf/2005.04367v1)

> The big data industry is facing new challenges as concerns about privacy leakage soar. One of the remedies to privacy breach incidents is to encapsulate computations over sensitive data within hardware-assisted Trusted Execution Environments (TEE). Such TEE-powered software is called secure enclaves. Secure enclaves hold various advantages against competing for privacy-preserving computation solutions. However, enclaves are much more challenging to build compared with ordinary software. The reason is that the development of TEE software must follow a restrictive programming model to make effective use of strong memory encryption and segregation enforced by hardware. These constraints transitively apply to all third-party dependencies of the software. If these dependencies do not officially support TEE hardware, TEE developers have to spend additional engineering effort in porting them. High development and maintenance cost is one of the major obstacles against adopting TEE-based privacy protection solutions in production. In this paper, we present our experience and achievements with regard to constructing and continuously maintaining a third-party library supply chain for TEE developers. In particular, we port a large collection of Rust third-party libraries into Intel SGX, one of the most mature trusted computing platforms. Our supply chain accepts upstream patches in a timely manner with SGX-specific security auditing. We have been able to maintain the SGX ports of 159 open-source Rust libraries with reasonable operational costs. Our work can effectively reduce the engineering cost of developing SGX enclaves for privacy-preserving data processing and exchange.

</details>

<details>

<summary>2020-05-09 05:35:52 - Formal Specification and Integration of Distributed Security Policies</summary>

- *Mohamed Mejri, Hamdi Yahyaoui*

- `1605.06233v2` - [abs](http://arxiv.org/abs/1605.06233v2) - [pdf](http://arxiv.org/pdf/1605.06233v2)

> We propose in this paper the Security Policy Language (SePL), which is a formal language for capturing and integrating distributed security policies. The syntax of SePL includes several operators for the integration of policies and it is endowed with a denotational semantics that is a generic semantics, i.e., which is independent of any evaluation environment. We prove the completeness of SePL with respect to sets theory. Furthermore, we provide a formalization of a subset of the eXtensible Access Control Markup Language (XACML), which is the well-known standard informal specification language of Web security policies. We provide also a semantics for XACML policy combining algorithms.

</details>

<details>

<summary>2020-05-09 08:34:57 - SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems</summary>

- *Edward Chou, Florian Tramèr, Giancarlo Pellegrino*

- `1812.00292v4` - [abs](http://arxiv.org/abs/1812.00292v4) - [pdf](http://arxiv.org/pdf/1812.00292v4)

> SentiNet is a novel detection framework for localized universal attacks on neural networks. These attacks restrict adversarial noise to contiguous portions of an image and are reusable with different images -- constraints that prove useful for generating physically-realizable attacks. Unlike most other works on adversarial detection, SentiNet does not require training a model or preknowledge of an attack prior to detection. Our approach is appealing due to the large number of possible mechanisms and attack-vectors that an attack-specific defense would have to consider. By leveraging the neural network's susceptibility to attacks and by using techniques from model interpretability and object detection as detection mechanisms, SentiNet turns a weakness of a model into a strength. We demonstrate the effectiveness of SentiNet on three different attacks -- i.e., data poisoning attacks, trojaned networks, and adversarial patches (including physically realizable attacks) -- and show that our defense is able to achieve very competitive performance metrics for all three threats. Finally, we show that SentiNet is robust against strong adaptive adversaries, who build adversarial patches that specifically target the components of SentiNet's architecture.

</details>

<details>

<summary>2020-05-09 09:27:12 - Local Context Normalization: Revisiting Local Normalization</summary>

- *Anthony Ortiz, Caleb Robinson, Dan Morris, Olac Fuentes, Christopher Kiekintveld, Md Mahmudulla Hassan, Nebojsa Jojic*

- `1912.05845v3` - [abs](http://arxiv.org/abs/1912.05845v3) - [pdf](http://arxiv.org/pdf/1912.05845v3)

> Normalization layers have been shown to improve convergence in deep neural networks, and even add useful inductive biases. In many vision applications the local spatial context of the features is important, but most common normalization schemes including Group Normalization (GN), Instance Normalization (IN), and Layer Normalization (LN) normalize over the entire spatial dimension of a feature. This can wash out important signals and degrade performance. For example, in applications that use satellite imagery, input images can be arbitrarily large; consequently, it is nonsensical to normalize over the entire area. Positional Normalization (PN), on the other hand, only normalizes over a single spatial position at a time. A natural compromise is to normalize features by local context, while also taking into account group level information. In this paper, we propose Local Context Normalization (LCN): a normalization layer where every feature is normalized based on a window around it and the filters in its group. We propose an algorithmic solution to make LCN efficient for arbitrary window sizes, even if every point in the image has a unique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN counterparts for object detection, semantic segmentation, and instance segmentation applications in several benchmark datasets, while keeping performance independent of the batch size and facilitating transfer learning.

</details>

<details>

<summary>2020-05-09 12:13:43 - Multi-Task Learning in Histo-pathology for Widely Generalizable Model</summary>

- *Jevgenij Gamper, Navid Alemi Kooohbanani, Nasir Rajpoot*

- `2005.08645v1` - [abs](http://arxiv.org/abs/2005.08645v1) - [pdf](http://arxiv.org/pdf/2005.08645v1)

> In this work we show preliminary results of deep multi-task learning in the area of computational pathology. We combine 11 tasks ranging from patch-wise oral cancer classification, one of the most prevalent cancers in the developing world, to multi-tissue nuclei instance segmentation and classification.

</details>

<details>

<summary>2020-05-09 18:49:28 - Interpreting Verbal Irony: Linguistic Strategies and the Connection to the Type of Semantic Incongruity</summary>

- *Debanjan Ghosh, Elena Musi, Kartikeya Upasani, Smaranda Muresan*

- `1911.00891v3` - [abs](http://arxiv.org/abs/1911.00891v3) - [pdf](http://arxiv.org/pdf/1911.00891v3)

> Human communication often involves the use of verbal irony or sarcasm, where the speakers usually mean the opposite of what they say. To better understand how verbal irony is expressed by the speaker and interpreted by the hearer we conduct a crowdsourcing task: given an utterance expressing verbal irony, users are asked to verbalize their interpretation of the speaker's ironic message. We propose a typology of linguistic strategies for verbal irony interpretation and link it to various theoretical linguistic frameworks. We design computational models to capture these strategies and present empirical studies aimed to answer three questions: (1) what is the distribution of linguistic strategies used by hearers to interpret ironic messages?; (2) do hearers adopt similar strategies for interpreting the speaker's ironic intent?; and (3) does the type of semantic incongruity in the ironic message (explicit vs. implicit) influence the choice of interpretation strategies by the hearers?

</details>

<details>

<summary>2020-05-10 06:14:59 - Maximal Algorithmic Caliber and Algorithmic Causal Network Inference: General Principles of Real-World General Intelligence?</summary>

- *Ben Goertzel*

- `2005.04589v1` - [abs](http://arxiv.org/abs/2005.04589v1) - [pdf](http://arxiv.org/pdf/2005.04589v1)

> Ideas and formalisms from far-from-equilibrium thermodynamics are ported to the context of stochastic computational processes, via following and extending Tadaki's algorithmic thermodynamics. A Principle of Maximum Algorithmic Caliber is proposed, providing guidance as to what computational processes one should hypothesize if one is provided constraints to work within. It is conjectured that, under suitable assumptions, computational processes obeying algorithmic Markov conditions will maximize algorithmic caliber. It is proposed that in accordance with this, real-world cognitive systems may operate in substantial part by modeling their environments and choosing their actions to be (approximate and compactly represented) algorithmic Markov networks. These ideas are suggested as potential early steps toward a general theory of the operation of pragmatic generally intelligent systems.

</details>

<details>

<summary>2020-05-10 14:35:43 - Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics</summary>

- *Guy Emerson*

- `2005.02991v2` - [abs](http://arxiv.org/abs/2005.02991v2) - [pdf](http://arxiv.org/pdf/2005.02991v2)

> Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge. In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference. This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.

</details>

<details>

<summary>2020-05-10 17:37:38 - Knowledge Graph semantic enhancement of input data for improving AI</summary>

- *Shreyansh Bhatt, Amit Sheth, Valerie Shalin, Jinjin Zhao*

- `2005.04726v1` - [abs](http://arxiv.org/abs/2005.04726v1) - [pdf](http://arxiv.org/pdf/2005.04726v1)

> Intelligent systems designed using machine learning algorithms require a large number of labeled data. Background knowledge provides complementary, real world factual information that can augment the limited labeled data to train a machine learning algorithm. The term Knowledge Graph (KG) is in vogue as for many practical applications, it is convenient and useful to organize this background knowledge in the form of a graph. Recent academic research and implemented industrial intelligent systems have shown promising performance for machine learning algorithms that combine training data with a knowledge graph. In this article, we discuss the use of relevant KGs to enhance input data for two applications that use machine learning -- recommendation and community detection. The KG improves both accuracy and explainability.

</details>

<details>

<summary>2020-05-11 06:19:01 - Hierarchical Attention Transformer Architecture For Syntactic Spell Correction</summary>

- *Abhishek Niranjan, M Ali Basha Shaik, Kushal Verma*

- `2005.04876v1` - [abs](http://arxiv.org/abs/2005.04876v1) - [pdf](http://arxiv.org/pdf/2005.04876v1)

> The attention mechanisms are playing a boosting role in advancements in sequence-to-sequence problems. Transformer architecture achieved new state of the art results in machine translation, and it's variants are since being introduced in several other sequence-to-sequence problems. Problems which involve a shared vocabulary, can benefit from the similar semantic and syntactic structure in the source and target sentences. With the motivation of building a reliable and fast post-processing textual module to assist all the text-related use cases in mobile phones, we take on the popular spell correction problem. In this paper, we propose multi encoder-single decoder variation of conventional transformer. Outputs from the three encoders with character level 1-gram, 2-grams and 3-grams inputs are attended in hierarchical fashion in the decoder. The context vectors from the encoders clubbed with self-attention amplify the n-gram properties at the character level and helps in accurate decoding. We demonstrate our model on spell correction dataset from Samsung Research, and report significant improvement of 0.11\%, 0.32\% and 0.69\% in character (CER), word (WER) and sentence (SER) error rates from existing state-of-the-art machine-translation architectures. Our architecture is also trains ~7.8 times faster, and is only about 1/3 in size from the next most accurate model.

</details>

<details>

<summary>2020-05-11 08:20:26 - Learning to hash with semantic similarity metrics and empirical KL divergence</summary>

- *Heikki Arponen, Tom E. Bishop*

- `2005.04917v1` - [abs](http://arxiv.org/abs/2005.04917v1) - [pdf](http://arxiv.org/pdf/2005.04917v1)

> Learning to hash is an efficient paradigm for exact and approximate nearest neighbor search from massive databases. Binary hash codes are typically extracted from an image by rounding output features from a CNN, which is trained on a supervised binary similar/ dissimilar task. Drawbacks of this approach are: (i) resulting codes do not necessarily capture semantic similarity of the input data (ii) rounding results in information loss, manifesting as decreased retrieval performance and (iii) Using only class-wise similarity as a target can lead to trivial solutions, simply encoding classifier outputs rather than learning more intricate relations, which is not detected by most performance metrics. We overcome (i) via a novel loss function encouraging the relative hash code distances of learned features to match those derived from their targets. We address (ii) via a differentiable estimate of the KL divergence between network outputs and a binary target distribution, resulting in minimal information loss when the features are rounded to binary. Finally, we resolve (iii) by focusing on a hierarchical precision metric. Efficiency of the methods is demonstrated with semantic image retrieval on the CIFAR-100, ImageNet and Conceptual Captions datasets, using similarities inferred from the WordNet label hierarchy or sentence embeddings.

</details>

<details>

<summary>2020-05-11 08:44:02 - KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis</summary>

- *Deepanway Ghosal, Devamanyu Hazarika, Abhinaba Roy, Navonil Majumder, Rada Mihalcea, Soujanya Poria*

- `2005.00791v2` - [abs](http://arxiv.org/abs/2005.00791v2) - [pdf](http://arxiv.org/pdf/2005.00791v2)

> Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.

</details>

<details>

<summary>2020-05-11 08:51:30 - Towards logical negation for compositional distributional semantics</summary>

- *Martha Lewis*

- `2005.04929v1` - [abs](http://arxiv.org/abs/2005.04929v1) - [pdf](http://arxiv.org/pdf/2005.04929v1)

> The categorical compositional distributional model of meaning gives the composition of words into phrases and sentences pride of place. However, it has so far lacked a model of logical negation. This paper gives some steps towards providing this operator, modelling it as a version of projection onto the subspace orthogonal to a word. We give a small demonstration of the operators performance in a sentence entailment task.

</details>

<details>

<summary>2020-05-11 13:56:58 - Evaluating Sparse Interpretable Word Embeddings for Biomedical Domain</summary>

- *Mohammad Amin Samadi, Mohammad Sadegh Akhondzadeh, Sayed Jalal Zahabi, Mohammad Hossein Manshaei, Zeinab Maleki, Payman Adibi*

- `2005.05114v1` - [abs](http://arxiv.org/abs/2005.05114v1) - [pdf](http://arxiv.org/pdf/2005.05114v1)

> Word embeddings have found their way into a wide range of natural language processing tasks including those in the biomedical domain. While these vector representations successfully capture semantic and syntactic word relations, hidden patterns and trends in the data, they fail to offer interpretability. Interpretability is a key means to justification which is an integral part when it comes to biomedical applications. We present an inclusive study on interpretability of word embeddings in the medical domain, focusing on the role of sparse methods. Qualitative and quantitative measurements and metrics for interpretability of word vector representations are provided. For the quantitative evaluation, we introduce an extensive categorized dataset that can be used to quantify interpretability based on category theory. Intrinsic and extrinsic evaluation of the studied methods are also presented. As for the latter, we propose datasets which can be utilized for effective extrinsic evaluation of word vectors in the biomedical domain. Based on our experiments, it is seen that sparse word vectors show far more interpretability while preserving the performance of their original vectors in downstream tasks.

</details>

<details>

<summary>2020-05-11 16:31:08 - Commonsense Evidence Generation and Injection in Reading Comprehension</summary>

- *Ye Liu, Tao Yang, Zeyu You, Wei Fan, Philip S. Yu*

- `2005.05240v1` - [abs](http://arxiv.org/abs/2005.05240v1) - [pdf](http://arxiv.org/pdf/2005.05240v1)

> Human tackle reading comprehension not only based on the given context itself but often rely on the commonsense beyond. To empower the machine with commonsense reasoning, in this paper, we propose a Commonsense Evidence Generation and Injection framework in reading comprehension, named CEGI. The framework injects two kinds of auxiliary commonsense evidence into comprehensive reading to equip the machine with the ability of rational thinking. Specifically, we build two evidence generators: the first generator aims to generate textual evidence via a language model; the other generator aims to extract factual evidence (automatically aligned text-triples) from a commonsense knowledge graph after graph completion. Those evidences incorporate contextual commonsense and serve as the additional inputs to the model. Thereafter, we propose a deep contextual encoder to extract semantic relationships among the paragraph, question, option, and evidence. Finally, we employ a capsule network to extract different linguistic units (word and phrase) from the relations, and dynamically predict the optimal option based on the extracted units. Experiments on the CosmosQA dataset demonstrate that the proposed CEGI model outperforms the current state-of-the-art approaches and achieves the accuracy (83.6%) on the leaderboard.

</details>

<details>

<summary>2020-05-11 20:19:13 - Shared task: Lexical semantic change detection in German (Student Project Report)</summary>

- *Adnan Ahmad, Kiflom Desta, Fabian Lang, Dominik Schlechtweg*

- `2001.07786v2` - [abs](http://arxiv.org/abs/2001.07786v2) - [pdf](http://arxiv.org/pdf/2001.07786v2)

> Recent NLP architectures have illustrated in various ways how semantic change can be captured across time and domains. However, in terms of evaluation there is a lack of benchmarks to compare the performance of these systems against each other. We present the results of the first shared task on unsupervised lexical semantic change detection (LSCD) in German based on the evaluation framework proposed by Schlechtweg et al. (2019).

</details>

<details>

<summary>2020-05-12 00:02:25 - Scones: Towards Conversational Authoring of Sketches</summary>

- *Forrest Huang, Eldon Schoop, David Ha, John Canny*

- `2005.07781v1` - [abs](http://arxiv.org/abs/2005.07781v1) - [pdf](http://arxiv.org/pdf/2005.07781v1)

> Iteratively refining and critiquing sketches are crucial steps to developing effective designs. We introduce Scones, a mixed-initiative, machine-learning-driven system that enables users to iteratively author sketches from text instructions. Scones is a novel deep-learning-based system that iteratively generates scenes of sketched objects composed with semantic specifications from natural language. Scones exceeds state-of-the-art performance on a text-based scene modification task, and introduces a mask-conditioned sketching model that can generate sketches with poses specified by high-level scene information. In an exploratory user evaluation of Scones, participants reported enjoying an iterative drawing task with Scones, and suggested additional features for further applications. We believe Scones is an early step towards automated, intelligent systems that support human-in-the-loop applications for communicating ideas through sketching in art and design.

</details>

<details>

<summary>2020-05-12 03:44:09 - Four-valued monitorability of $ω$-regular languages</summary>

- *Zhe Chen, Yunyun Chen, Robert M. Hierons, Yifan Wu*

- `2002.06737v2` - [abs](http://arxiv.org/abs/2002.06737v2) - [pdf](http://arxiv.org/pdf/2002.06737v2)

> Runtime Verification (RV) is a lightweight formal technique in which program or system execution is monitored and analyzed, to check whether certain properties are satisfied or violated after a finite number of steps. The use of RV has led to interest in deciding whether a property is monitorable: whether it is always possible for the satisfaction or violation of the property to be determined after a finite future continuation. However, classical two-valued monitorability suffers from two inherent limitations. First, a property can only be evaluated as monitorable or non-monitorable; no information is available regarding whether only one verdict (satisfaction or violation) can be detected. Second, monitorability is defined at the language-level and does not tell us whether satisfaction or violation can be detected starting from the current monitor state during system execution.   To address these limitations, this paper proposes a new notion of four-valued monitorability for $\omega$-languages and applies it at the state-level. Four-valued monitorability is more informative than two-valued monitorability as a property can be evaluated as a four-valued result, denoting that only satisfaction, only violation, or both are active for a monitorable property. We can also compute state-level weak monitorability, i.e., whether satisfaction or violation can be detected starting from a given state in a monitor, which enables state-level optimizations of monitoring algorithms. Based on a new six-valued semantics, we propose procedures for computing four-valued monitorability of $\omega$-regular languages, both at the language-level and at the state-level. We have developed a new tool that implements the proposed procedure for computing monitorability of LTL formulas.

</details>

<details>

<summary>2020-05-12 07:03:46 - Multi-Channel Transfer Learning of Chest X-ray Images for Screening of COVID-19</summary>

- *Sampa Misra, Seungwan Jeon, Seiyon Lee, Ravi Managuli, Chulhong Kim*

- `2005.05576v1` - [abs](http://arxiv.org/abs/2005.05576v1) - [pdf](http://arxiv.org/pdf/2005.05576v1)

> The 2019 novel coronavirus (COVID-19) has spread rapidly all over the world and it is affecting the whole society. The current gold standard test for screening COVID-19 patients is the polymerase chain reaction test. However, the COVID-19 test kits are not widely available and time-consuming. Thus, as an alternative, chest X-rays are being considered for quick screening. Since the presentation of COVID-19 in chest X-rays is varied in features and specialization in reading COVID-19 chest X-rays are required thus limiting its use for diagnosis. To address this challenge of reading chest X-rays by radiologists quickly, we present a multi-channel transfer learning model based on ResNet architecture to facilitate the diagnosis of COVID-19 chest X-ray. Three ResNet-based models (Models a, b, and c) were retrained using Dataset_A (1579 normal and 4429 diseased), Dataset_B (4245 pneumonia and 1763 non-pneumonia), and Dataset_C (184 COVID-19 and 5824 Non-COVID19), respectively, to classify (a) normal or diseased, (b) pneumonia or non-pneumonia, and (c) COVID-19 or non-COVID19. Finally, these three models were ensembled and fine-tuned using Dataset_D (1579 normal, 4245 pneumonia, and 184 COVID-19) to classify normal, pneumonia, and COVID-19 cases. Our results show that the ensemble model is more accurate than the single ResNet model, which is also re-trained using Dataset_D as it extracts more relevant semantic features for each class. Our approach provides a precision of 94 % and a recall of 100%. Thus, our method could potentially help clinicians in screening patients for COVID-19, thus facilitating immediate triaging and treatment for better outcomes.

</details>

<details>

<summary>2020-05-12 10:03:20 - Very High Resolution Land Cover Mapping of Urban Areas at Global Scale with Convolutional Neural Networks</summary>

- *Thomas Tilak, Arnaud Braun, David Chandler, Nicolas David, Sylvain Galopin, Amélie Lombard, Michaël Michaud, Camille Parisel, Matthieu Porte, Marjorie Robert*

- `2005.05652v1` - [abs](http://arxiv.org/abs/2005.05652v1) - [pdf](http://arxiv.org/pdf/2005.05652v1)

> This paper describes a methodology to produce a 7-classes land cover map of urban areas from very high resolution images and limited noisy labeled data. The objective is to make a segmentation map of a large area (a french department) with the following classes: asphalt, bare soil, building, grassland, mineral material (permeable artificialized areas), forest and water from 20cm aerial images and Digital Height Model. We created a training dataset on a few areas of interest aggregating databases, semi-automatic classification, and manual annotation to get a complete ground truth in each class. A comparative study of different encoder-decoder architectures (U-Net, U-Net with Resnet encoders, Deeplab v3+) is presented with different loss functions. The final product is a highly valuable land cover map computed from model predictions stitched together, binarized, and refined before vectorization.

</details>

<details>

<summary>2020-05-12 11:54:56 - RetinotopicNet: An Iterative Attention Mechanism Using Local Descriptors with Global Context</summary>

- *Thomas Kurbiel, Shahrzad Khaleghian*

- `2005.05701v1` - [abs](http://arxiv.org/abs/2005.05701v1) - [pdf](http://arxiv.org/pdf/2005.05701v1)

> Convolutional Neural Networks (CNNs) were the driving force behind many advancements in Computer Vision research in recent years. This progress has spawned many practical applications and we see an increased need to efficiently move CNNs to embedded systems today. However traditional CNNs lack the property of scale and rotation invariance: two of the most frequently encountered transformations in natural images. As a consequence CNNs have to learn different features for same objects at different scales. This redundancy is the main reason why CNNs need to be very deep in order to achieve the desired accuracy. In this paper we develop an efficient solution by reproducing how nature has solved the problem in the human brain. To this end we let our CNN operate on small patches extracted using the log-polar transform, which is known to be scale and rotation equivariant. Patches extracted in this way have the nice property of magnifying the central field and compressing the periphery. Hence we obtain local descriptors with global context information. However the processing of a single patch is usually not sufficient to achieve high accuracies in e.g. classification tasks. We therefore successively jump to several different locations, called saccades, thus building an understanding of the whole image. Since log-polar patches contain global context information, we can efficiently calculate following saccades using only the small patches. Saccades efficiently compensate for the lack of translation equivariance of the log-polar transform.

</details>

<details>

<summary>2020-05-12 12:31:27 - Preference Elicitation in Assumption-Based Argumentation</summary>

- *Quratul-ain Mahesar, Nir Oren, Wamberto W. Vasconcelos*

- `2005.05721v1` - [abs](http://arxiv.org/abs/2005.05721v1) - [pdf](http://arxiv.org/pdf/2005.05721v1)

> Various structured argumentation frameworks utilize preferences as part of their standard inference procedure to enable reasoning with preferences. In this paper, we consider an inverse of the standard reasoning problem, seeking to identify what preferences over assumptions could lead to a given set of conclusions being drawn. We ground our work in the Assumption-Based Argumentation (ABA) framework, and present an algorithm which computes and enumerates all possible sets of preferences over the assumptions in the system from which a desired conflict free set of conclusions can be obtained under a given semantic. After describing our algorithm, we establish its soundness, completeness and complexity.

</details>

<details>

<summary>2020-05-12 17:10:13 - Semantic Scaffolds for Pseudocode-to-Code Generation</summary>

- *Ruiqi Zhong, Mitchell Stern, Dan Klein*

- `2005.05927v1` - [abs](http://arxiv.org/abs/2005.05927v1) - [pdf](http://arxiv.org/pdf/2005.05927v1)

> We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.

</details>

<details>

<summary>2020-05-12 21:03:29 - Class-Incremental Learning for Semantic Segmentation Re-Using Neither Old Data Nor Old Labels</summary>

- *Marvin Klingner, Andreas Bär, Philipp Donn, Tim Fingscheidt*

- `2005.06050v1` - [abs](http://arxiv.org/abs/2005.06050v1) - [pdf](http://arxiv.org/pdf/2005.06050v1)

> While neural networks trained for semantic segmentation are essential for perception in autonomous driving, most current algorithms assume a fixed number of classes, presenting a major limitation when developing new autonomous driving systems with the need of additional classes. In this paper we present a technique implementing class-incremental learning for semantic segmentation without using the labeled data the model was initially trained on. Previous approaches still either rely on labels for both old and new classes, or fail to properly distinguish between them. We show how to overcome these problems with a novel class-incremental learning technique, which nonetheless requires labels only for the new classes. Specifically, (i) we introduce a new loss function that neither relies on old data nor on old labels, (ii) we show how new classes can be integrated in a modular fashion into pretrained semantic segmentation models, and finally (iii) we re-implement previous approaches in a unified setting to compare them to ours. We evaluate our method on the Cityscapes dataset, where we exceed the mIoU performance of all baselines by 3.5% absolute reaching a result, which is only 2.2% absolute below the upper performance limit of single-stage training, relying on all data and labels simultaneously.

</details>

<details>

<summary>2020-05-13 09:40:11 - eThor: Practical and Provably Sound Static Analysis of Ethereum Smart Contracts</summary>

- *Clara Schneidewind, Ilya Grishchenko, Markus Scherer, Matteo Maffei*

- `2005.06227v1` - [abs](http://arxiv.org/abs/2005.06227v1) - [pdf](http://arxiv.org/pdf/2005.06227v1)

> Ethereum has emerged as the most popular smart contract development platform, with hundreds of thousands of contracts stored on the blockchain and covering a variety of application scenarios, such as auctions, trading platforms, and so on. Given their financial nature, security vulnerabilities may lead to catastrophic consequences and, even worse, they can be hardly fixed as data stored on the blockchain, including the smart contract code itself, are immutable. An automated security analysis of these contracts is thus of utmost interest, but at the same time technically challenging for a variety of reasons, such as the specific transaction-oriented programming mechanisms, which feature a subtle semantics, and the fact that the blockchain data which the contract under analysis interacts with, including the code of callers and callees, are not statically known.   In this work, we present eThor, the first sound and automated static analyzer for EVM bytecode, which is based on an abstraction of the EVM bytecode semantics based on Horn clauses. In particular, our static analysis supports reachability properties, which we show to be sufficient for capturing interesting security properties for smart contracts (e.g., single-entrancy) as well as contract-specific functional properties. Our analysis is proven sound against a complete semantics of EVM bytecode and an experimental large-scale evaluation on real-world contracts demonstrates that eThor is practical and outperforms the state-of-the-art static analyzers: specifically, eThor is the only one to provide soundness guarantees, terminates on 95% of a representative set of real-world contracts, and achieves an F-measure (which combines sensitivity and specificity) of 89%.

</details>

<details>

<summary>2020-05-13 16:12:20 - Controlled Crowdsourcing for High-Quality QA-SRL Annotation</summary>

- *Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer, Ido Dagan*

- `1911.03243v2` - [abs](http://arxiv.org/abs/1911.03243v2) - [pdf](http://arxiv.org/pdf/1911.03243v2)

> Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.

</details>

<details>

<summary>2020-05-13 17:21:27 - On Embeddings in Relational Databases</summary>

- *Siddhant Arora, Srikanta Bedathur*

- `2005.06437v1` - [abs](http://arxiv.org/abs/2005.06437v1) - [pdf](http://arxiv.org/pdf/2005.06437v1)

> We address the problem of learning a distributed representation of entities in a relational database using a low-dimensional embedding. Low-dimensional embeddings aim to encapsulate a concise vector representation for an underlying dataset with minimum loss of information. Embeddings across entities in a relational database have been less explored due to the intricate data relations and representation complexity involved. Relational databases are an inter-weaved collection of relations that not only model relationships between entities but also record complex domain-specific quantitative and temporal attributes of data defining complex relationships among entities. Recent methods for learning an embedding constitute of a naive approach to consider complete denormalization of the database by materializing the full join of all tables and representing as a knowledge graph. This popular approach has certain limitations as it fails to capture the inter-row relationships and additional semantics encoded in the relational databases. In this paper we demonstrate; a better methodology for learning representations by exploiting the underlying semantics of columns in a table while using the relation joins and the latent inter-row relationships. Empirical results over a real-world database with evaluations on similarity join and table completion tasks support our proposition.

</details>

<details>

<summary>2020-05-13 18:34:15 - Many-Objective Software Remodularization using NSGA-III</summary>

- *Mohamed Wiem Mkaouer, Marouane Kessentini, Adnan Shaout, Patrice Koligheu, Slim Bechikh, Kalyanmoy Deb, Ali Ouni*

- `2005.06510v1` - [abs](http://arxiv.org/abs/2005.06510v1) - [pdf](http://arxiv.org/pdf/2005.06510v1)

> Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this paper, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and re-use the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.

</details>

<details>

<summary>2020-05-13 21:00:54 - Fashion Recommendation and Compatibility Prediction Using Relational Network</summary>

- *Maryam Moosaei, Yusan Lin, Hao Yang*

- `2005.06584v1` - [abs](http://arxiv.org/abs/2005.06584v1) - [pdf](http://arxiv.org/pdf/2005.06584v1)

> Fashion is an inherently visual concept and computer vision and artificial intelligence (AI) are playing an increasingly important role in shaping the future of this domain. Many research has been done on recommending fashion products based on the learned user preferences. However, in addition to recommending single items, AI can also help users create stylish outfits from items they already have, or purchase additional items that go well with their current wardrobe. Compatibility is the key factor in creating stylish outfits from single items. Previous studies have mostly focused on modeling pair-wise compatibility. There are a few approaches that consider an entire outfit, but these approaches have limitations such as requiring rich semantic information, category labels, and fixed order of items. Thus, they fail to effectively determine compatibility when such information is not available. In this work, we adopt a Relation Network (RN) to develop new compatibility learning models, Fashion RN and FashionRN-VSE, that addresses the limitations of existing approaches. FashionRN learns the compatibility of an entire outfit, with an arbitrary number of items, in an arbitrary order. We evaluated our model using a large dataset of 49,740 outfits that we collected from Polyvore website. Quantitatively, our experimental results demonstrate state of the art performance compared with alternative methods in the literature in both compatibility prediction and fill-in-the-blank test. Qualitatively, we also show that the item embedding learned by FashionRN indicate the compatibility among fashion items.

</details>

<details>

<summary>2020-05-13 21:06:59 - PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction</summary>

- *Majid Asgari-Bidhendi, Mehrdad Nasser, Behrooz Janfada, Behrouz Minaei-Bidgoli*

- `2005.06588v1` - [abs](http://arxiv.org/abs/2005.06588v1) - [pdf](http://arxiv.org/pdf/2005.06588v1)

> Relation extraction is the task of extracting semantic relations between entities in a sentence. It is an essential part of some natural language processing tasks such as information extraction, knowledge extraction, and knowledge base population. The main motivations of this research stem from a lack of a dataset for relation extraction in the Persian language as well as the necessity of extracting knowledge from the growing big-data in the Persian language for different applications. In this paper, we present "PERLEX" as the first Persian dataset for relation extraction, which is an expert-translated version of the "Semeval-2010-Task-8" dataset. Moreover, this paper addresses Persian relation extraction utilizing state-of-the-art language-agnostic algorithms. We employ six different models for relation extraction on the proposed bilingual dataset, including a non-neural model (as the baseline), three neural models, and two deep learning models fed by multilingual-BERT contextual word representations. The experiments result in the maximum f-score 77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation extraction in the Persian language.

</details>

<details>

<summary>2020-05-13 22:01:47 - Extended 2D Consensus Hippocampus Segmentation</summary>

- *Diedre Carmo, Bruna Silva, Clarissa Yasuda, Letícia Rittner, Roberto Lotufo*

- `1902.04487v5` - [abs](http://arxiv.org/abs/1902.04487v5) - [pdf](http://arxiv.org/pdf/1902.04487v5)

> Hippocampus segmentation plays a key role in diagnosing various brain disorders such as Alzheimer's disease, epilepsy, multiple sclerosis, cancer, depression and others. Nowadays, segmentation is still mainly performed manually by specialists. Segmentation done by experts is considered to be a gold-standard when evaluating automated methods, buts it is a time consuming and arduos task, requiring specialized personnel. In recent years, efforts have been made to achieve reliable automated segmentation. For years the best performing authomatic methods were multi atlas based with around 80-85% Dice coefficient and very time consuming, but machine learning methods are recently rising with promising time and accuracy performance. A method for volumetric hippocampus segmentation is presented, based on the consensus of tri-planar U-Net inspired fully convolutional networks (FCNNs), with some modifications, including residual connections, VGG weight transfers, batch normalization and a patch extraction technique employing data from neighbor patches. A study on the impact of our modifications to the classical U-Net architecture was performed. Our method achieves cutting edge performance in our dataset, with around 96% volumetric Dice accuracy in our test data. In a public validation dataset, HARP, we achieve 87.48% DICE. GPU execution time is in the order of seconds per volume, and source code is publicly available. Also, masks are shown to be similar to other recent state-of-the-art hippocampus segmentation methods in a third dataset, without manual annotations.

</details>

<details>

<summary>2020-05-13 23:44:17 - Towards Robustifying NLI Models Against Lexical Dataset Biases</summary>

- *Xiang Zhou, Mohit Bansal*

- `2005.04732v2` - [abs](http://arxiv.org/abs/2005.04732v2) - [pdf](http://arxiv.org/pdf/2005.04732v2)

> While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy. Our code and data are available at: https://github.com/owenzx/LexicalDebias-ACL2020

</details>

<details>

<summary>2020-05-14 06:28:12 - Relevance in Structured Argumentation</summary>

- *AnneMarie Borg, Christian Straßer*

- `1809.04861v2` - [abs](http://arxiv.org/abs/1809.04861v2) - [pdf](http://arxiv.org/pdf/1809.04861v2)

> We study properties related to relevance in non-monotonic consequence relations obtained by systems of structured argumentation. Relevance desiderata concern the robustness of a consequence relation under the addition of irrelevant information. For an account of what (ir)relevance amounts to we use syntactic and semantic considerations. Syntactic criteria have been proposed in the domain of relevance logic and were recently used in argumentation theory under the names of non-interference and crash-resistance. The basic idea is that the conclusions of a given argumentative theory should be robust under adding information that shares no propositional variables with the original database. Some semantic relevance criteria are known from non-monotonic logic. For instance, cautious monotony states that if we obtain certain conclusions from an argumentation theory, we may expect to still obtain the same conclusions if we add some of them to the given database. In this paper we investigate properties of structured argumentation systems that warrant relevance desiderata.

</details>

<details>

<summary>2020-05-14 12:09:23 - DRTS Parsing with Structure-Aware Encoding and Decoding</summary>

- *Qiankun Fu, Yue Zhang, Jiangming Liu, Meishan Zhang*

- `2005.06901v1` - [abs](http://arxiv.org/abs/2005.06901v1) - [pdf](http://arxiv.org/pdf/2005.06901v1)

> Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.

</details>

<details>

<summary>2020-05-14 16:15:58 - ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages</summary>

- *Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi*

- `2005.07105v1` - [abs](http://arxiv.org/abs/2005.07105v1) - [pdf](http://arxiv.org/pdf/2005.07105v1)

> In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for "zero-shot" open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.

</details>

<details>

<summary>2020-05-14 16:17:39 - A Novel CNet-assisted Evolutionary Level Repairer and Its Applications to Super Mario Bros</summary>

- *Tianye Shu, Ziqi Wang, Jialin Liu, Xin Yao*

- `2005.06148v2` - [abs](http://arxiv.org/abs/2005.06148v2) - [pdf](http://arxiv.org/pdf/2005.06148v2)

> Applying latent variable evolution to game level design has become more and more popular as little human expert knowledge is required. However, defective levels with illegal patterns may be generated due to the violation of constraints for level design. A traditional way of repairing the defective levels is programming specific rule-based repairers to patch the flaw. However, programming these constraints is sometimes complex and not straightforward. An autonomous level repairer which is capable of learning the constraints is needed. In this paper, we propose a novel approach, CNet, to learn the probability distribution of tiles giving its surrounding tiles on a set of real levels, and then detect the illegal tiles in generated new levels. Then, an evolutionary repairer is designed to search for optimal replacement schemes equipped with a novel search space being constructed with the help of CNet and a novel heuristic function. The proposed approaches are proved to be effective in our case study of repairing GAN-generated and artificially destroyed levels of Super Mario Bros. game. Our CNet-assisted evolutionary repairer can also be easily applied to other games of which the levels can be represented by a matrix of objects or tiles.

</details>

<details>

<summary>2020-05-14 18:51:53 - Knowledge-Based Matching of $n$-ary Tuples</summary>

- *Pierre Monnin, Miguel Couceiro, Amedeo Napoli, Adrien Coulet*

- `2002.08103v2` - [abs](http://arxiv.org/abs/2002.08103v2) - [pdf](http://arxiv.org/pdf/2002.08103v2)

> An increasing number of data and knowledge sources are accessible by human and software agents in the expanding Semantic Web. Sources may differ in granularity or completeness, and thus be complementary. Consequently, they should be reconciled in order to unlock the full potential of their conjoint knowledge. In particular, units should be matched within and across sources, and their level of relatedness should be classified into equivalent, more specific, or similar. This task is challenging since knowledge units can be heterogeneously represented in sources (e.g., in terms of vocabularies). In this paper, we focus on matching n-ary tuples in a knowledge base with a rule-based methodology. To alleviate heterogeneity issues, we rely on domain knowledge expressed by ontologies. We tested our method on the biomedical domain of pharmacogenomics by searching alignments among 50,435 n-ary tuples from four different real-world sources. Results highlight noteworthy agreements and particularities within and across sources.

</details>

<details>

<summary>2020-05-14 20:02:06 - Verification of Deep Convolutional Neural Networks Using ImageStars</summary>

- *Hoang-Dung Tran, Stanley Bak, Weiming Xiang, Taylor T. Johnson*

- `2004.05511v2` - [abs](http://arxiv.org/abs/2004.05511v2) - [pdf](http://arxiv.org/pdf/2004.05511v2)

> Convolutional Neural Networks (CNN) have redefined the state-of-the-art in many real-world applications, such as facial recognition, image classification, human pose estimation, and semantic segmentation. Despite their success, CNNs are vulnerable to adversarial attacks, where slight changes to their inputs may lead to sharp changes in their output in even well-trained networks. Set-based analysis methods can detect or prove the absence of bounded adversarial attacks, which can then be used to evaluate the effectiveness of neural network training methodology. Unfortunately, existing verification approaches have limited scalability in terms of the size of networks that can be analyzed.   In this paper, we describe a set-based framework that successfully deals with real-world CNNs, such as VGG16 and VGG19, that have high accuracy on ImageNet. Our approach is based on a new set representation called the ImageStar, which enables efficient exact and over-approximative analysis of CNNs. ImageStars perform efficient set-based analysis by combining operations on concrete images with linear programming (LP). Our approach is implemented in a tool called NNV, and can verify the robustness of VGG networks with respect to a small set of input states, derived from adversarial attacks, such as the DeepFool attack. The experimental results show that our approach is less conservative and faster than existing zonotope methods, such as those used in DeepZ, and the polytope method used in DeepPoly.

</details>

<details>

<summary>2020-05-15 10:30:32 - HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep Learning Techniques on HTML Analysis</summary>

- *Chidimma Opara, Bo Wei, Yingke Chen*

- `1909.01135v3` - [abs](http://arxiv.org/abs/1909.01135v3) - [pdf](http://arxiv.org/pdf/1909.01135v3)

> Recently, the development and implementation of phishing attacks require little technical skills and costs. This uprising has led to an ever-growing number of phishing attacks on the World Wide Web. Consequently, proactive techniques to fight phishing attacks have become extremely necessary. In this paper, we propose HTMLPhish, a deep learning based data-driven end-to-end automatic phishing web page classification approach. Specifically, HTMLPhish receives the content of the HTML document of a web page and employs Convolutional Neural Networks (CNNs) to learn the semantic dependencies in the textual contents of the HTML. The CNNs learn appropriate feature representations from the HTML document embeddings without extensive manual feature engineering. Furthermore, our proposed approach of the concatenation of the word and character embeddings allows our model to manage new features and ensure easy extrapolation to test data. We conduct comprehensive experiments on a dataset of more than 50,000 HTML documents that provides a distribution of phishing to benign web pages obtainable in the real-world that yields over 93 percent Accuracy and True Positive Rate. Also, HTMLPhish is a completely language-independent and client-side strategy which can, therefore, conduct web page phishing detection regardless of the textual language.

</details>

<details>

<summary>2020-05-15 16:11:33 - Challenges in Emotion Style Transfer: An Exploration with a Lexical Substitution Pipeline</summary>

- *David Helbig, Enrica Troiano, Roman Klinger*

- `2005.07617v1` - [abs](http://arxiv.org/abs/2005.07617v1) - [pdf](http://arxiv.org/pdf/2005.07617v1)

> We propose the task of emotion style transfer, which is particularly challenging, as emotions (here: anger, disgust, fear, joy, sadness, surprise) are on the fence between content and style. To understand the particular difficulties of this task, we design a transparent emotion style transfer pipeline based on three steps: (1) select the words that are promising to be substituted to change the emotion (with a brute-force approach and selection based on the attention mechanism of an emotion classifier), (2) find sets of words as candidates for substituting the words (based on lexical and distributional semantics), and (3) select the most promising combination of substitutions with an objective function which consists of components for content (based on BERT sentence embeddings), emotion (based on an emotion classifier), and fluency (based on a neural language model). This comparably straight-forward setup enables us to explore the task and understand in what cases lexical substitution can vary the emotional load of texts, how changes in content and style interact and if they are at odds. We further evaluate our pipeline quantitatively in an automated and an annotation study based on Tweets and find, indeed, that simultaneous adjustments of content and emotion are conflicting objectives: as we show in a qualitative analysis motivated by Scherer's emotion component model, this is particularly the case for implicit emotion expressions based on cognitive appraisal or descriptions of bodily reactions.

</details>

<details>

<summary>2020-05-15 18:33:42 - Enabling Seamless Device Association with DevLoc using Light Bulb Networks for Indoor IoT Environments</summary>

- *Michael Haus, Jörg Ott, Aaron Yi Ding*

- `2005.07731v1` - [abs](http://arxiv.org/abs/2005.07731v1) - [pdf](http://arxiv.org/pdf/2005.07731v1)

> To enable serendipitous interaction for indoor IoT environments, spontaneous device associations are of particular interest so that users set up a connection in an ad-hoc manner. Based on the similarity of light signals, our system named DevLoc takes advantage of ubiquitous light sources around us to perform continuous and seamless device grouping. We provide a configuration framework to control the spatial granularity of user's proximity by managing the lighting infrastructure through customized visible light communication. To realize either proximity-based or location-based services, we support two modes of device associations between different entities: device-to-device and device-to-area. Regarding the best performing method for device grouping, machine learning-based signal similarity performs in general best compared to distance and correlation metrics. Furthermore, we analyze patterns of device associations to improve the data privacy by recognizing semantic device groups, such as personal and stranger's devices, allowing automated data sharing policies.

</details>

<details>

<summary>2020-05-15 23:33:41 - Precise XSS detection and mitigation with Client-side Templates</summary>

- *Jose Carlos Pazos, Jean-Sebastien Legare, Ivan Beschastnikh, William Aiello*

- `2005.07826v1` - [abs](http://arxiv.org/abs/2005.07826v1) - [pdf](http://arxiv.org/pdf/2005.07826v1)

> We present XSnare, a fully client-side XSS solution, implemented as a Firefox extension. Our approach takes advantage of available previous knowledge of a web application's HTML template content, as well as the rich context available in the DOM to block XSS attacks. XSnare prevents XSS exploits by using a database of exploit descriptions, which are written with the help of previously recorded CVEs. CVEs for XSS are widely available and are one of the main ways to tackle zero-day exploits. XSnare effectively singles out potential injection points for exploits in the HTML and sanitizes content to prevent malicious payloads from appearing in the DOM.   XSnare can protect application users before application developers release patches and before server operators apply them.   We evaluated XSnare on 81 recent CVEs related to XSS attacks, and found that it defends against 94.2% of these exploits. To the best of our knowledge, XSnare is the first protection mechanism for XSS that is application-specific, and based on publicly available CVE information. We show that XSnare's specificity protects users against exploits which evade other, more generic, anti-XSS approaches.   Our performance evaluation shows that our extension's overhead on web page loading time is less than 10% for 72.6% of the sites in the Moz Top 500 list.

</details>

<details>

<summary>2020-05-16 02:17:04 - Neural Multi-Task Learning for Teacher Question Detection in Online Classrooms</summary>

- *Gale Yan Huang, Jiahao Chen, Haochen Liu, Weiping Fu, Wenbiao Ding, Jiliang Tang, Songfan Yang, Guoliang Li, Zitao Liu*

- `2005.07845v1` - [abs](http://arxiv.org/abs/2005.07845v1) - [pdf](http://arxiv.org/pdf/2005.07845v1)

> Asking questions is one of the most crucial pedagogical techniques used by teachers in class. It not only offers open-ended discussions between teachers and students to exchange ideas but also provokes deeper student thought and critical analysis. Providing teachers with such pedagogical feedback will remarkably help teachers improve their overall teaching quality over time in classrooms. Therefore, in this work, we build an end-to-end neural framework that automatically detects questions from teachers' audio recordings. Compared with traditional methods, our approach not only avoids cumbersome feature engineering, but also adapts to the task of multi-class question detection in real education scenarios. By incorporating multi-task learning techniques, we are able to strengthen the understanding of semantic relations among different types of questions. We conducted extensive experiments on the question detection tasks in a real-world online classroom dataset and the results demonstrate the superiority of our model in terms of various evaluation metrics.

</details>

<details>

<summary>2020-05-16 04:11:44 - iCapsNets: Towards Interpretable Capsule Networks for Text Classification</summary>

- *Zhengyang Wang, Xia Hu, Shuiwang Ji*

- `2006.00075v1` - [abs](http://arxiv.org/abs/2006.00075v1) - [pdf](http://arxiv.org/pdf/2006.00075v1)

> Many text classification applications require models with satisfying performance as well as good interpretability. Traditional machine learning methods are easy to interpret but have low accuracies. The development of deep learning models boosts the performance significantly. However, deep learning models are typically hard to interpret. In this work, we propose interpretable capsule networks (iCapsNets) to bridge this gap. iCapsNets use capsules to model semantic meanings and explore novel methods to increase interpretability. The design of iCapsNets is consistent with human intuition and enables it to produce human-understandable interpretation results. Notably, iCapsNets can be interpreted both locally and globally. In terms of local interpretability, iCapsNets offer a simple yet effective method to explain the predictions for each data sample. On the other hand, iCapsNets explore a novel way to explain the model's general behavior, achieving global interpretability. Experimental studies show that our iCapsNets yield meaningful local and global interpretation results, without suffering from significant performance loss compared to non-interpretable methods.

</details>

<details>

<summary>2020-05-16 05:07:59 - NIT-Agartala-NLP-Team at SemEval-2020 Task 8: Building Multimodal Classifiers to tackle Internet Humor</summary>

- *Steve Durairaj Swamy, Shubham Laddha, Basil Abdussalam, Debayan Datta, Anupam Jamatia*

- `2005.06943v2` - [abs](http://arxiv.org/abs/2005.06943v2) - [pdf](http://arxiv.org/pdf/2005.06943v2)

> The paper describes the systems submitted to SemEval-2020 Task 8: Memotion by the `NIT-Agartala-NLP-Team'. A dataset of 8879 memes was made available by the task organizers to train and test our models. Our systems include a Logistic Regression baseline, a BiLSTM + Attention-based learner and a transfer learning approach with BERT. For the three sub-tasks A, B and C, we attained ranks 24/33, 11/29 and 15/26, respectively. We highlight our difficulties in harnessing image information as well as some techniques and handcrafted features we employ to overcome these issues. We also discuss various modelling issues and theorize possible solutions and reasons as to why these problems persist.

</details>

<details>

<summary>2020-05-16 06:29:14 - Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection</summary>

- *Lei Zhong, Juan Cao, Qiang Sheng, Junbo Guo, Ziang Wang*

- `2005.07886v1` - [abs](http://arxiv.org/abs/2005.07886v1) - [pdf](http://arxiv.org/pdf/2005.07886v1)

> Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically. Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods. Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.

</details>

<details>

<summary>2020-05-16 09:47:19 - Sequential Sentence Matching Network for Multi-turn Response Selection in Retrieval-based Chatbots</summary>

- *Chao Xiong, Che Liu, Zijun Xu, Junfeng Jiang, Jieping Ye*

- `2005.07923v1` - [abs](http://arxiv.org/abs/2005.07923v1) - [pdf](http://arxiv.org/pdf/2005.07923v1)

> Recently, open domain multi-turn chatbots have attracted much interest from lots of researchers in both academia and industry. The dominant retrieval-based methods use context-response matching mechanisms for multi-turn response selection. Specifically, the state-of-the-art methods perform the context-response matching by word or segment similarity. However, these models lack a full exploitation of the sentence-level semantic information, and make simple mistakes that humans can easily avoid. In this work, we propose a matching network, called sequential sentence matching network (S2M), to use the sentence-level semantic information to address the problem. Firstly and most importantly, we find that by using the sentence-level semantic information, the network successfully addresses the problem and gets a significant improvement on matching, resulting in a state-of-the-art performance. Furthermore, we integrate the sentence matching we introduced here and the usual word similarity matching reported in the current literature, to match at different semantic levels. Experiments on three public data sets show that such integration further improves the model performance.

</details>

<details>

<summary>2020-05-16 11:11:48 - Logical Inferences with Comparatives and Generalized Quantifiers</summary>

- *Izumi Haruta, Koji Mineshima, Daisuke Bekki*

- `2005.07954v1` - [abs](http://arxiv.org/abs/2005.07954v1) - [pdf](http://arxiv.org/pdf/2005.07954v1)

> Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In this paper, we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving. We evaluate our system on three NLI datasets that contain complex logical inferences with comparatives, generalized quantifiers, and numerals. We show that the system outperforms previous logic-based systems as well as recent deep learning-based models.

</details>

<details>

<summary>2020-05-16 13:05:47 - Unsupervised Embedding-based Detection of Lexical Semantic Changes</summary>

- *Ehsaneddin Asgari, Christoph Ringlstetter, Hinrich Schütze*

- `2005.07979v1` - [abs](http://arxiv.org/abs/2005.07979v1) - [pdf](http://arxiv.org/pdf/2005.07979v1)

> This paper describes EmbLexChange, a system introduced by the "Life-Language" team for SemEval-2020 Task 1, on unsupervised detection of lexical-semantic changes. EmbLexChange is defined as the divergence between the embedding based profiles of word w (calculated with respect to a set of reference words) in the source and the target domains (source and target domains can be simply two time frames t1 and t2). The underlying assumption is that the lexical-semantic change of word w would affect its co-occurring words and subsequently alters the neighborhoods in the embedding spaces. We show that using a resampling framework for the selection of reference words, we can reliably detect lexical-semantic changes in English, German, Swedish, and Latin. EmbLexChange achieved second place in the binary detection of semantic changes in the SemEval-2020.

</details>

<details>

<summary>2020-05-16 16:31:08 - Towards classification parity across cohorts</summary>

- *Aarsh Patel, Rahul Gupta, Mukund Harakere, Satyapriya Krishna, Aman Alok, Peng Liu*

- `2005.08033v1` - [abs](http://arxiv.org/abs/2005.08033v1) - [pdf](http://arxiv.org/pdf/2005.08033v1)

> Recently, there has been a lot of interest in ensuring algorithmic fairness in machine learning where the central question is how to prevent sensitive information (e.g. knowledge about the ethnic group of an individual) from adding "unfair" bias to a learning algorithm (Feldman et al. (2015), Zemel et al. (2013)). This has led to several debiasing algorithms on word embeddings (Qian et al. (2019) , Bolukbasi et al. (2016)), coreference resolution (Zhao et al. (2018a)), semantic role labeling (Zhao et al. (2017)), etc. Most of these existing work deals with explicit sensitive features such as gender, occupations or race which doesn't work with data where such features are not captured due to privacy concerns. In this research work, we aim to achieve classification parity across explicit as well as implicit sensitive features. We define explicit cohorts as groups of people based on explicit sensitive attributes provided in the data (age, gender, race) whereas implicit cohorts are defined as groups of people with similar language usage. We obtain implicit cohorts by clustering embeddings of each individual trained on the language generated by them using a language model. We achieve two primary objectives in this work : [1.] We experimented and discovered classification performance differences across cohorts based on implicit and explicit features , [2] We improved classification parity by introducing modification to the loss function aimed to minimize the range of model performances across cohorts.

</details>

<details>

<summary>2020-05-16 21:10:10 - Single-Stage Semantic Segmentation from Image Labels</summary>

- *Nikita Araslanov, Stefan Roth*

- `2005.08104v1` - [abs](http://arxiv.org/abs/2005.08104v1) - [pdf](http://arxiv.org/pdf/2005.08104v1)

> Recent years have seen a rapid growth in new approaches improving the accuracy of semantic segmentation in a weakly supervised setting, i.e. with only image-level labels available for training. However, this has come at the cost of increased model complexity and sophisticated multi-stage training procedures. This is in contrast to earlier work that used only a single stage $-$ training one segmentation network on image labels $-$ which was abandoned due to inferior segmentation accuracy. In this work, we first define three desirable properties of a weakly supervised method: local consistency, semantic fidelity, and completeness. Using these properties as guidelines, we then develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single stage. We show that despite its simplicity, our method achieves results that are competitive with significantly more complex pipelines, substantially outperforming earlier single-stage methods.

</details>

<details>

<summary>2020-05-17 10:03:42 - Building a Hebrew Semantic Role Labeling Lexical Resource from Parallel Movie Subtitles</summary>

- *Ben Eyal, Michael Elhadad*

- `2005.08206v1` - [abs](http://arxiv.org/abs/2005.08206v1) - [pdf](http://arxiv.org/pdf/2005.08206v1)

> We present a semantic role labeling resource for Hebrew built semi-automatically through annotation projection from English. This corpus is derived from the multilingual OpenSubtitles dataset and includes short informal sentences, for which reliable linguistic annotations have been computed. We provide a fully annotated version of the data including morphological analysis, dependency syntax and semantic role labeling in both FrameNet and PropBank styles. Sentences are aligned between English and Hebrew, both sides include full annotations and the explicit mapping from the English arguments to the Hebrew ones. We train a neural SRL model on this Hebrew resource exploiting the pre-trained multilingual BERT transformer model, and provide the first available baseline model for Hebrew SRL as a reference point. The code we provide is generic and can be adapted to other languages to bootstrap SRL resources.

</details>

<details>

<summary>2020-05-17 14:18:04 - On the Combined Use of Extrinsic Semantic Resources for Medical Information Search</summary>

- *Mohammed Maree, Israa Noor, Khaled Rabayah, Mohammed Belkhatir, Saadat M. Alhashmi*

- `2005.08259v1` - [abs](http://arxiv.org/abs/2005.08259v1) - [pdf](http://arxiv.org/pdf/2005.08259v1)

> Semantic concepts and relations encoded in domain-specific ontologies and other medical semantic resources play a crucial role in deciphering terms in medical queries and documents. The exploitation of these resources for tackling the semantic gap issue has been widely studied in the literature. However, there are challenges that hinder their widespread use in real-world applications. Among these challenges is the insufficient knowledge individually encoded in existing medical ontologies, which is magnified when users express their information needs using long-winded natural language queries. In this context, many of the users query terms are either unrecognized by the used ontologies, or cause retrieving false positives that degrade the quality of current medical information search approaches. In this article, we explore the combination of multiple extrinsic semantic resources in the development of a full-fledged medical information search framework to: i) highlight and expand head medical concepts in verbose medical queries (i.e. concepts among query terms that significantly contribute to the informativeness and intent of a given query), ii) build semantically enhanced inverted index documents, iii) contribute to a heuristical weighting technique in the query document matching process. To demonstrate the effectiveness of the proposed approach, we conducted several experiments over the CLEF eHealth 2014 dataset. Findings indicate that the proposed method combining several extrinsic semantic resources proved to be more effective than related approaches in terms of precision measure.

</details>

<details>

<summary>2020-05-17 17:26:40 - TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</summary>

- *Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel*

- `2005.08314v1` - [abs](http://arxiv.org/abs/2005.08314v1) - [pdf](http://arxiv.org/pdf/2005.08314v1)

> Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider. Implementation of the model will be available at http://fburl.com/TaBERT .

</details>

<details>

<summary>2020-05-17 22:25:24 - Fixed Point Semantics for Stream Reasoning</summary>

- *Christian Antić*

- `2005.08384v1` - [abs](http://arxiv.org/abs/2005.08384v1) - [pdf](http://arxiv.org/pdf/2005.08384v1)

> Reasoning over streams of input data is an essential part of human intelligence. During the last decade {\em stream reasoning} has emerged as a research area within the AI-community with many potential applications. In fact, the increased availability of streaming data via services like Google and Facebook has raised the need for reasoning engines coping with data that changes at high rate. Recently, the rule-based formalism {\em LARS} for non-monotonic stream reasoning under the answer set semantics has been introduced. Syntactically, LARS programs are logic programs with negation incorporating operators for temporal reasoning, most notably {\em window operators} for selecting relevant time points. Unfortunately, by preselecting {\em fixed} intervals for the semantic evaluation of programs, the rigid semantics of LARS programs is not flexible enough to {\em constructively} cope with rapidly changing data dependencies. Moreover, we show that defining the answer set semantics of LARS in terms of FLP reducts leads to undesirable circular justifications similar to other ASP extensions. This paper fixes all of the aforementioned shortcomings of LARS. More precisely, we contribute to the foundations of stream reasoning by providing an operational fixed point semantics for a fully flexible variant of LARS and we show that our semantics is sound and constructive in the sense that answer sets are derivable bottom-up and free of circular justifications.

</details>

<details>

<summary>2020-05-18 06:04:58 - Text Classification with Few Examples using Controlled Generalization</summary>

- *Abhijit Mahabal, Jason Baldridge, Burcu Karagol Ayan, Vincent Perot, Dan Roth*

- `2005.08469v1` - [abs](http://arxiv.org/abs/2005.08469v1) - [pdf](http://arxiv.org/pdf/2005.08469v1)

> Training data for text classification is often limited in practice, especially for applications with many output classes or involving many related classification problems. This means classifiers must generalize from limited evidence, but the manner and extent of generalization is task dependent. Current practice primarily relies on pre-trained word embeddings to map words unseen in training to similar seen ones. Unfortunately, this squishes many components of meaning into highly restricted capacity. Our alternative begins with sparse pre-trained representations derived from unlabeled parsed corpora; based on the available training data, we select features that offers the relevant generalizations. This produces task-specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets.

</details>

<details>

<summary>2020-05-18 15:14:33 - Corpus of Chinese Dynastic Histories: Gender Analysis over Two Millennia</summary>

- *Sergey Zinin, Yang Xu*

- `2005.08793v1` - [abs](http://arxiv.org/abs/2005.08793v1) - [pdf](http://arxiv.org/pdf/2005.08793v1)

> Chinese dynastic histories form a large continuous linguistic space of approximately 2000 years, from the 3rd century BCE to the 18th century CE. The histories are documented in Classical (Literary) Chinese in a corpus of over 20 million characters, suitable for the computational analysis of historical lexicon and semantic change. However, there is no freely available open-source corpus of these histories, making Classical Chinese low-resource. This project introduces a new open-source corpus of twenty-four dynastic histories covered by Creative Commons license. An original list of Classical Chinese gender-specific terms was developed as a case study for analyzing the historical linguistic use of male and female terms. The study demonstrates considerable stability in the usage of these terms, with dominance of male terms. Exploration of word meanings uses keyword analysis of focus corpora created for genderspecific terms. This method yields meaningful semantic representations that can be used for future studies of diachronic semantics.

</details>

<details>

<summary>2020-05-18 16:39:16 - Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings</summary>

- *Katherine McCurdy, Oguz Serbetci*

- `2005.08864v1` - [abs](http://arxiv.org/abs/2005.08864v1) - [pdf](http://arxiv.org/pdf/2005.08864v1)

> Recent research has demonstrated that vector space models of semantics can reflect undesirable biases in human culture. Our investigation of crosslinguistic word embeddings reveals that topical gender bias interacts with, and is surpassed in magnitude by, the effect of grammatical gender associations, and both may be attenuated by corpus lemmatization. This finding has implications for downstream applications such as machine translation.

</details>

<details>

<summary>2020-05-18 16:40:41 - SenseBERT: Driving Some Sense into BERT</summary>

- *Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav Shoham*

- `1908.05646v2` - [abs](http://arxiv.org/abs/1908.05646v2) - [pdf](http://arxiv.org/pdf/1908.05646v2)

> The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the Word in Context task.

</details>

<details>

<summary>2020-05-18 17:57:24 - Reconstructing Maps from Text</summary>

- *Johnathan E. Avery, Robert L. Goldstone, Michael N. Jones*

- `2005.08932v1` - [abs](http://arxiv.org/abs/2005.08932v1) - [pdf](http://arxiv.org/pdf/2005.08932v1)

> Previous research has demonstrated that Distributional Semantic Models (DSMs) are capable of reconstructing maps from news corpora (Louwerse & Zwaan, 2009) and novels (Louwerse & Benesh, 2012). The capacity for reproducing maps is surprising since DSMs notoriously lack perceptual grounding (De Vega et al., 2012). In this paper we investigate the statistical sources required in language to infer maps, and resulting constraints placed on mechanisms of semantic representation. Study 1 brings word co-occurrence under experimental control to demonstrate that direct co-occurrence in language is necessary for traditional DSMs to successfully reproduce maps. Study 2 presents an instance-based DSM that is capable of reconstructing maps independent of the frequency of co-occurrence of city names.

</details>

<details>

<summary>2020-05-18 18:22:36 - Patch based Colour Transfer using SIFT Flow</summary>

- *Hana Alghamdi, Rozenn Dahyot*

- `2005.09015v1` - [abs](http://arxiv.org/abs/2005.09015v1) - [pdf](http://arxiv.org/pdf/2005.09015v1)

> We propose a new colour transfer method with Optimal Transport (OT) to transfer the colour of a sourceimage to match the colour of a target image of the same scene that may exhibit large motion changes betweenimages. By definition OT does not take into account any available information about correspondences whencomputing the optimal solution. To tackle this problem we propose to encode overlapping neighborhoodsof pixels using both their colour and spatial correspondences estimated using motion estimation. We solvethe high dimensional problem in 1D space using an iterative projection approach. We further introducesmoothing as part of the iterative algorithms for solving optimal transport namely Iterative DistributionTransport (IDT) and its variant the Sliced Wasserstein Distance (SWD). Experiments show quantitative andqualitative improvements over previous state of the art colour transfer methods.

</details>

<details>

<summary>2020-05-18 21:21:34 - (Re)construing Meaning in NLP</summary>

- *Sean Trott, Tiago Timponi Torrent, Nancy Chang, Nathan Schneider*

- `2005.09099v1` - [abs](http://arxiv.org/abs/2005.09099v1) - [pdf](http://arxiv.org/pdf/2005.09099v1)

> Human speakers have an extensive toolkit of ways to express themselves. In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding--namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed. We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics. We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.

</details>

<details>

<summary>2020-05-19 08:27:26 - Controlled Language and Baby Turing Test for General Conversational Intelligence</summary>

- *Anton Kolonin*

- `2005.09280v1` - [abs](http://arxiv.org/abs/2005.09280v1) - [pdf](http://arxiv.org/pdf/2005.09280v1)

> General conversational intelligence appears to be an important part of artificial general intelligence. Respectively, it requires accessible measures of the intelligence quality and controllable ways of its achievement, ideally - having the linguistic and semantic models represented in a reasonable way. Our work is suggesting to use Baby Turing Test approach to extend the classic Turing Test for conversational intelligence and controlled language based on semantic graph representation extensible for arbitrary subject domain. We describe how the two can be used together to build a general-purpose conversational system such as an intelligent assistant for online media and social network data processing.

</details>

<details>

<summary>2020-05-19 08:55:36 - Modeling relation paths for knowledge base completion via joint adversarial training</summary>

- *Chen Li, Xutan Peng, Shanghang Zhang, Hao Peng, Philip S. Yu, Min He, Linfeng Du, Lihong Wang*

- `1810.06033v2` - [abs](http://arxiv.org/abs/1810.06033v2) - [pdf](http://arxiv.org/pdf/1810.06033v2)

> Knowledge Base Completion (KBC), which aims at determining the missing relations between entity pairs, has received increasing attention in recent years. Most existing KBC methods focus on either embedding the Knowledge Base (KB) into a specific semantic space or leveraging the joint probability of Random Walks (RWs) on multi-hop paths. Only a few unified models take both semantic and path-related features into consideration with adequacy. In this paper, we propose a novel method to explore the intrinsic relationship between the single relation (i.e. 1-hop path) and multi-hop paths between paired entities. We use Hierarchical Attention Networks (HANs) to select important relations in multi-hop paths and encode them into low-dimensional vectors. By treating relations and multi-hop paths as two different input sources, we use a feature extractor, which is shared by two downstream components (i.e. relation classifier and source discriminator), to capture shared/similar information between them. By joint adversarial training, we encourage our model to extract features from the multi-hop paths which are representative for relation completion. We apply the trained model (except for the source discriminator) to several large-scale KBs for relation completion. Experimental results show that our method outperforms existing path information-based approaches. Since each sub-module of our model can be well interpreted, our model can be applied to a large number of relation learning tasks.

</details>

<details>

<summary>2020-05-19 12:38:22 - Good-Enough Compositional Data Augmentation</summary>

- *Jacob Andreas*

- `1904.09545v4` - [abs](http://arxiv.org/abs/1904.09545v4) - [pdf](http://arxiv.org/pdf/1904.09545v4)

> We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.

</details>

<details>

<summary>2020-05-19 13:04:02 - Embeddings as representation for symbolic music</summary>

- *Sebastian Garcia-Valencia*

- `2005.09406v1` - [abs](http://arxiv.org/abs/2005.09406v1) - [pdf](http://arxiv.org/pdf/2005.09406v1)

> A representation technique that allows encoding music in a way that contains musical meaning would improve the results of any model trained for computer music tasks like generation of melodies and harmonies of better quality. The field of natural language processing has done a lot of work in finding a way to capture the semantic meaning of words and sentences, and word embeddings have successfully shown the capabilities for such a task. In this paper, we experiment with embeddings to represent musical notes from 3 different variations of a dataset and analyze if the model can capture useful musical patterns. To do this, the resulting embeddings are visualized in projections using the t-SNE technique.

</details>

<details>

<summary>2020-05-19 17:27:00 - A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks</summary>

- *Angela S. Lin, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Chris Brockett, Debadeepta Dey, Bill Dolan*

- `2005.09606v1` - [abs](http://arxiv.org/abs/2005.09606v1) - [pdf](http://arxiv.org/pdf/2005.09606v1)

> Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many partially-overlapping text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured. Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions. To address these challenges, we first use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. We release the Microsoft Research Multimodal Aligned Recipe Corpus containing 150K pairwise alignments between recipes across 4,262 dishes with rich commonsense information.

</details>

<details>

<summary>2020-05-19 19:55:56 - Word-Emoji Embeddings from large scale Messaging Data reflect real-world Semantic Associations of Expressive Icons</summary>

- *Jens Helge Reelfs, Oliver Hohlfeld, Markus Strohmaier, Niklas Henckell*

- `2006.01207v1` - [abs](http://arxiv.org/abs/2006.01207v1) - [pdf](http://arxiv.org/pdf/2006.01207v1)

> We train word-emoji embeddings on large scale messaging data obtained from the Jodel online social network. Our data set contains more than 40 million sentences, of which 11 million sentences are annotated with a subset of the Unicode 13.0 standard Emoji list. We explore semantic emoji associations contained in this embedding by analyzing associations between emojis, between emojis and text, and between text and emojis. Our investigations demonstrate anecdotally that word-emoji embeddings trained on large scale messaging data can reflect real-world semantic associations. To enable further research we release the Jodel Emoji Embedding Dataset (JEED1488) containing 1488 emojis and their embeddings along 300 dimensions.

</details>

<details>

<summary>2020-05-19 20:24:02 - GLEAKE: Global and Local Embedding Automatic Keyphrase Extraction</summary>

- *Javad Rafiei Asl, Juan M. Banda*

- `2005.09740v1` - [abs](http://arxiv.org/abs/2005.09740v1) - [pdf](http://arxiv.org/pdf/2005.09740v1)

> Automated methods for granular categorization of large corpora of text documents have become increasingly more important with the rate scientific, news, medical, and web documents are growing in the last few years. Automatic keyphrase extraction (AKE) aims to automatically detect a small set of single or multi-words from within a single textual document that captures the main topics of the document. AKE plays an important role in various NLP and information retrieval tasks such as document summarization and categorization, full-text indexing, and article recommendation. Due to the lack of sufficient human-labeled data in different textual contents, supervised learning approaches are not ideal for automatic detection of keyphrases from the content of textual bodies. With the state-of-the-art advances in text embedding techniques, NLP researchers have focused on developing unsupervised methods to obtain meaningful insights from raw datasets. In this work, we introduce Global and Local Embedding Automatic Keyphrase Extractor (GLEAKE) for the task of AKE. GLEAKE utilizes single and multi-word embedding techniques to explore the syntactic and semantic aspects of the candidate phrases and then combines them into a series of embedding-based graphs. Moreover, GLEAKE applies network analysis techniques on each embedding-based graph to refine the most significant phrases as a final set of keyphrases. We demonstrate the high performance of GLEAKE by evaluating its results on five standard AKE datasets from different domains and writing styles and by showing its superiority with regards to other state-of-the-art methods.

</details>

<details>

<summary>2020-05-20 10:14:01 - GM-CTSC at SemEval-2020 Task 1: Gaussian Mixtures Cross Temporal Similarity Clustering</summary>

- *Pierluigi Cassotti, Annalina Caputo, Marco Polignano, Pierpaolo Basile*

- `2005.09946v1` - [abs](http://arxiv.org/abs/2005.09946v1) - [pdf](http://arxiv.org/pdf/2005.09946v1)

> This paper describes the system proposed for the SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection. We focused our approach on the detection problem. Given the semantics of words captured by temporal word embeddings in different time periods, we investigate the use of unsupervised methods to detect when the target word has gained or loosed senses. To this end, we defined a new algorithm based on Gaussian Mixture Models to cluster the target similarities computed over the two periods. We compared the proposed approach with a number of similarity-based thresholds. We found that, although the performance of the detection methods varies across the word embedding algorithms, the combination of Gaussian Mixture with Temporal Referencing resulted in our best system.

</details>

<details>

<summary>2020-05-20 13:45:49 - Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources</summary>

- *Magdalena Biesialska, Bardia Rafieian, Marta R. Costa-jussà*

- `2005.10048v1` - [abs](http://arxiv.org/abs/2005.10048v1) - [pdf](http://arxiv.org/pdf/2005.10048v1)

> In this work, we present an effective method for semantic specialization of word vector representations. To this end, we use traditional word embeddings and apply specialization methods to better capture semantic relations between words. In our approach, we leverage external knowledge from rich lexical resources such as BabelNet. We also show that our proposed post-specialization method based on an adversarial neural network with the Wasserstein distance allows to gain improvements over state-of-the-art methods on two tasks: word similarity and dialog state tracking.

</details>

<details>

<summary>2020-05-20 13:52:13 - Smart Contract Repair</summary>

- *Xiao Liang Yu, Omar Al-Bataineh, David Lo, Abhik Roychoudhury*

- `1912.05823v3` - [abs](http://arxiv.org/abs/1912.05823v3) - [pdf](http://arxiv.org/pdf/1912.05823v3)

> Smart contracts are automated or self-enforcing contracts that can be used to exchange assets without having to place trust in third parties. Many commercial transactions use smart contracts due to their potential benefits in terms of secure peer-to-peer transactions independent of external parties. Experience shows that many commonly used smart contracts are vulnerable to serious malicious attacks which may enable attackers to steal valuable assets of involving parties. There is therefore a need to apply analysis and automated repair techniques to detect and repair bugs in smart contracts before being deployed. In this work, we present the first general-purpose automated smart contract repair approach that is also gas-aware. Our repair method is search-based and searches among mutations of the buggy contract. Our method also considers the gas usage of the candidate patches by leveraging our novel notion of gas dominance relationship. We have made our smart contract repair tool SCRepair available open-source, for investigation by the wider community.

</details>

<details>

<summary>2020-05-20 14:01:13 - 4D Semantic Cardiac Magnetic Resonance Image Synthesis on XCAT Anatomical Model</summary>

- *Samaneh Abbasi-Sureshjani, Sina Amirrajab, Cristian Lorenz, Juergen Weese, Josien Pluim, Marcel Breeuwer*

- `2002.07089v3` - [abs](http://arxiv.org/abs/2002.07089v3) - [pdf](http://arxiv.org/pdf/2002.07089v3)

> We propose a hybrid controllable image generation method to synthesize anatomically meaningful 3D+t labeled Cardiac Magnetic Resonance (CMR) images. Our hybrid method takes the mechanistic 4D eXtended CArdiac Torso (XCAT) heart model as the anatomical ground truth and synthesizes CMR images via a data-driven Generative Adversarial Network (GAN). We employ the state-of-the-art SPatially Adaptive De-normalization (SPADE) technique for conditional image synthesis to preserve the semantic spatial information of ground truth anatomy. Using the parameterized motion model of the XCAT heart, we generate labels for 25 time frames of the heart for one cardiac cycle at 18 locations for the short axis view. Subsequently, realistic images are generated from these labels, with modality-specific features that are learned from real CMR image data. We demonstrate that style transfer from another cardiac image can be accomplished by using a style encoder network. Due to the flexibility of XCAT in creating new heart models, this approach can result in a realistic virtual population to address different challenges the medical image analysis research community is facing such as expensive data collection. Our proposed method has a great potential to synthesize 4D controllable CMR images with annotations and adaptable styles to be used in various supervised multi-site, multi-vendor applications in medical image analysis.

</details>

<details>

<summary>2020-05-20 23:44:26 - Semantic Search of Memes on Twitter</summary>

- *Jesus Perez-Martin, Benjamin Bustos, Magdalena Saldana*

- `2002.01462v4` - [abs](http://arxiv.org/abs/2002.01462v4) - [pdf](http://arxiv.org/pdf/2002.01462v4)

> Memes are becoming a useful source of data for analyzing behavior on social media. However, a problem to tackle is how to correctly identify a meme. As the number of memes published every day on social media is huge, there is a need for automatic methods for classifying and searching in large meme datasets. This paper proposes and compares several methods for automatically classifying images as memes. Also, we propose a method that allows us to implement a system for retrieving memes from a dataset using a textual query. We experimentally evaluate the methods using a large dataset of memes collected from Twitter users in Chile, which was annotated by a group of experts. Though some of the evaluated methods are effective, there is still room for improvement.

</details>

<details>

<summary>2020-05-21 04:37:43 - SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics</summary>

- *Da Yin, Tao Meng, Kai-Wei Chang*

- `2005.04114v4` - [abs](http://arxiv.org/abs/2005.04114v4) - [pdf](http://arxiv.org/pdf/2005.04114v4)

> We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics. The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification. We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks. Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT. We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.

</details>

<details>

<summary>2020-05-21 06:11:33 - Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator</summary>

- *Siamak Zamani Dadaneh, Shahin Boluki, Mingzhang Yin, Mingyuan Zhou, Xiaoning Qian*

- `2005.10477v1` - [abs](http://arxiv.org/abs/2005.10477v1) - [pdf](http://arxiv.org/pdf/2005.10477v1)

> Semantic hashing has become a crucial component of fast similarity search in many large-scale information retrieval systems, in particular, for text data. Variational auto-encoders (VAEs) with binary latent variables as hashing codes provide state-of-the-art performance in terms of precision for document retrieval. We propose a pairwise loss function with discrete latent VAE to reward within-class similarity and between-class dissimilarity for supervised hashing. Instead of solving the optimization relying on existing biased gradient estimators, an unbiased low-variance gradient estimator is adopted to optimize the hashing function by evaluating the non-differentiable loss function over two correlated sets of binary hashing codes to control the variance of gradient estimates. This new semantic hashing framework achieves superior performance compared to the state-of-the-arts, as demonstrated by our comprehensive experiments.

</details>

<details>

<summary>2020-05-21 07:57:20 - Text Matters but Speech Influences: A Computational Analysis of Syntactic Ambiguity Resolution</summary>

- *Won Ik Cho, Jeonghwa Cho, Woo Hyun Kang, Nam Soo Kim*

- `1910.09275v3` - [abs](http://arxiv.org/abs/1910.09275v3) - [pdf](http://arxiv.org/pdf/1910.09275v3)

> Analyzing how human beings resolve syntactic ambiguity has long been an issue of interest in the field of linguistics. It is, at the same time, one of the most challenging issues for spoken language understanding (SLU) systems as well. As syntactic ambiguity is intertwined with issues regarding prosody and semantics, the computational approach toward speech intention identification is expected to benefit from the observations of the human language processing mechanism. In this regard, we address the task with attentive recurrent neural networks that exploit acoustic and textual features simultaneously and reveal how the modalities interact with each other to derive sentence meaning. Utilizing a speech corpus recorded on Korean scripts of syntactically ambiguous utterances, we revealed that co-attention frameworks, namely multi-hop attention and cross-attention, show significantly superior performance in disambiguating speech intention. With further analysis, we demonstrate that the computational models reflect the internal relationship between auditory and linguistic processes.

</details>

<details>

<summary>2020-05-21 08:41:57 - An analysis on the use of autoencoders for representation learning: fundamentals, learning task case studies, explainability and challenges</summary>

- *David Charte, Francisco Charte, María J. del Jesus, Francisco Herrera*

- `2005.10516v1` - [abs](http://arxiv.org/abs/2005.10516v1) - [pdf](http://arxiv.org/pdf/2005.10516v1)

> In many machine learning tasks, learning a good representation of the data can be the key to building a well-performant solution. This is because most learning algorithms operate with the features in order to find models for the data. For instance, classification performance can improve if the data is mapped to a space where classes are easily separated, and regression can be facilitated by finding a manifold of data in the feature space. As a general rule, features are transformed by means of statistical methods such as principal component analysis, or manifold learning techniques such as Isomap or locally linear embedding. From a plethora of representation learning methods, one of the most versatile tools is the autoencoder. In this paper we aim to demonstrate how to influence its learned representations to achieve the desired learning behavior. To this end, we present a series of learning tasks: data embedding for visualization, image denoising, semantic hashing, detection of abnormal behaviors and instance generation. We model them from the representation learning perspective, following the state of the art methodologies in each field. A solution is proposed for each task employing autoencoders as the only learning method. The theoretical developments are put into practice using a selection of datasets for the different problems and implementing each solution, followed by a discussion of the results in each case study and a brief explanation of other six learning applications. We also explore the current challenges and approaches to explainability in the context of autoencoders. All of this helps conclude that, thanks to alterations in their structure as well as their objective function, autoencoders may be the core of a possible solution to many problems which can be modeled as a transformation of the feature space.

</details>

<details>

<summary>2020-05-21 09:05:11 - Glyce: Glyph-vectors for Chinese Character Representations</summary>

- *Yuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie, Fan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, Jiwei Li*

- `1901.10125v5` - [abs](http://arxiv.org/abs/1901.10125v5) - [pdf](http://arxiv.org/pdf/1901.10125v5)

> It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\% on the Fudan corpus for text classification. Code found at https://github.com/ShannonAI/glyce.

</details>

<details>

<summary>2020-05-21 13:01:07 - Editing in Style: Uncovering the Local Semantics of GANs</summary>

- *Edo Collins, Raja Bala, Bob Price, Sabine Süsstrunk*

- `2004.14367v2` - [abs](http://arxiv.org/abs/2004.14367v2) - [pdf](http://arxiv.org/pdf/2004.14367v2)

> While the quality of GAN image synthesis has improved tremendously in recent years, our ability to control and condition the output is still limited. Focusing on StyleGAN, we introduce a simple and effective method for making local, semantically-aware edits to a target output image. This is accomplished by borrowing elements from a source image, also a GAN output, via a novel manipulation of style vectors. Our method requires neither supervision from an external model, nor involves complex spatial morphing operations. Instead, it relies on the emergent disentanglement of semantic objects that is learned by StyleGAN during its training. Semantic editing is demonstrated on GANs producing human faces, indoor scenes, cats, and cars. We measure the locality and photorealism of the edits produced by our method, and find that it accomplishes both.

</details>

<details>

<summary>2020-05-21 14:09:14 - Wish You Were Here: Context-Aware Human Generation</summary>

- *Oran Gafni, Lior Wolf*

- `2005.10663v1` - [abs](http://arxiv.org/abs/2005.10663v1) - [pdf](http://arxiv.org/pdf/2005.10663v1)

> We present a novel method for inserting objects, specifically humans, into existing images, such that they blend in a photorealistic manner, while respecting the semantic context of the scene. Our method involves three subnetworks: the first generates the semantic map of the new person, given the pose of the other persons in the scene and an optional bounding box specification. The second network renders the pixels of the novel person and its blending mask, based on specifications in the form of multiple appearance components. A third network refines the generated face in order to match those of the target person. Our experiments present convincing high-resolution outputs in this novel and challenging application domain. In addition, the three networks are evaluated individually, demonstrating for example, state of the art results in pose transfer benchmarks.

</details>

<details>

<summary>2020-05-21 14:40:15 - Training a code-switching language model with monolingual data</summary>

- *Shun-Po Chuang, Tzu-Wei Sung, Hung-Yi Lee*

- `1911.06003v2` - [abs](http://arxiv.org/abs/1911.06003v2) - [pdf](http://arxiv.org/pdf/1911.06003v2)

> A lack of code-switching data complicates the training of code-switching (CS) language models. We propose an approach to train such CS language models on monolingual data only. By constraining and normalizing the output projection matrix in RNN-based language models, we bring embeddings of different languages closer to each other. Numerical and visualization results show that the proposed approaches remarkably improve the performance of CS language models trained on monolingual data. The proposed approaches are comparable or even better than training CS language models with artificially generated CS data. We additionally use unsupervised bilingual word translation to analyze whether semantically equivalent words in different languages are mapped together.

</details>

<details>

<summary>2020-05-21 19:20:23 - Java Decompiler Diversity and its Application to Meta-decompilation</summary>

- *Nicolas Harrand, César Soto-Valero, Martin Monperrus, Benoit Baudry*

- `2005.11315v1` - [abs](http://arxiv.org/abs/2005.11315v1) - [pdf](http://arxiv.org/pdf/2005.11315v1)

> During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84%, respectively 78%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler.

</details>

<details>

<summary>2020-05-21 20:10:38 - Unsupervised Domain Adaptation in Semantic Segmentation: a Review</summary>

- *Marco Toldo, Andrea Maracani, Umberto Michieli, Pietro Zanuttigh*

- `2005.10876v1` - [abs](http://arxiv.org/abs/2005.10876v1) - [pdf](http://arxiv.org/pdf/2005.10876v1)

> The aim of this paper is to give an overview of the recent advancements in the Unsupervised Domain Adaptation (UDA) of deep networks for semantic segmentation. This task is attracting a wide interest, since semantic segmentation models require a huge amount of labeled data and the lack of data fitting specific requirements is the main limitation in the deployment of these techniques. This problem has been recently explored and has rapidly grown with a large number of ad-hoc approaches. This motivates us to build a comprehensive overview of the proposed methodologies and to provide a clear categorization. In this paper, we start by introducing the problem, its formulation and the various scenarios that can be considered. Then, we introduce the different levels at which adaptation strategies may be applied: namely, at the input (image) level, at the internal features representation and at the output level. Furthermore, we present a detailed overview of the literature in the field, dividing previous methods based on the following (non mutually exclusive) categories: adversarial learning, generative-based, analysis of the classifier discrepancies, self-teaching, entropy minimization, curriculum learning and multi-task learning. Novel research directions are also briefly introduced to give a hint of interesting open problems in the field. Finally, a comparison of the performance of the various methods in the widely used autonomous driving scenario is presented.

</details>

<details>

<summary>2020-05-21 21:29:44 - Team Neuro at SemEval-2020 Task 8: Multi-Modal Fine Grain Emotion Classification of Memes using Multitask Learning</summary>

- *Sourya Dipta Das, Soumil Mandal*

- `2005.10915v1` - [abs](http://arxiv.org/abs/2005.10915v1) - [pdf](http://arxiv.org/pdf/2005.10915v1)

> In this article, we describe the system that we used for the memotion analysis challenge, which is Task 8 of SemEval-2020. This challenge had three subtasks where affect based sentiment classification of the memes was required along with intensities. The system we proposed combines the three tasks into a single one by representing it as multi-label hierarchical classification problem.Here,Multi-Task learning or Joint learning Procedure is used to train our model.We have used dual channels to extract text and image based features from separate Deep Neural Network Backbone and aggregate them to create task specific features. These task specific aggregated feature vectors ware then passed on to smaller networks with dense layers, each one assigned for predicting one type of fine grain sentiment label. Our Proposed method show the superiority of this system in few tasks to other best models from the challenge.

</details>

<details>

<summary>2020-05-22 03:12:14 - BARNet: Bilinear Attention Network with Adaptive Receptive Fields for Surgical Instrument Segmentation</summary>

- *Zhen-Liang Ni, Gui-Bin Bian, Guan-An Wang, Xiao-Hu Zhou, Zeng-Guang Hou, Xiao-Liang Xie, Zhen Li, Yu-Han Wang*

- `2001.07093v4` - [abs](http://arxiv.org/abs/2001.07093v4) - [pdf](http://arxiv.org/pdf/2001.07093v4)

> Surgical instrument segmentation is extremely important for computer-assisted surgery. Different from common object segmentation, it is more challenging due to the large illumination and scale variation caused by the special surgical scenes. In this paper, we propose a novel bilinear attention network with adaptive receptive field to solve these two challenges. For the illumination variation, the bilinear attention module can capture second-order statistics to encode global contexts and semantic dependencies between local pixels. With them, semantic features in challenging areas can be inferred from their neighbors and the distinction of various semantics can be boosted. For the scale variation, our adaptive receptive field module aggregates multi-scale features and automatically fuses them with different weights. Specifically, it encodes the semantic relationship between channels to emphasize feature maps with appropriate scales, changing the receptive field of subsequent convolutions. The proposed network achieves the best performance 97.47% mean IOU on Cata7 and comes first place on EndoVis 2017 by 10.10% IOU overtaking second-ranking method.

</details>

<details>

<summary>2020-05-22 06:04:50 - Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models</summary>

- *Mengxi Wei, Yifan He, Qiong Zhang*

- `2005.11017v1` - [abs](http://arxiv.org/abs/2005.11017v1) - [pdf](http://arxiv.org/pdf/2005.11017v1)

> Many business documents processed in modern NLP and IR pipelines are visually rich: in addition to text, their semantics can also be captured by visual traits such as layout, format, and fonts. We study the problem of information extraction from visually rich documents (VRDs) and present a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents. We further introduce new fine-tuning objectives to improve in-domain unsupervised fine-tuning to better utilize large amount of unlabeled in-domain data. We experiment on real world invoice and resume data sets and show that the proposed method outperforms strong text-based RoBERTa baselines by 6.3% absolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a few-shot setting, our method requires up to 30x less annotation data than the baseline to achieve the same level of performance at ~90% F1.

</details>

<details>

<summary>2020-05-22 07:36:08 - DevReplay: Automatic Repair with Editable Fix Pattern</summary>

- *Yuki Ueda, Takashi Ishio, Akinori Ihara, Kenichi Matsumoto*

- `2005.11040v1` - [abs](http://arxiv.org/abs/2005.11040v1) - [pdf](http://arxiv.org/pdf/2005.11040v1)

> Static analysis tools, or linters, detect violation of source code conventions to maintain project readability. Those tools automatically fix specific violations while developers edit the source code. However, existing tools are designed for the general conventions of programming languages. These tools do not check the project/API-specific conventions. We propose a novel static analysis tool DevReplay that generates code change patterns by mining the code change history, and we recommend changes using the matched patterns. Using DevReplay, developers can automatically detect and fix project/API-specific problems in the code editor and code review. Also, we evaluate the accuracy of DevReplay using automatic program repair tool benchmarks and real software. We found that DevReplay resolves more bugs than state-of-the-art APR tools. Finally, we submitted patches to the most popular open-source projects that are implemented by different languages, and project reviewers accepted 80% (8 of 10) patches. DevReplay is available on https://devreplay.github.io.

</details>

<details>

<summary>2020-05-22 13:39:14 - End-to-end Named Entity Recognition from English Speech</summary>

- *Hemant Yadav, Sreyan Ghosh, Yi Yu, Rajiv Ratn Shah*

- `2005.11184v1` - [abs](http://arxiv.org/abs/2005.11184v1) - [pdf](http://arxiv.org/pdf/2005.11184v1)

> Named entity recognition (NER) from text has been a widely studied problem and usually extracts semantic information from text. Until now, NER from speech is mostly studied in a two-step pipeline process that includes first applying an automatic speech recognition (ASR) system on an audio sample and then passing the predicted transcript to a NER tagger. In such cases, the error does not propagate from one step to another as both the tasks are not optimized in an end-to-end (E2E) fashion. Recent studies confirm that integrated approaches (e.g., E2E ASR) outperform sequential ones (e.g., phoneme based ASR). In this paper, we introduce a first publicly available NER annotated dataset for English speech and present an E2E approach, which jointly optimizes the ASR and NER tagger components. Experimental results show that the proposed E2E approach outperforms the classical two-step approach. We also discuss how NER from speech can be used to handle out of vocabulary (OOV) words in an ASR system.

</details>

<details>

<summary>2020-05-22 14:49:07 - A Generative Approach to Titling and Clustering Wikipedia Sections</summary>

- *Anjalie Field, Sascha Rothe, Simon Baumgartner, Cong Yu, Abe Ittycheriah*

- `2005.11216v1` - [abs](http://arxiv.org/abs/2005.11216v1) - [pdf](http://arxiv.org/pdf/2005.11216v1)

> We evaluate the performance of transformer encoders with various decoders for information organization through a new task: generation of section headings for Wikipedia articles. Our analysis shows that decoders containing attention mechanisms over the encoder output achieve high-scoring results by generating extractive text. In contrast, a decoder without attention better facilitates semantic encoding and can be used to generate section embeddings. We additionally introduce a new loss function, which further encourages the decoder to generate high-quality embeddings.

</details>

<details>

<summary>2020-05-22 18:32:35 - SentPWNet: A Unified Sentence Pair Weighting Network for Task-specific Sentence Embedding</summary>

- *Li Zhang, Han Wang, Lingxiao Li*

- `2005.11347v1` - [abs](http://arxiv.org/abs/2005.11347v1) - [pdf](http://arxiv.org/pdf/2005.11347v1)

> Pair-based metric learning has been widely adopted to learn sentence embedding in many NLP tasks such as semantic text similarity due to its efficiency in computation. Most existing works employed a sequence encoder model and utilized limited sentence pairs with a pair-based loss to learn discriminating sentence representation. However, it is known that the sentence representation can be biased when the sampled sentence pairs deviate from the true distribution of all sentence pairs. In this paper, our theoretical analysis shows that existing works severely suffered from a good pair sampling and instance weighting strategy. Instead of one time pair selection and learning on equal weighted pairs, we propose a unified locality weighting and learning framework to learn task-specific sentence embedding. Our model, SentPWNet, exploits the neighboring spatial distribution of each sentence as locality weight to indicate the informative level of sentence pair. Such weight is updated along with pair-loss optimization in each round, ensuring the model keep learning the most informative sentence pairs. Extensive experiments on four public available datasets and a self-collected place search benchmark with 1.4 million places clearly demonstrate that our model consistently outperforms existing sentence embedding methods with comparable efficiency.

</details>

<details>

<summary>2020-05-22 19:40:27 - Empowering Multilevel DSMLs with Integrated Runtime Verification</summary>

- *Fernando Macías, Adrian Rutle, Volker Stolz, Torben Scheffel, Malte Schmitz*

- `2005.11366v1` - [abs](http://arxiv.org/abs/2005.11366v1) - [pdf](http://arxiv.org/pdf/2005.11366v1)

> Within Model-Driven Software Engineering, Domain-Specific Modelling has proven to be a powerful technique to specify systems and systems' behaviour in a formal, yet understandable way. Runtime verification (RV) has been successfully used to verify the correctness of such behaviour. Specifying behaviour requires managing various levels of abstractions, making multilevel modelling (MLM) a suitable approach for this task. In this paper, we present an approach to combine MLM and RV with an example from the domain of distributed real-time systems. The semantics of the specified behaviour as well as the evaluation of correctness properties are given by model transformation rules. This facilitates simulation of the system and checking against real-time temporal logic correctness properties.

</details>

<details>

<summary>2020-05-23 00:05:56 - On the Idiosyncrasies of the Mandarin Chinese Classifier System</summary>

- *Shijia Liu, Hongyuan Mei, Adina Williams, Ryan Cotterell*

- `1902.10193v3` - [abs](http://arxiv.org/abs/1902.10193v3) - [pdf](http://arxiv.org/pdf/1902.10193v3)

> While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify. Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.

</details>

<details>

<summary>2020-05-23 04:25:58 - Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT</summary>

- *Han He, Jinho D. Choi*

- `1908.04943v4` - [abs](http://arxiv.org/abs/1908.04943v4) - [pdf](http://arxiv.org/pdf/1908.04943v4)

> This paper presents new state-of-the-art models for three tasks, part-of-speech tagging, syntactic parsing, and semantic parsing, using the cutting-edge contextualized embedding framework known as BERT. For each task, we first replicate and simplify the current state-of-the-art approach to enhance its model efficiency. We then evaluate our simplified approaches on those three tasks using token embeddings generated by BERT. 12 datasets in both English and Chinese are used for our experiments. The BERT models outperform the previously best-performing models by 2.5% on average (7.5% for the most significant case). Moreover, an in-depth analysis on the impact of BERT embeddings is provided using self-attention, which helps understanding in this rich yet representation. All models and source codes are available in public so that researchers can improve upon and utilize them to establish strong baselines for the next decade.

</details>

<details>

<summary>2020-05-23 15:30:38 - Underwater object detection using Invert Multi-Class Adaboost with deep learning</summary>

- *Long Chen, Zhihua Liu, Lei Tong, Zheheng Jiang, Shengke Wang, Junyu Dong, Huiyu Zhou*

- `2005.11552v1` - [abs](http://arxiv.org/abs/2005.11552v1) - [pdf](http://arxiv.org/pdf/2005.11552v1)

> In recent years, deep learning based methods have achieved promising performance in standard object detection. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) Objects in real applications are usually small and their images are blurry, and (2) images in the underwater datasets and real applications accompany heterogeneous noise. To address these two problems, we first propose a novel neural network architecture, namely Sample-WeIghted hyPEr Network (SWIPENet), for small object detection. SWIPENet consists of high resolution and semantic rich Hyper Feature Maps which can significantly improve small object detection accuracy. In addition, we propose a novel sample-weighted loss function which can model sample weights for SWIPENet, which uses a novel sample re-weighting algorithm, namely Invert Multi-Class Adaboost (IMA), to reduce the influence of noise on the proposed SWIPENet. Experiments on two underwater robot picking contest datasets URPC2017 and URPC2018 show that the proposed SWIPENet+IMA framework achieves better performance in detection accuracy against several state-of-the-art object detection approaches.

</details>

<details>

<summary>2020-05-24 07:48:05 - Ecological Semantics: Programming Environments for Situated Language Understanding</summary>

- *Ronen Tamari, Gabriel Stanovsky, Dafna Shahaf, Reut Tsarfaty*

- `2003.04567v2` - [abs](http://arxiv.org/abs/2003.04567v2) - [pdf](http://arxiv.org/pdf/2003.04567v2)

> Large-scale natural language understanding (NLU) systems have made impressive progress: they can be applied flexibly across a variety of tasks, and employ minimal structural assumptions. However, extensive empirical research has shown this to be a double-edged sword, coming at the cost of shallow understanding: inferior generalization, grounding and explainability. Grounded language learning approaches offer the promise of deeper understanding by situating learning in richer, more structured training environments, but are limited in scale to relatively narrow, predefined domains. How might we enjoy the best of both worlds: grounded, general NLU? Following extensive contemporary cognitive science, we propose treating environments as "first-class citizens" in semantic representations, worthy of research and development in their own right. Importantly, models should also be partners in the creation and configuration of environments, rather than just actors within them, as in existing approaches. To do so, we argue that models must begin to understand and program in the language of affordances (which define possible actions in a given situation) both for online, situated discourse comprehension, as well as large-scale, offline common-sense knowledge mining. To this end we propose an environment-oriented ecological semantics, outlining theoretical and practical approaches towards implementation. We further provide actual demonstrations building upon interactive fiction programming languages.

</details>

<details>

<summary>2020-05-24 08:20:04 - Synergistic Learning of Lung Lobe Segmentation and Hierarchical Multi-Instance Classification for Automated Severity Assessment of COVID-19 in CT Images</summary>

- *Kelei He, Wei Zhao, Xingzhi Xie, Wen Ji, Mingxia Liu, Zhenyu Tang, Feng Shi, Yang Gao, Jun Liu, Junfeng Zhang, Dinggang Shen*

- `2005.03832v2` - [abs](http://arxiv.org/abs/2005.03832v2) - [pdf](http://arxiv.org/pdf/2005.03832v2)

> Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19) will help detect infections early and assess the disease progression. Especially, automated severity assessment of COVID-19 in CT images plays an essential role in identifying cases that are in great need of intensive clinical care. However, it is often challenging to accurately assess the severity of this disease in CT images, due to variable infection regions in the lungs, similar imaging biomarkers, and large inter-case variations. To this end, we propose a synergistic learning framework for automated severity assessment of COVID-19 in 3D CT images, by jointly performing lung lobe segmentation and multi-instance classification. Considering that only a few infection regions in a CT image are related to the severity assessment, we first represent each input image by a bag that contains a set of 2D image patches (with each cropped from a specific slice). A multi-task multi-instance deep network (called M$^2$UNet) is then developed to assess the severity of COVID-19 patients and also segment the lung lobe simultaneously. Our M$^2$UNet consists of a patch-level encoder, a segmentation sub-network for lung lobe segmentation, and a classification sub-network for severity assessment (with a unique hierarchical multi-instance learning strategy). Here, the context information provided by segmentation can be implicitly employed to improve the performance of severity assessment. Extensive experiments were performed on a real COVID-19 CT image dataset consisting of 666 chest CT images, with results suggesting the effectiveness of our proposed method compared to several state-of-the-art methods.

</details>

<details>

<summary>2020-05-24 09:21:53 - Designing Normative Theories for Ethical and Legal Reasoning: LogiKEy Framework, Methodology, and Tool Support</summary>

- *Christoph Benzmüller, Xavier Parent, Leendert van der Torre*

- `1903.10187v6` - [abs](http://arxiv.org/abs/1903.10187v6) - [pdf](http://arxiv.org/pdf/1903.10187v6)

> A framework and methodology---termed LogiKEy---for the design and engineering of ethical reasoners, normative theories and deontic logics is presented. The overall motivation is the development of suitable means for the control and governance of intelligent autonomous systems. LogiKEy's unifying formal framework is based on semantical embeddings of deontic logics, logic combinations and ethico-legal domain theories in expressive classic higher-order logic (HOL). This meta-logical approach enables the provision of powerful tool support in LogiKEy: off-the-shelf theorem provers and model finders for HOL are assisting the LogiKEy designer of ethical intelligent agents to flexibly experiment with underlying logics and their combinations, with ethico-legal domain theories, and with concrete examples---all at the same time. Continuous improvements of these off-the-shelf provers, without further ado, leverage the reasoning performance in LogiKEy. Case studies, in which the LogiKEy framework and methodology has been applied and tested, give evidence that HOL's undecidability often does not hinder efficient experimentation.

</details>

<details>

<summary>2020-05-24 12:12:27 - DeepSQLi: Deep Semantic Learning for Testing SQL Injection</summary>

- *Muyang Liu, Ke Li, Tao Chen*

- `2005.11728v1` - [abs](http://arxiv.org/abs/2005.11728v1) - [pdf](http://arxiv.org/pdf/2005.11728v1)

> Security is unarguably the most serious concern for Web applications, to which SQL injection (SQLi) attack is one of the most devastating attacks. Automatically testing SQLi vulnerabilities is of ultimate importance, yet is unfortunately far from trivial to implement. This is because the existence of a huge, or potentially infinite, number of variants and semantic possibilities of SQL leading to SQLi attacks on various Web applications. In this paper, we propose a deep natural language processing based tool, dubbed DeepSQLi, to generate test cases for detecting SQLi vulnerabilities. Through adopting deep learning based neural language model and sequence of words prediction, DeepSQLi is equipped with the ability to learn the semantic knowledge embedded in SQLi attacks, allowing it to translate user inputs (or a test case) into a new test case, which is semantically related and potentially more sophisticated. Experiments are conducted to compare DeepSQLi with SQLmap, a state-of-the-art SQLi testing automation tool, on six real-world Web applications that are of different scales, characteristics and domains. Empirical results demonstrate the effectiveness and the remarkable superiority of DeepSQLi over SQLmap, such that more SQLi vulnerabilities can be identified by using a less number of test cases, whilst running much faster.

</details>

<details>

<summary>2020-05-24 14:37:07 - Req2Lib: A Semantic Neural Model for Software Library Recommendation</summary>

- *Zhensu Sun, Yan Liu, Ziming Cheng, Chen Yang, Pengyu Che*

- `2005.11757v1` - [abs](http://arxiv.org/abs/2005.11757v1) - [pdf](http://arxiv.org/pdf/2005.11757v1)

> Third-party libraries are crucial to the development of software projects. To get suitable libraries, developers need to search through millions of libraries by filtering, evaluating, and comparing. The vast number of libraries places a barrier for programmers to locate appropriate ones. To help developers, researchers have proposed automated approaches to recommend libraries based on library usage pattern. However, these prior studies can not sufficiently match user requirements and suffer from cold-start problem. In this work, we would like to make recommendations based on requirement descriptions to avoid these problems. To this end, we propose a novel neural approach called Req2Lib which recommends libraries given descriptions of the project requirement. We use a Sequence-to-Sequence model to learn the library linked-usage information and semantic information of requirement descriptions in natural language. Besides, we apply a domain-specific pre-trained word2vec model for word embedding, which is trained over textual corpus from Stack Overflow posts. In the experiment, we train and evaluate the model with data from 5,625 java projects. Our preliminary evaluation demonstrates that Req2Lib can recommend libraries accurately.

</details>

<details>

<summary>2020-05-25 02:05:38 - When Autonomous Systems Meet Accuracy and Transferability through AI: A Survey</summary>

- *Chongzhen Zhang, Jianrui Wang, Gary G. Yen, Chaoqiang Zhao, Qiyu Sun, Yang Tang, Feng Qian, Jürgen Kurths*

- `2003.12948v3` - [abs](http://arxiv.org/abs/2003.12948v3) - [pdf](http://arxiv.org/pdf/2003.12948v3)

> With widespread applications of artificial intelligence (AI), the capabilities of the perception, understanding, decision-making and control for autonomous systems have improved significantly in the past years. When autonomous systems consider the performance of accuracy and transferability, several AI methods, like adversarial learning, reinforcement learning (RL) and meta-learning, show their powerful performance. Here, we review the learning-based approaches in autonomous systems from the perspectives of accuracy and transferability. Accuracy means that a well-trained model shows good results during the testing phase, in which the testing set shares a same task or a data distribution with the training set. Transferability means that when a well-trained model is transferred to other testing domains, the accuracy is still good. Firstly, we introduce some basic concepts of transfer learning and then present some preliminaries of adversarial learning, RL and meta-learning. Secondly, we focus on reviewing the accuracy or transferability or both of them to show the advantages of adversarial learning, like generative adversarial networks (GANs), in typical computer vision tasks in autonomous systems, including image style transfer, image superresolution, image deblurring/dehazing/rain removal, semantic segmentation, depth estimation, pedestrian detection and person re-identification (re-ID). Then, we further review the performance of RL and meta-learning from the aspects of accuracy or transferability or both of them in autonomous systems, involving pedestrian tracking, robot navigation and robotic manipulation. Finally, we discuss several challenges and future topics for using adversarial learning, RL and meta-learning in autonomous systems.

</details>

<details>

<summary>2020-05-25 06:28:24 - CNN-based Patch Matching for Optical Flow with Thresholded Hinge Embedding Loss</summary>

- *Christian Bailer, Kiran Varanasi, Didier Stricker*

- `1607.08064v4` - [abs](http://arxiv.org/abs/1607.08064v4) - [pdf](http://arxiv.org/pdf/1607.08064v4)

> Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.

</details>

<details>

<summary>2020-05-25 18:44:53 - Incidental Supervision: Moving beyond Supervised Learning</summary>

- *Dan Roth*

- `2005.12339v1` - [abs](http://arxiv.org/abs/2005.12339v1) - [pdf](http://arxiv.org/pdf/2005.12339v1)

> Machine Learning and Inference methods have become ubiquitous in our attempt to induce more abstract representations of natural language text, visual scenes, and other messy, naturally occurring data, and support decisions that depend on it. However, learning models for these tasks is difficult partly because generating the necessary supervision signals for it is costly and does not scale. This paper describes several learning paradigms that are designed to alleviate the supervision bottleneck. It will illustrate their benefit in the context of multiple problems, all pertaining to inducing various levels of semantic representations from text.

</details>

<details>

<summary>2020-05-25 19:10:56 - Secure and User-Friendly Over-the-Air Firmware Distribution in a Portable Faraday Cage</summary>

- *Martin Striegel, Florian Jakobsmeier, Yacov Matveev, Johann Heyszl, Georg Sigl*

- `2005.12347v1` - [abs](http://arxiv.org/abs/2005.12347v1) - [pdf](http://arxiv.org/pdf/2005.12347v1)

> Setting up a large-scale wireless sensor network is challenging, as firmware must be distributed and trust between sensor nodes and a backend needs to be established. To perform this task efficiently, we propose an approach named Box, which utilizes an intelligent Faraday cage (FC). The FC acquires firmware images and secret keys from a backend, patches the firmware with the keys and deploys those customized images over the air to sensor nodes placed in the FC. Electromagnetic shielding protects this exchange against passive attackers. We place few demands on the sensor node, not requiring additional hardware components or firmware customized by the manufacturer. We describe this novel workflow, implement the Box and a backend system and demonstrate the feasibility of our approach by batch-deploying firmware to multiple commercial off-the-shelf sensor nodes. We conduct a user-study with 31 participants with diverse backgrounds and find, that our approach is both faster and more user-friendly than firmware distribution over a wired connection.

</details>

<details>

<summary>2020-05-25 20:54:23 - The Unreasonable Volatility of Neural Machine Translation Models</summary>

- *Marzieh Fadaee, Christof Monz*

- `2005.12398v1` - [abs](http://arxiv.org/abs/2005.12398v1) - [pdf](http://arxiv.org/pdf/2005.12398v1)

> Recent works have shown that Neural Machine Translation (NMT) models achieve impressive performance, however, questions about understanding the behavior of these models remain unanswered. We investigate the unexpected volatility of NMT models where the input is semantically and syntactically correct. We discover that with trivial modifications of source sentences, we can identify cases where \textit{unexpected changes} happen in the translation and in the worst case lead to mistranslations. This volatile behavior of translating extremely similar sentences in surprisingly different ways highlights the underlying generalization problem of current NMT models. We find that both RNN and Transformer models display volatile behavior in 26% and 19% of sentence variations, respectively.

</details>

<details>

<summary>2020-05-26 11:18:18 - Teacher-Student Framework Enhanced Multi-domain Dialogue Generation</summary>

- *Shuke Peng, Xinjing Huang, Zehao Lin, Feng Ji, Haiqing Chen, Yin Zhang*

- `1908.07137v2` - [abs](http://arxiv.org/abs/1908.07137v2) - [pdf](http://arxiv.org/pdf/1908.07137v2)

> Dialogue systems dealing with multi-domain tasks are highly required. How to record the state remains a key problem in a task-oriented dialogue system. Normally we use human-defined features as dialogue states and apply a state tracker to extract these features. However, the performance of such a system is limited by the error propagation of a state tracker. In this paper, we propose a dialogue generation model that needs no external state trackers and still benefits from human-labeled semantic data. By using a teacher-student framework, several teacher models are firstly trained in their individual domains, learn dialogue policies from labeled states. And then the learned knowledge and experience are merged and transferred to a universal student model, which takes raw utterance as its input. Experiments show that the dialogue system trained under our framework outperforms the one uses a belief tracker.

</details>

<details>

<summary>2020-05-27 02:02:57 - Counterfactual Detection meets Transfer Learning</summary>

- *Kelechi Nwaike, Licheng Jiao*

- `2005.13125v1` - [abs](http://arxiv.org/abs/2005.13125v1) - [pdf](http://arxiv.org/pdf/2005.13125v1)

> We can consider Counterfactuals as belonging in the domain of Discourse structure and semantics, A core area in Natural Language Understanding and in this paper, we introduce an approach to resolving counterfactual detection as well as the indexing of the antecedents and consequents of Counterfactual statements. While Transfer learning is already being applied to several NLP tasks, It has the characteristics to excel in a novel number of tasks. We show that detecting Counterfactuals is a straightforward Binary Classification Task that can be implemented with minimal adaptation on already existing model Architectures, thanks to a well annotated training data set,and we introduce a new end to end pipeline to process antecedents and consequents as an entity recognition task, thus adapting them into Token Classification.

</details>

<details>

<summary>2020-05-27 02:11:25 - Learning Semantic Program Embeddings with Graph Interval Neural Network</summary>

- *Yu Wang, Fengjuan Gao, Linzhang Wang, Ke Wang*

- `2005.09997v2` - [abs](http://arxiv.org/abs/2005.09997v2) - [pdf](http://arxiv.org/pdf/2005.09997v2)

> Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous and expensive message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals for mining the feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer.

</details>

<details>

<summary>2020-05-27 06:02:58 - Learning to segment from misaligned and partial labels</summary>

- *Simone Fobi, Terence Conlon, Jayant Taneja, Vijay Modi*

- `2005.13180v1` - [abs](http://arxiv.org/abs/2005.13180v1) - [pdf](http://arxiv.org/pdf/2005.13180v1)

> To extract information at scale, researchers increasingly apply semantic segmentation techniques to remotely-sensed imagery. While fully-supervised learning enables accurate pixel-wise segmentation, compiling the exhaustive datasets required is often prohibitively expensive. As a result, many non-urban settings lack the ground-truth needed for accurate segmentation. Existing open source infrastructure data for these regions can be inexact and non-exhaustive. Open source infrastructure annotations like OpenStreetMaps (OSM) are representative of this issue: while OSM labels provide global insights to road and building footprints, noisy and partial annotations limit the performance of segmentation algorithms that learn from them. In this paper, we present a novel and generalizable two-stage framework that enables improved pixel-wise image segmentation given misaligned and missing annotations. First, we introduce the Alignment Correction Network to rectify incorrectly registered open source labels. Next, we demonstrate a segmentation model -- the Pointer Segmentation Network -- that uses corrected labels to predict infrastructure footprints despite missing annotations. We test sequential performance on the AIRS dataset, achieving a mean intersection-over-union score of 0.79; more importantly, model performance remains stable as we decrease the fraction of annotations present. We demonstrate the transferability of our method to lower quality data, by applying the Alignment Correction Network to OSM labels to correct building footprints; we also demonstrate the accuracy of the Pointer Segmentation Network in predicting cropland boundaries in California from medium resolution data. Overall, our methodology is robust for multiple applications with varied amounts of training data present, thus offering a method to extract reliable information from noisy, partial data.

</details>

<details>

<summary>2020-05-27 07:58:35 - Learning Tversky Similarity</summary>

- *Javad Rahnama, Eyke Hüllermeier*

- `2006.11372v1` - [abs](http://arxiv.org/abs/2006.11372v1) - [pdf](http://arxiv.org/pdf/2006.11372v1)

> In this paper, we advocate Tversky's ratio model as an appropriate basis for computational approaches to semantic similarity, that is, the comparison of objects such as images in a semantically meaningful way. We consider the problem of learning Tversky similarity measures from suitable training data indicating whether two objects tend to be similar or dissimilar. Experimentally, we evaluate our approach to similarity learning on two image datasets, showing that is performs very well compared to existing methods.

</details>

<details>

<summary>2020-05-27 08:48:48 - Self-Supervised Representation Learning on Document Images</summary>

- *Adrian Cosma, Mihai Ghidoveanu, Michael Panaitescu-Liess, Marius Popescu*

- `2004.10605v2` - [abs](http://arxiv.org/abs/2004.10605v2) - [pdf](http://arxiv.org/pdf/2004.10605v2)

> This work analyses the impact of self-supervised pre-training on document images in the context of document image classification. While previous approaches explore the effect of self-supervision on natural images, we show that patch-based pre-training performs poorly on document images because of their different structural properties and poor intra-sample semantic information. We propose two context-aware alternatives to improve performance on the Tobacco-3482 image classification task. We also propose a novel method for self-supervision, which makes use of the inherent multi-modality of documents (image and text), which performs better than other popular self-supervised methods, including supervised ImageNet pre-training, on document image classification scenarios with a limited amount of data.

</details>

<details>

<summary>2020-05-27 08:49:42 - Real-Time Semantic Background Subtraction</summary>

- *Anthony Cioppa, Marc Van Droogenbroeck, Marc Braham*

- `2002.04993v3` - [abs](http://arxiv.org/abs/2002.04993v3) - [pdf](http://arxiv.org/pdf/2002.04993v3)

> Semantic background subtraction SBS has been shown to improve the performance of most background subtraction algorithms by combining them with semantic information, derived from a semantic segmentation network. However, SBS requires high-quality semantic segmentation masks for all frames, which are slow to compute. In addition, most state-of-the-art background subtraction algorithms are not real-time, which makes them unsuitable for real-world applications. In this paper, we present a novel background subtraction algorithm called Real-Time Semantic Background Subtraction (denoted RT-SBS) which extends SBS for real-time constrained applications while keeping similar performances. RT-SBS effectively combines a real-time background subtraction algorithm with high-quality semantic information which can be provided at a slower pace, independently for each pixel. We show that RT-SBS coupled with ViBe sets a new state of the art for real-time background subtraction algorithms and even competes with the non real-time state-of-the-art ones. Note that we provide python CPU and GPU implementations of RT-SBS at https://github.com/cioppaanthony/rt-sbs.

</details>

<details>

<summary>2020-05-27 09:38:58 - Obfuscation for Privacy-preserving Syntactic Parsing</summary>

- *Zhifeng Hu, Serhii Havrylov, Ivan Titov, Shay B. Cohen*

- `1904.09585v2` - [abs](http://arxiv.org/abs/1904.09585v2) - [pdf](http://arxiv.org/pdf/1904.09585v2)

> The goal of homomorphic encryption is to encrypt data such that another party can operate on it without being explicitly exposed to the content of the original data. We introduce an idea for a privacy-preserving transformation on natural language data, inspired by homomorphic encryption. Our primary tool is {\em obfuscation}, relying on the properties of natural language. Specifically, a given English text is obfuscated using a neural model that aims to preserve the syntactic relationships of the original sentence so that the obfuscated sentence can be parsed instead of the original one. The model works at the word level, and learns to obfuscate each word separately by changing it into a new word that has a similar syntactic role. The text obfuscated by our model leads to better performance on three syntactic parsers (two dependency and one constituency parsers) in comparison to an upper-bound random substitution baseline. More specifically, the results demonstrate that as more terms are obfuscated (by their part of speech), the substitution upper bound significantly degrades, while the neural model maintains a relatively high performing parser. All of this is done without much sacrifice of privacy compared to the random substitution upper bound. We also further analyze the results, and discover that the substituted words have similar syntactic properties, but different semantic content, compared to the original words.

</details>

<details>

<summary>2020-05-27 10:47:15 - Who is this Explanation for? Human Intelligence and Knowledge Graphs for eXplainable AI</summary>

- *Irene Celino*

- `2005.13275v1` - [abs](http://arxiv.org/abs/2005.13275v1) - [pdf](http://arxiv.org/pdf/2005.13275v1)

> eXplainable AI focuses on generating explanations for the output of an AI algorithm to a user, usually a decision-maker. Such user needs to interpret the AI system in order to decide whether to trust the machine outcome. When addressing this challenge, therefore, proper attention should be given to produce explanations that are interpretable by the target community of users. In this chapter, we claim for the need to better investigate what constitutes a human explanation, i.e. a justification of the machine behaviour that is interpretable and actionable by the human decision makers. In particular, we focus on the contributions that Human Intelligence can bring to eXplainable AI, especially in conjunction with the exploitation of Knowledge Graphs. Indeed, we call for a better interplay between Knowledge Representation and Reasoning, Social Sciences, Human Computation and Human-Machine Cooperation research -- as already explored in other AI branches -- in order to support the goal of eXplainable AI with the adoption of a Human-in-the-Loop approach.

</details>

<details>

<summary>2020-05-27 11:24:29 - GPT-too: A language-model-first approach for AMR-to-text generation</summary>

- *Manuel Mager, Ramon Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, Salim Roukos*

- `2005.09123v2` - [abs](http://arxiv.org/abs/2005.09123v2) - [pdf](http://arxiv.org/pdf/2005.09123v2)

> Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.

</details>

<details>

<summary>2020-05-27 11:56:22 - Accelerating Neural Network Inference by Overflow Aware Quantization</summary>

- *Hongwei Xie, Shuo Zhang, Huanghao Ding, Yafei Song, Baitao Shao, Conggang Hu, Ling Cai, Mingyang Li*

- `2005.13297v1` - [abs](http://arxiv.org/abs/2005.13297v1) - [pdf](http://arxiv.org/pdf/2005.13297v1)

> The inherent heavy computation of deep neural networks prevents their widespread applications. A widely used method for accelerating model inference is quantization, by replacing the input operands of a network using fixed-point values. Then the majority of computation costs focus on the integer matrix multiplication accumulation. In fact, high-bit accumulator leads to partially wasted computation and low-bit one typically suffers from numerical overflow. To address this problem, we propose an overflow aware quantization method by designing trainable adaptive fixed-point representation, to optimize the number of bits for each input tensor while prohibiting numeric overflow during the computation. With the proposed method, we are able to fully utilize the computing power to minimize the quantization loss and obtain optimized inference performance. To verify the effectiveness of our method, we conduct image classification, object detection, and semantic segmentation tasks on ImageNet, Pascal VOC, and COCO datasets, respectively. Experimental results demonstrate that the proposed method can achieve comparable performance with state-of-the-art quantization methods while accelerating the inference process by about 2 times.

</details>

<details>

<summary>2020-05-27 14:52:21 - The First Shared Task on Discourse Representation Structure Parsing</summary>

- *Lasha Abzianidze, Rik van Noord, Hessel Haagsma, Johan Bos*

- `2005.13399v1` - [abs](http://arxiv.org/abs/2005.13399v1) - [pdf](http://arxiv.org/pdf/2005.13399v1)

> The paper presents the IWCS 2019 shared task on semantic parsing where the goal is to produce Discourse Representation Structures (DRSs) for English sentences. DRSs originate from Discourse Representation Theory and represent scoped meaning representations that capture the semantics of negation, modals, quantification, and presupposition triggers. Additionally, concepts and event-participants in DRSs are described with WordNet synsets and the thematic roles from VerbNet. To measure similarity between two DRSs, they are represented in a clausal form, i.e. as a set of tuples. Participant systems were expected to produce DRSs in this clausal form. Taking into account the rich lexical information, explicit scope marking, a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser.

</details>

<details>

<summary>2020-05-27 14:57:01 - MOPT: Multi-Object Panoptic Tracking</summary>

- *Juana Valeria Hurtado, Rohit Mohan, Wolfram Burgard, Abhinav Valada*

- `2004.08189v2` - [abs](http://arxiv.org/abs/2004.08189v2) - [pdf](http://arxiv.org/pdf/2004.08189v2)

> Comprehensive understanding of dynamic scenes is a critical prerequisite for intelligent robots to autonomously operate in their environment. Research in this domain, which encompasses diverse perception problems, has primarily been focused on addressing specific tasks individually rather than modeling the ability to understand dynamic scenes holistically. In this paper, we introduce a novel perception task denoted as multi-object panoptic tracking (MOPT), which unifies the conventionally disjoint tasks of semantic segmentation, instance segmentation, and multi-object tracking. MOPT allows for exploiting pixel-level semantic information of 'thing' and 'stuff' classes, temporal coherence, and pixel-level associations over time, for the mutual benefit of each of the individual sub-problems. To facilitate quantitative evaluations of MOPT in a unified manner, we propose the soft panoptic tracking quality (sPTQ) metric. As a first step towards addressing this task, we propose the novel PanopticTrackNet architecture that builds upon the state-of-the-art top-down panoptic segmentation network EfficientPS by adding a new tracking head to simultaneously learn all sub-tasks in an end-to-end manner. Additionally, we present several strong baselines that combine predictions from state-of-the-art panoptic segmentation and multi-object tracking models for comparison. We present extensive quantitative and qualitative evaluations of both vision-based and LiDAR-based MOPT that demonstrate encouraging results.

</details>

<details>

<summary>2020-05-27 15:28:12 - Thirty Musts for Meaning Banking</summary>

- *Johan Bos, Lasha Abzianidze*

- `2005.13421v1` - [abs](http://arxiv.org/abs/2005.13421v1) - [pdf](http://arxiv.org/pdf/2005.13421v1)

> Meaning banking--creating a semantically annotated corpus for the purpose of semantic parsing or generation--is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper's format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4-11), and design of meaning representations (Section 12-30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future.

</details>

<details>

<summary>2020-05-27 17:20:20 - Scheduling Flows on a Switch to Optimize Response Times</summary>

- *Hamidreza Jahanjou, Rajmohan Rajaraman, David Stalfa*

- `2005.09724v3` - [abs](http://arxiv.org/abs/2005.09724v3) - [pdf](http://arxiv.org/pdf/2005.09724v3)

> We study the scheduling of flows on a switch with the goal of optimizing metrics related to the response time of the flows. The input to the problem is a sequence of flow requests on a switch, where the switch is represented by a bipartite graph with a capacity on each vertex (or port), and a flow request is an edge with associated demand. In each round, a subset of edges can be scheduled subject to the constraint that the total demand of the scheduled edges incident on any vertex is at most the capacity of the vertex. Previous work has essentially settled the complexity of metrics based on {\em completion time}. The objective of average or maximum {\em response time}, however, is much more challenging.   We present approximation algorithms for flow scheduling over a switch to optimize response time based metrics. For the average response time metric, whose NP-hardness follows directly from past work, we present an offline $O(1 + O(\log(n))/c)$ approximation algorithm for unit flows, assuming that the port capacities of the switch can be increased by a factor of $1 + c$, for any given positive integer $c$. For the maximum response time metric, we first establish that it is NP-hard to achieve an approximation factor of better than 4/3 without augmenting capacity. We then present an offline algorithm that achieves {\em optimal maximum response time}, assuming the capacity of each port is increased by at most $2 d_{max} - 1$, where $d_{max}$ is the maximum demand of any flow. Both algorithms are based on linear programming relaxations. We also study the online version of flow scheduling using the lens of competitive analysis, and present preliminary results along with experiments that evaluate the performance of fast online heuristics.

</details>

<details>

<summary>2020-05-27 21:21:46 - Live Trojan Attacks on Deep Neural Networks</summary>

- *Robby Costales, Chengzhi Mao, Raphael Norwitz, Bryan Kim, Junfeng Yang*

- `2004.11370v2` - [abs](http://arxiv.org/abs/2004.11370v2) - [pdf](http://arxiv.org/pdf/2004.11370v2)

> Like all software systems, the execution of deep learning models is dictated in part by logic represented as data in memory. For decades, attackers have exploited traditional software programs by manipulating this data. We propose a live attack on deep learning systems that patches model parameters in memory to achieve predefined malicious behavior on a certain set of inputs. By minimizing the size and number of these patches, the attacker can reduce the amount of network communication and memory overwrites, with minimal risk of system malfunctions or other detectable side effects. We demonstrate the feasibility of this attack by computing efficient patches on multiple deep learning models. We show that the desired trojan behavior can be induced with a few small patches and with limited access to training data. We describe the details of how this attack is carried out on real systems and provide sample code for patching TensorFlow model parameters in Windows and in Linux. Lastly, we present a technique for effectively manipulating entropy on perturbed inputs to bypass STRIP, a state-of-the-art run-time trojan detection technique.

</details>

<details>

<summary>2020-05-28 07:58:41 - Boosting Few-Shot Learning With Adaptive Margin Loss</summary>

- *Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, Liwei Wang*

- `2005.13826v1` - [abs](http://arxiv.org/abs/2005.13826v1) - [pdf](http://arxiv.org/pdf/2005.13826v1)

> Few-shot learning (FSL) has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in learning to generalize from a few examples. This paper proposes an adaptive margin principle to improve the generalization ability of metric-based meta-learning approaches for few-shot learning problems. Specifically, we first develop a class-relevant additive margin loss, where semantic similarity between each pair of classes is considered to separate samples in the feature embedding space from similar classes. Further, we incorporate the semantic context among all classes in a sampled training task and develop a task-relevant additive margin loss to better distinguish samples from different classes. Our adaptive margin method can be easily extended to a more realistic generalized FSL setting. Extensive experiments demonstrate that the proposed method can boost the performance of current metric-based meta-learning approaches, under both the standard FSL and generalized FSL settings.

</details>

<details>

<summary>2020-05-28 08:11:29 - Benchmarking neural embeddings for link prediction in knowledge graphs under semantic and structural changes</summary>

- *Asan Agibetov, Matthias Samwald*

- `2005.07654v2` - [abs](http://arxiv.org/abs/2005.07654v2) - [pdf](http://arxiv.org/pdf/2005.07654v2)

> Recently, link prediction algorithms based on neural embeddings have gained tremendous popularity in the Semantic Web community, and are extensively used for knowledge graph completion. While algorithmic advances have strongly focused on efficient ways of learning embeddings, fewer attention has been drawn to the different ways their performance and robustness can be evaluated. In this work we propose an open-source evaluation pipeline, which benchmarks the accuracy of neural embeddings in situations where knowledge graphs may experience semantic and structural changes. We define relation-centric connectivity measures that allow us to connect the link prediction capacity to the structure of the knowledge graph. Such an evaluation pipeline is especially important to simulate the accuracy of embeddings for knowledge graphs that are expected to be frequently updated.

</details>

<details>

<summary>2020-05-28 11:10:31 - Transition-based Semantic Dependency Parsing with Pointer Networks</summary>

- *Daniel Fernández-González, Carlos Gómez-Rodríguez*

- `2005.13344v2` - [abs](http://arxiv.org/abs/2005.13344v2) - [pdf](http://arxiv.org/pdf/2005.13344v2)

> Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT. The resulting system not only outperforms all existing transition-based models, but also matches the best fully-supervised accuracy to date on the SemEval 2015 Task 18 English datasets among previous state-of-the-art graph-based parsers.

</details>

<details>

<summary>2020-05-28 18:05:08 - Improving Community Resiliency and Emergency Response With Artificial Intelligence</summary>

- *Ben Ortiz, Laura Kahn, Marc Bosch, Philip Bogden, Viveca Pavon-Harr, Onur Savas, Ian McCulloh*

- `2005.14212v1` - [abs](http://arxiv.org/abs/2005.14212v1) - [pdf](http://arxiv.org/pdf/2005.14212v1)

> New crisis response and management approaches that incorporate the latest information technologies are essential in all phases of emergency preparedness and response, including the planning, response, recovery, and assessment phases. Accurate and timely information is as crucial as is rapid and coherent coordination among the responding organizations. We are working towards a multipronged emergency response tool that provide stakeholders timely access to comprehensive, relevant, and reliable information. The faster emergency personnel are able to analyze, disseminate and act on key information, the more effective and timelier their response will be and the greater the benefit to affected populations. Our tool consists of encoding multiple layers of open source geospatial data including flood risk location, road network strength, inundation maps that proxy inland flooding and computer vision semantic segmentation for estimating flooded areas and damaged infrastructure. These data layers are combined and used as input data for machine learning algorithms such as finding the best evacuation routes before, during and after an emergency or providing a list of available lodging for first responders in an impacted area for first. Even though our system could be used in a number of use cases where people are forced from one location to another, we demonstrate the feasibility of our system for the use case of Hurricane Florence in Lumberton, North Carolina.

</details>

<details>

<summary>2020-05-28 22:06:03 - On Incorporating Structural Information to improve Dialogue Response Generation</summary>

- *Nikita Moghe, Priyesh Vijayan, Balaraman Ravindran, Mitesh M. Khapra*

- `2005.14315v1` - [abs](http://arxiv.org/abs/2005.14315v1) - [pdf](http://arxiv.org/pdf/2005.14315v1)

> We consider the task of generating dialogue responses from background knowledge comprising of domain specific resources. Specifically, given a conversation around a movie, the task is to generate the next response based on background knowledge about the movie such as the plot, review, Reddit comments etc. This requires capturing structural, sequential and semantic information from the conversation context and the background resources. This is a new task and has not received much attention from the community. We propose a new architecture that uses the ability of BERT to capture deep contextualized representations in conjunction with explicit structure and sequence information. More specifically, we use (i) Graph Convolutional Networks (GCNs) to capture structural information, (ii) LSTMs to capture sequential information and (iii) BERT for the deep contextualized representations that capture semantic information. We analyze the proposed architecture extensively. To this end, we propose a plug-and-play Semantics-Sequences-Structures (SSS) framework which allows us to effectively combine such linguistic information. Through a series of experiments we make some interesting observations. First, we observe that the popular adaptation of the GCN model for NLP tasks where structural information (GCNs) was added on top of sequential information (LSTMs) performs poorly on our task. This leads us to explore interesting ways of combining semantic and structural information to improve the performance. Second, we observe that while BERT already outperforms other deep contextualized representations such as ELMo, it still benefits from the additional structural information explicitly added using GCNs. This is a bit surprising given the recent claims that BERT already captures structural information. Lastly, the proposed SSS framework gives an improvement of 7.95% over the baseline.

</details>

<details>

<summary>2020-05-28 22:56:12 - Neural Topological SLAM for Visual Navigation</summary>

- *Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh Gupta*

- `2005.12256v2` - [abs](http://arxiv.org/abs/2005.12256v2) - [pdf](http://arxiv.org/pdf/2005.12256v2)

> This paper studies the problem of image-goal navigation which involves navigating to the location indicated by a goal image in a novel previously unseen environment. To tackle this problem, we design topological representations for space that effectively leverage semantics and afford approximate geometric reasoning. At the heart of our representations are nodes with associated semantic features, that are interconnected using coarse geometric information. We describe supervised learning-based algorithms that can build, maintain and use such representations under noisy actuation. Experimental study in visually and physically realistic simulation suggests that our method builds effective representations that capture structural regularities and efficiently solve long-horizon navigation problems. We observe a relative improvement of more than 50% over existing methods that study this task.

</details>

<details>

<summary>2020-05-29 05:56:10 - FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval</summary>

- *Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, Hao Wang*

- `2005.09801v2` - [abs](http://arxiv.org/abs/2005.09801v2) - [pdf](http://arxiv.org/pdf/2005.09801v2)

> In this paper, we address the text and image matching in cross-modal retrieval of the fashion industry. Different from the matching in the general domain, the fashion matching is required to pay much more attention to the fine-grained information in the fashion images and texts. Pioneer approaches detect the region of interests (i.e., RoIs) from images and use the RoI embeddings as image representations. In general, RoIs tend to represent the "object-level" information in the fashion images, while fashion texts are prone to describe more detailed information, e.g. styles, attributes. RoIs are thus not fine-grained enough for fashion text and image matching. To this end, we propose FashionBERT, which leverages patches as image features. With the pre-trained BERT model as the backbone network, FashionBERT learns high level representations of texts and images. Meanwhile, we propose an adaptive loss to trade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text and image matching and cross-modal retrieval) are incorporated to evaluate FashionBERT. On the public dataset, experiments demonstrate FashionBERT achieves significant improvements in performances than the baseline and state-of-the-art approaches. In practice, FashionBERT is applied in a concrete cross-modal retrieval application. We provide the detailed matching performance and inference efficiency analysis.

</details>

<details>

<summary>2020-05-29 06:44:28 - Cats climb entails mammals move: preserving hyponymy in compositional distributional semantics</summary>

- *Gemma De las Cuevas, Andreas Klingler, Martha Lewis, Tim Netzer*

- `2005.14134v2` - [abs](http://arxiv.org/abs/2005.14134v2) - [pdf](http://arxiv.org/pdf/2005.14134v2)

> To give vector-based representations of meaning more structure, one approach is to use positive semidefinite (psd) matrices. These allow us to model similarity of words as well as the hyponymy or is-a relationship. Psd matrices can be learnt relatively easily in a given vector space $M\otimes M^*$, but to compose words to form phrases and sentences, we need representations in larger spaces. In this paper, we introduce a generic way of composing the psd matrices corresponding to words. We propose that psd matrices for verbs, adjectives, and other functional words be lifted to completely positive (CP) maps that match their grammatical type. This lifting is carried out by our composition rule called Compression, Compr. In contrast to previous composition rules like Fuzz and Phaser (a.k.a. KMult and BMult), Compr preserves hyponymy. Mathematically, Compr is itself a CP map, and is therefore linear and generally non-commutative. We give a number of proposals for the structure of Compr, based on spiders, cups and caps, and generate a range of composition rules. We test these rules on a small sentence entailment dataset, and see some improvements over the performance of Fuzz and Phaser.

</details>

<details>

<summary>2020-05-29 08:58:11 - Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing</summary>

- *Vedika Agarwal, Rakshith Shetty, Mario Fritz*

- `1912.07538v3` - [abs](http://arxiv.org/abs/1912.07538v3) - [pdf](http://arxiv.org/pdf/1912.07538v3)

> Despite significant success in Visual Question Answering (VQA), VQA models have been shown to be notoriously brittle to linguistic variations in the questions. Due to deficiencies in models and datasets, today's models often rely on correlations rather than predictions that are causal w.r.t. data. In this paper, we propose a novel way to analyze and measure the robustness of the state of the art models w.r.t semantic visual variations as well as propose ways to make models more robust against spurious correlations. Our method performs automated semantic image manipulations and tests for consistency in model predictions to quantify the model robustness as well as generate synthetic data to counter these problems. We perform our analysis on three diverse, state of the art VQA models and diverse question types with a particular focus on challenging counting questions. In addition, we show that models can be made significantly more robust against inconsistent predictions using our edited data. Finally, we show that results also translate to real-world error cases of state of the art models, which results in improved overall performance.

</details>

<details>

<summary>2020-05-29 13:56:31 - Harbsafe-162. A Domain-Specific Data Set for the Intrinsic Evaluation of Semantic Representations for Terminological Data</summary>

- *Susanne Arndt, Dieter Schnäpp*

- `2005.14576v1` - [abs](http://arxiv.org/abs/2005.14576v1) - [pdf](http://arxiv.org/pdf/2005.14576v1)

> The article presents Harbsafe-162, a domain-specific data set for evaluating distributional semantic models. It originates from a cooperation by Technische Universit\"at Braunschweig and the German Commission for Electrical, Electronic & Information Technologies of DIN and VDE, the Harbsafe project. One objective of the project is to apply distributional semantic models to terminological entries, that is, complex lexical data comprising of at least one or several terms, term phrases and a definition. This application is needed to solve a more complex problem: the harmonization of terminologies of standards and standards bodies (i.e. resolution of doublettes and inconsistencies). Due to a lack of evaluation data sets for terminological entries, the creation of Harbsafe-162 was a necessary step towards harmonization assistance. Harbsafe-162 covers data from nine electrotechnical standards in the domain of functional safety, IT security, and dependability. An intrinsic evaluation method in the form of a similarity rating task has been applied in which two linguists and three domain experts from standardization participated. The data set is used to evaluate a specific implementation of an established sentence embedding model. This implementation proves to be satisfactory for the domain-specific data so that further implementations for harmonization assistance may be brought forward by the project. Considering recent criticism on intrinsic evaluation methods, the article concludes with an evaluation of Harbsafe-162 and joins a more general discussion about the nature of similarity rating tasks. Harbsafe-162 has been made available for the community.

</details>

<details>

<summary>2020-05-29 15:53:06 - Vispi: Automatic Visual Perception and Interpretation of Chest X-rays</summary>

- *Xin Li, Rui Cao, Dongxiao Zhu*

- `1906.05190v3` - [abs](http://arxiv.org/abs/1906.05190v3) - [pdf](http://arxiv.org/pdf/1906.05190v3)

> Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.

</details>

<details>

<summary>2020-05-29 18:43:22 - A Comparative Study of Lexical Substitution Approaches based on Neural Language Models</summary>

- *Nikolay Arefyev, Boris Sheludko, Alexander Podolskiy, Alexander Panchenko*

- `2006.00031v1` - [abs](http://arxiv.org/abs/2006.00031v1) - [pdf](http://arxiv.org/pdf/2006.00031v1)

> Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper, we present a large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.

</details>

<details>

<summary>2020-05-29 19:45:25 - Expert2Coder: Capturing Divergent Brain Regions Using Mixture of Regression Experts</summary>

- *Subba Reddy Oota, Naresh Manwani, Raju S. Bapi*

- `1909.12299v2` - [abs](http://arxiv.org/abs/1909.12299v2) - [pdf](http://arxiv.org/pdf/1909.12299v2)

> fMRI semantic category understanding using linguistic encoding models attempts to learn a forward mapping that relates stimuli to the corresponding brain activation. State-of-the-art encoding models use a single global model (linear or non-linear) to predict brain activation given the stimulus. However, the critical assumption in these methods is that a priori different brain regions respond the same way to all the stimuli, that is, there is no modularity or specialization assumed for any region. This goes against the modularity theory, supported by many cognitive neuroscience investigations suggesting that there are functionally specialized regions in the brain. In this paper, we achieve this by clustering similar regions together and for every cluster we learn a different linear regression model using a mixture of linear experts model. The key idea here is that each linear expert captures the behaviour of similar brain regions. Given a new stimulus, the utility of the proposed model is twofold (i) predicts the brain activation as a weighted linear combination of the activations of multiple linear experts and (ii) to learn multiple experts corresponding to different brain regions. We argue that each expert captures activity patterns related to a particular region of interest (ROI) in the human brain. This study helps in understanding the brain regions that are activated together given different kinds of stimuli. Importantly, we suggest that the mixture of regression experts (MoRE) framework successfully combines the two principles of organization of function in the brain, namely that of specialization and integration. Experiments on fMRI data from paradigm 1 [1]where participants view linguistic stimuli show that the proposed MoRE model has better prediction accuracy compared to that of conventional models.

</details>

<details>

<summary>2020-05-29 20:56:55 - Proq: Projection-based Runtime Assertions for Debugging on a Quantum Computer</summary>

- *Gushu Li, Li Zhou, Nengkun Yu, Yufei Ding, Mingsheng Ying, Yuan Xie*

- `1911.12855v2` - [abs](http://arxiv.org/abs/1911.12855v2) - [pdf](http://arxiv.org/pdf/1911.12855v2)

> In this paper, we propose Proq, a runtime assertion scheme for testing and debugging quantum programs on a quantum computer. The predicates in Proq are represented by projections (or equivalently, closed subspaces of the state space), following Birkhoff-von Neumann quantum logic. The satisfaction of a projection by a quantum state can be directly checked upon a small number of projective measurements rather than a large number of repeated executions. On the theory side, we rigorously prove that checking projection-based assertions can help locate bugs or statistically assure that the semantic function of the tested program is close to what we expect, for both exact and approximate quantum programs. On the practice side, we consider hardware constraints and introduce several techniques to transform the assertions, making them directly executable on the measurement-restricted quantum computers. We also propose to achieve simplified assertion implementation using local projection technique with soundness guaranteed. We compare Proq with existing quantum program assertions and demonstrate the effectiveness and efficiency of Proq by its applications to assert two ingenious quantum algorithms, the Harrow-Hassidim-Lloyd algorithm and Shor's algorithm.

</details>

<details>

<summary>2020-05-29 22:56:25 - A frame semantics based approach to comparative study of digitized corpus</summary>

- *Abdelaziz Lakhfif, Mohamed Tayeb Laskri*

- `2006.00113v1` - [abs](http://arxiv.org/abs/2006.00113v1) - [pdf](http://arxiv.org/pdf/2006.00113v1)

> in this paper, we present a corpus linguistics based approach applied to analyzing digitized classical multilingual novels and narrative texts, from a semantic point of view. Digitized novels such as "the hobbit (Tolkien J. R. R., 1937)" and "the hound of the Baskervilles (Doyle A. C. 1901-1902)", which were widely translated to dozens of languages, provide rich materials for analyzing languages differences from several perspectives and within a number of disciplines like linguistics, philosophy and cognitive science. Taking motion events conceptualization as a case study, this paper, focus on the morphologic, syntactic, and semantic annotation process of English-Arabic aligned corpus created from a digitized novels, in order to re-examine the linguistic encodings of motion events in English and Arabic in terms of Frame Semantics. The present study argues that differences in motion events conceptualization across languages can be described with frame structure and frame-to-frame relations.

</details>

<details>

<summary>2020-05-29 22:56:43 - Design and Implementation of a Virtual 3D Educational Environment to improve Deaf Education</summary>

- *Abdelaziz Lakhfif*

- `2006.00114v1` - [abs](http://arxiv.org/abs/2006.00114v1) - [pdf](http://arxiv.org/pdf/2006.00114v1)

> Advances in NLP, knowledge representation and computer graphic technologies can provide us insights into the development of educational tool for Deaf people. Actual education materials and tools for deaf pupils present several problems, since textbooks are designed to support normal students in the classroom and most of them are not suitable for people with hearing disabilities. Virtual Reality (VR) technologies appear to be a good tool and a promising framework in the education of pupils with hearing disabilities. In this paper, we present a current research tasks surrounding the design and implementation of a virtual 3D educational environment based on X3D and H-Anim standards. The system generates and animates automatically Sign language sentence from a semantic representation that encode the whole meaning of the Arabic input text. Some aspects and issues in Sign language generation will be discussed, including the model of Sign representation that facilitate reuse and reduces the time of Sign generation, conversion of semantic components to sign features representation with regard to Sign language linguistics characteristics and how to generate realistic smooth gestural sequences using X3D content to performs transition between signs for natural-looking of animated avatar. Sign language sentences were evaluated by Algerian native Deaf people. The goal of the project is the development of a machine translation system from Arabic to Algerian Sign Language that can be used as educational tool for Deaf children in algerian primary schools.

</details>

<details>

<summary>2020-05-30 06:32:43 - Improving Slot Filling by Utilizing Contextual Information</summary>

- *Amir Pouran Ben Veyseh, Franck Dernoncourt, Thien Huu Nguyen*

- `1911.01680v2` - [abs](http://arxiv.org/abs/1911.01680v2) - [pdf](http://arxiv.org/pdf/1911.01680v2)

> Slot Filling (SF) is one of the sub-tasks of Spoken Language Understanding (SLU) which aims to extract semantic constituents from a given natural language utterance. It is formulated as a sequence labeling task. Recently, it has been shown that contextual information is vital for this task. However, existing models employ contextual information in a restricted manner, e.g., using self-attention. Such methods fail to distinguish the effects of the context on the word representation and the word label. To address this issue, in this paper, we propose a novel method to incorporate the contextual information in two different levels, i.e., representation level and task-specific (i.e., label) level. Our extensive experiments on three benchmark datasets on SF show the effectiveness of our model leading to new state-of-the-art results on all three benchmark datasets for the task of SF.

</details>

<details>

<summary>2020-05-30 14:59:07 - On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location</summary>

- *Osman Semih Kayhan, Jan C. van Gemert*

- `2003.07064v2` - [abs](http://arxiv.org/abs/2003.07064v2) - [pdf](http://arxiv.org/pdf/2003.07064v2)

> In this paper we challenge the common assumption that convolutional layers in modern CNNs are translation invariant. We show that CNNs can and will exploit the absolute spatial location by learning filters that respond exclusively to particular absolute locations by exploiting image boundary effects. Because modern CNNs filters have a huge receptive field, these boundary effects operate even far from the image boundary, allowing the network to exploit absolute spatial location all over the image. We give a simple solution to remove spatial location encoding which improves translation invariance and thus gives a stronger visual inductive bias which particularly benefits small data sets. We broadly demonstrate these benefits on several architectures and various applications such as image classification, patch matching, and two video classification datasets.

</details>

<details>

<summary>2020-05-30 18:28:49 - A Novel Approach for Generating SPARQL Queries from RDF Graphs</summary>

- *Emna Jabri*

- `2006.02862v1` - [abs](http://arxiv.org/abs/2006.02862v1) - [pdf](http://arxiv.org/pdf/2006.02862v1)

> This work is done as part of a research master's thesis project. The goal is to generate SPARQL queries based on user-supplied keywords to query RDF graphs. To do this, we first transformed the input ontology into an RDF graph that reflects the semantics represented in the ontology. Subsequently, we stored this RDF graph in the Neo4j graphical database to ensure efficient and persistent management of RDF data. At the time of the interrogation, we studied the different possible and desired interpretations of the request originally made by the user. We have also proposed to carry out a sort of transformation between the two query languages SPARQL and Cypher, which is specific to Neo4j. This allows us to implement the architecture of our system over a wide variety of BD-RDFs providing their query languages, without changing any of the other components of the system. Finally, we tested and evaluated our tool using different test bases, and it turned out that our tool is comprehensive, effective, and powerful enough.

</details>

<details>

<summary>2020-05-31 01:47:33 - Deep Cerebellar Nuclei Segmentation via Semi-Supervised Deep Context-Aware Learning from 7T Diffusion MRI</summary>

- *Jinyoung Kim, Remi Patriat, Jordan Kaplan, Oren Solomon, Noam Harel*

- `2004.09788v3` - [abs](http://arxiv.org/abs/2004.09788v3) - [pdf](http://arxiv.org/pdf/2004.09788v3)

> Deep cerebellar nuclei are a key structure of the cerebellum that are involved in processing motor and sensory information. It is thus a crucial step to accurately segment deep cerebellar nuclei for the understanding of the cerebellum system and its utility in deep brain stimulation treatment. However, it is challenging to clearly visualize such small nuclei under standard clinical magnetic resonance imaging (MRI) protocols and therefore precise segmentation is not feasible. Recent advances in 7 Tesla (T) MRI technology and great potential of deep neural networks facilitate automatic patient-specific segmentation. In this paper, we propose a novel deep learning framework (referred to as DCN-Net) for fast, accurate, and robust patient-specific segmentation of deep cerebellar dentate and interposed nuclei on 7T diffusion MRI. DCN-Net effectively encodes contextual information on the patch images without consecutive pooling operations and adding complexity via proposed dilated dense blocks. During the end-to-end training, label probabilities of dentate and interposed nuclei are independently learned with a hybrid loss, handling highly imbalanced data. Finally, we utilize self-training strategies to cope with the problem of limited labeled data. To this end, auxiliary dentate and interposed nuclei labels are created on unlabeled data by using DCN-Net trained on manual labels. We validate the proposed framework using 7T B0 MRIs from 60 subjects. Experimental results demonstrate that DCN-Net provides better segmentation than atlas-based deep cerebellar nuclei segmentation tools and other state-of-the-art deep neural networks in terms of accuracy and consistency. We further prove the effectiveness of the proposed components within DCN-Net in dentate and interposed nuclei segmentation.

</details>

<details>

<summary>2020-05-31 17:51:08 - Improve Document Embedding for Text Categorization Through Deep Siamese Neural Network</summary>

- *Erfaneh Gharavi, Hadi Veisi*

- `2006.00572v1` - [abs](http://arxiv.org/abs/2006.00572v1) - [pdf](http://arxiv.org/pdf/2006.00572v1)

> Due to the increasing amount of data on the internet, finding a highly-informative, low-dimensional representation for text is one of the main challenges for efficient natural language processing tasks including text classification. This representation should capture the semantic information of the text while retaining their relevance level for document classification. This approach maps the documents with similar topics to a similar space in vector space representation. To obtain representation for large text, we propose the utilization of deep Siamese neural networks. To embed document relevance in topics in the distributed representation, we use a Siamese neural network to jointly learn document representations. Our Siamese network consists of two sub-network of multi-layer perceptron. We examine our representation for the text categorization task on BBC news dataset. The results show that the proposed representations outperform the conventional and state-of-the-art representations in the text classification task on this dataset.

</details>


## 2020-06

<details>

<summary>2020-06-01 01:39:56 - Convolutional Neural Networks for Classification of Alzheimer's Disease: Overview and Reproducible Evaluation</summary>

- *Junhao Wen, Elina Thibeau-Sutre, Mauricio Diaz-Melo, Jorge Samper-Gonzalez, Alexandre Routier, Simona Bottani, Didier Dormont, Stanley Durrleman, Ninon Burgos, Olivier Colliot*

- `1904.07773v6` - [abs](http://arxiv.org/abs/1904.07773v6) - [pdf](http://arxiv.org/pdf/1904.07773v6)

> Over 30 papers have proposed to use convolutional neural network (CNN) for AD classification from anatomical MRI. However, the classification performance is difficult to compare across studies due to variations in components such as participant selection, image preprocessing or validation procedure. Moreover, these studies are hardly reproducible because their frameworks are not publicly accessible and because implementation details are lacking. Lastly, some of these papers may report a biased performance due to inadequate or unclear validation or model selection procedures. In the present work, we aim to address these limitations through three main contributions. First, we performed a systematic literature review and found that more than half of the surveyed papers may have suffered from data leakage. Our second contribution is the extension of our open-source framework for classification of AD using CNN and T1-weighted MRI. Finally, we used this framework to rigorously compare different CNN architectures. The data was split into training/validation/test sets at the very beginning and only the training/validation sets were used for model selection. To avoid any overfitting, the test sets were left untouched until the end of the peer-review process. Overall, the different 3D approaches (3D-subject, 3D-ROI, 3D-patch) achieved similar performances while that of the 2D slice approach was lower. Of note, the different CNN approaches did not perform better than a SVM with voxel-based features. The different approaches generalized well to similar populations but not to datasets with different inclusion criteria or demographical characteristics.

</details>

<details>

<summary>2020-06-01 03:16:05 - Symbol Spotting on Digital Architectural Floor Plans Using a Deep Learning-based Framework</summary>

- *Alireza Rezvanifar, Melissa Cote, Alexandra Branzan Albu*

- `2006.00684v1` - [abs](http://arxiv.org/abs/2006.00684v1) - [pdf](http://arxiv.org/pdf/2006.00684v1)

> This papers focuses on symbol spotting on real-world digital architectural floor plans with a deep learning (DL)-based framework. Traditional on-the-fly symbol spotting methods are unable to address the semantic challenge of graphical notation variability, i.e. low intra-class symbol similarity, an issue that is particularly important in architectural floor plan analysis. The presence of occlusion and clutter, characteristic of real-world plans, along with a varying graphical symbol complexity from almost trivial to highly complex, also pose challenges to existing spotting methods. In this paper, we address all of the above issues by leveraging recent advances in DL and adapting an object detection framework based on the You-Only-Look-Once (YOLO) architecture. We propose a training strategy based on tiles, avoiding many issues particular to DL-based object detection networks related to the relative small size of symbols compared to entire floor plans, aspect ratios, and data augmentation. Experiments on real-world floor plans demonstrate that our method successfully detects architectural symbols with low intra-class similarity and of variable graphical complexity, even in the presence of heavy occlusion and clutter. Additional experiments on the public SESYD dataset confirm that our proposed approach can deal with various degradation and noise levels and outperforms other symbol spotting methods.

</details>

<details>

<summary>2020-06-01 10:27:42 - Multi-scale Cloud Detection in Remote Sensing Images using a Dual Convolutional Neural Network</summary>

- *Markku Luotamo, Sari Metsämäki, Arto Klami*

- `2006.00836v1` - [abs](http://arxiv.org/abs/2006.00836v1) - [pdf](http://arxiv.org/pdf/2006.00836v1)

> Semantic segmentation by convolutional neural networks (CNN) has advanced the state of the art in pixel-level classification of remote sensing images. However, processing large images typically requires analyzing the image in small patches, and hence features that have large spatial extent still cause challenges in tasks such as cloud masking. To support a wider scale of spatial features while simultaneously reducing computational requirements for large satellite images, we propose an architecture of two cascaded CNN model components successively processing undersampled and full resolution images. The first component distinguishes between patches in the inner cloud area from patches at the cloud's boundary region. For the cloud-ambiguous edge patches requiring further segmentation, the framework then delegates computation to a fine-grained model component. We apply the architecture to a cloud detection dataset of complete Sentinel-2 multispectral images, approximately annotated for minimal false negatives in a land use application. On this specific task and data, we achieve a 16\% relative improvement in pixel accuracy over a CNN baseline based on patching.

</details>

<details>

<summary>2020-06-01 14:47:48 - Attention Word Embedding</summary>

- *Shashank Sonkar, Andrew E. Waters, Richard G. Baraniuk*

- `2006.00988v1` - [abs](http://arxiv.org/abs/2006.00988v1) - [pdf](http://arxiv.org/pdf/2006.00988v1)

> Word embedding models learn semantically rich vector representations of words and are widely used to initialize natural processing language (NLP) models. The popular continuous bag-of-words (CBOW) model of word2vec learns a vector embedding by masking a given word in a sentence and then using the other words as a context to predict it. A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others. We tackle this inefficiency by introducing the Attention Word Embedding (AWE) model, which integrates the attention mechanism into the CBOW model. We also propose AWE-S, which incorporates subword information. We demonstrate that AWE and AWE-S outperform the state-of-the-art word embedding models both on a variety of word similarity datasets and when used for initialization of NLP models.

</details>

<details>

<summary>2020-06-01 15:04:11 - A Neural Network Model of Lexical Competition during Infant Spoken Word Recognition</summary>

- *Mihaela Duta, Kim Plunkett*

- `2006.00999v1` - [abs](http://arxiv.org/abs/2006.00999v1) - [pdf](http://arxiv.org/pdf/2006.00999v1)

> Visual world studies show that upon hearing a word in a target-absent visual context containing related and unrelated items, toddlers and adults briefly direct their gaze towards phonologically related items, before shifting towards semantically and visually related ones. We present a neural network model that processes dynamic unfolding phonological representations and maps them to static internal semantic and visual representations. The model, trained on representations derived from real corpora, simulates this early phonological over semantic/visual preference. Our results support the hypothesis that incremental unfolding of a spoken word is in itself sufficient to account for the transient preference for phonological competitors over both unrelated and semantically and visually related ones. Phonological representations mapped dynamically in a bottom-up fashion to semantic-visual representations capture the early phonological preference effects reported in a visual world task. The semantic-visual preference observed later in such a trial does not require top-down feedback from a semantic or visual system.

</details>

<details>

<summary>2020-06-01 15:27:36 - Probing Emergent Semantics in Predictive Agents via Question Answering</summary>

- *Abhishek Das, Federico Carnevale, Hamza Merzic, Laura Rimell, Rosalia Schneider, Josh Abramson, Alden Hung, Arun Ahuja, Stephen Clark, Gregory Wayne, Felix Hill*

- `2006.01016v1` - [abs](http://arxiv.org/abs/2006.01016v1) - [pdf](http://arxiv.org/pdf/2006.01016v1)

> Recent work has shown how predictive modeling can endow agents with rich knowledge of their surroundings, improving their ability to act in complex environments. We propose question-answering as a general paradigm to decode and understand the representations that such agents develop, applying our method to two recent approaches to predictive modeling -action-conditional CPC (Guo et al., 2018) and SimCore (Gregor et al., 2019). After training agents with these predictive objectives in a visually-rich, 3D environment with an assortment of objects, colors, shapes, and spatial configurations, we probe their internal state representations with synthetic (English) questions, without backpropagating gradients from the question-answering decoder into the agent. The performance of different agents when probed this way reveals that they learn to encode factual, and seemingly compositional, information about objects, properties and spatial relations from their physical environment. Our approach is intuitive, i.e. humans can easily interpret responses of the model as opposed to inspecting continuous vectors, and model-agnostic, i.e. applicable to any modeling approach. By revealing the implicit knowledge of objects, quantities, properties and relations acquired by agents as they learn, question-conditional agent probing can stimulate the design and development of stronger predictive learning objectives.

</details>

<details>

<summary>2020-06-01 17:39:09 - SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word Models</summary>

- *Bin Wang, C. -C. Jay Kuo*

- `2002.06652v2` - [abs](http://arxiv.org/abs/2002.06652v2) - [pdf](http://arxiv.org/pdf/2002.06652v2)

> Sentence embedding is an important research topic in natural language processing (NLP) since it can transfer knowledge to downstream tasks. Meanwhile, a contextualized word representation, called BERT, achieves the state-of-the-art performance in quite a few NLP tasks. Yet, it is an open problem to generate a high quality sentence representation from BERT-based word models. It was shown in previous study that different layers of BERT capture different linguistic properties. This allows us to fusion information across layers to find better sentence representation. In this work, we study the layer-wise pattern of the word representation of deep contextualized models. Then, we propose a new sentence embedding method by dissecting BERT-based word models through geometric analysis of the space spanned by the word representation. It is called the SBERT-WK method. No further training is required in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and downstream supervised tasks. Furthermore, ten sentence-level probing tasks are presented for detailed linguistic analysis. Experiments show that SBERT-WK achieves the state-of-the-art performance. Our codes are publicly available.

</details>

<details>

<summary>2020-06-01 17:58:57 - NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg</summary>

- *Joshua Bambrick, Minjie Xu, Andy Almonte, Igor Malioutov, Guim Perarnau, Vittorio Selo, Iat Chong Chan*

- `2006.01117v1` - [abs](http://arxiv.org/abs/2006.01117v1) - [pdf](http://arxiv.org/pdf/2006.01117v1)

> Millions of news articles from hundreds of thousands of sources around the globe appear in news aggregators every day. Consuming such a volume of news presents an almost insurmountable challenge. For example, a reader searching on Bloomberg's system for news about the U.K. would find 10,000 articles on a typical day. Apple Inc., the world's most journalistically covered company, garners around 1,800 news articles a day.   We realized that a new kind of summarization engine was needed, one that would condense large volumes of news into short, easy to absorb points. The system would filter out noise and duplicates to identify and summarize key news about companies, countries or markets.   When given a user query, Bloomberg's solution, Key News Themes (or NSTM), leverages state-of-the-art semantic clustering techniques and novel summarization methods to produce comprehensive, yet concise, digests to dramatically simplify the news consumption process.   NSTM is available to hundreds of thousands of readers around the world and serves thousands of requests daily with sub-second latency. At ACL 2020, we will present a demo of NSTM.

</details>

<details>

<summary>2020-06-01 19:09:13 - Hybrid Improved Document-level Embedding (HIDE)</summary>

- *Satanik Mitra, Mamata Jenamani*

- `2006.01203v1` - [abs](http://arxiv.org/abs/2006.01203v1) - [pdf](http://arxiv.org/pdf/2006.01203v1)

> In recent times, word embeddings are taking a significant role in sentiment analysis. As the generation of word embeddings needs huge corpora, many applications use pretrained embeddings. In spite of the success, word embeddings suffers from certain drawbacks such as it does not capture sentiment information of a word, contextual information in terms of parts of speech tags and domain-specific information. In this work we propose HIDE a Hybrid Improved Document level Embedding which incorporates domain information, parts of speech information and sentiment information into existing word embeddings such as GloVe and Word2Vec. It combine improved word embeddings into document level embeddings. Further, Latent Semantic Analysis (LSA) has been used to represent documents as a vectors. HIDE is generated, combining LSA and document level embeddings, which is computed from improved word embeddings. We test HIDE with six different datasets and shown considerable improvement over the accuracy of existing pretrained word vectors such as GloVe and Word2Vec. We further compare our work with two existing document level sentiment analysis approaches. HIDE performs better than existing systems.

</details>

<details>

<summary>2020-06-01 21:34:05 - Planning Beyond the Sensing Horizon Using a Learned Context</summary>

- *Michael Everett, Justin Miller, Jonathan P. How*

- `1908.09171v3` - [abs](http://arxiv.org/abs/1908.09171v3) - [pdf](http://arxiv.org/pdf/1908.09171v3)

> Last-mile delivery systems commonly propose the use of autonomous robotic vehicles to increase scalability and efficiency. The economic inefficiency of collecting accurate prior maps for navigation motivates the use of planning algorithms that operate in unmapped environments. However, these algorithms typically waste time exploring regions that are unlikely to contain the delivery destination. Context is key information about structured environments that could guide exploration toward the unknown goal location, but the abstract idea is difficult to quantify for use in a planning algorithm. Some approaches specifically consider contextual relationships between objects, but would perform poorly in object-sparse environments like outdoors. Recent deep learning-based approaches consider context too generally, making training/transferability difficult. Therefore, this work proposes a novel formulation of utilizing context for planning as an image-to-image translation problem, which is shown to extract terrain context from semantic gridmaps, into a metric that an exploration-based planner can use. The proposed framework has the benefit of training on a static dataset instead of requiring a time-consuming simulator. Across 42 test houses with layouts from satellite images, the trained algorithm enables a robot to reach its goal 189\% faster than with a context-unaware planner, and within 63\% of the optimal path computed with a prior map. The proposed algorithm is also implemented on a vehicle with a forward-facing camera in a high-fidelity, Unreal simulation of neighborhood houses.

</details>

<details>

<summary>2020-06-01 22:01:15 - Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback</summary>

- *Ahmed Elgohary, Saghar Hosseini, Ahmed Hassan Awadallah*

- `2005.02539v2` - [abs](http://arxiv.org/abs/2005.02539v2) - [pdf](http://arxiv.org/pdf/2005.02539v2)

> We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback. We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction. While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research. SPLASH is publicly available at https://aka.ms/Splash_dataset.

</details>

<details>

<summary>2020-06-01 22:21:40 - NEMA: Automatic Integration of Large Network Management Databases</summary>

- *Fubao Wu, Han Hee Song, Jiangtao Yin, Lixin Gao, Mario Baldi, Narendra Anand*

- `2006.01294v1` - [abs](http://arxiv.org/abs/2006.01294v1) - [pdf](http://arxiv.org/pdf/2006.01294v1)

> Network management, whether for malfunction analysis, failure prediction, performance monitoring and improvement, generally involves large amounts of data from different sources. To effectively integrate and manage these sources, automatically finding semantic matches among their schemas or ontologies is crucial. Existing approaches on database matching mainly fall into two categories. One focuses on the schema-level matching based on schema properties such as field names, data types, constraints and schema structures. Network management databases contain massive tables (e.g., network products, incidents, security alert and logs) from different departments and groups with nonuniform field names and schema characteristics. It is not reliable to match them by those schema properties. The other category is based on the instance-level matching using general string similarity techniques, which are not applicable for the matching of large network management databases. In this paper, we develop a matching technique for large NEtwork MAnagement databases (NEMA) deploying instance-level matching for effective data integration and connection. We design matching metrics and scores for both numerical and non-numerical fields and propose algorithms for matching these fields. The effectiveness and efficiency of NEMA are evaluated by conducting experiments based on ground truth field pairs in large network management databases. Our measurement on large databases with 1,458 fields, each of which contains over 10 million records, reveals that the accuracies of NEMA are up to 95%. It achieves 2%-10% higher accuracy and 5x-14x speedup over baseline methods.

</details>

<details>

<summary>2020-06-02 02:12:19 - A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension</summary>

- *Jie Cai, Zhengzhou Zhu, Ping Nie, Qian Liu*

- `2006.01346v1` - [abs](http://arxiv.org/abs/2006.01346v1) - [pdf](http://arxiv.org/pdf/2006.01346v1)

> Pre-trained models have brought significant improvements to many NLP tasks and have been extensively analyzed. But little is known about the effect of fine-tuning on specific tasks. Intuitively, people may agree that a pre-trained model already learns semantic representations of words (e.g. synonyms are closer to each other) and fine-tuning further improves its capabilities which require more complicated reasoning (e.g. coreference resolution, entity boundary detection, etc). However, how to verify these arguments analytically and quantitatively is a challenging task and there are few works focus on this topic. In this paper, inspired by the observation that most probing tasks involve identifying matched pairs of phrases (e.g. coreference requires matching an entity and a pronoun), we propose a pairwise probe to understand BERT fine-tuning on the machine reading comprehension (MRC) task. Specifically, we identify five phenomena in MRC. According to pairwise probing tasks, we compare the performance of each layer's hidden representation of pre-trained and fine-tuned BERT. The proposed pairwise probe alleviates the problem of distraction from inaccurate model training and makes a robust and quantitative comparison. Our experimental analysis leads to highly confident conclusions: (1) Fine-tuning has little effect on the fundamental and low-level information and general semantic tasks. (2) For specific abilities required for downstream tasks, fine-tuned BERT is better than pre-trained BERT and such gaps are obvious after the fifth layer.

</details>

<details>

<summary>2020-06-02 02:22:10 - Generating Diverse Story Continuations with Controllable Semantics</summary>

- *Lifu Tu, Xiaoan Ding, Dong Yu, Kevin Gimpel*

- `1909.13434v2` - [abs](http://arxiv.org/abs/1909.13434v2) - [pdf](http://arxiv.org/pdf/1909.13434v2)

> We propose a simple and effective modeling framework for controlled generation of multiple, diverse outputs. We focus on the setting of generating the next sentence of a story given its context. As controllable dimensions, we consider several sentence attributes, including sentiment, length, predicates, frames, and automatically-induced clusters. Our empirical results demonstrate: (1) our framework is accurate in terms of generating outputs that match the target control values; (2) our model yields increased maximum metric scores compared to standard n-best list generation via beam search; (3) controlling generation with semantic frames leads to a stronger combination of diversity and quality than other control variables as measured by automatic metrics. We also conduct a human evaluation to assess the utility of providing multiple suggestions for creative writing, demonstrating promising results for the potential of controllable, diverse generation in a collaborative writing system.

</details>

<details>

<summary>2020-06-02 03:08:41 - Self-adaptive Re-weighted Adversarial Domain Adaptation</summary>

- *Shanshan Wang, Lei Zhang*

- `2006.00223v2` - [abs](http://arxiv.org/abs/2006.00223v2) - [pdf](http://arxiv.org/pdf/2006.00223v2)

> Existing adversarial domain adaptation methods mainly consider the marginal distribution and these methods may lead to either under transfer or negative transfer. To address this problem, we present a self-adaptive re-weighted adversarial domain adaptation approach, which tries to enhance domain alignment from the perspective of conditional distribution. In order to promote positive transfer and combat negative transfer, we reduce the weight of the adversarial loss for aligned features while increasing the adversarial force for those poorly aligned measured by the conditional entropy. Additionally, triplet loss leveraging source samples and pseudo-labeled target samples is employed on the confusing domain. Such metric loss ensures the distance of the intra-class sample pairs closer than the inter-class pairs to achieve the class-level alignment. In this way, the high accurate pseudolabeled target samples and semantic alignment can be captured simultaneously in the co-training process. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David's theorem. Empirical evidence demonstrates that the proposed model outperforms state of the arts on standard domain adaptation datasets.

</details>

<details>

<summary>2020-06-02 06:53:21 - Graph-Stega: Semantic Controllable Steganographic Text Generation Guided by Knowledge Graph</summary>

- *Zhongliang Yang, Baitao Gong, Yamin Li, Jinshuai Yang, Zhiwen Hu, Yongfeng Huang*

- `2006.08339v1` - [abs](http://arxiv.org/abs/2006.08339v1) - [pdf](http://arxiv.org/pdf/2006.08339v1)

> Most of the existing text generative steganographic methods are based on coding the conditional probability distribution of each word during the generation process, and then selecting specific words according to the secret information, so as to achieve information hiding. Such methods have their limitations which may bring potential security risks. Firstly, with the increase of embedding rate, these models will choose words with lower conditional probability, which will reduce the quality of the generated steganographic texts; secondly, they can not control the semantic expression of the final generated steganographic text. This paper proposes a new text generative steganography method which is quietly different from the existing models. We use a Knowledge Graph (KG) to guide the generation of steganographic sentences. On the one hand, we hide the secret information by coding the path in the knowledge graph, but not the conditional probability of each generated word; on the other hand, we can control the semantic expression of the generated steganographic text to a certain extent. The experimental results show that the proposed model can guarantee both the quality of the generated text and its semantic expression, which is a supplement and improvement to the current text generation steganography.

</details>

<details>

<summary>2020-06-02 07:01:09 - Towards a Human-Centred Cognitive Model of Visuospatial Complexity in Everyday Driving</summary>

- *Vasiliki Kondyli, Mehul Bhatt, Jakob Suchan*

- `2006.00059v2` - [abs](http://arxiv.org/abs/2006.00059v2) - [pdf](http://arxiv.org/pdf/2006.00059v2)

> We develop a human-centred, cognitive model of visuospatial complexity in everyday, naturalistic driving conditions. With a focus on visual perception, the model incorporates quantitative, structural, and dynamic attributes identifiable in the chosen context; the human-centred basis of the model lies in its behavioural evaluation with human subjects with respect to psychophysical measures pertaining to embodied visuoauditory attention. We report preliminary steps to apply the developed cognitive model of visuospatial complexity for human-factors guided dataset creation and benchmarking, and for its use as a semantic template for the (explainable) computational analysis of visuospatial complexity.

</details>

<details>

<summary>2020-06-02 10:47:56 - iASiS Open Data Graph: Automated Semantic Integration of Disease-Specific Knowledge</summary>

- *Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, Georgios Paliouras*

- `1912.08633v2` - [abs](http://arxiv.org/abs/1912.08633v2) - [pdf](http://arxiv.org/pdf/1912.08633v2)

> In biomedical research, unified access to up-to-date domain-specific knowledge is crucial, as such knowledge is continuously accumulated in scientific literature and structured resources. Identifying and extracting specific information is a challenging task and computational analysis of knowledge bases can be valuable in this direction. However, for disease-specific analyses researchers often need to compile their own datasets, integrating knowledge from different resources, or reuse existing datasets, that can be out-of-date. In this study, we propose a framework to automatically retrieve and integrate disease-specific knowledge into an up-to-date semantic graph, the iASiS Open Data Graph. This disease-specific semantic graph provides access to knowledge relevant to specific concepts and their individual aspects, in the form of concept relations and attributes. The proposed approach is implemented as an open-source framework and applied to three diseases (Lung Cancer, Dementia, and Duchenne Muscular Dystrophy). Exemplary queries are presented, investigating the potential of this automatically generated semantic graph as a basis for retrieval and analysis of disease-specific knowledge.

</details>

<details>

<summary>2020-06-02 12:25:16 - Semantic Object Accuracy for Generative Text-to-Image Synthesis</summary>

- *Tobias Hinz, Stefan Heinrich, Stefan Wermter*

- `1910.13321v2` - [abs](http://arxiv.org/abs/1910.13321v2) - [pdf](http://arxiv.org/pdf/1910.13321v2)

> Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g. whether an image generated from "a car driving down the street" contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.

</details>

<details>

<summary>2020-06-02 13:39:53 - DiscSense: Automated Semantic Analysis of Discourse Markers</summary>

- *Damien Sileo, Tim Van de Cruys, Camille Pradel, Philippe Muller*

- `2006.01603v1` - [abs](http://arxiv.org/abs/2006.01603v1) - [pdf](http://arxiv.org/pdf/2006.01603v1)

> Discourse markers ({\it by contrast}, {\it happily}, etc.) are words or phrases that are used to signal semantic and/or pragmatic relationships between clauses or sentences. Recent work has fruitfully explored the prediction of discourse markers between sentence pairs in order to learn accurate sentence representations, that are useful in various classification tasks. In this work, we take another perspective: using a model trained to predict discourse markers between sentence pairs, we predict plausible markers between sentence pairs with a known semantic relation (provided by existing classification datasets). These predictions allow us to study the link between discourse markers and the semantic relations annotated in classification datasets. Handcrafted mappings have been proposed between markers and discourse relations on a limited set of markers and a limited set of categories, but there exist hundreds of discourse markers expressing a wide variety of relations, and there is no consensus on the taxonomy of relations between competing discourse theories (which are largely built in a top-down fashion). By using an automatic rediction method over existing semantically annotated datasets, we provide a bottom-up characterization of discourse markers in English. The resulting dataset, named DiscSense, is publicly available.

</details>

<details>

<summary>2020-06-02 14:10:28 - Relational Learning Analysis of Social Politics using Knowledge Graph Embedding</summary>

- *Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, Hossam Faris, Pornpit Wongthongtham*

- `2006.01626v1` - [abs](http://arxiv.org/abs/2006.01626v1) - [pdf](http://arxiv.org/pdf/2006.01626v1)

> Knowledge Graphs (KGs) have gained considerable attention recently from both academia and industry. In fact, incorporating graph technology and the copious of various graph datasets have led the research community to build sophisticated graph analytics tools. Therefore, the application of KGs has extended to tackle a plethora of real-life problems in dissimilar domains. Despite the abundance of the currently proliferated generic KGs, there is a vital need to construct domain-specific KGs. Further, quality and credibility should be assimilated in the process of constructing and augmenting KGs, particularly those propagated from mixed-quality resources such as social media data. This paper presents a novel credibility domain-based KG Embedding framework. This framework involves capturing a fusion of data obtained from heterogeneous resources into a formal KG representation depicted by a domain ontology. The proposed approach makes use of various knowledge-based repositories to enrich the semantics of the textual contents, thereby facilitating the interoperability of information. The proposed framework also embodies a credibility module to ensure data quality and trustworthiness. The constructed KG is then embedded in a low-dimension semantically-continuous space using several embedding techniques. The utility of the constructed KG and its embeddings is demonstrated and substantiated on link prediction, clustering, and visualisation tasks.

</details>

<details>

<summary>2020-06-02 15:35:05 - Web Document Categorization Using Naive Bayes Classifier and Latent Semantic Analysis</summary>

- *Alireza Saleh Sedghpour, Mohammad Reza Saleh Sedghpour*

- `2006.01715v1` - [abs](http://arxiv.org/abs/2006.01715v1) - [pdf](http://arxiv.org/pdf/2006.01715v1)

> A rapid growth of web documents due to heavy use of World Wide Web necessitates efficient techniques to efficiently classify the document on the web. It is thus produced High volumes of data per second with high diversity. Automatically classification of these growing amounts of web document is One of the biggest challenges facing us today. Probabilistic classification algorithms such as Naive Bayes have become commonly used for web document classification. This problem is mainly because of the irrelatively high classification accuracy on plenty application areas as well as their lack of support to handle high dimensional and sparse data which is the exclusive characteristics of textual data representation. also it is common to Lack of attention and support the semantic relation between words using traditional feature selection method When dealing with the big data and large-scale web documents. In order to solve the problem, we proposed a method for web document classification that uses LSA to increase similarity of documents under the same class and improve the classification precision. Using this approach, we designed a faster and much accurate classifier for Web Documents. Experimental results have shown that using the mentioned preprocessing can improve accuracy and speed of Naive Bayes availably, the precision and recall metrics have indicated the improvement.

</details>

<details>

<summary>2020-06-02 20:50:43 - Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings</summary>

- *Vaibhav Kumar, Tenzin Singhay Bhotia, Vaibhav Kumar, Tanmoy Chakraborty*

- `2006.01938v1` - [abs](http://arxiv.org/abs/2006.01938v1) - [pdf](http://arxiv.org/pdf/2006.01938v1)

> Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology which not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighbouring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric - Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02%. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).

</details>

<details>

<summary>2020-06-02 22:31:40 - The Typology of Polysemy: A Multilingual Distributional Framework</summary>

- *Ella Rabinovich, Yang Xu, Suzanne Stevenson*

- `2006.01966v1` - [abs](http://arxiv.org/abs/2006.01966v1) - [pdf](http://arxiv.org/pdf/2006.01966v1)

> Lexical semantic typology has identified important cross-linguistic generalizations about the variation and commonalities in polysemy patterns---how languages package up meanings into words. Recent computational research has enabled investigation of lexical semantics at a much larger scale, but little work has explored lexical typology across semantic domains, nor the factors that influence cross-linguistic similarities. We present a novel computational framework that quantifies semantic affinity, the cross-linguistic similarity of lexical semantics for a concept. Our approach defines a common multilingual semantic space that enables a direct comparison of the lexical expression of concepts across languages. We validate our framework against empirical findings on lexical semantic typology at both the concept and domain levels. Our results reveal an intricate interaction between semantic domains and extra-linguistic factors, beyond language phylogeny, that co-shape the typology of polysemy across languages.

</details>

<details>

<summary>2020-06-02 22:56:16 - Dynamic Term-Modal Logics for First-Order Epistemic Planning</summary>

- *Andrés Occhipinti Liberman, Andreas Achen, Rasmus Kræmmer Rendsvig*

- `1906.06047v2` - [abs](http://arxiv.org/abs/1906.06047v2) - [pdf](http://arxiv.org/pdf/1906.06047v2)

> Many classical planning frameworks are built on first-order languages. The first-order expressive power is desirable for compactly representing actions via schemas, and for specifying quantified conditions such as $\neg\exists x\mathsf{blocks\_door}(x)$. In contrast, several recent epistemic planning frameworks are built on propositional epistemic logic. The epistemic language is useful to describe planning problems involving higher-order reasoning or epistemic goals such as $K_{a}\neg\mathsf{problem}$.   This paper develops a first-order version of Dynamic Epistemic Logic (DEL). In this framework, for example, $\exists xK_{x}\exists y\mathsf{blocks\_door}(y)$ is a formula. The formalism combines the strengths of DEL (higher-order reasoning) with those of first-order logic (lifted representation) to model multi-agent epistemic planning. The paper introduces an epistemic language with a possible-worlds semantics, followed by novel dynamics given by first-order action models and their execution via product updates. Taking advantage of the first-order machinery, epistemic action schemas are defined to provide compact, problem-independent domain descriptions, in the spirit of PDDL.   Concerning metatheory, the paper defines axiomatic normal term-modal logics, shows a Canonical Model Theorem-like result which allows establishing completeness through frame characterization formulas, shows decidability for the finite agent case, and shows a general completeness result for the dynamic extension by reduction axioms.

</details>

<details>

<summary>2020-06-03 04:54:28 - Sato: Contextual Semantic Type Detection in Tables</summary>

- *Dan Zhang, Yoshihiko Suhara, Jinfeng Li, Madelon Hulsebos, Çağatay Demiralp, Wang-Chiew Tan*

- `1911.06311v3` - [abs](http://arxiv.org/abs/1911.06311v3) - [pdf](http://arxiv.org/pdf/1911.06311v3)

> Detecting the semantic types of data columns in relational tables is important for various data preparation and information retrieval tasks such as data cleaning, schema matching, data discovery, and semantic search. However, existing detection approaches either perform poorly with dirty data, support only a limited number of semantic types, fail to incorporate the table context of columns or rely on large sample sizes for training data. We introduce Sato, a hybrid machine learning model to automatically detect the semantic types of columns in tables, exploiting the signals from the context as well as the column values. Sato combines a deep learning model trained on a large-scale table corpus with topic modeling and structured prediction to achieve support-weighted and macro average F1 scores of 0.925 and 0.735, respectively, exceeding the state-of-the-art performance by a significant margin. We extensively analyze the overall and per-type performance of Sato, discussing how individual modeling components, as well as feature categories, contribute to its performance.

</details>

<details>

<summary>2020-06-03 05:52:20 - Dialogue Coherence Assessment Without Explicit Dialogue Act Labels</summary>

- *Mohsen Mesgar, Sebastian Bücker, Iryna Gurevych*

- `1908.08486v2` - [abs](http://arxiv.org/abs/1908.08486v2) - [pdf](http://arxiv.org/pdf/1908.08486v2)

> Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence.

</details>

<details>

<summary>2020-06-03 09:02:37 - ACNN: a Full Resolution DCNN for Medical Image Segmentation</summary>

- *Xiao-Yun Zhou, Jian-Qing Zheng, Peichao Li, Guang-Zhong Yang*

- `1901.09203v4` - [abs](http://arxiv.org/abs/1901.09203v4) - [pdf](http://arxiv.org/pdf/1901.09203v4)

> Deep Convolutional Neural Networks (DCNNs) are used extensively in medical image segmentation and hence 3D navigation for robot-assisted Minimally Invasive Surgeries (MISs). However, current DCNNs usually use down sampling layers for increasing the receptive field and gaining abstract semantic information. These down sampling layers decrease the spatial dimension of feature maps, which can be detrimental to image segmentation. Atrous convolution is an alternative for the down sampling layer. It increases the receptive field whilst maintains the spatial dimension of feature maps. In this paper, a method for effective atrous rate setting is proposed to achieve the largest and fully-covered receptive field with a minimum number of atrous convolutional layers. Furthermore, a new and full resolution DCNN - Atrous Convolutional Neural Network (ACNN), which incorporates cascaded atrous II-blocks, residual learning and Instance Normalization (IN) is proposed. Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) image segmentation demonstrate that the proposed ACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net and Deeplabv3+, but with reduced trainable parameters.

</details>

<details>

<summary>2020-06-03 11:02:31 - A Mixed Initiative Semantic Web Framework for Process Composition</summary>

- *Jinghai Rao, Dimitar Dimitrov, Paul Hofmann, Norman Sadeh*

- `2006.02168v1` - [abs](http://arxiv.org/abs/2006.02168v1) - [pdf](http://arxiv.org/pdf/2006.02168v1)

> Semantic Web technologies offer the prospect of significantly reducing the amount of effort required to integrate existing enterprise functionality in support of new composite processes; whether within a given organization or across multiple ones. A significant body of work in this area has aimed to fully automate this process, while assuming that all functionality has already been encapsulated in the form of semantic web services with rich and accurate annotations. In this article, we argue that this assumption is often unrealistic. Instead, we describe a mixed initiative framework for semantic web service discovery and composition that aims at flexibly interleaving human decision making and automated functionality in environments where annotations may be incomplete and even inconsistent.

</details>

<details>

<summary>2020-06-03 11:21:42 - CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning</summary>

- *Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, Oliver Lemon*

- `2006.02174v1` - [abs](http://arxiv.org/abs/2006.02174v1) - [pdf](http://arxiv.org/pdf/2006.02174v1)

> Approaches to Grounded Language Learning typically focus on a single task-based final performance measure that may not depend on desirable properties of the learned hidden representations, such as their ability to predict salient attributes or to generalise to unseen situations. To remedy this, we present GROLLA, an evaluation framework for Grounded Language Learning with Attributes with three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular concerning attribute grounding. To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one. Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with abstract and situated attributes. By using diagnostic classifiers, we show that current models learn representations that are not expressive enough to encode object attributes (average F1 of 44.27). In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).

</details>

<details>

<summary>2020-06-03 17:14:44 - SQUIRREL: Testing Database Management Systems with Language Validity and Coverage Feedback</summary>

- *Rui Zhong, Yongheng Chen, Hong Hu, Hangfan Zhang, Wenke Lee, Dinghao Wu*

- `2006.02398v1` - [abs](http://arxiv.org/abs/2006.02398v1) - [pdf](http://arxiv.org/pdf/2006.02398v1)

> Fuzzing is an increasingly popular technique for verifying software functionalities and finding security vulnerabilities. However, current mutation-based fuzzers cannot effectively test database management systems (DBMSs), which strictly check inputs for valid syntax and semantics. Generation-based testing can guarantee the syntax correctness of the inputs, but it does not utilize any feedback, like code coverage, to guide the path exploration.   In this paper, we develop Squirrel, a novel fuzzing framework that considers both language validity and coverage feedback to test DBMSs. We design an intermediate representation (IR) to maintain SQL queries in a structural and informative manner. To generate syntactically correct queries, we perform type-based mutations on IR, including statement insertion, deletion and replacement. To mitigate semantic errors, we analyze each IR to identify the logical dependencies between arguments, and generate queries that satisfy these dependencies. We evaluated Squirrel on four popular DBMSs: SQLite, MySQL, PostgreSQL and MariaDB. Squirrel found 51 bugs in SQLite, 7 in MySQL and 5 in MariaDB. 52 of the bugs are fixed with 12 CVEs assigned. In our experiment, Squirrel achieves 2.4x-243.9x higher semantic correctness than state-of-the-art fuzzers, and explores 2.0x-10.9x more new edges than mutation-based tools. These results show that Squirrel is effective in finding memory errors of database management systems.

</details>

<details>

<summary>2020-06-03 17:37:08 - Unveiling Relations in the Industry 4.0 Standards Landscape based on Knowledge Graph Embeddings</summary>

- *Ariam Rivas, Irlán Grangel-González, Diego Collarana, Jens Lehmann, Maria-Esther Vidal*

- `2006.04556v1` - [abs](http://arxiv.org/abs/2006.04556v1) - [pdf](http://arxiv.org/pdf/2006.04556v1)

> Industry~4.0 (I4.0) standards and standardization frameworks have been proposed with the goal of \emph{empowering interoperability} in smart factories. These standards enable the description and interaction of the main components, systems, and processes inside of a smart factory. Due to the growing number of frameworks and standards, there is an increasing need for approaches that automatically analyze the landscape of I4.0 standards. Standardization frameworks classify standards according to their functions into layers and dimensions. However, similar standards can be classified differently across the frameworks, producing, thus, interoperability conflicts among them. Semantic-based approaches that rely on ontologies and knowledge graphs, have been proposed to represent standards, known relations among them, as well as their classification according to existing frameworks. Albeit informative, the structured modeling of the I4.0 landscape only provides the foundations for detecting interoperability issues. Thus, graph-based analytical methods able to exploit knowledge encoded by these approaches, are required to uncover alignments among standards. We study the relatedness among standards and frameworks based on community analysis to discover knowledge that helps to cope with interoperability conflicts between standards. We use knowledge graph embeddings to automatically create these communities exploiting the meaning of the existing relationships. In particular, we focus on the identification of similar standards, i.e., communities of standards, and analyze their properties to detect unknown relations. We empirically evaluate our approach on a knowledge graph of I4.0 standards using the Trans$^*$ family of embedding models for knowledge graph entities. Our results are promising and suggest that relations among standards can be detected accurately.

</details>

<details>

<summary>2020-06-03 18:43:11 - Directly Mapping RDF Databases to Property Graph Databases</summary>

- *Renzo Angles, Harsh Thakkar, Dominik Tomaszuk*

- `1912.02127v2` - [abs](http://arxiv.org/abs/1912.02127v2) - [pdf](http://arxiv.org/pdf/1912.02127v2)

> RDF triplestores and property graph databases are two approaches for data management which are based on modeling, storing, and querying graph-like data. In spite of such common principles, they present special features that complicate the task of database interoperability. While there exist some methods to transform RDF graphs into property graphs, and vice versa, they lack compatibility and a solid formal foundation. This paper presents three direct mappings (schema-dependent and schema-independent) for transforming an RDF database into a property graph database, including data and schema. We show that two of the proposed mappings satisfy the properties of semantics preservation and information preservation. The existence of both mappings allows us to conclude that the property graph data model subsumes the information capacity of the RDF data model.

</details>

<details>

<summary>2020-06-04 09:14:05 - Twinning automata and regular expressions for string static analysis</summary>

- *Luca Negrini, Vincenzo Arceri, Pietro Ferrara, Agostino Cortesi*

- `2006.02715v1` - [abs](http://arxiv.org/abs/2006.02715v1) - [pdf](http://arxiv.org/pdf/2006.02715v1)

> In this paper we formalize and prove the soundness of Tarsis, a new abstract domain based on the abstract interpretation theory that approximates string values through finite state automata. The main novelty of Tarsis is that it works over an alphabet of strings instead of single characters. On the one hand, such approach requires a more complex and refined definition of the widening operator, and the abstract semantics of string operators. On the other hand, it is in position to obtain strictly more precise results than than state-of-the-art approaches. We implemented a prototype of Tarsis, and we applied it on some case studies taken from some of the most popular Java libraries manipulating string values. The experimental results confirm that Tarsis is in position to obtain strictly more precise results than existing analyses.

</details>

<details>

<summary>2020-06-04 10:25:10 - pyBART: Evidence-based Syntactic Transformations for IE</summary>

- *Aryeh Tiktinsky, Yoav Goldberg, Reut Tsarfaty*

- `2005.01306v2` - [abs](http://arxiv.org/abs/2005.01306v2) - [pdf](http://arxiv.org/pdf/2005.01306v2)

> Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks. However, their utility can be improved. These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit. Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications. Proposals like English Enhanced UD improve the situation by extending universal dependency trees with additional explicit arcs. However, they are not available to Python users, and are also limited in coverage. We introduce a broad-coverage, data-driven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit. We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation. The library can work as a standalone package or be integrated within a spaCy NLP pipeline. When evaluated in a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns.

</details>

<details>

<summary>2020-06-04 16:48:45 - Linguists Who Use Probabilistic Models Love Them: Quantification in Functional Distributional Semantics</summary>

- *Guy Emerson*

- `2006.03002v1` - [abs](http://arxiv.org/abs/2006.03002v1) - [pdf](http://arxiv.org/pdf/2006.03002v1)

> Functional Distributional Semantics provides a computationally tractable framework for learning truth-conditional semantics from a corpus. Previous work in this framework has provided a probabilistic version of first-order logic, recasting quantification as Bayesian inference. In this paper, I show how the previous formulation gives trivial truth values when a precise quantifier is used with vague predicates. I propose an improved account, avoiding this problem by treating a vague predicate as a distribution over precise predicates. I connect this account to recent work in the Rational Speech Acts framework on modelling generic quantification, and I extend this to modelling donkey sentences. Finally, I explain how the generic quantifier can be both pragmatically complex and yet computationally simpler than precise quantifiers.

</details>

<details>

<summary>2020-06-04 22:54:51 - INSET: Sentence Infilling with INter-SEntential Transformer</summary>

- *Yichen Huang, Yizhe Zhang, Oussama Elachqar, Yu Cheng*

- `1911.03892v2` - [abs](http://arxiv.org/abs/1911.03892v2) - [pdf](http://arxiv.org/pdf/1911.03892v2)

> Missing sentence generation (or sentence infilling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2. We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.

</details>

<details>

<summary>2020-06-05 02:03:25 - Egocentric Object Manipulation Graphs</summary>

- *Eadom Dessalene, Michael Maynord, Chinmaya Devaraj, Cornelia Fermuller, Yiannis Aloimonos*

- `2006.03201v1` - [abs](http://arxiv.org/abs/2006.03201v1) - [pdf](http://arxiv.org/pdf/2006.03201v1)

> We introduce Egocentric Object Manipulation Graphs (Ego-OMG) - a novel representation for activity modeling and anticipation of near future actions integrating three components: 1) semantic temporal structure of activities, 2) short-term dynamics, and 3) representations for appearance. Semantic temporal structure is modeled through a graph, embedded through a Graph Convolutional Network, whose states model characteristics of and relations between hands and objects. These state representations derive from all three levels of abstraction, and span segments delimited by the making and breaking of hand-object contact. Short-term dynamics are modeled in two ways: A) through 3D convolutions, and B) through anticipating the spatiotemporal end points of hand trajectories, where hands come into contact with objects. Appearance is modeled through deep spatiotemporal features produced through existing methods. We note that in Ego-OMG it is simple to swap these appearance features, and thus Ego-OMG is complementary to most existing action anticipation methods. We evaluate Ego-OMG on the EPIC Kitchens Action Anticipation Challenge. The consistency of the egocentric perspective of EPIC Kitchens allows for the utilization of the hand-centric cues upon which Ego-OMG relies. We demonstrate state-of-the-art performance, outranking all other previous published methods by large margins and ranking first on the unseen test set and second on the seen test set of the EPIC Kitchens Action Anticipation Challenge. We attribute the success of Ego-OMG to the modeling of semantic structure captured over long timespans. We evaluate the design choices made through several ablation studies. Code will be released upon acceptance

</details>

<details>

<summary>2020-06-05 09:00:36 - Multi-modal Feature Fusion with Feature Attention for VATEX Captioning Challenge 2020</summary>

- *Ke Lin, Zhuoxin Gan, Liwei Wang*

- `2006.03315v1` - [abs](http://arxiv.org/abs/2006.03315v1) - [pdf](http://arxiv.org/pdf/2006.03315v1)

> This report describes our model for VATEX Captioning Challenge 2020. First, to gather information from multiple domains, we extract motion, appearance, semantic and audio features. Then we design a feature attention module to attend on different feature when decoding. We apply two types of decoders, top-down and X-LAN and ensemble these models to get the final result. The proposed method outperforms official baseline with a significant gap. We achieve 76.0 CIDEr and 50.0 CIDEr on English and Chinese private test set. We rank 2nd on both English and Chinese private test leaderboard.

</details>

<details>

<summary>2020-06-05 12:53:43 - BIMCV COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients</summary>

- *Maria de la Iglesia Vayá, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant, Xavier Barber, Domingo Orozco-Beltrán, Francisco García-García, Marisa Caparrós, Germán González, Jose María Salinas*

- `2006.01174v3` - [abs](http://arxiv.org/abs/2006.01174v3) - [pdf](http://arxiv.org/pdf/2006.01174v3)

> This paper describes BIMCV COVID-19+, a large dataset from the Valencian Region Medical ImageBank (BIMCV) containing chest X-ray images CXR (CR, DX) and computed tomography (CT) imaging of COVID-19+ patients along with their radiological findings and locations, pathologies, radiological reports (in Spanish), DICOM metadata, Polymerase chain reaction (PCR), Immunoglobulin G (IgG) and Immunoglobulin M (IgM) diagnostic antibody tests. The findings have been mapped onto standard Unified Medical Language System (UMLS) terminology and cover a wide spectrum of thoracic entities, unlike the considerably more reduced number of entities annotated in previous datasets. Images are stored in high resolution and entities are localized with anatomical labels and stored in a Medical Imaging Data Structure (MIDS) format. In addition, 10 images were annotated by a team of radiologists to include semantic segmentation of radiological findings. This first iteration of the database includes 1,380 CX, 885 DX and 163 CT studies from 1,311 COVID-19+ patients. This is, to the best of our knowledge, the largest COVID-19+ dataset of images available in an open format. The dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/bimcv-covid19.

</details>

<details>

<summary>2020-06-05 14:39:41 - Segmentation of Surgical Instruments for Minimally-Invasive Robot-Assisted Procedures Using Generative Deep Neural Networks</summary>

- *Iñigo Azqueta-Gavaldon, Florian Fröhlich, Klaus Strobl, Rudolph Triebel*

- `2006.03486v1` - [abs](http://arxiv.org/abs/2006.03486v1) - [pdf](http://arxiv.org/pdf/2006.03486v1)

> This work proves that semantic segmentation on minimally invasive surgical instruments can be improved by using training data that has been augmented through domain adaptation. The benefit of this method is twofold. Firstly, it suppresses the need of manually labeling thousands of images by transforming synthetic data into realistic-looking data. To achieve this, a CycleGAN model is used, which transforms a source dataset to approximate the domain distribution of a target dataset. Secondly, this newly generated data with perfect labels is utilized to train a semantic segmentation neural network, U-Net. This method shows generalization capabilities on data with variability regarding its rotation- position- and lighting conditions. Nevertheless, one of the caveats of this approach is that the model is unable to generalize well to other surgical instruments with a different shape from the one used for training. This is driven by the lack of a high variance in the geometric distribution of the training data. Future work will focus on making the model more scale-invariant and able to adapt to other types of surgical instruments previously unseen by the training.

</details>

<details>

<summary>2020-06-05 16:32:43 - Permutation Matters: Anisotropic Convolutional Layer for Learning on Point Clouds</summary>

- *Zhongpai Gao, Guangtao Zhai, Junchi Yan, Xiaokang Yang*

- `2005.13135v2` - [abs](http://arxiv.org/abs/2005.13135v2) - [pdf](http://arxiv.org/pdf/2005.13135v2)

> It has witnessed a growing demand for efficient representation learning on point clouds in many 3D computer vision applications. Behind the success story of convolutional neural networks (CNNs) is that the data (e.g., images) are Euclidean structured. However, point clouds are irregular and unordered. Various point neural networks have been developed with isotropic filters or using weighting matrices to overcome the structure inconsistency on point clouds. However, isotropic filters or weighting matrices limit the representation power. In this paper, we propose a permutable anisotropic convolutional operation (PAI-Conv) that calculates soft-permutation matrices for each point using dot-product attention according to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In fact, dot product with kernel points is by analogy with the dot-product with keys in Transformer as widely used in natural language processing (NLP). From this perspective, PAI-Conv can be regarded as the transformer for point clouds, which is physically meaningful and is robust to cooperate with the efficient random point sampling method. Comprehensive experiments on point clouds demonstrate that PAI-Conv produces competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.

</details>

<details>

<summary>2020-06-06 01:00:15 - Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report Generation</summary>

- *Mingjie Li, Fuyu Wang, Xiaojun Chang, Xiaodan Liang*

- `2006.03744v1` - [abs](http://arxiv.org/abs/2006.03744v1) - [pdf](http://arxiv.org/pdf/2006.03744v1)

> Beyond the common difficulties faced in the natural image captioning, medical report generation specifically requires the model to describe a medical image with a fine-grained and semantic-coherence paragraph that should satisfy both medical commonsense and logic. Previous works generally extract the global image features and attempt to generate a paragraph that is similar to referenced reports; however, this approach has two limitations. Firstly, the regions of primary interest to radiologists are usually located in a small area of the global image, meaning that the remainder parts of the image could be considered as irrelevant noise in the training procedure. Secondly, there are many similar sentences used in each medical report to describe the normal regions of the image, which causes serious data bias. This deviation is likely to teach models to generate these inessential sentences on a regular basis. To address these problems, we propose an Auxiliary Signal-Guided Knowledge Encoder-Decoder (ASGK) to mimic radiologists' working patterns. In more detail, ASGK integrates internal visual feature fusion and external medical linguistic information to guide medical knowledge transfer and learning. The core structure of ASGK consists of a medical graph encoder and a natural language decoder, inspired by advanced Generative Pre-Training (GPT). Experiments on the CX-CHR dataset and our COVID-19 CT Report dataset demonstrate that our proposed ASGK is able to generate a robust and accurate report, and moreover outperforms state-of-the-art methods on both medical terminology classification and paragraph generation metrics.

</details>

<details>

<summary>2020-06-06 09:07:15 - BHN: A Brain-like Heterogeneous Network</summary>

- *Tao Liu*

- `2005.12826v2` - [abs](http://arxiv.org/abs/2005.12826v2) - [pdf](http://arxiv.org/pdf/2005.12826v2)

> The human brain works in an unsupervised way, and more than one brain region is essential for lighting up intelligence. Inspired by this, we propose a brain-like heterogeneous network (BHN), which can cooperatively learn a lot of distributed representations and one global attention representation. By optimizing distributed, self-supervised, and gradient-isolated objective functions in a minimax fashion, our model improves its representations, which are generated from patches of pictures or frames of videos in experiments.

</details>

<details>

<summary>2020-06-06 20:09:39 - GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering</summary>

- *Chuang Niu, Jun Zhang, Ge Wang, Jimin Liang*

- `2002.11863v2` - [abs](http://arxiv.org/abs/2002.11863v2) - [pdf](http://arxiv.org/pdf/2002.11863v2)

> We propose a self-supervised Gaussian ATtention network for image Clustering (GATCluster). Rather than extracting intermediate features first and then performing the traditional clustering algorithm, GATCluster directly outputs semantic cluster labels without further post-processing. Theoretically, we give a Label Feature Theorem to guarantee the learned features are one-hot encoded vectors, and the trivial solutions are avoided. To train the GATCluster in a completely unsupervised manner, we design four self-learning tasks with the constraints of transformation invariance, separability maximization, entropy analysis, and attention mapping. Specifically, the transformation invariance and separability maximization tasks learn the relationships between sample pairs. The entropy analysis task aims to avoid trivial solutions. To capture the object-oriented semantics, we design a self-supervised attention mechanism that includes a parameterized attention module and a soft-attention loss. All the guiding signals for clustering are self-generated during the training process. Moreover, we develop a two-step learning algorithm that is memory-efficient for clustering large-size images. Extensive experiments demonstrate the superiority of our proposed method in comparison with the state-of-the-art image clustering benchmarks. Our code has been made publicly available at https://github.com/niuchuangnn/GATCluster.

</details>

<details>

<summary>2020-06-06 20:25:07 - Tuning a variational autoencoder for data accountability problem in the Mars Science Laboratory ground data system</summary>

- *Dounia Lakhmiri, Ryan Alimo, Sebastien Le Digabel*

- `2006.03962v1` - [abs](http://arxiv.org/abs/2006.03962v1) - [pdf](http://arxiv.org/pdf/2006.03962v1)

> The Mars Curiosity rover is frequently sending back engineering and science data that goes through a pipeline of systems before reaching its final destination at the mission operations center making it prone to volume loss and data corruption. A ground data system analysis (GDSA) team is charged with the monitoring of this flow of information and the detection of anomalies in that data in order to request a re-transmission when necessary. This work presents $\Delta$-MADS, a derivative-free optimization method applied for tuning the architecture and hyperparameters of a variational autoencoder trained to detect the data with missing patches in order to assist the GDSA team in their mission.

</details>

<details>

<summary>2020-06-07 01:17:18 - Medical Concept Normalization in User Generated Texts by Learning Target Concept Embeddings</summary>

- *Katikapalli Subramanyam Kalyan, S. Sangeetha*

- `2006.04014v1` - [abs](http://arxiv.org/abs/2006.04014v1) - [pdf](http://arxiv.org/pdf/2006.04014v1)

> Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a vocabulary. It is much beyond simple string matching and requires a deep semantic understanding of concept mentions. Recent research approach concept normalization as either text classification or text matching. The main drawback in existing a) text classification approaches is ignoring valuable target concepts information in learning input concept mention representation b) text matching approach is the need to separately generate target concept embeddings which is time and resource consuming. Our proposed model overcomes these drawbacks by jointly learning the representations of input concept mention and target concepts. First, it learns the input concept mention representation using RoBERTa. Second, it finds cosine similarity between embeddings of input concept mention and all the target concepts. Here, embeddings of target concepts are randomly initialized and then updated during training. Finally, the target concept with maximum cosine similarity is assigned to the input concept mention. Our model surpasses all the existing methods across three standard datasets by improving accuracy up to 2.31%.

</details>

<details>

<summary>2020-06-07 09:32:41 - Contextualisation of Data Flow Diagrams for security analysis</summary>

- *Shamal Faily, Riccardo Scandariato, Adam Shostack, Laurens Sion, Duncan Ki-Aries*

- `2006.04098v1` - [abs](http://arxiv.org/abs/2006.04098v1) - [pdf](http://arxiv.org/pdf/2006.04098v1)

> Data flow diagrams (DFDs) are popular for sketching systems for subsequent threat modelling. Their limited semantics make reasoning about them difficult, but enriching them endangers their simplicity and subsequent ease of take up. We present an approach for reasoning about tainted data flows in design-level DFDs by putting them in context with other complementary usability and requirements models. We illustrate our approach using a pilot study, where tainted data flows were identified without any augmentations to either the DFD or its complementary models.

</details>

<details>

<summary>2020-06-07 11:48:20 - DAugNet: Unsupervised, Multi-source, Multi-target, and Life-long Domain Adaptation for Semantic Segmentation of Satellite Images</summary>

- *Onur Tasar, Alain Giros, Yuliya Tarabalka, Pierre Alliez, Sébastien Clerc*

- `2005.06216v2` - [abs](http://arxiv.org/abs/2005.06216v2) - [pdf](http://arxiv.org/pdf/2005.06216v2)

> The domain adaptation of satellite images has recently gained an increasing attention to overcome the limited generalization abilities of machine learning models when segmenting large-scale satellite images. Most of the existing approaches seek for adapting the model from one domain to another. However, such single-source and single-target setting prevents the methods from being scalable solutions, since nowadays multiple source and target domains having different data distributions are usually available. Besides, the continuous proliferation of satellite images necessitates the classifiers to adapt to continuously increasing data. We propose a novel approach, coined DAugNet, for unsupervised, multi-source, multi-target, and life-long domain adaptation of satellite images. It consists of a classifier and a data augmentor. The data augmentor, which is a shallow network, is able to perform style transfer between multiple satellite images in an unsupervised manner, even when new data are added over the time. In each training iteration, it provides the classifier with diversified data, which makes the classifier robust to large data distribution difference between the domains. Our extensive experiments prove that DAugNet significantly better generalizes to new geographic locations than the existing approaches.

</details>

<details>

<summary>2020-06-07 14:26:32 - An Empirical Meta-analysis of the Life Sciences (Linked?) Open Data on the Web</summary>

- *Maulik R. Kamdar, Mark A. Musen*

- `2006.04161v1` - [abs](http://arxiv.org/abs/2006.04161v1) - [pdf](http://arxiv.org/pdf/2006.04161v1)

> While the biomedical community has published several "open data" sources in the last decade, most researchers still endure severe logistical and technical challenges to discover, query, and integrate heterogeneous data and knowledge from multiple sources. To tackle these challenges, the community has experimented with Semantic Web and linked data technologies to create the Life Sciences Linked Open Data (LSLOD) cloud. In this paper, we extract schemas from more than 80 publicly available biomedical linked data graphs into an LSLOD schema graph and conduct an empirical meta-analysis to evaluate the extent of semantic heterogeneity across the LSLOD cloud. We observe that several LSLOD sources exist as stand-alone data sources that are not inter-linked with other sources, use unpublished schemas with minimal reuse or mappings, and have elements that are not useful for data integration from a biomedical perspective. We envision that the LSLOD schema graph and the findings from this research will aid researchers who wish to query and integrate data and knowledge from multiple biomedical sources simultaneously on the Web.

</details>

<details>

<summary>2020-06-07 18:41:56 - Self-supervised classification of dynamic obstacles using the temporal information provided by videos</summary>

- *Sid Ali Hamideche, Florent Chiaroni, Mohamed-Cherif Rahal*

- `1910.09094v2` - [abs](http://arxiv.org/abs/1910.09094v2) - [pdf](http://arxiv.org/pdf/1910.09094v2)

> Nowadays, autonomous driving systems can detect, segment, and classify the surrounding obstacles using a monocular camera. However, state-of-the-art methods solving these tasks generally perform a fully supervised learning process and require a large amount of training labeled data. On another note, some self-supervised learning approaches can deal with detection and segmentation of dynamic obstacles using the temporal information available in video sequences. In this work, we propose to classify the detected obstacles depending on their motion pattern. We present a novel self-supervised framework consisting of learning offline clusters from temporal patch sequences and considering these clusters as labeled sets to train a real-time image classifier. The presented model outperforms state-of-the-art unsupervised image classification methods on large-scale diverse driving video dataset BDD100K.

</details>

<details>

<summary>2020-06-08 02:39:59 - A Transductive Multi-Head Model for Cross-Domain Few-Shot Learning</summary>

- *Jianan Jiang, Zhenpeng Li, Yuhong Guo, Jieping Ye*

- `2006.11384v1` - [abs](http://arxiv.org/abs/2006.11384v1) - [pdf](http://arxiv.org/pdf/2006.11384v1)

> In this paper, we present a new method, Transductive Multi-Head Few-Shot learning (TMHFS), to address the Cross-Domain Few-Shot Learning (CD-FSL) challenge. The TMHFS method extends the Meta-Confidence Transduction (MCT) and Dense Feature-Matching Networks (DFMN) method [2] by introducing a new prediction head, i.e, an instance-wise global classification network based on semantic information, after the common feature embedding network. We train the embedding network with the multiple heads, i.e,, the MCT loss, the DFMN loss and the semantic classifier loss, simultaneously in the source domain. For the few-shot learning in the target domain, we first perform fine-tuning on the embedding network with only the semantic global classifier and the support instances, and then use the MCT part to predict labels of the query set with the fine-tuned embedding network. Moreover, we further exploit data augmentation techniques during the fine-tuning and test stages to improve the prediction performance. The experimental results demonstrate that the proposed methods greatly outperform the strong baseline, fine-tuning, on four different target domains.

</details>

<details>

<summary>2020-06-08 06:57:18 - Learning the Compositional Visual Coherence for Complementary Recommendations</summary>

- *Zhi Li, Bo Wu, Qi Liu, Likang Wu, Hongke Zhao, Tao Mei*

- `2006.04380v1` - [abs](http://arxiv.org/abs/2006.04380v1) - [pdf](http://arxiv.org/pdf/2006.04380v1)

> Complementary recommendations, which aim at providing users product suggestions that are supplementary and compatible with their obtained items, have become a hot topic in both academia and industry in recent years. %However, it is challenging due to its complexity and subjectivity. Existing work mainly focused on modeling the co-purchased relations between two items, but the compositional associations of item collections are largely unexplored. Actually, when a user chooses the complementary items for the purchased products, it is intuitive that she will consider the visual semantic coherence (such as color collocations, texture compatibilities) in addition to global impressions. Towards this end, in this paper, we propose a novel Content Attentive Neural Network (CANN) to model the comprehensive compositional coherence on both global contents and semantic contents. Specifically, we first propose a \textit{Global Coherence Learning} (GCL) module based on multi-heads attention to model the global compositional coherence. Then, we generate the semantic-focal representations from different semantic regions and design a \textit{Focal Coherence Learning} (FCL) module to learn the focal compositional coherence from different semantic-focal representations. Finally, we optimize the CANN in a novel compositional optimization strategy. Extensive experiments on the large-scale real-world data clearly demonstrate the effectiveness of CANN compared with several state-of-the-art methods.

</details>

<details>

<summary>2020-06-08 08:09:33 - ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation</summary>

- *Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang*

- `2001.11314v3` - [abs](http://arxiv.org/abs/2001.11314v3) - [pdf](http://arxiv.org/pdf/2001.11314v3)

> Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves state-of-the-art results with a much smaller amount of pre-training data and parameters on a range of language generation tasks, including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue generation (Persona-Chat) and generative question answering (CoQA).

</details>

<details>

<summary>2020-06-08 09:09:28 - CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</summary>

- *Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt*

- `1909.09436v3` - [abs](http://arxiv.org/abs/1909.09436v3) - [pdf](http://arxiv.org/pdf/1909.09436v3)

> Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.   To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.   We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.

</details>

<details>

<summary>2020-06-08 11:27:25 - On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation</summary>

- *Wei Zhao, Goran Glavaš, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger*

- `2005.01196v3` - [abs](http://arxiv.org/abs/2005.01196v3) - [pdf](http://arxiv.org/pdf/2005.01196v3)

> Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish "translationese", i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.

</details>

<details>

<summary>2020-06-08 12:30:25 - Combining word embeddings and convolutional neural networks to detect duplicated questions</summary>

- *Yoan Dimitrov*

- `2006.04513v1` - [abs](http://arxiv.org/abs/2006.04513v1) - [pdf](http://arxiv.org/pdf/2006.04513v1)

> Detecting semantic similarities between sentences is still a challenge today due to the ambiguity of natural languages. In this work, we propose a simple approach to identifying semantically similar questions by combining the strengths of word embeddings and Convolutional Neural Networks (CNNs). In addition, we demonstrate how the cosine similarity metric can be used to effectively compare feature vectors. Our network is trained on the Quora dataset, which contains over 400k question pairs. We experiment with different embedding approaches such as Word2Vec, Fasttext, and Doc2Vec and investigate the effects these approaches have on model performance. Our model achieves competitive results on the Quora dataset and complements the well-established evidence that CNNs can be utilized for paraphrase detection tasks.

</details>

<details>

<summary>2020-06-08 19:58:58 - ObjSim: Lightweight Automatic Patch Prioritization via Object Similarity</summary>

- *Ali Ghanbari*

- `2006.04911v1` - [abs](http://arxiv.org/abs/2006.04911v1) - [pdf](http://arxiv.org/pdf/2006.04911v1)

> In the context of test case based automatic program repair (APR), patches that pass all the test cases but fail to fix the bug are called overfitted patches. Currently, patches generated by APR tools get inspected manually by the users to find and adopt genuine fixes. Being a laborious activity hindering widespread adoption of APR, automatic identification of overfitted patches has lately been the topic of active research. This paper presents engineering details of ObjSim: a fully automatic, lightweight similarity-based patch prioritization tool for JVM-based languages. The tool works by comparing the system state at the exit point(s) of patched method before and after patching and prioritizing patches that result in state that is more similar to that of original, unpatched version on passing tests while less similar on failing ones. Our experiments with patches generated by the recent APR tool PraPR for fixable bugs from Defects4J v1.4.0 show that ObjSim prioritizes 16.67% more genuine fixes in top-1 place. A demo video of the tool is located at https://bit.ly/2K8gnYV.

</details>

<details>

<summary>2020-06-09 07:49:49 - GAP++: Learning to generate target-conditioned adversarial examples</summary>

- *Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue*

- `2006.05097v1` - [abs](http://arxiv.org/abs/2006.05097v1) - [pdf](http://arxiv.org/pdf/2006.05097v1)

> Adversarial examples are perturbed inputs which can cause a serious threat for machine learning models. Finding these perturbations is such a hard task that we can only use the iterative methods to traverse. For computational efficiency, recent works use adversarial generative networks to model the distribution of both the universal or image-dependent perturbations directly. However, these methods generate perturbations only rely on input images. In this work, we propose a more general-purpose framework which infers target-conditioned perturbations dependent on both input image and target label. Different from previous single-target attack models, our model can conduct target-conditioned attacks by learning the relations of attack target and the semantics in image. Using extensive experiments on the datasets of MNIST and CIFAR10, we show that our method achieves superior performance with single target attack models and obtains high fooling rates with small perturbation norms.

</details>

<details>

<summary>2020-06-09 09:52:44 - Physically constrained short-term vehicle trajectory forecasting with naive semantic maps</summary>

- *Albert Dulian, John C. Murray*

- `2006.05159v1` - [abs](http://arxiv.org/abs/2006.05159v1) - [pdf](http://arxiv.org/pdf/2006.05159v1)

> Urban environments manifest a high level of complexity, and therefore it is of vital importance for safety systems embedded within autonomous vehicles (AVs) to be able to accurately predict the short-term future motion of nearby agents. This problem can be further understood as generating a sequence of future coordinates for a given agent based on its past motion data e.g. position, velocity, acceleration etc, and whilst current approaches demonstrate plausible results they have a propensity to neglect a scene's physical constrains. In this paper we propose the model based on a combination of the CNN and LSTM encoder-decoder architecture that learns to extract a relevant road features from semantic maps as well as general motion of agents and uses this learned representation to predict their short-term future trajectories. We train and validate the model on the publicly available dataset that provides data from urban areas, allowing us to examine it in challenging and uncertain scenarios. We show that our model is not only capable of anticipating future motion whilst taking into consideration road boundaries, but can also effectively and precisely predict trajectories for a longer time horizon than initially trained for.

</details>

<details>

<summary>2020-06-09 15:55:31 - A Tree-based Dictionary Learning Framework</summary>

- *Renato Budinich, Gerlind Plonka*

- `1909.03267v2` - [abs](http://arxiv.org/abs/1909.03267v2) - [pdf](http://arxiv.org/pdf/1909.03267v2)

> We propose a new outline for adaptive dictionary learning methods for sparse encoding based on a hierarchical clustering of the training data. Through recursive application of a clustering method, the data is organized into a binary partition tree representing a multiscale structure. The dictionary atoms are defined adaptively based on the data clusters in the partition tree. This approach can be interpreted as a generalization of a discrete Haar wavelet transform. Furthermore, any prior knowledge on the wanted structure of the dictionary elements can be simply incorporated. The computational complexity of our proposed algorithm depends on the employed clustering method and on the chosen similarity measure between data points. Thanks to the multiscale properties of the partition tree, our dictionary is structured: when using Orthogonal Matching Pursuit to reconstruct patches from a natural image, dictionary atoms corresponding to nodes being closer to the root node in the tree have a tendency to be used with greater coefficients.

</details>

<details>

<summary>2020-06-09 17:56:27 - Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation</summary>

- *Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas S. Huang, Humphrey Shi*

- `2003.08040v3` - [abs](http://arxiv.org/abs/2003.08040v3) - [pdf](http://arxiv.org/pdf/2003.08040v3)

> We consider the problem of unsupervised domain adaptation for semantic segmentation by easing the domain shift between the source domain (synthetic data) and the target domain (real data) in this work. State-of-the-art approaches prove that performing semantic-level alignment is helpful in tackling the domain shift issue. Based on the observation that stuff categories usually share similar appearances across images of different domains while things (i.e. object instances) have much larger differences, we propose to improve the semantic-level alignment with different strategies for stuff regions and for things: 1) for the stuff categories, we generate feature representation for each class and conduct the alignment operation from the target domain to the source domain; 2) for the thing categories, we generate feature representation for each individual instance and encourage the instance in the target domain to align with the most similar one in the source domain. In this way, the individual differences within thing categories will also be considered to alleviate over-alignment. In addition to our proposed method, we further reveal the reason why the current adversarial loss is often unstable in minimizing the distribution discrepancy and show that our method can help ease this issue by minimizing the most similar stuff and instance features between the source and the target domains. We conduct extensive experiments in two unsupervised domain adaptation tasks, i.e. GTA5 to Cityscapes and SYNTHIA to Cityscapes, and achieve the new state-of-the-art segmentation accuracy.

</details>

<details>

<summary>2020-06-09 22:38:27 - Alleviating Semantic-level Shift: A Semi-supervised Domain Adaptation Method for Semantic Segmentation</summary>

- *Zhonghao Wang, Yunchao Wei, Rogerior Feris, Jinjun Xiong, Wen-Mei Hwu, Thomas S. Huang, Humphrey Shi*

- `2004.00794v2` - [abs](http://arxiv.org/abs/2004.00794v2) - [pdf](http://arxiv.org/pdf/2004.00794v2)

> Learning segmentation from synthetic data and adapting to real data can significantly relieve human efforts in labelling pixel-level masks. A key challenge of this task is how to alleviate the data distribution discrepancy between the source and target domains, i.e. reducing domain shift. The common approach to this problem is to minimize the discrepancy between feature distributions from different domains through adversarial training. However, directly aligning the feature distribution globally cannot guarantee consistency from a local view (i.e. semantic-level), which prevents certain semantic knowledge learned on the source domain from being applied to the target domain. To tackle this issue, we propose a semi-supervised approach named Alleviating Semantic-level Shift (ASS), which can successfully promote the distribution consistency from both global and local views. Specifically, leveraging a small number of labeled data from the target domain, we directly extract semantic-level feature representations from both the source and the target domains by averaging the features corresponding to same categories advised by pixel-level masks. We then feed the produced features to the discriminator to conduct semantic-level adversarial learning, which collaborates with the adversarial learning from the global view to better alleviate the domain shift. We apply our ASS to two domain adaptation tasks, from GTA5 to Cityscapes and from Synthia to Cityscapes. Extensive experiments demonstrate that: (1) ASS can significantly outperform the current unsupervised state-of-the-arts by employing a small number of annotated samples from the target domain; (2) ASS can beat the oracle model trained on the whole target dataset by over 3 points by augmenting the synthetic source data with annotated samples from the target domain without suffering from the prevalent problem of overfitting to the source domain.

</details>

<details>

<summary>2020-06-10 01:42:40 - Using an expert deviation carrying the knowledge of climate data in usual clustering algorithms</summary>

- *Emmanuel Biabiany, Vincent Page, Didier Bernard, Hélène Paugam-Moisy*

- `2006.05603v1` - [abs](http://arxiv.org/abs/2006.05603v1) - [pdf](http://arxiv.org/pdf/2006.05603v1)

> In order to help physicists to expand their knowledge of the climate in the Lesser Antilles, we aim to identify the spatio-temporal configurations using clustering analysis on wind speed and cumulative rainfall datasets. But we show that using the L2 norm in conventional clustering methods as K-Means (KMS) and Hierarchical Agglomerative Clustering (HAC) can induce undesirable effects. So, we propose to replace Euclidean distance (L2) by a dissimilarity measure named Expert Deviation (ED). Based on the symmetrized Kullback-Leibler divergence, the ED integrates the properties of the observed physical parameters and climate knowledge. This measure helps comparing histograms of four patches, corresponding to geographical zones, that are influenced by atmospheric structures. The combined evaluation of the internal homogeneity and the separation of the clusters obtained using ED and L2 was performed. The results, which are compared using the silhouette index, show five clusters with high indexes. For the two available datasets one can see that, unlike KMS-L2, KMS-ED discriminates the daily situations favorably, giving more physical meaning to the clusters discovered by the algorithm. The effect of patches is observed in the spatial analysis of representative elements for KMS-ED. The ED is able to produce different configurations which makes the usual atmospheric structures clearly identifiable. Atmospheric physicists can interpret the locations of the impact of each cluster on a specific zone according to atmospheric structures. KMS-L2 does not lead to such an interpretability, because the situations represented are spatially quite smooth. This climatological study illustrates the advantage of using ED as a new approach.

</details>

<details>

<summary>2020-06-10 03:01:59 - A survey on deep hashing for image retrieval</summary>

- *Xiaopeng Zhang*

- `2006.05627v1` - [abs](http://arxiv.org/abs/2006.05627v1) - [pdf](http://arxiv.org/pdf/2006.05627v1)

> Hashing has been widely used in approximate nearest search for large-scale database retrieval for its computation and storage efficiency. Deep hashing, which devises convolutional neural network architecture to exploit and extract the semantic information or feature of images, has received increasing attention recently. In this survey, several deep supervised hashing methods for image retrieval are evaluated and I conclude three main different directions for deep supervised hashing methods. Several comments are made at the end. Moreover, to break through the bottleneck of the existing hashing methods, I propose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise a CNN architecture to extract the semantic features of images and design a loss function to encourage similar images projected close. To this end, I propose a concept: shadow of the CNN output. During optimization process, the CNN output and its shadow are guiding each other so as to achieve the optimal solution as much as possible. Several experiments on dataset CIFAR-10 show the satisfying performance of SRH.

</details>

<details>

<summary>2020-06-10 07:50:44 - Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network</summary>

- *Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou, Yijia Liu, Han Liu, Ting Liu*

- `2006.05702v1` - [abs](http://arxiv.org/abs/2006.05702v1) - [pdf](http://arxiv.org/pdf/2006.05702v1)

> In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other few-shot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word's similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model -- TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.

</details>

<details>

<summary>2020-06-10 10:05:16 - Data science on industrial data -- Today's challenges in brown field applications</summary>

- *Tilman Klaeger, Sebastian Gottschall, Lukas Oehm*

- `2006.05757v1` - [abs](http://arxiv.org/abs/2006.05757v1) - [pdf](http://arxiv.org/pdf/2006.05757v1)

> Much research is done on data analytics and machine learning. In industrial processes large amounts of data are available and many researchers are trying to work with this data. In practical approaches one finds many pitfalls restraining the application of modern technologies especially in brown field applications. With this paper we want to show state of the art and what to expect when working with stock machines in the field. A major focus in this paper is on data collection which can be more cumbersome than most people might expect. Also data quality for machine learning applications is a challenge once leaving the laboratory. In this area one has to expect the lack of semantic description of the data as well as very little ground truth being available for training and verification of machine learning models. A last challenge is IT security and passing data through firewalls.

</details>

<details>

<summary>2020-06-10 14:49:08 - Heterogeneous Graph Attention Networks for Early Detection of Rumors on Twitter</summary>

- *Qi Huang, Junshuai Yu, Jia Wu, Bin Wang*

- `2006.05866v1` - [abs](http://arxiv.org/abs/2006.05866v1) - [pdf](http://arxiv.org/pdf/2006.05866v1)

> With the rapid development of mobile Internet technology and the widespread use of mobile devices, it becomes much easier for people to express their opinions on social media. The openness and convenience of social media platforms provide a free expression for people but also cause new social problems. The widespread of false rumors on social media can bring about the panic of the public and damage personal reputation, which makes rumor automatic detection technology become particularly necessary. The majority of existing methods for rumor detection focus on mining effective features from text contents, user profiles, and patterns of propagation. Nevertheless, these methods do not take full advantage of global semantic relations of the text contents, which characterize the semantic commonality of rumors as a key factor for detecting rumors. In this paper, we construct a tweet-word-user heterogeneous graph based on the text contents and the source tweet propagations of rumors. A meta-path based heterogeneous graph attention network framework is proposed to capture the global semantic relations of text contents, together with the global structure information of source tweet propagations for rumor detection. Experiments on real-world Twitter data demonstrate the superiority of the proposed approach, which also has a comparable ability to detect rumors at a very early stage.

</details>

<details>

<summary>2020-06-11 01:17:56 - Learning Navigation Costs from Demonstration with Semantic Observations</summary>

- *Tianyu Wang, Vikas Dhiman, Nikolay Atanasov*

- `2006.05043v2` - [abs](http://arxiv.org/abs/2006.05043v2) - [pdf](http://arxiv.org/pdf/2006.05043v2)

> This paper focuses on inverse reinforcement learning (IRL) for autonomous robot navigation using semantic observations. The objective is to infer a cost function that explains demonstrated behavior while relying only on the expert's observations and state-control trajectory. We develop a map encoder, which infers semantic class probabilities from the observation sequence, and a cost encoder, defined as deep neural network over the semantic features. Since the expert cost is not directly observable, the representation parameters can only be optimized by differentiating the error between demonstrated controls and a control policy computed from the cost estimate. The error is optimized using a closed-form subgradient computed only over a subset of promising states via a motion planning algorithm. We show that our approach learns to follow traffic rules in the autonomous driving CARLA simulator by relying on semantic observations of cars, sidewalks and road lanes.

</details>

<details>

<summary>2020-06-11 09:20:56 - SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA</summary>

- *Daniel Hershcovich, Zohar Aizenbud, Leshem Choshen, Elior Sulem, Ari Rappoport, Omri Abend*

- `1903.02953v3` - [abs](http://arxiv.org/abs/1903.02953v3) - [pdf](http://arxiv.org/pdf/1903.02953v3)

> We present the SemEval 2019 shared task on UCCA parsing in English, German and French, and discuss the participating systems and results. UCCA is a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings. Full results can be found in the task's website \url{https://competitions.codalab.org/competitions/19160}.

</details>

<details>

<summary>2020-06-11 16:34:37 - Chameleon: Learning Model Initializations Across Tasks With Different Schemas</summary>

- *Lukas Brinkmeyer, Rafael Rego Drumond, Randolf Scholz, Josif Grabocka, Lars Schmidt-Thieme*

- `1909.13576v4` - [abs](http://arxiv.org/abs/1909.13576v4) - [pdf](http://arxiv.org/pdf/1909.13576v4)

> Parametric models, and particularly neural networks, require weight initialization as a starting point for gradient-based optimization. Recent work shows that a specific initial parameter set can be learned from a population of supervised learning tasks. Using this initial parameter set enables a fast convergence for unseen classes even when only a handful of instances is available (model-agnostic meta-learning). Currently, methods for learning model initializations are limited to a population of tasks sharing the same schema, i.e., the same number, order, type, and semantics of predictor and target variables. In this paper, we address the problem of meta-learning parameter initialization across tasks with different schemas, i.e., if the number of predictors varies across tasks, while they still share some variables. We propose Chameleon, a model that learns to align different predictor schemas to a common representation. In experiments on 23 datasets of the OpenML-CC18 benchmark, we show that Chameleon can successfully learn parameter initializations across tasks with different schemas, presenting, to the best of our knowledge, the first cross-dataset few-shot classification approach for unstructured data.

</details>

<details>

<summary>2020-06-11 20:50:05 - Semantic Robustness of Models of Source Code</summary>

- *Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh Jha, Thomas Reps*

- `2002.03043v2` - [abs](http://arxiv.org/abs/2002.03043v2) - [pdf](http://arxiv.org/pdf/2002.03043v2)

> Deep neural networks are vulnerable to adversarial examples - small input perturbations that result in incorrect predictions. We study this problem for models of source code, where we want the network to be robust to source-code modifications that preserve code functionality. (1) We define a powerful adversary that can employ sequences of parametric, semantics-preserving program transformations; (2) we show how to perform adversarial training to learn models robust to such adversaries; (3) we conduct an evaluation on different languages and architectures, demonstrating significant quantitative gains in robustness.

</details>

<details>

<summary>2020-06-11 23:10:02 - SegNBDT: Visual Decision Rules for Segmentation</summary>

- *Alvin Wan, Daniel Ho, Younjin Song, Henk Tillman, Sarah Adel Bargal, Joseph E. Gonzalez*

- `2006.06868v1` - [abs](http://arxiv.org/abs/2006.06868v1) - [pdf](http://arxiv.org/pdf/2006.06868v1)

> The black-box nature of neural networks limits model decision interpretability, in particular for high-dimensional inputs in computer vision and for dense pixel prediction tasks like segmentation. To address this, prior work combines neural networks with decision trees. However, such models (1) perform poorly when compared to state-of-the-art segmentation models or (2) fail to produce decision rules with spatially-grounded semantic meaning. In this work, we build a hybrid neural-network and decision-tree model for segmentation that (1) attains neural network segmentation accuracy and (2) provides semi-automatically constructed visual decision rules such as "Is there a window?". We obtain semantic visual meaning by extending saliency methods to segmentation and attain accuracy by leveraging insights from neural-backed decision trees, a deep learning analog of decision trees for image classification. Our model SegNBDT attains accuracy within ~2-4% of the state-of-the-art HRNetV2 segmentation model while also retaining explainability; we achieve state-of-the-art performance for explainable models on three benchmark datasets -- Pascal-Context (49.12%), Cityscapes (79.01%), and Look Into Person (51.64%). Furthermore, user studies suggest visual decision rules are more interpretable, particularly for incorrect predictions. Code and pretrained models can be found at https://github.com/daniel-ho/SegNBDT.

</details>

<details>

<summary>2020-06-12 05:58:56 - PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames</summary>

- *Yuqi Yang, Shilin Liu, Hao Pan, Yang Liu, Xin Tong*

- `1808.04952v2` - [abs](http://arxiv.org/abs/1808.04952v2) - [pdf](http://arxiv.org/pdf/1808.04952v2)

> Surface meshes are widely used shape representations and capture finer geometry data than point clouds or volumetric grids, but are challenging to apply CNNs directly due to their non-Euclidean structure. We use parallel frames on surface to define PFCNNs that enable effective feature learning on surface meshes by mimicking standard convolutions faithfully. In particular, the convolution of PFCNN not only maps local surface patches onto flat tangent planes, but also aligns the tangent planes such that they locally form a flat Euclidean structure, thus enabling recovery of standard convolutions. The alignment is achieved by the tool of locally flat connections borrowed from discrete differential geometry, which can be efficiently encoded and computed by parallel frame fields. In addition, the lack of canonical axis on surface is handled by sampling with the frame directions. Experiments show that for tasks including classification, segmentation and registration on deformable geometric domains, as well as semantic scene segmentation on rigid domains, PFCNNs achieve robust and superior performances without using sophisticated input features than state-of-the-art surface based CNNs.

</details>

<details>

<summary>2020-06-12 08:30:04 - Spatial Firewalls: Quarantining Malware Epidemics in Large Scale Massive Wireless Networks</summary>

- *Hesham Elsawy, Mustafa A. Kishk, Mohamed-Slim Alouini*

- `2006.05059v2` - [abs](http://arxiv.org/abs/2006.05059v2) - [pdf](http://arxiv.org/pdf/2006.05059v2)

> Billions of wireless devices are foreseen to participate in big data aggregation and smart automation in order to interface the cyber and physical worlds. Such large-scale ultra-dense wireless connectivity is vulnerable to malicious software (malware) epidemics. Malware worms can exploit multi-hop wireless connectivity to stealthily diffuse throughout the wireless network without being noticed to security servers at the core network. Compromised devices can then be used by adversaries to remotely launch cyber attacks that cause large-scale critical physical damage and threaten public safety. This article overviews the types, threats, and propagation models for malware epidemics in large-scale wireless networks (LSWN). Then, the article proposes a novel and cost efficient countermeasure against malware epidemics in LSWN, denoted as spatial firewalls. It is shown that equipping a strategically selected small portion (i.e., less than 10\%) of the devices with state-of-the-art security mechanisms is sufficient to create spatially secured zones that quarantine malware epidemics. Quarantined infected devices are then cured by on-demand localized software patching. To this end, several firewall deployment strategies are discussed and compared.

</details>

<details>

<summary>2020-06-12 09:02:41 - Learning Effective Representations for Person-Job Fit by Feature Fusion</summary>

- *Junshu Jiang, Songyun Ye, Wei Wang, Jingran Xu, Xiaosheng Luo*

- `2006.07017v1` - [abs](http://arxiv.org/abs/2006.07017v1) - [pdf](http://arxiv.org/pdf/2006.07017v1)

> Person-job fit is to match candidates and job posts on online recruitment platforms using machine learning algorithms. The effectiveness of matching algorithms heavily depends on the learned representations for the candidates and job posts. In this paper, we propose to learn comprehensive and effective representations of the candidates and job posts via feature fusion. First, in addition to applying deep learning models for processing the free text in resumes and job posts, which is adopted by existing methods, we extract semantic entities from the whole resume (and job post) and then learn features for them. By fusing the features from the free text and the entities, we get a comprehensive representation for the information explicitly stated in the resume and job post. Second, however, some information of a candidate or a job may not be explicitly captured in the resume or job post. Nonetheless, the historical applications including accepted and rejected cases can reveal some implicit intentions of the candidates or recruiters. Therefore, we propose to learn the representations of implicit intentions by processing the historical applications using LSTM. Last, by fusing the representations for the explicit and implicit intentions, we get a more comprehensive and effective representation for person-job fit. Experiments over 10 months real data show that our solution outperforms existing methods with a large margin. Ablation studies confirm the contribution of each component of the fused representation. The extracted semantic entities help interpret the matching results during the case study.

</details>

<details>

<summary>2020-06-12 12:19:06 - NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing</summary>

- *Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim Fedorov, Evgeny Burnaev*

- `2006.07116v1` - [abs](http://arxiv.org/abs/2006.07116v1) - [pdf](http://arxiv.org/pdf/2006.07116v1)

> Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure more reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We believe that our results have high potential of usage for both NAS and NLP communities.

</details>

<details>

<summary>2020-06-13 10:13:55 - How Far are We from Effective Context Modeling? An Exploratory Study on Semantic Parsing in Context</summary>

- *Qian Liu, Bei Chen, Jiaqi Guo, Jian-Guang Lou, Bin Zhou, Dongmei Zhang*

- `2002.00652v2` - [abs](http://arxiv.org/abs/2002.00652v2) - [pdf](http://arxiv.org/pdf/2002.00652v2)

> Recently semantic parsing in context has received considerable attention, which is challenging since there are complex contextual phenomena. Previous works verified their proposed methods in limited scenarios, which motivates us to conduct an exploratory study on context modeling methods under real-world semantic parsing in context. We present a grammar-based decoding semantic parser and adapt typical context modeling methods on top of it. We evaluate 13 context modeling methods on two large complex cross-domain datasets, and our best model achieves state-of-the-art performances on both datasets with significant improvements. Furthermore, we summarize the most frequent contextual phenomena, with a fine-grained analysis on representative models, which may shed light on potential research directions. Our code is available at https://github.com/microsoft/ContextualSP.

</details>

<details>

<summary>2020-06-13 12:49:44 - Will Dependency Conflicts Affect My Program's Semantics?</summary>

- *Ying Wang, Rongxin Wu, Chao Wang, Ming Wen, Yepang Liu, Shing-Chi Cheung, Hai Yu, Chang Xu, Zhiliang Zhu*

- `2006.07633v1` - [abs](http://arxiv.org/abs/2006.07633v1) - [pdf](http://arxiv.org/pdf/2006.07633v1)

> Java projects are often built on top of various third-party libraries. If multiple versions of a library exist on the classpath, JVM will only load one version and shadow the others, which we refer to as dependency conflicts. This would give rise to semantic conflict (SC) issues, if the library APIs referenced by a project have identical method signatures but inconsistent semantics across the loaded and shadowed versions of libraries. SC issues are difficult for developers to diagnose in practice, since understanding them typically requires domain knowledge. Although adapting the existing test generation technique for dependency conflict issues, Riddle, to detect SC issues is feasible, its effectiveness is greatly compromised. This is mainly because Riddle randomly generates test inputs, while the SC issues typically require specific arguments in the tests to be exposed. To address that, we conducted an empirical study of 75 real SC issues to understand the characteristics of such specific arguments in the test cases that can capture the SC issues. Inspired by our empirical findings, we propose an automated testing technique Sensor, which synthesizes test cases using ingredients from the project under test to trigger inconsistent behaviors of the APIs with the same signatures in conflicting library versions. Our evaluation results show that \textsc{Sensor} is effective and useful: it achieved a $Precision$ of 0.803 and a $Recall$ of 0.760 on open-source projects and a $Precision$ of 0.821 on industrial projects; it detected 150 semantic conflict issues in 29 projects, 81.8\% of which had been confirmed as real bugs.

</details>

<details>

<summary>2020-06-13 23:04:14 - Physically-interpretable classification of biological network dynamics for complex collective motions</summary>

- *Keisuke Fujii, Naoya Takeishi, Motokazu Hojo, Yuki Inaba, Yoshinobu Kawahara*

- `1905.04859v2` - [abs](http://arxiv.org/abs/1905.04859v2) - [pdf](http://arxiv.org/pdf/1905.04859v2)

> Understanding biological network dynamics is a fundamental issue in various scientific and engineering fields. Network theory is capable of revealing the relationship between elements and their propagation; however, for complex collective motions, the network properties often transiently and complexly change. A fundamental question addressed here pertains to the classification of collective motion network based on physically-interpretable dynamical properties. Here we apply a data-driven spectral analysis called graph dynamic mode decomposition, which obtains the dynamical properties for collective motion classification. Using a ballgame as an example, we classified the strategic collective motions in different global behaviours and discovered that, in addition to the physical properties, the contextual node information was critical for classification. Furthermore, we discovered the label-specific stronger spectra in the relationship among the nearest agents, providing physical and semantic interpretations. Our approach contributes to the understanding of principles of biological complex network dynamics from the perspective of nonlinear dynamical systems.

</details>

<details>

<summary>2020-06-14 01:52:29 - MixMOOD: A systematic approach to class distribution mismatch in semi-supervised learning using deep dataset dissimilarity measures</summary>

- *Saul Calderon-Ramirez, Luis Oala, Jordina Torrents-Barrena, Shengxiang Yang, Armaghan Moemeni, Wojciech Samek, Miguel A. Molina-Cabello*

- `2006.07767v1` - [abs](http://arxiv.org/abs/2006.07767v1) - [pdf](http://arxiv.org/pdf/2006.07767v1)

> In this work, we propose MixMOOD - a systematic approach to mitigate effect of class distribution mismatch in semi-supervised deep learning (SSDL) with MixMatch. This work is divided into two components: (i) an extensive out of distribution (OOD) ablation test bed for SSDL and (ii) a quantitative unlabelled dataset selection heuristic referred to as MixMOOD. In the first part, we analyze the sensitivity of MixMatch accuracy under 90 different distribution mismatch scenarios across three multi-class classification tasks. These are designed to systematically understand how OOD unlabelled data affects MixMatch performance. In the second part, we propose an efficient and effective method, called deep dataset dissimilarity measures (DeDiMs), to compare labelled and unlabelled datasets. The proposed DeDiMs are quick to evaluate and model agnostic. They use the feature space of a generic Wide-ResNet and can be applied prior to learning. Our test results reveal that supposed semantic similarity between labelled and unlabelled data is not a good heuristic for unlabelled data selection. In contrast, strong correlation between MixMatch accuracy and the proposed DeDiMs allow us to quantitatively rank different unlabelled datasets ante hoc according to expected MixMatch accuracy. This is what we call MixMOOD. Furthermore, we argue that the MixMOOD approach can aid to standardize the evaluation of different semi-supervised learning techniques under real world scenarios involving out of distribution data.

</details>

<details>

<summary>2020-06-14 03:04:19 - LSTM-TrajGAN: A Deep Learning Approach to Trajectory Privacy Protection</summary>

- *Jinmeng Rao, Song Gao, Yuhao Kang, Qunying Huang*

- `2006.10521v1` - [abs](http://arxiv.org/abs/2006.10521v1) - [pdf](http://arxiv.org/pdf/2006.10521v1)

> The prevalence of location-based services contributes to the explosive growth of individual-level trajectory data and raises public concerns about privacy issues. In this research, we propose a novel LSTM-TrajGAN approach, which is an end-to-end deep learning model to generate privacy-preserving synthetic trajectory data for data sharing and publication. We design a loss metric function TrajLoss to measure the trajectory similarity losses for model training and optimization. The model is evaluated on the trajectory-user-linking task on a real-world semantic trajectory dataset. Compared with other common geomasking methods, our model can better prevent users from being re-identified, and it also preserves essential spatial, temporal, and thematic characteristics of the real trajectory data. The model better balances the effectiveness of trajectory privacy protection and the utility for spatial and temporal analyses, which offers new insights into the GeoAI-powered privacy protection.

</details>

<details>

<summary>2020-06-14 06:10:51 - Disentanglement for Discriminative Visual Recognition</summary>

- *Xiaofeng Liu*

- `2006.07810v1` - [abs](http://arxiv.org/abs/2006.07810v1) - [pdf](http://arxiv.org/pdf/2006.07810v1)

> Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization in a controllable manner remains an open issue. For instance, various factors such as identity-specific attributes, pose, illumination and expression affect the appearance of face images. Disentangling the identity-specific factors is potentially beneficial for facial expression recognition (FER). This chapter systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this chapter, these problems are casted as either a deep metric learning problem or an adversarial minimax game in the latent space. For the former choice, a generalized adaptive (N+M)-tuplet clusters loss function together with the identity-aware hard-negative mining and online positive mining scheme can be used for identity-invariant FER. The better FER performance can be achieved by combining the deep metric loss and softmax loss in a unified two fully connected layer branches framework via joint optimization. For the latter solution, it is possible to equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginal independent to the task-relevant/irrelevant semantic and latent variations. The framework achieves top performance on a serial of tasks, including lighting, makeup, disguise-tolerant face recognition and facial attributes recognition. This chapter systematically summarize the popular and practical solution for disentanglement to achieve more discriminative visual recognition.

</details>

<details>

<summary>2020-06-14 08:49:07 - Span-based Localizing Network for Natural Language Video Localization</summary>

- *Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou*

- `2004.13931v2` - [abs](http://arxiv.org/abs/2004.13931v2) - [pdf](http://arxiv.org/pdf/2004.13931v2)

> Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.

</details>

<details>

<summary>2020-06-14 19:14:22 - TabFact: A Large-scale Dataset for Table-based Fact Verification</summary>

- *Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang Wang*

- `1909.02164v5` - [abs](http://arxiv.org/abs/1909.02164v5) - [pdf](http://arxiv.org/pdf/1909.02164v5)

> The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in \url{https://github.com/wenhuchen/Table-Fact-Checking}.

</details>

<details>

<summary>2020-06-15 02:59:52 - Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder</summary>

- *Daya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang, Ming Zhou*

- `2006.08101v1` - [abs](http://arxiv.org/abs/2006.08101v1) - [pdf](http://arxiv.org/pdf/2006.08101v1)

> Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs. Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation. To address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts. Our approach works in an encoder-decoder manner and is equipped with a Vector Quantised-Variational Autoencoder, where the encoder outputs representations from a distribution over discrete variables. Such discrete representations enable automatically selecting relevant evidence, which not only facilitates evidence-aware generation, but also provides a natural way to uncover rationales behind the generation. Our approach provides state-of-the-art performance on both Event2Mind and ATOMIC datasets. More importantly, we find that with discrete representations, our model selectively uses evidence to generate different inferential texts.

</details>

<details>

<summary>2020-06-15 05:47:05 - Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models</summary>

- *Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, Xiang Ren*

- `1911.06194v2` - [abs](http://arxiv.org/abs/1911.06194v2) - [pdf](http://arxiv.org/pdf/1911.06194v2)

> The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the importance of each word and phrase. Following the formulation, we propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling and Occlusion (SOC) algorithm. Human and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms help to visualize semantic composition captured by models, extract classification rules and improve human trust of models. Project page: https://inklab.usc.edu/hiexpl/

</details>

<details>

<summary>2020-06-15 16:05:27 - Learning to map source code to software vulnerability using code-as-a-graph</summary>

- *Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro Morari*

- `2006.08614v1` - [abs](http://arxiv.org/abs/2006.08614v1) - [pdf](http://arxiv.org/pdf/2006.08614v1)

> We explore the applicability of Graph Neural Networks in learning the nuances of source code from a security perspective. Specifically, whether signatures of vulnerabilities in source code can be learned from its graph representation, in terms of relationships between nodes and edges. We create a pipeline we call AI4VA, which first encodes a sample source code into a Code Property Graph. The extracted graph is then vectorized in a manner which preserves its semantic information. A Gated Graph Neural Network is then trained using several such graphs to automatically extract templates differentiating the graph of a vulnerable sample from a healthy one. Our model outperforms static analyzers, classic machine learning, as well as CNN and RNN-based deep learning models on two of the three datasets we experiment with. We thus show that a code-as-graph encoding is more meaningful for vulnerability detection than existing code-as-photo and linear sequence encoding approaches. (Submitted Oct 2019, Paper #28, ICST)

</details>

<details>

<summary>2020-06-15 17:00:01 - Why Normalizing Flows Fail to Detect Out-of-Distribution Data</summary>

- *Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson*

- `2006.08545v1` - [abs](http://arxiv.org/abs/2006.08545v1) - [pdf](http://arxiv.org/pdf/2006.08545v1)

> Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.

</details>

<details>

<summary>2020-06-15 17:54:53 - Towards Verifying Robustness of Neural Networks Against Semantic Perturbations</summary>

- *Jeet Mohapatra, Tsui-Wei, Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel*

- `1912.09533v2` - [abs](http://arxiv.org/abs/1912.09533v2) - [pdf](http://arxiv.org/pdf/1912.09533v2)

> Verifying robustness of neural networks given a specified threat model is a fundamental yet challenging task. While current verification methods mainly focus on the $\ell_p$-norm threat model of the input instances, robustness verification against semantic adversarial attacks inducing large $\ell_p$-norm perturbations, such as color shifting and lighting adjustment, are beyond their capacity. To bridge this gap, we propose \textit{Semantify-NN}, a model-agnostic and generic robustness verification approach against semantic perturbations for neural networks. By simply inserting our proposed \textit{semantic perturbation layers} (SP-layers) to the input layer of any given model, \textit{Semantify-NN} is model-agnostic, and any $\ell_p$-norm based verification tools can be used to verify the model robustness against semantic perturbations. We illustrate the principles of designing the SP-layers and provide examples including semantic perturbations to image classification in the space of hue, saturation, lightness, brightness, contrast and rotation, respectively. In addition, an efficient refinement technique is proposed to further significantly improve the semantic certificate. Experiments on various network architectures and different datasets demonstrate the superior verification performance of \textit{Semantify-NN} over $\ell_p$-norm-based verification frameworks that naively convert semantic perturbation to $\ell_p$-norm. The results show that \textit{Semantify-NN} can support robustness verification against a wide range of semantic perturbations.   Code available https://github.com/JeetMo/Semantify-NN

</details>

<details>

<summary>2020-06-15 21:23:24 - Ordering Dimensions with Nested Dropout Normalizing Flows</summary>

- *Artur Bekasov, Iain Murray*

- `2006.08777v1` - [abs](http://arxiv.org/abs/2006.08777v1) - [pdf](http://arxiv.org/pdf/2006.08777v1)

> The latent space of normalizing flows must be of the same dimensionality as their output space. This constraint presents a problem if we want to learn low-dimensional, semantically meaningful representations. Recent work has provided compact representations by fitting flows constrained to manifolds, but hasn't defined a density off that manifold. In this work we consider flows with full support in data space, but with ordered latent variables. Like in PCA, the leading latent dimensions define a sequence of manifolds that lie close to the data. We note a trade-off between the flow likelihood and the quality of the ordering, depending on the parameterization of the flow.

</details>

<details>

<summary>2020-06-16 01:23:39 - Generative Semantic Hashing Enhanced via Boltzmann Machines</summary>

- *Lin Zheng, Qinliang Su, Dinghan Shen, Changyou Chen*

- `2006.08858v1` - [abs](http://arxiv.org/abs/2006.08858v1) - [pdf](http://arxiv.org/pdf/2006.08858v1)

> Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.

</details>

<details>

<summary>2020-06-16 03:56:13 - An ASP-Based Approach to Counterfactual Explanations for Classification</summary>

- *Leopoldo Bertossi*

- `2004.13237v2` - [abs](http://arxiv.org/abs/2004.13237v2) - [pdf](http://arxiv.org/pdf/2004.13237v2)

> We propose answer-set programs that specify and compute counterfactual interventions as a basis for causality-based explanations to decisions produced by classification models. They can be applied with black-box models and models that can be specified as logic programs, such as rule-based classifiers. The main focus in on the specification and computation of maximum responsibility causal explanations. The use of additional semantic knowledge is investigated.

</details>

<details>

<summary>2020-06-16 09:15:08 - MC-BERT: Efficient Language Pre-Training via a Meta Controller</summary>

- *Zhenhui Xu, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, Tie-Yan Liu*

- `2006.05744v2` - [abs](http://arxiv.org/abs/2006.05744v2) - [pdf](http://arxiv.org/pdf/2006.05744v2)

> Pre-trained contextual representations (e.g., BERT) have become the foundation to achieve state-of-the-art results on many NLP tasks. However, large-scale pre-training is computationally expensive. ELECTRA, an early attempt to accelerate pre-training, trains a discriminative model that predicts whether each input token was replaced by a generator. Our studies reveal that ELECTRA's success is mainly due to its reduced complexity of the pre-training task: the binary classification (replaced token detection) is more efficient to learn than the generation task (masked language modeling). However, such a simplified task is less semantically informative. To achieve better efficiency and effectiveness, we propose a novel meta-learning framework, MC-BERT. The pre-training task is a multi-choice cloze test with a reject option, where a meta controller network provides training input and candidates. Results over GLUE natural language understanding benchmark demonstrate that our proposed method is both efficient and effective: it outperforms baselines on GLUE semantic tasks given the same computational budget.

</details>

<details>

<summary>2020-06-16 09:57:40 - The SPPD System for Schema Guided Dialogue State Tracking Challenge</summary>

- *Miao Li, Haoqi Xiong, Yunbo Cao*

- `2006.09035v1` - [abs](http://arxiv.org/abs/2006.09035v1) - [pdf](http://arxiv.org/pdf/2006.09035v1)

> This paper introduces one of our group's work on the Dialog System Technology Challenges 8 (DSTC8), the SPPD system for Schema Guided dialogue state tracking challenge. This challenge, named as Track 4 in DSTC8, provides a brand new and challenging dataset for developing scalable multi-domain dialogue state tracking algorithms for real world dialogue systems. We propose a zero-shot dialogue state tracking system for this task. The key components of the system is a number of BERT based zero-shot NLU models that can effectively capture semantic relations between natural language descriptions of services' schemas and utterances from dialogue turns. We also propose some strategies to make the system better to exploit information from longer dialogue history and to overcome the slot carryover problem for multi-domain dialogues. The experimental results show that the proposed system achieves a significant improvement compared with the baseline system.

</details>

<details>

<summary>2020-06-16 14:23:27 - Results of the seventh edition of the BioASQ Challenge</summary>

- *Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara, Georgios Paliouras*

- `2006.09174v1` - [abs](http://arxiv.org/abs/2006.09174v1) - [pdf](http://arxiv.org/pdf/2006.09174v1)

> The results of the seventh edition of the BioASQ challenge are presented in this paper. The aim of the BioASQ challenge is the promotion of systems and methodologies through the organization of a challenge on the tasks of large-scale biomedical semantic indexing and question answering. In total, 30 teams with more than 100 systems participated in the challenge this year. As in previous years, the best systems were able to outperform the strong baselines. This suggests that state-of-the-art systems are continuously improving, pushing the frontier of research.

</details>

<details>

<summary>2020-06-16 15:35:38 - Applying Social Event Data for the Management of Cellular Networks</summary>

- *Sergio Fortes, David Palacios, Inmaculada Serrano, Raquel Barco*

- `2006.09258v1` - [abs](http://arxiv.org/abs/2006.09258v1) - [pdf](http://arxiv.org/pdf/2006.09258v1)

> Internet provides a growing variety of social data sources: calendars, event aggregators, social networks, browsers, etc. Also, the mechanisms to gather information from these sources, such as web services, semantic web and big data techniques have become more accessible and efficient. This allows a detailed prediction of the main expected events and their associated crowds. Due to the increasing requirements for service provision, particularly in urban areas, having information on those events would be extremely useful for Operations, Administration and Maintenance (OAM) tasks, since the social events largely affect the cellular network performance. Therefore, this paper presents a framework for the automatic acquisition and processing of social data, as well as their association with network elements (NEs) and their performance. The main functionalities of this system, which have been devised to directly work in real networks, are defined and developed. Different OAM applications of the proposed approach are analyzed and the system is evaluated in a real deployment.

</details>

<details>

<summary>2020-06-16 15:43:12 - Structured and Localized Image Restoration</summary>

- *Thomas Eboli, Alex Nowak-Vila, Jian Sun, Francis Bach, Jean Ponce, Alessandro Rudi*

- `2006.09261v1` - [abs](http://arxiv.org/abs/2006.09261v1) - [pdf](http://arxiv.org/pdf/2006.09261v1)

> We present a novel approach to image restoration that leverages ideas from localized structured prediction and non-linear multi-task learning. We optimize a penalized energy function regularized by a sum of terms measuring the distance between patches to be restored and clean patches from an external database gathered beforehand. The resulting estimator comes with strong statistical guarantees leveraging local dependency properties of overlapping patches. We derive the corresponding algorithms for energies based on the mean-squared and Euclidean norm errors. Finally, we demonstrate the practical effectiveness of our model on different image restoration problems using standard benchmarks.

</details>

<details>

<summary>2020-06-16 16:11:04 - Communicative need modulates competition in language change</summary>

- *Andres Karjus, Richard A. Blythe, Simon Kirby, Kenny Smith*

- `2006.09277v1` - [abs](http://arxiv.org/abs/2006.09277v1) - [pdf](http://arxiv.org/pdf/2006.09277v1)

> All living languages change over time. The causes for this are many, one being the emergence and borrowing of new linguistic elements. Competition between the new elements and older ones with a similar semantic or grammatical function may lead to speakers preferring one of them, and leaving the other to go out of use. We introduce a general method for quantifying competition between linguistic elements in diachronic corpora which does not require language-specific resources other than a sufficiently large corpus. This approach is readily applicable to a wide range of languages and linguistic subsystems. Here, we apply it to lexical data in five corpora differing in language, type, genre, and time span. We find that changes in communicative need are consistently predictive of lexical competition dynamics. Near-synonymous words are more likely to directly compete if they belong to a topic of conversation whose importance to language users is constant over time, possibly leading to the extinction of one of the competing words. By contrast, in topics which are increasing in importance for language users, near-synonymous words tend not to compete directly and can coexist. This suggests that, in addition to direct competition between words, language change can be driven by competition between topics or semantic subspaces.

</details>

<details>

<summary>2020-06-16 16:51:53 - Lung Segmentation and Nodule Detection in Computed Tomography Scan using a Convolutional Neural Network Trained Adversarially using Turing Test Loss</summary>

- *Rakshith Sathish, Rachana Sathish, Ramanathan Sethuraman, Debdoot Sheet*

- `2006.09308v1` - [abs](http://arxiv.org/abs/2006.09308v1) - [pdf](http://arxiv.org/pdf/2006.09308v1)

> Lung cancer is the most common form of cancer found worldwide with a high mortality rate. Early detection of pulmonary nodules by screening with a low-dose computed tomography (CT) scan is crucial for its effective clinical management. Nodules which are symptomatic of malignancy occupy about 0.0125 - 0.025\% of volume in a CT scan of a patient. Manual screening of all slices is a tedious task and presents a high risk of human errors. To tackle this problem we propose a computationally efficient two stage framework. In the first stage, a convolutional neural network (CNN) trained adversarially using Turing test loss segments the lung region. In the second stage, patches sampled from the segmented region are then classified to detect the presence of nodules. The proposed method is experimentally validated on the LUNA16 challenge dataset with a dice coefficient of $0.984\pm0.0007$ for 10-fold cross-validation.

</details>

<details>

<summary>2020-06-16 17:06:02 - Domain Adaptation with Morphologic Segmentation</summary>

- *Jonathan Klein, Sören Pirk, Dominik L. Michels*

- `2006.09322v1` - [abs](http://arxiv.org/abs/2006.09322v1) - [pdf](http://arxiv.org/pdf/2006.09322v1)

> We present a novel domain adaptation framework that uses morphologic segmentation to translate images from arbitrary input domains (real and synthetic) into a uniform output domain. Our framework is based on an established image-to-image translation pipeline that allows us to first transform the input image into a generalized representation that encodes morphology and semantics - the edge-plus-segmentation map (EPS) - which is then transformed into an output domain. Images transformed into the output domain are photo-realistic and free of artifacts that are commonly present across different real (e.g. lens flare, motion blur, etc.) and synthetic (e.g. unrealistic textures, simplified geometry, etc.) data sets. Our goal is to establish a preprocessing step that unifies data from multiple sources into a common representation that facilitates training downstream tasks in computer vision. This way, neural networks for existing tasks can be trained on a larger variety of training data, while they are also less affected by overfitting to specific data sets. We showcase the effectiveness of our approach by qualitatively and quantitatively evaluating our method on four data sets of simulated and real data of urban scenes. Additional results can be found on the project website available at http://jonathank.de/research/eps/ .

</details>

<details>

<summary>2020-06-16 17:59:24 - Semantic Curiosity for Active Visual Learning</summary>

- *Devendra Singh Chaplot, Helen Jiang, Saurabh Gupta, Abhinav Gupta*

- `2006.09367v1` - [abs](http://arxiv.org/abs/2006.09367v1) - [pdf](http://arxiv.org/pdf/2006.09367v1)

> In this paper, we study the task of embodied interactive learning for object detection. Given a set of environments (and some labeling budget), our goal is to learn an object detector by having an agent select what data to obtain labels for. How should an exploration policy decide which trajectory should be labeled? One possibility is to use a trained object detector's failure cases as an external reward. However, this will require labeling millions of frames required for training RL policies, which is infeasible. Instead, we explore a self-supervised approach for training our exploration policy by introducing a notion of semantic curiosity. Our semantic curiosity policy is based on a simple observation -- the detection outputs should be consistent. Therefore, our semantic curiosity rewards trajectories with inconsistent labeling behavior and encourages the exploration policy to explore such areas. The exploration policy trained via semantic curiosity generalizes to novel scenes and helps train an object detector that outperforms baselines trained with other possible alternatives such as random exploration, prediction-error curiosity, and coverage-maximizing exploration.

</details>

<details>

<summary>2020-06-16 18:23:58 - The Role of Verb Semantics in Hungarian Verb-Object Order</summary>

- *Dorottya Demszky, László Kálmán, Dan Jurafsky, Beth Levin*

- `2006.09432v1` - [abs](http://arxiv.org/abs/2006.09432v1) - [pdf](http://arxiv.org/pdf/2006.09432v1)

> Hungarian is often referred to as a discourse-configurational language, since the structural position of constituents is determined by their logical function (topic or comment) rather than their grammatical function (e.g., subject or object). We build on work by Koml\'osy (1989) and argue that in addition to discourse context, the lexical semantics of the verb also plays a significant role in determining Hungarian word order. In order to investigate the role of lexical semantics in determining Hungarian word order, we conduct a large-scale, data-driven analysis on the ordering of 380 transitive verbs and their objects, as observed in hundreds of thousands of examples extracted from the Hungarian Gigaword Corpus. We test the effect of lexical semantics on the ordering of verbs and their objects by grouping verbs into 11 semantic classes. In addition to the semantic class of the verb, we also include two control features related to information structure, object definiteness and object NP weight, chosen to allow a comparison of their effect size to that of verb semantics. Our results suggest that all three features have a significant effect on verb-object ordering in Hungarian and among these features, the semantic class of the verb has the largest effect. Specifically, we find that stative verbs, such as fed "cover", jelent "mean" and \"ovez "surround", tend to be OV-preferring (with the exception of psych verbs which are strongly VO-preferring) and non-stative verbs, such as b\'ir\'al "judge", cs\"okkent "reduce" and cs\'okol "kiss", verbs tend to be VO-preferring. These findings support our hypothesis that lexical semantic factors influence word order in Hungarian.

</details>

<details>

<summary>2020-06-17 01:32:48 - CO-Search: COVID-19 Information Retrieval with Semantic Search, Question Answering, and Abstractive Summarization</summary>

- *Andre Esteva, Anuprit Kale, Romain Paulus, Kazuma Hashimoto, Wenpeng Yin, Dragomir Radev, Richard Socher*

- `2006.09595v1` - [abs](http://arxiv.org/abs/2006.09595v1) - [pdf](http://arxiv.org/pdf/2006.09595v1)

> The COVID-19 global pandemic has resulted in international efforts to understand, track, and mitigate the disease, yielding a significant corpus of COVID-19 and SARS-CoV-2-related publications across scientific disciplines. As of May 2020, 128,000 coronavirus-related publications have been collected through the COVID-19 Open Research Dataset Challenge. Here we present CO-Search, a retriever-ranker semantic search engine designed to handle complex queries over the COVID-19 literature, potentially aiding overburdened health workers in finding scientific answers during a time of crisis. The retriever is built from a Siamese-BERT encoder that is linearly composed with a TF-IDF vectorizer, and reciprocal-rank fused with a BM25 vectorizer. The ranker is composed of a multi-hop question-answering module, that together with a multi-paragraph abstractive summarizer adjust retriever scores. To account for the domain-specific and relatively limited dataset, we generate a bipartite graph of document paragraphs and citations, creating 1.3 million (citation title, paragraph) tuples for training the encoder. We evaluate our system on the data of the TREC-COVID information retrieval challenge. CO-Search obtains top performance on the datasets of the first and second rounds, across several key metrics: normalized discounted cumulative gain, precision, mean average precision, and binary preference.

</details>

<details>

<summary>2020-06-17 02:32:36 - Canonicalizing Open Knowledge Bases with Multi-Layered Meta-Graph Neural Network</summary>

- *Tianwen Jiang, Tong Zhao, Bing Qin, Ting Liu, Nitesh V. Chawla, Meng Jiang*

- `2006.09610v1` - [abs](http://arxiv.org/abs/2006.09610v1) - [pdf](http://arxiv.org/pdf/2006.09610v1)

> Noun phrases and relational phrases in Open Knowledge Bases are often not canonical, leading to redundant and ambiguous facts. In this work, we integrate structural information (from which tuple, which sentence) and semantic information (semantic similarity) to do the canonicalization. We represent the two types of information as a multi-layered graph: the structural information forms the links across the sentence, relational phrase, and noun phrase layers; the semantic information forms weighted intra-layer links for each layer. We propose a graph neural network model to aggregate the representations of noun phrases and relational phrases through the multi-layered meta-graph structure. Experiments show that our model outperforms existing approaches on a public datasets in general domain.

</details>

<details>

<summary>2020-06-17 05:32:14 - XRayGAN: Consistency-preserving Generation of X-ray Images from Radiology Reports</summary>

- *Xingyi Yang, Nandiraju Gireesh, Eric Xing, Pengtao Xie*

- `2006.10552v1` - [abs](http://arxiv.org/abs/2006.10552v1) - [pdf](http://arxiv.org/pdf/2006.10552v1)

> To effectively train medical students to become qualified radiologists, a large number of X-ray images collected from patients with diverse medical conditions are needed. However, due to data privacy concerns, such images are typically difficult to obtain. To address this problem, we develop methods to generate view-consistent, high-fidelity, and high-resolution X-ray images from radiology reports to facilitate radiology training of medical students. This task is presented with several challenges. First, from a single report, images with different views (e.g., frontal, lateral) need to be generated. How to ensure consistency of these images (i.e., make sure they are about the same patient)? Second, X-ray images are required to have high resolution. Otherwise, many details of diseases would be lost. How to generate high-resolutions images? Third, radiology reports are long and have complicated structure. How to effectively understand their semantics to generate high-fidelity images that accurately reflect the contents of the reports? To address these three challenges, we propose an XRayGAN composed of three modules: (1) a view consistency network that maximizes the consistency between generated frontal-view and lateral-view images; (2) a multi-scale conditional GAN that progressively generates a cascade of images with increasing resolution; (3) a hierarchical attentional encoder that learns the latent semantics of a radiology report by capturing its hierarchical linguistic structure and various levels of clinical importance of words and sentences. Experiments on two radiology datasets demonstrate the effectiveness of our methods. To our best knowledge, this work represents the first one generating consistent and high-resolution X-ray images from radiology reports. The code is available at https://github.com/UCSD-AI4H/XRayGAN.

</details>

<details>

<summary>2020-06-17 06:42:06 - $R^3$: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge</summary>

- *Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng*

- `2004.13248v4` - [abs](http://arxiv.org/abs/2004.13248v4) - [pdf](http://arxiv.org/pdf/2004.13248v4)

> We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context which could include shared commonsense or world knowledge between the speaker and the listener. While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality. Human evaluation shows that our system generates sarcasm better than human annotators 34% of the time, and better than a reinforced hybrid baseline 90% of the time.

</details>

<details>

<summary>2020-06-17 14:25:36 - On the Learnability of Concepts: With Applications to Comparing Word Embedding Algorithms</summary>

- *Adam Sutton, Nello Cristianini*

- `2006.09896v1` - [abs](http://arxiv.org/abs/2006.09896v1) - [pdf](http://arxiv.org/pdf/2006.09896v1)

> Word Embeddings are used widely in multiple Natural Language Processing (NLP) applications. They are coordinates associated with each word in a dictionary, inferred from statistical properties of these words in a large corpus. In this paper we introduce the notion of "concept" as a list of words that have shared semantic content. We use this notion to analyse the learnability of certain concepts, defined as the capability of a classifier to recognise unseen members of a concept after training on a random subset of it. We first use this method to measure the learnability of concepts on pretrained word embeddings. We then develop a statistical analysis of concept learnability, based on hypothesis testing and ROC curves, in order to compare the relative merits of various embedding algorithms using a fixed corpora and hyper parameters. We find that all embedding methods capture the semantic content of those word lists, but fastText performs better than the others.

</details>

<details>

<summary>2020-06-17 14:38:44 - Learning Colour Representations of Search Queries</summary>

- *Paridhi Maheshwari, Manoj Ghuhan, Vishwa Vinay*

- `2006.09904v1` - [abs](http://arxiv.org/abs/2006.09904v1) - [pdf](http://arxiv.org/pdf/2006.09904v1)

> Image search engines rely on appropriately designed ranking features that capture various aspects of the content semantics as well as the historic popularity. In this work, we consider the role of colour in this relevance matching process. Our work is motivated by the observation that a significant fraction of user queries have an inherent colour associated with them. While some queries contain explicit colour mentions (such as 'black car' and 'yellow daisies'), other queries have implicit notions of colour (such as 'sky' and 'grass'). Furthermore, grounding queries in colour is not a mapping to a single colour, but a distribution in colour space. For instance, a search for 'trees' tends to have a bimodal distribution around the colours green and brown. We leverage historical clickthrough data to produce a colour representation for search queries and propose a recurrent neural network architecture to encode unseen queries into colour space. We also show how this embedding can be learnt alongside a cross-modal relevance ranker from impression logs where a subset of the result images were clicked. We demonstrate that the use of a query-image colour distance feature leads to an improvement in the ranker performance as measured by users' preferences of clicked versus skipped images.

</details>

<details>

<summary>2020-06-17 14:56:08 - FISHING Net: Future Inference of Semantic Heatmaps In Grids</summary>

- *Noureldin Hendy, Cooper Sloan, Feng Tian, Pengfei Duan, Nick Charchut, Yuesong Xie, Chuang Wang, James Philbin*

- `2006.09917v1` - [abs](http://arxiv.org/abs/2006.09917v1) - [pdf](http://arxiv.org/pdf/2006.09917v1)

> For autonomous robots to navigate a complex environment, it is crucial to understand the surrounding scene both geometrically and semantically. Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception. Choosing a single unified representation for all sensors simplifies the task of perception and fusion. In this work, we present an end-to-end pipeline that performs semantic segmentation and short term prediction using a top-down representation. Our approach consists of an ensemble of neural networks which take in sensor data from different sensor modalities and transform them into a single common top-down semantic grid representation. We find this representation favorable as it is agnostic to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene. Because the modalities share a single output representation, they can be easily aggregated to produce a fused output. In this work we predict short-term semantic grids but the framework can be extended to other tasks. This approach offers a simple, extensible, end-to-end approach for multi-modal perception and prediction.

</details>

<details>

<summary>2020-06-17 16:33:01 - IGLOO: Slicing the Features Space to Represent Sequences</summary>

- *Vsevolod Sourkov*

- `1807.03402v3` - [abs](http://arxiv.org/abs/1807.03402v3) - [pdf](http://arxiv.org/pdf/1807.03402v3)

> Historically, Recurrent neural networks (RNNs) and its variants such as LSTM and GRU and more recently Transformers have been the standard go-to components when processing sequential data with neural networks. One notable issue is the relative difficulty to deal with long sequences (i.e. more than 20,000 steps). We introduce IGLOO, a new neural network architecture which aims at being efficient for short sequences but also at being able to deal with long sequences. IGLOOs core idea is to use the relationships between non-local patches sliced out of the features maps of successively applied convolutions to build a representation for the sequence. We show that the model can deal with dependencies of more than 20,000 steps in a reasonable time frame. We stress test IGLOO on the copy-memory and addition tasks, as well as permuted MNIST (98.4%). For a larger task we apply this new structure to the Wikitext-2 dataset Merity et al. (2017b) and achieve a perplexity in line with baseline Transformers but lower than baseline AWD-LSTM. We also present how IGLOO is already used today in production for bioinformatics tasks.

</details>

<details>

<summary>2020-06-17 17:16:38 - Fast Object Classification and Meaningful Data Representation of Segmented Lidar Instances</summary>

- *Lukas Hahn, Frederik Hasecke, Anton Kummert*

- `2006.10011v1` - [abs](http://arxiv.org/abs/2006.10011v1) - [pdf](http://arxiv.org/pdf/2006.10011v1)

> Object detection algorithms for Lidar data have seen numerous publications in recent years, reporting good results on dataset benchmarks oriented towards automotive requirements. Nevertheless, many of these are not deployable to embedded vehicle systems, as they require immense computational power to be executed close to real time. In this work, we propose a way to facilitate real-time Lidar object classification on CPU. We show how our approach uses segmented object instances to extract important features, enabling a computationally efficient batch-wise classification. For this, we introduce a data representation which translates three-dimensional information into small image patches, using decomposed normal vector images. We couple this with dedicated object statistics to handle edge cases. We apply our method on the tasks of object detection and semantic segmentation, as well as the relatively new challenge of panoptic segmentation. Through evaluation, we show, that our algorithm is capable of producing good results on public data, while running in real time on CPU without using specific optimisation.

</details>

<details>

<summary>2020-06-17 17:36:48 - Deep Learning feature selection to unhide demographic recommender systems factors</summary>

- *Jesús Bobadilla, Ángel González-Prieto, Fernando Ortega, Raúl Lara-Cabrera*

- `2006.12379v1` - [abs](http://arxiv.org/abs/2006.12379v1) - [pdf](http://arxiv.org/pdf/2006.12379v1)

> Extracting demographic features from hidden factors is an innovative concept that provides multiple and relevant applications. The matrix factorization model generates factors which do not incorporate semantic knowledge. This paper provides a deep learning-based method: DeepUnHide, able to extract demographic information from the users and items factors in collaborative filtering recommender systems. The core of the proposed method is the gradient-based localization used in the image processing literature to highlight the representative areas of each classification class. Validation experiments make use of two public datasets and current baselines. Results show the superiority of DeepUnHide to make feature selection and demographic classification, compared to the state of art of feature selection methods. Relevant and direct applications include recommendations explanation, fairness in collaborative filtering and recommendation to groups of users.

</details>

<details>

<summary>2020-06-17 18:30:30 - Extensively Matching for Few-shot Learning Event Detection</summary>

- *Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen*

- `2006.10093v1` - [abs](http://arxiv.org/abs/2006.10093v1) - [pdf](http://arxiv.org/pdf/2006.10093v1)

> Current event detection models under super-vised learning settings fail to transfer to newevent types. Few-shot learning has not beenexplored in event detection even though it al-lows a model to perform well with high gener-alization on new event types. In this work, weformulate event detection as a few-shot learn-ing problem to enable to extend event detec-tion to new event types. We propose two novelloss factors that matching examples in the sup-port set to provide more training signals to themodel. Moreover, these training signals can beapplied in many metric-based few-shot learn-ing models. Our extensive experiments on theACE-2005 dataset (under a few-shot learningsetting) show that the proposed method can im-prove the performance of few-shot learning

</details>

<details>

<summary>2020-06-18 01:55:53 - EnclaveDom: Privilege Separation for Large-TCB Applications in Trusted Execution Environments</summary>

- *Marcela S. Melara, Michael J. Freedman, Mic Bowman*

- `1907.13245v2` - [abs](http://arxiv.org/abs/1907.13245v2) - [pdf](http://arxiv.org/pdf/1907.13245v2)

> Trusted executions environments (TEEs) such as Intel(R) SGX provide hardware-isolated execution areas in memory, called enclaves. By running only the most trusted application components in the enclave, TEEs enable developers to minimize the TCB of their applications thereby helping to protect sensitive application data. However, porting existing applications to TEEs often requires considerable refactoring efforts, as TEEs provide a restricted interface to standard OS features. To ease development efforts, TEE application developers often choose to run their unmodified application in a library OS container that provides a full in-enclave OS interface. Yet, this large-TCB development approach now leaves sensitive in-enclave data exposed to potential bugs or vulnerabilities in third-party code imported into the application. Importantly, because the TEE libOS and the application run in the same enclave address space, even the libOS management data structures (e.g. file descriptor table) may be vulnerable to attack, where in traditional OSes these data structures may be protected via privilege isolation.   We present EnclaveDom, a privilege separation system for large-TCB TEE applications that partitions an enclave into tagged memory regions, and enforces per-region access rules at the granularity of individual in-enclave functions. EnclaveDom is implemented on Intel SGX using Memory Protection Keys (MPK) for memory tagging. To evaluate the security and performance impact of EnclaveDom, we integrated EnclaveDom with the Graphene-SGX library OS. While no product or component can be absolutely secure, our prototype helps protect internal libOS management data structures against tampering by application-level code. At every libOS system call, EnclaveDom then only grants access to those internal data structures which the syscall needs to perform its task.

</details>

<details>

<summary>2020-06-18 12:47:18 - CERT: Contrastive Self-supervised Learning for Language Understanding</summary>

- *Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, Pengtao Xie*

- `2005.12766v2` - [abs](http://arxiv.org/abs/2005.12766v2) - [pdf](http://arxiv.org/pdf/2005.12766v2)

> Pretrained language models such as BERT, GPT have shown great effectiveness in language understanding. The auxiliary predictive tasks in existing pretraining approaches are mostly defined on tokens, thus may not be able to capture sentence-level semantics very well. To address this issue, we propose CERT: Contrastive self-supervised Encoder Representations from Transformers, which pretrains language representation models using contrastive self-supervised learning at the sentence level. CERT creates augmentations of original sentences using back-translation. Then it finetunes a pretrained language encoder (e.g., BERT) by predicting whether two augmented sentences originate from the same sentence. CERT is simple to use and can be flexibly plugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11 natural language understanding tasks in the GLUE benchmark where CERT outperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks, and performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks, CERT outperforms BERT. The data and code are available at https://github.com/UCSD-AI4H/CERT

</details>

<details>

<summary>2020-06-18 15:30:06 - FUSE: Multi-Faceted Set Expansion by Coherent Clustering of Skip-grams</summary>

- *Wanzheng Zhu, Hongyu Gong, Jiaming Shen, Chao Zhang, Jingbo Shang, Suma Bhat, Jiawei Han*

- `1910.04345v3` - [abs](http://arxiv.org/abs/1910.04345v3) - [pdf](http://arxiv.org/pdf/1910.04345v3)

> Set expansion aims to expand a small set of seed entities into a complete set of relevant entities. Most existing approaches assume the input seed set is unambiguous and completely ignore the multi-faceted semantics of seed entities. As a result, given the seed set {"Canon", "Sony", "Nikon"}, previous models return one mixed set of entities that are either Camera Brands or Japanese Companies. In this paper, we study the task of multi-faceted set expansion, which aims to capture all semantic facets in the seed set and return multiple sets of entities, one for each semantic facet. We propose an unsupervised framework, FUSE, which consists of three major components: (1) facet discovery module: identifies all semantic facets of each seed entity by extracting and clustering its skip-grams, and (2) facet fusion module: discovers shared semantic facets of the entire seed set by an optimization formulation, and (3) entity expansion module: expands each semantic facet by utilizing a masked language model with pre-trained BERT models. Extensive experiments demonstrate that FUSE can accurately identify multiple semantic facets of the seed set and generate quality entities for each facet.

</details>

<details>

<summary>2020-06-18 18:59:44 - Semantic Linking Maps for Active Visual Object Search</summary>

- *Zhen Zeng, Adrian Röfer, Odest Chadwicke Jenkins*

- `2006.10807v1` - [abs](http://arxiv.org/abs/2006.10807v1) - [pdf](http://arxiv.org/pdf/2006.10807v1)

> We aim for mobile robots to function in a variety of common human environments. Such robots need to be able to reason about the locations of previously unseen target objects. Landmark objects can help this reasoning by narrowing down the search space significantly. More specifically, we can exploit background knowledge about common spatial relations between landmark and target objects. For example, seeing a table and knowing that cups can often be found on tables aids the discovery of a cup. Such correlations can be expressed as distributions over possible pairing relationships of objects. In this paper, we propose an active visual object search strategy method through our introduction of the Semantic Linking Maps (SLiM) model. SLiM simultaneously maintains the belief over a target object's location as well as landmark objects' locations, while accounting for probabilistic inter-object spatial relations. Based on SLiM, we describe a hybrid search strategy that selects the next best view pose for searching for the target object based on the maintained belief. We demonstrate the efficiency of our SLiM-based search strategy through comparative experiments in simulated environments. We further demonstrate the real-world applicability of SLiM-based search in scenarios with a Fetch mobile manipulation robot.

</details>

<details>

<summary>2020-06-18 21:05:27 - Deep Image Translation for Enhancing Simulated Ultrasound Images</summary>

- *Lin Zhang, Tiziano Portenier, Christoph Paulus, Orcun Goksel*

- `2006.10850v1` - [abs](http://arxiv.org/abs/2006.10850v1) - [pdf](http://arxiv.org/pdf/2006.10850v1)

> Ultrasound simulation based on ray tracing enables the synthesis of highly realistic images. It can provide an interactive environment for training sonographers as an educational tool. However, due to high computational demand, there is a trade-off between image quality and interactivity, potentially leading to sub-optimal results at interactive rates. In this work we introduce a deep learning approach based on adversarial training that mitigates this trade-off by improving the quality of simulated images with constant computation time. An image-to-image translation framework is utilized to translate low quality images into high quality versions. To incorporate anatomical information potentially lost in low quality images, we additionally provide segmentation maps to image translation. Furthermore, we propose to leverage information from acoustic attenuation maps to better preserve acoustic shadows and directional artifacts, an invaluable feature for ultrasound image interpretation. The proposed method yields an improvement of 7.2% in Fr\'{e}chet Inception Distance and 8.9% in patch-based Kullback-Leibler divergence.

</details>

<details>

<summary>2020-06-19 01:08:38 - Learning Temporal Attention in Dynamic Graphs with Bilinear Interactions</summary>

- *Boris Knyazev, Carolyn Augusta, Graham W. Taylor*

- `1909.10367v2` - [abs](http://arxiv.org/abs/1909.10367v2) - [pdf](http://arxiv.org/pdf/1909.10367v2)

> Reasoning about graphs evolving over time is a challenging concept in many domains, such as bioinformatics, physics, and social networks. We consider a common case in which edges can be short term interactions (e.g., messaging) or long term structural connections (e.g., friendship). In practice, long term edges are often specified by humans. Human-specified edges can be both expensive to produce and suboptimal for the downstream task. To alleviate these issues, we propose a model based on temporal point processes and variational autoencoders that learns to infer temporal attention between nodes by observing node communication. As temporal attention drives between-node feature propagation, using the dynamics of node interactions to learn this key component provides more flexibility while simultaneously avoiding issues associated with human-specified edges. We also propose a bilinear transformation layer for pairs of node features instead of concatenation, typically used in prior work, and demonstrate its superior performance in all cases. In experiments on two datasets in the dynamic link prediction task, our model often outperforms the baseline model that requires a human-specified graph. Moreover, our learned attention is semantically interpretable and infers connections similar to actual graphs.

</details>

<details>

<summary>2020-06-19 10:21:17 - A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures</summary>

- *Meishan Zhang*

- `2006.11056v1` - [abs](http://arxiv.org/abs/2006.11056v1) - [pdf](http://arxiv.org/pdf/2006.11056v1)

> Syntactic and semantic parsing has been investigated for decades, which is one primary topic in the natural language processing community. This article aims for a brief survey on this topic. The parsing community includes many tasks, which are difficult to be covered fully. Here we focus on two of the most popular formalizations of parsing: constituent parsing and dependency parsing. Constituent parsing is majorly targeted to syntactic analysis, and dependency parsing can handle both syntactic and semantic analysis. This article briefly reviews the representative models of constituent parsing and dependency parsing, and also dependency graph parsing with rich semantics. Besides, we also review the closely-related topics such as cross-domain, cross-lingual and joint parsing models, parser application as well as corpus development of parsing in the article.

</details>

<details>

<summary>2020-06-19 11:25:36 - Differentiable Language Model Adversarial Attacks on Categorical Sequence Classifiers</summary>

- *I. Fursov, A. Zaytsev, N. Kluchnikov, A. Kravchenko, E. Burnaev*

- `2006.11078v1` - [abs](http://arxiv.org/abs/2006.11078v1) - [pdf](http://arxiv.org/pdf/2006.11078v1)

> An adversarial attack paradigm explores various scenarios for the vulnerability of deep learning models: minor changes of the input can force a model failure. Most of the state of the art frameworks focus on adversarial attacks for images and other structured model inputs, but not for categorical sequences models.   Successful attacks on classifiers of categorical sequences are challenging because the model input is tokens from finite sets, so a classifier score is non-differentiable with respect to inputs, and gradient-based attacks are not applicable. Common approaches deal with this problem working at a token level, while the discrete optimization problem at hand requires a lot of resources to solve.   We instead use a fine-tuning of a language model for adversarial attacks as a generator of adversarial examples. To optimize the model, we define a differentiable loss function that depends on a surrogate classifier score and on a deep learning model that evaluates approximate edit distance. So, we control both the adversability of a generated sequence and its similarity to the initial sequence.   As a result, we obtain semantically better samples. Moreover, they are resistant to adversarial training and adversarial detectors. Our model works for diverse datasets on bank transactions, electronic health records, and NLP datasets.

</details>

<details>

<summary>2020-06-19 18:57:51 - AutoOD: Automated Outlier Detection via Curiosity-guided Search and Self-imitation Learning</summary>

- *Yuening Li, Zhengzhang Chen, Daochen Zha, Kaixiong Zhou, Haifeng Jin, Haifeng Chen, Xia Hu*

- `2006.11321v1` - [abs](http://arxiv.org/abs/2006.11321v1) - [pdf](http://arxiv.org/pdf/2006.11321v1)

> Outlier detection is an important data mining task with numerous practical applications such as intrusion detection, credit card fraud detection, and video surveillance. However, given a specific complicated task with big data, the process of building a powerful deep learning based system for outlier detection still highly relies on human expertise and laboring trials. Although Neural Architecture Search (NAS) has shown its promise in discovering effective deep architectures in various domains, such as image classification, object detection, and semantic segmentation, contemporary NAS methods are not suitable for outlier detection due to the lack of intrinsic search space, unstable search process, and low sample efficiency. To bridge the gap, in this paper, we propose AutoOD, an automated outlier detection framework, which aims to search for an optimal neural network model within a predefined search space. Specifically, we firstly design a curiosity-guided search strategy to overcome the curse of local optimality. A controller, which acts as a search agent, is encouraged to take actions to maximize the information gain about the controller's internal belief. We further introduce an experience replay mechanism based on self-imitation learning to improve the sample efficiency. Experimental results on various real-world benchmark datasets demonstrate that the deep model identified by AutoOD achieves the best performance, comparing with existing handcrafted models and traditional search methods.

</details>

<details>

<summary>2020-06-19 22:18:03 - Deep Context Maps: Agent Trajectory Prediction using Location-specific Latent Maps</summary>

- *Igor Gilitschenski, Guy Rosman, Arjun Gupta, Sertac Karaman, Daniela Rus*

- `1912.06785v2` - [abs](http://arxiv.org/abs/1912.06785v2) - [pdf](http://arxiv.org/pdf/1912.06785v2)

> In this paper, we propose a novel approach for agent motion prediction in cluttered environments. One of the main challenges in predicting agent motion is accounting for location and context-specific information. Our main contribution is the concept of learning context maps to improve the prediction task. Context maps are a set of location-specific latent maps that are trained alongside the predictor. Thus, the proposed maps are capable of capturing location context beyond visual context cues (e.g. usual average speeds and typical trajectories) or predefined map primitives (such as lanes and stop lines). We pose context map learning as a multi-task training problem and describe our map model and its incorporation into a state-of-the-art trajectory predictor. In extensive experiments, it is shown that use of learned maps can significantly improve predictor accuracy. Furthermore, the performance can be additionally boosted by providing partial knowledge of map semantics.

</details>

<details>

<summary>2020-06-20 10:19:29 - Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition</summary>

- *Ionut Cosmin Duta, Li Liu, Fan Zhu, Ling Shao*

- `2006.11538v1` - [abs](http://arxiv.org/abs/2006.11538v1) - [pdf](http://arxiv.org/pdf/2006.11538v1)

> This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv

</details>

<details>

<summary>2020-06-20 11:19:50 - Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks</summary>

- *Diego Marcheggiani, Jasmijn Bastings, Ivan Titov*

- `1804.08313v2` - [abs](http://arxiv.org/abs/1804.08313v2) - [pdf](http://arxiv.org/pdf/1804.08313v2)

> Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of machine translation methods. In this work, we are the first to incorporate information about predicate-argument structure of source sentences (namely, semantic-role representations) into neural machine translation. We use Graph Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware versions on the English--German language pair.

</details>

<details>

<summary>2020-06-20 13:57:55 - Learning aligned embeddings for semi-supervised word translation using Maximum Mean Discrepancy</summary>

- *Antonio H. O. Fonseca, David van Dijk*

- `2006.11578v1` - [abs](http://arxiv.org/abs/2006.11578v1) - [pdf](http://arxiv.org/pdf/2006.11578v1)

> Word translation is an integral part of language translation. In machine translation, each language is considered a domain with its own word embedding. The alignment between word embeddings allows linking semantically equivalent words in multilingual contexts. Moreover, it offers a way to infer cross-lingual meaning for words without a direct translation. Current methods for word embedding alignment are either supervised, i.e. they require known word pairs, or learn a cross-domain transformation on fixed embeddings in an unsupervised way. Here we propose an end-to-end approach for word embedding alignment that does not require known word pairs. Our method, termed Word Alignment through MMD (WAM), learns embeddings that are aligned during sentence translation training using a localized Maximum Mean Discrepancy (MMD) constraint between the embeddings. We show that our method not only out-performs unsupervised methods, but also supervised methods that train on known word translations.

</details>

<details>

<summary>2020-06-20 19:29:50 - Dynamic Symbolic Execution of Higher-Order Functions</summary>

- *Shu-Hung You, Robert Bruce Findler, Christos Dimoulas*

- `2006.11639v1` - [abs](http://arxiv.org/abs/2006.11639v1) - [pdf](http://arxiv.org/pdf/2006.11639v1)

> The effectiveness of concolic testing deteriorates as the size of programs increases. A promising way out is to test programs modularly, e.g., on a per function or class basis. Alas, this idea hits a roadblock in modern programming languages In modern languages, components expect functions, objects, and even classes as inputs. The crux of the problem is that existing concolic testing techniques cannot faithfully capture the complex interactions between a higher-order program and its inputs in order to distill it in a first-order formula that an SMT solver can work with. In this paper, we take the first step towards solving the problem; we offer a design, semantics, and prototype for concolic testing of higher-order functions. Inspired by work on higher-order symbolic execution, our model constructs inputs for higher-order functions with a canonical shape. This enables the concolic tester to keep track of which pieces of the control-flow path of the higher-order function depend on the shape of its input and which do not. The concolic tester encodes the pieces that do not depend on the shape of the input as a first-order formula. Subsequently, similar to a first-order concolic tester, it leverages an SMT solver to produce another input with the same shape but that explores a different control-flow path of the higher-order function. As a separate dimension, the concolic tester iteratively explores the canonical shapes for the input and, investigating all the ways a higher-order function can interact with its input, searching for bugs. To validate our design, we prove that if a higher-order function has a bug, our concolic tester will eventually construct an input that triggers the bug. Using our design as a blueprint, we implement a prototype concolic tester and confirm that it discovers bugs in a variety of higher-order programs from the literature.

</details>

<details>

<summary>2020-06-20 19:51:13 - A Nearest Neighbor Network to Extract Digital Terrain Models from 3D Point Clouds</summary>

- *Mohammed Yousefhussien, David J. Kelbe, Carl Salvaggio*

- `2005.10745v2` - [abs](http://arxiv.org/abs/2005.10745v2) - [pdf](http://arxiv.org/pdf/2005.10745v2)

> When 3D-point clouds from overhead sensors are used as input to remote sensing data exploitation pipelines, a large amount of effort is devoted to data preparation. Among the multiple stages of the preprocessing chain, estimating the Digital Terrain Model (DTM) model is considered to be of a high importance; however, this remains a challenge, especially for raw point clouds derived from optical imagery. Current algorithms estimate the ground points using either a set of geometrical rules that require tuning multiple parameters and human interaction, or cast the problem as a binary classification machine learning task where ground and non-ground classes are found. In contrast, here we present an algorithm that directly operates on 3D-point clouds and estimate the underlying DTM for the scene using an end-to-end approach without the need to classify points into ground and non-ground cover types. Our model learns neighborhood information and seamlessly integrates this with point-wise and block-wise global features. We validate our model using the ISPRS 3D Semantic Labeling Contest LiDAR data, as well as three scenes generated using dense stereo matching, representative of high-rise buildings, lower urban structures, and a dense old-city residential area. We compare our findings with two widely used software packages for DTM extraction, namely ENVI and LAStools. Our preliminary results show that the proposed method is able to achieve an overall Mean Absolute Error of 11.5% compared to 29% and 16% for ENVI and LAStools.

</details>

<details>

<summary>2020-06-20 22:43:53 - Semantically Tied Paired Cycle Consistency for Any-Shot Sketch-based Image Retrieval</summary>

- *Anjan Dutta, Zeynep Akata*

- `2006.11397v1` - [abs](http://arxiv.org/abs/2006.11397v1) - [pdf](http://arxiv.org/pdf/2006.11397v1)

> Low-shot sketch-based image retrieval is an emerging task in computer vision, allowing to retrieve natural images relevant to hand-drawn sketch queries that are rarely seen during the training phase. Related prior works either require aligned sketch-image pairs that are costly to obtain or inefficient memory fusion layer for mapping the visual information to a semantic space. In this paper, we address any-shot, i.e. zero-shot and few-shot, sketch-based image retrieval (SBIR) tasks, where we introduce the few-shot setting for SBIR. For solving these tasks, we propose a semantically aligned paired cycle-consistent generative adversarial network (SEM-PCYC) for any-shot SBIR, where each branch of the generative adversarial network maps the visual information from sketch and image to a common semantic space via adversarial training. Each of these branches maintains cycle consistency that only requires supervision at the category level, and avoids the need of aligned sketch-image pairs. A classification criteria on the generators' outputs ensures the visual to semantic space mapping to be class-specific. Furthermore, we propose to combine textual and hierarchical side information via an auto-encoder that selects discriminating side information within a same end-to-end model. Our results demonstrate a significant boost in any-shot SBIR performance over the state-of-the-art on the extended version of the challenging Sketchy, TU-Berlin and QuickDraw datasets.

</details>

<details>

<summary>2020-06-21 11:07:37 - Patch Based Classification of Remote Sensing Data: A Comparison of 2D-CNN, SVM and NN Classifiers</summary>

- *Mahesh Pal, Akshay, Himanshu Rohilla, B. Charan Teja*

- `2006.11767v1` - [abs](http://arxiv.org/abs/2006.11767v1) - [pdf](http://arxiv.org/pdf/2006.11767v1)

> Pixel based algorithms including back propagation neural networks (NN) and support vector machines (SVM) have been widely used for remotely sensed image classifications. Within last few years, deep learning based image classifier like convolution neural networks (2D-CNN) are becoming popular alternatives to these classifiers. In this paper, we compare performance of patch based SVM and NN with that of a deep learning algorithms comprising of 2D-CNN and fully connected layers. Similar to CNN which utilise image patches to derive features for further classification, we propose to use patches as an input in place of individual pixel with both SVM and NN classifiers. Two datasets, one multispectral and other hyperspectral data was used to compare the performance of different classifiers. Results with both datasets suggest the effectiveness of patch based SVM and NN classifiers in comparison to state of art 2D-CNN classifier.

</details>

<details>

<summary>2020-06-21 14:10:47 - Improving Image Captioning with Better Use of Captions</summary>

- *Zhan Shi, Xu Zhou, Xipeng Qiu, Xiaodan Zhu*

- `2006.11807v1` - [abs](http://arxiv.org/abs/2006.11807v1) - [pdf](http://arxiv.org/pdf/2006.11807v1)

> Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics.

</details>

<details>

<summary>2020-06-22 02:09:44 - Word Sense Disambiguation using Knowledge-based Word Similarity</summary>

- *Sunjae Kwon, Dongsuk Oh, Youngjoong Ko*

- `1911.04015v2` - [abs](http://arxiv.org/abs/1911.04015v2) - [pdf](http://arxiv.org/pdf/1911.04015v2)

> In natural language processing, word-sense disambiguation (WSD) is an open problem concerned with identifying the correct sense of words in a particular context. To address this problem, we introduce a novel knowledge-based WSD system. We suggest the adoption of two methods in our system. First, we suggest a novel method to encode the word vector representation by considering the graphical semantic relationships from the lexical knowledge-base. Second, we propose a method for extracting the contextual words from the text for analyzing an ambiguous word based on the similarity of word vector representations. To validate the effectiveness of our WSD system, we conducted experiments on the five benchmark English WSD corpora (Senseval-02, Senseval-03, SemEval-07, SemEval-13, and SemEval-15). The obtained results demonstrated that the suggested methods significantly enhanced the WSD performance. Furthermore, our system outperformed the existing knowledge-based WSD systems and showed a performance comparable to that of the state-of-the-art supervised WSD systems.

</details>

<details>

<summary>2020-06-22 04:49:47 - Efficient text generation of user-defined topic using generative adversarial networks</summary>

- *Chenhan Yuan, Yi-chin Huang, Cheng-Hung Tsai*

- `2006.12005v1` - [abs](http://arxiv.org/abs/2006.12005v1) - [pdf](http://arxiv.org/pdf/2006.12005v1)

> This study focused on efficient text generation using generative adversarial networks (GAN). Assuming that the goal is to generate a paragraph of a user-defined topic and sentimental tendency, conventionally the whole network has to be re-trained to obtain new results each time when a user changes the topic. This would be time-consuming and impractical. Therefore, we propose a User-Defined GAN (UD-GAN) with two-level discriminators to solve this problem. The first discriminator aims to guide the generator to learn paragraph-level information and sentence syntactic structure, which is constructed by multiple-LSTMs. The second one copes with higher-level information, such as the user-defined sentiment and topic for text generation. The cosine similarity based on TF-IDF and length penalty are adopted to determine the relevance of the topic. Then, the second discriminator is re-trained with the generator if the topic or sentiment for text generation is modified. The system evaluations are conducted to compare the performance of the proposed method with other GAN-based ones. The objective results showed that the proposed method is capable of generating texts with less time than others and the generated text is related to the user-defined topic and sentiment. We will further investigate the possibility of incorporating more detailed paragraph information such as semantics into text generation to enhance the result.

</details>

<details>

<summary>2020-06-22 07:17:57 - Folding-based compression of point cloud attributes</summary>

- *Maurice Quach, Giuseppe Valenzise, Frederic Dufaux*

- `2002.04439v3` - [abs](http://arxiv.org/abs/2002.04439v3) - [pdf](http://arxiv.org/pdf/2002.04439v3)

> Existing techniques to compress point cloud attributes leverage either geometric or video-based compression tools. We explore a radically different approach inspired by recent advances in point cloud representation learning. Point clouds can be interpreted as 2D manifolds in 3D space. Specifically, we fold a 2D grid onto a point cloud and we map attributes from the point cloud onto the folded 2D grid using a novel optimized mapping method. This mapping results in an image, which opens a way to apply existing image processing techniques on point cloud attributes. However, as this mapping process is lossy in nature, we propose several strategies to refine it so that attributes can be mapped to the 2D grid with minimal distortion. Moreover, this approach can be flexibly applied to point cloud patches in order to better adapt to local geometric complexity. In this work, we consider point cloud attribute compression; thus, we compress this image with a conventional 2D image codec. Our preliminary results show that the proposed folding-based coding scheme can already reach performance similar to the latest MPEG Geometry-based PCC (G-PCC) codec.

</details>

<details>

<summary>2020-06-22 08:12:38 - An In-Depth Security Assessment of Maritime Container Terminal Software Systems</summary>

- *Joseph O. Eichenhofer, Elisa Heymann, Barton P. Miller, Arnold Kang*

- `2006.12056v1` - [abs](http://arxiv.org/abs/2006.12056v1) - [pdf](http://arxiv.org/pdf/2006.12056v1)

> Attacks on software systems occur world-wide on a daily basis targeting individuals, corporations, and governments alike. The systems that facilitate maritime shipping are at risk of serious disruptions, and these disruptions can stem from vulnerabilities in the software and processes used in these systems. These vulnerabilities leave such systems open to cyber-attack. Assessments of the security of maritime shipping systems have focused on identifying risks but have not taken the critical (and expensive) next step of actually identifying vulnerabilities present in these systems. While such risk assessments are important, they have not provided the detailed identification of security issues in the systems that control these ports and their terminals. In response, we formed a key collaboration between an experienced academic cybersecurity team and a well-known commercial software provider that manages maritime shipping. We performed an analysis of the information flow involved in the maritime shipping process, and then executed an in-depth vulnerability assessment of the software that manages freight systems. In this paper, we show the flow of information involved in the freight shipping process and explain how we performed the in-depth assessment, summarizing our findings. Like every large software system, maritime shipping systems have vulnerabilities.

</details>

<details>

<summary>2020-06-22 09:09:22 - Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</summary>

- *Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu*

- `2004.00849v2` - [abs](http://arxiv.org/abs/2004.00849v2) - [pdf](http://arxiv.org/pdf/2004.00849v2)

> We propose Pixel-BERT to align image pixels with text by deep multi-modal transformers that jointly learn visual and language embedding in a unified end-to-end framework. We aim to build a more accurate and thorough connection between image pixels and language semantics directly from image and sentence pairs instead of using region-based image features as the most recent vision and language tasks. Our Pixel-BERT which aligns semantic connection in pixel and text level solves the limitation of task-specific visual representation for vision and language tasks. It also relieves the cost of bounding box annotations and overcomes the unbalance between semantic labels in visual task and language semantic. To provide a better representation for down-stream tasks, we pre-train a universal end-to-end model with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We propose to use a random pixel sampling mechanism to enhance the robustness of visual representation and to apply the Masked Language Model and Image-Text Matching as pre-training tasks. Extensive experiments on downstream tasks with our pre-trained model show that our approach makes the most state-of-the-arts in downstream tasks, including Visual Question Answering (VQA), image-text retrieval, Natural Language for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of a single model in VQA task by 2.17 points compared with SOTA under fair comparison.

</details>

<details>

<summary>2020-06-22 09:59:39 - Exploiting Non-Taxonomic Relations for Measuring Semantic Similarity and Relatedness in WordNet</summary>

- *Mohannad AlMousa, Rachid Benlamri, Richard Khoury*

- `2006.12106v1` - [abs](http://arxiv.org/abs/2006.12106v1) - [pdf](http://arxiv.org/pdf/2006.12106v1)

> Various applications in the areas of computational linguistics and artificial intelligence employ semantic similarity to solve challenging tasks, such as word sense disambiguation, text classification, information retrieval, machine translation, and document clustering. Previous work on semantic similarity followed a mono-relational approach using mostly the taxonomic relation "ISA". This paper explores the benefits of using all types of non-taxonomic relations in large linked data, such as WordNet knowledge graph, to enhance existing semantic similarity and relatedness measures. We propose a holistic poly-relational approach based on a new relation-based information content and non-taxonomic-based weighted paths to devise a comprehensive semantic similarity and relatedness measure. To demonstrate the benefits of exploiting non-taxonomic relations in a knowledge graph, we used three strategies to deploy non-taxonomic relations at different granularity levels. We conducted experiments on four well-known gold standard datasets, and the results demonstrated the robustness and scalability of the proposed semantic similarity and relatedness measure, which significantly improves existing similarity measures.

</details>

<details>

<summary>2020-06-22 13:26:37 - HookNet: multi-resolution convolutional neural networks for semantic segmentation in histopathology whole-slide images</summary>

- *Mart van Rijthoven, Maschenka Balkenhol, Karina Siliņa, Jeroen van der Laak, Francesco Ciompi*

- `2006.12230v1` - [abs](http://arxiv.org/abs/2006.12230v1) - [pdf](http://arxiv.org/pdf/2006.12230v1)

> We propose HookNet, a semantic segmentation model for histopathology whole-slide images, which combines context and details via multiple branches of encoder-decoder convolutional neural networks. Concentricpatches at multiple resolutions with different fields of view are used to feed different branches of HookNet, and intermediate representations are combined via a hooking mechanism. We describe a framework to design and train HookNet for achieving high-resolution semantic segmentation and introduce constraints to guarantee pixel-wise alignment in feature maps during hooking. We show the advantages of using HookNet in two histopathology image segmentation tasks where tissue type prediction accuracy strongly depends on contextual information, namely (1) multi-class tissue segmentation in breast cancer and, (2) segmentation of tertiary lymphoid structures and germinal centers in lung cancer. Weshow the superiority of HookNet when compared with single-resolution U-Net models working at different resolutions as well as with a recently published multi-resolution model for histopathology image segmentation

</details>

<details>

<summary>2020-06-22 19:24:52 - Graph Neural Networks and Reinforcement Learning for Behavior Generation in Semantic Environments</summary>

- *Patrick Hart, Alois Knoll*

- `2006.12576v1` - [abs](http://arxiv.org/abs/2006.12576v1) - [pdf](http://arxiv.org/pdf/2006.12576v1)

> Most reinforcement learning approaches used in behavior generation utilize vectorial information as input. However, this requires the network to have a pre-defined input-size -- in semantic environments this means assuming the maximum number of vehicles. Additionally, this vectorial representation is not invariant to the order and number of vehicles. To mitigate the above-stated disadvantages, we propose combining graph neural networks with actor-critic reinforcement learning. As graph neural networks apply the same network to every vehicle and aggregate incoming edge information, they are invariant to the number and order of vehicles. This makes them ideal candidates to be used as networks in semantic environments -- environments consisting of objects lists. Graph neural networks exhibit some other advantages that make them favorable to be used in semantic environments. The relational information is explicitly given and does not have to be inferred. Moreover, graph neural networks propagate information through the network and can gather higher-degree information. We demonstrate our approach using a highway lane-change scenario and compare the performance of graph neural networks to conventional ones. We show that graph neural networks are capable of handling scenarios with a varying number and order of vehicles during training and application.

</details>

<details>

<summary>2020-06-23 00:14:15 - Category-wise Attack: Transferable Adversarial Examples for Anchor Free Object Detection</summary>

- *Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu*

- `2003.04367v4` - [abs](http://arxiv.org/abs/2003.04367v4) - [pdf](http://arxiv.org/pdf/2003.04367v4)

> Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbations can completely change the classification results. Their vulnerability has led to a surge of research in this direction. However, most works dedicated to attacking anchor-based object detection models. In this work, we aim to present an effective and efficient algorithm to generate adversarial examples to attack anchor-free object models based on two approaches. First, we conduct category-wise instead of instance-wise attacks on the object detectors. Second, we leverage the high-level semantic information to generate the adversarial examples. Surprisingly, the generated adversarial examples it not only able to effectively attack the targeted anchor-free object detector but also to be transferred to attack other object detectors, even anchor-based detectors such as Faster R-CNN.

</details>

<details>

<summary>2020-06-23 04:12:09 - Improving Query Safety at Pinterest</summary>

- *Abhijit Mahabal, Yinrui Li, Rajat Raina, Daniel Sun, Revati Mahajan, Jure Leskovec*

- `2006.11511v2` - [abs](http://arxiv.org/abs/2006.11511v2) - [pdf](http://arxiv.org/pdf/2006.11511v2)

> Query recommendations in search engines is a double edged sword, with undeniable benefits but potential of harm. Identifying unsafe queries is necessary to protect users from inappropriate query suggestions. However, identifying these is non-trivial because of the linguistic diversity resulting from large vocabularies, social-group-specific slang and typos, and because the inappropriateness of a term depends on the context. Here we formulate the problem as query-set expansion, where we are given a small and potentially biased seed set and the aim is to identify a diverse set of semantically related queries. We present PinSets, a system for query-set expansion, which applies a simple yet powerful mechanism to search user sessions, expanding a tiny seed set into thousands of related queries at nearly perfect precision, deep into the tail, along with explanations that are easy to interpret. PinSets owes its high quality expansion to using a hybrid of textual and behavioral techniques (i.e., treating queries both as compositional and as black boxes). Experiments show that, for the domain of drugs-related queries, PinSets expands 20 seed queries into 15,670 positive training examples at over 99\% precision. The generated expansions have diverse vocabulary and correctly handles words with ambiguous safety. PinSets decreased unsafe query suggestions at Pinterest by 90\%.

</details>

<details>

<summary>2020-06-23 09:02:52 - Data Efficient Stagewise Knowledge Distillation</summary>

- *Akshay Kulkarni, Navid Panchi, Sharath Chandra Raparthy, Shital Chiddarwar*

- `1911.06786v3` - [abs](http://arxiv.org/abs/1911.06786v3) - [pdf](http://arxiv.org/pdf/1911.06786v3)

> Despite the success of Deep Learning (DL), the deployment of modern DL models requiring large computational power poses a significant problem for resource-constrained systems. This necessitates building compact networks that reduce computations while preserving performance. Traditional Knowledge Distillation (KD) methods that transfer knowledge from teacher to student (a) use a single-stage and (b) require the whole data set while distilling the knowledge to the student. In this work, we propose a new method called Stagewise Knowledge Distillation (SKD) which builds on traditional KD methods by progressive stagewise training to leverage the knowledge gained from the teacher, resulting in data-efficient distillation process. We evaluate our method on classification and semantic segmentation tasks. We show, across the tested tasks, significant performance gains even with a fraction of the data used in distillation, without compromising on the metric. We also compare our method with existing KD techniques and show that SKD outperforms them. Moreover, our method can be viewed as a generalized model compression technique that complements other model compression methods such as quantization or pruning.

</details>

<details>

<summary>2020-06-23 13:47:58 - Neural relation extraction: a survey</summary>

- *Mehmet Aydar, Ozge Bozal, Furkan Ozbay*

- `2007.04247v1` - [abs](http://arxiv.org/abs/2007.04247v1) - [pdf](http://arxiv.org/pdf/2007.04247v1)

> Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.

</details>

<details>

<summary>2020-06-23 14:47:41 - Domain Adaptation for Semantic Parsing</summary>

- *Zechang Li, Yuxuan Lai, Yansong Feng, Dongyan Zhao*

- `2006.13071v1` - [abs](http://arxiv.org/abs/2006.13071v1) - [pdf](http://arxiv.org/pdf/2006.13071v1)

> Recently, semantic parsing has attracted much attention in the community. Although many neural modeling efforts have greatly improved the performance, it still suffers from the data scarcity issue. In this paper, we propose a novel semantic parser for domain adaptation, where we have much fewer annotated data in the target domain compared to the source domain. Our semantic parser benefits from a two-stage coarse-to-fine framework, thus can provide different and accurate treatments for the two stages, i.e., focusing on domain invariant and domain specific information, respectively. In the coarse stage, our novel domain discrimination component and domain relevance attention encourage the model to learn transferable domain general structures. In the fine stage, the model is guided to concentrate on domain related details. Experiments on a benchmark dataset show that our method consistently outperforms several popular domain adaptation strategies. Additionally, we show that our model can well exploit limited target data to capture the difference between the source and target domain, even when the target domain has far fewer training instances.

</details>

<details>

<summary>2020-06-23 16:55:45 - Logical Neural Networks</summary>

- *Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit Neelam, Ankita Likhyani, Santosh Srivastava*

- `2006.13155v1` - [abs](http://arxiv.org/abs/2006.13155v1) - [pdf](http://arxiv.org/pdf/2006.13155v1)

> We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.

</details>

<details>

<summary>2020-06-23 18:28:08 - Attentional Graph Convolutional Networks for Knowledge Concept Recommendation in MOOCs in a Heterogeneous View</summary>

- *Shen Wang, Jibing Gong, Jinlong Wang, Wenzheng Feng, Hao Peng, Jie Tang, Philip S. Yu*

- `2006.13257v1` - [abs](http://arxiv.org/abs/2006.13257v1) - [pdf](http://arxiv.org/pdf/2006.13257v1)

> Massive open online courses are becoming a modish way for education, which provides a large-scale and open-access learning opportunity for students to grasp the knowledge. To attract students' interest, the recommendation system is applied by MOOCs providers to recommend courses to students. However, as a course usually consists of a number of video lectures, with each one covering some specific knowledge concepts, directly recommending courses overlook students'interest to some specific knowledge concepts. To fill this gap, in this paper, we study the problem of knowledge concept recommendation. We propose an end-to-end graph neural network-based approach calledAttentionalHeterogeneous Graph Convolutional Deep Knowledge Recommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other recommendation problems, it suffers from sparsity issues. To address this issue, we leverage both content information and context information to learn the representation of entities via graph convolution network. In addition to students and knowledge concepts, we consider other types of entities (e.g., courses, videos, teachers) and construct a heterogeneous information network to capture the corresponding fruitful semantic relationships among different types of entities and incorporate them into the representation learning process. Specifically, we use meta-path on the HIN to guide the propagation of students' preferences. With the help of these meta-paths, the students' preference distribution with respect to a candidate knowledge concept can be captured. Furthermore, we propose an attention mechanism to adaptively fuse the context information from different meta-paths, in order to capture the different interests of different students. The promising experiment results show that the proposedACKRecis able to effectively recommend knowledge concepts to students pursuing online learning in MOOCs.

</details>

<details>

<summary>2020-06-24 02:47:40 - DINGO: an ontology for projects and grants linked data</summary>

- *Diego Chialva, Alexis-Michel Mugabushaka*

- `2006.13438v1` - [abs](http://arxiv.org/abs/2006.13438v1) - [pdf](http://arxiv.org/pdf/2006.13438v1)

> We present DINGO (Data INtegration for Grants Ontology), an ontology that provides a machine readable extensible framework to model data for semantically-enabled applications relative to projects, funding, actors, and, notably, funding policies in the research landscape. DINGO is designed to yield high modeling power and elasticity to cope with the huge variety in funding, research and policy practices, which makes it applicable also to other areas besides research where funding is an important aspect. We discuss its main features, the principles followed for its development, its community uptake, its maintenance and evolution.

</details>

<details>

<summary>2020-06-24 04:05:48 - DeepMnemonic: Password Mnemonic Generation via Deep Attentive Encoder-Decoder Model</summary>

- *Yao Cheng, Chang Xu, Zhen Hai, Yingjiu Li*

- `2006.13462v1` - [abs](http://arxiv.org/abs/2006.13462v1) - [pdf](http://arxiv.org/pdf/2006.13462v1)

> Strong passwords are fundamental to the security of password-based user authentication systems. In recent years, much effort has been made to evaluate password strength or to generate strong passwords. Unfortunately, the usability or memorability of the strong passwords has been largely neglected. In this paper, we aim to bridge the gap between strong password generation and the usability of strong passwords. We propose to automatically generate textual password mnemonics, i.e., natural language sentences, which are intended to help users better memorize passwords. We introduce \textit{DeepMnemonic}, a deep attentive encoder-decoder framework which takes a password as input and then automatically generates a mnemonic sentence for the password. We conduct extensive experiments to evaluate DeepMnemonic on the real-world data sets. The experimental results demonstrate that DeepMnemonic outperforms a well-known baseline for generating semantically meaningful mnemonic sentences. Moreover, the user study further validates that the generated mnemonic sentences by DeepMnemonic are useful in helping users memorize strong passwords.

</details>

<details>

<summary>2020-06-24 04:17:40 - Face-to-Music Translation Using a Distance-Preserving Generative Adversarial Network with an Auxiliary Discriminator</summary>

- *Chelhwon Kim, Andrew Port, Mitesh Patel*

- `2006.13469v1` - [abs](http://arxiv.org/abs/2006.13469v1) - [pdf](http://arxiv.org/pdf/2006.13469v1)

> Learning a mapping between two unrelated domains-such as image and audio, without any supervision is a challenging task. In this work, we propose a distance-preserving generative adversarial model to translate images of human faces into an audio domain. The audio domain is defined by a collection of musical note sounds recorded by 10 different instrument families (NSynth \cite{nsynth2017}) and a distance metric where the instrument family class information is incorporated together with a mel-frequency cepstral coefficients (MFCCs) feature. To enforce distance-preservation, a loss term that penalizes difference between pairwise distances of the faces and the translated audio samples is used. Further, we discover that the distance preservation constraint in the generative adversarial model leads to reduced diversity in the translated audio samples, and propose the use of an auxiliary discriminator to enhance the diversity of the translations while using the distance preservation constraint. We also provide a visual demonstration of the results and numerical analysis of the fidelity of the translations. A video demo of our proposed model's learned translation is available in https://www.dropbox.com/s/the176w9obq8465/face_to_musical_note.mov?dl=0.

</details>

<details>

<summary>2020-06-24 06:34:25 - On Analyzing Annotation Consistency in Online Abusive Behavior Datasets</summary>

- *Md Rabiul Awal, Rui Cao, Roy Ka-Wei Lee, Sandra Mitrović*

- `2006.13507v1` - [abs](http://arxiv.org/abs/2006.13507v1) - [pdf](http://arxiv.org/pdf/2006.13507v1)

> Online abusive behavior is an important issue that breaks the cohesiveness of online social communities and even raises public safety concerns in our societies. Motivated by this rising issue, researchers have proposed, collected, and annotated online abusive content datasets. These datasets play a critical role in facilitating the research on online hate speech and abusive behaviors. However, the annotation of such datasets is a difficult task; it is often contentious on what should be the true label of a given text as the semantic difference of the labels may be blurred (e.g., abusive and hate) and often subjective. In this study, we proposed an analytical framework to study the annotation consistency in online hate and abusive content datasets. We applied our proposed framework to evaluate the consistency of the annotation in three popular datasets that are widely used in online hate speech and abusive behavior studies. We found that there is still a substantial amount of annotation inconsistency in the existing datasets, particularly when the labels are semantically similar.

</details>

<details>

<summary>2020-06-24 08:51:02 - Event Representation Learning Enhanced with External Commonsense Knowledge</summary>

- *Xiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, Junwen Duan*

- `1909.05190v2` - [abs](http://arxiv.org/abs/1909.05190v2) - [pdf](http://arxiv.org/pdf/1909.05190v2)

> Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as script event prediction. On the other hand, events extracted from raw texts lacks of commonsense knowledge, such as the intents and emotions of the event participants, which are useful for distinguishing event pairs when there are only subtle differences in their surface realizations. To address this issue, this paper proposes to leverage external commonsense knowledge about the intent and sentiment of the event. Experiments on three event-related tasks, i.e., event similarity, script event prediction and stock market prediction, show that our model obtains much better event embeddings for the tasks, achieving 78% improvements on hard similarity task, yielding more precise inferences on subsequent events under given contexts, and better accuracies in predicting the volatilities of the stock market.

</details>

<details>

<summary>2020-06-24 12:12:14 - Unsupervised Discovery of Interpretable Directions in the GAN Latent Space</summary>

- *Andrey Voynov, Artem Babenko*

- `2002.03754v3` - [abs](http://arxiv.org/abs/2002.03754v3) - [pdf](http://arxiv.org/pdf/2002.03754v3)

> The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection.

</details>

<details>

<summary>2020-06-24 17:29:26 - Automated Chest CT Image Segmentation of COVID-19 Lung Infection based on 3D U-Net</summary>

- *Dominik Müller, Iñaki Soto Rey, Frank Kramer*

- `2007.04774v1` - [abs](http://arxiv.org/abs/2007.04774v1) - [pdf](http://arxiv.org/pdf/2007.04774v1)

> The coronavirus disease 2019 (COVID-19) affects billions of lives around the world and has a significant impact on public healthcare. Due to rising skepticism towards the sensitivity of RT-PCR as screening method, medical imaging like computed tomography offers great potential as alternative. For this reason, automated image segmentation is highly desired as clinical decision support for quantitative assessment and disease monitoring. However, publicly available COVID-19 imaging data is limited which leads to overfitting of traditional approaches. To address this problem, we propose an innovative automated segmentation pipeline for COVID-19 infected regions, which is able to handle small datasets by utilization as variant databases. Our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation. For further reduction of the overfitting risk, we implemented a standard 3D U-Net architecture instead of new or computational complex neural network architectures. Through a 5-fold cross-validation on 20 CT scans of COVID-19 patients, we were able to develop a highly accurate as well as robust segmentation model for lungs and COVID-19 infected regions without overfitting on the limited data. Our method achieved Dice similarity coefficients of 0.956 for lungs and 0.761 for infection. We demonstrated that the proposed method outperforms related approaches, advances the state-of-the-art for COVID-19 segmentation and improves medical image analysis with limited data. The code and model are available under the following link: https://github.com/frankkramer-lab/covid19.MIScnn

</details>

<details>

<summary>2020-06-24 17:49:05 - Improving task-specific representation via 1M unlabelled images without any extra knowledge</summary>

- *Aayush Bansal*

- `2006.13919v1` - [abs](http://arxiv.org/abs/2006.13919v1) - [pdf](http://arxiv.org/pdf/2006.13919v1)

> We present a case-study to improve the task-specific representation by leveraging a million unlabelled images without any extra knowledge. We propose an exceedingly simple method of conditioning an existing representation on a diverse data distribution and observe that a model trained on diverse examples acts as a better initialization. We extensively study our findings for the task of surface normal estimation and semantic segmentation from a single image. We improve surface normal estimation on NYU-v2 depth dataset and semantic segmentation on PASCAL VOC by 4% over base model. We did not use any task-specific knowledge or auxiliary tasks, neither changed hyper-parameters nor made any modification in the underlying neural network architecture.

</details>

<details>

<summary>2020-06-24 17:58:19 - Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity</summary>

- *Nina Poerner, Ulli Waltinger, Hinrich Schütze*

- `1911.03700v3` - [abs](http://arxiv.org/abs/1911.03700v3) - [pdf](http://arxiv.org/pdf/1911.03700v3)

> We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Sch\"utze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson's r over single-source systems.

</details>

<details>

<summary>2020-06-25 05:17:03 - IQA: Interactive Query Construction in Semantic Question Answering Systems</summary>

- *Hamid Zafar, Mohnish Dubey, Jens Lehmann, Elena Demidova*

- `2006.11534v3` - [abs](http://arxiv.org/abs/2006.11534v3) - [pdf](http://arxiv.org/pdf/2006.11534v3)

> Semantic Question Answering (SQA) systems automatically interpret user questions expressed in a natural language in terms of semantic queries. This process involves uncertainty, such that the resulting queries do not always accurately match the user intent, especially for more complex and less common questions. In this article, we aim to empower users in guiding SQA systems towards the intended semantic queries through interaction. We introduce IQA - an interaction scheme for SQA pipelines. This scheme facilitates seamless integration of user feedback in the question answering process and relies on Option Gain - a novel metric that enables efficient and intuitive user interaction. Our evaluation shows that using the proposed scheme, even a small number of user interactions can lead to significant improvements in the performance of SQA systems.

</details>

<details>

<summary>2020-06-25 07:42:58 - Semantic Understanding of Foggy Scenes with Purely Synthetic Data</summary>

- *Martin Hahner, Dengxin Dai, Christos Sakaridis, Jan-Nico Zaech, Luc Van Gool*

- `1910.03997v2` - [abs](http://arxiv.org/abs/1910.03997v2) - [pdf](http://arxiv.org/pdf/1910.03997v2)

> This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen real-world foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data.

</details>

<details>

<summary>2020-06-25 08:13:32 - A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation</summary>

- *Jan Deriu, Katsiaryna Mlynchyk, Philippe Schläpfer, Alvaro Rodrigo, Dirk von Grünigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, Mark Cieliebak*

- `2004.07633v2` - [abs](http://arxiv.org/abs/2004.07633v2) - [pdf](http://arxiv.org/pdf/2004.07633v2)

> In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database called Operation Trees (OT). This representation allows us to invert the annotation process without losing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of query tokens to OT operations. In our method, we randomly generate OTs from a context-free grammar. Afterwards, annotators have to write the appropriate natural language question that is represented by the OT. Finally, the annotators assign the tokens to the OT operations. We apply the method to create a new corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases. We compare OTTA to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our corpus is a challenging dataset and that the token alignment can be leveraged to increase the performance significantly.

</details>

<details>

<summary>2020-06-25 09:15:42 - LSBert: A Simple Framework for Lexical Simplification</summary>

- *Jipeng Qiang, Yun Li, Yi Zhu, Yunhao Yuan, Xindong Wu*

- `2006.14939v1` - [abs](http://arxiv.org/abs/2006.14939v1) - [pdf](http://arxiv.org/pdf/2006.14939v1)

> Lexical simplification (LS) aims to replace complex words in a given sentence with their simpler alternatives of equivalent meaning, to simplify the sentence. Recently unsupervised lexical simplification approaches only rely on the complex word itself regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious candidates. In this paper, we propose a lexical simplification framework LSBert based on pretrained representation model Bert, that is capable of (1) making use of the wider context when both detecting the words in need of simplification and generating substitue candidates, and (2) taking five high-quality features into account for ranking candidates, including Bert prediction order, Bert-based language model, and the paraphrase database PPDB, in addition to the word frequency and word similarity commonly used in other LS methods. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and obtains obvious improvement compared with these baselines, outperforming the state-of-the-art by 29.8 Accuracy points on three well-known benchmarks.

</details>

<details>

<summary>2020-06-25 14:19:41 - Plausible Reasoning about EL-Ontologies using Concept Interpolation</summary>

- *Yazmín Ibáñez-García, Víctor Gutiérrez-Basulto, Steven Schockaert*

- `2006.14437v1` - [abs](http://arxiv.org/abs/2006.14437v1) - [pdf](http://arxiv.org/pdf/2006.14437v1)

> Description logics (DLs) are standard knowledge representation languages for modelling ontologies, i.e. knowledge about concepts and the relations between them. Unfortunately, DL ontologies are difficult to learn from data and time-consuming to encode manually. As a result, ontologies for broad domains are almost inevitably incomplete. In recent years, several data-driven approaches have been proposed for automatically extending such ontologies. One family of methods rely on characterizations of concepts that are derived from text descriptions. While such characterizations do not capture ontological knowledge directly, they encode information about the similarity between different concepts, which can be exploited for filling in the gaps in existing ontologies. To this end, several inductive inference mechanisms have already been proposed, but these have been defined and used in a heuristic fashion. In this paper, we instead propose an inductive inference mechanism which is based on a clear model-theoretic semantics, and can thus be tightly integrated with standard deductive reasoning. We particularly focus on interpolation, a powerful commonsense reasoning mechanism which is closely related to cognitive models of category-based induction. Apart from the formalization of the underlying semantics, as our main technical contribution we provide computational complexity bounds for reasoning in EL with this interpolation mechanism.

</details>

<details>

<summary>2020-06-25 16:43:51 - Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks (Extended Version)</summary>

- *Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, Ronghui Gu*

- `2003.07959v4` - [abs](http://arxiv.org/abs/2003.07959v4) - [pdf](http://arxiv.org/pdf/2003.07959v4)

> Verifying real-world programs often requires inferring loop invariants with nonlinear constraints. This is especially true in programs that perform many numerical operations, such as control systems for avionics or industrial plants. Recently, data-driven methods for loop invariant inference have shown promise, especially on linear invariants. However, applying data-driven inference to nonlinear loop invariants is challenging due to the large numbers of and magnitudes of high-order terms, the potential for overfitting on a small number of samples, and the large space of possible inequality bounds.   In this paper, we introduce a new neural architecture for general SMT learning, the Gated Continuous Logic Network (G-CLN), and apply it to nonlinear loop invariant learning. G-CLNs extend the Continuous Logic Network (CLN) architecture with gating units and dropout, which allow the model to robustly learn general invariants over large numbers of terms. To address overfitting that arises from finite program sampling, we introduce fractional sampling---a sound relaxation of loop semantics to continuous functions that facilitates unbounded sampling on real domain. We additionally design a new CLN activation function, the Piecewise Biased Quadratic Unit (PBQU), for naturally learning tight inequality bounds.   We incorporate these methods into a nonlinear loop invariant inference system that can learn general nonlinear loop invariants. We evaluate our system on a benchmark of nonlinear loop invariants and show it solves 26 out of 27 problems, 3 more than prior work, with an average runtime of 53.3 seconds. We further demonstrate the generic learning ability of G-CLNs by solving all 124 problems in the linear Code2Inv benchmark. We also perform a quantitative stability evaluation and show G-CLNs have a convergence rate of $97.5\%$ on quadratic problems, a $39.2\%$ improvement over CLN models.

</details>

<details>

<summary>2020-06-25 19:20:07 - LPar -- A Distributed Multi Agent platform for building Polyglot, Omni Channel and Industrial grade Natural Language Interfaces</summary>

- *Pranav Sharma*

- `2006.14666v1` - [abs](http://arxiv.org/abs/2006.14666v1) - [pdf](http://arxiv.org/pdf/2006.14666v1)

> The goal of serving and delighting customers in a personal and near human like manner is very high on automation agendas of most Enterprises. Last few years, have seen huge progress in Natural Language Processing domain which has led to deployments of conversational agents in many enterprises. Most of the current industrial deployments tend to use Monolithic Single Agent designs that model the entire knowledge and skill of the Domain. While this approach is one of the fastest to market, the monolithic design makes it very hard to scale beyond a point. There are also challenges in seamlessly leveraging many tools offered by sub fields of Natural Language Processing and Information Retrieval in a single solution. The sub fields that can be leveraged to provide relevant information are, Question and Answer system, Abstractive Summarization, Semantic Search, Knowledge Graph etc. Current deployments also tend to be very dependent on the underlying Conversational AI platform (open source or commercial) , which is a challenge as this is a fast evolving space and no one platform can be considered future proof even in medium term of 3-4 years. Lately,there is also work done to build multi agent solutions that tend to leverage a concept of master agent. While this has shown promise, this approach still makes the master agent in itself difficult to scale. To address these challenges, we introduce LPar, a distributed multi agent platform for large scale industrial deployment of polyglot, diverse and inter-operable agents. The asynchronous design of LPar supports dynamically expandable domain. We also introduce multiple strategies available in the LPar system to elect the most suitable agent to service a customer query.

</details>

<details>

<summary>2020-06-25 19:40:20 - Fully Convolutional Open Set Segmentation</summary>

- *Hugo Oliveira, Caio Silva, Gabriel L. S. Machado, Keiller Nogueira, Jefersson A. dos Santos*

- `2006.14673v1` - [abs](http://arxiv.org/abs/2006.14673v1) - [pdf](http://arxiv.org/pdf/2006.14673v1)

> In semantic segmentation knowing about all existing classes is essential to yield effective results with the majority of existing approaches. However, these methods trained in a Closed Set of classes fail when new classes are found in the test phase. It means that they are not suitable for Open Set scenarios, which are very common in real-world computer vision and remote sensing applications. In this paper, we discuss the limitations of Closed Set segmentation and propose two fully convolutional approaches to effectively address Open Set semantic segmentation: OpenFCN and OpenPCS. OpenFCN is based on the well-known OpenMax algorithm, configuring a new application of this approach in segmentation settings. OpenPCS is a fully novel approach based on feature-space from DNN activations that serve as features for computing PCA and multi-variate gaussian likelihood in a lower dimensional space. Experiments were conducted on the well-known Vaihingen and Potsdam segmentation datasets. OpenFCN showed little-to-no improvement when compared to the simpler and much more time efficient SoftMax thresholding, while being between some orders of magnitude slower. OpenPCS achieved promising results in almost all experiments by overcoming both OpenFCN and SoftMax thresholding. OpenPCS is also a reasonable compromise between the runtime performances of the extremely fast SoftMax thresholding and the extremely slow OpenFCN, being close able to run close to real-time. Experiments also indicate that OpenPCS is effective, robust and suitable for Open Set segmentation, being able to improve the recognition of unknown class pixels without reducing the accuracy on the known class pixels.

</details>

<details>

<summary>2020-06-25 21:10:04 - Deep Q-Network-Driven Catheter Segmentation in 3D US by Hybrid Constrained Semi-Supervised Learning and Dual-UNet</summary>

- *Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With*

- `2006.14702v1` - [abs](http://arxiv.org/abs/2006.14702v1) - [pdf](http://arxiv.org/pdf/2006.14702v1)

> Catheter segmentation in 3D ultrasound is important for computer-assisted cardiac intervention. However, a large amount of labeled images are required to train a successful deep convolutional neural network (CNN) to segment the catheter, which is expensive and time-consuming. In this paper, we propose a novel catheter segmentation approach, which requests fewer annotations than the supervised learning method, but nevertheless achieves better performance. Our scheme considers a deep Q learning as the pre-localization step, which avoids voxel-level annotation and which can efficiently localize the target catheter. With the detected catheter, patch-based Dual-UNet is applied to segment the catheter in 3D volumetric data. To train the Dual-UNet with limited labeled images and leverage information of unlabeled images, we propose a novel semi-supervised scheme, which exploits unlabeled images based on hybrid constraints from predictions. Experiments show the proposed scheme achieves a higher performance than state-of-the-art semi-supervised methods, while it demonstrates that our method is able to learn from large-scale unlabeled images.

</details>

<details>

<summary>2020-06-25 23:36:28 - Differentiable Product Quantization for End-to-End Embedding Compression</summary>

- *Ting Chen, Lala Li, Yizhou Sun*

- `1908.09756v3` - [abs](http://arxiv.org/abs/1908.09756v3) - [pdf](http://arxiv.org/pdf/1908.09756v3)

> Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238$\times$) at negligible or no performance cost on 10 datasets across three different language tasks.

</details>

<details>

<summary>2020-06-26 00:18:10 - Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin using Recursive Neural Networks</summary>

- *Minh Nguyen, Gia H. Ngo, Nancy F. Chen*

- `1912.09913v2` - [abs](http://arxiv.org/abs/1912.09913v2) - [pdf](http://arxiv.org/pdf/1912.09913v2)

> Logographs (Chinese characters) have recursive structures (i.e. hierarchies of sub-units in logographs) that contain phonological and semantic information, as developmental psychology literature suggests that native speakers leverage on the structures to learn how to read. Exploiting these structures could potentially lead to better embeddings that can benefit many downstream tasks. We propose building hierarchical logograph (character) embeddings from logograph recursive structures using treeLSTM, a recursive neural network. Using recursive neural network imposes a prior on the mapping from logographs to embeddings since the network must read in the sub-units in logographs according to the order specified by the recursive structures. Based on human behavior in language learning and reading, we hypothesize that modeling logographs' structures using recursive neural network should be beneficial. To verify this claim, we consider two tasks (1) predicting logographs' Cantonese pronunciation from logographic structures and (2) language modeling. Empirical results show that the proposed hierarchical embeddings outperform baseline approaches. Diagnostic analysis suggests that hierarchical embeddings constructed using treeLSTM is less sensitive to distractors, thus is more robust, especially on complex logographs.

</details>

<details>

<summary>2020-06-26 07:42:09 - A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning</summary>

- *Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, Zhi Jin*

- `1909.06983v3` - [abs](http://arxiv.org/abs/1909.06983v3) - [pdf](http://arxiv.org/pdf/1909.06983v3)

> Code completion, one of the most useful features in the Integrated Development Environments (IDEs), can accelerate software development by suggesting the libraries, APIs, and method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program's representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. To address these challenges, in this paper, we propose a self-attentional neural architecture for code completion with multi-task learning. To utilize the hierarchical structural information of the programs, we present a novel method that considers the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable the knowledge sharing between related tasks, we creatively propose a Multi-Task Learning (MTL) framework to learn two related tasks in code completion jointly. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.

</details>

<details>

<summary>2020-06-26 15:29:28 - Deep Networks as Logical Circuits: Generalization and Interpretation</summary>

- *Christopher Snyder, Sriram Vishwanath*

- `2003.11619v2` - [abs](http://arxiv.org/abs/2003.11619v2) - [pdf](http://arxiv.org/pdf/2003.11619v2)

> Not only are Deep Neural Networks (DNNs) black box models, but also we frequently conceptualize them as such. We lack good interpretations of the mechanisms linking inputs to outputs. Therefore, we find it difficult to analyze in human-meaningful terms (1) what the network learned and (2) whether the network learned. We present a hierarchical decomposition of the DNN discrete classification map into logical (AND/OR) combinations of intermediate (True/False) classifiers of the input. Those classifiers that can not be further decomposed, called atoms, are (interpretable) linear classifiers. Taken together, we obtain a logical circuit with linear classifier inputs that computes the same label as the DNN. This circuit does not structurally resemble the network architecture, and it may require many fewer parameters, depending on the configuration of weights. In these cases, we obtain simultaneously an interpretation and generalization bound (for the original DNN), connecting two fronts which have historically been investigated separately. Unlike compression techniques, our representation is. We motivate the utility of this perspective by studying DNNs in simple, controlled settings, where we obtain superior generalization bounds despite using only combinatorial information (e.g. no margin information). We demonstrate how to "open the black box" on the MNIST dataset. We show that the learned, internal, logical computations correspond to semantically meaningful (unlabeled) categories that allow DNN descriptions in plain English. We improve the generalization of an already trained network by interpreting, diagnosing, and replacing components the logical circuit that is the DNN.

</details>

<details>

<summary>2020-06-26 16:38:28 - CrackGAN: Pavement Crack Detection Using Partially Accurate Ground Truths Based on Generative Adversarial Learning</summary>

- *Kaige Zhang, Yingtao Zhang, Heng-Da Cheng*

- `1909.08216v2` - [abs](http://arxiv.org/abs/1909.08216v2) - [pdf](http://arxiv.org/pdf/1909.08216v2)

> Fully convolutional network is a powerful tool for per-pixel semantic segmentation/detection. However, it is problematic when coping with crack detection using partially accurate ground truths (GTs): the network may easily converge to the status that treats all the pixels as background (BG) and still achieves a very good loss, named "All Black" phenomenon, due to the unavailability of accurate GTs and the data imbalance. To tackle this problem, we propose crack-patch-only (CPO) supervised generative adversarial learning for end-to-end training, which forces the network to always produce crack-GT images while reserves both crack and BG-image translation abilities by feeding a larger-size crack image into an asymmetric U-shape generator to overcome the "All Black" issue. The proposed approach is validated using four crack datasets; and achieves state-of-the-art performance comparing with that of the recently published works in efficiency and accuracy.

</details>

<details>

<summary>2020-06-27 00:24:30 - Linking Points With Labels in 3D: A Review of Point Cloud Semantic Segmentation</summary>

- *Yuxing Xie, Jiaojiao Tian, Xiao Xiang Zhu*

- `1908.08854v3` - [abs](http://arxiv.org/abs/1908.08854v3) - [pdf](http://arxiv.org/pdf/1908.08854v3)

> 3D Point Cloud Semantic Segmentation (PCSS) is attracting increasing interest, due to its applicability in remote sensing, computer vision and robotics, and due to the new possibilities offered by deep learning techniques. In order to provide a needed up-to-date review of recent developments in PCSS, this article summarizes existing studies on this topic. Firstly, we outline the acquisition and evolution of the 3D point cloud from the perspective of remote sensing and computer vision, as well as the published benchmarks for PCSS studies. Then, traditional and advanced techniques used for Point Cloud Segmentation (PCS) and PCSS are reviewed and compared. Finally, important issues and open questions in PCSS studies are discussed.

</details>

<details>

<summary>2020-06-27 03:57:48 - XI Commandments of Kubernetes Security: A Systematization of Knowledge Related to Kubernetes Security Practices</summary>

- *Md. Shazibul Islam Shamim, Farzana Ahamed Bhuiyan, Akond Rahman*

- `2006.15275v1` - [abs](http://arxiv.org/abs/2006.15275v1) - [pdf](http://arxiv.org/pdf/2006.15275v1)

> Kubernetes is an open-source software for automating management of computerized services. Organizations, such as IBM, Capital One and Adidas use Kubernetes to deploy and manage their containers, and have reported benefits related to deployment frequency. Despite reported benefits, Kubernetes deployments are susceptible to security vulnerabilities, such as those that occurred at Tesla in 2018. A systematization of Kubernetes security practices can help practitioners mitigate vulnerabilities in their Kubernetes deployments. The goal of this paper is to help practitioners in securing their Kubernetes installations through a systematization of knowledge related to Kubernetes security practices. We systematize knowledge by applying qualitative analysis on 104 Internet artifacts. We identify 11 security practices that include (i) implementation of role-based access control (RBAC) authorization to provide least privilege, (ii) applying security patches to keep Kubernetes updated, and (iii) implementing pod and network specific security policies.

</details>

<details>

<summary>2020-06-27 08:24:26 - Video-Grounded Dialogues with Pretrained Generation Language Models</summary>

- *Hung Le, Steven C. H. Hoi*

- `2006.15319v1` - [abs](http://arxiv.org/abs/2006.15319v1) - [pdf](http://arxiv.org/pdf/2006.15319v1)

> Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.

</details>

<details>

<summary>2020-06-27 20:12:33 - GPT-GNN: Generative Pre-Training of Graph Neural Networks</summary>

- *Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun*

- `2006.15437v1` - [abs](http://arxiv.org/abs/2006.15437v1) - [pdf](http://arxiv.org/pdf/2006.15437v1)

> Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.

</details>

<details>

<summary>2020-06-27 21:51:38 - A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with Bilingual Semantic Similarity Rewards</summary>

- *Zi-Yi Dou, Sachin Kumar, Yulia Tsvetkov*

- `2006.15454v1` - [abs](http://arxiv.org/abs/2006.15454v1) - [pdf](http://arxiv.org/pdf/2006.15454v1)

> Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English--Chinese and English--German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines.

</details>

<details>

<summary>2020-06-27 22:22:58 - Recurrent Hierarchical Topic-Guided RNN for Language Generation</summary>

- *Dandan Guo, Bo Chen, Ruiying Lu, Mingyuan Zhou*

- `1912.10337v2` - [abs](http://arxiv.org/abs/1912.10337v2) - [pdf](http://arxiv.org/pdf/1912.10337v2)

> To simultaneously capture syntax and global semantics from a text corpus, we propose a new larger-context recurrent neural network (RNN) based language model, which extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation. Moving beyond a conventional RNN-based language model that ignores long-range word dependencies and sentence order, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence topic dependencies. For inference, we develop a hybrid of stochastic-gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes. Experimental results on a variety of real-world text corpora demonstrate that the proposed model not only outperforms larger-context RNN-based language models, but also learns interpretable recurrent multilayer topics and generates diverse sentences and paragraphs that are syntactically correct and semantically coherent.

</details>

<details>

<summary>2020-06-28 14:39:20 - A Survey on Instance Segmentation: State of the art</summary>

- *Abdul Mueed Hafiz, Ghulam Mohiuddin Bhat*

- `2007.00047v1` - [abs](http://arxiv.org/abs/2007.00047v1) - [pdf](http://arxiv.org/pdf/2007.00047v1)

> Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation -- its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.

</details>

<details>

<summary>2020-06-28 19:59:48 - A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer's Type</summary>

- *Trevor Cohen, Serguei Pakhomov*

- `2005.03593v2` - [abs](http://arxiv.org/abs/2005.03593v2) - [pdf](http://arxiv.org/pdf/2005.03593v2)

> In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls. The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and the other trained on transcripts from patients with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance. However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables. In this paper, we interrogate neural LMs trained on participants with and without dementia using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency. We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.

</details>

<details>

<summary>2020-06-28 23:47:04 - Modeling Label Semantics for Predicting Emotional Reactions</summary>

- *Radhika Gaonkar, Heeyoung Kwon, Mohaddeseh Bastan, Niranjan Balasubramanian, Nathanael Chambers*

- `2006.05489v2` - [abs](http://arxiv.org/abs/2006.05489v2) - [pdf](http://arxiv.org/pdf/2006.05489v2)

> Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. They ignore information that may be conveyed by the emotion labels themselves. We propose that the semantics of emotion labels can guide a model's attention when representing the input story. Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.

</details>

<details>

<summary>2020-06-29 07:56:22 - A Framework for Pre-processing of Social Media Feeds based on Integrated Local Knowledge Base</summary>

- *Taiwo Kolajo, Olawande Daramola, Ayodele Adebiyi, Seth Aaditeshwar*

- `2006.15854v1` - [abs](http://arxiv.org/abs/2006.15854v1) - [pdf](http://arxiv.org/pdf/2006.15854v1)

> Most of the previous studies on the semantic analysis of social media feeds have not considered the issue of ambiguity that is associated with slangs, abbreviations, and acronyms that are embedded in social media posts. These noisy terms have implicit meanings and form part of the rich semantic context that must be analysed to gain complete insights from social media feeds. This paper proposes an improved framework for pre-processing of social media feeds for better performance. To do this, the use of an integrated knowledge base (ikb) which comprises a local knowledge source (Naijalingo), urban dictionary and internet slang was combined with the adapted Lesk algorithm to facilitate semantic analysis of social media feeds. Experimental results showed that the proposed approach performed better than existing methods when it was tested on three machine learning models, which are support vector machines, multilayer perceptron, and convolutional neural networks. The framework had an accuracy of 94.07% on a standardized dataset, and 99.78% on localised dataset when used to extract sentiments from tweets. The improved performance on the localised dataset reveals the advantage of integrating the use of local knowledge sources into the process of analysing social media feeds particularly in interpreting slangs/acronyms/abbreviations that have contextually rooted meanings.

</details>

<details>

<summary>2020-06-29 16:37:51 - Multichannel CNN with Attention for Text Classification</summary>

- *Zhenyu Liu, Haiwei Huang, Chaohong Lu, Shengfei Lyu*

- `2006.16174v1` - [abs](http://arxiv.org/abs/2006.16174v1) - [pdf](http://arxiv.org/pdf/2006.16174v1)

> Recent years, the approaches based on neural networks have shown remarkable potential for sentence modeling. There are two main neural network structures: recurrent neural network (RNN) and convolution neural network (CNN). RNN can capture long term dependencies and store the semantics of the previous information in a fixed-sized vector. However, RNN is a biased model and its ability to extract global semantics is restricted by the fixed-sized vector. Alternatively, CNN is able to capture n-gram features of texts by utilizing convolutional filters. But the width of convolutional filters restricts its performance. In order to combine the strengths of the two kinds of networks and alleviate their shortcomings, this paper proposes Attention-based Multichannel Convolutional Neural Network (AMCNN) for text classification. AMCNN utilizes a bi-directional long short-term memory to encode the history and future information of words into high dimensional representations, so that the information of both the front and back of the sentence can be fully expressed. Then the scalar attention and vectorial attention are applied to obtain multichannel representations. The scalar attention can calculate the word-level importance and the vectorial attention can calculate the feature-level importance. In the classification task, AMCNN uses a CNN structure to cpture word relations on the representations generated by the scalar and vectorial attention mechanism instead of calculating the weighted sums. It can effectively extract the n-gram features of the text. The experimental results on the benchmark datasets demonstrate that AMCNN achieves better performance than state-of-the-art methods. In addition, the visualization results verify the semantic richness of multichannel representations.

</details>

<details>

<summary>2020-06-29 22:57:42 - Empower Entity Set Expansion via Language Model Probing</summary>

- *Yunyi Zhang, Jiaming Shen, Jingbo Shang, Jiawei Han*

- `2004.13897v2` - [abs](http://arxiv.org/abs/2004.13897v2) - [pdf](http://arxiv.org/pdf/2004.13897v2)

> Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction. Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations. In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue. In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names. Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.

</details>

<details>

<summary>2020-06-30 00:45:59 - Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints</summary>

- *Mengtian Li, Ersin Yumer, Deva Ramanan*

- `1905.04753v4` - [abs](http://arxiv.org/abs/1905.04753v4) - [pdf](http://arxiv.org/pdf/1905.04753v4)

> In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: "given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.

</details>

<details>

<summary>2020-06-30 01:58:27 - Decomposing Word Embedding with the Capsule Network</summary>

- *Xin Liu, Qingcai Chen, Yan Liu, Joanna Siebert, Baotian Hu, Xiangping Wu, Buzhou Tang*

- `2004.13844v2` - [abs](http://arxiv.org/abs/2004.13844v2) - [pdf](http://arxiv.org/pdf/2004.13844v2)

> Word sense disambiguation tries to learn the appropriate sense of an ambiguous word in a given context. The existing pre-trained language methods and the methods based on multi-embeddings of word did not explore the power of the unsupervised word embedding sufficiently.   In this paper, we discuss a capsule network-based approach, taking advantage of capsule's potential for recognizing highly overlapping features and dealing with segmentation. We propose a Capsule network-based method to Decompose the unsupervised word Embedding of an ambiguous word into context specific Sense embedding, called CapsDecE2S. In this approach, the unsupervised ambiguous embedding is fed into capsule network to produce its multiple morpheme-like vectors, which are defined as the basic semantic language units of meaning. With attention operations, CapsDecE2S integrates the word context to reconstruct the multiple morpheme-like vectors into the context-specific sense embedding. To train CapsDecE2S, we propose a sense matching training method. In this method, we convert the sense learning into a binary classification that explicitly learns the relation between senses by the label of matching and non-matching. The CapsDecE2S was experimentally evaluated on two sense learning tasks, i.e., word in context and word sense disambiguation. Results on two public corpora Word-in-Context and English all-words Word Sense Disambiguation show that, the CapsDecE2S model achieves the new state-of-the-art for the word in context and word sense disambiguation tasks.

</details>

<details>

<summary>2020-06-30 02:16:21 - Invariant Diffs</summary>

- *Ashwin Kallingal Joshy, Wei Le*

- `1911.07988v2` - [abs](http://arxiv.org/abs/1911.07988v2) - [pdf](http://arxiv.org/pdf/1911.07988v2)

> Software development is inherently incremental. Nowadays, many software companies adopt an agile process and a shorter release cycle, where software needs to be delivered faster with quality assurances. On the other hand, the majority of existing program analysis tools still target single versions of programs and are slow and inflexible to handle changes. In the popular version control systems such as git, the program changes are still presented using source code diffs. It is hard to understand what program conditions are changed and which source code lines cause them. In this paper, we propose to compute "invariant diffs" to specify changes. Similar to source diffs that report common code and code churns, we define version invariants to represent program conditions that are common across versions, and invariant churns to show the changes of program conditions between versions. We designed a static demand-driven, path-sensitive analysis to compute and compare invariants for multiple versions of programs using multiversion control flow graphs. We report invariant diffs at the matched program points where comparing invariants are meaningful. Importantly, our analysis correlates source diffs with invariant diffs to explain what source code changes lead to the property changes. We implemented our algorithms in a tool called $H_2$ and performed experiments on 104 versions of programs. Our results show that we are able to compute invariant diffs correctly within reasonable amount of time. The version invariants can capture the common properties of program versions even constructed by different persons, and the invariant churns can specify the semantics of changes such as how a patch changed a buggy condition to a correct condition.

</details>

<details>

<summary>2020-06-30 09:36:21 - BitMix: Data Augmentation for Image Steganalysis</summary>

- *In-Jae Yu, Wonhyuk Ahn, Seung-Hun Nam, Heung-Kyu Lee*

- `2006.16625v1` - [abs](http://arxiv.org/abs/2006.16625v1) - [pdf](http://arxiv.org/pdf/2006.16625v1)

> Convolutional neural networks (CNN) for image steganalysis demonstrate better performances with employing concepts from high-level vision tasks. The major employed concept is to use data augmentation to avoid overfitting due to limited data. To augment data without damaging the message embedding, only rotating multiples of 90 degrees or horizontally flipping are used in steganalysis, which generates eight fixed results from one sample. To overcome this limitation, we propose BitMix, a data augmentation method for spatial image steganalysis. BitMix mixes a cover and stego image pair by swapping the random patch and generates an embedding adaptive label with the ratio of the number of pixels modified in the swapped patch to those in the cover-stego pair. We explore optimal hyperparameters, the ratio of applying BitMix in the mini-batch, and the size of the bounding box for swapping patch. The results reveal that using BitMix improves the performance of spatial image steganalysis and better than other data augmentation methods.

</details>

<details>

<summary>2020-06-30 10:46:18 - Attention-Based Neural Networks for Sentiment Attitude Extraction using Distant Supervision</summary>

- *Nicolay Rusnachenko, Natalia Loukachevitch*

- `2006.13730v2` - [abs](http://arxiv.org/abs/2006.13730v2) - [pdf](http://arxiv.org/pdf/2006.13730v2)

> In the sentiment attitude extraction task, the aim is to identify <<attitudes>> -- sentiment relations between entities mentioned in text. In this paper, we provide a study on attention-based context encoders in the sentiment attitude extraction task. For this task, we adapt attentive context encoders of two types: (1) feature-based; (2) self-based. In our study, we utilize the corpus of Russian analytical texts RuSentRel and automatically constructed news collection RuAttitudes for enriching the training set. We consider the problem of attitude extraction as two-class (positive, negative) and three-class (positive, negative, neutral) classification tasks for whole documents. Our experiments with the RuSentRel corpus show that the three-class classification models, which employ the RuAttitudes corpus for training, result in 10% increase and extra 3% by F1, when model architectures include the attention mechanism. We also provide the analysis of attention weight distributions in dependence on the term type.

</details>

<details>

<summary>2020-06-30 11:59:53 - Classification Confidence Estimation with Test-Time Data-Augmentation</summary>

- *Yuval Bahat, Gregory Shakhnarovich*

- `2006.16705v1` - [abs](http://arxiv.org/abs/2006.16705v1) - [pdf](http://arxiv.org/pdf/2006.16705v1)

> Machine learning plays an increasingly significant role in many aspects of our lives (including medicine, transportation, security, justice and other domains), making the potential consequences of false predictions increasingly devastating. These consequences may be mitigated if we can automatically flag such false predictions and potentially assign them to alternative, more reliable mechanisms, that are possibly more costly and involve human attention. This suggests the task of detecting errors, which we tackle in this paper for the case of visual classification. To this end, we propose a novel approach for classification confidence estimation. We apply a set of semantics-preserving image transformations to the input image, and show how the resulting image sets can be used to estimate confidence in the classifier's prediction. We demonstrate the potential of our approach by extensively evaluating it on a wide variety of classifier architectures and datasets, including ResNext/ImageNet, achieving state of the art performance. This paper constitutes a significant revision of our earlier work in this direction (Bahat & Shakhnarovich, 2018).

</details>

<details>

<summary>2020-06-30 15:49:12 - Ontology-guided Semantic Composition for Zero-Shot Learning</summary>

- *Jiaoyan Chen, Freddy Lecue, Yuxia Geng, Jeff Z. Pan, Huajun Chen*

- `2006.16917v1` - [abs](http://arxiv.org/abs/2006.16917v1) - [pdf](http://arxiv.org/pdf/2006.16917v1)

> Zero-shot learning (ZSL) is a popular research problem that aims at predicting for those classes that have never appeared in the training stage by utilizing the inter-class relationship with some side information. In this study, we propose to model the compositional and expressive semantics of class labels by an OWL (Web Ontology Language) ontology, and further develop a new ZSL framework with ontology embedding. The effectiveness has been verified by some primary experiments on animal image classification and visual question answering.

</details>

<details>

<summary>2020-06-30 16:23:10 - Traceability Support for Multi-Lingual Software Projects</summary>

- *Yalin Liu, Jinfeng Lin, Jane Cleland-Huang*

- `2006.16940v1` - [abs](http://arxiv.org/abs/2006.16940v1) - [pdf](http://arxiv.org/pdf/2006.16940v1)

> Software traceability establishes associations between diverse software artifacts such as requirements, design, code, and test cases. Due to the non-trivial costs of manually creating and maintaining links, many researchers have proposed automated approaches based on information retrieval techniques. However, many globally distributed software projects produce software artifacts written in two or more languages. The use of intermingled languages reduces the efficacy of automated tracing solutions. In this paper, we first analyze and discuss patterns of intermingled language use across multiple projects, and then evaluate several different tracing algorithms including the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono- and cross-lingual word embeddings with the Generative Vector Space Model (GVSM). Based on an analysis of 14 Chinese-English projects, our results show that best performance is achieved using mono-lingual word embeddings integrated into GVSM with machine translation as a preprocessing step.

</details>

<details>

<summary>2020-06-30 19:11:51 - Adversarial Mutual Information for Text Generation</summary>

- *Boyuan Pan, Yazheng Yang, Kaizhao Liang, Bhavya Kailkhura, Zhongming Jin, Xian-Sheng Hua, Deng Cai, Bo Li*

- `2007.00067v1` - [abs](http://arxiv.org/abs/2007.00067v1) - [pdf](http://arxiv.org/pdf/2007.00067v1)

> Recent advances in maximizing mutual information (MI) between the source and target have demonstrated its effectiveness in text generation. However, previous works paid little attention to modeling the backward network of MI (i.e., dependency from the target to the source), which is crucial to the tightness of the variational information maximization lower bound. In this paper, we propose Adversarial Mutual Information (AMI): a text generation framework which is formed as a novel saddle point (min-max) optimization aiming to identify joint interactions between the source and target. Within this framework, the forward and backward networks are able to iteratively promote or demote each other's generated instances by comparing the real and synthetic data distributions. We also develop a latent noise sampling strategy that leverages random variations at the high-level semantic space to enhance the long term dependency in the generation process. Extensive experiments based on different text generation tasks demonstrate that the proposed AMI framework can significantly outperform several strong baselines, and we also show that AMI has potential to lead to a tighter lower bound of maximum mutual information for the variational information maximization problem.

</details>

<details>

<summary>2020-06-30 19:36:38 - Deep Geometric Texture Synthesis</summary>

- *Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or*

- `2007.00074v1` - [abs](http://arxiv.org/abs/2007.00074v1) - [pdf](http://arxiv.org/pdf/2007.00074v1)

> Recently, deep generative adversarial networks for image generation have advanced rapidly; yet, only a small amount of research has focused on generative models for irregular structures, particularly meshes. Nonetheless, mesh generation and synthesis remains a fundamental topic in computer graphics. In this work, we propose a novel framework for synthesizing geometric textures. It learns geometric texture statistics from local neighborhoods (i.e., local triangular patches) of a single reference 3D model. It learns deep features on the faces of the input triangulation, which is used to subdivide and generate offsets across multiple scales, without parameterization of the reference or target mesh. Our network displaces mesh vertices in any direction (i.e., in the normal and tangential direction), enabling synthesis of geometric textures, which cannot be expressed by a simple 2D displacement map. Learning and synthesizing on local geometric patches enables a genus-oblivious framework, facilitating texture transfer between shapes of different genus.

</details>

<details>

<summary>2020-06-30 19:38:18 - Software Ethology: An Accurate, Resilient, and Cross-Architecture Binary Analysis Framework</summary>

- *Derrick McKee, Nathan Burow, Mathias Payer*

- `1906.02928v3` - [abs](http://arxiv.org/abs/1906.02928v3) - [pdf](http://arxiv.org/pdf/1906.02928v3)

> When reverse engineering a binary, the analyst must first understand the semantics of the binary's functions through either manual or automatic analysis. Manual semantic analysis is time-consuming, because abstractions provided by high level languages, such as type information, variable scope, or comments are lost, and past analyses cannot apply to the current analysis task. Existing automated binary analysis tools currently suffer from low accuracy in determining semantic function identification in the presence of diverse compilation environments.   We introduce Software Ethology, a binary analysis approach for determining the semantic similarity of functions. Software Ethology abstracts semantic behavior as classification vectors of program state changes resulting from a function executing with a specified input state, and uses these vectors as a unique fingerprint for identification. All existing semantic identifiers determine function similarity via code measurements, and suffer from high inaccuracy when classifying functions from compilation environments different from their ground truth source. Since Software Ethology does not rely on code measurements, its accuracy is resilient to changes in compiler, compiler version, optimization level, or even different source implementing equivalent functionality.   Tinbergen, our prototype Software Ethology implementation, leverages a virtual execution environment and a fuzzer to generate the classification vectors. In evaluating Tinbergen's feasibility as a semantic function identifier by identifying functions in coreutils-8.30, we achieve a high .805 average accuracy. Compared to the state-of-the-art, Tinbergen is 1.5 orders of magnitude faster when training, 50% faster in answering queries, and, when identifying functions in binaries generated from differing compilation environments, is 30%-61% more accurate.

</details>


## 2020-07

<details>

<summary>2020-07-01 06:45:27 - TransINT: Embedding Implication Rules in Knowledge Graphs with Isomorphic Intersections of Linear Subspaces</summary>

- *So Yeon Min, Preethi Raghavan, Peter Szolovits*

- `2007.00271v1` - [abs](http://arxiv.org/abs/2007.00271v1) - [pdf](http://arxiv.org/pdf/2007.00271v1)

> Knowledge Graphs (KG), composed of entities and relations, provide a structured representation of knowledge. For easy access to statistical approaches on relational data, multiple methods to embed a KG into f(KG) $\in$ R^d have been introduced. We propose TransINT, a novel and interpretable KG embedding method that isomorphically preserves the implication ordering among relations in the embedding space. Given implication rules, TransINT maps set of entities (tied by a relation) to continuous sets of vectors that are inclusion-ordered isomorphically to relation implications. With a novel parameter sharing scheme, TransINT enables automatic training on missing but implied facts without rule grounding. On a benchmark dataset, we outperform the best existing state-of-the-art rule integration embedding methods with significant margins in link Prediction and triple Classification. The angles between the continuous sets embedded by TransINT provide an interpretable way to mine semantic relatedness and implication rules among relations.

</details>

<details>

<summary>2020-07-01 07:29:35 - Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation</summary>

- *Andreas Pfeuffer, Klaus Dietmayer*

- `2007.00290v1` - [abs](http://arxiv.org/abs/2007.00290v1) - [pdf](http://arxiv.org/pdf/2007.00290v1)

> Computer vision tasks such as semantic segmentation perform very well in good weather conditions, but if the weather turns bad, they have problems to achieve this performance in these conditions. One possibility to obtain more robust and reliable results in adverse weather conditions is to use video-segmentation approaches instead of commonly used single-image segmentation methods. Video-segmentation approaches capture temporal information of the previous video-frames in addition to current image information, and hence, they are more robust against disturbances, especially if they occur in only a few frames of the video-sequence. However, video-segmentation approaches, which are often based on recurrent neural networks, cannot be applied in real-time applications anymore, since their recurrent structures in the network are computational expensive. For instance, the inference time of the LSTM-ICNet, in which recurrent units are placed at proper positions in the single-segmentation approach ICNet, increases up to 61 percent compared to the basic ICNet. Hence, in this work, the LSTM-ICNet is sped up by modifying the recurrent units of the network so that it becomes real-time capable again. Experiments on different datasets and various weather conditions show that the inference time can be decreased by about 23 percent by these modifications, while they achieve similar performance than the LSTM-ICNet and outperform the single-segmentation approach enormously in adverse weather conditions.

</details>

<details>

<summary>2020-07-01 10:54:27 - Unsupervised Semantic Hashing with Pairwise Reconstruction</summary>

- *Casper Hansen, Christian Hansen, Jakob Grue Simonsen, Stephen Alstrup, Christina Lioma*

- `2007.00380v1` - [abs](http://arxiv.org/abs/2007.00380v1) - [pdf](http://arxiv.org/pdf/2007.00380v1)

> Semantic Hashing is a popular family of methods for efficient similarity search in large-scale datasets. In Semantic Hashing, documents are encoded as short binary vectors (i.e., hash codes), such that semantic similarity can be efficiently computed using the Hamming distance. Recent state-of-the-art approaches have utilized weak supervision to train better performing hashing models. Inspired by this, we present Semantic Hashing with Pairwise Reconstruction (PairRec), which is a discrete variational autoencoder based hashing model. PairRec first encodes weakly supervised training pairs (a query document and a semantically similar document) into two hash codes, and then learns to reconstruct the same query document from both of these hash codes (i.e., pairwise reconstruction). This pairwise reconstruction enables our model to encode local neighbourhood structures within the hash code directly through the decoder. We experimentally compare PairRec to traditional and state-of-the-art approaches, and obtain significant performance improvements in the task of document similarity search.

</details>

<details>

<summary>2020-07-01 16:40:00 - Exploiting the Logits: Joint Sign Language Recognition and Spell-Correction</summary>

- *Christina Runkel, Stefan Dorenkamp, Hartmut Bauermeister, Michael Moeller*

- `2007.00603v1` - [abs](http://arxiv.org/abs/2007.00603v1) - [pdf](http://arxiv.org/pdf/2007.00603v1)

> Machine learning techniques have excelled in the automatic semantic analysis of images, reaching human-level performances on challenging benchmarks. Yet, the semantic analysis of videos remains challenging due to the significantly higher dimensionality of the input data, respectively, the significantly higher need for annotated training examples. By studying the automatic recognition of German sign language videos, we demonstrate that on the relatively scarce training data of 2.800 videos, modern deep learning architectures for video analysis (such as ResNeXt) along with transfer learning on large gesture recognition tasks, can achieve about 75% character accuracy. Considering that this leaves us with a probability of under 25% that a 5 letter word is spelled correctly, spell-correction systems are crucial for producing readable outputs. The contribution of this paper is to propose a convolutional neural network for spell-correction that expects the softmax outputs of the character recognition network (instead of a misspelled word) as an input. We demonstrate that purely learning on softmax inputs in combination with scarce training data yields overfitting as the network learns the inputs by heart. In contrast, training the network on several variants of the logits of the classification output i.e. scaling by a constant factor, adding of random noise, mixing of softmax and hardmax inputs or purely training on hardmax inputs, leads to better generalization while benefitting from the significant information hidden in these outputs (that have 98% top-5 accuracy), yielding a readable text despite the comparably low character accuracy.

</details>

<details>

<summary>2020-07-01 19:25:34 - ConFoc: Content-Focus Protection Against Trojan Attacks on Neural Networks</summary>

- *Miguel Villarreal-Vasquez, Bharat Bhargava*

- `2007.00711v1` - [abs](http://arxiv.org/abs/2007.00711v1) - [pdf](http://arxiv.org/pdf/2007.00711v1)

> Deep Neural Networks (DNNs) have been applied successfully in computer vision. However, their wide adoption in image-related applications is threatened by their vulnerability to trojan attacks. These attacks insert some misbehavior at training using samples with a mark or trigger, which is exploited at inference or testing time. In this work, we analyze the composition of the features learned by DNNs at training. We identify that they, including those related to the inserted triggers, contain both content (semantic information) and style (texture information), which are recognized as a whole by DNNs at testing time. We then propose a novel defensive technique against trojan attacks, in which DNNs are taught to disregard the styles of inputs and focus on their content only to mitigate the effect of triggers during the classification. The generic applicability of the approach is demonstrated in the context of a traffic sign and a face recognition application. Each of them is exposed to a different attack with a variety of triggers. Results show that the method reduces the attack success rate significantly to values < 1% in all the tested attacks while keeping as well as improving the initial accuracy of the models when processing both benign and adversarial data.

</details>

<details>

<summary>2020-07-01 20:39:39 - Build2Vec: Building Representation in Vector Space</summary>

- *Mahmoud Abdelrahman, Adrian Chong, Clayton Miller*

- `2007.00740v1` - [abs](http://arxiv.org/abs/2007.00740v1) - [pdf](http://arxiv.org/pdf/2007.00740v1)

> In this paper, we represent a methodology of a graph embeddings algorithm that is used to transform labeled property graphs obtained from a Building Information Model (BIM). Industrial Foundation Classes (IFC) is a standard schema for BIM, which is utilized to convert the building data into a graph representation. We used node2Vec with biased random walks to extract semantic similarities between different building components and represent them in a multi-dimensional vector space. A case study implementation is conducted on a net-zero-energy building located at the National University of Singapore (SDE4). This approach shows promising machine learning applications in capturing the semantic relations and similarities of different building objects, more specifically, spatial and spatio-temporal data.

</details>

<details>

<summary>2020-07-02 01:38:41 - Object Goal Navigation using Goal-Oriented Semantic Exploration</summary>

- *Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov*

- `2007.00643v2` - [abs](http://arxiv.org/abs/2007.00643v2) - [pdf](http://arxiv.org/pdf/2007.00643v2)

> This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.

</details>

<details>

<summary>2020-07-02 11:07:53 - Higher-order Logic as Lingua Franca -- Integrating Argumentative Discourse and Deep Logical Analysis</summary>

- *David Fuenmayor, Christoph Benzmüller*

- `2007.01019v1` - [abs](http://arxiv.org/abs/2007.01019v1) - [pdf](http://arxiv.org/pdf/2007.01019v1)

> We present an approach towards the deep, pluralistic logical analysis of argumentative discourse that benefits from the application of state-of-the-art automated reasoning technology for classical higher-order logic. Thanks to its expressivity this logic can adopt the status of a uniform \textit{lingua franca} allowing the encoding of both formalized arguments (their deep logical structure) and dialectical interactions (their attack and support relations). We illustrate this by analyzing an excerpt from an argumentative debate on climate engineering.   Another, novel contribution concerns the definition of abstract, language-theoretical foundations for the characterization and assessment of shallow semantical embeddings (SSEs) of non-classical logics in classical higher-order logic, which constitute a pillar stone of our approach.   The novel perspective we draw enables more concise and more elegant characterizations of semantical embeddings of logics and logic combinations, which is demonstrated with several examples.

</details>

<details>

<summary>2020-07-02 11:31:59 - Imparting Interpretability to Word Embeddings while Preserving Semantic Structure</summary>

- *Lutfi Kerem Senel, Ihsan Utlu, Furkan Şahinuç, Haldun M. Ozaktas, Aykut Koç*

- `1807.07279v4` - [abs](http://arxiv.org/abs/1807.07279v4) - [pdf](http://arxiv.org/pdf/1807.07279v4)

> As an ubiquitous method in natural language processing, word embeddings are extensively employed to map semantic properties of words into a dense vector representation. They capture semantic and syntactic relations among words but the vectors corresponding to the words are only meaningful relative to each other. Neither the vector nor its dimensions have any absolute, interpretable meaning. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related, along predefined concepts. Therefore, we impart interpretability to the word embedding by assigning meaning to its vector dimensions. The predefined concepts are derived from an external lexical resource, which in this paper is chosen as Roget's Thesaurus. We observe that alignment along the chosen concepts is not limited to words in the Thesaurus and extends to other related words as well. We quantify the extent of interpretability and assignment of meaning from our experimental results. Manual human evaluation results have also been presented to further verify that the proposed method increases interpretability. We also demonstrate the preservation of semantic coherence of the resulting vector space by using word-analogy and word-similarity tests. These tests show that the interpretability-imparted word embeddings that are obtained by the proposed framework do not sacrifice performances in common benchmark tests.

</details>

<details>

<summary>2020-07-02 13:02:54 - Scene Graph Reasoning for Visual Question Answering</summary>

- *Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, Stephan Günnemann*

- `2007.01072v1` - [abs](http://arxiv.org/abs/2007.01072v1) - [pdf](http://arxiv.org/pdf/2007.01072v1)

> Visual question answering is concerned with answering free-form questions about an image. Since it requires a deep linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires techniques from both computer vision and natural language processing. We propose a novel method that approaches the task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene. As a first step, we derive a scene graph which describes the objects in the image, as well as their attributes and their mutual relationships. A reinforcement agent then learns to autonomously navigate over the extracted scene graph to generate paths, which are then the basis for deriving answers. We conduct a first experimental study on the challenging GQA dataset with manually curated scene graphs, where our method almost reaches the level of human performance.

</details>

<details>

<summary>2020-07-02 19:47:47 - SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing</summary>

- *Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak Lee, Bo Li*

- `1906.07927v4` - [abs](http://arxiv.org/abs/1906.07927v4) - [pdf](http://arxiv.org/pdf/1906.07927v4)

> Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee "subtle perturbation" by limiting the $L_p$ norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate "unrestricted adversarial examples".   In particular, we propose an algorithm \emph{SemanticAdv} which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various "adversarial" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against \emph{real-world black-box} services such as Azure face verification service based on transferability.   To further demonstrate the applicability of \emph{SemanticAdv} beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.

</details>

<details>

<summary>2020-07-02 21:25:18 - Indoor Scene Recognition in 3D</summary>

- *Shengyu Huang, Mikhail Usvyatsov, Konrad Schindler*

- `2002.12819v2` - [abs](http://arxiv.org/abs/2002.12819v2) - [pdf](http://arxiv.org/pdf/2002.12819v2)

> Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating in indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way of improving scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy.

</details>

<details>

<summary>2020-07-03 06:32:48 - MIRA: Leveraging Multi-Intention Co-click Information in Web-scale Document Retrieval using Deep Neural Networks</summary>

- *Yusi Zhang, Chuanjie Liu, Angen Luo, Hui Xue, Xuan Shan, Yuxiang Luo, Yiqian Xia, Yuanchi Yan, Haidong Wang*

- `2007.01510v1` - [abs](http://arxiv.org/abs/2007.01510v1) - [pdf](http://arxiv.org/pdf/2007.01510v1)

> We study the problem of deep recall model in industrial web search, which is, given a user query, retrieve hundreds of most relevance documents from billions of candidates. The common framework is to train two encoding models based on neural embedding which learn the distributed representations of queries and documents separately and match them in the latent semantic space. However, all the exiting encoding models only leverage the information of the document itself, which is often not sufficient in practice when matching with query terms, especially for the hard tail queries. In this work we aim to leverage the additional information for each document from its co-click neighbour to help document retrieval. The challenges include how to effectively extract information and eliminate noise when involving co-click information in deep model while meet the demands of billion-scale data size for real time online inference.   To handle the noise in co-click relations, we firstly propose a web-scale Multi-Intention Co-click document Graph(MICG) which builds the co-click connections between documents on click intention level but not on document level. Then we present an encoding framework MIRA based on Bert and graph attention networks which leverages a two-factor attention mechanism to aggregate neighbours. To meet the online latency requirements, we only involve neighbour information in document side, which can save the time-consuming query neighbor search in real time serving. We conduct extensive offline experiments on both public dataset and private web-scale dataset from two major commercial search engines demonstrating the effectiveness and scalability of the proposed method compared with several baselines. And a further case study reveals that co-click relations mainly help improve web search quality from two aspects: key concept enhancing and query term complementary.

</details>

<details>

<summary>2020-07-03 12:36:28 - Effective writing style imitation via combinatorial paraphrasing</summary>

- *Tommi Gröndahl, N. Asokan*

- `1905.13464v3` - [abs](http://arxiv.org/abs/1905.13464v3) - [pdf](http://arxiv.org/pdf/1905.13464v3)

> Stylometry can be used to profile or deanonymize authors against their will based on writing style. Style transfer provides a defence. Current techniques typically use either encoder-decoder architectures or rule-based algorithms. Crucially, style transfer must reliably retain original semantic content to be actually deployable. We conduct a multifaceted evaluation of three state-of-the-art encoder-decoder style transfer techniques, and show that all fail at semantic retainment. In particular, they do not produce appropriate paraphrases, but only retain original content in the trivial case of exactly reproducing the text. To mitigate this problem we propose ParChoice: a technique based on the combinatorial application of multiple paraphrasing algorithms. ParChoice strongly outperforms the encoder-decoder baselines in semantic retainment. Additionally, compared to baselines that achieve non-negligible semantic retainment, ParChoice has superior style transfer performance. We also apply ParChoice to multi-author style imitation (not considered by prior work), where we achieve up to 75% imitation success among five authors. Furthermore, when compared to two state-of-the-art rule-based style transfer techniques, ParChoice has markedly better semantic retainment. Combining ParChoice with the best performing rule-based baseline (Mutant-X) also reaches the highest style transfer success on the Brennan-Greenstadt and Extended-Brennan-Greenstadt corpora, with much less impact on original meaning than when using the rule-based baseline techniques alone. Finally, we highlight a critical problem that afflicts all current style transfer techniques: the adversary can use the same technique for thwarting style transfer via adversarial training. We show that adding randomness to style transfer helps to mitigate the effectiveness of adversarial training.

</details>

<details>

<summary>2020-07-03 14:25:22 - Towards the Adoption of OMG Standards in the Development of SOA-Based IoT Systems</summary>

- *Bruno Costa, Paulo F. Pires, Flavia C. Delicato*

- `2007.01713v1` - [abs](http://arxiv.org/abs/2007.01713v1) - [pdf](http://arxiv.org/pdf/2007.01713v1)

> A common feature of the Internet of Things (IoT) is the high heterogeneity, regarding network protocols, data formats, hardware and software platforms. Aiming to deal with such a degree of heterogeneity, several frameworks have applied the Model-Driven Development (MDD) to build IoT applications. On the software architecture viewpoint, the literature has shown that the Service-Oriented Architecture (SOA) is a promising style to address the interoperability of entities composing these solutions. Some features of IoT make it challenging to analyze the impact of design decisions on the SOA-based IoT applications behavior. Thus, it is a key requirement to simulate the model to verify whether the system performs as expected before its implementation. Although the literature has identified that the SOA style is suitable for addressing the interoperability, existing modelling languages do not consider SOA elements as first-class citizens when designing IoT applications. Furthermore, although existing MDD frameworks provide modeling languages comprising well-defined syntax, they lack execution semantics, thus, are not suitable for model execution and analysis. This work aims at addressing these issues by introducing IoTDraw. The framework provides a fully OMG-compliant executable modeling language for SOA-based IoT systems; thus, its specifications can be implemented by any tool implementing OMG standards.

</details>

<details>

<summary>2020-07-03 15:25:54 - SCAN: Learning to Classify Images without Labels</summary>

- *Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool*

- `2005.12320v2` - [abs](http://arxiv.org/abs/2005.12320v2) - [pdf](http://arxiv.org/pdf/2005.12320v2)

> Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is made publicly available at https://github.com/wvangansbeke/Unsupervised-Classification.

</details>

<details>

<summary>2020-07-03 16:22:34 - Evaluating Uncertainty Estimation Methods on 3D Semantic Segmentation of Point Clouds</summary>

- *Swaroop Bhandary K, Nico Hochgeschwender, Paul Plöger, Frank Kirchner, Matias Valdenegro-Toro*

- `2007.01787v1` - [abs](http://arxiv.org/abs/2007.01787v1) - [pdf](http://arxiv.org/pdf/2007.01787v1)

> Deep learning models are extensively used in various safety critical applications. Hence these models along with being accurate need to be highly reliable. One way of achieving this is by quantifying uncertainty. Bayesian methods for UQ have been extensively studied for Deep Learning models applied on images but have been less explored for 3D modalities such as point clouds often used for Robots and Autonomous Systems. In this work, we evaluate three uncertainty quantification methods namely Deep Ensembles, MC-Dropout and MC-DropConnect on the DarkNet21Seg 3D semantic segmentation model and comprehensively analyze the impact of various parameters such as number of models in ensembles or forward passes, and drop probability values, on task performance and uncertainty estimate quality. We find that Deep Ensembles outperforms other methods in both performance and uncertainty metrics. Deep ensembles outperform other methods by a margin of 2.4% in terms of mIOU, 1.3% in terms of accuracy, while providing reliable uncertainty for decision making.

</details>

<details>

<summary>2020-07-03 16:27:23 - Learning Permutation Invariant Representations using Memory Networks</summary>

- *Shivam Kalra, Mohammed Adnan, Graham Taylor, Hamid Tizhoosh*

- `1911.07984v2` - [abs](http://arxiv.org/abs/1911.07984v2) - [pdf](http://arxiv.org/pdf/1911.07984v2)

> Many real-world tasks such as classification of digital histopathology images and 3D object detection involve learning from a set of instances. In these cases, only a group of instances or a set, collectively, contains meaningful information and therefore only the sets have labels, and not individual data instances. In this work, we present a permutation invariant neural network called Memory-based Exchangeable Model (MEM) for learning set functions. The MEM model consists of memory units that embed an input sequence to high-level features enabling the model to learn inter-dependencies among instances through a self-attention mechanism. We evaluated the learning ability of MEM on various toy datasets, point cloud classification, and classification of lung whole slide images (WSIs) into two subtypes of lung cancer---Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma. We systematically extracted patches from lung WSIs downloaded from The Cancer Genome Atlas~(TCGA) dataset, the largest public repository of WSIs, achieving a competitive accuracy of 84.84\% for classification of two sub-types of lung cancer. The results on other datasets are promising as well, and demonstrate the efficacy of our model.

</details>

<details>

<summary>2020-07-03 16:40:37 - Exploration and Discovery of the COVID-19 Literature through Semantic Visualization</summary>

- *Jingxuan Tu, Marc Verhagen, Brent Cochran, James Pustejovsky*

- `2007.01800v1` - [abs](http://arxiv.org/abs/2007.01800v1) - [pdf](http://arxiv.org/pdf/2007.01800v1)

> We are developing semantic visualization techniques in order to enhance exploration and enable discovery over large datasets of complex networks of relations. Semantic visualization is a method of enabling exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves (i) NLP to extract named entities, relations and knowledge graphs from the original data; (ii) indexing the output and creating representations for all relevant entities and relations that can be visualized in many different ways, e.g., as tag clouds, heat maps, graphs, etc.; (iii) applying parameter reduction operations to the extracted relations, creating "relation containers", or functional entities that can also be visualized using the same methods, allowing the visualization of multiple relations, partial pathways, and exploration across multiple dimensions. Our hope is that this will enable the discovery of novel inferences over relations in complex data that otherwise would go unnoticed. We have applied this to analysis of the recently released CORD-19 dataset.

</details>

<details>

<summary>2020-07-04 01:04:58 - Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation</summary>

- *Shun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, Hung-yi Lee*

- `2005.10678v2` - [abs](http://arxiv.org/abs/2005.10678v2) - [pdf](http://arxiv.org/pdf/2005.10678v2)

> Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.

</details>

<details>

<summary>2020-07-04 16:29:54 - Towards Semantic Detection of Smells in Cloud Infrastructure Code</summary>

- *Indika Kumara, Zoe Vasileiou, Georgios Meditskos, Damian A. Tamburri, Willem-Jan Van Den Heuvel, Anastasios Karakostas, Stefanos Vrochidis, Ioannis Kompatsiaris*

- `2007.02135v1` - [abs](http://arxiv.org/abs/2007.02135v1) - [pdf](http://arxiv.org/pdf/2007.02135v1)

> Automated deployment and management of Cloud applications relies on descriptions of their deployment topologies, often referred to as Infrastructure Code. As the complexity of applications and their deployment models increases, developers inadvertently introduce software smells to such code specifications, for instance, violations of good coding practices, modular structure, and more. This paper presents a knowledge-driven approach enabling developers to identify the aforementioned smells in deployment descriptions. We detect smells with SPARQL-based rules over pattern-based OWL 2 knowledge graphs capturing deployment models. We show the feasibility of our approach with a prototype and three case studies.

</details>

<details>

<summary>2020-07-05 01:19:57 - Human Action Attribute Learning From Video Data Using Low-Rank Representations</summary>

- *Tong Wu, Prudhvi Gurram, Raghuveer M. Rao, Waheed U. Bajwa*

- `1612.07857v2` - [abs](http://arxiv.org/abs/1612.07857v2) - [pdf](http://arxiv.org/pdf/1612.07857v2)

> Representation of human actions as a sequence of human body movements or action attributes enables the development of models for human activity recognition and summarization. We present an extension of the low-rank representation (LRR) model, termed the clustering-aware structure-constrained low-rank representation (CS-LRR) model, for unsupervised learning of human action attributes from video data. Our model is based on the union-of-subspaces (UoS) framework, and integrates spectral clustering into the LRR optimization problem for better subspace clustering results. We lay out an efficient linear alternating direction method to solve the CS-LRR optimization problem. We also introduce a hierarchical subspace clustering approach, termed hierarchical CS-LRR, to learn the attributes without the need for a priori specification of their number. By visualizing and labeling these action attributes, the hierarchical model can be used to semantically summarize long video sequences of human actions at multiple resolutions. A human action or activity can also be uniquely represented as a sequence of transitions from one action attribute to another, which can then be used for human action recognition. We demonstrate the effectiveness of the proposed model for semantic summarization and action recognition through comprehensive experiments on five real-world human action datasets.

</details>

<details>

<summary>2020-07-05 05:54:02 - Unsupervised Paraphrasing via Deep Reinforcement Learning</summary>

- *A. B. Siddique, Samet Oymak, Vagelis Hristidis*

- `2007.02244v1` - [abs](http://arxiv.org/abs/2007.02244v1) - [pdf](http://arxiv.org/pdf/2007.02244v1)

> Paraphrasing is expressing the meaning of an input sentence in different wording while maintaining fluency (i.e., grammatical and syntactical correctness). Most existing work on paraphrasing use supervised models that are limited to specific domains (e.g., image captions). Such models can neither be straightforwardly transferred to other domains nor generalize well, and creating labeled training data for new domains is expensive and laborious. The need for paraphrasing across different domains and the scarcity of labeled training data in many such domains call for exploring unsupervised paraphrase generation methods. We propose Progressive Unsupervised Paraphrasing (PUP): a novel unsupervised paraphrase generation method based on deep reinforcement learning (DRL). PUP uses a variational autoencoder (trained using a non-parallel corpus) to generate a seed paraphrase that warm-starts the DRL model. Then, PUP progressively tunes the seed paraphrase guided by our novel reward function which combines semantic adequacy, language fluency, and expression diversity measures to quantify the quality of the generated paraphrases in each iteration without needing parallel sentences. Our extensive experimental evaluation shows that PUP outperforms unsupervised state-of-the-art paraphrasing techniques in terms of both automatic metrics and user studies on four real datasets. We also show that PUP outperforms domain-adapted supervised algorithms on several datasets. Our evaluation also shows that PUP achieves a great trade-off between semantic similarity and diversity of expression.

</details>

<details>

<summary>2020-07-05 08:10:30 - Tweets Sentiment Analysis via Word Embeddings and Machine Learning Techniques</summary>

- *Aditya Sharma, Alex Daniels*

- `2007.04303v1` - [abs](http://arxiv.org/abs/2007.04303v1) - [pdf](http://arxiv.org/pdf/2007.04303v1)

> Sentiment analysis of social media data consists of attitudes, assessments, and emotions which can be considered a way human think. Understanding and classifying the large collection of documents into positive and negative aspects are a very difficult task. Social networks such as Twitter, Facebook, and Instagram provide a platform in order to gather information about peoples sentiments and opinions. Considering the fact that people spend hours daily on social media and share their opinion on various different topics helps us analyze sentiments better. More and more companies are using social media tools to provide various services and interact with customers. Sentiment Analysis (SA) classifies the polarity of given tweets to positive and negative tweets in order to understand the sentiments of the public. This paper aims to perform sentiment analysis of real-time 2019 election twitter data using the feature selection model word2vec and the machine learning algorithm random forest for sentiment classification. Word2vec with Random Forest improves the accuracy of sentiment analysis significantly compared to traditional methods such as BOW and TF-IDF. Word2vec improves the quality of features by considering contextual semantics of words in a text hence improving the accuracy of machine learning and sentiment analysis.

</details>

<details>

<summary>2020-07-05 12:12:09 - Challenges in Designing Exploit Mitigations for Deeply Embedded Systems</summary>

- *Ali Abbasi, Jos Wetzels, Thorsten Holz, Sandro Etalle*

- `2007.02307v1` - [abs](http://arxiv.org/abs/2007.02307v1) - [pdf](http://arxiv.org/pdf/2007.02307v1)

> Memory corruption vulnerabilities have been around for decades and rank among the most prevalent vulnerabilities in embedded systems. Yet this constrained environment poses unique design and implementation challenges that significantly complicate the adoption of common hardening techniques. Combined with the irregular and involved nature of embedded patch management, this results in prolonged vulnerability exposure windows and vulnerabilities that are relatively easy to exploit. Considering the sensitive and critical nature of many embedded systems, this situation merits significant improvement. In this work, we present the first quantitative study of exploit mitigation adoption in 42 embedded operating systems, showing the embedded world to significantly lag behind the general-purpose world. To improve the security of deeply embedded systems, we subsequently present {\mu}Armor, an approach to address some of the key gaps identified in our quantitative analysis. {\mu}Armor raises the bar for exploitation of embedded memory corruption vulnerabilities, while being adoptable on the short term without incurring prohibitive extra performance or storage costs.

</details>

<details>

<summary>2020-07-05 14:34:10 - Neural Relation Prediction for Simple Question Answering over Knowledge Graph</summary>

- *Amin Abolghasemi, Saeedeh Momtazi*

- `2002.07715v3` - [abs](http://arxiv.org/abs/2002.07715v3) - [pdf](http://arxiv.org/pdf/2002.07715v3)

> Knowledge graphs are widely used as a typical resource to provide answers to factoid questions. In simple question answering over knowledge graphs, relation extraction aims to predict the relation of a factoid question from a set of predefined relation types. Most recent methods take advantage of neural networks to match a question with all predefined relations. In this paper, we propose an instance-based method to capture the underlying relation of question and to this aim, we detect matching paraphrases of a new question which share the same relation, and their corresponding relation is selected as our prediction. The idea of our model roots in the fact that a relation can be expressed with various forms of questions while these forms share lexically or semantically similar terms and concepts. Our experiments on the SimpleQuestions dataset show that the proposed model achieves better accuracy compared to the state-of-the-art relation extraction models.

</details>

<details>

<summary>2020-07-05 18:54:10 - Attention-based Joint Detection of Object and Semantic Part</summary>

- *Keval Morabia, Jatin Arora, Tara Vijaykumar*

- `2007.02419v1` - [abs](http://arxiv.org/abs/2007.02419v1) - [pdf](http://arxiv.org/pdf/2007.02419v1)

> In this paper, we address the problem of joint detection of objects like dog and its semantic parts like face, leg, etc. Our model is created on top of two Faster-RCNN models that share their features to perform a novel Attention-based feature fusion of related Object and Part features to get enhanced representations of both. These representations are used for final classification and bounding box regression separately for both models. Our experiments on the PASCAL-Part 2010 dataset show that joint detection can simultaneously improve both object detection and part detection in terms of mean Average Precision (mAP) at IoU=0.5.

</details>

<details>

<summary>2020-07-06 00:31:21 - BiO-Net: Learning Recurrent Bi-directional Connections for Encoder-Decoder Architecture</summary>

- *Tiange Xiang, Chaoyi Zhang, Dongnan Liu, Yang Song, Heng Huang, Weidong Cai*

- `2007.00243v2` - [abs](http://arxiv.org/abs/2007.00243v2) - [pdf](http://arxiv.org/pdf/2007.00243v2)

> U-Net has become one of the state-of-the-art deep learning-based approaches for modern computer vision tasks such as semantic segmentation, super resolution, image denoising, and inpainting. Previous extensions of U-Net have focused mainly on the modification of its existing building blocks or the development of new functional modules for performance gains. As a result, these variants usually lead to an unneglectable increase in model complexity. To tackle this issue in such U-Net variants, in this paper, we present a novel Bi-directional O-shape network (BiO-Net) that reuses the building blocks in a recurrent manner without introducing any extra parameters. Our proposed bi-directional skip connections can be directly adopted into any encoder-decoder architecture to further enhance its capabilities in various task domains. We evaluated our method on various medical image analysis tasks and the results show that our BiO-Net significantly outperforms the vanilla U-Net as well as other state-of-the-art methods. Our code is available at https://github.com/tiangexiang/BiO-Net.

</details>

<details>

<summary>2020-07-06 03:06:26 - Adversarial T-shirt! Evading Person Detectors in A Physical World</summary>

- *Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin*

- `1910.11099v3` - [abs](http://arxiv.org/abs/1910.11099v3) - [pdf](http://arxiv.org/pdf/1910.11099v3)

> It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decisionmakers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we proposed adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person's pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to-rigid objects such as T-shirts. We show that the proposed method achieves74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.

</details>

<details>

<summary>2020-07-06 03:24:23 - Crossing Variational Autoencoders for Answer Retrieval</summary>

- *Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng, Meng Jiang*

- `2005.02557v2` - [abs](http://arxiv.org/abs/2005.02557v2) - [pdf](http://arxiv.org/pdf/2005.02557v2)

> Answer retrieval is to find the most aligned answer from a large set of candidates given a question. Learning vector representations of questions/answers is the key factor. Question-answer alignment and question/answer semantics are two important signals for learning the representations. Existing methods learned semantic representations with dual encoders or dual variational auto-encoders. The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes. However, the alignment and semantics were too separate to capture the aligned semantics between question and answer. In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions. Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.

</details>

<details>

<summary>2020-07-06 09:18:20 - Sosed: a tool for finding similar software projects</summary>

- *Egor Bogomolov, Yaroslav Golubev, Artyom Lobanov, Vladimir Kovalenko, Timofey Bryksin*

- `2007.02599v1` - [abs](http://arxiv.org/abs/2007.02599v1) - [pdf](http://arxiv.org/pdf/2007.02599v1)

> In this paper, we present Sosed, a tool for discovering similar software projects. We use fastText to compute the embeddings of subtokens into a dense space for 120,000 GitHub repositories in 200 languages. Then, we cluster embeddings to identify groups of semantically similar sub-tokens that reflect topics in source code. We use a dataset of 9 million GitHub projects as a reference search base. To identify similar projects, we compare the distributions of clusters among their sub-tokens. The tool receives an arbitrary project as input, extracts sub-tokens in 16 most popular programming languages, computes cluster distribution, and finds projects with the closest distribution in the search base. We labeled subtoken clusters with short descriptions to enable Sosed to produce interpretable output. Sosed is available at https://github.com/JetBrains-Research/sosed/. The tool demo is available at https://www.youtube.com/watch?v=LYLkztCGRt8. The multi-language extractor of sub-tokens is available separately at https://github.com/JetBrains-Research/buckwheat/.

</details>

<details>

<summary>2020-07-06 11:09:40 - GCN for HIN via Implicit Utilization of Attention and Meta-paths</summary>

- *Di Jin, Zhizhi Yu, Dongxiao He, Carl Yang, Philip S. Yu, Jiawei Han*

- `2007.02643v1` - [abs](http://arxiv.org/abs/2007.02643v1) - [pdf](http://arxiv.org/pdf/2007.02643v1)

> Heterogeneous information network (HIN) embedding, aiming to map the structure and semantic information in a HIN to distributed representations, has drawn considerable research attention. Graph neural networks for HIN embeddings typically adopt a hierarchical attention (including node-level and meta-path-level attentions) to capture the information from meta-path-based neighbors. However, this complicated attention structure often cannot achieve the function of selecting meta-paths due to severe overfitting. Moreover, when propagating information, these methods do not distinguish direct (one-hop) meta-paths from indirect (multi-hop) ones. But from the perspective of network science, direct relationships are often believed to be more essential, which can only be used to model direct information propagation. To address these limitations, we propose a novel neural network method via implicitly utilizing attention and meta-paths, which can relieve the severe overfitting brought by the current over-parameterized attention mechanisms on HIN. We first use the multi-layer graph convolutional network (GCN) framework, which performs a discriminative aggregation at each layer, along with stacking the information propagation of direct linked meta-paths layer-by-layer, realizing the function of attentions for selecting meta-paths in an indirect way. We then give an effective relaxation and improvement via introducing a new propagation operation which can be separated from aggregation. That is, we first model the whole propagation process with well-defined probabilistic diffusion dynamics, and then introduce a random graph-based constraint which allows it to reduce noise with the increase of layers. Extensive experiments demonstrate the superiority of the new approach over state-of-the-art methods.

</details>

<details>

<summary>2020-07-06 11:53:15 - Hinting Semantic Parsing with Statistical Word Sense Disambiguation</summary>

- *Ritwik Bose, Siddharth Vashishtha, James Allen*

- `2006.15942v2` - [abs](http://arxiv.org/abs/2006.15942v2) - [pdf](http://arxiv.org/pdf/2006.15942v2)

> The task of Semantic Parsing can be approximated as a transformation of an utterance into a logical form graph where edges represent semantic roles and nodes represent word senses. The resulting representation should be capture the meaning of the utterance and be suitable for reasoning. Word senses and semantic roles are interdependent, meaning errors in assigning word senses can cause errors in assigning semantic roles and vice versa. While statistical approaches to word sense disambiguation outperform logical, rule-based semantic parsers for raw word sense assignment, these statistical word sense disambiguation systems do not produce the rich role structure or detailed semantic representation of the input. In this work, we provide hints from a statistical WSD system to guide a logical semantic parser to produce better semantic type assignments while maintaining the soundness of the resulting logical forms. We observe an improvement of up to 10.5% in F-score, however we find that this improvement comes at a cost to the structural integrity of the parse

</details>

<details>

<summary>2020-07-06 12:03:14 - A Broad-Coverage Deep Semantic Lexicon for Verbs</summary>

- *James Allen, Hannah An, Ritwik Bose, Will de Beaumont, Choh Man Teng*

- `2007.02670v1` - [abs](http://arxiv.org/abs/2007.02670v1) - [pdf](http://arxiv.org/pdf/2007.02670v1)

> Progress on deep language understanding is inhibited by the lack of a broad coverage lexicon that connects linguistic behavior to ontological concepts and axioms. We have developed COLLIE-V, a deep lexical resource for verbs, with the coverage of WordNet and syntactic and semantic details that meet or exceed existing resources. Bootstrapping from a hand-built lexicon and ontology, new ontological concepts and lexical entries, together with semantic role preferences and entailment axioms, are automatically derived by combining multiple constraints from parsing dictionary definitions and examples. We evaluated the accuracy of the technique along a number of different dimensions and were able to obtain high accuracy in deriving new concepts and lexical entries. COLLIE-V is publicly available.

</details>

<details>

<summary>2020-07-06 13:20:44 - Distance and routing labeling schemes for cube-free median graphs</summary>

- *Victor Chepoi, Arnaud Labourel, Sebastien Ratel*

- `1809.10508v2` - [abs](http://arxiv.org/abs/1809.10508v2) - [pdf](http://arxiv.org/pdf/1809.10508v2)

> Distance labeling schemes are schemes that label the vertices of a graph with short labels in such a way that the distance between any two vertices $u$ and $v$ can be determined efficiently by merely inspecting the labels of $u$ and $v$, without using any other information. Similarly, routing labeling schemes label the vertices of a graph in a such a way that given the labels of a source node and a destination node, it is possible to compute efficiently the port number of the edge from the source that heads in the direction of the destination. One of important problems is finding natural classes of graphs admitting distance and/or routing labeling schemes with labels of polylogarithmic size. In this paper, we show that the class of cube-free median graphs on $n$ nodes enjoys distance and routing labeling schemes with labels of $O(\log^3 n)$ bits.

</details>

<details>

<summary>2020-07-06 14:59:31 - An Exploratory Analysis of Microcode as a Building Block for System Defenses</summary>

- *Benjamin Kollenda, Philipp Koppe, Marc Fyrbiak, Christian Kison, Christof Paar, Thorsten Holz*

- `2007.03549v1` - [abs](http://arxiv.org/abs/2007.03549v1) - [pdf](http://arxiv.org/pdf/2007.03549v1)

> Microcode is an abstraction layer used by modern x86 processors that interprets user-visible CISC instructions to hardware-internal RISC instructions. The capability to update x86 microcode enables a vendor to modify CPU behavior in-field, and thus patch erroneous microarchitectural processes or even implement new features. Most prominently, the recent Spectre and Meltdown vulnerabilities were mitigated by Intel via microcode updates. Unfortunately, microcode is proprietary and closed source, and there is little publicly available information on its inner workings.   In this paper, we present new reverse engineering results that extend and complement the public knowledge of proprietary microcode. Based on these novel insights, we show how modern system defenses and tools can be realized in microcode on a commercial, off-the-shelf AMD x86 CPU. We demonstrate how well-established system security defenses such as timing attack mitigations, hardware-assisted address sanitization, and instruction set randomization can be realized in microcode. We also present a proof-of-concept implementation of a microcode-assisted instrumentation framework. Finally, we show how a secure microcode update mechanism and enclave functionality can be implemented in microcode to realize a small trusted execution environment. All microcode programs and the whole infrastructure needed to reproduce and extend our results are publicly available.

</details>

<details>

<summary>2020-07-06 15:49:18 - Exploration of Optimized Semantic Segmentation Architectures for edge-Deployment on Drones</summary>

- *Vivek Parmar, Narayani Bhatia, Shubham Negi, Manan Suri*

- `2007.02839v1` - [abs](http://arxiv.org/abs/2007.02839v1) - [pdf](http://arxiv.org/pdf/2007.02839v1)

> In this paper, we present an analysis on the impact of network parameters for semantic segmentation architectures in context of UAV data processing. We present the analysis on the DroneDeploy Segmentation benchmark. Based on the comparative analysis we identify the optimal network architecture to be FPN-EfficientNetB3 with pretrained encoder backbones based on Imagenet Dataset. The network achieves IoU score of 0.65 and F1-score of 0.71 over the validation dataset. We also compare the various architectures in terms of their memory footprint and inference latency with further exploration of the impact of TensorRT based optimizations. We achieve memory savings of ~4.1x and latency improvement of 10% compared to Model: FPN and Backbone: InceptionResnetV2.

</details>

<details>

<summary>2020-07-06 20:03:21 - Scalable Data Classification for Security and Privacy</summary>

- *Paulo Tanaka, Sameet Sapra, Nikolay Laptev*

- `2006.14109v5` - [abs](http://arxiv.org/abs/2006.14109v5) - [pdf](http://arxiv.org/pdf/2006.14109v5)

> Content based data classification is an open challenge. Traditional Data Loss Prevention (DLP)-like systems solve this problem by fingerprinting the data in question and monitoring endpoints for the fingerprinted data. With a large number of constantly changing data assets in Facebook, this approach is both not scalable and ineffective in discovering what data is where. This paper is about an end-to-end system built to detect sensitive semantic types within Facebook at scale and enforce data retention and access controls automatically.   The approach described here is our first end-to-end privacy system that attempts to solve this problem by incorporating data signals, machine learning, and traditional fingerprinting techniques to map out and classify all data within Facebook. The described system is in production achieving a 0.9+ average F2 scores across various privacy classes while handling a large number of data assets across dozens of data stores.

</details>

<details>

<summary>2020-07-06 21:26:15 - A Term-Rewriting Semantics for Imperative Style Programming</summary>

- *David Plaisted, Lee Barnett*

- `2007.03075v1` - [abs](http://arxiv.org/abs/2007.03075v1) - [pdf](http://arxiv.org/pdf/2007.03075v1)

> Term rewriting systems have a simple syntax and semantics and facilitate proofs of correctness. However, they are not as popular in industry or academia as imperative languages. We define a term rewriting based abstract programming language with an imperative style and a precise semantics allowing programs to be translatable into efficient imperative languages, to obtain proofs of correctness together with efficient execution. This language is designed to facilitate translations into correct programs in imperative languages with assignment statements, iteration, recursion, arrays, pointers, and side effects. It can also be used in place of a pseudo-programming language to specify algorithms.

</details>

<details>

<summary>2020-07-07 00:40:21 - S2ORC: The Semantic Scholar Open Research Corpus</summary>

- *Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Dan S. Weld*

- `1911.02782v3` - [abs](http://arxiv.org/abs/1911.02782v3) - [pdf](http://arxiv.org/pdf/1911.02782v3)

> We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.

</details>

<details>

<summary>2020-07-07 06:23:50 - Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning</summary>

- *Jianyu Chen, Shengbo Eben Li, Masayoshi Tomizuka*

- `2001.08726v3` - [abs](http://arxiv.org/abs/2001.08726v3) - [pdf](http://arxiv.org/pdf/2001.08726v3)

> Unlike popular modularized framework, end-to-end autonomous driving seeks to solve the perception, decision and control problems in an integrated way, which can be more adapting to new scenarios and easier to generalize at scale. However, existing end-to-end approaches are often lack of interpretability, and can only deal with simple driving tasks like lane keeping. In this paper, we propose an interpretable deep reinforcement learning method for end-to-end autonomous driving, which is able to handle complex urban scenarios. A sequential latent environment model is introduced and learned jointly with the reinforcement learning process. With this latent model, a semantic birdeye mask can be generated, which is enforced to connect with a certain intermediate property in today's modularized framework for the purpose of explaining the behaviors of learned policy. The latent space also significantly reduces the sample complexity of reinforcement learning. Comparison tests with a simulated autonomous car in CARLA show that the performance of our method in urban scenarios with crowded surrounding vehicles dominates many baselines including DQN, DDPG, TD3 and SAC. Moreover, through masked outputs, the learned policy is able to provide a better explanation of how the car reasons about the driving environment. The codes and videos of this work are available at our github repo and project website.

</details>

<details>

<summary>2020-07-07 09:49:47 - DAM: Deliberation, Abandon and Memory Networks for Generating Detailed and Non-repetitive Responses in Visual Dialogue</summary>

- *Xiaoze Jiang, Jing Yu, Yajing Sun, Zengchang Qin, Zihao Zhu, Yue Hu, Qi Wu*

- `2007.03310v1` - [abs](http://arxiv.org/abs/2007.03310v1) - [pdf](http://arxiv.org/pdf/2007.03310v1)

> Visual Dialogue task requires an agent to be engaged in a conversation with human about an image. The ability of generating detailed and non-repetitive responses is crucial for the agent to achieve human-like conversation. In this paper, we propose a novel generative decoding architecture to generate high-quality responses, which moves away from decoding the whole encoded semantics towards the design that advocates both transparency and flexibility. In this architecture, word generation is decomposed into a series of attention-based information selection steps, performed by the novel recurrent Deliberation, Abandon and Memory (DAM) module. Each DAM module performs an adaptive combination of the response-level semantics captured from the encoder and the word-level semantics specifically selected for generating each word. Therefore, the responses contain more detailed and non-repetitive descriptions while maintaining the semantic accuracy. Furthermore, DAM is flexible to cooperate with existing visual dialogue encoders and adaptive to the encoder structures by constraining the information selection mode in DAM. We apply DAM to three typical encoders and verify the performance on the VisDial v1.0 dataset. Experimental results show that the proposed models achieve new state-of-the-art performance with high-quality responses. The code is available at https://github.com/JXZe/DAM.

</details>

<details>

<summary>2020-07-07 11:00:27 - Diverse and Styled Image Captioning Using SVD-Based Mixture of Recurrent Experts</summary>

- *Marzieh Heidari, Mehdi Ghatee, Ahmad Nickabadi, Arash Pourhasan Nezhad*

- `2007.03338v1` - [abs](http://arxiv.org/abs/2007.03338v1) - [pdf](http://arxiv.org/pdf/2007.03338v1)

> With great advances in vision and natural language processing, the generation of image captions becomes a need. In a recent paper, Mathews, Xie and He [1], extended a new model to generate styled captions by separating semantics and style. In continuation of this work, here a new captioning model is developed including an image encoder to extract the features, a mixture of recurrent networks to embed the set of extracted features to a set of words, and a sentence generator that combines the obtained words as a stylized sentence. The resulted system that entitled as Mixture of Recurrent Experts (MoRE), uses a new training algorithm that derives singular value decomposition (SVD) from weighting matrices of Recurrent Neural Networks (RNNs) to increase the diversity of captions. Each decomposition step depends on a distinctive factor based on the number of RNNs in MoRE. Since the used sentence generator gives a stylized language corpus without paired images, our captioning model can do the same. Besides, the styled and diverse captions are extracted without training on a densely labeled or styled dataset. To validate this captioning model, we use Microsoft COCO which is a standard factual image caption dataset. We show that the proposed captioning model can generate a diverse and stylized image captions without the necessity of extra-labeling. The results also show better descriptions in terms of content accuracy.

</details>

<details>

<summary>2020-07-07 15:22:54 - Network Embedding with Completely-imbalanced Labels</summary>

- *Zheng Wang, Xiaojun Ye, Chaokun Wang, Jian Cui, Philip S. Yu*

- `2007.03545v1` - [abs](http://arxiv.org/abs/2007.03545v1) - [pdf](http://arxiv.org/pdf/2007.03545v1)

> Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSDNE guarantees both intra-class similarity and inter-class dissimilarity in an approximate way. The other method is RECT which is a new class of graph neural networks. Different from RSDNE, to benefit from the completely-imbalanced labels, RECT explores the class-semantic knowledge. This enables RECT to handle networks with node features and multi-label setting. Experimental results on several real-world datasets demonstrate the superiority of the proposed methods.

</details>

<details>

<summary>2020-07-07 15:58:10 - Using Semantic Web Services for AI-Based Research in Industry 4.0</summary>

- *Lukas Malburg, Patrick Klein, Ralph Bergmann*

- `2007.03580v1` - [abs](http://arxiv.org/abs/2007.03580v1) - [pdf](http://arxiv.org/pdf/2007.03580v1)

> The transition to Industry 4.0 requires smart manufacturing systems that are easily configurable and provide a high level of flexibility during manufacturing in order to achieve mass customization or to support cloud manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with Artificial Intelligence (AI) methods find their way into manufacturing shop floors. For using AI methods in the context of Industry 4.0, semantic web services are indispensable to provide a reasonable abstraction of the underlying manufacturing capabilities. In this paper, we present semantic web services for AI-based research in Industry 4.0. Therefore, we developed more than 300 semantic web services for a physical simulation factory based on Web Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology (WSMO) and linked them to an already existing domain ontology for intelligent manufacturing control. Suitable for the requirements of CPS environments, our pre- and postconditions are verified in near real-time by invoking other semantic web services in contrast to complex reasoning within the knowledge base. Finally, we evaluate our implementation by executing a cyber-physical workflow composed of semantic web services using a workflow management system.

</details>

<details>

<summary>2020-07-07 16:03:23 - Expressiveness of SETAFs and Support-Free ADFs under 3-valued Semantics</summary>

- *Wolfgang Dvořák, Atefeh Keshavarzi Zafarghandi, Stefan Woltran*

- `2007.03581v1` - [abs](http://arxiv.org/abs/2007.03581v1) - [pdf](http://arxiv.org/pdf/2007.03581v1)

> Generalizing the attack structure in argumentation frameworks (AFs) has been studied in different ways. Most prominently, the binary attack relation of Dung frameworks has been extended to the notion of collective attacks. The resulting formalism is often termed SETAFs. Another approach is provided via abstract dialectical frameworks (ADFs), where acceptance conditions specify the relation between arguments; restricting these conditions naturally allows for so-called support-free ADFs. The aim of the paper is to shed light on the relation between these two different approaches. To this end, we investigate and compare the expressiveness of SETAFs and support-free ADFs under the lens of 3-valued semantics. Our results show that it is only the presence of unsatisfiable acceptance conditions in support-free ADFs that discriminate the two approaches.

</details>

<details>

<summary>2020-07-07 16:41:57 - A Frobenius Algebraic Analysis for Parasitic Gaps</summary>

- *Michael Moortgat, Mehrnoosh Sadrzadeh, Gijs Wijnholds*

- `2005.05639v2` - [abs](http://arxiv.org/abs/2005.05639v2) - [pdf](http://arxiv.org/pdf/2005.05639v2)

> The interpretation of parasitic gaps is an ostensible case of non-linearity in natural language composition. Existing categorial analyses, both in the typelogical and in the combinatory traditions, rely on explicit forms of syntactic copying. We identify two types of parasitic gapping where the duplication of semantic content can be confined to the lexicon. Parasitic gaps in adjuncts are analysed as forms of generalized coordination with a polymorphic type schema for the head of the adjunct phrase. For parasitic gaps affecting arguments of the same predicate, the polymorphism is associated with the lexical item that introduces the primary gap. Our analysis is formulated in terms of Lambek calculus extended with structural control modalities. A compositional translation relates syntactic types and derivations to the interpreting compact closed category of finite dimensional vector spaces and linear maps with Frobenius algebras over it. When interpreted over the necessary semantic spaces, the Frobenius algebras provide the tools to model the proposed instances of lexical polymorphism.

</details>

<details>

<summary>2020-07-08 08:33:42 - COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio Representations</summary>

- *Xavier Favory, Konstantinos Drossos, Tuomas Virtanen, Xavier Serra*

- `2006.08386v2` - [abs](http://arxiv.org/abs/2006.08386v2) - [pdf](http://arxiv.org/pdf/2006.08386v2)

> Audio representation learning based on deep neural networks (DNNs) emerged as an alternative approach to hand-crafted features. For achieving high performance, DNNs often need a large amount of annotated data which can be difficult and costly to obtain. In this paper, we propose a method for learning audio representations, aligning the learned latent representations of audio and associated tags. Aligning is done by maximizing the agreement of the latent representations of audio and tags, using a contrastive loss. The result is an audio embedding model which reflects acoustic and semantic characteristics of sounds. We evaluate the quality of our embedding model, measuring its performance as a feature extractor on three different tasks (namely, sound event recognition, and music genre and musical instrument classification), and investigate what type of characteristics the model captures. Our results are promising, sometimes in par with the state-of-the-art in the considered tasks and the embeddings produced with our method are well correlated with some acoustic descriptors.

</details>

<details>

<summary>2020-07-08 11:14:23 - Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion</summary>

- *Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, Jingsong Yu*

- `2007.04032v1` - [abs](http://arxiv.org/abs/2007.04032v1) - [pdf](http://arxiv.org/pdf/2007.04032v1)

> Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. Although several efforts have been made for CRS, two major issues still remain to be solved. First, the conversation data itself lacks of sufficient contextual information for accurately understanding users' preference. Second, there is a semantic gap between natural language expression and item-level user preference. To address these issues, we incorporate both word-oriented and entity-oriented knowledge graphs (KG) to enhance the data representations in CRSs, and adopt Mutual Information Maximization to align the word-level and entity-level semantic spaces. Based on the aligned semantic representations, we further develop a KG-enhanced recommender component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or entities in the response text. Extensive experiments have demonstrated the effectiveness of our approach in yielding better performance on both recommendation and conversation tasks.

</details>

<details>

<summary>2020-07-08 11:29:18 - A Data and Compute Efficient Design for Limited-Resources Deep Learning</summary>

- *Mirgahney Mohamed, Gabriele Cesa, Taco S. Cohen, Max Welling*

- `2004.09691v2` - [abs](http://arxiv.org/abs/2004.09691v2) - [pdf](http://arxiv.org/pdf/2004.09691v2)

> Thanks to their improved data efficiency, equivariant neural networks have gained increased interest in the deep learning community. They have been successfully applied in the medical domain where symmetries in the data can be effectively exploited to build more accurate and robust models. To be able to reach a much larger body of patients, mobile, on-device implementations of deep learning solutions have been developed for medical applications. However, equivariant models are commonly implemented using large and computationally expensive architectures, not suitable to run on mobile devices. In this work, we design and test an equivariant version of MobileNetV2 and further optimize it with model quantization to enable more efficient inference. We achieve close-to state of the art performance on the Patch Camelyon (PCam) medical dataset while being more computationally efficient.

</details>

<details>

<summary>2020-07-08 11:51:59 - Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation</summary>

- *Guolei Sun, Wenguan Wang, Jifeng Dai, Luc Van Gool*

- `2007.01947v2` - [abs](http://arxiv.org/abs/2007.01947v2) - [pdf](http://arxiv.org/pdf/2007.01947v2)

> This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the classifier discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually benefiting semantic segmentation learning. More essentially, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its efficacy and generalizability. Moreover, our approach ranked 1st place in the Weakly-Supervised Semantic Segmentation Track of CVPR2020 Learning from Imperfect Data Challenge.

</details>

<details>

<summary>2020-07-08 12:26:12 - Adaptive Transformers for Learning Multimodal Representations</summary>

- *Prajjwal Bhargava*

- `2005.07486v3` - [abs](http://arxiv.org/abs/2005.07486v3) - [pdf](http://arxiv.org/pdf/2005.07486v3)

> The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.

</details>

<details>

<summary>2020-07-08 15:59:14 - Dung's semantics satisfy attack removal monotonicity</summary>

- *Leila Amgoud, Srdjan Vesic*

- `2007.04221v1` - [abs](http://arxiv.org/abs/2007.04221v1) - [pdf](http://arxiv.org/pdf/2007.04221v1)

> We show that preferred, stable, complete, and grounded semantics satisfy attack removal monotonicity. This means that if an attack from b to a is removed, the status of a cannot worsen, e.g. if a was skeptically accepted, it cannot become rejected.

</details>

<details>

<summary>2020-07-09 07:03:17 - Monocular Vision based Crowdsourced 3D Traffic Sign Positioning with Unknown Camera Intrinsics and Distortion Coefficients</summary>

- *Hemang Chawla, Matti Jukola, Elahe Arani, Bahram Zonooz*

- `2007.04592v1` - [abs](http://arxiv.org/abs/2007.04592v1) - [pdf](http://arxiv.org/pdf/2007.04592v1)

> Autonomous vehicles and driver assistance systems utilize maps of 3D semantic landmarks for improved decision making. However, scaling the mapping process as well as regularly updating such maps come with a huge cost. Crowdsourced mapping of these landmarks such as traffic sign positions provides an appealing alternative. The state-of-the-art approaches to crowdsourced mapping use ground truth camera parameters, which may not always be known or may change over time. In this work, we demonstrate an approach to computing 3D traffic sign positions without knowing the camera focal lengths, principal point, and distortion coefficients a priori. We validate our proposed approach on a public dataset of traffic signs in KITTI. Using only a monocular color camera and GPS, we achieve an average single journey relative and absolute positioning accuracy of 0.26 m and 1.38 m, respectively.

</details>

<details>

<summary>2020-07-09 15:14:19 - Hierarchical nucleation in deep neural networks</summary>

- *Diego Doimo, Aldo Glielmo, Alessio Ansuini, Alessandro Laio*

- `2007.03506v2` - [abs](http://arxiv.org/abs/2007.03506v2) - [pdf](http://arxiv.org/pdf/2007.03506v2)

> Deep convolutional networks (DCNs) learn meaningful representations where data that share the same abstract characteristics are positioned closer and closer. Understanding these representations and how they are generated is of unquestioned practical and theoretical interest. In this work we study the evolution of the probability density of the ImageNet dataset across the hidden layers in some state-of-the-art DCNs. We find that the initial layers generate a unimodal probability density getting rid of any structure irrelevant for classification. In subsequent layers density peaks arise in a hierarchical fashion that mirrors the semantic hierarchy of the concepts. Density peaks corresponding to single categories appear only close to the output and via a very sharp transition which resembles the nucleation process of a heterogeneous liquid. This process leaves a footprint in the probability density of the output layer where the topography of the peaks allows reconstructing the semantic relationships of the categories.

</details>

<details>

<summary>2020-07-09 15:21:28 - Invertible Zero-Shot Recognition Flows</summary>

- *Yuming Shen, Jie Qin, Lei Huang*

- `2007.04873v1` - [abs](http://arxiv.org/abs/2007.04873v1) - [pdf](http://arxiv.org/pdf/2007.04873v1)

> Deep generative models have been successfully applied to Zero-Shot Learning (ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the hardness of training with ZSL-oriented regularizers and the limited generation quality) hinder the existing generative ZSL models from fully bypassing the seen-unseen bias. To tackle the above limitations, for the first time, this work incorporates a new family of generative models (i.e., flow-based models) into ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data embeddings (i.e., the semantic factors and the non-semantic ones) with the forward pass of an invertible flow network, while the reverse pass generates data samples. This procedure theoretically extends conventional generative flows to a factorized conditional scheme. To explicitly solve the bias problem, our model enlarges the seen-unseen distributional discrepancy based on negative sample-based distance measurement. Notably, IZF works flexibly with either a naive Bayesian classifier or a held-out trainable one for zero-shot recognition. Experiments on widely-adopted ZSL benchmarks demonstrate the significant performance gain of IZF over existing methods, in both classic and generalized settings.

</details>

<details>

<summary>2020-07-09 17:18:26 - Technical Report of "Deductive Joint Support for Rational Unrestricted Rebuttal"</summary>

- *Marcos Cramer, Meghna Bhadra*

- `2005.03620v2` - [abs](http://arxiv.org/abs/2005.03620v2) - [pdf](http://arxiv.org/pdf/2005.03620v2)

> In ASPIC-style structured argumentation an argument can rebut another argument by attacking its conclusion. Two ways of formalizing rebuttal have been proposed: In restricted rebuttal, the attacked conclusion must have been arrived at with a defeasible rule, whereas in unrestricted rebuttal, it may have been arrived at with a strict rule, as long as at least one of the antecedents of this strict rule was already defeasible. One systematic way of choosing between various possible definitions of a framework for structured argumentation is to study what rationality postulates are satisfied by which definition, for example whether the closure postulate holds, i.e. whether the accepted conclusions are closed under strict rules. While having some benefits, the proposal to use unrestricted rebuttal faces the problem that the closure postulate only holds for the grounded semantics but fails when other argumentation semantics are applied, whereas with restricted rebuttal the closure postulate always holds. In this paper we propose that ASPIC-style argumentation can benefit from keeping track not only of the attack relation between arguments, but also the relation of deductive joint support that holds between a set of arguments and an argument that was constructed from that set using a strict rule. By taking this deductive joint support relation into account while determining the extensions, the closure postulate holds with unrestricted rebuttal under all admissibility-based semantics. We define the semantics of deductive joint support through the flattening method.

</details>

<details>

<summary>2020-07-09 22:41:33 - Density Matrices with Metric for Derivational Ambiguity</summary>

- *Adriana D. Correia, Michael Moortgat, Henk T. C. Stoof*

- `1908.07347v3` - [abs](http://arxiv.org/abs/1908.07347v3) - [pdf](http://arxiv.org/pdf/1908.07347v3)

> Recent work on vector-based compositional natural language semantics has proposed the use of density matrices to model lexical ambiguity and (graded) entailment (e.g. Piedeleu et al 2015, Bankova et al 2019, Sadrzadeh et al 2018). Ambiguous word meanings, in this work, are represented as mixed states, and the compositional interpretation of phrases out of their constituent parts takes the form of a strongly monoidal functor sending the derivational morphisms of a pregroup syntax to linear maps in FdHilb. Our aims in this paper are threefold. Firstly, we replace the pregroup front end by a Lambek categorial grammar with directional implications expressing a word's selectional requirements. By the Curry-Howard correspondence, the derivations of the grammar's type logic are associated with terms of the (ordered) linear lambda calculus; these terms can be read as programs for compositional meaning assembly with density matrices as the target semantic spaces. Secondly, we extend on the existing literature and introduce a symmetric, nondegenerate bilinear form called a "metric" that defines a canonical isomorphism between a vector space and its dual, allowing us to keep a distinction between left and right implication. Thirdly, we use this metric to define density matrix spaces in a directional form, modeling the ubiquitous derivational ambiguity of natural language syntax, and show how this alows an integrated treatment of lexical and derivational forms of ambiguity controlled at the level of the interpretation.

</details>

<details>

<summary>2020-07-10 02:32:25 - Multi-Granularity Modularized Network for Abstract Visual Reasoning</summary>

- *Xiangru Tang, Haoyuan Wang, Xiang Pan, Jiyang Qi*

- `2007.04670v2` - [abs](http://arxiv.org/abs/2007.04670v2) - [pdf](http://arxiv.org/pdf/2007.04670v2)

> Abstract visual reasoning connects mental abilities to the physical world, which is a crucial factor in cognitive development. Most toddlers display sensitivity to this skill, but it is not easy for machines. Aimed at it, we focus on the Raven Progressive Matrices Test, designed to measure cognitive reasoning. Recent work designed some black-boxes to solve it in an end-to-end fashion, but they are incredibly complicated and difficult to explain. Inspired by cognitive studies, we propose a Multi-Granularity Modularized Network (MMoN) to bridge the gap between the processing of raw sensory information and symbolic reasoning. Specifically, it learns modularized reasoning functions to model the semantic rule from the visual grounding in a neuro-symbolic and semi-supervision way. To comprehensively evaluate MMoN, our experiments are conducted on the dataset of both seen and unseen reasoning rules. The result shows that MMoN is well suited for abstract visual reasoning and also explainable on the generalization test.

</details>

<details>

<summary>2020-07-10 04:24:55 - Automatic Web Service Composition -- Models, Complexity and Applications</summary>

- *Paul Diac*

- `2007.03896v2` - [abs](http://arxiv.org/abs/2007.03896v2) - [pdf](http://arxiv.org/pdf/2007.03896v2)

> The automatic composition of web services refers to how services can be used in a complex and aggregate manner, to serve a specific and known functionality. Given a list of services described by the input and output parameters, and a request of a similar structure: the initially known and required parameters; a solution can be designed to automatically search for the set of web services that satisfy the request, under certain constraints. We first propose two very efficient algorithms that solve the problem of the automatic composition of the web services as it was formulated in the competitions organized in 2005 and 2008. The algorithms obtain much better results than the rest of the participants with respect to execution time and even composition size. Evaluation consists of running the previous and the proposed solutions on given benchmarks and generated tests. Further, we design two new models to match service's parameters, extending the semantic expressiveness of the 2008 challenge. The initial goal is to resolve some simple and practical use-cases that cannot be expressed in the previous models. We also adhere to modern service description languages, like OpenAPI and especially schema.org. Algorithms for the new models can solve instances of significant size. Addressing a wider and more realistic perspective, we define the online version of the composition problem. In this regard, we consider that web services and compositions requests can be added and removed in real-time, and the system must handle such operations on the fly. It is necessary to maintain the workflows for users who actively run the compositions over time. As for the new semantic models, we propose new algorithms and provide comprehensive evaluation by generating test cases that simulate all corner cases.

</details>

<details>

<summary>2020-07-10 21:35:17 - GloVeInit at SemEval-2020 Task 1: Using GloVe Vector Initialization for Unsupervised Lexical Semantic Change Detection</summary>

- *Vaibhav Jain*

- `2007.05618v1` - [abs](http://arxiv.org/abs/2007.05618v1) - [pdf](http://arxiv.org/pdf/2007.05618v1)

> This paper presents a vector initialization approach for the SemEval2020 Task 1: Unsupervised Lexical Semantic Change Detection. Given two corpora belonging to different time periods and a set of target words, this task requires us to classify whether a word gained or lost a sense over time (subtask 1) and to rank them on the basis of the changes in their word senses (subtask 2). The proposed approach is based on using Vector Initialization method to align GloVe embeddings. The idea is to consecutively train GloVe embeddings for both corpora, while using the first model to initialize the second one. This paper is based on the hypothesis that GloVe embeddings are more suited for the Vector Initialization method than SGNS embeddings. It presents an intuitive reasoning behind this hypothesis, and also talks about the impact of various factors and hyperparameters on the performance of the proposed approach. Our model ranks 13th and 10th among 33 teams in the two subtasks. The implementation has been shared publicly.

</details>

<details>

<summary>2020-07-10 22:25:27 - Learning Accurate and Human-Like Driving using Semantic Maps and Attention</summary>

- *Simon Hecker, Dengxin Dai, Alexander Liniger, Luc Van Gool*

- `2007.07218v1` - [abs](http://arxiv.org/abs/2007.07218v1) - [pdf](http://arxiv.org/pdf/2007.07218v1)

> This paper investigates how end-to-end driving models can be improved to drive more accurately and human-like. To tackle the first issue we exploit semantic and visual maps from HERE Technologies and augment the existing Drive360 dataset with such. The maps are used in an attention mechanism that promotes segmentation confidence masks, thus focusing the network on semantic classes in the image that are important for the current driving situation. Human-like driving is achieved using adversarial learning, by not only minimizing the imitation loss with respect to the human driver but by further defining a discriminator, that forces the driving model to produce action sequences that are human-like. Our models are trained and evaluated on the Drive360 + HERE dataset, which features 60 hours and 3000 km of real-world driving data. Extensive experiments show that our driving models are more accurate and behave more human-like than previous methods.

</details>

<details>

<summary>2020-07-11 04:40:46 - Towards a Near Universal Time Series Data Mining Tool: Introducing the Matrix Profile</summary>

- *Chin-Chia Michael Yeh*

- `1811.03064v2` - [abs](http://arxiv.org/abs/1811.03064v2) - [pdf](http://arxiv.org/pdf/1811.03064v2)

> The last decade has seen a flurry of research on all-pairs-similarity-search (or, self-join) for text, DNA, and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. Surprisingly, however, little progress has been made on addressing this problem for time series subsequences. In this thesis, we have introduced a near universal time series data mining tool called matrix profile which solves the all-pairs-similarity-search problem and caches the output in an easy-to-access fashion. The proposed algorithm is not only parameter-free, exact and scalable, but also applicable for both single and multidimensional time series. By building time series data mining methods on top of matrix profile, many time series data mining tasks (e.g., motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering) can be efficiently solved. Because the same matrix profile can be shared by a diverse set of time series data mining methods, matrix profile is versatile and computed-once-use-many-times data structure. We demonstrate the utility of matrix profile for many time series data mining problems, including motif discovery, discord discovery, weakly labeled time series classification, and representation learning on domains as diverse as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We hope the matrix profile is not the end but the beginning of many more time series data mining projects.

</details>

<details>

<summary>2020-07-11 16:26:16 - Nodule2vec: a 3D Deep Learning System for Pulmonary Nodule Retrieval Using Semantic Representation</summary>

- *Ilia Kravets, Tal Heletz, Hayit Greenspan*

- `2007.07081v1` - [abs](http://arxiv.org/abs/2007.07081v1) - [pdf](http://arxiv.org/pdf/2007.07081v1)

> Content-based retrieval supports a radiologist decision making process by presenting the doctor the most similar cases from the database containing both historical diagnosis and further disease development history. We present a deep learning system that transforms a 3D image of a pulmonary nodule from a CT scan into a low-dimensional embedding vector. We demonstrate that such a vector representation preserves semantic information about the nodule and offers a viable approach for content-based image retrieval (CBIR). We discuss the theoretical limitations of the available datasets and overcome them by applying transfer learning of the state-of-the-art lung nodule detection model. We evaluate the system using the LIDC-IDRI dataset of thoracic CT scans. We devise a similarity score and show that it can be utilized to measure similarity 1) between annotations of the same nodule by different radiologists and 2) between the query nodule and the top four CBIR results. A comparison between doctors and algorithm scores suggests that the benefit provided by the system to the radiologist end-user is comparable to obtaining a second radiologist's opinion.

</details>

<details>

<summary>2020-07-11 19:47:47 - Open Domain Suggestion Mining Leveraging Fine-Grained Analysis</summary>

- *Shreya Singal, Tanishq Goel, Shivang Chopra, Sonika Dahiya*

- `2007.04297v2` - [abs](http://arxiv.org/abs/2007.04297v2) - [pdf](http://arxiv.org/pdf/2007.04297v2)

> Suggestion mining tasks are often semantically complex and lack sophisticated methodologies that can be applied to real-world data. The presence of suggestions across a large diversity of domains and the absence of large labelled and balanced datasets render this task particularly challenging to deal with. In an attempt to overcome these challenges, we propose a two-tier pipeline that leverages Discourse Marker based oversampling and fine-grained suggestion mining techniques to retrieve suggestions from online forums. Through extensive comparison on a real-world open-domain suggestion dataset, we demonstrate how the oversampling technique combined with transformer based fine-grained analysis can beat the state of the art. Additionally, we perform extensive qualitative and qualitative analysis to give construct validity to our proposed pipeline. Finally, we discuss the practical, computational and reproducibility aspects of the deployment of our pipeline across the web.

</details>

<details>

<summary>2020-07-12 08:51:07 - Unsupervised Keyphrase Extraction from Scientific Publications</summary>

- *Eirini Papagiannopoulou, Grigorios Tsoumakas*

- `1808.03712v3` - [abs](http://arxiv.org/abs/1808.03712v3) - [pdf](http://arxiv.org/pdf/1808.03712v3)

> We propose a novel unsupervised keyphrase extraction approach that filters candidate keywords using outlier detection. It starts by training word embeddings on the target document to capture semantic regularities among the words. It then uses the minimum covariance determinant estimator to model the distribution of non-keyphrase word vectors, under the assumption that these vectors come from the same distribution, indicative of their irrelevance to the semantics expressed by the dimensions of the learned vector representation. Candidate keyphrases only consist of words that are detected as outliers of this dominant distribution. Empirical results show that our approach outperforms state-of-the-art and recent unsupervised keyphrase extraction methods.

</details>

<details>

<summary>2020-07-12 10:42:12 - A Speech Act Classifier for Persian Texts and its Application in Identifying Rumors</summary>

- *Zoleikha Jahanbakhsh-Nagadeh, Mohammad-Reza Feizi-Derakhshi, Arash Sharifi*

- `1901.03904v4` - [abs](http://arxiv.org/abs/1901.03904v4) - [pdf](http://arxiv.org/pdf/1901.03904v4)

> Speech Acts (SAs) are one of the important areas of pragmatics, which give us a better understanding of the state of mind of the people and convey an intended language function. Knowledge of the SA of a text can be helpful in analyzing that text in natural language processing applications. This study presents a dictionary-based statistical technique for Persian SA recognition. The proposed technique classifies a text into seven classes of SA based on four criteria: lexical, syntactic, semantic, and surface features. WordNet as the tool for extracting synonym and enriching features dictionary is utilized. To evaluate the proposed technique, we utilized four classification methods including Random Forest (RF), Support Vector Machine (SVM), Naive Bayes (NB), and K-Nearest Neighbors (KNN). The experimental results demonstrate that the proposed method using RF and SVM as the best classifiers achieved a state-of-the-art performance with an accuracy of 0.95 for classification of Persian SAs. Our original vision of this work is introducing an application of SA recognition on social media content, especially the common SA in rumors. Therefore, the proposed system utilized to determine the common SAs in rumors. The results showed that Persian rumors are often expressed in three SA classes including narrative, question, and threat, and in some cases with the request SA.

</details>

<details>

<summary>2020-07-12 12:59:40 - A mathematical model for universal semantics</summary>

- *Weinan E, Yajun Zhou*

- `1907.12293v7` - [abs](http://arxiv.org/abs/1907.12293v7) - [pdf](http://arxiv.org/pdf/1907.12293v7)

> We characterize the meaning of words with language-independent numerical fingerprints, through a mathematical analysis of recurring patterns in texts. Approximating texts by Markov processes on a long-range time scale, we are able to extract topics, discover synonyms, and sketch semantic fields from a particular document of moderate length, without consulting external knowledge-base or thesaurus. Our Markov semantic model allows us to represent each topical concept by a low-dimensional vector, interpretable as algebraic invariants in succinct statistical operations on the document, targeting local environments of individual words. These language-independent semantic representations enable a robot reader to both understand short texts in a given language (automated question-answering) and match medium-length texts across different languages (automated word translation). Our semantic fingerprints quantify local meaning of words in 14 representative languages across 5 major language families, suggesting a universal and cost-effective mechanism by which human languages are processed at the semantic level. Our protocols and source codes are publicly available on https://github.com/yajun-zhou/linguae-naturalis-principia-mathematica

</details>

<details>

<summary>2020-07-12 15:32:16 - Attention or memory? Neurointerpretable agents in space and time</summary>

- *Lennart Bramlage, Aurelio Cortese*

- `2007.04862v2` - [abs](http://arxiv.org/abs/2007.04862v2) - [pdf](http://arxiv.org/pdf/2007.04862v2)

> In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) processes. This interaction is thought to support dimensionality reduction of task representations, restricting computations to relevant features. However, it remains unclear whether these properties can translate into real algorithmic advantages for artificial agents, especially in dynamic environments. We design a model incorporating a self-attention mechanism that implements task-state representations in semantic feature-space, and test it on a battery of Atari games. To evaluate the agent's selective properties, we add a large volume of task-irrelevant features to observations. In line with neuroscience predictions, self-attention leads to increased robustness to noise compared to benchmark models. Strikingly, this self-attention mechanism is general enough, such that it can be naturally extended to implement a transient working-memory, able to solve a partially observable maze task. Lastly, we highlight the predictive quality of attended stimuli. Because we use semantic observations, we can uncover not only which features the agent elects to base decisions on, but also how it chooses to compile more complex, relational features from simpler ones. These results formally illustrate the benefits of attention in deep RL and provide evidence for the interpretability of self-attention mechanisms.

</details>

<details>

<summary>2020-07-12 19:54:32 - Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation</summary>

- *Aditya Mogadala, Marius Mosbach, Dietrich Klakow*

- `2007.06077v1` - [abs](http://arxiv.org/abs/2007.06077v1) - [pdf](http://arxiv.org/pdf/2007.06077v1)

> Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.

</details>

<details>

<summary>2020-07-12 23:49:12 - Explainable Recommendation via Interpretable Feature Mapping and Evaluation of Explainability</summary>

- *Deng Pan, Xiangrui Li, Xin Li, Dongxiao Zhu*

- `2007.06133v1` - [abs](http://arxiv.org/abs/2007.06133v1) - [pdf](http://arxiv.org/pdf/2007.06133v1)

> Latent factor collaborative filtering (CF) has been a widely used technique for recommender system by learning the semantic representations of users and items. Recently, explainable recommendation has attracted much attention from research community. However, trade-off exists between explainability and performance of the recommendation where metadata is often needed to alleviate the dilemma. We present a novel feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features, achieving both satisfactory accuracy and explainability in the recommendations by simultaneous minimization of rating prediction loss and interpretation loss. To evaluate the explainability, we propose two new evaluation metrics specifically designed for aspect-level explanation using surrogate ground truth. Experimental results demonstrate a strong performance in both recommendation and explaining explanation, eliminating the need for metadata. Code is available from https://github.com/pd90506/AMCF.

</details>

<details>

<summary>2020-07-13 03:26:44 - Broadening Label-based Argumentation Semantics with May-Must Scales (May-Must Argumentation)</summary>

- *Ryuta Arisaka, Takayuki Ito*

- `2001.05730v3` - [abs](http://arxiv.org/abs/2001.05730v3) - [pdf](http://arxiv.org/pdf/2001.05730v3)

> The semantics as to which set of arguments in a given argumentation graph may be acceptable (acceptability semantics) can be characterised in a few different ways. Among them, labelling-based approach allows for concise and flexible determination of acceptability statuses of arguments through assignment of a label indicating acceptance, rejection, or undecided to each argument. In this work, we contemplate a way of broadening it by accommodating may- and must- conditions for an argument to be accepted or rejected, as determined by the number(s) of rejected and accepted attacking arguments. We show that the broadened label-based semantics can be used to express more mild indeterminacy than inconsistency for acceptability judgement when, for example, it may be the case that an argument is accepted and when it may also be the case that it is rejected. We identify that finding which conditions a labelling satisfies for every argument can be an undecidable problem, which has an unfavourable implication to existence of a semantics. We propose to address this problem by enforcing a labelling to maximally respect the conditions, while keeping the rest that would necessarily cause non-termination labelled undecided. Several semantics will be presented and the relation among them will be noted. Towards the end, we will touch upon possible research directions that can be pursued further.

</details>

<details>

<summary>2020-07-13 08:49:00 - Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification</summary>

- *Yucan Zhou, Yu Wang, Jianfei Cai, Yu Zhou, Qinghua Hu, Weiping Wang*

- `2007.06240v1` - [abs](http://arxiv.org/abs/2007.06240v1) - [pdf](http://arxiv.org/pdf/2007.06240v1)

> Deep neural networks are highly effective when a large number of labeled samples are available but fail with few-shot classification tasks. Recently, meta-learning methods have received much attention, which train a meta-learner on massive additional tasks to gain the knowledge to instruct the few-shot classification. Usually, the training tasks are randomly sampled and performed indiscriminately, often making the meta-learner stuck into a bad local optimum. Some works in the optimization of deep neural networks have shown that a better arrangement of training data can make the classifier converge faster and perform better. Inspired by this idea, we propose an easy-to-hard expert meta-training strategy to arrange the training tasks properly, where easy tasks are preferred in the first phase, then, hard tasks are emphasized in the second phase. A task hardness aware module is designed and integrated into the training procedure to estimate the hardness of a task based on the distinguishability of its categories. In addition, we explore multiple hardness measurements including the semantic relation, the pairwise Euclidean distance, the Hausdorff distance, and the Hilbert-Schmidt independence criterion. Experimental results on the miniImageNet and tieredImageNetSketch datasets show that the meta-learners can obtain better results with our expert training strategy.

</details>

<details>

<summary>2020-07-13 10:20:58 - Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing</summary>

- *Piyush Yadav, Dhaval Salwala, Edward Curry*

- `2007.06292v1` - [abs](http://arxiv.org/abs/2007.06292v1) - [pdf](http://arxiv.org/pdf/2007.06292v1)

> Complex Event Processing (CEP) is an event processing paradigm to perform real-time analytics over streaming data and match high-level event patterns. Presently, CEP is limited to process structured data stream. Video streams are complicated due to their unstructured data model and limit CEP systems to perform matching over them. This work introduces a graph-based structure for continuous evolving video streams, which enables the CEP system to query complex video event patterns. We propose the Video Event Knowledge Graph (VEKG), a graph driven representation of video data. VEKG models video objects as nodes and their relationship interaction as edges over time and space. It creates a semantic knowledge representation of video data derived from the detection of high-level semantic concepts from the video using an ensemble of deep learning models. A CEP-based state optimization - VEKG-Time Aggregated Graph (VEKG-TAG) is proposed over VEKG representation for faster event detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides a summarized view of the VEKG graph over a given time length. We defined a set of nine event pattern rules for two domains (Activity Recognition and Traffic Management), which act as a query and applied over VEKG graphs to discover complex event patterns. To show the efficacy of our approach, we performed extensive experiments over 801 video clips across 10 datasets. The proposed VEKG approach was compared with other state-of-the-art methods and was able to detect complex event patterns over videos with F-Score ranging from 0.44 to 0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.

</details>

<details>

<summary>2020-07-13 18:53:45 - DERauth: A Battery-based Authentication Scheme for Distributed Energy Resources</summary>

- *Ioannis Zografopoulos, Charalambos Konstantinou*

- `2007.06625v1` - [abs](http://arxiv.org/abs/2007.06625v1) - [pdf](http://arxiv.org/pdf/2007.06625v1)

> Over the past decades, power systems have experienced drastic transformations in order to address the growth in energy demand, reduce carbon emissions, and enhance power quality and energy efficiency. This shift to the smart grid concept involves, among others, the utilization of distributed energy resources (DERs) such as rooftop solar panels and storage systems, contributing towards grid decentralization while improving control over power generation. In order to seamlessly integrate DERs into power systems, embedded devices are used to support the communication and control functions of DERs. As a result, vulnerabilities of such components can be ported to the industrial environment. Insecure control networks and protocols further exacerbate the problem. Towards reducing the attack surface, we present an authentication scheme for DERs, DERauth, which leverages the inherent entropy of the DER battery energy storage system (BESS) as a root-of-trust. The DER authentication is achieved using a challenge-reply mechanism that relies on the corresponding DER's BESS state-of-charge (SoC) and voltage measurements. A dynamically updating process ensures that the BESS state is up-to-date. We evaluate our proof-of-concept in a prototype development that uses lithium-ion (li-ion) batteries for the BESS. The robustness of our design is assessed against modeling attacks performed by neural networks.

</details>

<details>

<summary>2020-07-14 08:52:55 - Questionnaire analysis to define the most suitable survey for port-noise investigation</summary>

- *Andrea Cerniglia, Davide Chiarella, Paola Cutugno, Lucia Marconi, Anna Magrini, Gelsomina Di Feo, Melissa Ferretti*

- `2007.06915v1` - [abs](http://arxiv.org/abs/2007.06915v1) - [pdf](http://arxiv.org/pdf/2007.06915v1)

> The high level of noise pollution affecting the areas between ports and logistic platforms represents a problem that can be faced from different points of view. Acoustic monitoring, mapping, short-term measurements, port and road traffic flows analyses can give useful indications on the strategies to be proposed for a better management of the problem. A survey campaign through the preparation of questionnaires to be submitted to the population exposed to noise in the back-port areas will help to better understand the subjective point of view. The paper analyses a sample of questions suitable for the specific research, chosen as part of the wide database of questionnaires internationally proposed for subjective investigations. The preliminary results of a first data collection campaign are considered to verify the adequacy of the number, the type of questions, and the type of sample noise used for the survey. The questionnaire will be optimized to be distributed in the TRIPLO project (TRansports and Innovative sustainable connections between Ports and LOgistic platforms). The results of this survey will be the starting point for the linguistic investigation carried out in combination with the acoustic monitoring, to improve understanding the connections between personal feeling and technical aspects.

</details>

<details>

<summary>2020-07-14 10:30:36 - Structure-Invariant Testing for Machine Translation</summary>

- *Pinjia He, Clara Meister, Zhendong Su*

- `1907.08710v3` - [abs](http://arxiv.org/abs/1907.08710v3) - [pdf](http://arxiv.org/pdf/1907.08710v3)

> In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.   To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of "similar" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5\% and 70\% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.

</details>

<details>

<summary>2020-07-14 15:57:07 - Modeling the Semantics of States and State Machines</summary>

- *Sabah Al-Fedaghi*

- `2007.07138v1` - [abs](http://arxiv.org/abs/2007.07138v1) - [pdf](http://arxiv.org/pdf/2007.07138v1)

> A system s behavior is typically specified through models such as state diagrams that describe how the system should behave. According to researchers, it is not clear what a state actually represents regarding the system to be modeled. Standards do not provide adequate definitions of or sufficient guidance on the use of states. Studies show these inconsistencies can lead to poor or incomplete specifications, which in turn could result in project delays or increase the cost of the system design. This paper aims to establish a precise definition of the notion of states and state machines, a goal motivated by system modelers (e.g., requirement engineers) need to understand key concepts and vocabulary such as states and state machine, which are major behavioral modeling tools (e.g., in UML). State is the main notion of a state machine in which events drive state changes. This raises questions about the nature of these state-related notations. The semantics of these concepts is based on a new modeling methodology called the thinging machine applied to a number of examples of existing models. The thinging machine semantics is founded on five elementary actions that divide the static model into changes/states upon which events are defined.

</details>

<details>

<summary>2020-07-14 17:53:01 - Intelligent requirements engineering from natural language and their chaining toward CAD models</summary>

- *Alain-Jérôme Fougères, Egon Ostrosi*

- `2007.07825v1` - [abs](http://arxiv.org/abs/2007.07825v1) - [pdf](http://arxiv.org/pdf/2007.07825v1)

> This paper assumes that design language plays an important role in how designers design and on the creativity of designers. Designers use and develop models as an aid to thinking, a focus for discussion and decision-making and a means of evaluating the reliability of the proposals. This paper proposes an intelligent method for requirements engineering from natural language and their chaining toward CAD models. The transition from linguistic analysis to the representation of engineering requirements consists of the translation of the syntactic structure into semantic form represented by conceptual graphs. Based on the isomorphism between conceptual graphs and predicate logic, a formal language of the specification is proposed. The outcome of this language is chained and translated in Computer Aided Three-Dimensional Interactive Application (CATIA) models. The tool (EGEON: Engineering desiGn sEmantics elabOration and applicatioN) is developed to represent the semantic network of engineering requirements. A case study on the design of a car door hinge is presented to illustrates the proposed method.

</details>

<details>

<summary>2020-07-14 18:29:49 - Using Holographically Compressed Embeddings in Question Answering</summary>

- *Salvador E. Barbosa*

- `2007.07287v1` - [abs](http://arxiv.org/abs/2007.07287v1) - [pdf](http://arxiv.org/pdf/2007.07287v1)

> Word vector representations are central to deep learning natural language processing models. Many forms of these vectors, known as embeddings, exist, including word2vec and GloVe. Embeddings are trained on large corpora and learn the word's usage in context, capturing the semantic relationship between words. However, the semantics from such training are at the level of distinct words (known as word types), and can be ambiguous when, for example, a word type can be either a noun or a verb. In question answering, parts-of-speech and named entity types are important, but encoding these attributes in neural models expands the size of the input. This research employs holographic compression of pre-trained embeddings, to represent a token, its part-of-speech, and named entity type, in the same dimension as representing only the token. The implementation, in a modified question answering recurrent deep learning network, shows that semantic relationships are preserved, and yields strong performance.

</details>

<details>

<summary>2020-07-14 23:31:14 - Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics</summary>

- *Vinay V. Ramasesh, Ethan Dyer, Maithra Raghu*

- `2007.07400v1` - [abs](http://arxiv.org/abs/2007.07400v1) - [pdf](http://arxiv.org/pdf/2007.07400v1)

> A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.

</details>

<details>

<summary>2020-07-15 01:48:03 - Data Sampling on MDS-resistant 10th Generation Intel Core (Ice Lake)</summary>

- *Daniel Moghimi*

- `2007.07428v1` - [abs](http://arxiv.org/abs/2007.07428v1) - [pdf](http://arxiv.org/pdf/2007.07428v1)

> Microarchitectural Data Sampling (MDS) is a set of hardware vulnerabilities in Intel CPUs that allows an attacker to leak bytes of data from memory loads and stores across various security boundaries. On affected CPUs, some of these vulnerabilities were patched via microcode updates. Additionally, Intel announced that the newest microarchitectures, namely Cascade Lake and Ice Lake, were not affected by MDS. While Cascade Lake turned out to be vulnerable to the ZombieLoad v2 MDS attack (also known as TAA), Ice Lake was not affected by this attack.   In this technical report, we show a variant of MSBDS (CVE2018-12126), an MDS attack, also known as Fallout, that works on Ice Lake CPUs. This variant was automatically synthesized using Transynther, a tool to find new variants of Meltdown-type attacks. Based on the findings of Transynther, we analyze different microcodes regarding this issue, showing that only microcode versions after January 2020 prevent exploitation of the vulnerability. These results show that Transynther is a valuable tool to find new variants, and also to test for regressions possibly introduced with microcode updates.

</details>

<details>

<summary>2020-07-15 08:04:14 - Bitcoin Trace-Net: Formal Contract Verification at Signing Time</summary>

- *James Chiang*

- `2007.07528v1` - [abs](http://arxiv.org/abs/2007.07528v1) - [pdf](http://arxiv.org/pdf/2007.07528v1)

> Smart contracting protocols promise to regulate the transfer of cryptocurrency amongst participants in a trustless manner. A safe smart contract implementation should ensure that each participant can always append a contract transaction to the blockchain in order move the contract towards secure completion. To this goal, we propose Bitcoin Trace-Net, a contract verification framework which generates an executable symbolic model from the underlying contract implementation. A Trace-Net model consists of a Petri Net formalism enriched with a Dolev-Yao-like actor knowledge model. The explicit symbolic actor knowledge model supports the verification of contracts featuring cryptographic sub-protocols, which may not be observable on the blockchain. Trace-Net is sufficiently expressive to accurately model blockchain semantics such as the delay between a transaction broadcast and its subsequent confirmation, as well as adversarial blockchain reorganizations of finite depths, both of which can break smart contract safety. As an implementation level framework, Trace-Net can be instantiated at run-time to monitor and verify smart contract protocol executions.

</details>

<details>

<summary>2020-07-15 10:57:38 - Confidence Regularized Self-Training</summary>

- *Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V. K. Vijaya Kumar, Jinsong Wang*

- `1908.09822v3` - [abs](http://arxiv.org/abs/1908.09822v3) - [pdf](http://arxiv.org/pdf/1908.09822v3)

> Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST.

</details>

<details>

<summary>2020-07-15 11:29:25 - Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision</summary>

- *Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, In So Kweon*

- `2004.07703v4` - [abs](http://arxiv.org/abs/2004.07703v4) - [pdf](http://arxiv.org/pdf/2004.07703v4)

> Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model; from this adaptation, we separate the target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard split. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.

</details>

<details>

<summary>2020-07-15 13:01:44 - Logic Constrained Pointer Networks for Interpretable Textual Similarity</summary>

- *Subhadeep Maji, Rohan Kumar, Manish Bansal, Kalyani Roy, Pawan Goyal*

- `2007.07670v1` - [abs](http://arxiv.org/abs/2007.07670v1) - [pdf](http://arxiv.org/pdf/2007.07670v1)

> Systematically discovering semantic relationships in text is an important and extensively studied area in Natural Language Processing, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence-level scores via subsequence alignments has been proposed as a way to make models more interpretable. We study the problem of aligning components of sentences leading to an interpretable model for semantic textual similarity. In this paper, we introduce a novel pointer network based model with a sentinel gating function to align constituent chunks, which are represented using BERT. We improve this base model with a loss function to equally penalize misalignments in both sentences, ensuring the alignments are bidirectional. Finally, to guide the network with structured external knowledge, we introduce first-order logic constraints based on ConceptNet and syntactic knowledge. The model achieves an F1 score of 97.73 and 96.32 on the benchmark SemEval datasets for the chunk alignment task, showing large improvements over the existing solutions. Source code is available at https://github.com/manishb89/interpretable_sentence_similarity

</details>

<details>

<summary>2020-07-15 14:17:02 - Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources</summary>

- *Qianhui Wu, Zijia Lin, Guoxin Wang, Hui Chen, Börje F. Karlsson, Biqing Huang, Chin-Yew Lin*

- `1911.06161v2` - [abs](http://arxiv.org/abs/1911.06161v2) - [pdf](http://arxiv.org/pdf/1911.06161v2)

> For languages with no annotated resources, transferring knowledge from rich-resource languages is an effective solution for named entity recognition (NER). While all existing methods directly transfer from source-learned model to a target language, in this paper, we propose to fine-tune the learned model with a few similar examples given a test case, which could benefit the prediction by leveraging the structural and semantic information conveyed in such similar examples. To this end, we present a meta-learning algorithm to find a good model parameter initialization that could fast adapt to the given test case and propose to construct multiple pseudo-NER tasks for meta-training by computing sentence similarities. To further improve the model's generalization ability across different languages, we introduce a masking scheme and augment the loss function with an additional maximum term during meta-training. We conduct extensive experiments on cross-lingual named entity recognition with minimal resources over five target languages. The results show that our approach significantly outperforms existing state-of-the-art methods across the board.

</details>

<details>

<summary>2020-07-15 17:43:03 - GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension</summary>

- *Yu Chen, Lingfei Wu, Mohammed J. Zaki*

- `1908.00059v2` - [abs](http://arxiv.org/abs/1908.00059v2) - [pdf](http://arxiv.org/pdf/1908.00059v2)

> Conversational machine comprehension (MC) has proven significantly more challenging compared to traditional MC since it requires better utilization of conversation history. However, most existing approaches do not effectively capture conversation history and thus have trouble handling questions involving coreference or ellipsis. Moreover, when reasoning over passage text, most of them simply treat it as a word sequence without exploring rich semantic relationships among words. In this paper, we first propose a simple yet effective graph structure learning technique to dynamically construct a question and conversation history aware context graph at each conversation turn. Then we propose a novel Recurrent Graph Neural Network, and based on that, we introduce a flow mechanism to model the temporal dependencies in a sequence of context graphs. The proposed GraphFlow model can effectively capture conversational flow in a dialog, and shows competitive performance compared to existing state-of-the-art methods on CoQA, QuAC and DoQA benchmarks. In addition, visualization experiments show that our proposed model can offer good interpretability for the reasoning process.

</details>

<details>

<summary>2020-07-15 19:22:44 - TraceCaps: A Capsule-based Neural Network for Semantic Segmentation</summary>

- *Tao Sun, Zhewei Wang, C. D. Smith, Jundong Liu*

- `1901.02920v2` - [abs](http://arxiv.org/abs/1901.02920v2) - [pdf](http://arxiv.org/pdf/1901.02920v2)

> In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure. We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network. Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions. With the capability to extracted part-whole information, our traceback pipeline can potentially be utilized as the building blocks to design interpretable neural networks. Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variants.

</details>

<details>

<summary>2020-07-15 20:41:41 - Context Matters: Recovering Human Semantic Structure from Machine Learning Analysis of Large-Scale Text Corpora</summary>

- *Marius Cătălin Iordan, Tyler Giallanza, Cameron T. Ellis, Nicole M. Beckage, Jonathan D. Cohen*

- `1910.06954v3` - [abs](http://arxiv.org/abs/1910.06954v3) - [pdf](http://arxiv.org/pdf/1910.06954v3)

> Applying machine learning algorithms to large-scale, text-based corpora (embeddings) presents a unique opportunity to investigate at scale how human semantic knowledge is organized and how people use it to judge fundamental relationships, such as similarity between concepts. However, efforts to date have shown a substantial discrepancy between algorithm predictions and empirical judgments. Here, we introduce a novel approach of generating embeddings motivated by the psychological theory that semantic context plays a critical role in human judgments. Specifically, we train state-of-the-art machine learning algorithms using contextually-constrained text corpora and show that this greatly improves predictions of similarity judgments and feature ratings. By improving the correspondence between representations derived using embeddings generated by machine learning methods and empirical measurements of human judgments, the approach we describe helps advance the use of large-scale text corpora to understand the structure of human semantic representations.

</details>

<details>

<summary>2020-07-15 23:10:43 - On the Generation, Structure, and Semantics of Grammar Patterns in Source Code Identifiers</summary>

- *Christian D. Newman, Reem S. AlSuhaibani, Michael J. Decker, Anthony Peruma, Dishant Kaushik, Mohamed Wiem Mkaouer, Emily Hill*

- `2007.08033v1` - [abs](http://arxiv.org/abs/2007.08033v1) - [pdf](http://arxiv.org/pdf/2007.08033v1)

> Identifiers make up a majority of the text in code. They are one of the most basic mediums through which developers describe the code they create and understand the code that others create. Therefore, understanding the patterns latent in identifier naming practices and how accurately we are able to automatically model these patterns is vital if researchers are to support developers and automated analysis approaches in comprehending and creating identifiers correctly and optimally. This paper investigates identifiers by studying sequences of part-of-speech annotations, referred to as grammar patterns. This work advances our understanding of these patterns and our ability to model them by 1) establishing common naming patterns in different types of identifiers, such as class and attribute names; 2) analyzing how different patterns influence comprehension; and 3) studying the accuracy of state-of-the-art techniques for part-of-speech annotations, which are vital in automatically modeling identifier naming patterns, in order to establish their limits and paths toward improvement. To do this, we manually annotate a dataset of 1,335 identifiers from 20 open-source systems and use this dataset to study naming patterns, semantics, and tagger accuracy.

</details>

<details>

<summary>2020-07-16 00:38:27 - SafeRESTScript: Statically Checking REST API Consumers</summary>

- *Nuno Burnay, Antónia Lopes, Vasco T. Vasconcelos*

- `2007.08048v1` - [abs](http://arxiv.org/abs/2007.08048v1) - [pdf](http://arxiv.org/pdf/2007.08048v1)

> Consumption of REST services has become a popular means of invoking code provided by third parties, particularly in web applications. Nowadays programmers of web applications can choose TypeScript over JavaScript to benefit from static type checking that enables validating calls to local functions or to those provided by libraries. Errors in calls to REST services, however, can only be found at run-time. In this paper, we present SafeRESTScript (SRS, for short) a language that extends the support of static analysis to calls to REST services, with the ability to statically find common errors such as missing or invalid data in REST calls and misuse of the results from such calls. SafeRESTScript features a syntax similar to JavaScript and is equipped with (i) a rich collection of types (including objects, arrays and refinement types)and (ii) primitives to natively support REST calls that are statically validated against specifications of the corresponding APIs. Specifications are written in HeadREST, a language that also features refinement types and supports the description of semantic aspects of REST APIs in a style reminiscent of Hoare triples. We present SafeRESTScript and its validation system, based on a general-purpose verification tool (Boogie). The evaluation of SafeRESTScript and of the prototype implementations for its validator, available in the form of an Eclipse plugin, is also discussed.

</details>

<details>

<summary>2020-07-16 09:48:06 - Deep Learning for Abstract Argumentation Semantics</summary>

- *Dennis Craandijk, Floris Bex*

- `2007.07629v2` - [abs](http://arxiv.org/abs/2007.07629v2) - [pdf](http://arxiv.org/pdf/2007.07629v2)

> In this paper, we present a learning-based approach to determining acceptance of arguments under several abstract argumentation semantics. More specifically, we propose an argumentation graph neural network (AGNN) that learns a message-passing algorithm to predict the likelihood of an argument being accepted. The experimental results demonstrate that the AGNN can almost perfectly predict the acceptability under different semantics and scales well for larger argumentation frameworks. Furthermore, analysing the behaviour of the message-passing algorithm shows that the AGNN learns to adhere to basic principles of argument semantics as identified in the literature, and can thus be trained to predict extensions under the different semantics - we show how the latter can be done for multi-extension semantics by using AGNNs to guide a basic search. We publish our code at https://github.com/DennisCraandijk/DL-Abstract-Argumentation

</details>

<details>

<summary>2020-07-16 12:33:28 - Deep ahead-of-threat virtual patching</summary>

- *Fady Copty, Andre Kassis, Sharon Keidar-Barner, Dov Murik*

- `2007.08296v1` - [abs](http://arxiv.org/abs/2007.08296v1) - [pdf](http://arxiv.org/pdf/2007.08296v1)

> Many applications have security vulnerabilities that can be exploited. It is practically impossible to find all of them due to the NP-complete nature of the testing problem. Security solutions provide defenses against these attacks through continuous application testing, fast-patching of vulnerabilities, automatic deployment of patches, and virtual patching detection techniques deployed in network and endpoint security tools. These techniques are limited by the need to find vulnerabilities before the black-hats. We propose an innovative technique to virtually patch vulnerabilities before they are found. We leverage testing techniques for supervised-learning data generation, and show how artificial intelligence techniques can use this data to create predictive deep neural-network models that read an application's input and predict in real time whether it is a potential malicious input. We set up an ahead-of-threat experiment in which we generated data on old versions of an application, and then evaluated the predictive model accuracy on vulnerabilities found years later. Our experiments show ahead-of-threat detection on LibXML2 and LibTIFF vulnerabilities with 91.3% and 93.7% accuracy, respectively. We expect to continue work on this field of research and provide ahead-of-threat virtual patching for more libraries. Success in this research can change the current state of endless racing after application vulnerabilities and put the defenders one step ahead of the attackers

</details>

<details>

<summary>2020-07-16 15:53:02 - SLK-NER: Exploiting Second-order Lexicon Knowledge for Chinese NER</summary>

- *Dou Hu, Lingwei Wei*

- `2007.08416v1` - [abs](http://arxiv.org/abs/2007.08416v1) - [pdf](http://arxiv.org/pdf/2007.08416v1)

> Although character-based models using lexicon have achieved promising results for Chinese named entity recognition (NER) task, some lexical words would introduce erroneous information due to wrongly matched words. Existing researches proposed many strategies to integrate lexicon knowledge. However, they performed with simple first-order lexicon knowledge, which provided insufficient word information and still faced the challenge of matched word boundary conflicts; or explored the lexicon knowledge with graph where higher-order information introducing negative words may disturb the identification. To alleviate the above limitations, we present new insight into second-order lexicon knowledge (SLK) of each character in the sentence to provide more lexical word information including semantic and word boundary features. Based on these, we propose a SLK-based model with a novel strategy to integrate the above lexicon knowledge. The proposed model can exploit more discernible lexical words information with the help of global context. Experimental results on three public datasets demonstrate the validity of SLK. The proposed model achieves more excellent performance than the state-of-the-art comparison methods.

</details>

<details>

<summary>2020-07-16 16:17:09 - Meta-Gradient Reinforcement Learning with an Objective Discovered Online</summary>

- *Zhongwen Xu, Hado van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, David Silver*

- `2007.08433v1` - [abs](http://arxiv.org/abs/2007.08433v1) - [pdf](http://arxiv.org/pdf/2007.08433v1)

> Deep reinforcement learning includes a broad family of algorithms that parameterise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that defines its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, flexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efficiency, eventually outperforming the median score of a strong actor-critic baseline.

</details>

<details>

<summary>2020-07-16 16:57:52 - Label Efficient Visual Abstractions for Autonomous Driving</summary>

- *Aseem Behl, Kashyap Chitta, Aditya Prakash, Eshed Ohn-Bar, Andreas Geiger*

- `2005.10091v2` - [abs](http://arxiv.org/abs/2005.10091v2) - [pdf](http://arxiv.org/pdf/2005.10091v2)

> It is well known that semantic segmentation can be used as an effective intermediate representation for learning driving policies. However, the task of street scene semantic segmentation requires expensive annotations. Furthermore, segmentation algorithms are often trained irrespective of the actual driving task, using auxiliary image-space loss functions which are not guaranteed to maximize driving metrics such as safety or distance traveled per intervention. In this work, we seek to quantify the impact of reducing segmentation annotation costs on learned behavior cloning agents. We analyze several segmentation-based intermediate representations. We use these visual abstractions to systematically study the trade-off between annotation efficiency and driving performance, i.e., the types of classes labeled, the number of image samples used to learn the visual abstraction model, and their granularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers several practical insights into how segmentation-based visual abstractions can be exploited in a more label efficient manner. Surprisingly, we find that state-of-the-art driving performance can be achieved with orders of magnitude reduction in annotation cost. Beyond label efficiency, we find several additional training benefits when leveraging visual abstractions, such as a significant reduction in the variance of the learned policy when compared to state-of-the-art end-to-end driving models.

</details>

<details>

<summary>2020-07-16 19:51:02 - Advances in Deep Learning for Hyperspectral Image Analysis--Addressing Challenges Arising in Practical Imaging Scenarios</summary>

- *Xiong Zhou, Saurabh Prasad*

- `2007.08592v1` - [abs](http://arxiv.org/abs/2007.08592v1) - [pdf](http://arxiv.org/pdf/2007.08592v1)

> Deep neural networks have proven to be very effective for computer vision tasks, such as image classification, object detection, and semantic segmentation -- these are primarily applied to color imagery and video. In recent years, there has been an emergence of deep learning algorithms being applied to hyperspectral and multispectral imagery for remote sensing and biomedicine tasks. These multi-channel images come with their own unique set of challenges that must be addressed for effective image analysis. Challenges include limited ground truth (annotation is expensive and extensive labeling is often not feasible), and high dimensional nature of the data (each pixel is represented by hundreds of spectral bands), despite being presented by a large amount of unlabeled data and the potential to leverage multiple sensors/sources that observe the same scene. In this chapter, we will review recent advances in the community that leverage deep learning for robust hyperspectral image analysis despite these unique challenges -- specifically, we will review unsupervised, semi-supervised and active learning approaches to image analysis, as well as transfer learning approaches for multi-source (e.g. multi-sensor, or multi-temporal) image analysis.

</details>

<details>

<summary>2020-07-16 20:32:54 - Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval</summary>

- *Christopher Thomas, Adriana Kovashka*

- `2007.08617v1` - [abs](http://arxiv.org/abs/2007.08617v1) - [pdf](http://arxiv.org/pdf/2007.08617v1)

> The abundance of multimodal data (e.g. social media posts) has inspired interest in cross-modal retrieval methods. Popular approaches rely on a variety of metric learning losses, which prescribe what the proximity of image and text should be, in the learned space. However, most prior methods have focused on the case where image and text convey redundant information; in contrast, real-world image-text pairs convey complementary information with little overlap. Further, images in news articles and media portray topics in a visually diverse fashion; thus, we need to take special care to ensure a meaningful image representation. We propose novel within-modality losses which encourage semantic coherency in both the text and image subspaces, which does not necessarily align with visual coherency. Our method ensures that not only are paired images and texts close, but the expected image-image and text-text relationships are also observed. Our approach improves the results of cross-modal retrieval on four datasets compared to five baselines.

</details>

<details>

<summary>2020-07-17 04:06:09 - A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</summary>

- *Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo*

- `2007.08742v1` - [abs](http://arxiv.org/abs/2007.08742v1) - [pdf](http://arxiv.org/pdf/2007.08742v1)

> Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.

</details>

<details>

<summary>2020-07-17 04:13:08 - Controllable Image Synthesis via SegVAE</summary>

- *Yen-Chi Cheng, Hsin-Ying Lee, Min Sun, Ming-Hsuan Yang*

- `2007.08397v2` - [abs](http://arxiv.org/abs/2007.08397v2) - [pdf](http://arxiv.org/pdf/2007.08397v2)

> Flexible user controls are desirable for content creation and image editing. A semantic map is commonly used intermediate representation for conditional image generation. Compared to the operation on raw RGB pixels, the semantic map enables simpler user modification. In this work, we specifically target at generating semantic maps given a label-set consisting of desired categories. The proposed framework, SegVAE, synthesizes semantic maps in an iterative manner using conditional variational autoencoder. Quantitative and qualitative experiments demonstrate that the proposed model can generate realistic and diverse semantic maps. We also apply an off-the-shelf image-to-image translation model to generate realistic RGB images to better understand the quality of the synthesized semantic maps. Furthermore, we showcase several real-world image-editing applications including object removal, object insertion, and object replacement.

</details>

<details>

<summary>2020-07-17 07:47:10 - Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing</summary>

- *Ricardo Ñanculef, Francisco Mena, Antonio Macaluso, Stefano Lodi, Claudio Sartori*

- `2007.08799v1` - [abs](http://arxiv.org/abs/2007.08799v1) - [pdf](http://arxiv.org/pdf/2007.08799v1)

> Semantic hashing is an emerging technique for large-scale similarity search based on representing high-dimensional data using similarity-preserving binary codes used for efficient indexing and search. It has recently been shown that variational autoencoders, with Bernoulli latent representations parametrized by neural nets, can be successfully trained to learn such codes in supervised and unsupervised scenarios, improving on more traditional methods thanks to their ability to handle the binary constraints architecturally. However, the scenario where labels are scarce has not been studied yet.   This paper investigates the robustness of hashing methods based on variational autoencoders to the lack of supervision, focusing on two semi-supervised approaches currently in use. The first augments the variational autoencoder's training objective to jointly model the distribution over the data and the class labels. The second approach exploits the annotations to define an additional pairwise loss that enforces consistency between the similarity in the code (Hamming) space and the similarity in the label space. Our experiments show that both methods can significantly increase the hash codes' quality. The pairwise approach can exhibit an advantage when the number of labelled points is large. However, we found that this method degrades quickly and loses its advantage when labelled samples decrease. To circumvent this problem, we propose a novel supervision method in which the model uses its label distribution predictions to implement the pairwise objective. Compared to the best baseline, this procedure yields similar performance in fully supervised settings but improves the results significantly when labelled data is scarce. Our code is made publicly available at https://github.com/amacaluso/SSB-VAE.

</details>

<details>

<summary>2020-07-17 09:37:37 - Design and Interpretation of Universal Adversarial Patches in Face Detection</summary>

- *Xiao Yang, Fangyun Wei, Hongyang Zhang, Jun Zhu*

- `1912.05021v3` - [abs](http://arxiv.org/abs/1912.05021v3) - [pdf](http://arxiv.org/pdf/1912.05021v3)

> We consider universal adversarial patches for faces -- small visual elements whose addition to a face image reliably destroys the performance of face detectors. Unlike previous work that mostly focused on the algorithmic design of adversarial examples in terms of improving the success rate as an attacker, in this work we show an interpretation of such patches that can prevent the state-of-the-art face detectors from detecting the real faces. We investigate a phenomenon: patches designed to suppress real face detection appear face-like. This phenomenon holds generally across different initialization, locations, scales of patches, backbones, and state-of-the-art face detection frameworks. We propose new optimization-based approaches to automatic design of universal adversarial patches for varying goals of the attack, including scenarios in which true positives are suppressed without introducing false positives. Our proposed algorithms perform well on real-world datasets, deceiving state-of-the-art face detectors in terms of multiple precision/recall metrics and transferability.

</details>

<details>

<summary>2020-07-17 10:07:40 - Probabilistic Future Prediction for Video Scene Understanding</summary>

- *Anthony Hu, Fergal Cotter, Nikhil Mohan, Corina Gurau, Alex Kendall*

- `2003.06409v2` - [abs](http://arxiv.org/abs/2003.06409v2) - [pdf](http://arxiv.org/pdf/2003.06409v2)

> We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have seen) and the future distribution (what we observe actually happens). During inference, diverse futures are generated by sampling from the present distribution.

</details>

<details>

<summary>2020-07-17 15:53:40 - GMNet: Graph Matching Network for Large Scale Part Semantic Segmentation in the Wild</summary>

- *Umberto Michieli, Edoardo Borsato, Luca Rossi, Pietro Zanuttigh*

- `2007.09073v1` - [abs](http://arxiv.org/abs/2007.09073v1) - [pdf](http://arxiv.org/pdf/2007.09073v1)

> The semantic segmentation of parts of objects in the wild is a challenging task in which multiple instances of objects and multiple parts within those objects must be detected in the scene. This problem remains nowadays very marginally explored, despite its fundamental importance towards detailed object understanding. In this work, we propose a novel framework combining higher object-level context conditioning and part-level spatial relationships to address the task. To tackle object-level ambiguity, a class-conditioning module is introduced to retain class-level semantics when learning parts-level semantics. In this way, mid-level features carry also this information prior to the decoding stage. To tackle part-level ambiguity and localization we propose a novel adjacency graph-based module that aims at matching the relative spatial relationships between ground truth and predicted parts. The experimental evaluation on the Pascal-Part dataset shows that we achieve state-of-the-art results on this task.

</details>

<details>

<summary>2020-07-17 15:58:05 - Generating Person Images with Appearance-aware Pose Stylizer</summary>

- *Siyu Huang, Haoyi Xiong, Zhi-Qi Cheng, Qingzhong Wang, Xingran Zhou, Bihan Wen, Jun Huan, Dejing Dou*

- `2007.09077v1` - [abs](http://arxiv.org/abs/2007.09077v1) - [pdf](http://arxiv.org/pdf/2007.09077v1)

> Generation of high-quality person images is challenging, due to the sophisticated entanglements among image factors, e.g., appearance, pose, foreground, background, local details, global structures, etc. In this paper, we present a novel end-to-end framework to generate realistic person images based on given person poses and appearances. The core of our framework is a novel generator called Appearance-aware Pose Stylizer (APS) which generates human images by coupling the target pose with the conditioned person appearance progressively. The framework is highly flexible and controllable by effectively decoupling various complex person image factors in the encoding phase, followed by re-coupling them in the decoding phase. In addition, we present a new normalization method named adaptive patch normalization, which enables region-specific normalization and shows a good performance when adopted in person image generation model. Experiments on two benchmark datasets show that our method is capable of generating visually appealing and realistic-looking results using arbitrary image and pose inputs.

</details>

<details>

<summary>2020-07-17 16:16:56 - Graph Convolutional Network-based Suspicious Communication Pair Estimation for Industrial Control Systems</summary>

- *Tatsumi Oba, Tadahiro Taniguchi*

- `2007.10204v1` - [abs](http://arxiv.org/abs/2007.10204v1) - [pdf](http://arxiv.org/pdf/2007.10204v1)

> Whitelisting is considered an effective security monitoring method for networks used in industrial control systems, where the whitelists consist of observed tuples of the IP address of the server, the TCP/UDP port number, and IP address of the client (communication triplets). However, this method causes frequent false detections. To reduce false positives due to a simple whitelist-based judgment, we propose a new framework for scoring communications to judge whether the communications not present in whitelists are normal or anomalous. To solve this problem, we developed a graph convolutional network-based suspicious communication pair estimation using relational graph convolution networks, and evaluated its performance. For this, we collected the network traffic of three factories owned by Panasonic Corporation, Japan. The proposed method achieved a receiver operating characteristic area under the curve of 0.957, which outperforms baseline approaches such as DistMult, a method that directly optimizes the node embeddings, and heuristics, which score the triplets using first- and second-order proximities of multigraphs. This method enables security operators to concentrate on significant alerts.

</details>

<details>

<summary>2020-07-17 18:40:46 - WordCraft: An Environment for Benchmarking Commonsense Agents</summary>

- *Minqi Jiang, Jelena Luketina, Nantas Nardelli, Pasquale Minervini, Philip H. S. Torr, Shimon Whiteson, Tim Rocktäschel*

- `2007.09185v1` - [abs](http://arxiv.org/abs/2007.09185v1) - [pdf](http://arxiv.org/pdf/2007.09185v1)

> The ability to quickly solve a wide range of real-world tasks requires a commonsense understanding of the world. Yet, how to best extract such knowledge from natural language corpora and integrate it with reinforcement learning (RL) agents remains an open challenge. This is partly due to the lack of lightweight simulation environments that sufficiently reflect the semantics of the real world and provide knowledge sources grounded with respect to observations in an RL environment. To better enable research on agents making use of commonsense knowledge, we propose WordCraft, an RL environment based on Little Alchemy 2. This lightweight environment is fast to run and built upon entities and relations inspired by real-world semantics. We evaluate several representation learning methods on this new benchmark and propose a new method for integrating knowledge graphs with an RL agent.

</details>

<details>

<summary>2020-07-17 19:01:06 - On correctness of an n queens program</summary>

- *Włodzimierz Drabent*

- `1909.07479v5` - [abs](http://arxiv.org/abs/1909.07479v5) - [pdf](http://arxiv.org/pdf/1909.07479v5)

> Thom Fr\"uhwirth presented a short, elegant and efficient Prolog program for the n queens problem. However the program may be seen as rather tricky and one may be not convinced about its correctness. This paper explains the program in a declarative way, and provides proofs of its correctness and completeness. The specification and the proofs are declarative, i.e. they abstract from any operational semantics. The specification is approximate, it is unnecessary to describe the program's semantics exactly. Despite the program works on non-ground terms, this work employs the standard semantics, based on logical consequence and Herbrand interpretations.   Another purpose of the paper is to present an example of precise declarative reasoning about the semantics of a logic program.

</details>

<details>

<summary>2020-07-17 19:46:18 - OBA: An Ontology-Based Framework for Creating REST APIs for Knowledge Graphs</summary>

- *Daniel Garijo, Maximiliano Osorio*

- `2007.09206v1` - [abs](http://arxiv.org/abs/2007.09206v1) - [pdf](http://arxiv.org/pdf/2007.09206v1)

> In recent years, Semantic Web technologies have been increasingly adopted by researchers, industry and public institutions to describe and link data on the Web, create web annotations and consume large knowledge graphs like Wikidata and DBPedia. However, there is still a knowledge gap between ontology engineers, who design, populate and create knowledge graphs; and web developers, who need to understand, access and query these knowledge graphs but are not familiar with ontologies, RDF or SPARQL. In this paper we describe the Ontology-Based APIs framework (OBA), our approach to automatically create REST APIs from ontologies while following RESTful API best practices. Given an ontology (or ontology network) OBA uses standard technologies familiar to web developers (OpenAPI Specification, JSON) and combines them with W3C standards (OWL, JSON-LD frames and SPARQL) to create maintainable APIs with documentation, units tests, automated validation of resources and clients (in Python, Javascript, etc.) for non Semantic Web experts to access the contents of a target knowledge graph. We showcase OBA with three examples that illustrate the capabilities of the framework for different ontologies.

</details>

<details>

<summary>2020-07-17 21:05:44 - Multi-Perspective Semantic Information Retrieval in the Biomedical Domain</summary>

- *Samarth Rawal*

- `2008.01526v1` - [abs](http://arxiv.org/abs/2008.01526v1) - [pdf](http://arxiv.org/pdf/2008.01526v1)

> Information Retrieval (IR) is the task of obtaining pieces of data (such as documents) that are relevant to a particular query or need from a large repository of information. IR is a valuable component of several downstream Natural Language Processing (NLP) tasks. Practically, IR is at the heart of many widely-used technologies like search engines. While probabilistic ranking functions like the Okapi BM25 function have been utilized in IR systems since the 1970's, modern neural approaches pose certain advantages compared to their classical counterparts. In particular, the release of BERT (Bidirectional Encoder Representations from Transformers) has had a significant impact in the NLP community by demonstrating how the use of a Masked Language Model trained on a large corpus of data can improve a variety of downstream NLP tasks, including sentence classification and passage re-ranking. IR Systems are also important in the biomedical and clinical domains. Given the increasing amount of scientific literature across biomedical domain, the ability find answers to specific clinical queries from a repository of millions of articles is a matter of practical value to medical professionals. Moreover, there are domain-specific challenges present, including handling clinical jargon and evaluating the similarity or relatedness of various medical symptoms when determining the relevance between a query and a sentence. This work presents contributions to several aspects of the Biomedical Semantic Information Retrieval domain. First, it introduces Multi-Perspective Sentence Relevance, a novel methodology of utilizing BERT-based models for contextual IR. The system is evaluated using the BioASQ Biomedical IR Challenge. Finally, practical contributions in the form of a live IR system for medics and a proposed challenge on the Living Systematic Review clinical task are provided.

</details>

<details>

<summary>2020-07-17 22:08:36 - Analyzing and Improving Neural Networks by Generating Semantic Counterexamples through Differentiable Rendering</summary>

- *Lakshya Jain, Varun Chandrasekaran, Uyeong Jang, Wilson Wu, Andrew Lee, Andy Yan, Steven Chen, Somesh Jha, Sanjit A. Seshia*

- `1910.00727v2` - [abs](http://arxiv.org/abs/1910.00727v2) - [pdf](http://arxiv.org/pdf/1910.00727v2)

> Even as deep neural networks (DNNs) have achieved remarkable success on vision-related tasks, their performance is brittle to transformations in the input. Of particular interest are semantic transformations that model changes that have a basis in the physical world, such as rotations, translations, changes in lighting or camera pose. In this paper, we show how differentiable rendering can be utilized to generate images that are informative, yet realistic, and which can be used to analyze DNN performance and improve its robustness through data augmentation. Given a differentiable renderer and a DNN, we show how to use off-the-shelf attacks from adversarial machine learning to generate semantic counterexamples -- images where semantic features are changed as to produce misclassifications or misdetections. We validate our approach on DNNs for image classification and object detection. For classification, we show that semantic counterexamples, when used to augment the dataset, (i) improve generalization performance (ii) enhance robustness to semantic transformations, and (iii) transfer between models. Additionally, in comparison to sampling-based semantic augmentation, our technique generates more informative data in a sample efficient manner.

</details>

<details>

<summary>2020-07-18 02:37:32 - DWMD: Dimensional Weighted Orderwise Moment Discrepancy for Domain-specific Hidden Representation Matching</summary>

- *Rongzhe Wei, Fa Zhang, Bo Dong, Qinghua Zheng*

- `2007.09312v1` - [abs](http://arxiv.org/abs/2007.09312v1) - [pdf](http://arxiv.org/pdf/2007.09312v1)

> Knowledge transfer from a source domain to a different but semantically related target domain has long been an important topic in the context of unsupervised domain adaptation (UDA). A key challenge in this field is establishing a metric that can exactly measure the data distribution discrepancy between two homogeneous domains and adopt it in distribution alignment, especially in the matching of feature representations in the hidden activation space. Existing distribution matching approaches can be interpreted as failing to either explicitly orderwise align higher-order moments or satisfy the prerequisite of certain assumptions in practical uses. We propose a novel moment-based probability distribution metric termed dimensional weighted orderwise moment discrepancy (DWMD) for feature representation matching in the UDA scenario. Our metric function takes advantage of a series for high-order moment alignment, and we theoretically prove that our DWMD metric function is error-free, which means that it can strictly reflect the distribution differences between domains and is valid without any feature distribution assumption. In addition, since the discrepancies between probability distributions in each feature dimension are different, dimensional weighting is considered in our function. We further calculate the error bound of the empirical estimate of the DWMD metric in practical applications. Comprehensive experiments on benchmark datasets illustrate that our method yields state-of-the-art distribution metrics.

</details>

<details>

<summary>2020-07-18 10:51:21 - Mapping computational thinking mindsets between educational levels with cognitive network science</summary>

- *Massimo Stella, Anastasiya Kapuza, Catherine Cramer, Stephen Uzzo*

- `2007.09402v1` - [abs](http://arxiv.org/abs/2007.09402v1) - [pdf](http://arxiv.org/pdf/2007.09402v1)

> Computational thinking is a way of reasoning about the world in terms of data. This mindset channels number crunching toward an ambition to discover knowledge through logic, models and simulations. Here we show how computational cognitive science can be used to reconstruct and analyse the structure of computational thinking mindsets (forma mentis in Latin) through complex networks. As a case study, we investigate cognitive networks tied to key concepts of computational thinking provided by: (i) 159 high school students enrolled in a science curriculum and (ii) 59 researchers in complex systems and simulations. Researchers' reconstructed forma mentis highlighted a positive mindset about scientific modelling, semantically framing data and simulations as ways of discovering nature. Students correctly identified different aspects of logic reasoning but perceived "computation" as a distressing, anxiety-eliciting task, framed with math jargon and lacking links to real-world discovery. Students' mindsets around "data", "model" and "simulations" critically revealed no awareness of numerical modelling as a way for understanding the world. Our findings provide evidence of a crippled computational thinking mindset in students, who acquire mathematical skills that are not channelled toward real-world discovery through coding. This unlinked knowledge ends up being perceived as distressing number-crunching expertise with no relevant outcome. The virtuous mindset of researchers reported here indicates that computational thinking can be restored by training students specifically in coding, modelling and simulations in relation to discovering nature. Our approach opens innovative ways for quantifying computational thinking and enhancing its development through mindset reconstruction.

</details>

<details>

<summary>2020-07-18 11:10:45 - Learning Visual Commonsense for Robust Scene Graph Generation</summary>

- *Alireza Zareian, Zhecan Wang, Haoxuan You, Shih-Fu Chang*

- `2006.09623v2` - [abs](http://arxiv.org/abs/2006.09623v2) - [pdf](http://arxiv.org/pdf/2006.09623v2)

> Scene graph generation models understand the scene through object and predicate recognition, but are prone to mistakes due to the challenges of perception in the wild. Perception errors often lead to nonsensical compositions in the output scene graph, which do not follow real-world rules and patterns, and can be corrected using commonsense knowledge. We propose the first method to acquire visual commonsense such as affordance and intuitive physics automatically from data, and use that to improve the robustness of scene understanding. To this end, we extend Transformer models to incorporate the structure of scene graphs, and train our Global-Local Attention Transformer on a scene graph corpus. Once trained, our model can be applied on any scene graph generation model and correct its obvious mistakes, resulting in more semantically plausible scene graphs. Through extensive experiments, we show our model learns commonsense better than any alternative, and improves the accuracy of state-of-the-art scene graph generation methods.

</details>

<details>

<summary>2020-07-18 15:17:38 - Improving Interpretability of Word Embeddings by Generating Definition and Usage</summary>

- *Haitong Zhang, Yongping Du, Jiaxin Sun, Qingxiao Li*

- `1912.05898v2` - [abs](http://arxiv.org/abs/1912.05898v2) - [pdf](http://arxiv.org/pdf/1912.05898v2)

> Word embeddings are substantially successful in capturing semantic relations among words. However, these lexical semantics are difficult to be interpreted. Definition modeling provides a more intuitive way to evaluate embeddings by utilizing them to generate natural language definitions of corresponding words. This task is of great significance for practical application and in-depth understanding of word representations. We propose a novel framework for definition modeling, which can generate reasonable and understandable context-dependent definitions. Moreover, we introduce usage modeling and study whether it is possible to utilize embeddings to generate example sentences of words. These ways are a more direct and explicit expression of embedding's semantics for better interpretability. We extend the single task model to multi-task setting and investigate several joint multi-task models to combine usage modeling and definition modeling together. Experimental results on existing Oxford dataset and a new collected Oxford-2019 dataset show that our single-task model achieves the state-of-the-art result in definition modeling and the multi-task learning methods are helpful for two tasks to improve the performance.

</details>

<details>

<summary>2020-07-18 18:40:40 - A novel approach to sentiment analysis in Persian using discourse and external semantic information</summary>

- *Rahim Dehkharghani, Hojjat Emami*

- `2007.09495v1` - [abs](http://arxiv.org/abs/2007.09495v1) - [pdf](http://arxiv.org/pdf/2007.09495v1)

> Sentiment analysis attempts to identify, extract and quantify affective states and subjective information from various types of data such as text, audio, and video. Many approaches have been proposed to extract the sentiment of individuals from documents written in natural languages in recent years. The majority of these approaches have focused on English, while resource-lean languages such as Persian suffer from the lack of research work and language resources. Due to this gap in Persian, the current work is accomplished to introduce new methods for sentiment analysis which have been applied on Persian. The proposed approach in this paper is two-fold: The first one is based on classifier combination, and the second one is based on deep neural networks which benefits from word embedding vectors. Both approaches takes advantage of local discourse information and external knowledge bases, and also cover several language issues such as negation and intensification, andaddresses different granularity levels, namely word, aspect, sentence, phrase and document-levels. To evaluate the performance of the proposed approach, a Persian dataset is collected from Persian hotel reviews referred as hotel reviews. The proposed approach has been compared to counterpart methods based on the benchmark dataset. The experimental results approve the effectiveness of the proposed approach when compared to related works.

</details>

<details>

<summary>2020-07-19 01:25:53 - Leveraging Seen and Unseen Semantic Relationships for Generative Zero-Shot Learning</summary>

- *Maunil R Vyas, Hemanth Venkateswara, Sethuraman Panchanathan*

- `2007.09549v1` - [abs](http://arxiv.org/abs/2007.09549v1) - [pdf](http://arxiv.org/pdf/2007.09549v1)

> Zero-shot learning (ZSL) addresses the unseen class recognition problem by leveraging semantic information to transfer knowledge from seen classes to unseen classes. Generative models synthesize the unseen visual features and convert ZSL into a classical supervised learning problem. These generative models are trained using the seen classes and are expected to implicitly transfer the knowledge from seen to unseen classes. However, their performance is stymied by overfitting, which leads to substandard performance on Generalized Zero-Shot learning (GZSL). To address this concern, we propose the novel LsrGAN, a generative model that Leverages the Semantic Relationship between seen and unseen categories and explicitly performs knowledge transfer by incorporating a novel Semantic Regularized Loss (SR-Loss). The SR-loss guides the LsrGAN to generate visual features that mirror the semantic relationships between seen and unseen classes. Experiments on seven benchmark datasets, including the challenging Wikipedia text-based CUB and NABirds splits, and Attribute-based AWA, CUB, and SUN, demonstrates the superiority of the LsrGAN compared to previous state-of-the-art approaches under both ZSL and GZSL. Code is available at https: // github. com/ Maunil/ LsrGAN

</details>

<details>

<summary>2020-07-19 01:44:32 - UiO-UvA at SemEval-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection</summary>

- *Andrey Kutuzov, Mario Giulianelli*

- `2005.00050v3` - [abs](http://arxiv.org/abs/2005.00050v3) - [pdf](http://arxiv.org/pdf/2005.00050v3)

> We apply contextualised word embeddings to lexical semantic change detection in the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking words by the degree of their semantic drift over time. We analyse the performance of two contextualising architectures (BERT and ELMo) and three change detection algorithms. We find that the most effective algorithms rely on the cosine similarity between averaged token embeddings and the pairwise distances between token embeddings. They outperform strong baselines by a large margin (in the post-evaluation phase, we have the best Subtask 2 submission for SemEval-2020 Task 1), but interestingly, the choice of a particular algorithm depends on the distribution of gold scores in the test set.

</details>

<details>

<summary>2020-07-19 02:11:53 - From Spatial Relations to Spatial Configurations</summary>

- *Soham Dan, Parisa Kordjamshidi, Julia Bonn, Archna Bhatia, Jon Cai, Martha Palmer, Dan Roth*

- `2007.09557v1` - [abs](http://arxiv.org/abs/2007.09557v1) - [pdf](http://arxiv.org/pdf/2007.09557v1)

> Spatial Reasoning from language is essential for natural language understanding. Supporting it requires a representation scheme that can capture spatial phenomena encountered in language as well as in images and videos. Existing spatial representations are not sufficient for describing spatial configurations used in complex tasks. This paper extends the capabilities of existing spatial representation languages and increases coverage of the semantic aspects that are needed to ground the spatial meaning of natural language text in the world. Our spatial relation language is able to represent a large, comprehensive set of spatial concepts crucial for reasoning and is designed to support the composition of static and dynamic spatial configurations. We integrate this language with the Abstract Meaning Representation(AMR) annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with the fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole.

</details>

<details>

<summary>2020-07-19 07:34:18 - Self-similarity Student for Partial Label Histopathology Image Segmentation</summary>

- *Hsien-Tzu Cheng, Chun-Fu Yeh, Po-Chen Kuo, Andy Wei, Keng-Chi Liu, Mong-Chi Ko, Kuan-Hua Chao, Yu-Ching Peng, Tyng-Luh Liu*

- `2007.09610v1` - [abs](http://arxiv.org/abs/2007.09610v1) - [pdf](http://arxiv.org/pdf/2007.09610v1)

> Delineation of cancerous regions in gigapixel whole slide images (WSIs) is a crucial diagnostic procedure in digital pathology. This process is time-consuming because of the large search space in the gigapixel WSIs, causing chances of omission and misinterpretation at indistinct tumor lesions. To tackle this, the development of an automated cancerous region segmentation method is imperative. We frame this issue as a modeling problem with partial label WSIs, where some cancerous regions may be misclassified as benign and vice versa, producing patches with noisy labels. To learn from these patches, we propose Self-similarity Student, combining teacher-student model paradigm with similarity learning. Specifically, for each patch, we first sample its similar and dissimilar patches according to spatial distance. A teacher-student model is then introduced, featuring the exponential moving average on both student model weights and teacher predictions ensemble. While our student model takes patches, teacher model takes all their corresponding similar and dissimilar patches for learning robust representation against noisy label patches. Following this similarity learning, our similarity ensemble merges similar patches' ensembled predictions as the pseudo-label of a given patch to counteract its noisy label. On the CAMELYON16 dataset, our method substantially outperforms state-of-the-art noise-aware learning methods by 5$\%$ and the supervised-trained baseline by 10$\%$ in various degrees of noise. Moreover, our method is superior to the baseline on our TVGH TURP dataset with 2$\%$ improvement, demonstrating the generalizability to more clinical histopathology segmentation tasks.

</details>

<details>

<summary>2020-07-19 11:57:46 - Unsupervised Representation Learning by Predicting Random Distances</summary>

- *Hu Wang, Guansong Pang, Chunhua Shen, Congbo Ma*

- `1912.12186v2` - [abs](http://arxiv.org/abs/1912.12186v2) - [pdf](http://arxiv.org/pdf/1912.12186v2)

> Deep neural networks have gained tremendous success in a broad range of machine learning tasks due to its remarkable capability to learn semantic-rich features from high-dimensional data. However, they often require large-scale labelled data to successfully learn such features, which significantly hinders their adaption into unsupervised learning tasks, such as anomaly detection and clustering, and limits their applications into critical domains where obtaining massive labelled data is prohibitively expensive. To enable unsupervised learning on those domains, in this work we propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a theoretically proven approach to obtain approximately preserved distances. To well predict these random distances, the representation learner is optimised to learn genuine class structures that are implicitly embedded in the randomly projected space. Empirical results on 19 real-world datasets show that our learned representations substantially outperform a few state-of-the-art competing methods in both anomaly detection and clustering tasks. Code is available at https://git.io/RDP

</details>

<details>

<summary>2020-07-19 13:46:34 - Prediction Intervals: Split Normal Mixture from Quality-Driven Deep Ensembles</summary>

- *Tárik S. Salem, Helge Langseth, Heri Ramampiaro*

- `2007.09670v1` - [abs](http://arxiv.org/abs/2007.09670v1) - [pdf](http://arxiv.org/pdf/2007.09670v1)

> Prediction intervals are a machine- and human-interpretable way to represent predictive uncertainty in a regression analysis. In this paper, we present a method for generating prediction intervals along with point estimates from an ensemble of neural networks. We propose a multi-objective loss function fusing quality measures related to prediction intervals and point estimates, and a penalty function, which enforces semantic integrity of the results and stabilizes the training process of the neural networks. The ensembled prediction intervals are aggregated as a split normal mixture accounting for possible multimodality and asymmetricity of the posterior predictive distribution, and resulting in prediction intervals that capture aleatoric and epistemic uncertainty. Our results show that both our quality-driven loss function and our aggregation method contribute to well-calibrated prediction intervals and point estimates.

</details>

<details>

<summary>2020-07-19 14:33:03 - One-Shot Learning for Language Modelling</summary>

- *Talip Ucar, Adrian Gonzalez-Martin, Matthew Lee, Adrian Daniel Szwarc*

- `2007.09679v1` - [abs](http://arxiv.org/abs/2007.09679v1) - [pdf](http://arxiv.org/pdf/2007.09679v1)

> Humans can infer a great deal about the meaning of a word, using the syntax and semantics of surrounding words even if it is their first time reading or hearing it. We can also generalise the learned concept of the word to new tasks. Despite great progress in achieving human-level performance in certain tasks (Silver et al., 2016), learning from one or few examples remains a key challenge in machine learning, and has not thoroughly been explored in Natural Language Processing (NLP).   In this work we tackle the problem of oneshot learning for an NLP task by employing ideas from recent developments in machine learning: embeddings, attention mechanisms (softmax) and similarity measures (cosine, Euclidean, Poincare, and Minkowski). We adapt the framework suggested in matching networks (Vinyals et al., 2016), and explore the effectiveness of the aforementioned methods in one, two and three-shot learning problems on the task of predicting missing word explored in (Vinyals et al., 2016) by using the WikiText-2 dataset. Our work contributes in two ways: Our first contribution is that we explore the effectiveness of different distance metrics on k-shot learning, and show that there is no single best distance metric for k-shot learning, which challenges common belief. We found that the performance of a distance metric depends on the number of shots used during training. The second contribution of our work is that we establish a benchmark for one, two, and three-shot learning on a language task with a publicly available dataset that can be used to benchmark against in future research.

</details>

<details>

<summary>2020-07-19 15:13:00 - An Energy Ontology for Global City Indicators (ISO 37120)</summary>

- *Alanna Komisar, Mark S. Fox*

- `2008.04070v1` - [abs](http://arxiv.org/abs/2008.04070v1) - [pdf](http://arxiv.org/pdf/2008.04070v1)

> To create tomorrow's smarter cities, today's initiatives will need to create measurable improvements. However, a city is a complex system and measuring its performance generates a breadth of issues. Specifically, determining what criteria should be measured, how indications should be defined, and how should the identified indicators be derived. This working paper is one in series that addresses the creation of a Semantic Web based representation of the 17 different themes of ISO 37120 indicators as part of the larger PolisGnosis Project (Fox, 2017). We define a standard ontology for representing general knowledge for the Energy Theme indicators, and for representing both the definition and data used to derive the Energy indicators.

</details>

<details>

<summary>2020-07-19 15:48:35 - STAN: Towards Describing Bytecodes of Smart Contract</summary>

- *Xiaoqi Li, Ting Chen, Xiapu Luo, Tao Zhang, Le Yu, Zhou Xu*

- `2007.09696v1` - [abs](http://arxiv.org/abs/2007.09696v1) - [pdf](http://arxiv.org/pdf/2007.09696v1)

> More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users.

</details>

<details>

<summary>2020-07-19 19:13:20 - Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks</summary>

- *Diego de Vargas Feijo, Viviane Pereira Moreira*

- `2007.09757v1` - [abs](http://arxiv.org/abs/2007.09757v1) - [pdf](http://arxiv.org/pdf/2007.09757v1)

> BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.

</details>

<details>

<summary>2020-07-19 22:50:20 - A Multi-Semantic Metapath Model for Large Scale Heterogeneous Network Representation Learning</summary>

- *Xuandong Zhao, Jinbao Xue, Jin Yu, Xi Li, Hongxia Yang*

- `2007.11380v1` - [abs](http://arxiv.org/abs/2007.11380v1) - [pdf](http://arxiv.org/pdf/2007.11380v1)

> Network Embedding has been widely studied to model and manage data in a variety of real-world applications. However, most existing works focus on networks with single-typed nodes or edges, with limited consideration of unbalanced distributions of nodes and edges. In real-world applications, networks usually consist of billions of various types of nodes and edges with abundant attributes. To tackle these challenges, in this paper we propose a multi-semantic metapath (MSM) model for large scale heterogeneous representation learning. Specifically, we generate multi-semantic metapath-based random walks to construct the heterogeneous neighborhood to handle the unbalanced distributions and propose a unified framework for the embedding learning. We conduct systematical evaluations for the proposed framework on two challenging datasets: Amazon and Alibaba. The results empirically demonstrate that MSM can achieve relatively significant gains over previous state-of-arts on link prediction.

</details>

<details>

<summary>2020-07-20 03:40:51 - Attention Sequence to Sequence Model for Machine Remaining Useful Life Prediction</summary>

- *Mohamed Ragab, Zhenghua Chen, Min Wu, Chee-Keong Kwoh, Ruqiang Yan, Xiaoli Li*

- `2007.09868v1` - [abs](http://arxiv.org/abs/2007.09868v1) - [pdf](http://arxiv.org/pdf/2007.09868v1)

> Accurate estimation of remaining useful life (RUL) of industrial equipment can enable advanced maintenance schedules, increase equipment availability and reduce operational costs. However, existing deep learning methods for RUL prediction are not completely successful due to the following two reasons. First, relying on a single objective function to estimate the RUL will limit the learned representations and thus affect the prediction accuracy. Second, while longer sequences are more informative for modelling the sensor dynamics of equipment, existing methods are less effective to deal with very long sequences, as they mainly focus on the latest information. To address these two problems, we develop a novel attention-based sequence to sequence with auxiliary task (ATS2S) model. In particular, our model jointly optimizes both reconstruction loss to empower our model with predictive capabilities (by predicting next input sequence given current input sequence) and RUL prediction loss to minimize the difference between the predicted RUL and actual RUL. Furthermore, to better handle longer sequence, we employ the attention mechanism to focus on all the important input information during training process. Finally, we propose a new dual-latent feature representation to integrate the encoder features and decoder hidden states, to capture rich semantic information in data. We conduct extensive experiments on four real datasets to evaluate the efficacy of the proposed method. Experimental results show that our proposed method can achieve superior performance over 13 state-of-the-art methods consistently.

</details>

<details>

<summary>2020-07-20 08:38:36 - Towards an ontology of HTTP interactions</summary>

- *Mathieu Lirzin, Béatrice Markhoff*

- `2007.13475v1` - [abs](http://arxiv.org/abs/2007.13475v1) - [pdf](http://arxiv.org/pdf/2007.13475v1)

> Enterprise information systems have adopted Web-based foundations for exchanges between heterogeneous programmes. These programs provide and consume via Web APIs some resources identified by URIs, whose representations are transmitted via HTTP. Furthermore HTTP remains at the heart of all Web developments (Semantic Web, linked data, IoT...). Thus, situations where a program must be able to reason about HTTP interactions (request-response) are multiplying. This requires an explicit formal specification of a shared conceptualization of those interactions. A proposal for an RDF vocabulary exists, developed with a view to carrying out web application conformity tests and record the tests outputs. This vocabulary has already been reused. In this paper we propose to adapt and extend it for making it more reusable.

</details>

<details>

<summary>2020-07-20 10:27:18 - Bayesian optimization for automatic design of face stimuli</summary>

- *Pedro F. da Costa, Romy Lorenz, Ricardo Pio Monti, Emily Jones, Robert Leech*

- `2007.09989v1` - [abs](http://arxiv.org/abs/2007.09989v1) - [pdf](http://arxiv.org/pdf/2007.09989v1)

> Investigating the cognitive and neural mechanisms involved with face processing is a fundamental task in modern neuroscience and psychology. To date, the majority of such studies have focused on the use of pre-selected stimuli. The absence of personalized stimuli presents a serious limitation as it fails to account for how each individual face processing system is tuned to cultural embeddings or how it is disrupted in disease. In this work, we propose a novel framework which combines generative adversarial networks (GANs) with Bayesian optimization to identify individual response patterns to many different faces. Formally, we employ Bayesian optimization to efficiently search the latent space of state-of-the-art GAN models, with the aim to automatically generate novel faces, to maximize an individual subject's response. We present results from a web-based proof-of-principle study, where participants rated images of themselves generated via performing Bayesian optimization over the latent space of a GAN. We show how the algorithm can efficiently locate an individual's optimal face while mapping out their response across different semantic transformations of a face; inter-individual analyses suggest how the approach can provide rich information about individual differences in face processing.

</details>

<details>

<summary>2020-07-20 12:23:39 - Knowledge Graph Extraction from Videos</summary>

- *Louis Mahon, Eleonora Giunchiglia, Bowen Li, Thomas Lukasiewicz*

- `2007.10040v1` - [abs](http://arxiv.org/abs/2007.10040v1) - [pdf](http://arxiv.org/pdf/2007.10040v1)

> Nearly all existing techniques for automated video annotation (or captioning) describe videos using natural language sentences. However, this has several shortcomings: (i) it is very hard to then further use the generated natural language annotations in automated data processing, (ii) generating natural language annotations requires to solve the hard subtask of generating semantically precise and syntactically correct natural language sentences, which is actually unrelated to the task of video annotation, (iii) it is difficult to quantitatively measure performance, as standard metrics (e.g., accuracy and F1-score) are inapplicable, and (iv) annotations are language-specific. In this paper, we propose the new task of knowledge graph extraction from videos, i.e., producing a description in the form of a knowledge graph of the contents of a given video. Since no datasets exist for this task, we also include a method to automatically generate them, starting from datasets where videos are annotated with natural language. We then describe an initial deep-learning model for knowledge graph extraction from videos, and report results on MSVD* and MSR-VTT*, two datasets obtained from MSVD and MSR-VTT using our method.

</details>

<details>

<summary>2020-07-20 13:32:02 - Shopping in the Multiverse: A Counterfactual Approach to In-Session Attribution</summary>

- *Jacopo Tagliabue, Bingqing Yu*

- `2007.10087v1` - [abs](http://arxiv.org/abs/2007.10087v1) - [pdf](http://arxiv.org/pdf/2007.10087v1)

> We tackle the challenge of in-session attribution for on-site search engines in eCommerce. We phrase the problem as a causal counterfactual inference, and contrast the approach with rule-based systems from industry settings and prediction models from the multi-touch attribution literature. We approach counterfactuals in analogy with treatments in formal semantics, explicitly modeling possible outcomes through alternative shopper timelines; in particular, we propose to learn a generative browsing model over a target shop, leveraging the latent space induced by prod2vec embeddings; we show how natural language queries can be effectively represented in the same space and how "search intervention" can be performed to assess causal contribution. Finally, we validate the methodology on a synthetic dataset, mimicking important patterns emerged in customer interviews and qualitative analysis, and we present preliminary findings on an industry dataset from a partnering shop.

</details>

<details>

<summary>2020-07-20 16:08:36 - Learning latent representations across multiple data domains using Lifelong VAEGAN</summary>

- *Fei Ye, Adrian G. Bors*

- `2007.10221v1` - [abs](http://arxiv.org/abs/2007.10221v1) - [pdf](http://arxiv.org/pdf/2007.10221v1)

> The problem of catastrophic forgetting occurs in deep learning models trained on multiple databases in a sequential manner. Recently, generative replay mechanisms (GRM), have been proposed to reproduce previously learned knowledge aiming to reduce the forgetting. However, such approaches lack an appropriate inference model and therefore can not provide latent representations of data. In this paper, we propose a novel lifelong learning approach, namely the Lifelong VAEGAN (L-VAEGAN), which not only induces a powerful generative replay network but also learns meaningful latent representations, benefiting representation learning. L-VAEGAN can allow to automatically embed the information associated with different domains into several clusters in the latent space, while also capturing semantically meaningful shared latent variables, across different data domains. The proposed model supports many downstream tasks that traditional generative replay methods can not, including interpolation and inference across different data domains.

</details>

<details>

<summary>2020-07-20 21:34:27 - Automated Measurements of Key Morphological Features of Human Embryos for IVF</summary>

- *Brian D. Leahy, Won-Dong Jang, Helen Y. Yang, Robbert Struyven, Donglai Wei, Zhe Sun, Kylie R. Lee, Charlotte Royston, Liz Cam, Yael Kalma, Foad Azem, Dalit Ben-Yosef, Hanspeter Pfister, Daniel Needleman*

- `2006.00067v2` - [abs](http://arxiv.org/abs/2006.00067v2) - [pdf](http://arxiv.org/pdf/2006.00067v2)

> A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the highest quality embryo to transfer to the patient in the hopes of achieving a pregnancy. Time-lapse microscopy provides clinicians with a wealth of information for selecting embryos. However, the resulting movies of embryos are currently analyzed manually, which is time consuming and subjective. Here, we automate feature extraction of time-lapse microscopy of human embryos with a machine-learning pipeline of five convolutional neural networks (CNNs). Our pipeline consists of (1) semantic segmentation of the regions of the embryo, (2) regression predictions of fragment severity, (3) classification of the developmental stage, and object instance segmentation of (4) cells and (5) pronuclei. Our approach greatly speeds up the measurement of quantitative, biologically relevant features that may aid in embryo selection.

</details>

<details>

<summary>2020-07-21 04:16:46 - AinnoSeg: Panoramic Segmentation with High Perfomance</summary>

- *Jiahong Wu, Jianfei Lu, Xinxin Kang, Yiming Zhang, Yinhang Tang, Jianfei Song, Ze Huang, Shenglan Ben, Jiashui Huang, Faen Zhang*

- `2007.10591v1` - [abs](http://arxiv.org/abs/2007.10591v1) - [pdf](http://arxiv.org/pdf/2007.10591v1)

> Panoramic segmentation is a scene where image segmentation tasks is more difficult. With the development of CNN networks, panoramic segmentation tasks have been sufficiently developed.However, the current panoramic segmentation algorithms are more concerned with context semantics, but the details of image are not processed enough. Moreover, they cannot solve the problems which contains the accuracy of occluded object segmentation,little object segmentation,boundary pixel in object segmentation etc. Aiming to address these issues, this paper presents some useful tricks. (a) By changing the basic segmentation model, the model can take into account the large objects and the boundary pixel classification of image details. (b) Modify the loss function so that it can take into account the boundary pixels of multiple objects in the image. (c) Use a semi-supervised approach to regain control of the training process. (d) Using multi-scale training and reasoning. All these operations named AinnoSeg, AinnoSeg can achieve state-of-art performance on the well-known dataset ADE20K.

</details>

<details>

<summary>2020-07-21 09:01:52 - Morphological Skip-Gram: Using morphological knowledge to improve word representation</summary>

- *Flávio Santos, Hendrik Macedo, Thiago Bispo, Cleber Zanchettin*

- `2007.10055v2` - [abs](http://arxiv.org/abs/2007.10055v2) - [pdf](http://arxiv.org/pdf/2007.10055v2)

> Natural language processing models have attracted much interest in the deep learning community. This branch of study is composed of some applications such as machine translation, sentiment analysis, named entity recognition, question and answer, and others. Word embeddings are continuous word representations, they are an essential module for those applications and are generally used as input word representation to the deep learning models. Word2Vec and GloVe are two popular methods to learn word embeddings. They achieve good word representations, however, they learn representations with limited information because they ignore the morphological information of the words and consider only one representation vector for each word. This approach implies that Word2Vec and GloVe are unaware of the word inner structure. To mitigate this problem, the FastText model represents each word as a bag of characters n-grams. Hence, each n-gram has a continuous vector representation, and the final word representation is the sum of its characters n-grams vectors. Nevertheless, the use of all n-grams character of a word is a poor approach since some n-grams have no semantic relation with their words and increase the amount of potentially useless information. This approach also increases the training phase time. In this work, we propose a new method for training word embeddings, and its goal is to replace the FastText bag of character n-grams for a bag of word morphemes through the morphological analysis of the word. Thus, words with similar context and morphemes are represented by vectors close to each other. To evaluate our new approach, we performed intrinsic evaluations considering 15 different tasks, and the results show a competitive performance compared to FastText.

</details>

<details>

<summary>2020-07-21 11:21:26 - Human Abnormality Detection Based on Bengali Text</summary>

- *M. F. Mridha, Md. Saifur Rahman, Abu Quwsar Ohi*

- `2007.10718v1` - [abs](http://arxiv.org/abs/2007.10718v1) - [pdf](http://arxiv.org/pdf/2007.10718v1)

> In the field of natural language processing and human-computer interaction, human attitudes and sentiments have attracted the researchers. However, in the field of human-computer interaction, human abnormality detection has not been investigated extensively and most works depend on image-based information. In natural language processing, effective meaning can potentially convey by all words. Each word may bring out difficult encounters because of their semantic connection with ideas or categories. In this paper, an efficient and effective human abnormality detection model is introduced, that only uses Bengali text. This proposed model can recognize whether the person is in a normal or abnormal state by analyzing their typed Bengali text. To the best of our knowledge, this is the first attempt in developing a text based human abnormality detection system. We have created our Bengali dataset (contains 2000 sentences) that is generated by voluntary conversations. We have performed the comparative analysis by using Naive Bayes and Support Vector Machine as classifiers. Two different feature extraction techniques count vector, and TF-IDF is used to experiment on our constructed dataset. We have achieved a maximum 89% accuracy and 92% F1-score with our constructed dataset in our experiment.

</details>

<details>

<summary>2020-07-21 14:05:51 - BAKSA at SemEval-2020 Task 9: Bolstering CNN with Self-Attention for Sentiment Analysis of Code Mixed Text</summary>

- *Ayush Kumar, Harsh Agarwal, Keshav Bansal, Ashutosh Modi*

- `2007.10819v1` - [abs](http://arxiv.org/abs/2007.10819v1) - [pdf](http://arxiv.org/pdf/2007.10819v1)

> Sentiment Analysis of code-mixed text has diversified applications in opinion mining ranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for sentiment analysis of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct sentiment among multiple sentiment bearing units. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh_6 respectively.

</details>

<details>

<summary>2020-07-21 14:05:56 - IITK at SemEval-2020 Task 10: Transformers for Emphasis Selection</summary>

- *Vipul Singhal, Sahil Dhull, Rishabh Agarwal, Ashutosh Modi*

- `2007.10820v1` - [abs](http://arxiv.org/abs/2007.10820v1) - [pdf](http://arxiv.org/pdf/2007.10820v1)

> This paper describes the system proposed for addressing the research problem posed in Task 10 of SemEval-2020: Emphasis Selection For Written Text in Visual Media. We propose an end-to-end model that takes as input the text and corresponding to each word gives the probability of the word to be emphasized. Our results show that transformer-based models are particularly effective in this task. We achieved the best Matchm score (described in section 2.2) of 0.810 and were ranked third on the leaderboard.

</details>

<details>

<summary>2020-07-21 14:06:26 - IITK at SemEval-2020 Task 8: Unimodal and Bimodal Sentiment Analysis of Internet Memes</summary>

- *Vishal Keswani, Sakshi Singh, Suryansh Agarwal, Ashutosh Modi*

- `2007.10822v1` - [abs](http://arxiv.org/abs/2007.10822v1) - [pdf](http://arxiv.org/pdf/2007.10822v1)

> Social media is abundant in visual and textual information presented together or in isolation. Memes are the most popular form, belonging to the former class. In this paper, we present our approaches for the Memotion Analysis problem as posed in SemEval-2020 Task 8. The goal of this task is to classify memes based on their emotional content and sentiment. We leverage techniques from Natural Language Processing (NLP) and Computer Vision (CV) towards the sentiment classification of internet memes (Subtask A). We consider Bimodal (text and image) as well as Unimodal (text-only) techniques in our study ranging from the Na\"ive Bayes classifier to Transformer-based approaches. Our results show that a text-only approach, a simple Feed Forward Neural Network (FFNN) with Word2vec embeddings as input, performs superior to all the others. We stand first in the Sentiment analysis task with a relative improvement of 63% over the baseline macro-F1 score. Our work is relevant to any task concerned with the combination of different modalities.

</details>

<details>

<summary>2020-07-21 14:06:59 - newsSweeper at SemEval-2020 Task 11: Context-Aware Rich Feature Representations For Propaganda Classification</summary>

- *Paramansh Singh, Siraj Sandhu, Subham Kumar, Ashutosh Modi*

- `2007.10827v1` - [abs](http://arxiv.org/abs/2007.10827v1) - [pdf](http://arxiv.org/pdf/2007.10827v1)

> This paper describes our submissions to SemEval 2020 Task 11: Detection of Propaganda Techniques in News Articles for each of the two subtasks of Span Identification and Technique Classification. We make use of pre-trained BERT language model enhanced with tagging techniques developed for the task of Named Entity Recognition (NER), to develop a system for identifying propaganda spans in the text. For the second subtask, we incorporate contextual features in a pre-trained RoBERTa model for the classification of propaganda techniques. We were ranked 5th in the propaganda technique classification subtask.

</details>

<details>

<summary>2020-07-21 14:08:02 - CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE</summary>

- *Soumya Ranjan Dash, Sandeep Routray, Prateek Varshney, Ashutosh Modi*

- `2007.10830v1` - [abs](http://arxiv.org/abs/2007.10830v1) - [pdf](http://arxiv.org/pdf/2007.10830v1)

> In this paper, we describe our system for Task 4 of SemEval 2020, which involves differentiating between natural language statements that confirm to common sense and those that do not. The organizers propose three subtasks - first, selecting between two sentences, the one which is against common sense. Second, identifying the most crucial reason why a statement does not make sense. Third, generating novel reasons for explaining the against common sense statement. Out of the three subtasks, this paper reports the system description of subtask A and subtask B. This paper proposes a model based on transformer neural network architecture for addressing the subtasks. The novelty in work lies in the architecture design, which handles the logical implication of contradicting statements and simultaneous information extraction from both sentences. We use a parallel instance of transformers, which is responsible for a boost in the performance. We achieved an accuracy of 94.8% in subtask A and 89% in subtask B on the test set.

</details>

<details>

<summary>2020-07-21 14:45:53 - IITK-RSA at SemEval-2020 Task 5: Detecting Counterfactuals</summary>

- *Anirudh Anil Ojha, Rohin Garg, Shashank Gupta, Ashutosh Modi*

- `2007.10866v1` - [abs](http://arxiv.org/abs/2007.10866v1) - [pdf](http://arxiv.org/pdf/2007.10866v1)

> This paper describes our efforts in tackling Task 5 of SemEval-2020. The task involved detecting a class of textual expressions known as counterfactuals and separating them into their constituent elements. Counterfactual statements describe events that have not or could not have occurred and the possible implications of such events. While counterfactual reasoning is natural for humans, understanding these expressions is difficult for artificial agents due to a variety of linguistic subtleties. Our final submitted approaches were an ensemble of various fine-tuned transformer-based and CNN-based models for the first subtask and a transformer model with dependency tree information for the second subtask. We ranked 4-th and 9-th in the overall leaderboard. We also explored various other approaches that involved the use of classical methods, other neural architectures and the incorporation of different linguistic features.

</details>

<details>

<summary>2020-07-21 15:06:58 - problemConquero at SemEval-2020 Task 12: Transformer and Soft label-based approaches</summary>

- *Karishma Laud, Jagriti Singh, Randeep Kumar Sahu, Ashutosh Modi*

- `2007.10877v1` - [abs](http://arxiv.org/abs/2007.10877v1) - [pdf](http://arxiv.org/pdf/2007.10877v1)

> In this paper, we present various systems submitted by our team problemConquero for SemEval-2020 Shared Task 12 Multilingual Offensive Language Identification in Social Media. We participated in all the three sub-tasks of OffensEval-2020, and our final submissions during the evaluation phase included transformer-based approaches and a soft label-based approach. BERT based fine-tuned models were submitted for each language of sub-task A (offensive tweet identification). RoBERTa based fine-tuned model for sub-task B (automatic categorization of offense types) was submitted. We submitted two models for sub-task C (offense target identification), one using soft labels and the other using BERT based fine-tuned model. Our ranks for sub-task A were Greek-19 out of 37, Turkish-22 out of 46, Danish-26 out of 39, Arabic-39 out of 53, and English-20 out of 85. We achieved a rank of 28 out of 43 for sub-task B. Our best rank for sub-task C was 20 out of 39 using BERT based fine-tuned model.

</details>

<details>

<summary>2020-07-21 22:09:07 - Ordered Functional Decision Diagrams: A Functional Semantics For Binary Decision Diagrams</summary>

- *Joan Thibault, Khalil Ghorbal*

- `2003.09340v4` - [abs](http://arxiv.org/abs/2003.09340v4) - [pdf](http://arxiv.org/pdf/2003.09340v4)

> We introduce a novel framework, termed $\lambda$DD, that revisits Binary Decision Diagrams from a purely functional point of view. The framework allows to classify the already existing variants, including the most recent ones like Chain-DD and ESRBDD, as implementations of a special class of ordered models. We enumerate, in a principled way, all the models of this class and isolate its most expressive model. This new model, termed $\lambda$DD-O-NUCX, is suitable for both dense and sparse Boolean functions, and is moreover invariant by negation. The canonicity of $\lambda$DD-O-NUCX is formally verified using the Coq proof assistant. We furthermore give bounds on the size of the different diagrams: the potential gain achieved by more expressive models can be at most linear in the number of variables n.

</details>

<details>

<summary>2020-07-22 00:20:23 - Semi-Supervised Learning Approach to Discover Enterprise User Insights from Feedback and Support</summary>

- *Xin Deng, Ross Smith, Genevieve Quintin*

- `2007.09303v3` - [abs](http://arxiv.org/abs/2007.09303v3) - [pdf](http://arxiv.org/pdf/2007.09303v3)

> With the evolution of the cloud and customer centric culture, we inherently accumulate huge repositories of textual reviews, feedback, and support data.This has driven enterprises to seek and research engagement patterns, user network analysis, topic detections, etc.However, huge manual work is still necessary to mine data to be able to mine actionable outcomes. In this paper, we proposed and developed an innovative Semi-Supervised Learning approach by utilizing Deep Learning and Topic Modeling to have a better understanding of the user voice.This approach combines a BERT-based multiclassification algorithm through supervised learning combined with a novel Probabilistic and Semantic Hybrid Topic Inference (PSHTI) Model through unsupervised learning, aiming at automating the process of better identifying the main topics or areas as well as the sub-topics from the textual feedback and support.There are three major break-through: 1. As the advancement of deep learning technology, there have been tremendous innovations in the NLP field, yet the traditional topic modeling as one of the NLP applications lag behind the tide of deep learning. In the methodology and technical perspective, we adopt transfer learning to fine-tune a BERT-based multiclassification system to categorize the main topics and then utilize the novel PSHTI model to infer the sub-topics under the predicted main topics. 2. The traditional unsupervised learning-based topic models or clustering methods suffer from the difficulty of automatically generating a meaningful topic label, but our system enables mapping the top words to the self-help issues by utilizing domain knowledge about the product through web-crawling. 3. This work provides a prominent showcase by leveraging the state-of-the-art methodology in the real production to help shed light to discover user insights and drive business investment priorities.

</details>

<details>

<summary>2020-07-22 03:03:51 - An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on Bert</summary>

- *Wei Bao, Hongshu Che, Jiandong Zhang*

- `2005.01006v3` - [abs](http://arxiv.org/abs/2005.01006v3) - [pdf](http://arxiv.org/pdf/2005.01006v3)

> Natural Language Processing (NLP) has been widely used in the semantic analysis in recent years. Our paper mainly discusses a methodology to analyze the effect that context has on human perception of similar words, which is the third task of SemEval 2020. We apply several methods in calculating the distance between two embedding vector generated by Bidirectional Encoder Representation from Transformer (BERT). Our team will_go won the 1st place in Finnish language track of subtask1, the second place in English track of subtask1.

</details>

<details>

<summary>2020-07-22 04:56:23 - IITK at the FinSim Task: Hypernym Detection in Financial Domain via Context-Free and Contextualized Word Embeddings</summary>

- *Vishal Keswani, Sakshi Singh, Ashutosh Modi*

- `2007.11201v1` - [abs](http://arxiv.org/abs/2007.11201v1) - [pdf](http://arxiv.org/pdf/2007.11201v1)

> In this paper, we present our approaches for the FinSim 2020 shared task on "Learning Semantic Representations for the Financial Domain". The goal of this task is to classify financial terms into the most relevant hypernym (or top-level) concept in an external ontology. We leverage both context-dependent and context-independent word embeddings in our analysis. Our systems deploy Word2vec embeddings trained from scratch on the corpus (Financial Prospectus in English) along with pre-trained BERT embeddings. We divide the test dataset into two subsets based on a domain rule. For one subset, we use unsupervised distance measures to classify the term. For the second subset, we use simple supervised classifiers like Naive Bayes, on top of the embeddings, to arrive at a final prediction. Finally, we combine both the results. Our system ranks 1st based on both the metrics, i.e., mean rank and accuracy.

</details>

<details>

<summary>2020-07-22 08:00:15 - Editable Neural Networks</summary>

- *Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko*

- `2004.00345v2` - [abs](http://arxiv.org/abs/2004.00345v2) - [pdf](http://arxiv.org/pdf/2004.00345v2)

> These days deep neural networks are ubiquitously used in a wide range of tasks, from image classification and machine translation to face identification and self-driving cars. In many applications, a single model error can lead to devastating financial, reputational and even life-threatening consequences. Therefore, it is crucially important to correct model mistakes quickly as they appear. In this work, we investigate the problem of neural network editing $-$ how one can efficiently patch a mistake of the model on a particular sample, without influencing the model behavior on other samples. Namely, we propose Editable Training, a model-agnostic training technique that encourages fast editing of the trained model. We empirically demonstrate the effectiveness of this method on large-scale image classification and machine translation tasks.

</details>

<details>

<summary>2020-07-22 09:35:04 - Deep Learning Based Segmentation of Various Brain Lesions for Radiosurgery</summary>

- *Siang-Ruei Wu, Hao-Yun Chang, Florence T Su, Heng-Chun Liao, Wanju Tseng, Chun-Chih Liao, Feipei Lai, Feng-Ming Hsu, Furen Xiao*

- `2007.11784v1` - [abs](http://arxiv.org/abs/2007.11784v1) - [pdf](http://arxiv.org/pdf/2007.11784v1)

> Semantic segmentation of medical images with deep learning models is rapidly developed. In this study, we benchmarked state-of-the-art deep learning segmentation algorithms on our clinical stereotactic radiosurgery dataset, demonstrating the strengths and weaknesses of these algorithms in a fairly practical scenario. In particular, we compared the model performances with respect to their sampling method, model architecture, and the choice of loss functions, identifying the suitable settings for their applications and shedding light on the possible improvements.

</details>

<details>

<summary>2020-07-22 12:47:11 - To Be or Not To Be a Verbal Multiword Expression: A Quest for Discriminating Features</summary>

- *Caroline Pasquer, Agata Savary, Jean-Yves Antoine, Carlos Ramisch, Nicolas Labroche, Arnaud Giacometti*

- `2007.11381v1` - [abs](http://arxiv.org/abs/2007.11381v1) - [pdf](http://arxiv.org/pdf/2007.11381v1)

> Automatic identification of mutiword expressions (MWEs) is a pre-requisite for semantically-oriented downstream applications. This task is challenging because MWEs, especially verbal ones (VMWEs), exhibit surface variability. However, this variability is usually more restricted than in regular (non-VMWE) constructions, which leads to various variability profiles. We use this fact to determine the optimal set of features which could be used in a supervised classification setting to solve a subproblem of VMWE identification: the identification of occurrences of previously seen VMWEs. Surprisingly, a simple custom frequency-based feature selection method proves more efficient than other standard methods such as Chi-squared test, information gain or decision trees. An SVM classifier using the optimal set of only 6 features outperforms the best systems from a recent shared task on the French seen data.

</details>

<details>

<summary>2020-07-22 14:08:33 - Fast and Precise On-the-fly Patch Validation for All</summary>

- *Lingchao Chen, Lingming Zhang*

- `2007.11449v1` - [abs](http://arxiv.org/abs/2007.11449v1) - [pdf](http://arxiv.org/pdf/2007.11449v1)

> Generate-and-validate (G&V) automated program repair (APR) techniques have been extensively studied during the past decade. Meanwhile, such techniques can be extremely time-consuming due to manipulation of the program code to fabricate a large number of patches and also repeated executions of tests on patches to identify potential fixes. PraPR, a recent G&V APR technique, reduces these costs by modifying program code directly at the level of compiled bytecode, and further performing on-the-fly patching by allowing multiple patches to be tested within the same JVM session. However, PraPR is limited due to its pattern-based, bytecode-level nature and it is basically unsound/imprecise as it assumes that patch executions do not change global JVM state and affect later patch executions on the same JVM session. Inspired by the PraPR work, we propose a unified patch validation framework, named UniAPR, which aims to speed up the patch validation for both bytecode and source-code APR via on-the-fly patching; furthermore, UniAPR addresses the imprecise patch validation issue by resetting the JVM global state via runtime bytecode transformation. We have implemented UniAPR as a fully automated Maven Plugin. We have also performed the first study of on-the-fly patch validation for state-of-the-art source-code-level APR. Our experiments show the first empirical evidence that vanilla on-the-fly patch validation can be imprecise/unsound; in contrast, our UniAPR framework can speed up state-of-the-art APR by over an order of magnitude without incurring any imprecision in patch validation, enabling all existing APR techniques to explore a larger search space to fix more bugs in the near future. Furthermore, UniAPR directly enables hybrid source and bytecode APR to fix substantially more bugs than all state-of-the-art APR techniques (under the same time limit) in the near future.

</details>

<details>

<summary>2020-07-22 17:14:00 - Symbolic Partial-Order Execution for Testing Multi-Threaded Programs</summary>

- *Daniel Schemmel, Julian Büning, César Rodríguez, David Laprell, Klaus Wehrle*

- `2005.06688v2` - [abs](http://arxiv.org/abs/2005.06688v2) - [pdf](http://arxiv.org/pdf/2005.06688v2)

> We describe a technique for systematic testing of multi-threaded programs. We combine Quasi-Optimal Partial-Order Reduction, a state-of-the-art technique that tackles path explosion due to interleaving non-determinism, with symbolic execution to handle data non-determinism. Our technique iteratively and exhaustively finds all executions of the program. It represents program executions using partial orders and finds the next execution using an underlying unfolding semantics. We avoid the exploration of redundant program traces using cutoff events. We implemented our technique as an extension of KLEE and evaluated it on a set of large multi-threaded C programs. Our experiments found several previously undiscovered bugs and undefined behaviors in memcached and GNU sort, showing that the new method is capable of finding bugs in industrial-size benchmarks.

</details>

<details>

<summary>2020-07-22 17:59:49 - Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors</summary>

- *Zuxuan Wu, Ser-Nam Lim, Larry Davis, Tom Goldstein*

- `1910.14667v2` - [abs](http://arxiv.org/abs/1910.14667v2) - [pdf](http://arxiv.org/pdf/1910.14667v2)

> We present a systematic study of adversarial attacks on state-of-the-art object detection frameworks. Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors. Through extensive experiments, we benchmark the effectiveness of adversarially trained patches under both white-box and black-box settings, and quantify transferability of attacks between datasets, object classes, and detector models. Finally, we present a detailed study of physical world attacks using printed posters and wearable clothes, and rigorously quantify the performance of such attacks with different metrics.

</details>

<details>

<summary>2020-07-22 20:51:58 - Analogical Reasoning for Visually Grounded Language Acquisition</summary>

- *Bo Wu, Haoyu Qin, Alireza Zareian, Carl Vondrick, Shih-Fu Chang*

- `2007.11668v1` - [abs](http://arxiv.org/abs/2007.11668v1) - [pdf](http://arxiv.org/pdf/2007.11668v1)

> Children acquire language subconsciously by observing the surrounding world and listening to descriptions. They can discover the meaning of words even without explicit language knowledge, and generalize to novel compositions effortlessly. In this paper, we bring this ability to AI, by studying the task of Visually grounded Language Acquisition (VLA). We propose a multimodal transformer model augmented with a novel mechanism for analogical reasoning, which approximates novel compositions by learning semantic mapping and reasoning operations from previously seen compositions. Our proposed method, Analogical Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data (video frames and transcripts), and after observing a set of compositions such as "washing apple" or "cutting carrot", it can generalize and recognize new compositions in new video frames, such as "washing carrot" or "cutting apple". To this end, ARTNet refers to relevant instances in the training data and uses their visual features and captions to establish analogies with the query image. Then it chooses the suitable verb and noun to create a new composition that describes the new image best. Extensive experiments on an instructional video dataset demonstrate that the proposed method achieves significantly better generalization capability and recognition accuracy compared to state-of-the-art transformer models.

</details>

<details>

<summary>2020-07-23 08:18:02 - METEOR: Learning Memory and Time Efficient Representations from Multi-modal Data Streams</summary>

- *Amila Silva, Shanika Karunasekera, Christopher Leckie, Ling Luo*

- `2007.11847v1` - [abs](http://arxiv.org/abs/2007.11847v1) - [pdf](http://arxiv.org/pdf/2007.11847v1)

> Many learning tasks involve multi-modal data streams, where continuous data from different modes convey a comprehensive description about objects. A major challenge in this context is how to efficiently interpret multi-modal information in complex environments. This has motivated numerous studies on learning unsupervised representations from multi-modal data streams. These studies aim to understand higher-level contextual information (e.g., a Twitter message) by jointly learning embeddings for the lower-level semantic units in different modalities (e.g., text, user, and location of a Twitter message). However, these methods directly associate each low-level semantic unit with a continuous embedding vector, which results in high memory requirements. Hence, deploying and continuously learning such models in low-memory devices (e.g., mobile devices) becomes a problem. To address this problem, we present METEOR, a novel MEmory and Time Efficient Online Representation learning technique, which: (1) learns compact representations for multi-modal data by sharing parameters within semantically meaningful groups and preserves the domain-agnostic semantics; (2) can be accelerated using parallel processes to accommodate different stream rates while capturing the temporal changes of the units; and (3) can be easily extended to capture implicit/explicit external knowledge related to multi-modal data streams. We evaluate METEOR using two types of multi-modal data streams (i.e., social media streams and shopping transaction streams) to demonstrate its ability to adapt to different domains. Our results show that METEOR preserves the quality of the representations while reducing memory usage by around 80% compared to the conventional memory-intensive embeddings.

</details>

<details>

<summary>2020-07-23 10:11:43 - Harnessing spatial homogeneity of neuroimaging data: patch individual filter layers for CNNs</summary>

- *Fabian Eitel, Jan Philipp Albrecht, Martin Weygandt, Friedemann Paul, Kerstin Ritter*

- `2007.11899v1` - [abs](http://arxiv.org/abs/2007.11899v1) - [pdf](http://arxiv.org/pdf/2007.11899v1)

> Neuroimaging data, e.g. obtained from magnetic resonance imaging (MRI), is comparably homogeneous due to (1) the uniform structure of the brain and (2) additional efforts to spatially normalize the data to a standard template using linear and non-linear transformations. Convolutional neural networks (CNNs), in contrast, have been specifically designed for highly heterogeneous data, such as natural images, by sliding convolutional filters over different positions in an image. Here, we suggest a new CNN architecture that combines the idea of hierarchical abstraction in neural networks with a prior on the spatial homogeneity of neuroimaging data: Whereas early layers are trained globally using standard convolutional layers, we introduce for higher, more abstract layers patch individual filters (PIF). By learning filters in individual image regions (patches) without sharing weights, PIF layers can learn abstract features faster and with fewer samples. We thoroughly evaluated PIF layers for three different tasks and data sets, namely sex classification on UK Biobank data, Alzheimer's disease detection on ADNI data and multiple sclerosis detection on private hospital data. We demonstrate that CNNs using PIF layers result in higher accuracies, especially in low sample size settings, and need fewer training epochs for convergence. To the best of our knowledge, this is the first study which introduces a prior on brain MRI for CNN learning.

</details>

<details>

<summary>2020-07-23 15:07:58 - Interpreting the Latent Space of GANs via Correlation Analysis for Controllable Concept Manipulation</summary>

- *Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li*

- `2006.10132v2` - [abs](http://arxiv.org/abs/2006.10132v2) - [pdf](http://arxiv.org/pdf/2006.10132v2)

> Generative adversarial nets (GANs) have been successfully applied in many fields like image generation, inpainting, super-resolution and drug discovery, etc., by now, the inner process of GANs is far from been understood. To get deeper insight of the intrinsic mechanism of GANs, in this paper, a method for interpreting the latent space of GANs by analyzing the correlation between latent variables and the corresponding semantic contents in generated images is proposed. Unlike previous methods that focus on dissecting models via feature visualization, the emphasis of this work is put on the variables in latent space, i.e. how the latent variables affect the quantitative analysis of generated results. Given a pretrained GAN model with weights fixed, the latent variables are intervened to analyze their effect on the semantic content in generated images. A set of controlling latent variables can be derived for specific content generation, and the controllable semantic content manipulation be achieved. The proposed method is testified on the datasets Fashion-MNIST and UT Zappos50K, experiment results show its effectiveness.

</details>

<details>

<summary>2020-07-23 15:39:53 - HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for Code-Mixed Texts</summary>

- *Aditya Srivastava, V. Harsha Vardhan*

- `2007.12076v1` - [abs](http://arxiv.org/abs/2007.12076v1) - [pdf](http://arxiv.org/pdf/2007.12076v1)

> Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with. In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts, and with an F1 score of 67.1%, we demonstrate that simple convolution and attention may well produce reasonable results.

</details>

<details>

<summary>2020-07-23 16:03:50 - Word Embeddings: Stability and Semantic Change</summary>

- *Lucas Rettenmeier*

- `2007.16006v1` - [abs](http://arxiv.org/abs/2007.16006v1) - [pdf](http://arxiv.org/pdf/2007.16006v1)

> Word embeddings are computed by a class of techniques within natural language processing (NLP), that create continuous vector representations of words in a language from a large text corpus. The stochastic nature of the training process of most embedding techniques can lead to surprisingly strong instability, i.e. subsequently applying the same technique to the same data twice, can produce entirely different results. In this work, we present an experimental study on the instability of the training process of three of the most influential embedding techniques of the last decade: word2vec, GloVe and fastText. Based on the experimental results, we propose a statistical model to describe the instability of embedding techniques and introduce a novel metric to measure the instability of the representation of an individual word. Finally, we propose a method to minimize the instability - by computing a modified average over multiple runs - and apply it to a specific linguistic problem: The detection and quantification of semantic change, i.e. measuring changes in the meaning and usage of words over time.

</details>

<details>

<summary>2020-07-23 16:27:48 - Web Similarity in Sets of Search Terms using Database Queries</summary>

- *Andrew R. Cohen, Paul M. B. Vitanyi*

- `1502.05957v2` - [abs](http://arxiv.org/abs/1502.05957v2) - [pdf](http://arxiv.org/pdf/1502.05957v2)

> Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or another large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a common similarity (common semantics) on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity of members of a set according to all (upper semi)computable properties. We develop the theory and give applications of classifying using Amazon, Wikipedia, and the NCBI website from the National Institutes of Health. The last gives new correlations between health hazards. A restriction of the NWD to a set of two yields the earlier normalized google distance (NGD) but no combination of the NGD's of pairs in a set can extract the information the NWD extracts from the set. The NWD enables a new contextual (different databases) learning approachbased on Kolmogorov complexity theory that incorporates knowledge from these databases.

</details>

<details>

<summary>2020-07-23 19:07:06 - Efficient Residue Number System Based Winograd Convolution</summary>

- *Zhi-Gang Liu, Matthew Mattina*

- `2007.12216v1` - [abs](http://arxiv.org/abs/2007.12216v1) - [pdf](http://arxiv.org/pdf/2007.12216v1)

> Prior research has shown that Winograd algorithm can reduce the computational complexity of convolutional neural networks (CNN) with weights and activations represented in floating point. However it is difficult to apply the scheme to the inference of low-precision quantized (e.g. INT8) networks. Our work extends the Winograd algorithm to Residue Number System (RNS). The minimal complexity convolution is computed precisely over large transformation tile (e.g. 10 x 10 to 16 x 16) of filters and activation patches using the Winograd transformation and low cost (e.g. 8-bit) arithmetic without degrading the prediction accuracy of the networks during inference. The arithmetic complexity reduction is up to 7.03x while the performance improvement is up to 2.30x to 4.69x for 3 x 3 and 5 x 5 filters respectively.

</details>

<details>

<summary>2020-07-24 02:03:55 - Validation of Automatically Generated Patches: An Appetizer</summary>

- *Ali Ghanbari*

- `1912.00117v2` - [abs](http://arxiv.org/abs/1912.00117v2) - [pdf](http://arxiv.org/pdf/1912.00117v2)

> In the context of test case based automated program repair (APR), the research community call the patches that pass all the test cases but fail to actually fix the bug test case overfitted patches. Currently, overfitted patches has to be manually inspected by the users. Being a labor intensive activity that hinders widespread adoption of APR tools, automatic validation of APR-generated patches has been the topic of research in recent years. In this paper, we point out the limitations of the existing techniques/methodologies that call for further research, and introduce two promising directions toward effective automatic patch validation: (1) motivated by the relative effectiveness of anti-patterns, we propose to use statistical techniques to avoid the uncomputability of applying some of the anti-pattern rules and automate the technique. Our results show that we achieve at least 57% precision. (2) We present a proposal for a semi-automatic technique that helps the programmers in finding properties of the patched methods and stress testing the patches based on those properties so as to filter out overfitted ones as many as possible.

</details>

<details>

<summary>2020-07-24 02:16:03 - Neural Geometric Parser for Single Image Camera Calibration</summary>

- *Jinwoo Lee, Minhyuk Sung, Hyunjoon Lee, Junho Kim*

- `2007.11855v2` - [abs](http://arxiv.org/abs/2007.11855v2) - [pdf](http://arxiv.org/pdf/2007.11855v2)

> We propose a neural geometric parser learning single image camera calibration for man-made scenes. Unlike previous neural approaches that rely only on semantic cues obtained from neural networks, our approach considers both semantic and geometric cues, resulting in significant accuracy improvement. The proposed framework consists of two networks. Using line segments of an image as geometric cues, the first network estimates the zenith vanishing point and generates several candidates consisting of the camera rotation and focal length. The second network evaluates each candidate based on the given image and the geometric cues, where prior knowledge of man-made scenes is used for the evaluation. With the supervision of datasets consisting of the horizontal line and focal length of the images, our networks can be trained to estimate the same camera parameters. Based on the Manhattan world assumption, we can further estimate the camera rotation and focal length in a weakly supervised manner. The experimental results reveal that the performance of our neural approach is significantly higher than that of existing state-of-the-art camera calibration techniques for single images of indoor and outdoor scenes.

</details>

<details>

<summary>2020-07-24 02:55:13 - Large image datasets: A pyrrhic win for computer vision?</summary>

- *Vinay Uday Prabhu, Abeba Birhane*

- `2006.16923v2` - [abs](http://arxiv.org/abs/2006.16923v2) - [pdf](http://arxiv.org/pdf/2006.16923v2)

> In this paper we investigate problematic practices and consequences of large scale vision datasets. We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class-wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both society broadly and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique the pros and cons of these. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation processes.

</details>

<details>

<summary>2020-07-24 08:01:29 - A Semantics-Assisted Video Captioning Model Trained with Scheduled Sampling</summary>

- *Haoran Chen, Ke Lin, Alexander Maye, Jianming Li, Xiaolin Hu*

- `1909.00121v3` - [abs](http://arxiv.org/abs/1909.00121v3) - [pdf](http://arxiv.org/pdf/1909.00121v3)

> Given the features of a video, recurrent neural networks can be used to automatically generate a caption for the video. Existing methods for video captioning have at least three limitations. First, semantic information has been widely applied to boost the performance of video captioning models, but existing networks often fail to provide meaningful semantic features. Second, the Teacher Forcing algorithm is often utilized to optimize video captioning models, but during training and inference, different strategies are applied to guide word generation, leading to poor performance. Third, current video captioning models are prone to generate relatively short captions that express video contents inappropriately. Toward resolving these three problems, we suggest three corresponding improvements. First of all, we propose a metric to compare the quality of semantic features, and utilize appropriate features as input for a semantic detection network (SDN) with adequate complexity in order to generate meaningful semantic features for videos. Then, we apply a scheduled sampling strategy that gradually transfers the training phase from a teacher-guided manner toward a more self-teaching manner. Finally, the ordinary logarithm probability loss function is leveraged by sentence length so that the inclination of generating short sentences is alleviated. Our model achieves better results than previous models on the YouTube2Text dataset and is competitive with the previous best model on the MSR-VTT dataset.

</details>

<details>

<summary>2020-07-24 09:50:26 - MULTISEM at SemEval-2020 Task 3: Fine-tuning BERT for Lexical Meaning</summary>

- *Aina Garí Soler, Marianna Apidianaki*

- `2007.12432v1` - [abs](http://arxiv.org/abs/2007.12432v1) - [pdf](http://arxiv.org/pdf/2007.12432v1)

> We present the MULTISEM systems submitted to SemEval 2020 Task 3: Graded Word Similarity in Context (GWSC). We experiment with injecting semantic knowledge into pre-trained BERT models through fine-tuning on lexical semantic tasks related to GWSC. We use existing semantically annotated datasets and propose to approximate similarity through automatically generated lexical substitutes in context. We participate in both GWSC subtasks and address two languages, English and Finnish. Our best English models occupy the third and fourth positions in the ranking for the two subtasks. Performance is lower for the Finnish models which are mid-ranked in the respective subtasks, highlighting the important role of data availability for fine-tuning.

</details>

<details>

<summary>2020-07-24 13:53:04 - A short letter on the dot product between rotated Fourier transforms</summary>

- *Aaron R. Voelker*

- `2007.13462v1` - [abs](http://arxiv.org/abs/2007.13462v1) - [pdf](http://arxiv.org/pdf/2007.13462v1)

> Spatial Semantic Pointers (SSPs) have recently emerged as a powerful tool for representing and transforming continuous space, with numerous applications to cognitive modelling and deep learning. Fundamental to SSPs is the notion of "similarity" between vectors representing different points in $n$-dimensional space -- typically the dot product or cosine similarity between vectors with rotated unit-length complex coefficients in the Fourier domain. The similarity measure has previously been conjectured to be a Gaussian function of Euclidean distance. Contrary to this conjecture, we derive a simple trigonometric formula relating spatial displacement to similarity, and prove that, in the case where the Fourier coefficients are uniform i.i.d., the expected similarity is a product of normalized sinc functions: $\prod_{k=1}^{n} \operatorname{sinc} \left( a_k \right)$, where $\mathbf{a} \in \mathbb{R}^n$ is the spatial displacement between the two $n$-dimensional points. This establishes a direct link between space and the similarity of SSPs, which in turn helps bolster a useful mathematical framework for architecting neural networks that manipulate spatial structures.

</details>

<details>

<summary>2020-07-24 14:50:10 - Personalised Visual Art Recommendation by Learning Latent Semantic Representations</summary>

- *Bereket Abera Yilma, Najib Aghenda, Marcelo Romero, Yannick Naudet, Herve Panetto*

- `2008.02687v1` - [abs](http://arxiv.org/abs/2008.02687v1) - [pdf](http://arxiv.org/pdf/2008.02687v1)

> In Recommender systems, data representation techniques play a great role as they have the power to entangle, hide and reveal explanatory factors embedded within datasets. Hence, they influence the quality of recommendations. Specifically, in Visual Art (VA) recommendations the complexity of the concepts embodied within paintings, makes the task of capturing semantics by machines far from trivial. In VA recommendation, prominent works commonly use manually curated metadata to drive recommendations. Recent works in this domain aim at leveraging visual features extracted using Deep Neural Networks (DNN). However, such data representation approaches are resource demanding and do not have a direct interpretation, hindering user acceptance. To address these limitations, we introduce an approach for Personalised Recommendation of Visual arts based on learning latent semantic representation of paintings. Specifically, we trained a Latent Dirichlet Allocation (LDA) model on textual descriptions of paintings. Our LDA model manages to successfully uncover non-obvious semantic relationships between paintings whilst being able to offer explainable recommendations. Experimental evaluations demonstrate that our method tends to perform better than exploiting visual features extracted using pre-trained Deep Neural Networks.

</details>

<details>

<summary>2020-07-24 16:02:14 - IR-BERT: Leveraging BERT for Semantic Search in Background Linking for News Articles</summary>

- *Anup Anand Deshmukh, Udhav Sethi*

- `2007.12603v1` - [abs](http://arxiv.org/abs/2007.12603v1) - [pdf](http://arxiv.org/pdf/2007.12603v1)

> This work describes our two approaches for the background linking task of TREC 2020 News Track. The main objective of this task is to recommend a list of relevant articles that the reader should refer to in order to understand the context and gain background information of the query article. Our first approach focuses on building an effective search query by combining weighted keywords extracted from the query document and uses BM25 for retrieval. The second approach leverages the capability of SBERT (Nils Reimers et al.) to learn contextual representations of the query in order to perform semantic search over the corpus. We empirically show that employing a language model benefits our approach in understanding the context as well as the background of the query article. The proposed approaches are evaluated on the TREC 2018 Washington Post dataset and our best model outperforms the TREC median as well as the highest scoring model of 2018 in terms of the nDCG@5 metric. We further propose a diversity measure to evaluate the effectiveness of the various approaches in retrieving a diverse set of documents. This would potentially motivate researchers to work on introducing diversity in their recommended list. We have open sourced our implementation on Github and plan to submit our runs for the background linking task in TREC 2020.

</details>

<details>

<summary>2020-07-24 18:29:43 - COVID-19 Knowledge Graph: Accelerating Information Retrieval and Discovery for Scientific Literature</summary>

- *Colby Wise, Vassilis N. Ioannidis, Miguel Romero Calvo, Xiang Song, George Price, Ninad Kulkarni, Ryan Brand, Parminder Bhatia, George Karypis*

- `2007.12731v1` - [abs](http://arxiv.org/abs/2007.12731v1) - [pdf](http://arxiv.org/pdf/2007.12731v1)

> The coronavirus disease (COVID-19) has claimed the lives of over 350,000 people and infected more than 6 million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID-19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.

</details>

<details>

<summary>2020-07-25 11:20:38 - TSIT: A Simple and Versatile Framework for Image-to-Image Translation</summary>

- *Liming Jiang, Changxu Zhang, Mingyang Huang, Chunxiao Liu, Jianping Shi, Chen Change Loy*

- `2007.12072v2` - [abs](http://arxiv.org/abs/2007.12072v2) - [pdf](http://arxiv.org/pdf/2007.12072v2)

> We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations.

</details>

<details>

<summary>2020-07-25 12:10:16 - Crowdsourced 3D Mapping: A Combined Multi-View Geometry and Self-Supervised Learning Approach</summary>

- *Hemang Chawla, Matti Jukola, Terence Brouns, Elahe Arani, Bahram Zonooz*

- `2007.12918v1` - [abs](http://arxiv.org/abs/2007.12918v1) - [pdf](http://arxiv.org/pdf/2007.12918v1)

> The ability to efficiently utilize crowdsourced visual data carries immense potential for the domains of large scale dynamic mapping and autonomous driving. However, state-of-the-art methods for crowdsourced 3D mapping assume prior knowledge of camera intrinsics. In this work, we propose a framework that estimates the 3D positions of semantically meaningful landmarks such as traffic signs without assuming known camera intrinsics, using only monocular color camera and GPS. We utilize multi-view geometry as well as deep learning based self-calibration, depth, and ego-motion estimation for traffic sign positioning, and show that combining their strengths is important for increasing the map coverage. To facilitate research on this task, we construct and make available a KITTI based 3D traffic sign ground truth positioning dataset. Using our proposed framework, we achieve an average single-journey relative and absolute positioning accuracy of 39cm and 1.26m respectively, on this dataset.

</details>

<details>

<summary>2020-07-25 13:46:58 - Insightful Assistant: AI-compatible Operation Graph Representations for Enhancing Industrial Conversational Agents</summary>

- *Bekir Bayrak, Florian Giger, Christian Meurisch*

- `2007.12929v1` - [abs](http://arxiv.org/abs/2007.12929v1) - [pdf](http://arxiv.org/pdf/2007.12929v1)

> Advances in voice-controlled assistants paved the way into the consumer market. For professional or industrial use, the capabilities of such assistants are too limited or too time-consuming to implement due to the higher complexity of data, possible AI-based operations, and requests. In the light of these deficits, this paper presents Insightful Assistant---a pipeline concept based on a novel operation graph representation resulting from the intents detected. Using a predefined set of semantically annotated (executable) functions, each node of the operation graph is assigned to a function for execution. Besides basic operations, such functions can contain artificial intelligence (AI) based operations (e.g., anomaly detection). The result is then visualized to the user according to type and extracted user preferences in an automated way. We further collected a unique crowd-sourced set of 869 requests, each with four different variants expected visualization, for an industrial dataset. The evaluation of our proof-of-concept prototype on this dataset shows its feasibility: it achieves an accuracy of up to 95.0% (74.5%) for simple (complex) request detection with different variants and a top3-accuracy up to 95.4% for data-/user-adaptive visualization.

</details>

<details>

<summary>2020-07-25 14:30:03 - Gradient Regularized Contrastive Learning for Continual Domain Adaptation</summary>

- *Peng Su, Shixiang Tang, Peng Gao, Di Qiu, Ni Zhao, Xiaogang Wang*

- `2007.12942v1` - [abs](http://arxiv.org/abs/2007.12942v1) - [pdf](http://arxiv.org/pdf/2007.12942v1)

> Human beings can quickly adapt to environmental changes by leveraging learning experience. However, the poor ability of adapting to dynamic environments remains a major challenge for AI models. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labeled source domain and a sequence of unlabeled target domains. There are two major obstacles in this problem: domain shifts and catastrophic forgetting. In this work, we propose Gradient Regularized Contrastive Learning to solve the above obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforces the gradient of contrastive loss not to increase the supervised training loss on the source domain, which maintains the discriminative power of learned features; (2) regularizes the gradient update on the new domain not to increase the classification loss on the old target domains, which enables the model to adapt to an in-coming target domain while preserving the performance of previously observed domains. Hence our method can jointly learn both semantically discriminative and domain-invariant features with labeled source domain and unlabeled target domains. The experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the state-of-the-art.

</details>

<details>

<summary>2020-07-25 14:41:51 - MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part Disentanglement</summary>

- *Rinon Gal, Amit Bermano, Hao Zhang, Daniel Cohen-Or*

- `2007.12944v1` - [abs](http://arxiv.org/abs/2007.12944v1) - [pdf](http://arxiv.org/pdf/2007.12944v1)

> We present MRGAN, a multi-rooted adversarial network which generates part-disentangled 3D point-cloud shapes without part-based shape supervision. The network fuses multiple branches of tree-structured graph convolution layers which produce point clouds, with learnable constant inputs at the tree roots. Each branch learns to grow a different shape part, offering control over the shape generation at the part level. Our network encourages disentangled generation of semantic parts via two key ingredients: a root-mixing training strategy which helps decorrelate the different branches to facilitate disentanglement, and a set of loss terms designed with part disentanglement and shape semantics in mind. Of these, a novel convexity loss incentivizes the generation of parts that are more convex, as semantic parts tend to be. In addition, a root-dropping loss further ensures that each root seeds a single part, preventing the degeneration or over-growth of the point-producing branches. We evaluate the performance of our network on a number of 3D shape classes, and offer qualitative and quantitative comparisons to previous works and baseline approaches. We demonstrate the controllability offered by our part-disentangled generation through two applications for shape modeling: part mixing and individual part variation, without receiving segmented shapes as input.

</details>

<details>

<summary>2020-07-25 14:49:31 - Duluth at SemEval-2020 Task 12: Offensive Tweet Identification in English with Logistic Regression</summary>

- *Ted Pedersen*

- `2007.12946v1` - [abs](http://arxiv.org/abs/2007.12946v1) - [pdf](http://arxiv.org/pdf/2007.12946v1)

> This paper describes the Duluth systems that participated in SemEval--2020 Task 12, Multilingual Offensive Language Identification in Social Media (OffensEval--2020). We participated in the three English language tasks. Our systems provide a simple Machine Learning baseline using logistic regression. We trained our models on the distantly supervised training data made available by the task organizers and used no other resources. As might be expected we did not rank highly in the comparative evaluation: 79th of 85 in Task A, 34th of 43 in Task B, and 24th of 39 in Task C. We carried out a qualitative analysis of our results and found that the class labels in the gold standard data are somewhat noisy. We hypothesize that the extremely high accuracy (> 90%) of the top ranked systems may reflect methods that learn the training data very well but may not generalize to the task of identifying offensive language in English. This analysis includes examples of tweets that despite being mildly redacted are still offensive.

</details>

<details>

<summary>2020-07-25 14:56:10 - Duluth at SemEval-2019 Task 6: Lexical Approaches to Identify and Categorize Offensive Tweets</summary>

- *Ted Pedersen*

- `2007.12949v1` - [abs](http://arxiv.org/abs/2007.12949v1) - [pdf](http://arxiv.org/pdf/2007.12949v1)

> This paper describes the Duluth systems that participated in SemEval--2019 Task 6, Identifying and Categorizing Offensive Language in Social Media (OffensEval). For the most part these systems took traditional Machine Learning approaches that built classifiers from lexical features found in manually labeled training data. However, our most successful system for classifying a tweet as offensive (or not) was a rule-based black--list approach, and we also experimented with combining the training data from two different but related SemEval tasks. Our best systems in each of the three OffensEval tasks placed in the middle of the comparative evaluation, ranking 57th of 103 in task A, 39th of 75 in task B, and 44th of 65 in task C.

</details>

<details>

<summary>2020-07-25 20:42:21 - HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast Biopsy Images</summary>

- *Sachin Mehta, Ximing Lu, Donald Weaver, Joann G. Elmore, Hannaneh Hajishirzi, Linda Shapiro*

- `2007.13007v1` - [abs](http://arxiv.org/abs/2007.13007v1) - [pdf](http://arxiv.org/pdf/2007.13007v1)

> Training end-to-end networks for classifying gigapixel size histopathological images is computationally intractable. Most approaches are patch-based and first learn local representations (patch-wise) before combining these local representations to produce image-level decisions. However, dividing large tissue structures into patches limits the context available to these networks, which may reduce their ability to learn representations from clinically relevant structures. In this paper, we introduce a novel attention-based network, the Holistic ATtention Network (HATNet) to classify breast biopsy images. We streamline the histopathological image classification pipeline and show how to learn representations from gigapixel size images end-to-end. HATNet extends the bag-of-words approach and uses self-attention to encode global information, allowing it to learn representations from clinically relevant tissue structures without any explicit supervision. It outperforms the previous best network Y-Net, which uses supervision in the form of tissue-level segmentation masks, by 8%. Importantly, our analysis reveals that HATNet learns representations from clinically relevant structures, and it matches the classification accuracy of human pathologists for this challenging test set. Our source code is available at \url{https://github.com/sacmehta/HATNet}

</details>

<details>

<summary>2020-07-26 00:46:46 - Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</summary>

- *Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao*

- `2004.06165v5` - [abs](http://arxiv.org/abs/2004.06165v5) - [pdf](http://arxiv.org/pdf/2004.06165v5)

> Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.

</details>

<details>

<summary>2020-07-26 07:13:32 - A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges</summary>

- *Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, Jian Sun*

- `2007.13069v1` - [abs](http://arxiv.org/abs/2007.13069v1) - [pdf](http://arxiv.org/pdf/2007.13069v1)

> Question Answering (QA) over Knowledge Base (KB) aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge bases. In order to make KBQA more applicable in actual scenarios, researchers have shifted their attention from simple questions to complex questions, which require more KB triples and constraint inference. In this paper, we introduce the recent advances in complex QA. Besides traditional methods relying on templates and rules, the research is categorized into a taxonomy that contains two main branches, namely Information Retrieval-based and Neural Semantic Parsing-based. After describing the methods of these branches, we analyze directions for future research and introduce the models proposed by the Alime team.

</details>

<details>

<summary>2020-07-26 08:17:10 - SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction</summary>

- *Sriram N N, Buyu Liu, Francesco Pittaluga, Manmohan Chandraker*

- `2007.13078v1` - [abs](http://arxiv.org/abs/2007.13078v1) - [pdf](http://arxiv.org/pdf/2007.13078v1)

> We propose advances that address two key challenges in future trajectory prediction: (i) multimodality in both training data and predictions and (ii) constant time inference regardless of number of agents. Existing trajectory predictions are fundamentally limited by lack of diversity in training data, which is difficult to acquire with sufficient coverage of possible modes. Our first contribution is an automatic method to simulate diverse trajectories in the top-view. It uses pre-existing datasets and maps as initialization, mines existing trajectories to represent realistic driving behaviors and uses a multi-agent vehicle dynamics simulator to generate diverse new trajectories that cover various modes and are consistent with scene layout constraints. Our second contribution is a novel method that generates diverse predictions while accounting for scene semantics and multi-agent interactions, with constant-time inference independent of the number of agents. We propose a convLSTM with novel state pooling operations and losses to predict scene-consistent states of multiple agents in a single forward pass, along with a CVAE for diversity. We validate our proposed multi-agent trajectory prediction approach by training and testing on the proposed simulated dataset and existing real datasets of traffic scenes. In both cases, our approach outperforms SOTA methods by a large margin, highlighting the benefits of both our diverse dataset simulation and constant-time diverse trajectory prediction methods.

</details>

<details>

<summary>2020-07-26 08:27:20 - Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery</summary>

- *Emanuele Colleoni, Philip Edwards, Danail Stoyanov*

- `2007.09107v2` - [abs](http://arxiv.org/abs/2007.09107v2) - [pdf](http://arxiv.org/pdf/2007.09107v2)

> Semantic tool segmentation in surgical videos is important for surgical scene understanding and computer-assisted interventions as well as for the development of robotic automation. The problem is challenging because different illumination conditions, bleeding, smoke and occlusions can reduce algorithm robustness. At present labelled data for training deep learning models is still lacking for semantic surgical instrument segmentation and in this paper we show that it may be possible to use robot kinematic data coupled with laparoscopic images to alleviate the labelling problem. We propose a new deep learning based model for parallel processing of both laparoscopic and simulation images for robust segmentation of surgical tools. Due to the lack of laparoscopic frames annotated with both segmentation ground truth and kinematic information a new custom dataset was generated using the da Vinci Research Kit (dVRK) and is made available.

</details>

<details>

<summary>2020-07-26 10:12:42 - U2-ONet: A Two-level Nested Octave U-structure with Multiscale Attention Mechanism for Moving Instances Segmentation</summary>

- *Chenjie Wang, Chengyuan Li, Bin Luo*

- `2007.13092v1` - [abs](http://arxiv.org/abs/2007.13092v1) - [pdf](http://arxiv.org/pdf/2007.13092v1)

> Most scenes in practical applications are dynamic scenes containing moving objects, so segmenting accurately moving objects is crucial for many computer vision applications. In order to efficiently segment out all moving objects in the scene, regardless of whether the object has a predefined semantic label, we propose a two-level nested Octave U-structure network with a multiscale attention mechanism called U2-ONet. Each stage of U2-ONet is filled with our newly designed Octave ReSidual U-block (ORSU) to enhance the ability to obtain more context information at different scales while reducing spatial redundancy of feature maps. In order to efficiently train our multi-scale deep network, we introduce a hierarchical training supervision strategy that calculates the loss at each level while adding a knowledge matching loss to keep the optimization consistency. Experimental results show that our method achieves state-of-the-art performance in several general moving objects segmentation datasets.

</details>

<details>

<summary>2020-07-26 17:26:20 - KUISAIL at SemEval-2020 Task 12: BERT-CNN for Offensive Speech Identification in Social Media</summary>

- *Ali Safaya, Moutasem Abdullatif, Deniz Yuret*

- `2007.13184v1` - [abs](http://arxiv.org/abs/2007.13184v1) - [pdf](http://arxiv.org/pdf/2007.13184v1)

> In this paper, we describe our approach to utilize pre-trained BERT models with Convolutional Neural Networks for sub-task A of the Multilingual Offensive Language Identification shared task (OffensEval 2020), which is a part of the SemEval 2020. We show that combining CNN with BERT is better than using BERT on its own, and we emphasize the importance of utilizing pre-trained language models for downstream tasks. Our system, ranked 4th with macro averaged F1-Score of 0.897 in Arabic, 4th with score of 0.843 in Greek, and 3rd with score of 0.814 in Turkish. Additionally, we present ArabicBERT, a set of pre-trained transformer language models for Arabic that we share with the community.

</details>

<details>

<summary>2020-07-27 12:22:36 - No One is Perfect: Analysing the Performance of Question Answering Components over the DBpedia Knowledge Graph</summary>

- *Kuldeep Singh, Ioanna Lytra, Arun Sethupat Radhakrishna, Saeedeh Shekarpour, Maria-Esther Vidal, Jens Lehmann*

- `1809.10044v2` - [abs](http://arxiv.org/abs/1809.10044v2) - [pdf](http://arxiv.org/pdf/1809.10044v2)

> Question answering (QA) over knowledge graphs has gained significant momentum over the past five years due to the increasing availability of large knowledge graphs and the rising importance of question answering for user interaction. DBpedia has been the most prominently used knowledge graph in this setting and most approaches currently use a pipeline of processing steps connecting a sequence of components. In this article, we analyse and micro evaluate the behaviour of 29 available QA components for DBpedia knowledge graph that were released by the research community since 2010. As a result, we provide a perspective on collective failure cases, suggest characteristics of QA components that prevent them from performing better and provide future challenges and research directions for the field.

</details>

<details>

<summary>2020-07-27 23:58:54 - ULD@NUIG at SemEval-2020 Task 9: Generative Morphemes with an Attention Model for Sentiment Analysis in Code-Mixed Text</summary>

- *Koustava Goswami, Priya Rani, Bharathi Raja Chakravarthi, Theodorus Fransen, John P. McCrae*

- `2008.01545v1` - [abs](http://arxiv.org/abs/2008.01545v1) - [pdf](http://arxiv.org/pdf/2008.01545v1)

> Code mixing is a common phenomena in multilingual societies where people switch from one language to another for various reasons. Recent advances in public communication over different social media sites have led to an increase in the frequency of code-mixed usage in written language. In this paper, we present the Generative Morphemes with Attention (GenMA) Model sentiment analysis system contributed to SemEval 2020 Task 9 SentiMix. The system aims to predict the sentiments of the given English-Hindi code-mixed tweets without using word-level language tags instead inferring this automatically using a morphological model. The system is based on a novel deep neural network (DNN) architecture, which has outperformed the baseline F1-score on the test data-set as well as the validation data-set. Our results can be found under the user name "koustava" on the "Sentimix Hindi English" page

</details>

<details>

<summary>2020-07-28 01:27:14 - Representation Learning with Video Deep InfoMax</summary>

- *R Devon Hjelm, Philip Bachman*

- `2007.13278v2` - [abs](http://arxiv.org/abs/2007.13278v2) - [pdf](http://arxiv.org/pdf/2007.13278v2)

> Self-supervised learning has made unsupervised pretraining relevant again for difficult computer vision tasks. The most effective self-supervised methods involve prediction tasks based on features extracted from diverse views of the data. DeepInfoMax (DIM) is a self-supervised method which leverages the internal structure of deep networks to construct such views, forming prediction tasks between local features which depend on small patches in an image and global features which depend on the whole image. In this paper, we extend DIM to the video domain by leveraging similar structure in spatio-temporal networks, producing a method we call Video Deep InfoMax(VDIM). We find that drawing views from both natural-rate sequences and temporally-downsampled sequences yields results on Kinetics-pretrained action recognition tasks which match or outperform prior state-of-the-art methods that use more costly large-time-scale transformer models. We also examine the effects of data augmentation and fine-tuning methods, accomplishingSoTA by a large margin when training only on the UCF-101 dataset.

</details>

<details>

<summary>2020-07-28 03:47:26 - SalamNET at SemEval-2020 Task12: Deep Learning Approach for Arabic Offensive Language Detection</summary>

- *Fatemah Husain, Jooyeon Lee, Samuel Henry, Ozlem Uzuner*

- `2007.13974v1` - [abs](http://arxiv.org/abs/2007.13974v1) - [pdf](http://arxiv.org/pdf/2007.13974v1)

> This paper describes SalamNET, an Arabic offensive language detection system that has been submitted to SemEval 2020 shared task 12: Multilingual Offensive Language Identification in Social Media. Our approach focuses on applying multiple deep learning models and conducting in depth error analysis of results to provide system implications for future development considerations. To pursue our goal, a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and Long-Short Term Memory (LSTM) models with different design architectures have been developed and evaluated. The SalamNET, a Bi-directional Gated Recurrent Unit (Bi-GRU) based model, reports a macro-F1 score of 0.83.

</details>

<details>

<summary>2020-07-28 09:17:50 - Improving Generative Adversarial Networks with Local Coordinate Coding</summary>

- *Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang, Mingkui Tan*

- `2008.00942v1` - [abs](http://arxiv.org/abs/2008.00942v1) - [pdf](http://arxiv.org/pdf/2008.00942v1)

> Generative adversarial networks (GANs) have shown remarkable success in generating realistic data from some predefined prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data. However, such latent distribution may incur difficulties in data sampling for GANs. In this paper, rather than sampling from the predefined prior distribution, we propose an LCCGAN model with local coordinate coding (LCC) to improve the performance of generating data. First, we propose an LCC sampling method in LCCGAN to sample meaningful points from the latent manifold. With the LCC sampling method, we can exploit the local information on the latent manifold and thus produce new data with promising quality. Second, we propose an improved version, namely LCCGAN++, by introducing a higher-order term in the generator approximation. This term is able to achieve better approximation and thus further improve the performance. More critically, we derive the generalization bound for both LCCGAN and LCCGAN++ and prove that a low-dimensional input is sufficient to achieve good generalization performance. Extensive experiments on four benchmark datasets demonstrate the superiority of the proposed method over existing GANs.

</details>

<details>

<summary>2020-07-28 09:45:52 - The STEM-ECR Dataset: Grounding Scientific Entity References in STEM Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources</summary>

- *Jennifer D'Souza, Anett Hoppe, Arthur Brack, Mohamad Yaser Jaradeh, Sören Auer, Ralph Ewerth*

- `2003.01006v4` - [abs](http://arxiv.org/abs/2003.01006v4) - [pdf](http://arxiv.org/pdf/2003.01006v4)

> We introduce the STEM (Science, Technology, Engineering, and Medicine) Dataset for Scientific Entity Extraction, Classification, and Resolution, version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to provide a benchmark for the evaluation of scientific entity extraction, classification, and resolution tasks in a domain-independent fashion. It comprises abstracts in 10 STEM disciplines that were found to be the most prolific ones on a major publishing platform. We describe the creation of such a multidisciplinary corpus and highlight the obtained findings in terms of the following features: 1) a generic conceptual formalism for scientific entities in a multidisciplinary scientific context; 2) the feasibility of the domain-independent human annotation of scientific entities under such a generic formalism; 3) a performance benchmark obtainable for automatic extraction of multidisciplinary scientific entities using BERT-based neural models; 4) a delineated 3-step entity resolution procedure for human annotation of the scientific entities via encyclopedic entity linking and lexicographic word sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic links and lexicographic senses for our entities. Our findings cumulatively indicate that human annotation and automatic learning of multidisciplinary scientific concepts as well as their semantic disambiguation in a wide-ranging setting as STEM is reasonable.

</details>

<details>

<summary>2020-07-28 13:30:46 - ECNU-SenseMaker at SemEval-2020 Task 4: Leveraging Heterogeneous Knowledge Resources for Commonsense Validation and Explanation</summary>

- *Qian Zhao, Siyu Tao, Jie Zhou, Linlin Wang, Xin Lin, Liang He*

- `2007.14200v1` - [abs](http://arxiv.org/abs/2007.14200v1) - [pdf](http://arxiv.org/pdf/2007.14200v1)

> This paper describes our system for SemEval-2020 Task 4: Commonsense Validation and Explanation (Wang et al., 2020). We propose a novel Knowledge-enhanced Graph Attention Network (KEGAT) architecture for this task, leveraging heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and unstructured text to better improve the ability of a machine in commonsense understanding. This model has a powerful commonsense inference capability via utilizing suitable commonsense incorporation methods and upgraded data augmentation techniques. Besides, an internal sharing mechanism is cooperated to prohibit our model from insufficient and excessive reasoning for commonsense. As a result, this model performs quite well in both validation and explanation. For instance, it achieves state-of-the-art accuracy in the subtask called Commonsense Explanation (Multi-Choice). We officially name the system as ECNU-SenseMaker. Code is publicly available at https://github.com/ECNU-ICA/ECNU-SenseMaker.

</details>

<details>

<summary>2020-07-28 17:50:42 - Information Extraction of Clinical Trial Eligibility Criteria</summary>

- *Yitong Tseo, M. I. Salkola, Ahmed Mohamed, Anuj Kumar, Freddy Abnousi*

- `2006.07296v6` - [abs](http://arxiv.org/abs/2006.07296v6) - [pdf](http://arxiv.org/pdf/2006.07296v6)

> Clinical trials predicate subject eligibility on a diversity of criteria ranging from patient demographics to food allergies. Trials post their requirements as semantically complex, unstructured free-text. Formalizing trial criteria to a computer-interpretable syntax would facilitate eligibility determination. In this paper, we investigate an information extraction (IE) approach for grounding criteria from trials in ClinicalTrials(dot)gov to a shared knowledge base. We frame the problem as a novel knowledge base population task, and implement a solution combining machine learning and context free grammar. To our knowledge, this work is the first criteria extraction system to apply attention-based conditional random field architecture for named entity recognition (NER), and word2vec embedding clustering for named entity linking (NEL). We release the resources and core components of our system on GitHub at https://github.com/facebookresearch/Clinical-Trial-Parser. Finally, we report our per module and end to end performances; we conclude that our system is competitive with Criteria2Query, which we view as the current state-of-the-art in criteria extraction.

</details>

<details>

<summary>2020-07-28 19:52:21 - Measuring prominence of scientific work in online news as a proxy for impact</summary>

- *James Ravenscroft, Amanda Clare, Maria Liakata*

- `2007.14454v1` - [abs](http://arxiv.org/abs/2007.14454v1) - [pdf](http://arxiv.org/pdf/2007.14454v1)

> The impact made by a scientific paper on the work of other academics has many established metrics, including metrics based on citation counts and social media commenting. However, determination of the impact of a scientific paper on the wider society is less well established. For example, is it important for scientific work to be newsworthy? Here we present a new corpus of newspaper articles linked to the scientific papers that they describe. We find that Impact Case studies submitted to the UK Research Excellence Framework (REF) 2014 that refer to scientific papers mentioned in newspaper articles were awarded a higher score in the REF assessment. The papers associated with these case studies also feature prominently in the newspaper articles. We hypothesise that such prominence can be a useful proxy for societal impact. We therefore provide a novel baseline approach for measuring the prominence of scientific papers mentioned within news articles. Our measurement of prominence is based on semantic similarity through a graph-based ranking algorithm. We find that scientific papers with an associated REF case study are more likely to have a stronger prominence score. This supports our hypothesis that linguistic prominence in news can be used to suggest the wider non-academic impact of scientific work.

</details>

<details>

<summary>2020-07-28 20:35:56 - Construction and Usage of a Human Body Common Coordinate Framework Comprising Clinical, Semantic, and Spatial Ontologies</summary>

- *Katy Börner, Ellen M. Quardokus, Bruce W. Herr II, Leonard E. Cross, Elizabeth G. Record, Yingnan Ju, Andreas D. Bueckle, James P. Sluka, Jonathan C. Silverstein, Kristen M. Browne, Sanjay Jain, Clive H. Wasserfall, Marda L. Jorgensen, Jeffrey M. Spraggins, Nathan H. Patterson, Mark A. Musen, Griffin M. Weber*

- `2007.14474v1` - [abs](http://arxiv.org/abs/2007.14474v1) - [pdf](http://arxiv.org/pdf/2007.14474v1)

> The National Institutes of Health's (NIH) Human Biomolecular Atlas Program (HuBMAP) aims to create a comprehensive high-resolution atlas of all the cells in the healthy human body. Multiple laboratories across the United States are collecting tissue specimens from different organs of donors who vary in sex, age, and body size. Integrating and harmonizing the data derived from these samples and 'mapping' them into a common three-dimensional (3D) space is a major challenge. The key to making this possible is a 'Common Coordinate Framework' (CCF), which provides a semantically annotated, 3D reference system for the entire body. The CCF enables contributors to HuBMAP to 'register' specimens and datasets within a common spatial reference system, and it supports a standardized way to query and 'explore' data in a spatially and semantically explicit manner. [...] This paper describes the construction and usage of a CCF for the human body and its reference implementation in HuBMAP. The CCF consists of (1) a CCF Clinical Ontology, which provides metadata about the specimen and donor (the 'who'); (2) a CCF Semantic Ontology, which describes 'what' part of the body a sample came from and details anatomical structures, cell types, and biomarkers (ASCT+B); and (3) a CCF Spatial Ontology, which indicates 'where' a tissue sample is located in a 3D coordinate system. An initial version of all three CCF ontologies has been implemented for the first HuBMAP Portal release. It was successfully used by Tissue Mapping Centers to semantically annotate and spatially register 48 kidney and spleen tissue blocks. The blocks can be queried and explored in their clinical, semantic, and spatial context via the CCF user interface in the HuBMAP Portal.

</details>

<details>

<summary>2020-07-28 22:12:17 - End-to-end speech-to-dialog-act recognition</summary>

- *Viet-Trung Dang, Tianyu Zhao, Sei Ueno, Hirofumi Inaguma, Tatsuya Kawahara*

- `2004.11419v2` - [abs](http://arxiv.org/abs/2004.11419v2) - [pdf](http://arxiv.org/pdf/2004.11419v2)

> Spoken language understanding, which extracts intents and/or semantic concepts in utterances, is conventionally formulated as a post-processing of automatic speech recognition. It is usually trained with oracle transcripts, but needs to deal with errors by ASR. Moreover, there are acoustic features which are related with intents but not represented with the transcripts. In this paper, we present an end-to-end model which directly converts speech into dialog acts without the deterministic transcription process. In the proposed model, the dialog act recognition network is conjunct with an acoustic-to-word ASR model at its latent layer before the softmax layer, which provides a distributed representation of word-level ASR decoding information. Then, the entire network is fine-tuned in an end-to-end manner. This allows for stable training as well as robustness against ASR errors. The model is further extended to conduct DA segmentation jointly. Evaluations with the Switchboard corpus demonstrate that the proposed method significantly improves dialog act recognition accuracy from the conventional pipeline framework.

</details>

<details>

<summary>2020-07-29 08:59:52 - Understanding Optical Music Recognition</summary>

- *Jorge Calvo-Zaragoza, Jan Hajič Jr., Alexander Pacha*

- `1908.03608v3` - [abs](http://arxiv.org/abs/1908.03608v3) - [pdf](http://arxiv.org/pdf/1908.03608v3)

> For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: few introductory materials are available, and furthermore the field has struggled with defining itself and building a shared terminology. In this tutorial, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.

</details>

<details>

<summary>2020-07-29 16:19:30 - Text-based classification of interviews for mental health -- juxtaposing the state of the art</summary>

- *Joppe Valentijn Wouts*

- `2008.01543v1` - [abs](http://arxiv.org/abs/2008.01543v1) - [pdf](http://arxiv.org/pdf/2008.01543v1)

> Currently, the state of the art for classification of psychiatric illness is based on audio-based classification. This thesis aims to design and evaluate a state of the art text classification network on this challenge. The hypothesis is that a well designed text-based approach poses a strong competition against the state-of-the-art audio based approaches. Dutch natural language models are being limited by the scarcity of pre-trained monolingual NLP models, as a result Dutch natural language models have a low capture of long range semantic dependencies over sentences. For this issue, this thesis presents belabBERT, a new Dutch language model extending the RoBERTa[15] architecture. belabBERT is trained on a large Dutch corpus (+32GB) of web crawled texts. After this thesis evaluates the strength of text-based classification, a brief exploration is done, extending the framework to a hybrid text- and audio-based classification. The goal of this hybrid framework is to show the principle of hybridisation with a very basic audio-classification network. The overall goal is to create the foundations for a hybrid psychiatric illness classification, by proving that the new text-based classification is already a strong stand-alone solution.

</details>

<details>

<summary>2020-07-30 10:55:56 - Bayesian Optimization for Developmental Robotics with Meta-Learning by Parameters Bounds Reduction</summary>

- *Maxime Petit, Emmanuel Dellandrea, Liming Chen*

- `2007.15375v1` - [abs](http://arxiv.org/abs/2007.15375v1) - [pdf](http://arxiv.org/pdf/2007.15375v1)

> In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3% vs 78.9% of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).

</details>

<details>

<summary>2020-07-30 16:19:59 - Beyond $\mathcal{H}$-Divergence: Domain Adaptation Theory With Jensen-Shannon Divergence</summary>

- *Changjian Shui, Qi Chen, Jun Wen, Fan Zhou, Christian Gagné, Boyu Wang*

- `2007.15567v1` - [abs](http://arxiv.org/abs/2007.15567v1) - [pdf](http://arxiv.org/pdf/2007.15567v1)

> We reveal the incoherence between the widely-adopted empirical domain adversarial training and its generally-assumed theoretical counterpart based on $\mathcal{H}$-divergence. Concretely, we find that $\mathcal{H}$-divergence is not equivalent to Jensen-Shannon divergence, the optimization objective in domain adversarial training. To this end, we establish a new theoretical framework by directly proving the upper and lower target risk bounds based on joint distributional Jensen-Shannon divergence. We further derive bi-directional upper bounds for marginal and conditional shifts. Our framework exhibits inherent flexibilities for different transfer learning problems, which is usable for various scenarios where $\mathcal{H}$-divergence-based theory fails to adapt. From an algorithmic perspective, our theory enables a generic guideline unifying principles of semantic conditional matching, feature marginal matching, and label marginal shift correction. We employ algorithms for each principle and empirically validate the benefits of our framework on real datasets.

</details>

<details>

<summary>2020-07-30 17:19:16 - funcGNN: A Graph Neural Network Approach to Program Similarity</summary>

- *Aravind Nair, Avijit Roy, Karl Meinke*

- `2007.13239v3` - [abs](http://arxiv.org/abs/2007.13239v3) - [pdf](http://arxiv.org/pdf/2007.13239v3)

> Program similarity is a fundamental concept, central to the solution of software engineering tasks such as software plagiarism, clone identification, code refactoring and code search. Accurate similarity estimation between programs requires an in-depth understanding of their structure, semantics and flow. A control flow graph (CFG), is a graphical representation of a program which captures its logical control flow and hence its semantics. A common approach is to estimate program similarity by analysing CFGs using graph similarity measures, e.g. graph edit distance (GED). However, graph edit distance is an NP-hard problem and computationally expensive, making the application of graph similarity techniques to complex software programs impractical. This study intends to examine the effectiveness of graph neural networks to estimate program similarity, by analysing the associated control flow graphs. We introduce funcGNN, which is a graph neural network trained on labeled CFG pairs to predict the GED between unseen program pairs by utilizing an effective embedding vector. To our knowledge, this is the first time graph neural networks have been applied on labeled CFGs for estimating the similarity between high-level language programs. Results: We demonstrate the effectiveness of funcGNN to estimate the GED between programs and our experimental analysis demonstrates how it achieves a lower error rate (0.00194), with faster (23 times faster than the quickest traditional GED approximation method) and better scalability compared with the state of the art methods. funcGNN posses the inductive learning ability to infer program structure and generalise to unseen programs. The graph embedding of a program proposed by our methodology could be applied to several related software engineering problems (such as code plagiarism and clone identification) thus opening multiple research directions.

</details>

<details>

<summary>2020-07-30 17:58:16 - Rewriting a Deep Generative Model</summary>

- *David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba*

- `2007.15646v1` - [abs](http://arxiv.org/abs/2007.15646v1) - [pdf](http://arxiv.org/pdf/2007.15646v1)

> A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.

</details>

<details>

<summary>2020-07-31 03:29:49 - Interactive Text Graph Mining with a Prolog-based Dialog Engine</summary>

- *Paul Tarau, Eduardo Blanco*

- `2008.00956v1` - [abs](http://arxiv.org/abs/2008.00956v1) - [pdf](http://arxiv.org/pdf/2008.00956v1)

> On top of a neural network-based dependency parser and a graph-based natural language processing module we design a Prolog-based dialog engine that explores interactively a ranked fact database extracted from a text document.   We reorganize dependency graphs to focus on the most relevant content elements of a sentence and integrate sentence identifiers as graph nodes.   Additionally, after ranking the graph we take advantage of the implicit semantic information that dependency links and WordNet bring in the form of subject-verb-object, is-a and part-of relations.   Working on the Prolog facts and their inferred consequences, the dialog engine specializes the text graph with respect to a query and reveals interactively the document's most relevant content elements.   The open-source code of the integrated system is available at https://github.com/ptarau/DeepRank .   Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2020-07-31 09:30:48 - Neural Style Transfer for Remote Sensing</summary>

- *Maria Karatzoglidi, Georgios Felekis, Eleni Charou*

- `2007.15920v1` - [abs](http://arxiv.org/abs/2007.15920v1) - [pdf](http://arxiv.org/pdf/2007.15920v1)

> The well-known technique outlined in the paper of Leon A. Gatys et al., A Neural Algorithm of Artistic Style, has become a trending topic both in academic literature and industrial applications. Neural Style Transfer (NST) constitutes an essential tool for a wide range of applications, such as artistic stylization of 2D images, user-assisted creation tools and production tools for entertainment applications. The purpose of this study is to present a method for creating artistic maps from satellite images, based on the NST algorithm. This method includes three basic steps (i) application of semantic image segmentation on the original satellite image, dividing its content into classes (i.e. land, water), (ii) application of neural style transfer for each class and (iii) creation of a collage, i.e. an artistic image consisting of a combination of the two stylized image generated on the previous step.

</details>

<details>

<summary>2020-07-31 14:27:59 - XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on Anatomically Variable XCAT Phantoms</summary>

- *Sina Amirrajab, Samaneh Abbasi-Sureshjani, Yasmina Al Khalil, Cristian Lorenz, Juergen Weese, Josien Pluim, Marcel Breeuwer*

- `2007.13408v2` - [abs](http://arxiv.org/abs/2007.13408v2) - [pdf](http://arxiv.org/pdf/2007.13408v2)

> Generative adversarial networks (GANs) have provided promising data enrichment solutions by synthesizing high-fidelity images. However, generating large sets of labeled images with new anatomical variations remains unexplored. We propose a novel method for synthesizing cardiac magnetic resonance (CMR) images on a population of virtual subjects with a large anatomical variation, introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human phantom. We investigate two conditional image synthesis approaches grounded on a semantically-consistent mask-guided image generation technique: 4-class and 8-class XCAT-GANs. The 4-class technique relies on only the annotations of the heart; while the 8-class technique employs a predicted multi-tissue label map of the heart-surrounding organs and provides better guidance for our conditional image synthesis. For both techniques, we train our conditional XCAT-GAN with real images paired with corresponding labels and subsequently at the inference time, we substitute the labels with the XCAT derived ones. Therefore, the trained network accurately transfers the tissue-specific textures to the new label maps. By creating 33 virtual subjects of synthetic CMR images at the end-diastolic and end-systolic phases, we evaluate the usefulness of such data in the downstream cardiac cavity segmentation task under different augmentation strategies. Results demonstrate that even with only 20% of real images (40 volumes) seen during training, segmentation performance is retained with the addition of synthetic CMR images. Moreover, the improvement in utilizing synthetic images for augmenting the real data is evident through the reduction of Hausdorff distance up to 28% and an increase in the Dice score up to 5%, indicating a higher similarity to the ground truth in all dimensions.

</details>

<details>

<summary>2020-07-31 14:34:35 - Learning-based Computer-aided Prescription Model for Parkinson's Disease: A Data-driven Perspective</summary>

- *Yinghuan Shi, Wanqi Yang, Kim-Han Thung, Hao Wang, Yang Gao, Yang Pan, Li Zhang, Dinggang Shen*

- `2007.16103v1` - [abs](http://arxiv.org/abs/2007.16103v1) - [pdf](http://arxiv.org/pdf/2007.16103v1)

> In this paper, we study a novel problem: "automatic prescription recommendation for PD patients." To realize this goal, we first build a dataset by collecting 1) symptoms of PD patients, and 2) their prescription drug provided by neurologists. Then, we build a novel computer-aided prescription model by learning the relation between observed symptoms and prescription drug. Finally, for the new coming patients, we could recommend (predict) suitable prescription drug on their observed symptoms by our prescription model. From the methodology part, our proposed model, namely Prescription viA Learning lAtent Symptoms (PALAS), could recommend prescription using the multi-modality representation of the data. In PALAS, a latent symptom space is learned to better model the relationship between symptoms and prescription drug, as there is a large semantic gap between them. Moreover, we present an efficient alternating optimization method for PALAS. We evaluated our method using the data collected from 136 PD patients at Nanjing Brain Hospital, which can be regarded as a large dataset in PD research community. The experimental results demonstrate the effectiveness and clinical potential of our method in this recommendation task, if compared with other competing methods.

</details>

<details>

<summary>2020-07-31 15:08:14 - On Package Freshness in Linux Distributions</summary>

- *Damien Legay, Alexandre Decan, Tom Mens*

- `2007.16123v1` - [abs](http://arxiv.org/abs/2007.16123v1) - [pdf](http://arxiv.org/pdf/2007.16123v1)

> The open-source Linux operating system is available through a wide variety of distributions, each containing a collection of installable software packages. It can be important to keep these packages as fresh as possible to benefit from new features, bug fixes and security patches. However, not all distributions place the same emphasis on package freshness. We conducted a survey in the first half of 2020 with 170 Linux users to gauge their perception of package freshness in the distributions they use, the value they place on package freshness and the reasons why they do so, and the methods they use to update packages. The results of this survey reveal that, for the aforementioned reasons, keeping packages up to date is an important concern to Linux users and that they install and update packages through their distribution's official repositories whenever possible, but often resort to third-party repositories and package managers for proprietary software and programming language libraries. Some distributions are perceived to be much quicker in deploying package updates than others. These results are valuable to assess the requirements and expectations of Linux users in terms of package freshness.

</details>

<details>

<summary>2020-07-31 16:53:18 - Foveation for Segmentation of Ultra-High Resolution Images</summary>

- *Chen Jin, Ryutaro Tanno, Moucheng Xu, Thomy Mertzanidou, Daniel C. Alexander*

- `2007.15124v2` - [abs](http://arxiv.org/abs/2007.15124v2) - [pdf](http://arxiv.org/pdf/2007.15124v2)

> Segmentation of ultra-high resolution images is challenging because of their enormous size, consisting of millions or even billions of pixels. Typical solutions include dividing input images into patches of fixed size and/or down-sampling to meet memory constraints. Such operations incur information loss in the field-of-view (FoV) i.e., spatial coverage and the image resolution. The impact on segmentation performance is, however, as yet understudied. In this work, we start with a motivational experiment which demonstrates that the trade-off between FoV and resolution affects the segmentation performance on ultra-high resolution images---and furthermore, its influence also varies spatially according to the local patterns in different areas. We then introduce foveation module, a learnable "dataloader" which, for a given ultra-high resolution image, adaptively chooses the appropriate configuration (FoV/resolution trade-off) of the input patch to feed to the downstream segmentation model at each spatial location of the image. The foveation module is jointly trained with the segmentation network to maximise the task performance. We demonstrate on three publicly available high-resolution image datasets that the foveation module consistently improves segmentation performance over the cases trained with patches of fixed FoV/resolution trade-off. Our approach achieves the SoTA performance on the DeepGlobe aerial image dataset. On the Gleason2019 histopathology dataset, our model achieves better segmentation accuracy for the two most clinically important and ambiguous classes (Gleason Grade 3 and 4) than the top performers in the challenge by 13.1% and 7.5%, and improves on the average performance of 6 human experts by 6.5% and 7.5%. Our code and trained models are available at $\text{https://github.com/lxasqjc/Foveation-Segmentation}$.

</details>

<details>

<summary>2020-07-31 18:28:14 - Classical linear logic, cobordisms and categorial grammars</summary>

- *Sergey Slavnov*

- `1911.03962v3` - [abs](http://arxiv.org/abs/1911.03962v3) - [pdf](http://arxiv.org/pdf/1911.03962v3)

> We propose a categorial grammar based on classical multiplicative linear logic.   This can be seen as an extension of abstract categorial grammars (ACG) and is at least as expressive. However, constituents of {\it linear logic grammars (LLG)} are not abstract ${\lambda}$-terms, but simply tuples of words with labeled endpoints and supplied with specific {\it plugging instructions}: the sets of endpoints are subdivided into the {\it incoming} and the {\it outgoing} parts. We call such objects {\it word cobordisms}.   A key observation is that word cobordisms can be organized in a category, very similar to the familiar category of topological cobordisms. This category is symmetric monoidal closed and compact closed and thus is a model of linear $\lambda$-calculus and classical, as well as intuitionistic linear logic. This allows us using linear logic as a typing system for word cobordisms.   At least, this gives a concrete and intuitive representation of ACG.   We think, however, that the category of word cobordisms, which has a rich structure and is independent of any grammar, might be interesting on its own right.

</details>

<details>

<summary>2020-07-31 23:08:48 - Operationalizing Declarative and Procedural Knowledge: a Benchmark on Logic Programming Petri Nets (LPPNs)</summary>

- *Giovanni Sileno*

- `1701.07657v2` - [abs](http://arxiv.org/abs/1701.07657v2) - [pdf](http://arxiv.org/pdf/1701.07657v2)

> Modelling, specifying and reasoning about complex systems requires to process in an integrated fashion declarative and procedural aspects of the target domain. The paper reports on an experiment conducted with a propositional version of Logic Programming Petri Nets (LPPNs), a notation extending Petri Nets with logic programming constructs. Two semantics are presented: a denotational semantics that fully maps the notation to ASP via Event Calculus; and a hybrid operational semantics that process separately the causal mechanisms via Petri nets, and the constraints associated to objects and to events via Answer Set Programming (ASP). These two alternative specifications enable an empirical evaluation in terms of computational efficiency. Experimental results show that the hybrid semantics is more efficient w.r.t. sequences, whereas the two semantics follows the same behaviour w.r.t. branchings (although the denotational one performs better in absolute terms).

</details>


## 2020-08

<details>

<summary>2020-08-01 00:37:05 - Dissipating with Relations: Implication for the Entity-Relationship Model</summary>

- *Sabah Al-Fedaghi*

- `2008.00135v1` - [abs](http://arxiv.org/abs/2008.00135v1) - [pdf](http://arxiv.org/pdf/2008.00135v1)

> Difficulties arise when conceptual modeling lacks ontological clarity and rigorous definitions, which is especially the case in the relationship construct. Evidence shows that use of relationships is often problematic when it comes to communicating the form of meaning of an application domain. Research on this topic is important because relationships are central to a number of approaches and commonly used by practitioners. In this paper, we study the notion of relation or relationship in the context of conceptual modeling. Specifically, we focus on the notion of relationship used in the entity-relationship (ER) model. The ER model is scrutinized through a new form of conceptual modeling called the thinging machine (TM) to pursue further understanding of the semantics of the relationship concept. The ER model is composed of three fundamental categories (i.e., entity, relationship and attribute), whereas TM is built from one ontological category called the thing/machine (thimac). Several ER diagrams are re-casted as TM diagrams, creating a categorical collision with interesting implications regarding the status of the conception of relationship in a conceptual model. The re-modeling shows that the relational construct is dissipated into TM flows of things and chronology of events.

</details>

<details>

<summary>2020-08-01 11:23:37 - Meta-DRN: Meta-Learning for 1-Shot Image Segmentation</summary>

- *Atmadeep Banerjee*

- `2008.00247v1` - [abs](http://arxiv.org/abs/2008.00247v1) - [pdf](http://arxiv.org/pdf/2008.00247v1)

> Modern deep learning models have revolutionized the field of computer vision. But, a significant drawback of most of these models is that they require a large number of labelled examples to generalize properly. Recent developments in few-shot learning aim to alleviate this requirement. In this paper, we propose a novel lightweight CNN architecture for 1-shot image segmentation. The proposed model is created by taking inspiration from well-performing architectures for semantic segmentation and adapting it to the 1-shot domain. We train our model using 4 meta-learning algorithms that have worked well for image classification and compare the results. For the chosen dataset, our proposed model has a 70% lower parameter count than the benchmark, while having better or comparable mean IoU scores using all 4 of the meta-learning algorithms.

</details>

<details>

<summary>2020-08-01 18:40:00 - Modeling ASR Ambiguity for Dialogue State Tracking Using Word Confusion Networks</summary>

- *Vaishali Pal, Fabien Guillot, Manish Shrivastava, Jean-Michel Renders, Laurent Besacier*

- `2002.00768v2` - [abs](http://arxiv.org/abs/2002.00768v2) - [pdf](http://arxiv.org/pdf/2002.00768v2)

> Spoken dialogue systems typically use a list of top-N ASR hypotheses for inferring the semantic meaning and tracking the state of the dialogue. However ASR graphs, such as confusion networks (confnets), provide a compact representation of a richer hypothesis space than a top-N ASR list. In this paper, we study the benefits of using confusion networks with a state-of-the-art neural dialogue state tracker (DST). We encode the 2-dimensional confnet into a 1-dimensional sequence of embeddings using an attentional confusion network encoder which can be used with any DST system. Our confnet encoder is plugged into the state-of-the-art 'Global-locally Self-Attentive Dialogue State Tacker' (GLAD) model for DST and obtains significant improvements in both accuracy and inference time compared to using top-N ASR hypotheses.

</details>

<details>

<summary>2020-08-01 19:22:51 - Tense, aspect and mood based event extraction for situation analysis and crisis management</summary>

- *Ali Hürriyetoğlu*

- `2008.01555v1` - [abs](http://arxiv.org/abs/2008.01555v1) - [pdf](http://arxiv.org/pdf/2008.01555v1)

> Nowadays event extraction systems mainly deal with a relatively small amount of information about temporal and modal qualifications of situations, primarily processing assertive sentences in the past tense. However, systems with a wider coverage of tense, aspect and mood can provide better analyses and can be used in a wider range of text analysis applications. This thesis develops such a system for Turkish language. This is accomplished by extending Open Source Information Mining and Analysis (OPTIMA) research group's event extraction software, by implementing appropriate extensions in the semantic representation format, by adding a partial grammar which improves the TAM (Tense, Aspect and Mood) marker, adverb analysis and matching functions of ExPRESS, and by constructing an appropriate lexicon in the standard of CORLEONE. These extensions are based on iv the theory of anchoring relations (Tem\"urc\"u, 2007, 2011) which is a crosslinguistically applicable semantic framework for analyzing tense, aspect and mood related categories. The result is a system which can, in addition to extracting basic event structures, classify sentences given in news reports according to their temporal, modal and volitional/illocutionary values. Although the focus is on news reports of natural disasters, disease outbreaks and man-made disasters in Turkish language, the approach can be adapted to other languages, domains and genres. This event extraction and classification system, with further developments, can provide a basis for automated browsing systems for preventing environmental and humanitarian risk.

</details>

<details>

<summary>2020-08-02 14:01:20 - Algebraic Extension Ring Framework for Non-Commutative Asymmetric Cryptography</summary>

- *Pedro Hecht*

- `2002.08343v2` - [abs](http://arxiv.org/abs/2002.08343v2) - [pdf](http://arxiv.org/pdf/2002.08343v2)

> Post-Quantum Cryptography PQC attempts to find cryptographic protocols resistant to attacks using Shors polynomial time algorithm for numerical field problems or Grovers algorithm to find the unique input to a black-box function that produces a particular output value. The use of non-standard algebraic structures like non-commutative or non-associative structures, combined with one-way trapdoor functions derived from combinatorial group theory, are mainly unexplored choices for these new kinds of protocols and overlooked in current PQC solutions. In this paper, we develop an algebraic extension ring framework who could be applied to different asymmetric protocols, i.e. key exchange, key transport, enciphering, digital signature, zero-knowledge authentication, oblivious transfer, secret sharing etc.. A valuable feature is that there is no need for big number libraries as all arithmetic is performed in F256 extension field operations (precisely the AES field). We assume that the new framework is cryptographical secure against strong classical attacks like the sometimes-useful length-based attack, Romankovs linearization attacks and Tsabans algebraic span attack. This statement is based on the non-linear structure of the selected platform which proved to be useful protecting the AES protocol. Otherwise, it could resist post-quantum attacks Grover, Shor and be particularly useful for computational platforms with limited capabilities like USB cryptographic keys or smartcards. Semantic security IND-CCA2 could also be inferred for this new platform.

</details>

<details>

<summary>2020-08-03 04:12:31 - High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands</summary>

- *Dibakar Gope, Jesse Beu, Matthew Mattina*

- `2008.00638v1` - [abs](http://arxiv.org/abs/2008.00638v1) - [pdf](http://arxiv.org/pdf/2008.00638v1)

> Matrix multiplications between asymmetric bit-width operands, especially between 8- and 4-bit operands are likely to become a fundamental kernel of many important workloads including neural networks and machine learning. While existing SIMD matrix multiplication instructions for symmetric bit-width operands can support operands of mixed precision by zero- or sign-extending the narrow operand to match the size of the other operands, they cannot exploit the benefit of narrow bit-width of one of the operands. We propose a new SIMD matrix multiplication instruction that uses mixed precision on its inputs (8- and 4-bit operands) and accumulates product values into narrower 16-bit output accumulators, in turn allowing the SIMD operation at 128-bit vector width to process a greater number of data elements per instruction to improve processing throughput and memory bandwidth utilization without increasing the register read- and write-port bandwidth in CPUs. The proposed asymmetric-operand-size SIMD instruction offers 2x improvement in throughput of matrix multiplication in comparison to throughput obtained using existing symmetric-operand-size instructions while causing negligible (0.05%) overflow from 16-bit accumulators for representative machine learning workloads. The asymmetric-operand-size instruction not only can improve matrix multiplication throughput in CPUs, but also can be effective to support multiply-and-accumulate (MAC) operation between 8- and 4-bit operands in state-of-the-art DNN hardware accelerators (e.g., systolic array microarchitecture in Google TPU, etc.) and offer similar improvement in matrix multiply performance seamlessly without violating the various implementation constraints. We demonstrate how a systolic array architecture designed for symmetric-operand-size instructions could be modified to support an asymmetric-operand-sized instruction.

</details>

<details>

<summary>2020-08-03 06:33:11 - The pursuit of beauty: Converting image labels to meaningful vectors</summary>

- *Savvas Karatsiolis, Andreas Kamilaris*

- `2008.00665v1` - [abs](http://arxiv.org/abs/2008.00665v1) - [pdf](http://arxiv.org/pdf/2008.00665v1)

> A challenge of the computer vision community is to understand the semantics of an image, in order to allow image reconstruction based on existing high-level features or to better analyze (semi-)labelled datasets. Towards addressing this challenge, this paper introduces a method, called Occlusion-based Latent Representations (OLR), for converting image labels to meaningful representations that capture a significant amount of data semantics. Besides being informational rich, these representations compose a disentangled low-dimensional latent space where each image label is encoded into a separate vector. We evaluate the quality of these representations in a series of experiments whose results suggest that the proposed model can capture data concepts and discover data interrelations.

</details>

<details>

<summary>2020-08-03 06:43:11 - Deep Learning based Topic Analysis on Financial Emerging Event Tweets</summary>

- *Shaan Aryaman, Nguwi Yok Yen*

- `2008.00670v1` - [abs](http://arxiv.org/abs/2008.00670v1) - [pdf](http://arxiv.org/pdf/2008.00670v1)

> Financial analyses of stock markets rely heavily on quantitative approaches in an attempt to predict subsequent or market movements based on historical prices and other measurable metrics. These quantitative analyses might have missed out on un-quantifiable aspects like sentiment and speculation that also impact the market. Analyzing vast amounts of qualitative text data to understand public opinion on social media platform is one approach to address this gap. This work carried out topic analysis on 28264 financial tweets [1] via clustering to discover emerging events in the stock market. Three main topics were discovered to be discussed frequently within the period. First, the financial ratio EPS is a measure that has been discussed frequently by investors. Secondly, short selling of shares were discussed heavily, it was often mentioned together with Morgan Stanley. Thirdly, oil and energy sectors were often discussed together with policy. These tweets were semantically clustered by a method consisting of word2vec algorithm to obtain word embeddings that map words to vectors. Semantic word clusters were then formed. Each tweet was then vectorized using the Term Frequency-Inverse Document Frequency (TF-IDF) values of the words it consisted of and based on which clusters its words were in. Tweet vectors were then converted to compressed representations by training a deep-autoencoder. K-means clusters were then formed. This method reduces dimensionality and produces dense vectors, in contrast to the usual Vector Space Model. Topic modelling with Latent Dirichlet Allocation (LDA) and top frequent words were used to analyze clusters and reveal emerging events.

</details>

<details>

<summary>2020-08-03 08:59:06 - Photon: A Robust Cross-Domain Text-to-SQL System</summary>

- *Jichuan Zeng, Xi Victoria Lin, Caiming Xiong, Richard Socher, Michael R. Lyu, Irwin King, Steven C. H. Hoi*

- `2007.15280v2` - [abs](http://arxiv.org/abs/2007.15280v2) - [pdf](http://arxiv.org/pdf/2007.15280v2)

> Natural language interfaces to databases (NLIDB) democratize end user access to relational data. Due to fundamental differences between natural language communication and programming, it is common for end users to issue questions that are ambiguous to the system or fall outside the semantic scope of its underlying query language. We present Photon, a robust, modular, cross-domain NLIDB that can flag natural language input to which a SQL mapping cannot be immediately determined. Photon consists of a strong neural semantic parser (63.2\% structure accuracy on the Spider dev benchmark), a human-in-the-loop question corrector, a SQL executor and a response generator. The question corrector is a discriminative neural sequence editor which detects confusion span(s) in the input question and suggests rephrasing until a translatable input is given by the user or a maximum number of iterations are conducted. Experiments on simulated data show that the proposed method effectively improves the robustness of text-to-SQL system against untranslatable user input. The live demo of our system is available at http://naturalsql.com.

</details>

<details>

<summary>2020-08-03 12:52:23 - Multi-Task Driven Explainable Diagnosis of COVID-19 using Chest X-ray Images</summary>

- *Aakarsh Malhotra, Surbhi Mittal, Puspita Majumdar, Saheb Chhabra, Kartik Thakral, Mayank Vatsa, Richa Singh, Santanu Chaudhury, Ashwin Pudrod, Anjali Agrawal*

- `2008.03205v1` - [abs](http://arxiv.org/abs/2008.03205v1) - [pdf](http://arxiv.org/pdf/2008.03205v1)

> With increasing number of COVID-19 cases globally, all the countries are ramping up the testing numbers. While the RT-PCR kits are available in sufficient quantity in several countries, others are facing challenges with limited availability of testing kits and processing centers in remote areas. This has motivated researchers to find alternate methods of testing which are reliable, easily accessible and faster. Chest X-Ray is one of the modalities that is gaining acceptance as a screening modality. Towards this direction, the paper has two primary contributions. Firstly, we present the COVID-19 Multi-Task Network which is an automated end-to-end network for COVID-19 screening. The proposed network not only predicts whether the CXR has COVID-19 features present or not, it also performs semantic segmentation of the regions of interest to make the model explainable. Secondly, with the help of medical professionals, we manually annotate the lung regions of 9000 frontal chest radiographs taken from ChestXray-14, CheXpert and a consolidated COVID-19 dataset. Further, 200 chest radiographs pertaining to COVID-19 patients are also annotated for semantic segmentation. This database will be released to the research community.

</details>

<details>

<summary>2020-08-03 13:06:03 - Bias-based Universal Adversarial Patch Attack for Automatic Check-out</summary>

- *Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, Hang Yu*

- `2005.09257v3` - [abs](http://arxiv.org/abs/2005.09257v3) - [pdf](http://arxiv.org/pdf/2005.09257v3)

> Adversarial examples are inputs with imperceptible perturbations that easily misleading deep neural networks(DNNs). Recently, adversarial patch, with noise confined to a small and localized patch, has emerged for its easy feasibility in real-world scenarios. However, existing strategies failed to generate adversarial patches with strong generalization ability. In other words, the adversarial patches were input-specific and failed to attack images from all classes, especially unseen ones during training. To address the problem, this paper proposes a bias-based framework to generate class-agnostic universal adversarial patches with strong generalization ability, which exploits both the perceptual and semantic bias of models. Regarding the perceptual bias, since DNNs are strongly biased towards textures, we exploit the hard examples which convey strong model uncertainties and extract a textural patch prior from them by adopting the style similarities. The patch prior is more close to decision boundaries and would promote attacks. To further alleviate the heavy dependency on large amounts of data in training universal attacks, we further exploit the semantic bias. As the class-wise preference, prototypes are introduced and pursued by maximizing the multi-class margin to help universal training. Taking AutomaticCheck-out (ACO) as the typical scenario, extensive experiments including white-box and black-box settings in both digital-world(RPC, the largest ACO related dataset) and physical-world scenario(Taobao and JD, the world' s largest online shopping platforms) are conducted. Experimental results demonstrate that our proposed framework outperforms state-of-the-art adversarial patch attack methods.

</details>

<details>

<summary>2020-08-03 13:54:47 - Towards a Semantic Model of the GDPR Register of Processing Activities</summary>

- *Paul Ryan, Harshvardhan J. Pandit, Rob Brennan*

- `2008.00877v1` - [abs](http://arxiv.org/abs/2008.00877v1) - [pdf](http://arxiv.org/pdf/2008.00877v1)

> A core requirement for GDPR compliance is the maintenance of a register of processing activities (ROPA). Our analysis of six ROPA templates from EU data protection regulators shows the scope and granularity of a ROPA is subject to widely varying guidance in different jurisdictions. We present a consolidated data model based on common concepts and relationships across analysed templates. We then analyse the extent of using the Data Privacy Vocabulary - a vocabulary specification for GDPR. We show that the DPV currently does not provide sufficient concepts to represent the ROPA data model and propose an extension to fill this gap. This will enable creation of a pan-EU information management framework for interoperability between organisations and regulators for GDPR compliance.

</details>

<details>

<summary>2020-08-03 14:47:41 - On the Efficiency of Test Suite based Program Repair: A Systematic Assessment of 16 Automated Repair Systems for Java Programs</summary>

- *Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, Tegawendé F. Bissyandé, Dongsun Kim, Peng Wu, Jacques Klein, Xiaoguang Mao, Yves Le Traon*

- `2008.00914v1` - [abs](http://arxiv.org/abs/2008.00914v1) - [pdf](http://arxiv.org/pdf/2008.00914v1)

> Test-based automated program repair has been a prolific field of research in software engineering in the last decade. Many approaches have indeed been proposed, which leverage test suites as a weak, but affordable, approximation to program specifications. Although the literature regularly sets new records on the number of benchmark bugs that can be fixed, several studies increasingly raise concerns about the limitations and biases of state-of-the-art approaches. For example, the correctness of generated patches has been questioned in a number of studies, while other researchers pointed out that evaluation schemes may be misleading with respect to the processing of fault localization results. Nevertheless, there is little work addressing the efficiency of patch generation, with regard to the practicality of program repair. In this paper, we fill this gap in the literature, by providing an extensive review on the efficiency of test suite based program repair. Our objective is to assess the number of generated patch candidates, since this information is correlated to (1) the strategy to traverse the search space efficiently in order to select sensical repair attempts, (2) the strategy to minimize the test effort for identifying a plausible patch, (3) as well as the strategy to prioritize the generation of a correct patch. To that end, we perform a large-scale empirical study on the efficiency, in terms of quantity of generated patch candidates of the 16 open-source repair tools for Java programs. The experiments are carefully conducted under the same fault localization configurations to limit biases.

</details>

<details>

<summary>2020-08-03 20:48:31 - Weakly-Supervised Semantic Segmentation via Sub-category Exploration</summary>

- *Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai, Ming-Hsuan Yang*

- `2008.01183v1` - [abs](http://arxiv.org/abs/2008.01183v1) - [pdf](http://arxiv.org/pdf/2008.01183v1)

> Existing weakly-supervised semantic segmentation methods using image-level annotations typically rely on initial responses to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts, due to the fact that the network does not need the entire object for optimizing the objective function. To enforce the network to pay attention to other parts of an object, we propose a simple yet effective approach that introduces a self-supervised task by exploiting the sub-category information. Specifically, we perform clustering on image features to generate pseudo sub-categories labels within each annotated parent class, and construct a sub-category objective to assign the network to a more challenging task. By iteratively clustering image features, the training process does not limit itself to the most discriminative object parts, hence improving the quality of the response maps. We conduct extensive analysis to validate the proposed method and show that our approach performs favorably against the state-of-the-art approaches.

</details>

<details>

<summary>2020-08-03 20:51:01 - Generative Adversarial Networks for Synthesizing InSAR Patches</summary>

- *Philipp Sibler, Yuanyuan Wang, Stefan Auer, Mohsin Ali, Xiao Xiang Zhu*

- `2008.01184v1` - [abs](http://arxiv.org/abs/2008.01184v1) - [pdf](http://arxiv.org/pdf/2008.01184v1)

> Generative Adversarial Networks (GANs) have been employed with certain success for image translation tasks between optical and real-valued SAR intensity imagery. Applications include aiding interpretability of SAR scenes with their optical counterparts by artificial patch generation and automatic SAR-optical scene matching. The synthesis of artificial complex-valued InSAR image stacks asks for, besides good perceptual quality, more stringent quality metrics like phase noise and phase coherence. This paper provides a signal processing model of generative CNN structures, describes effects influencing those quality metrics and presents a mapping scheme of complex-valued data to given CNN structures based on popular Deep Learning frameworks.

</details>

<details>

<summary>2020-08-03 21:19:08 - Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization</summary>

- *Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai, Ming-Hsuan Yang*

- `2008.01201v1` - [abs](http://arxiv.org/abs/2008.01201v1) - [pdf](http://arxiv.org/pdf/2008.01201v1)

> Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels. However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss. To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map. Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy. In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches.

</details>

<details>

<summary>2020-08-03 21:48:50 - Generalized Zero-Shot Domain Adaptation via Coupled Conditional Variational Autoencoders</summary>

- *Qian Wang, Toby P. Breckon*

- `2008.01214v1` - [abs](http://arxiv.org/abs/2008.01214v1) - [pdf](http://arxiv.org/pdf/2008.01214v1)

> Domain adaptation approaches aim to exploit useful information from the source domain where supervised learning examples are easier to obtain to address a learning problem in the target domain where there is no or limited availability of such examples. In classification problems, domain adaptation has been studied under varying supervised, unsupervised and semi-supervised conditions. However, a common situation when the labelled samples are available for a subset of target domain classes has been overlooked. In this paper, we formulate this particular domain adaptation problem within a generalized zero-shot learning framework by treating the labelled source domain samples as semantic representations for zero-shot learning. For this particular problem, neither conventional domain adaptation approaches nor zero-shot learning algorithms directly apply. To address this generalized zero-shot domain adaptation problem, we present a novel Coupled Conditional Variational Autoencoder (CCVAE) which can generate synthetic target domain features for unseen classes from their source domain counterparts. Extensive experiments have been conducted on three domain adaptation datasets including a bespoke X-ray security checkpoint dataset to simulate a real-world application in aviation security. The results demonstrate the effectiveness of our proposed approach both against established benchmarks and in terms of real-world applicability.

</details>

<details>

<summary>2020-08-03 23:28:07 - Sketch-Driven Regular Expression Generation from Natural Language and Examples</summary>

- *Xi Ye, Qiaochu Chen, Xinyu Wang, Isil Dillig, Greg Durrett*

- `1908.05848v2` - [abs](http://arxiv.org/abs/1908.05848v2) - [pdf](http://arxiv.org/pdf/1908.05848v2)

> Recent systems for converting natural language descriptions into regular expressions (regexes) have achieved some success, but typically deal with short, formulaic text and can only produce simple regexes. Realworld regexes are complex, hard to describe with brief sentences, and sometimes require examples to fully convey the user's intent. We present a framework for regex synthesis in this setting where both natural language (NL) and examples are available. First, a semantic parser (either grammar-based or neural) maps the natural language description into an intermediate sketch, which is an incomplete regex containing holes to denote missing components. Then a program synthesizer searches over the regex space defined by the sketch and finds a regex that is consistent with the given string examples. Our semantic parser can be trained purely from weak supervision based on correctness of the synthesized regex, or it can leverage heuristically-derived sketches. We evaluate on two prior datasets (Kushman and Barzilay, 2013; Locascio et al., 2016) and a real-world dataset from Stack Overflow. Our system achieves state-of-the-art performance on the prior datasets and solves 57% of the real-world dataset, which existing neural systems completely fail on.

</details>

<details>

<summary>2020-08-04 04:08:02 - TextCaps: a Dataset for Image Captioning with Reading Comprehension</summary>

- *Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh*

- `2003.12462v2` - [abs](http://arxiv.org/abs/2003.12462v2) - [pdf](http://arxiv.org/pdf/2003.12462v2)

> Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.

</details>

<details>

<summary>2020-08-04 08:37:35 - Self-Augmentation: Generalizing Deep Networks to Unseen Classes for Few-Shot Learning</summary>

- *Jin-Woo Seo, Hong-Gyu Jung, Seong-Whan Lee*

- `2004.00251v3` - [abs](http://arxiv.org/abs/2004.00251v3) - [pdf](http://arxiv.org/pdf/2004.00251v3)

> Few-shot learning aims to classify unseen classes with a few training examples. While recent works have shown that standard mini-batch training with a carefully designed training strategy can improve generalization ability for unseen classes, well-known problems in deep networks such as memorizing training statistics have been less explored for few-shot learning. To tackle this issue, we propose self-augmentation that consolidates self-mix and self-distillation. Specifically, we exploit a regional dropout technique called self-mix, in which a patch of an image is substituted into other values in the same image. Then, we employ a backbone network that has auxiliary branches with its own classifier to enforce knowledge sharing. Lastly, we present a local representation learner to further exploit a few training examples for unseen classes. Experimental results show that the proposed method outperforms the state-of-the-art methods for prevalent few-shot benchmarks and improves the generalization ability.

</details>

<details>

<summary>2020-08-04 14:45:08 - Exploiting Game Theory for Analysing Justifications</summary>

- *Simon Marynissen, Bart Bogaerts, Marc Denecker*

- `2008.01609v1` - [abs](http://arxiv.org/abs/2008.01609v1) - [pdf](http://arxiv.org/pdf/2008.01609v1)

> Justification theory is a unifying semantic framework. While it has its roots in non-monotonic logics, it can be applied to various areas in computer science, especially in explainable reasoning; its most central concept is a justification: an explanation why a property holds (or does not hold) in a model. In this paper, we continue the study of justification theory by means of three major contributions. The first is studying the relation between justification theory and game theory. We show that justification frameworks can be seen as a special type of games. The established connection provides the theoretical foundations for our next two contributions. The second contribution is studying under which condition two different dialects of justification theory (graphs as explanations vs trees as explanations) coincide. The third contribution is establishing a precise criterion of when a semantics induced by justification theory yields consistent results. In the past proving that such semantics were consistent took cumbersome and elaborate proofs. We show that these criteria are indeed satisfied for all common semantics of logic programming. This paper is under consideration for acceptance in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2020-08-04 15:10:44 - Semantic based model of Conceptual Work Products for formal verification of complex interactive systems</summary>

- *Mohcine Madkour, Keith Butler, Eric Mercer, Ali Bahrami, Cui Tao*

- `2008.01623v1` - [abs](http://arxiv.org/abs/2008.01623v1) - [pdf](http://arxiv.org/pdf/2008.01623v1)

> Many clinical workflows depend on interactive computer systems for highly technical, conceptual work products, such as diagnoses, treatment plans, care coordination, and case management. We describe an automatic logic reasoner to verify objective specifications for these highly technical, but abstract, work products that are essential to care. The conceptual work products specifications serve as a fundamental output requirement, which must be clearly stated, correct and solvable. There is strategic importance for such specifications because, in turn, they enable system model checking to verify that machine functions taken with user procedures are actually able to achieve these abstract products. We chose case management of Multiple Sclerosis (MS) outpatients as our use case for its challenging complexity. As a first step, we illustrate how graphical class and state diagrams from UML can be developed and critiqued with subject matter experts to serve as specifications of the conceptual work product of case management. A key feature is that the specification must be declarative and thus independent of any process or technology. Our Work Domain Ontology with tools from Semantic Web is needed to translate UML class and state diagrams for verification of solvability with automatic reasoning. The solvable model will then be ready for subsequent use with model checking on the system of human procedures and machine functions. We used the expressive rule language SPARQL Inferencing Notation (SPIN) to develop formal representations of the UML class diagram, the state machine, and their interactions. Using SPIN, we proved the consistency of the interactions of static and dynamic concepts. We discussed how the new SPIN rule engine could be incorporated in the Object Management Group (OMG) Ontology Definition Metamodel (ODM)

</details>

<details>

<summary>2020-08-04 16:53:43 - Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations</summary>

- *Florian Tramèr, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, Jörn-Henrik Jacobsen*

- `2002.04599v2` - [abs](http://arxiv.org/abs/2002.04599v2) - [pdf](http://arxiv.org/pdf/2002.04599v2)

> Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied sensitivity-based adversarial examples introduce semantically-small changes to an input that result in a different model prediction. This paper studies a complementary failure mode, invariance-based adversarial examples, that introduce minimal semantic changes that modify an input's true label yet preserve the model's prediction. We demonstrate fundamental tradeoffs between these two types of adversarial examples.   We show that defenses against sensitivity-based attacks actively harm a model's accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types. In particular, we break state-of-the-art adversarially-trained and certifiably-robust models by generating small perturbations that the models are (provably) robust to, yet that change an input's class according to human labelers. Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of overly-robust predictive features in standard datasets.

</details>

<details>

<summary>2020-08-04 18:20:04 - Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs</summary>

- *Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, Xavier Amatriain*

- `2008.13546v1` - [abs](http://arxiv.org/abs/2008.13546v1) - [pdf](http://arxiv.org/pdf/2008.13546v1)

> People increasingly search online for answers to their medical questions but the rate at which medical questions are asked online significantly exceeds the capacity of qualified people to answer them. This leaves many questions unanswered or inadequately answered. Many of these questions are not unique, and reliable identification of similar questions would enable more efficient and effective question answering schema. COVID-19 has only exacerbated this problem. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs, but there is no way for people to ask their question and know if it is answered on one of these pages. While many research efforts have focused on the problem of general question similarity, these approaches do not generalize well to domains that require expert knowledge to determine semantic similarity, such as the medical domain. In this paper, we show how a double fine-tuning approach of pretraining a neural network on medical question-answer pairs followed by fine-tuning on medical question-question pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pretraining tasks yield an accuracy below 78.7% on this task, our model achieves an accuracy of 82.6% with the same number of training examples, an accuracy of 80.0% with a much smaller training set, and an accuracy of 84.5% when the full corpus of medical question-answer data is used. We also describe a currently live system that uses the trained model to match user questions to COVID-related FAQs.

</details>

<details>

<summary>2020-08-04 19:12:20 - Towards Emergent Language Symbolic Semantic Segmentation and Model Interpretability</summary>

- *Alberto Santamaria-Pang, James Kubricht, Aritra Chowdhury, Chitresh Bhushan, Peter Tu*

- `2007.09448v2` - [abs](http://arxiv.org/abs/2007.09448v2) - [pdf](http://arxiv.org/pdf/2007.09448v2)

> Recent advances in methods focused on the grounding problem have resulted in techniques that can be used to construct a symbolic language associated with a specific domain. Inspired by how humans communicate complex ideas through language, we developed a generalized Symbolic Semantic ($\text{S}^2$) framework for interpretable segmentation. Unlike adversarial models (e.g., GANs), we explicitly model cooperation between two agents, a Sender and a Receiver, that must cooperate to achieve a common goal. The Sender receives information from a high layer of a segmentation network and generates a symbolic sentence derived from a categorical distribution. The Receiver obtains the symbolic sentences and co-generates the segmentation mask. In order for the model to converge, the Sender and Receiver must learn to communicate using a private language. We apply our architecture to segment tumors in the TCGA dataset. A UNet-like architecture is used to generate input to the Sender network which produces a symbolic sentence, and a Receiver network co-generates the segmentation mask based on the sentence. Our Segmentation framework achieved similar or better performance compared with state-of-the-art segmentation methods. In addition, our results suggest direct interpretation of the symbolic sentences to discriminate between normal and tumor tissue, tumor morphology, and other image characteristics.

</details>

<details>

<summary>2020-08-04 23:59:11 - Event Prediction in the Big Data Era: A Systematic Survey</summary>

- *Liang Zhao*

- `2007.09815v3` - [abs](http://arxiv.org/abs/2007.09815v3) - [pdf](http://arxiv.org/pdf/2007.09815v3)

> Events are occurrences in specific locations, time, and semantics that nontrivially impact either our society or the nature, such as civil unrest, system failures, and epidemics. It is highly desirable to be able to anticipate the occurrence of such events in advance in order to reduce the potential social upheaval and damage caused. Event prediction, which has traditionally been prohibitively challenging, is now becoming a viable option in the big data era and is thus experiencing rapid growth. There is a large amount of existing work that focuses on addressing the challenges involved, including heterogeneous multi-faceted outputs, complex dependencies, and streaming data feeds. Most existing event prediction methods were initially designed to deal with specific application domains, though the techniques and evaluation procedures utilized are usually generalizable across different domains. However, it is imperative yet difficult to cross-reference the techniques across different domains, given the absence of a comprehensive literature survey for event prediction. This paper aims to provide a systematic and comprehensive survey of the technologies, applications, and evaluations of event prediction in the big data era. First, systematic categorization and summary of existing techniques are presented, which facilitate domain experts' searches for suitable techniques and help model developers consolidate their research at the frontiers. Then, comprehensive categorization and summary of major application domains are provided. Evaluation metrics and procedures are summarized and standardized to unify the understanding of model performance among stakeholders, model developers, and domain experts in various application domains. Finally, open problems and future directions for this promising and important domain are elucidated and discussed.

</details>

<details>

<summary>2020-08-05 02:34:44 - A Large Scale Analysis of Android-Web Hybridization</summary>

- *Abhishek Tiwari, Jyoti Prakash, Sascha Gross, Christian Hammer*

- `2008.01725v2` - [abs](http://arxiv.org/abs/2008.01725v2) - [pdf](http://arxiv.org/pdf/2008.01725v2)

> Many Android applications embed webpages via WebView components and execute JavaScript code within Android. Hybrid applications leverage dedicated APIs to load a resource and render it in a WebView. Furthermore, Android objects can be shared with the JavaScript world. However, bridging the interfaces of the Android and JavaScript world might also incur severe security threats: Potentially untrusted webpages and their JavaScript might interfere with the Android environment and its access to native features. No general analysis is currently available to assess the implications of such hybrid apps bridging the two worlds. To understand the semantics and effects of hybrid apps, we perform a large-scale study on the usage of the hybridization APIs in the wild. We analyze and categorize the parameters to hybridization APIs for 7,500 randomly selected and the 196 most popular applications from the Google Playstore as well as 1000 malware samples. Our results advance the general understanding of hybrid applications, as well as implications for potential program analyses, and the current security situation: We discovered thousands of flows of sensitive data from Android to JavaScript, the vast majority of which could flow to potentially untrustworthy code. Our analysis identified numerous web pages embedding vulnerabilities, which we exemplarily exploited. Additionally, we discovered a multitude of applications in which potentially untrusted JavaScript code may interfere with (trusted) Android objects, both in benign and malign applications.

</details>

<details>

<summary>2020-08-05 03:04:20 - Simultaneous Semantic Alignment Network for Heterogeneous Domain Adaptation</summary>

- *Shuang Li, Binhui Xie, Jiashu Wu, Ying Zhao, Chi Harold Liu, Zhengming Ding*

- `2008.01677v2` - [abs](http://arxiv.org/abs/2008.01677v2) - [pdf](http://arxiv.org/pdf/2008.01677v2)

> Heterogeneous domain adaptation (HDA) transfers knowledge across source and target domains that present heterogeneities e.g., distinct domain distributions and difference in feature type or dimension. Most previous HDA methods tackle this problem through learning a domain-invariant feature subspace to reduce the discrepancy between domains. However, the intrinsic semantic properties contained in data are under-explored in such alignment strategy, which is also indispensable to achieve promising adaptability. In this paper, we propose a Simultaneous Semantic Alignment Network (SSAN) to simultaneously exploit correlations among categories and align the centroids for each category across domains. In particular, we propose an implicit semantic correlation loss to transfer the correlation knowledge of source categorical prediction distributions to target domain. Meanwhile, by leveraging target pseudo-labels, a robust triplet-centroid alignment mechanism is explicitly applied to align feature representations for each category. Notably, a pseudo-label refinement procedure with geometric similarity involved is introduced to enhance the target pseudo-label assignment accuracy. Comprehensive experiments on various HDA tasks across text-to-image, image-to-image and text-to-text successfully validate the superiority of our SSAN against state-of-the-art HDA methods. The code is publicly available at https://github.com/BIT-DA/SSAN.

</details>

<details>

<summary>2020-08-05 08:34:16 - Towards Ranking-based Semantics for Abstract Argumentation using Conditional Logic Semantics</summary>

- *Kenneth Skiba, Matthias Thimm*

- `2008.02735v1` - [abs](http://arxiv.org/abs/2008.02735v1) - [pdf](http://arxiv.org/pdf/2008.02735v1)

> We propose a novel ranking-based semantics for Dung-style argumentation frameworks with the help of conditional logics. Using an intuitive translation for an argumentation framework to generate conditionals, we can apply nonmonotonic inference systems to generate a ranking on possible worlds. With this ranking we construct a ranking for our arguments. With a small extension to this ranking-based semantics we already satisfy some desirable properties for a ranking over arguments.

</details>

<details>

<summary>2020-08-05 09:32:05 - eclingo: A solver for Epistemic Logic Programs</summary>

- *Pedro Cabalar, Jorge Fandinno, Javier Garea, Javier Romero, Torsten Schaub*

- `2008.02018v1` - [abs](http://arxiv.org/abs/2008.02018v1) - [pdf](http://arxiv.org/pdf/2008.02018v1)

> We describe eclingo, a solver for epistemic logic programs under Gelfond 1991 semantics built upon the Answer Set Programming system clingo. The input language of eclingo uses the syntax extension capabilities of clingo to define subjective literals that, as usual in epistemic logic programs, allow for checking the truth of a regular literal in all or in some of the answer sets of a program. The eclingo solving process follows a guess and check strategy. It first generates potential truth values for subjective literals and, in a second step, it checks the obtained result with respect to the cautious and brave consequences of the program. This process is implemented using the multi-shot functionalities of clingo. We have also implemented some optimisations, aiming at reducing the search space and, therefore, increasing eclingo's efficiency in some scenarios. Finally, we compare the efficiency of eclingo with two state-of-the-art solvers for epistemic logic programs on a pair of benchmark scenarios and show that eclingo generally outperforms their obtained results. Under consideration for acceptance in TPLP.

</details>

<details>

<summary>2020-08-05 10:13:41 - Extracting and Leveraging Nodule Features with Lung Inpainting for Local Feature Augmentation</summary>

- *Sebastian Guendel, Arnaud Arindra Adiyoso Setio, Sasa Grbic, Andreas Maier, Dorin Comaniciu*

- `2008.02030v1` - [abs](http://arxiv.org/abs/2008.02030v1) - [pdf](http://arxiv.org/pdf/2008.02030v1)

> Chest X-ray (CXR) is the most common examination for fast detection of pulmonary abnormalities. Recently, automated algorithms have been developed to classify multiple diseases and abnormalities in CXR scans. However, because of the limited availability of scans containing nodules and the subtle properties of nodules in CXRs, state-of-the-art methods do not perform well on nodule classification. To create additional data for the training process, standard augmentation techniques are applied. However, the variance introduced by these methods are limited as the images are typically modified globally. In this paper, we propose a method for local feature augmentation by extracting local nodule features using a generative inpainting network. The network is applied to generate realistic, healthy tissue and structures in patches containing nodules. The nodules are entirely removed in the inpainted representation. The extraction of the nodule features is processed by subtraction of the inpainted patch from the nodule patch. With arbitrary displacement of the extracted nodules in the lung area across different CXR scans and further local modifications during training, we significantly increase the nodule classification performance and outperform state-of-the-art augmentation methods.

</details>

<details>

<summary>2020-08-05 13:00:34 - Duality Diagram Similarity: a generic framework for initialization selection in task transfer learning</summary>

- *Kshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, Gemma Roig*

- `2008.02107v1` - [abs](http://arxiv.org/abs/2008.02107v1) - [pdf](http://arxiv.org/pdf/2008.02107v1)

> In this paper, we tackle an open research question in transfer learning, which is selecting a model initialization to achieve high performance on a new task, given several pre-trained models. We propose a new highly efficient and accurate approach based on duality diagram similarity (DDS) between deep neural networks (DNNs). DDS is a generic framework to represent and compare data of different feature dimensions. We validate our approach on the Taskonomy dataset by measuring the correspondence between actual transfer learning performance rankings on 17 taskonomy tasks and predicted rankings. Computing DDS based ranking for $17\times17$ transfers requires less than 2 minutes and shows a high correlation ($0.86$) with actual transfer learning rankings, outperforming state-of-the-art methods by a large margin ($10\%$) on the Taskonomy benchmark. We also demonstrate the robustness of our model selection approach to a new task, namely Pascal VOC semantic segmentation. Additionally, we show that our method can be applied to select the best layer locations within a DNN for transfer learning on 2D, 3D and semantic tasks on NYUv2 and Pascal VOC datasets.

</details>

<details>

<summary>2020-08-05 14:07:47 - Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs</summary>

- *Tarcísio Souza Costa, Simon Gottschalk, Elena Demidova*

- `2004.11861v2` - [abs](http://arxiv.org/abs/2004.11861v2) - [pdf](http://arxiv.org/pdf/2004.11861v2)

> Semantic Question Answering (QA) is a crucial technology to facilitate intuitive user access to semantic information stored in knowledge graphs. Whereas most of the existing QA systems and datasets focus on entity-centric questions, very little is known about these systems' performance in the context of events. As new event-centric knowledge graphs emerge, datasets for such questions gain importance. In this paper, we present the Event-QA dataset for answering event-centric questions over knowledge graphs. Event-QA contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.

</details>

<details>

<summary>2020-08-05 16:26:50 - 6VecLM: Language Modeling in Vector Space for IPv6 Target Generation</summary>

- *Tianyu Cui, Gang Xiong, Gaopeng Gou, Junzheng Shi, Wei Xia*

- `2008.02213v1` - [abs](http://arxiv.org/abs/2008.02213v1) - [pdf](http://arxiv.org/pdf/2008.02213v1)

> Fast IPv6 scanning is challenging in the field of network measurement as it requires exploring the whole IPv6 address space but limited by current computational power. Researchers propose to obtain possible active target candidate sets to probe by algorithmically analyzing the active seed sets. However, IPv6 addresses lack semantic information and contain numerous addressing schemes, leading to the difficulty of designing effective algorithms. In this paper, we introduce our approach 6VecLM to explore achieving such target generation algorithms. The architecture can map addresses into a vector space to interpret semantic relationships and uses a Transformer network to build IPv6 language models for predicting address sequence. Experiments indicate that our approach can perform semantic classification on address space. By adding a new generation approach, our model possesses a controllable word innovation capability compared to conventional language models. The work outperformed the state-of-the-art target generation algorithms on two active address datasets by reaching more quality candidate sets.

</details>

<details>

<summary>2020-08-05 16:59:59 - DaRLing: A Datalog rewriter for OWL 2 RL ontological reasoning under SPARQL queries</summary>

- *Alessio Fiorentino, Jessica Zangari, Marco Manna*

- `2008.02232v1` - [abs](http://arxiv.org/abs/2008.02232v1) - [pdf](http://arxiv.org/pdf/2008.02232v1)

> The W3C Web Ontology Language (OWL) is a powerful knowledge representation formalism at the basis of many semantic-centric applications. Since its unrestricted usage makes reasoning undecidable already in case of very simple tasks, expressive yet decidable fragments have been identified. Among them, we focus on OWL 2 RL, which offers a rich variety of semantic constructors, apart from supporting all RDFS datatypes. Although popular Web resources - such as DBpedia - fall in OWL 2 RL, only a few systems have been designed and implemented for this fragment. None of them, however, fully satisfy all the following desiderata: (i) being freely available and regularly maintained; (ii) supporting query answering and SPARQL queries; (iii) properly applying the sameAs property without adopting the unique name assumption; (iv) dealing with concrete datatypes. To fill the gap, we present DaRLing, a freely available Datalog rewriter for OWL 2 RL ontological reasoning under SPARQL queries. In particular, we describe its architecture, the rewriting strategies it implements, and the result of an experimental evaluation that demonstrates its practical applicability. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2020-08-05 20:15:30 - Categories of Semantic Concepts</summary>

- *James Hefford, Vincent Wang, Matthew Wilson*

- `2004.10741v2` - [abs](http://arxiv.org/abs/2004.10741v2) - [pdf](http://arxiv.org/pdf/2004.10741v2)

> Modelling concept representation is a foundational problem in the study of cognition and linguistics. This work builds on the confluence of conceptual tools from G\"ardenfors semantic spaces, categorical compositional linguistics, and applied category theory to present a domain-independent and categorical formalism of 'concept'.

</details>

<details>

<summary>2020-08-05 20:43:55 - A Neural-Symbolic Framework for Mental Simulation</summary>

- *Michael Kissner*

- `2008.02356v1` - [abs](http://arxiv.org/abs/2008.02356v1) - [pdf](http://arxiv.org/pdf/2008.02356v1)

> We present a neural-symbolic framework for observing the environment and continuously learning visual semantics and intuitive physics to reproduce them in an interactive simulation. The framework consists of five parts, a neural-symbolic hybrid network based on capsules for inverse graphics, an episodic memory to store observations, an interaction network for intuitive physics, a meta-learning agent that continuously improves the framework and a querying language that acts as the framework's interface for simulation. By means of lifelong meta-learning, the capsule network is expanded and trained continuously, in order to better adapt to its environment with each iteration. This enables it to learn new semantics using a few-shot approach and with minimal input from an oracle over its lifetime. From what it learned through observation, the part for intuitive physics infers all the required physical properties of the objects in a scene, enabling predictions. Finally, a custom query language ties all parts together, which allows to perform various mental simulation tasks, such as navigation, sorting and simulation of a game environment, with which we illustrate the potential of our novel approach.

</details>

<details>

<summary>2020-08-06 04:53:37 - Towards Learning a Universal Non-Semantic Representation of Speech</summary>

- *Joel Shor, Aren Jansen, Ronnie Maor, Oran Lang, Omry Tuval, Felix de Chaumont Quitry, Marco Tagliasacchi, Ira Shavitt, Dotan Emanuel, Yinnon Haviv*

- `2002.12764v6` - [abs](http://arxiv.org/abs/2002.12764v6) - [pdf](http://arxiv.org/pdf/2002.12764v6)

> The ultimate goal of transfer learning is to reduce labeled data requirements by exploiting a pre-existing embedding model trained for different datasets or tasks. The visual and language communities have established benchmarks to compare embeddings, but the speech community has yet to do so. This paper proposes a benchmark for comparing speech representations on non-semantic tasks, and proposes a representation based on an unsupervised triplet-loss objective. The proposed representation outperforms other representations on the benchmark, and even exceeds state-of-the-art performance on a number of transfer learning tasks. The embedding is trained on a publicly available dataset, and it is tested on a variety of low-resource downstream tasks, including personalization tasks and medical domain. The benchmark, models, and evaluation code are publicly released.

</details>

<details>

<summary>2020-08-06 05:12:11 - DeText: A Deep Text Ranking Framework with BERT</summary>

- *Weiwei Guo, Xiaowei Liu, Sida Wang, Huiji Gao, Ananth Sankar, Zimeng Yang, Qi Guo, Liang Zhang, Bo Long, Bee-Chung Chen, Deepak Agarwal*

- `2008.02460v1` - [abs](http://arxiv.org/abs/2008.02460v1) - [pdf](http://arxiv.org/pdf/2008.02460v1)

> Ranking is the most important component in a search system. Mostsearch systems deal with large amounts of natural language data,hence an effective ranking system requires a deep understandingof text semantics. Recently, deep learning based natural languageprocessing (deep NLP) models have generated promising results onranking systems. BERT is one of the most successful models thatlearn contextual embedding, which has been applied to capturecomplex query-document relations for search ranking. However,this is generally done by exhaustively interacting each query wordwith each document word, which is inefficient for online servingin search product systems. In this paper, we investigate how tobuild an efficient BERT-based ranking model for industry use cases.The solution is further extended to a general ranking framework,DeText, that is open sourced and can be applied to various rankingproductions. Offline and online experiments of DeText on threereal-world search systems present significant improvement overstate-of-the-art approaches.

</details>

<details>

<summary>2020-08-06 08:23:55 - Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion</summary>

- *Roland Hafner, Tim Hertweck, Philipp Klöppner, Michael Bloesch, Michael Neunert, Markus Wulfmeier, Saran Tunyasuvunakool, Nicolas Heess, Martin Riedmiller*

- `2008.12228v1` - [abs](http://arxiv.org/abs/2008.12228v1) - [pdf](http://arxiv.org/pdf/2008.12228v1)

> Modern Reinforcement Learning (RL) algorithms promise to solve difficult motor control problems directly from raw sensory inputs. Their attraction is due in part to the fact that they can represent a general class of methods that allow to learn a solution with a reasonably set reward and minimal prior knowledge, even in situations where it is difficult or expensive for a human expert. For RL to truly make good on this promise, however, we need algorithms and learning setups that can work across a broad range of problems with minimal problem specific adjustments or engineering. In this paper, we study this idea of generality in the locomotion domain. We develop a learning framework that can learn sophisticated locomotion behavior for a wide spectrum of legged robots, such as bipeds, tripeds, quadrupeds and hexapods, including wheeled variants. Our learning framework relies on a data-efficient, off-policy multi-task RL algorithm and a small set of reward functions that are semantically identical across robots. To underline the general applicability of the method, we keep the hyper-parameter settings and reward definitions constant across experiments and rely exclusively on on-board sensing. For nine different types of robots, including a real-world quadruped robot, we demonstrate that the same algorithm can rapidly learn diverse and reusable locomotion skills without any platform specific adjustments or additional instrumentation of the learning setup.

</details>

<details>

<summary>2020-08-06 10:04:53 - On the Semantics of Abstract Argumentation Frameworks: A Logic Programming Approach</summary>

- *Gianvincenzo Alfano, Sergio Greco, Francesco Parisi, Irina Trubitsyna*

- `2008.02550v1` - [abs](http://arxiv.org/abs/2008.02550v1) - [pdf](http://arxiv.org/pdf/2008.02550v1)

> Recently there has been an increasing interest in frameworks extending Dung's abstract Argumentation Framework (AF). Popular extensions include bipolar AFs and AFs with recursive attacks and necessary supports. Although the relationships between AF semantics and Partial Stable Models (PSMs) of logic programs has been deeply investigated, this is not the case for more general frameworks extending AF.   In this paper we explore the relationships between AF-based frameworks and PSMs. We show that every AF-based framework $\Delta$ can be translated into a logic program $P_\Delta$ so that the extensions prescribed by different semantics of $\Delta$ coincide with subsets of the PSMs of $P_\Delta$. We provide a logic programming approach that characterizes, in an elegant and uniform way, the semantics of several AF-based frameworks. This result allows also to define the semantics for new AF-based frameworks, such as AFs with recursive attacks and recursive deductive supports.   Under consideration for publication in Theory and Practice of Logic Programming.

</details>

<details>

<summary>2020-08-06 11:59:00 - Semantic Sentiment Analysis Based on Probabilistic Graphical Models and Recurrent Neural Network</summary>

- *Ukachi Osisiogu*

- `2009.00234v1` - [abs](http://arxiv.org/abs/2009.00234v1) - [pdf](http://arxiv.org/pdf/2009.00234v1)

> Sentiment Analysis is the task of classifying documents based on the sentiments expressed in textual form, this can be achieved by using lexical and semantic methods. The purpose of this study is to investigate the use of semantics to perform sentiment analysis based on probabilistic graphical models and recurrent neural networks. In the empirical evaluation, the classification performance of the graphical models was compared with some traditional machine learning classifiers and a recurrent neural network. The datasets used for the experiments were IMDB movie reviews, Amazon Consumer Product reviews, and Twitter Review datasets. After this empirical study, we conclude that the inclusion of semantics for sentiment analysis tasks can greatly improve the performance of a classifier, as the semantic feature extraction methods reduce uncertainties in classification resulting in more accurate predictions.

</details>

<details>

<summary>2020-08-06 14:19:51 - Injecting Prior Knowledge into Image Caption Generation</summary>

- *Arushi Goel, Basura Fernando, Thanh-Son Nguyen, Hakan Bilen*

- `1911.10082v2` - [abs](http://arxiv.org/abs/1911.10082v2) - [pdf](http://arxiv.org/pdf/1911.10082v2)

> Automatically generating natural language descriptions from an image is a challenging problem in artificial intelligence that requires a good understanding of the visual and textual signals and the correlations between them. The state-of-the-art methods in image captioning struggles to approach human level performance, especially when data is limited. In this paper, we propose to improve the performance of the state-of-the-art image captioning models by incorporating two sources of prior knowledge: (i) a conditional latent topic attention, that uses a set of latent variables (topics) as an anchor to generate highly probable words and, (ii) a regularization technique that exploits the inductive biases in syntactic and semantic structure of captions and improves the generalization of image captioning models. Our experiments validate that our method produces more human interpretable captions and also leads to significant improvements on the MSCOCO dataset in both the full and low data regimes.

</details>

<details>

<summary>2020-08-06 15:02:38 - Pairwise Relation Learning for Semi-supervised Gland Segmentation</summary>

- *Yutong Xie, Jianpeng Zhang, Zhibin Liao, Chunhua Shen, Johan Verjans, Yong Xia*

- `2008.02699v1` - [abs](http://arxiv.org/abs/2008.02699v1) - [pdf](http://arxiv.org/pdf/2008.02699v1)

> Accurate and automated gland segmentation on histology tissue images is an essential but challenging task in the computer-aided diagnosis of adenocarcinoma. Despite their prevalence, deep learning models always require a myriad number of densely annotated training images, which are difficult to obtain due to extensive labor and associated expert costs related to histology image annotations. In this paper, we propose the pairwise relation-based semi-supervised (PRS^2) model for gland segmentation on histology images. This model consists of a segmentation network (S-Net) and a pairwise relation network (PR-Net). The S-Net is trained on labeled data for segmentation, and PR-Net is trained on both labeled and unlabeled data in an unsupervised way to enhance its image representation ability via exploiting the semantic consistency between each pair of images in the feature space. Since both networks share their encoders, the image representation ability learned by PR-Net can be transferred to S-Net to improve its segmentation performance. We also design the object-level Dice loss to address the issues caused by touching glands and combine it with other two loss functions for S-Net. We evaluated our model against five recent methods on the GlaS dataset and three recent methods on the CRAG dataset. Our results not only demonstrate the effectiveness of the proposed PR-Net and object-level Dice loss, but also indicate that our PRS^2 model achieves the state-of-the-art gland segmentation performance on both benchmarks.

</details>

<details>

<summary>2020-08-06 20:18:53 - Semantic Complexity in End-to-End Spoken Language Understanding</summary>

- *Joseph P. McKenna, Samridhi Choudhary, Michael Saxon, Grant P. Strimel, Athanasios Mouchtaris*

- `2008.02858v1` - [abs](http://arxiv.org/abs/2008.02858v1) - [pdf](http://arxiv.org/pdf/2008.02858v1)

> End-to-end spoken language understanding (SLU) models are a class of model architectures that predict semantics directly from speech. Because of their input and output types, we refer to them as speech-to-interpretation (STI) models. Previous works have successfully applied STI models to targeted use cases, such as recognizing home automation commands, however no study has yet addressed how these models generalize to broader use cases. In this work, we analyze the relationship between the performance of STI models and the difficulty of the use case to which they are applied. We introduce empirical measures of dataset semantic complexity to quantify the difficulty of the SLU tasks. We show that near-perfect performance metrics for STI models reported in the literature were obtained with datasets that have low semantic complexity values. We perform experiments where we vary the semantic complexity of a large, proprietary dataset and show that STI model performance correlates with our semantic complexity measures, such that performance increases as complexity values decrease. Our results show that it is important to contextualize an STI model's performance with the complexity values of its training dataset to reveal the scope of its applicability.

</details>

<details>

<summary>2020-08-06 21:28:36 - Efficient Neural Query Auto Completion</summary>

- *Sida Wang, Weiwei Guo, Huiji Gao, Bo Long*

- `2008.02879v1` - [abs](http://arxiv.org/abs/2008.02879v1) - [pdf](http://arxiv.org/pdf/2008.02879v1)

> Query Auto Completion (QAC), as the starting point of information retrieval tasks, is critical to user experience. Generally it has two steps: generating completed query candidates according to query prefixes, and ranking them based on extracted features. Three major challenges are observed for a query auto completion system: (1) QAC has a strict online latency requirement. For each keystroke, results must be returned within tens of milliseconds, which poses a significant challenge in designing sophisticated language models for it. (2) For unseen queries, generated candidates are of poor quality as contextual information is not fully utilized. (3) Traditional QAC systems heavily rely on handcrafted features such as the query candidate frequency in search logs, lacking sufficient semantic understanding of the candidate.   In this paper, we propose an efficient neural QAC system with effective context modeling to overcome these challenges. On the candidate generation side, this system uses as much information as possible in unseen prefixes to generate relevant candidates, increasing the recall by a large margin. On the candidate ranking side, an unnormalized language model is proposed, which effectively captures deep semantics of queries. This approach presents better ranking performance over state-of-the-art neural ranking methods and reduces $\sim$95\% latency compared to neural language modeling methods. The empirical results on public datasets show that our model achieves a good balance between accuracy and efficiency. This system is served in LinkedIn job search with significant product impact observed.

</details>

<details>

<summary>2020-08-06 21:33:44 - Webly Supervised Semantic Embeddings for Large Scale Zero-Shot Learning</summary>

- *Yannick Le Cacheux, Adrian Popescu, Hervé Le Borgne*

- `2008.02880v1` - [abs](http://arxiv.org/abs/2008.02880v1) - [pdf](http://arxiv.org/pdf/2008.02880v1)

> Zero-shot learning (ZSL) makes object recognition in images possible in absence of visual training data for a part of the classes from a dataset. When the number of classes is large, classes are usually represented by semantic class prototypes learned automatically from unannotated text collections. This typically leads to much lower performances than with manually designed semantic prototypes such as attributes. While most ZSL works focus on the visual aspect and reuse standard semantic prototypes learned from generic text collections, we focus on the problem of semantic class prototype design for large scale ZSL. More specifically, we investigate the use of noisy textual metadata associated to photos as text collections, as we hypothesize they are likely to provide more plausible semantic embeddings for visual classes if exploited appropriately. We thus make use of a source-based voting strategy to improve the robustness of semantic prototypes. Evaluation on the large scale ImageNet dataset shows a significant improvement in ZSL performances over two strong baselines, and over usual semantic embeddings used in previous works. We show that this improvement is obtained for several embedding methods, leading to state of the art results when one uses automatically created visual and text features.

</details>

<details>

<summary>2020-08-07 01:24:33 - A Deeper Look at Salient Object Detection: Bi-stream Network with a Small Training Dataset</summary>

- *Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin*

- `2008.02938v1` - [abs](http://arxiv.org/abs/2008.02938v1) - [pdf](http://arxiv.org/pdf/2008.02938v1)

> Compared with the conventional hand-crafted approaches, the deep learning based methods have achieved tremendous performance improvements by training exquisitely crafted fancy networks over large-scale training sets. However, do we really need large-scale training set for salient object detection (SOD)? In this paper, we provide a deeper insight into the interrelationship between the SOD performances and the training sets. To alleviate the conventional demands for large-scale training data, we provide a feasible way to construct a novel small-scale training set, which only contains 4K images. Moreover, we propose a novel bi-stream network to take full advantage of our proposed small training set, which is consisted of two feature backbones with different structures, achieving complementary semantical saliency fusion via the proposed gate control unit. To our best knowledge, this is the first attempt to use a small-scale training set to outperform state-of-the-art models which are trained on large-scale training sets; nevertheless, our method can still achieve the leading state-of-the-art performance on five benchmark datasets.

</details>

<details>

<summary>2020-08-07 01:28:59 - Hierarchical Deep Convolutional Neural Networks for Multi-category Diagnosis of Gastrointestinal Disorders on Histopathological Images</summary>

- *Rasoul Sali, Sodiq Adewole, Lubaina Ehsan, Lee A. Denson, Paul Kelly, Beatrice C. Amadi, Lori Holtz, Syed Asad Ali, Sean R. Moore, Sana Syed, Donald E. Brown*

- `2005.03868v2` - [abs](http://arxiv.org/abs/2005.03868v2) - [pdf](http://arxiv.org/pdf/2005.03868v2)

> Deep convolutional neural networks(CNNs) have been successful for a wide range of computer vision tasks, including image classification. A specific area of the application lies in digital pathology for pattern recognition in the tissue-based diagnosis of gastrointestinal(GI) diseases. This domain can utilize CNNs to translate histopathological images into precise diagnostics. This is challenging since these complex biopsies are heterogeneous and require multiple levels of assessment. This is mainly due to structural similarities in different parts of the GI tract and shared features among different gut diseases. Addressing this problem with a flat model that assumes all classes (parts of the gut and their diseases) are equally difficult to distinguish leads to an inadequate assessment of each class. Since the hierarchical model restricts classification error to each sub-class, it leads to a more informative model than a flat model. In this paper, we propose to apply the hierarchical classification of biopsy images from different parts of the GI tract and the receptive diseases within each. We embedded a class hierarchy into the plain VGGNet to take advantage of its layers' hierarchical structure. The proposed model was evaluated using an independent set of image patches from 373 whole slide images. The results indicate that the hierarchical model can achieve better results than the flat model for multi-category diagnosis of GI disorders using histopathological images.

</details>

<details>

<summary>2020-08-07 01:49:40 - Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair</summary>

- *Haoye Tian, Kui Liu, Abdoul Kader Kaboreé, Anil Koyuncu, Li Li, Jacques Klein, Tegawendé F. Bissyandé*

- `2008.02944v1` - [abs](http://arxiv.org/abs/2008.02944v1) - [pdf](http://arxiv.org/pdf/2008.02944v1)

> A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations to learn deep features that may encode the properties of patch correctness. Our work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in predicting patch correctness on a deduplicated dataset of 1000 labeled patches. Our study shows that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.

</details>

<details>

<summary>2020-08-07 03:46:21 - Role-Based Deception in Enterprise Networks</summary>

- *Iffat Anjum, Mu Zhu, Isaac Polinsky, William Enck, Michael K. Reiter, Munindar Singh*

- `2008.02979v1` - [abs](http://arxiv.org/abs/2008.02979v1) - [pdf](http://arxiv.org/pdf/2008.02979v1)

> Historically, enterprise network reconnaissance is an active process, often involving port scanning. However, as routers and switches become more complex, they also become more susceptible to compromise. From this vantage point, an attacker can passively identify high-value hosts such as the workstations of IT administrators, C-suite executives, and finance personnel. The goal of this paper is to develop a technique to deceive and dissuade such adversaries. We propose HoneyRoles, which uses honey connections to build metaphorical haystacks around the network traffic of client hosts belonging to high-value organizational roles. The honey connections also act as network canaries to signal network compromise, thereby dissuading the adversary from acting on information observed in network flows. We design a prototype implementation of HoneyRoles using an OpenFlow SDN controller and evaluate its security using the PRISM probabilistic model checker. Our performance evaluation shows that HoneyRoles has a small effect on network request completion time and our security analysis demonstrates that once an alert is raised, HoneyRoles can quickly identify the compromised switch with high probability. In doing so, we show that a role-based network deception is a promising approach for defending against adversaries that have compromised network devices.

</details>

<details>

<summary>2020-08-07 07:16:40 - A Context-based Disambiguation Model for Sentiment Concepts Using a Bag-of-concepts Approach</summary>

- *Zeinab Rajabi, MohammadReza Valavi, Maryam Hourali*

- `2008.03020v1` - [abs](http://arxiv.org/abs/2008.03020v1) - [pdf](http://arxiv.org/pdf/2008.03020v1)

> With the widespread dissemination of user-generated content on different social networks, and online consumer systems such as Amazon, the quantity of opinionated information available on the Internet has been increased. One of the main tasks of the sentiment analysis is to detect polarity within a text. The existing polarity detection methods mainly focus on keywords and their naive frequency counts; however, they less regard the meanings and implicit dimensions of the natural concepts. Although background knowledge plays a critical role in determining the polarity of concepts, it has been disregarded in polarity detection methods. This study presents a context-based model to solve ambiguous polarity concepts using commonsense knowledge. First, a model is presented to generate a source of ambiguous sentiment concepts based on SenticNet by computing the probability distribution. Then the model uses a bag-of-concepts approach to remove ambiguities and semantic augmentation with the ConceptNet handling to overcome lost knowledge. ConceptNet is a large-scale semantic network with a large number of commonsense concepts. In this paper, the point mutual information (PMI) measure is used to select the contextual concepts having strong relationships with ambiguous concepts. The polarity of the ambiguous concepts is precisely detected using positive/negative contextual concepts and the relationship of the concepts in the semantic knowledge base. The text representation scheme is semantically enriched using Numberbatch, which is a word embedding model based on the concepts from the ConceptNet semantic network. The proposed model is evaluated by applying a corpus of product reviews, called Semeval. The experimental results revealed an accuracy rate of 82.07%, representing the effectiveness of the proposed model.

</details>

<details>

<summary>2020-08-07 13:16:28 - IMS at SemEval-2020 Task 1: How low can you go? Dimensionality in Lexical Semantic Change Detection</summary>

- *Jens Kaiser, Dominik Schlechtweg, Sean Papay, Sabine Schulte im Walde*

- `2008.03164v1` - [abs](http://arxiv.org/abs/2008.03164v1) - [pdf](http://arxiv.org/pdf/2008.03164v1)

> We present the results of our system for SemEval-2020 Task 1 that exploits a commonly used lexical semantic change detection model based on Skip-Gram with Negative Sampling. Our system focuses on Vector Initialization (VI) alignment, compares VI to the currently top-ranking models for Subtask 2 and demonstrates that these can be outperformed if we optimize VI dimensionality. We demonstrate that differences in performance can largely be attributed to model-specific sources of noise, and we reveal a strong relationship between dimensionality and frequency-induced noise in VI alignment. Our results suggest that lexical semantic change models integrating vector space alignment should pay more attention to the role of the dimensionality parameter.

</details>

<details>

<summary>2020-08-07 17:24:53 - SemEval-2020 Task 10: Emphasis Selection for Written Text in Visual Media</summary>

- *Amirreza Shirani, Franck Dernoncourt, Nedim Lipka, Paul Asente, Jose Echevarria, Thamar Solorio*

- `2008.03274v1` - [abs](http://arxiv.org/abs/2008.03274v1) - [pdf](http://arxiv.org/pdf/2008.03274v1)

> In this paper, we present the main findings and compare the results of SemEval-2020 Task 10, Emphasis Selection for Written Text in Visual Media. The goal of this shared task is to design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in textual content to enable automated design assistance in authoring. The main focus is on short text instances for social media, with a variety of examples, from social media posts to inspirational quotes. Participants were asked to model emphasis using plain text with no additional context from the user or other design considerations. SemEval-2020 Emphasis Selection shared task attracted 197 participants in the early phase and a total of 31 teams made submissions to this task. The highest-ranked submission achieved 0.823 Matchm score. The analysis of systems submitted to the task indicates that BERT and RoBERTa were the most common choice of pre-trained models used, and part of speech tag (POS) was the most useful feature. Full results can be found on the task's website.

</details>

<details>

<summary>2020-08-07 19:38:51 - The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 Challenge</summary>

- *Nicholas Heller, Fabian Isensee, Klaus H. Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin, Miofei Han, Guang Yao, Yaozong Gao, Yao Zhang, Yixin Wang, Feng Hou, Jiawei Yang, Guangwei Xiong, Jiang Tian, Cheng Zhong, Jun Ma, Jack Rickman, Joshua Dean, Bethany Stai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Heather Kaluzniak, Shaneabbas Raza, Joel Rosenberg, Keenan Moore, Edward Walczak, Zachary Rengel, Zach Edgerton, Ranveer Vasdev, Matthew Peterson, Sean McSweeney, Sarah Peterson, Arveen Kalapara, Niranjan Sathianathen, Nikolaos Papanikolopoulos, Christopher Weight*

- `1912.01054v2` - [abs](http://arxiv.org/abs/1912.01054v2) - [pdf](http://arxiv.org/pdf/1912.01054v2)

> There is a large body of literature linking anatomic and geometric characteristics of kidney tumors to perioperative and oncologic outcomes. Semantic segmentation of these tumors and their host kidneys is a promising tool for quantitatively characterizing these lesions, but its adoption is limited due to the manual effort required to produce high-quality 3D segmentations of these structures. Recently, methods based on deep learning have shown excellent results in automatic 3D segmentation, but they require large datasets for training, and there remains little consensus on which methods perform best. The 2019 Kidney and Kidney Tumor Segmentation challenge (KiTS19) was a competition held in conjunction with the 2019 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) which sought to address these issues and stimulate progress on this automatic segmentation problem. A training set of 210 cross sectional CT images with kidney tumors was publicly released with corresponding semantic segmentation masks. 106 teams from five continents used this data to develop automated systems to predict the true segmentation masks on a test set of 90 CT images for which the corresponding ground truth segmentations were kept private. These predictions were scored and ranked according to their average So rensen-Dice coefficient between the kidney and tumor across all 90 cases. The winning team achieved a Dice of 0.974 for kidney and 0.851 for tumor, approaching the inter-annotator performance on kidney (0.983) but falling short on tumor (0.923). This challenge has now entered an "open leaderboard" phase where it serves as a challenging benchmark in 3D semantic segmentation.

</details>

<details>

<summary>2020-08-07 22:25:36 - Diversifying Task-oriented Dialogue Response Generation with Prototype Guided Paraphrasing</summary>

- *Phillip Lippe, Pengjie Ren, Hinda Haned, Bart Voorn, Maarten de Rijke*

- `2008.03391v1` - [abs](http://arxiv.org/abs/2008.03391v1) - [pdf](http://arxiv.org/pdf/2008.03391v1)

> Existing methods for Dialogue Response Generation (DRG) in Task-oriented Dialogue Systems (TDSs) can be grouped into two categories: template-based and corpus-based. The former prepare a collection of response templates in advance and fill the slots with system actions to produce system responses at runtime. The latter generate system responses token by token by taking system actions into account. While template-based DRG provides high precision and highly predictable responses, they usually lack in terms of generating diverse and natural responses when compared to (neural) corpus-based approaches. Conversely, while corpus-based DRG methods are able to generate natural responses, we cannot guarantee their precision or predictability. Moreover, the diversity of responses produced by today's corpus-based DRG methods is still limited. We propose to combine the merits of template-based and corpus-based DRGs by introducing a prototype-based, paraphrasing neural network, called P2-Net, which aims to enhance quality of the responses in terms of both precision and diversity. Instead of generating a response from scratch, P2-Net generates system responses by paraphrasing template-based responses. To guarantee the precision of responses, P2-Net learns to separate a response into its semantics, context influence, and paraphrasing noise, and to keep the semantics unchanged during paraphrasing. To introduce diversity, P2-Net randomly samples previous conversational utterances as prototypes, from which the model can then extract speaking style information. We conduct extensive experiments on the MultiWOZ dataset with both automatic and human evaluations. The results show that P2-Net achieves a significant improvement in diversity while preserving the semantics of responses.

</details>

<details>

<summary>2020-08-08 07:43:49 - Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation</summary>

- *Won Ik Cho, Donghyun Kwak, Ji Won Yoon, Nam Soo Kim*

- `2005.08213v2` - [abs](http://arxiv.org/abs/2005.08213v2) - [pdf](http://arxiv.org/pdf/2005.08213v2)

> Speech is one of the most effective means of communication and is full of information that helps the transmission of utterer's thoughts. However, mainly due to the cumbersome processing of acoustic features, phoneme or word posterior probability has frequently been discarded in understanding the natural language. Thus, some recent spoken language understanding (SLU) modules have utilized end-to-end structures that preserve the uncertainty information. This further reduces the propagation of speech recognition error and guarantees computational efficiency. We claim that in this process, the speech comprehension can benefit from the inference of massive pre-trained language models (LMs). We transfer the knowledge from a concrete Transformer-based text LM to an SLU module which can face a data shortage, based on recent cross-modal distillation methodologies. We demonstrate the validity of our proposal upon the performance on Fluent Speech Command, an English SLU benchmark. Thereby, we experimentally verify our hypothesis that the knowledge could be shared from the top layer of the LM to a fully speech-based module, in which the abstracted speech is expected to meet the semantic representation.

</details>

<details>

<summary>2020-08-08 07:53:44 - An ASP approach for reasoning in a concept-aware multipreferential lightweight DL</summary>

- *Laura Giordano, Daniele Theseider Dupré*

- `2006.04387v2` - [abs](http://arxiv.org/abs/2006.04387v2) - [pdf](http://arxiv.org/pdf/2006.04387v2)

> In this paper we develop a concept aware multi-preferential semantics for dealing with typicality in description logics, where preferences are associated with concepts, starting from a collection of ranked TBoxes containing defeasible concept inclusions. Preferences are combined to define a preferential interpretation in which defeasible inclusions can be evaluated. The construction of the concept-aware multipreference semantics is related to Brewka's framework for qualitative preferences. We exploit Answer Set Programming (in particular, asprin) to achieve defeasible reasoning under the multipreference approach for the lightweight description logic EL+bot.   The paper is under consideration for acceptance in TPLP.

</details>

<details>

<summary>2020-08-08 09:27:48 - Bidirectional Mapping Generative Adversarial Networks for Brain MR to PET Synthesis</summary>

- *Shengye Hu, Baiying Lei, Yong Wang, Zhiguang Feng, Yanyan Shen, Shuqiang Wang*

- `2008.03483v1` - [abs](http://arxiv.org/abs/2008.03483v1) - [pdf](http://arxiv.org/pdf/2008.03483v1)

> Fusing multi-modality medical images, such as MR and PET, can provide various anatomical or functional information about human body. But PET data is always unavailable due to different reasons such as cost, radiation, or other limitations. In this paper, we propose a 3D end-to-end synthesis network, called Bidirectional Mapping Generative Adversarial Networks (BMGAN), where image contexts and latent vector are effectively used and jointly optimized for brain MR-to-PET synthesis. Concretely, a bidirectional mapping mechanism is designed to embed the semantic information of PET images into the high dimensional latent space. And the 3D DenseU-Net generator architecture and the extensive objective functions are further utilized to improve the visual quality of synthetic results. The most appealing part is that the proposed method can synthesize the perceptually realistic PET images while preserving the diverse brain structures of different subjects. Experimental results demonstrate that the performance of the proposed method outperforms other competitive cross-modality synthesis methods in terms of quantitative measures, qualitative displays, and classification evaluation.

</details>

<details>

<summary>2020-08-08 14:48:44 - Towards Metric Temporal Answer Set Programming</summary>

- *Pedro Cabalar, Martin Dieguez, Torsten Schaub, Anna Schuhmann*

- `2008.02038v2` - [abs](http://arxiv.org/abs/2008.02038v2) - [pdf](http://arxiv.org/pdf/2008.02038v2)

> We elaborate upon the theoretical foundations of a metric temporal extension of Answer Set Programming. In analogy to previous extensions of ASP with constructs from Linear Temporal and Dynamic Logic, we accomplish this in the setting of the logic of Here-and-There and its non-monotonic extension, called Equilibrium Logic. More precisely, we develop our logic on the same semantic underpinnings as its predecessors and thus use a simple time domain of bounded time steps. This allows us to compare all variants in a uniform framework and ultimately combine them in a common implementation.

</details>

<details>

<summary>2020-08-09 00:40:54 - Consumer UAV Cybersecurity Vulnerability Assessment Using Fuzzing Tests</summary>

- *David Rudo, Kai Zeng*

- `2008.03621v1` - [abs](http://arxiv.org/abs/2008.03621v1) - [pdf](http://arxiv.org/pdf/2008.03621v1)

> Unmanned Aerial Vehicles (UAVs) are remote-controlled vehicles capable of flight and are present in a variety of environments from military operations to domestic enjoyment. These vehicles are great assets, but just as their pilot can control them remotely, cyberattacks can be executed in a similar manner. Cyber attacks on UAVs can bring a plethora of issues to physical and virtual systems. Such malfunctions are capable of giving an attacker the ability to steal data, incapacitate the UAV, or hijack the UAV. To mitigate such attacks, it is necessary to identify and patch vulnerabilities that may be maliciously exploited. In this paper, a new UAV vulnerability is explored with related UAV security practices identified for possible exploitation using large streams of data sent at specific ports. The more in-depth model involves strings of data involving FTP-specific keywords sent to the UAV's FTP port in the form of a fuzzing test and launching thousands of packets at other ports on the UAV as well. During these tests, virtual and physical systems are monitored extensively to identify specific patterns and vulnerabilities. This model is applied to a Parrot Bebop 2, which accurately portrays a UAV that had their network compromised by an attacker and portrays many lower-end UAV models for domestic use. During testings, the Parrot Bebop 2 is monitored for degradation in GPS performance, video speed, the UAV's reactivity to the pilot, motor function, and the accuracy of the UAV's sensor data. All these points of monitoring give a comprehensive view of the UAV's reaction to each individual test. In this paper, countermeasures to combat the exploitation of this vulnerability will be discussed as well as possible attacks that can branch from the fuzzing tests.

</details>

<details>

<summary>2020-08-09 21:25:15 - Micro-Batch Training with Batch-Channel Normalization and Weight Standardization</summary>

- *Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille*

- `1903.10520v2` - [abs](http://arxiv.org/abs/1903.10520v2) - [pdf](http://arxiv.org/pdf/1903.10520v2)

> Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only 1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients; BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.

</details>

<details>

<summary>2020-08-10 01:30:31 - Quran Intelligent Ontology Construction Approach Using Association Rules Mining</summary>

- *Fouzi Harrag, Abdullah Al-Nasser, Abdullah Al-Musnad, Rayan Al-Shaya*

- `2008.03232v2` - [abs](http://arxiv.org/abs/2008.03232v2) - [pdf](http://arxiv.org/pdf/2008.03232v2)

> Ontology can be seen as a formal representation of knowledge. They have been investigated in many artificial intelligence studies including semantic web, software engineering, and information retrieval. The aim of ontology is to develop knowledge representations that can be shared and reused. This research project is concerned with the use of association rules to extract the Quran ontology. The manual acquisition of ontologies from Quran verses can be very costly; therefore, we need an intelligent system for Quran ontology construction using patternbased schemes and associations rules to discover Quran concepts and semantics relations from Quran verses. Our system is based on the combination of statistics and linguistics methods to extract concepts and conceptual relations from Quran. In particular, a linguistic pattern-based approach is exploited to extract specific concepts from the Quran, while the conceptual relations are found based on association rules technique. The Quran ontology will offer a new and powerful representation of Quran knowledge, and the association rules will help to represent the relations between all classes of connected concepts in the Quran ontology.

</details>

<details>

<summary>2020-08-10 04:32:32 - IF-Net: An Illumination-invariant Feature Network</summary>

- *Po-Heng Chen, Zhao-Xu Luo, Zu-Kuan Huang, Chun Yang, Kuan-Wen Chen*

- `2008.03897v1` - [abs](http://arxiv.org/abs/2008.03897v1) - [pdf](http://arxiv.org/pdf/2008.03897v1)

> Feature descriptor matching is a critical step is many computer vision applications such as image stitching, image retrieval and visual localization. However, it is often affected by many practical factors which will degrade its performance. Among these factors, illumination variations are the most influential one, and especially no previous descriptor learning works focus on dealing with this problem. In this paper, we propose IF-Net, aimed to generate a robust and generic descriptor under crucial illumination changes conditions. We find out not only the kind of training data important but also the order it is presented. To this end, we investigate several dataset scheduling methods and propose a separation training scheme to improve the matching accuracy. Further, we propose a ROI loss and hard-positive mining strategy along with the training scheme, which can strengthen the ability of generated descriptor dealing with large illumination change conditions. We evaluate our approach on public patch matching benchmark and achieve the best results compared with several state-of-the-arts methods. To show the practicality, we further evaluate IF-Net on the task of visual localization under large illumination changes scenes, and achieves the best localization accuracy.

</details>

<details>

<summary>2020-08-10 07:12:43 - Rethinking of the Image Salient Object Detection: Object-level Semantic Saliency Re-ranking First, Pixel-wise Saliency Refinement Latter</summary>

- *Zhenyu Wu, Shuai Li, Chenglizhao Chen, Aimin Hao, Hong Qin*

- `2008.05397v1` - [abs](http://arxiv.org/abs/2008.05397v1) - [pdf](http://arxiv.org/pdf/2008.05397v1)

> The real human attention is an interactive activity between our visual system and our brain, using both low-level visual stimulus and high-level semantic information. Previous image salient object detection (SOD) works conduct their saliency predictions in a multi-task manner, i.e., performing pixel-wise saliency regression and segmentation-like saliency refinement at the same time, which degenerates their feature backbones in revealing semantic information. However, given an image, we tend to pay more attention to those regions which are semantically salient even in the case that these regions are perceptually not the most salient ones at first glance. In this paper, we divide the SOD problem into two sequential tasks: 1) we propose a lightweight, weakly supervised deep network to coarsely locate those semantically salient regions first; 2) then, as a post-processing procedure, we selectively fuse multiple off-the-shelf deep models on these semantically salient regions as the pixel-wise saliency refinement. In sharp contrast to the state-of-the-art (SOTA) methods that focus on learning pixel-wise saliency in "single image" using perceptual clues mainly, our method has investigated the "object-level semantic ranks between multiple images", of which the methodology is more consistent with the real human attention mechanism. Our method is simple yet effective, which is the first attempt to consider the salient object detection mainly as an object-level semantic re-ranking problem.

</details>

<details>

<summary>2020-08-10 10:20:49 - ASP(AC): Answer Set Programming with Algebraic Constraints</summary>

- *Thomas Eiter, Rafael Kiesel*

- `2008.04008v1` - [abs](http://arxiv.org/abs/2008.04008v1) - [pdf](http://arxiv.org/pdf/2008.04008v1)

> Weighted Logic is a powerful tool for the specification of calculations over semirings that depend on qualitative information. Using a novel combination of Weighted Logic and Here-and-There (HT) Logic, in which this dependence is based on intuitionistic grounds, we introduce Answer Set Programming with Algebraic Constraints (ASP(AC)), where rules may contain constraints that compare semiring values to weighted formula evaluations. Such constraints provide streamlined access to a manifold of constructs available in ASP, like aggregates, choice constraints, and arithmetic operators. They extend some of them and provide a generic framework for defining programs with algebraic computation, which can be fruitfully used e.g. for provenance semantics of datalog programs. While undecidable in general, expressive fragments of ASP(AC) can be exploited for effective problem-solving in a rich framework. This work is under consideration for acceptance in Theory and Practice of Logic Programming.

</details>

<details>

<summary>2020-08-10 12:16:44 - Driving among Flatmobiles: Bird-Eye-View occupancy grids from a monocular camera for holistic trajectory planning</summary>

- *Abdelhak Loukkal, Yves Grandvalet, Tom Drummond, You Li*

- `2008.04047v1` - [abs](http://arxiv.org/abs/2008.04047v1) - [pdf](http://arxiv.org/pdf/2008.04047v1)

> Camera-based end-to-end driving neural networks bring the promise of a low-cost system that maps camera images to driving control commands. These networks are appealing because they replace laborious hand engineered building blocks but their black-box nature makes them difficult to delve in case of failure. Recent works have shown the importance of using an explicit intermediate representation that has the benefits of increasing both the interpretability and the accuracy of networks' decisions. Nonetheless, these camera-based networks reason in camera view where scale is not homogeneous and hence not directly suitable for motion forecasting. In this paper, we introduce a novel monocular camera-only holistic end-to-end trajectory planning network with a Bird-Eye-View (BEV) intermediate representation that comes in the form of binary Occupancy Grid Maps (OGMs). To ease the prediction of OGMs in BEV from camera images, we introduce a novel scheme where the OGMs are first predicted as semantic masks in camera view and then warped in BEV using the homography between the two planes. The key element allowing this transformation to be applied to 3D objects such as vehicles, consists in predicting solely their footprint in camera-view, hence respecting the flat world hypothesis implied by the homography.

</details>

<details>

<summary>2020-08-11 02:12:23 - Fingerprinting the Fingerprinters: Learning to Detect Browser Fingerprinting Behaviors</summary>

- *Umar Iqbal, Steven Englehardt, Zubair Shafiq*

- `2008.04480v1` - [abs](http://arxiv.org/abs/2008.04480v1) - [pdf](http://arxiv.org/pdf/2008.04480v1)

> Browser fingerprinting is an invasive and opaque stateless tracking technique. Browser vendors, academics, and standards bodies have long struggled to provide meaningful protections against browser fingerprinting that are both accurate and do not degrade user experience. We propose FP-Inspector, a machine learning based syntactic-semantic approach to accurately detect browser fingerprinting. We show that FP-Inspector performs well, allowing us to detect 26% more fingerprinting scripts than the state-of-the-art. We show that an API-level fingerprinting countermeasure, built upon FP-Inspector, helps reduce website breakage by a factor of 2. We use FP-Inspector to perform a measurement study of browser fingerprinting on top-100K websites. We find that browser fingerprinting is now present on more than 10% of the top-100K websites and over a quarter of the top-10K websites. We also discover previously unreported uses of JavaScript APIs by fingerprinting scripts suggesting that they are looking to exploit APIs in new and unexpected ways.

</details>

<details>

<summary>2020-08-11 05:22:11 - Localizing Patch Points From One Exploit</summary>

- *Shiqi Shen, Aashish Kolluri, Zhen Dong, Prateek Saxena, Abhik Roychoudhury*

- `2008.04516v1` - [abs](http://arxiv.org/abs/2008.04516v1) - [pdf](http://arxiv.org/pdf/2008.04516v1)

> Automatic patch generation can significantly reduce the window of exposure after a vulnerability is disclosed. Towards this goal, a long-standing problem has been that of patch localization: to find a program point at which a patch can be synthesized. We present PatchLoc, one of the first systems which automatically identifies such a location in a vulnerable binary, given just one exploit, with high accuracy. PatchLoc does not make any assumptions about the availability of source code, test suites, or specialized knowledge of the vulnerability. PatchLoc pinpoints valid patch locations in large real-world applications with high accuracy for about 88% of 43 CVEs we study. These results stem from a novel approach to automatically synthesizing a test-suite which enables probabilistically ranking and effectively differentiating between candidate program patch locations.

</details>

<details>

<summary>2020-08-11 11:02:22 - Semantic-based End-to-End Learning for Typhoon Intensity Prediction</summary>

- *Hamada M. Zahera, Mohamed Ahmed Sherif, Axel Ngonga*

- `2003.13779v2` - [abs](http://arxiv.org/abs/2003.13779v2) - [pdf](http://arxiv.org/pdf/2003.13779v2)

> Disaster prediction is one of the most critical tasks towards disaster surveillance and preparedness. Existing technologies employ different machine learning approaches to predict incoming disasters from historical environmental data. However, for short-term disasters (e.g., earthquakes), historical data alone has a limited prediction capability. Therefore, additional sources of warnings are required for accurate prediction. We consider social media as a supplementary source of knowledge in addition to historical environmental data. However, social media posts (e.g., tweets) is very informal and contains only limited content. To alleviate these limitations, we propose the combination of semantically-enriched word embedding models to represent entities in tweets with their semantic representations computed with the traditionalword2vec. Moreover, we study how the correlation between social media posts and typhoons magnitudes (also called intensities)-in terms of volume and sentiments of tweets-. Based on these insights, we propose an end-to-end based framework that learns from disaster-related tweets and environmental data to improve typhoon intensity prediction. This paper is an extension of our work originally published in K-CAP 2019 [32]. We extended this paper by building our framework with state-of-the-art deep neural models, up-dated our dataset with new typhoons and their tweets to-date and benchmark our approach against recent baselines in disaster prediction. Our experimental results show that our approach outperforms the accuracy of the state-of-the-art baselines in terms of F1-score with (CNN by12.1%and BiLSTM by3.1%) improvement compared with last experiments

</details>

<details>

<summary>2020-08-11 12:03:01 - Learning to Cluster under Domain Shift</summary>

- *Willi Menapace, Stéphane Lathuilière, Elisa Ricci*

- `2008.04646v1` - [abs](http://arxiv.org/abs/2008.04646v1) - [pdf](http://arxiv.org/pdf/2008.04646v1)

> While unsupervised domain adaptation methods based on deep architectures have achieved remarkable success in many computer vision tasks, they rely on a strong assumption, i.e. labeled source data must be available. In this work we overcome this assumption and we address the problem of transferring knowledge from a source to a target domain when both source and target data have no annotations. Inspired by recent works on deep clustering, our approach leverages information from data gathered from multiple source domains to build a domain-agnostic clustering model which is then refined at inference time when target data become available. Specifically, at training time we propose to optimize a novel information-theoretic loss which, coupled with domain-alignment layers, ensures that our model learns to correctly discover semantic labels while discarding domain-specific features. Importantly, our architecture design ensures that at inference time the resulting source model can be effectively adapted to the target domain without having access to source data, thanks to feature alignment and self-supervision. We evaluate the proposed approach in a variety of settings, considering several domain adaptation benchmarks and we show that our method is able to automatically discover relevant semantic information even in presence of few target samples and yields state-of-the-art results on multiple domain adaptation benchmarks.

</details>

<details>

<summary>2020-08-11 12:26:00 - S2OSC: A Holistic Semi-Supervised Approach for Open Set Classification</summary>

- *Yang Yang, Zhen-Qiang Sun, Hui Xiong, Jian Yang*

- `2008.04662v1` - [abs](http://arxiv.org/abs/2008.04662v1) - [pdf](http://arxiv.org/pdf/2008.04662v1)

> Open set classification (OSC) tackles the problem of determining whether the data are in-class or out-of-class during inference, when only provided with a set of in-class examples at training time. Traditional OSC methods usually train discriminative or generative models with in-class data, then utilize the pre-trained models to classify test data directly. However, these methods always suffer from embedding confusion problem, i.e., partial out-of-class instances are mixed with in-class ones of similar semantics, making it difficult to classify. To solve this problem, we unify semi-supervised learning to develop a novel OSC algorithm, S2OSC, that incorporates out-of-class instances filtering and model re-training in a transductive manner. In detail, given a pool of newly coming test data, S2OSC firstly filters distinct out-of-class instances using the pre-trained model, and annotates super-class for them. Then, S2OSC trains a holistic classification model by combing in-class and out-of-class labeled data and remaining unlabeled test data in semi-supervised paradigm, which also integrates pre-trained model for knowledge distillation to further separate mixed instances. Despite its simplicity, the experimental results show that S2OSC achieves state-of-the-art performance across a variety of OSC tasks, including 85.4% of F1 on CIFAR-10 with only 300 pseudo-labels. We also demonstrate how S2OSC can be expanded to incremental OSC setting effectively with streaming data.

</details>

<details>

<summary>2020-08-11 13:54:11 - A Neural Generative Model for Joint Learning Topics and Topic-Specific Word Embeddings</summary>

- *Lixing Zhu, Yulan He, Deyu Zhou*

- `2008.04702v1` - [abs](http://arxiv.org/abs/2008.04702v1) - [pdf](http://arxiv.org/pdf/2008.04702v1)

> We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.

</details>

<details>

<summary>2020-08-11 15:00:41 - Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation in Autonomous Driving</summary>

- *Xiaofeng Liu, Yimeng Zhang, Xiongchang Liu, Song Bai, Site Li, Jane You*

- `2008.04751v1` - [abs](http://arxiv.org/abs/2008.04751v1) - [pdf](http://arxiv.org/pdf/2008.04751v1)

> Semantic segmentation is important for many real-world systems, e.g., autonomous vehicles, which predict the class of each pixel. Recently, deep networks achieved significant progress w.r.t. the mean Intersection-over Union (mIoU) with the cross-entropy loss. However, the cross-entropy loss can essentially ignore the difference of severity for an autonomous car with different wrong prediction mistakes. For example, predicting the car to the road is much more servery than recognize it as the bus. Targeting for this difficulty, we develop a Wasserstein training framework to explore the inter-class correlation by defining its ground metric as misclassification severity. The ground metric of Wasserstein distance can be pre-defined following the experience on a specific task. From the optimization perspective, we further propose to set the ground metric as an increasing function of the pre-defined ground metric. Furthermore, an adaptively learning scheme of the ground matrix is proposed to utilize the high-fidelity CARLA simulator. Specifically, we follow a reinforcement alternative learning scheme. The experiments on both CamVid and Cityscapes datasets evidenced the effectiveness of our Wasserstein loss. The SegNet, ENet, FCN and Deeplab networks can be adapted following a plug-in manner. We achieve significant improvements on the predefined important classes, and much longer continuous playtime in our simulator.

</details>

<details>

<summary>2020-08-11 19:10:32 - Campus3D: A Photogrammetry Point Cloud Benchmark for Hierarchical Understanding of Outdoor Scene</summary>

- *Xinke Li, Chongshou Li, Zekun Tong, Andrew Lim, Junsong Yuan, Yuwei Wu, Jing Tang, Raymond Huang*

- `2008.04968v1` - [abs](http://arxiv.org/abs/2008.04968v1) - [pdf](http://arxiv.org/pdf/2008.04968v1)

> Learning on 3D scene-based point cloud has received extensive attention as its promising application in many fields, and well-annotated and multisource datasets can catalyze the development of those data-driven approaches. To facilitate the research of this area, we present a richly-annotated 3D point cloud dataset for multiple outdoor scene understanding tasks and also an effective learning framework for its hierarchical segmentation task. The dataset was generated via the photogrammetric processing on unmanned aerial vehicle (UAV) images of the National University of Singapore (NUS) campus, and has been point-wisely annotated with both hierarchical and instance-based labels. Based on it, we formulate a hierarchical learning problem for 3D point cloud segmentation and propose a measurement evaluating consistency across various hierarchies. To solve this problem, a two-stage method including multi-task (MT) learning and hierarchical ensemble (HE) with consistency consideration is proposed. Experimental results demonstrate the superiority of the proposed method and potential advantages of our hierarchical annotations. In addition, we benchmark results of semantic and instance segmentation, which is accessible online at https://3d.dataset.site with the dataset and all source codes.

</details>

<details>

<summary>2020-08-11 22:04:11 - Dual Convolutional Neural Networks for Breast Mass Segmentation and Diagnosis in Mammography</summary>

- *Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies, David Laurenson*

- `2008.02957v2` - [abs](http://arxiv.org/abs/2008.02957v2) - [pdf](http://arxiv.org/pdf/2008.02957v2)

> Deep convolutional neural networks (CNNs) have emerged as a new paradigm for Mammogram diagnosis. Contemporary CNN-based computer-aided-diagnosis (CAD) for breast cancer directly extract latent features from input mammogram image and ignore the importance of morphological features. In this paper, we introduce a novel deep learning framework for mammogram image processing, which computes mass segmentation and simultaneously predict diagnosis results. Specifically, our method is constructed in a dual-path architecture that solves the mapping in a dual-problem manner, with an additional consideration of important shape and boundary knowledge. One path called the Locality Preserving Learner (LPL), is devoted to hierarchically extracting and exploiting intrinsic features of the input. Whereas the other path, called the Conditional Graph Learner (CGL) focuses on generating geometrical features via modeling pixel-wise image to mask correlations. By integrating the two learners, both the semantics and structure are well preserved and the component learning paths in return complement each other, contributing an improvement to the mass segmentation and cancer classification problem at the same time. We evaluated our method on two most used public mammography datasets, DDSM and INbreast. Experimental results show that DualCoreNet achieves the best mammography segmentation and classification simultaneously, outperforming recent state-of-the-art models.

</details>

<details>

<summary>2020-08-11 22:08:27 - Changes, States, and Events: The Thread from Staticity to Dynamism in the Conceptual Modeling of Systems</summary>

- *Sabah Al-Fedaghi*

- `2008.05017v1` - [abs](http://arxiv.org/abs/2008.05017v1) - [pdf](http://arxiv.org/pdf/2008.05017v1)

> This paper examines the concept of change in conceptual modeling. Change is inherent in the nature of things and has increasingly become a focus of much interest and investigation. Change can be modeled as a transition between two states of a finite state machine (FSM). This change represents an exploratory starting point in this paper. Accordingly, a sample FSM that models a car s transmission system is re-expressed in terms of a new modeling methodology called thinging machine (TM) modeling. Recasting the car-transmission model involves developing (1) an S model that captures the static aspects, (2) a D model that identifies states, and (3) a B model that specifies the behavior. The analysis progresses as follows. - S represents an atemporal diagrammatic description that embeds underlying compositions (static changes) from which the roots of system behavior can be traced. - S is broken down into multiple subsystems that correspond to static states (ordered constitutive components). - Introducing time into static states converts these states into events, and the behavior (B) model is constructed based on the chronology of these events. The analysis shows that FSM states are static (atemporal) changes that introduce temporal events as carriers of behavior. This result enhances the semantics of the concepts of change, states, and events in modeling and shows how to specify a system s behavior through its static description.

</details>

<details>

<summary>2020-08-11 23:32:56 - Pretrained Semantic Speech Embeddings for End-to-End Spoken Language Understanding via Cross-Modal Teacher-Student Learning</summary>

- *Pavel Denisov, Ngoc Thang Vu*

- `2007.01836v2` - [abs](http://arxiv.org/abs/2007.01836v2) - [pdf](http://arxiv.org/pdf/2007.01836v2)

> Spoken language understanding is typically based on pipeline architectures including speech recognition and natural language understanding steps. These components are optimized independently to allow usage of available data, but the overall system suffers from error propagation. In this paper, we propose a novel training method that enables pretrained contextual embeddings to process acoustic features. In particular, we extend it with an encoder of pretrained speech recognition systems in order to construct end-to-end spoken language understanding systems. Our proposed method is based on the teacher-student framework across speech and text modalities that aligns the acoustic and the semantic latent spaces. Experimental results in three benchmarks show that our system reaches the performance comparable to the pipeline architecture without using any training data and outperforms it after fine-tuning with ten examples per class on two out of three benchmarks.

</details>

<details>

<summary>2020-08-12 09:54:17 - Pixel-level Corrosion Detection on Metal Constructions by Fusion of Deep Learning Semantic and Contour Segmentation</summary>

- *Iason Katsamenis, Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis, Athanasios Voulodimos*

- `2008.05204v1` - [abs](http://arxiv.org/abs/2008.05204v1) - [pdf](http://arxiv.org/pdf/2008.05204v1)

> Corrosion detection on metal constructions is a major challenge in civil engineering for quick, safe and effective inspection. Existing image analysis approaches tend to place bounding boxes around the defected region which is not adequate both for structural analysis and pre-fabrication, an innovative construction concept which reduces maintenance cost, time and improves safety. In this paper, we apply three semantic segmentation-oriented deep learning models (FCN, U-Net and Mask R-CNN) for corrosion detection, which perform better in terms of accuracy and time and require a smaller number of annotated samples compared to other deep models, e.g. CNN. However, the final images derived are still not sufficiently accurate for structural analysis and pre-fabrication. Thus, we adopt a novel data projection scheme that fuses the results of color segmentation, yielding accurate but over-segmented contours of a region, with a processed area of the deep masks, resulting in high-confidence corroded pixels.

</details>

<details>

<summary>2020-08-12 11:21:23 - Rule-based Anomaly Detection for Railway Signalling Networks</summary>

- *Markus Heinrich, Arwed Gölz, Tolga Arul, Stefan Katzenbeisser*

- `2008.05241v1` - [abs](http://arxiv.org/abs/2008.05241v1) - [pdf](http://arxiv.org/pdf/2008.05241v1)

> We propose a rule-based anomaly detection system for railway signalling that mitigates attacks by a Dolev-Yao attacker who is able to inject control commands and to perform semantic attacks. The system as well mitigates the effects of a compromised signal box that an attacker uses to issue licit but mistimed control messages. We consider an attacker that could cause train derailments and collisions, if our countermeasure is not employed. We apply safety principles of railway operation to a distributed anomaly detection system that inspects incoming commands on the signals and points. The proposed anomaly detection system detects all attacks of our model without producing false positives, while it requires only a small amount of overhead in terms of network communication and latency compared to normal train operation.

</details>

<details>

<summary>2020-08-12 11:44:01 - Learning to Learn from Mistakes: Robust Optimization for Adversarial Noise</summary>

- *Alex Serban, Erik Poll, Joost Visser*

- `2008.05247v1` - [abs](http://arxiv.org/abs/2008.05247v1) - [pdf](http://arxiv.org/pdf/2008.05247v1)

> Sensitivity to adversarial noise hinders deployment of machine learning algorithms in security-critical applications. Although many adversarial defenses have been proposed, robustness to adversarial noise remains an open problem. The most compelling defense, adversarial training, requires a substantial increase in processing time and it has been shown to overfit on the training data. In this paper, we aim to overcome these limitations by training robust models in low data regimes and transfer adversarial knowledge between different models. We train a meta-optimizer which learns to robustly optimize a model using adversarial examples and is able to transfer the knowledge learned to new models, without the need to generate new adversarial examples. Experimental results show the meta-optimizer is consistent across different architectures and data sets, suggesting it is possible to automatically patch adversarial vulnerabilities.

</details>

<details>

<summary>2020-08-12 12:08:25 - Guided Collaborative Training for Pixel-wise Semi-Supervised Learning</summary>

- *Zhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, Rynson W. H. Lau*

- `2008.05258v1` - [abs](http://arxiv.org/abs/2008.05258v1) - [pdf](http://arxiv.org/pdf/2008.05258v1)

> We investigate the generalization of semi-supervised learning (SSL) to diverse pixel-wise tasks. Although SSL methods have achieved impressive results in image classification, the performances of applying them to pixel-wise tasks are unsatisfactory due to their need for dense outputs. In addition, existing pixel-wise SSL approaches are only suitable for certain tasks as they usually require to use task-specific properties. In this paper, we present a new SSL framework, named Guided Collaborative Training (GCT), for pixel-wise tasks, with two main technical contributions. First, GCT addresses the issues caused by the dense outputs through a novel flaw detector. Second, the modules in GCT learn from unlabeled data collaboratively through two newly proposed constraints that are independent of task-specific properties. As a result, GCT can be applied to a wide range of pixel-wise tasks without structural adaptation. Our extensive experiments on four challenging vision tasks, including semantic segmentation, real image denoising, portrait image matting, and night image enhancement, show that GCT outperforms state-of-the-art SSL methods by a large margin. Our code available at: https://github.com/ZHKKKe/PixelSSL.

</details>

<details>

<summary>2020-08-12 13:02:48 - Text Classification based on Multi-granularity Attention Hybrid Neural Network</summary>

- *Zhenyu Liu, Chaohong Lu, Haiwei Huang, Shengfei Lyu, Zhenchao Tao*

- `2008.05282v1` - [abs](http://arxiv.org/abs/2008.05282v1) - [pdf](http://arxiv.org/pdf/2008.05282v1)

> Neural network-based approaches have become the driven forces for Natural Language Processing (NLP) tasks. Conventionally, there are two mainstream neural architectures for NLP tasks: the recurrent neural network (RNN) and the convolution neural network (ConvNet). RNNs are good at modeling long-term dependencies over input texts, but preclude parallel computation. ConvNets do not have memory capability and it has to model sequential data as un-ordered features. Therefore, ConvNets fail to learn sequential dependencies over the input texts, but it is able to carry out high-efficient parallel computation. As each neural architecture, such as RNN and ConvNets, has its own pro and con, integration of different architectures is assumed to be able to enrich the semantic representation of texts, thus enhance the performance of NLP tasks. However, few investigation explores the reconciliation of these seemingly incompatible architectures. To address this issue, we propose a hybrid architecture based on a novel hierarchical multi-granularity attention mechanism, named Multi-granularity Attention-based Hybrid Neural Network (MahNN). The attention mechanism is to assign different weights to different parts of the input sequence to increase the computation efficiency and performance of neural models. In MahNN, two types of attentions are introduced: the syntactical attention and the semantical attention. The syntactical attention computes the importance of the syntactic elements (such as words or sentence) at the lower symbolic level and the semantical attention is used to compute the importance of the embedded space dimension corresponding to the upper latent semantics. We adopt the text classification as an exemplifying way to illustrate the ability of MahNN to understand texts.

</details>

<details>

<summary>2020-08-12 15:29:11 - Improving the Performance of Fine-Grain Image Classifiers via Generative Data Augmentation</summary>

- *Shashank Manjunath, Aitzaz Nathaniel, Jeff Druce, Stan German*

- `2008.05381v1` - [abs](http://arxiv.org/abs/2008.05381v1) - [pdf](http://arxiv.org/pdf/2008.05381v1)

> Recent advances in machine learning (ML) and computer vision tools have enabled applications in a wide variety of arenas such as financial analytics, medical diagnostics, and even within the Department of Defense. However, their widespread implementation in real-world use cases poses several challenges: (1) many applications are highly specialized, and hence operate in a \emph{sparse data} domain; (2) ML tools are sensitive to their training sets and typically require cumbersome, labor-intensive data collection and data labelling processes; and (3) ML tools can be extremely "black box," offering users little to no insight into the decision-making process or how new data might affect prediction performance. To address these challenges, we have designed and developed Data Augmentation from Proficient Pre-Training of Robust Generative Adversarial Networks (DAPPER GAN), an ML analytics support tool that automatically generates novel views of training images in order to improve downstream classifier performance. DAPPER GAN leverages high-fidelity embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to create novel imagery for previously unseen classes. We experimentally evaluate this technique on the Stanford Cars dataset, demonstrating improved vehicle make and model classification accuracy and reduced requirements for real data using our GAN based data augmentation framework. The method's validity was supported through an analysis of classifier performance on both augmented and non-augmented datasets, achieving comparable or better accuracy with up to 30\% less real data across visually similar classes. To support this method, we developed a novel augmentation method that can manipulate semantically meaningful dimensions (e.g., orientation) of the target object in the embedding space.

</details>

<details>

<summary>2020-08-12 21:46:52 - Metric Learning vs Classification for Disentangled Music Representation Learning</summary>

- *Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin, Juhan Nam*

- `2008.03729v2` - [abs](http://arxiv.org/abs/2008.03729v2) - [pdf](http://arxiv.org/pdf/2008.03729v2)

> Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.

</details>

<details>

<summary>2020-08-12 21:54:00 - Disentangled Multidimensional Metric Learning for Music Similarity</summary>

- *Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin, Juhan Nam*

- `2008.03720v2` - [abs](http://arxiv.org/abs/2008.03720v2) - [pdf](http://arxiv.org/pdf/2008.03720v2)

> Music similarity search is useful for a variety of creative tasks such as replacing one music recording with another recording with a similar "feel", a common task in video editing. For this task, it is typically necessary to define a similarity metric to compare one recording to another. Music similarity, however, is hard to define and depends on multiple simultaneous notions of similarity (i.e. genre, mood, instrument, tempo). While prior work ignore this issue, we embrace this idea and introduce the concept of multidimensional similarity and unify both global and specialized similarity metrics into a single, semantically disentangled multidimensional similarity metric. To do so, we adapt a variant of deep metric learning called conditional similarity networks to the audio domain and extend it using track-based information to control the specificity of our model. We evaluate our method and show that our single, multidimensional model outperforms both specialized similarity spaces and alternative baselines. We also run a user-study and show that our approach is favored by human annotators as well.

</details>

<details>

<summary>2020-08-12 22:58:20 - End-to-End Neural Transformer Based Spoken Language Understanding</summary>

- *Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann*

- `2008.10984v1` - [abs](http://arxiv.org/abs/2008.10984v1) - [pdf](http://arxiv.org/pdf/2008.10984v1)

> Spoken language understanding (SLU) refers to the process of inferring the semantic information from audio signals. While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing (NLP), their merits in a closely related field, i.e., spoken language understanding (SLU) have not beed investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end transformer SLU predicts the domains, intents and slots in the Fluent Speech Commands dataset with accuracy equal to 98.1 \%, 99.6 \%, and 99.6 \%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4 \% while the size of our model is 25\% smaller than that of these architectures. Additionally, due to independent sub-space projections in the self-attention layer, the model is highly parallelizable which makes it a good candidate for on-device SLU.

</details>

<details>

<summary>2020-08-13 02:54:50 - Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit</summary>

- *Zhen Zeng, Jianzong Wang, Ning Cheng, Jing Xiao*

- `2008.05656v1` - [abs](http://arxiv.org/abs/2008.05656v1) - [pdf](http://arxiv.org/pdf/2008.05656v1)

> Recent neural speech synthesis systems have gradually focused on the control of prosody to improve the quality of synthesized speech, but they rarely consider the variability of prosody and the correlation between prosody and semantics together. In this paper, a prosody learning mechanism is proposed to model the prosody of speech based on TTS system, where the prosody information of speech is extracted from the melspectrum by a prosody learner and combined with the phoneme sequence to reconstruct the mel-spectrum. Meanwhile, the sematic features of text from the pre-trained language model is introduced to improve the prosody prediction results. In addition, a novel self-attention structure, named as local attention, is proposed to lift this restriction of input text length, where the relative position information of the sequence is modeled by the relative position matrices so that the position encodings is no longer needed. Experiments on English and Mandarin show that speech with more satisfactory prosody has obtained in our model. Especially in Mandarin synthesis, our proposed model outperforms baseline model with a MOS gap of 0.08, and the overall naturalness of the synthesized speech has been significantly improved.

</details>

<details>

<summary>2020-08-13 02:59:52 - Cognitive Representation Learning of Self-Media Online Article Quality</summary>

- *Yiru Wang, Shen Huang, Gongfu Li, Qiang Deng, Dongliang Liao, Pengda Si, Yujiu Yang, Jin Xu*

- `2008.05658v1` - [abs](http://arxiv.org/abs/2008.05658v1) - [pdf](http://arxiv.org/pdf/2008.05658v1)

> The automatic quality assessment of self-media online articles is an urgent and new issue, which is of great value to the online recommendation and search. Different from traditional and well-formed articles, self-media online articles are mainly created by users, which have the appearance characteristics of different text levels and multi-modal hybrid editing, along with the potential characteristics of diverse content, different styles, large semantic spans and good interactive experience requirements. To solve these challenges, we establish a joint model CoQAN in combination with the layout organization, writing characteristics and text semantics, designing different representation learning subnetworks, especially for the feature learning process and interactive reading habits on mobile terminals. It is more consistent with the cognitive style of expressing an expert's evaluation of articles. We have also constructed a large scale real-world assessment dataset. Extensive experimental results show that the proposed framework significantly outperforms state-of-the-art methods, and effectively learns and integrates different factors of the online article quality assessment.

</details>

<details>

<summary>2020-08-13 11:32:47 - On the Importance of Local Information in Transformer Based Models</summary>

- *Madhura Pande, Aakriti Budhraja, Preksha Nema, Pratyush Kumar, Mitesh M. Khapra*

- `2008.05828v1` - [abs](http://arxiv.org/abs/2008.05828v1) - [pdf](http://arxiv.org/pdf/2008.05828v1)

> The self-attention module is a key component of Transformer-based models, wherein each token pays attention to every other token. Recent studies have shown that these heads exhibit syntactic, semantic, or local behaviour. Some studies have also identified promise in restricting this attention to be local, i.e., a token attending to other tokens only in a small neighbourhood around it. However, no conclusive evidence exists that such local attention alone is sufficient to achieve high accuracy on multiple NLP tasks. In this work, we systematically analyse the role of locality information in learnt models and contrast it with the role of syntactic information. More specifically, we first do a sensitivity analysis and show that, at every layer, the representation of a token is much more sensitive to tokens in a small neighborhood around it than to tokens which are syntactically related to it. We then define an attention bias metric to determine whether a head pays more attention to local tokens or to syntactically related tokens. We show that a larger fraction of heads have a locality bias as compared to a syntactic bias. Having established the importance of local attention heads, we train and evaluate models where varying fractions of the attention heads are constrained to be local. Such models would be more efficient as they would have fewer computations in the attention layer. We evaluate these models on 4 GLUE datasets (QQP, SST-2, MRPC, QNLI) and 2 MT datasets (En-De, En-Ru) and clearly demonstrate that such constrained models have comparable performance to the unconstrained models. Through this systematic evaluation we establish that attention in Transformer-based models can be constrained to be local without affecting performance.

</details>

<details>

<summary>2020-08-13 14:40:46 - Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations</summary>

- *Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, Raquel Urtasun*

- `2008.05930v1` - [abs](http://arxiv.org/abs/2008.05930v1) - [pdf](http://arxiv.org/pdf/2008.05930v1)

> In this paper we propose a novel end-to-end learnable network that performs joint perception, prediction and motion planning for self-driving vehicles and produces interpretable intermediate representations. Unlike existing neural motion planners, our motion planning costs are consistent with our perception and prediction estimates. This is achieved by a novel differentiable semantic occupancy representation that is explicitly used as cost by the motion planning process. Our network is learned end-to-end from human demonstrations. The experiments in a large-scale manual-driving dataset and closed-loop simulation show that the proposed model significantly outperforms state-of-the-art planners in imitating the human behaviors while producing much safer trajectories.

</details>

<details>

<summary>2020-08-13 16:45:51 - Déjà Vu: Side-Channel Analysis of Mozilla's NSS</summary>

- *Sohaib ul Hassan, Iaroslav Gridin, Ignacio M. Delgado-Lozano, Cesar Pereida García, Jesús-Javier Chi-Domínguez, Alejandro Cabrera Aldaya, Billy Bob Brumley*

- `2008.06004v1` - [abs](http://arxiv.org/abs/2008.06004v1) - [pdf](http://arxiv.org/pdf/2008.06004v1)

> Recent work on Side Channel Analysis (SCA) targets old, well-known vulnerabilities, even previously exploited, reported, and patched in high-profile cryptography libraries. Nevertheless, researchers continue to find and exploit the same vulnerabilities in old and new products, highlighting a big issue among vendors: effectively tracking and fixing security vulnerabilities when disclosure is not done directly to them. In this work, we present another instance of this issue by performing the first library-wide SCA security evaluation of Mozilla's NSS security library. We use a combination of two independently-developed SCA security frameworks to identify and test security vulnerabilities. Our evaluation uncovers several new vulnerabilities in NSS affecting DSA, ECDSA, and RSA cryptosystems. We exploit said vulnerabilities and implement key recovery attacks using signals---extracted through different techniques such as timing, microarchitecture, and EM---and improved lattice methods.

</details>

<details>

<summary>2020-08-13 18:38:21 - Discovering and Categorising Language Biases in Reddit</summary>

- *Xavier Ferrer, Tom van Nuenen, Jose M. Such, Natalia Criado*

- `2008.02754v2` - [abs](http://arxiv.org/abs/2008.02754v2) - [pdf](http://arxiv.org/pdf/2008.02754v2)

> We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.

</details>

<details>

<summary>2020-08-14 01:05:09 - Model Robustness with Text Classification: Semantic-preserving adversarial attacks</summary>

- *Rahul Singh, Tarun Joshi, Vijayan N. Nair, Agus Sudjianto*

- `2008.05536v2` - [abs](http://arxiv.org/abs/2008.05536v2) - [pdf](http://arxiv.org/pdf/2008.05536v2)

> We propose algorithms to create adversarial attacks to assess model robustness in text classification problems. They can be used to create white box attacks and black box attacks while at the same time preserving the semantics and syntax of the original text. The attacks cause significant number of flips in white-box setting and same rule based can be used in black-box setting. In a black-box setting, the attacks created are able to reverse decisions of transformer based architectures.

</details>

<details>

<summary>2020-08-14 02:43:57 - Speech To Semantics: Improve ASR and NLU Jointly via All-Neural Interfaces</summary>

- *Milind Rao, Anirudh Raju, Pranav Dheram, Bach Bui, Ariya Rastrow*

- `2008.06173v1` - [abs](http://arxiv.org/abs/2008.06173v1) - [pdf](http://arxiv.org/pdf/2008.06173v1)

> We consider the problem of spoken language understanding (SLU) of extracting natural language intents and associated slot arguments or named entities from speech that is primarily directed at voice assistants. Such a system subsumes both automatic speech recognition (ASR) as well as natural language understanding (NLU). An end-to-end joint SLU model can be built to a required specification opening up the opportunity to deploy on hardware constrained scenarios like devices enabling voice assistants to work offline, in a privacy preserving manner, whilst also reducing server costs.   We first present models that extract utterance intent directly from speech without intermediate text output. We then present a compositional model, which generates the transcript using the Listen Attend Spell ASR system and then extracts interpretation using a neural NLU model. Finally, we contrast these methods to a jointly trained end-to-end joint SLU model, consisting of ASR and NLU subsystems which are connected by a neural network based interface instead of text, that produces transcripts as well as NLU interpretation. We show that the jointly trained model shows improvements to ASR incorporating semantic information from NLU and also improves NLU by exposing it to ASR confusion encoded in the hidden layer.

</details>

<details>

<summary>2020-08-14 09:16:47 - An Empirical Evaluation of GDPR Compliance Violations in Android mHealth Apps</summary>

- *Ming Fan, Le Yu, Sen Chen, Hao Zhou, Xiapu Luo, Shuyue Li, Yang Liu, Jun Liu, Ting Liu*

- `2008.05864v2` - [abs](http://arxiv.org/abs/2008.05864v2) - [pdf](http://arxiv.org/pdf/2008.05864v2)

> The purpose of the General Data Protection Regulation (GDPR) is to provide improved privacy protection. If an app controls personal data from users, it needs to be compliant with GDPR. However, GDPR lists general rules rather than exact step-by-step guidelines about how to develop an app that fulfills the requirements. Therefore, there may exist GDPR compliance violations in existing apps, which would pose severe privacy threats to app users. In this paper, we take mobile health applications (mHealth apps) as a peephole to examine the status quo of GDPR compliance in Android apps. We first propose an automated system, named \mytool, to bridge the semantic gap between the general rules of GDPR and the app implementations by identifying the data practices declared in the app privacy policy and the data relevant behaviors in the app code. Then, based on \mytool, we detect three kinds of GDPR compliance violations, including the incompleteness of privacy policy, the inconsistency of data collections, and the insecurity of data transmission. We perform an empirical evaluation of 796 mHealth apps. The results reveal that 189 (23.7\%) of them do not provide complete privacy policies. Moreover, 59 apps collect sensitive data through different measures, but 46 (77.9\%) of them contain at least one inconsistent collection behavior. Even worse, among the 59 apps, only 8 apps try to ensure the transmission security of collected data. However, all of them contain at least one encryption or SSL misuse. Our work exposes severe privacy issues to raise awareness of privacy protection for app users and developers.

</details>

<details>

<summary>2020-08-14 17:06:25 - SMT-based Safety Verification of Parameterised Multi-Agent Systems</summary>

- *Paolo Felli, Alessandro Gianola, Marco Montali*

- `2008.04774v2` - [abs](http://arxiv.org/abs/2008.04774v2) - [pdf](http://arxiv.org/pdf/2008.04774v2)

> In this paper we study the verification of parameterised multi-agent systems (MASs), and in particular the task of verifying whether unwanted states, characterised as a given state formula, are reachable in a given MAS, i.e., whether the MAS is unsafe. The MAS is parameterised and the model only describes the finite set of possible agent templates, while the actual number of concrete agent instances for each template is unbounded and cannot be foreseen. This makes the state-space infinite. As safety may of course depend on the number of agent instances in the system, the verification result must be correct irrespective of such number. We solve this problem via infinite-state model checking based on satisfiability modulo theories (SMT), relying on the theory of array-based systems: we present parameterised MASs as particular array-based systems, under two execution semantics for the MAS, which we call concurrent and interleaved. We prove our decidability results under these assumptions and illustrate our implementation approach, called SAFE: the Swarm Safety Detector, based on the third-party model checker MCMT, which we evaluate experimentally. Finally, we discuss how this approach lends itself to richer parameterised and data-aware MAS settings beyond the state-of-the-art solutions in the literature, which we leave as future work.

</details>

<details>

<summary>2020-08-14 18:35:38 - Making Distributed Mobile Applications SAFE: Enforcing User Privacy Policies on Untrusted Applications with Secure Application Flow Enforcement</summary>

- *Adriana Szekeres, Irene Zhang, Katelin Bailey, Isaac Ackerman, Haichen Shen, Franziska Roesner, Dan R. K. Ports, Arvind Krishnamurthy, Henry M. Levy*

- `2008.06536v1` - [abs](http://arxiv.org/abs/2008.06536v1) - [pdf](http://arxiv.org/pdf/2008.06536v1)

> Today's mobile devices sense, collect, and store huge amounts of personal information, which users share with family and friends through a wide range of applications. Once users give applications access to their data, they must implicitly trust that the apps correctly maintain data privacy. As we know from both experience and all-too-frequent press articles, that trust is often misplaced. While users do not trust applications, they do trust their mobile devices and operating systems. Unfortunately, sharing applications are not limited to mobile clients but must also run on cloud services to share data between users. In this paper, we leverage the trust that users have in their mobile OSes to vet cloud services. To do so, we define a new Secure Application Flow Enforcement (SAFE) framework, which requires cloud services to attest to a system stack that will enforce policies provided by the mobile OS for user data. We implement a mobile OS that enforces SAFE policies on unmodified mobile apps and two systems for enforcing policies on untrusted cloud services. Using these prototypes, we demonstrate that it is possible to enforce existing user privacy policies on unmodified applications.

</details>

<details>

<summary>2020-08-15 13:29:53 - Amora: Black-box Adversarial Morphing Attack</summary>

- *Run Wang, Felix Juefei-Xu, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma, Yang Liu*

- `1912.03829v5` - [abs](http://arxiv.org/abs/1912.03829v5) - [pdf](http://arxiv.org/pdf/1912.03829v5)

> Nowadays, digital facial content manipulation has become ubiquitous and realistic with the success of generative adversarial networks (GANs), making face recognition (FR) systems suffer from unprecedented security concerns. In this paper, we investigate and introduce a new type of adversarial attack to evade FR systems by manipulating facial content, called \textbf{\underline{a}dversarial \underline{mor}phing \underline{a}ttack} (a.k.a. Amora). In contrast to adversarial noise attack that perturbs pixel intensity values by adding human-imperceptible noise, our proposed adversarial morphing attack works at the semantic level that perturbs pixels spatially in a coherent manner. To tackle the black-box attack problem, we devise a simple yet effective joint dictionary learning pipeline to obtain a proprietary optical flow field for each attack. Our extensive evaluation on two popular FR systems demonstrates the effectiveness of our adversarial morphing attack at various levels of morphing intensity with smiling facial expression manipulations. Both open-set and closed-set experimental results indicate that a novel black-box adversarial attack based on local deformation is possible, and is vastly different from additive noise attacks. The findings of this work potentially pave a new research direction towards a more thorough understanding and investigation of image-based adversarial attacks and defenses.

</details>

<details>

<summary>2020-08-15 20:01:23 - Model Patching: Closing the Subgroup Performance Gap with Data Augmentation</summary>

- *Karan Goel, Albert Gu, Yixuan Li, Christopher Ré*

- `2008.06775v1` - [abs](http://arxiv.org/abs/2008.06775v1) - [pdf](http://arxiv.org/pdf/2008.06775v1)

> Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL's effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.

</details>

<details>

<summary>2020-08-16 10:39:50 - TopicBERT: A Transformer transfer learning based memory-graph approach for multimodal streaming social media topic detection</summary>

- *Meysam Asgari-Chenaghlu, Mohammad-Reza Feizi-Derakhshi, Leili farzinvash, Mohammad-Ali Balafar, Cina Motamed*

- `2008.06877v1` - [abs](http://arxiv.org/abs/2008.06877v1) - [pdf](http://arxiv.org/pdf/2008.06877v1)

> Real time nature of social networks with bursty short messages and their respective large data scale spread among vast variety of topics are research interest of many researchers. These properties of social networks which are known as 5'Vs of big data has led to many unique and enlightenment algorithms and techniques applied to large social networking datasets and data streams. Many of these researches are based on detection and tracking of hot topics and trending social media events that help revealing many unanswered questions. These algorithms and in some cases software products mostly rely on the nature of the language itself. Although, other techniques such as unsupervised data mining methods are language independent but many requirements for a comprehensive solution are not met. Many research issues such as noisy sentences that adverse grammar and new online user invented words are challenging maintenance of a good social network topic detection and tracking methodology; The semantic relationship between words and in most cases, synonyms are also ignored by many of these researches. In this research, we use Transformers combined with an incremental community detection algorithm. Transformer in one hand, provides the semantic relation between words in different contexts. On the other hand, the proposed graph mining technique enhances the resulting topics with aid of simple structural rules. Named entity recognition from multimodal data, image and text, labels the named entities with entity type and the extracted topics are tuned using them. All operations of proposed system has been applied with big social data perspective under NoSQL technologies. In order to present a working and systematic solution, we combined MongoDB with Neo4j as two major database systems of our work. The proposed system shows higher precision and recall compared to other methods in three different datasets.

</details>

<details>

<summary>2020-08-16 16:33:59 - LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices</summary>

- *Radu Alexandru Rosu, Peer Schütt, Jan Quenzel, Sven Behnke*

- `1912.05905v3` - [abs](http://arxiv.org/abs/1912.05905v3) - [pdf](http://arxiv.org/pdf/1912.05905v3)

> Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. However, applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes as input raw point clouds. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.

</details>

<details>

<summary>2020-08-16 19:38:06 - Bayesian Cycle-Consistent Generative Adversarial Networks via Marginalizing Latent Sampling</summary>

- *Haoran You, Yu Cheng, Tianheng Cheng, Chunliang Li, Pan Zhou*

- `1811.07465v3` - [abs](http://arxiv.org/abs/1811.07465v3) - [pdf](http://arxiv.org/pdf/1811.07465v3)

> Recent techniques built on Generative Adversarial Networks (GANs), such as Cycle-Consistent GANs, are able to learn mappings among different domains built from unpaired datasets, through min-max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for inter-domain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.

</details>

<details>

<summary>2020-08-16 20:51:17 - Efficient Knowledge Graph Validation via Cross-Graph Representation Learning</summary>

- *Yaqing Wang, Fenglong Ma, Jing Gao*

- `2008.06995v1` - [abs](http://arxiv.org/abs/2008.06995v1) - [pdf](http://arxiv.org/pdf/2008.06995v1)

> Recent advances in information extraction have motivated the automatic construction of huge Knowledge Graphs (KGs) by mining from large-scale text corpus. However, noisy facts are unavoidably introduced into KGs that could be caused by automatic extraction. To validate the correctness of facts (i.e., triplets) inside a KG, one possible approach is to map the triplets into vector representations by capturing the semantic meanings of facts. Although many representation learning approaches have been developed for knowledge graphs, these methods are not effective for validation. They usually assume that facts are correct, and thus may overfit noisy facts and fail to detect such facts. Towards effective KG validation, we propose to leverage an external human-curated KG as auxiliary information source to help detect the errors in a target KG. The external KG is built upon human-curated knowledge repositories and tends to have high precision. On the other hand, although the target KG built by information extraction from texts has low precision, it can cover new or domain-specific facts that are not in any human-curated repositories. To tackle this challenging task, we propose a cross-graph representation learning framework, i.e., CrossVal, which can leverage an external KG to validate the facts in the target KG efficiently. This is achieved by embedding triplets based on their semantic meanings, drawing cross-KG negative samples and estimating a confidence score for each triplet based on its degree of correctness. We evaluate the proposed framework on datasets across different domains. Experimental results show that the proposed framework achieves the best performance compared with the state-of-the-art methods on large-scale KGs.

</details>

<details>

<summary>2020-08-16 23:38:02 - Training CNN Classifiers for Semantic Segmentation using Partially Annotated Images: with Application on Human Thigh and Calf MRI</summary>

- *Chun Kit Wong, Stephanie Marchesseau, Maria Kalimeri, Tiang Siew Yap, Serena S. H. Teo, Lingaraj Krishna, Alfredo Franco-Obregón, Stacey K. H. Tay, Chin Meng Khoo, Philip T. H. Lee, Melvin K. S. Leow, John J. Totman, Mary C. Stephenson*

- `2008.07030v1` - [abs](http://arxiv.org/abs/2008.07030v1) - [pdf](http://arxiv.org/pdf/2008.07030v1)

> Objective: Medical image datasets with pixel-level labels tend to have a limited number of organ or tissue label classes annotated, even when the images have wide anatomical coverage. With supervised learning, multiple classifiers are usually needed given these partially annotated datasets. In this work, we propose a set of strategies to train one single classifier in segmenting all label classes that are heterogeneously annotated across multiple datasets without moving into semi-supervised learning. Methods: Masks were first created from each label image through a process we termed presence masking. Three presence masking modes were evaluated, differing mainly in weightage assigned to the annotated and unannotated classes. These masks were then applied to the loss function during training to remove the influence of unannotated classes. Results: Evaluation against publicly available CT datasets shows that presence masking is a viable method for training class-generic classifiers. Our class-generic classifier can perform as well as multiple class-specific classifiers combined, while the training duration is similar to that required for one class-specific classifier. Furthermore, the class-generic classifier can outperform the class-specific classifiers when trained on smaller datasets. Finally, consistent results are observed from evaluations against human thigh and calf MRI datasets collected in-house. Conclusion: The evaluation outcomes show that presence masking is capable of significantly improving both training and inference efficiency across imaging modalities and anatomical regions. Improved performance may even be observed on small datasets. Significance: Presence masking strategies can reduce the computational resources and costs involved in manual medical image annotations. All codes are publicly available at https://github.com/wong-ck/DeepSegment.

</details>

<details>

<summary>2020-08-17 01:46:51 - A Visual Analytics Framework for Contrastive Network Analysis</summary>

- *Takanori Fujiwara, Jian Zhao, Francine Chen, Kwan-Liu Ma*

- `2008.00151v2` - [abs](http://arxiv.org/abs/2008.00151v2) - [pdf](http://arxiv.org/pdf/2008.00151v2)

> A common network analysis task is comparison of two networks to identify unique characteristics in one network with respect to the other. For example, when comparing protein interaction networks derived from normal and cancer tissues, one essential task is to discover protein-protein interactions unique to cancer tissues. However, this task is challenging when the networks contain complex structural (and semantic) relations. To address this problem, we design ContraNA, a visual analytics framework leveraging both the power of machine learning for uncovering unique characteristics in networks and also the effectiveness of visualization for understanding such uniqueness. The basis of ContraNA is cNRL, which integrates two machine learning schemes, network representation learning (NRL) and contrastive learning (CL), to generate a low-dimensional embedding that reveals the uniqueness of one network when compared to another. ContraNA provides an interactive visualization interface to help analyze the uniqueness by relating embedding results and network structures as well as explaining the learned features by cNRL. We demonstrate the usefulness of ContraNA with two case studies using real-world datasets. We also evaluate through a controlled user study with 12 participants on network comparison tasks. The results show that participants were able to both effectively identify unique characteristics from complex networks and interpret the results obtained from cNRL.

</details>

<details>

<summary>2020-08-17 03:20:05 - Putting the Semantics into Semantic Versioning</summary>

- *Patrick Lam, Jens Dietrich, David J. Pearce*

- `2008.07069v1` - [abs](http://arxiv.org/abs/2008.07069v1) - [pdf](http://arxiv.org/pdf/2008.07069v1)

> The long-standing aspiration for software reuse has made astonishing strides in the past few years. Many modern software development ecosystems now come with rich sets of publicly-available components contributed by the community. Downstream developers can leverage these upstream components, boosting their productivity.   However, components evolve at their own pace. This imposes obligations on and yields benefits for downstream developers, especially since changes can be breaking, requiring additional downstream work to adapt to. Upgrading too late leaves downstream vulnerable to security issues and missing out on useful improvements; upgrading too early results in excess work. Semantic versioning has been proposed as an elegant mechanism to communicate levels of compatibility, enabling downstream developers to automate dependency upgrades.   While it is questionable whether a version number can adequately characterize version compatibility in general, we argue that developers would greatly benefit from tools such as semantic version calculators to help them upgrade safely. The time is now for the research community to develop such tools: large component ecosystems exist and are accessible, component interactions have become observable through automated builds, and recent advances in program analysis make the development of relevant tools feasible. In particular, contracts (both traditional and lightweight) are a promising input to semantic versioning calculators, which can suggest whether an upgrade is likely to be safe.

</details>

<details>

<summary>2020-08-17 05:40:06 - Shifu2: A Network Representation Learning Based Model for Advisor-advisee Relationship Mining</summary>

- *Jiaying Liu, Feng Xia, Lei Wang, Bo Xu, Xiangjie Kong, Hanghang Tong, Irwin King*

- `2008.07097v1` - [abs](http://arxiv.org/abs/2008.07097v1) - [pdf](http://arxiv.org/pdf/2008.07097v1)

> The advisor-advisee relationship represents direct knowledge heritage, and such relationship may not be readily available from academic libraries and search engines. This work aims to discover advisor-advisee relationships hidden behind scientific collaboration networks. For this purpose, we propose a novel model based on Network Representation Learning (NRL), namely Shifu2, which takes the collaboration network as input and the identified advisor-advisee relationship as output. In contrast to existing NRL models, Shifu2 considers not only the network structure but also the semantic information of nodes and edges. Shifu2 encodes nodes and edges into low-dimensional vectors respectively, both of which are then utilized to identify advisor-advisee relationships. Experimental results illustrate improved stability and effectiveness of the proposed model over state-of-the-art methods. In addition, we generate a large-scale academic genealogy dataset by taking advantage of Shifu2.

</details>

<details>

<summary>2020-08-17 08:04:11 - Logical Semantics, Dialogical Argumentation, and Textual Entailment</summary>

- *Davide Catta, Richard Moot, Christian Retoré*

- `2008.07138v1` - [abs](http://arxiv.org/abs/2008.07138v1) - [pdf](http://arxiv.org/pdf/2008.07138v1)

> In this chapter, we introduce a new dialogical system for first order classical logic which is close to natural language argumentation, and we prove its completeness with respect to usual classical validity. We combine our dialogical system with the Grail syntactic and semantic parser developed by the second author in order to address automated textual entailment, that is, we use it for deciding whether or not a sentence is a consequence of a short text. This work-which connects natural language semantics and argumentation with dialogical logic-can be viewed as a step towards an inferentialist view of natural language semantics.

</details>

<details>

<summary>2020-08-17 08:09:49 - PSCS: A Path-based Neural Model for Semantic Code Search</summary>

- *Zhensu Sun, Yan Liu, Chen Yang, Yu Qian*

- `2008.03042v2` - [abs](http://arxiv.org/abs/2008.03042v2) - [pdf](http://arxiv.org/pdf/2008.03042v2)

> To obtain code snippets for reuse, programmers prefer to search for related documents, e.g., blogs or Q&A, instead of code itself. The major reason is due to the semantic diversity and mismatch between queries and code snippets. Deep learning models have been proposed to address this challenge. Compared with approaches using information retrieval techniques, deep learning models do not suffer from the information loss caused by refining user intention into keywords. However, the performance of previous works is not satisfactory because they ignore the importance of code structure. When the semantics of code (e.g., identifier names, APIs) are ambiguous, code structure may be the only feature for the model to utilize. In that case, previous works relearn the structural information from lexical tokens of code, which is extremely difficult for a model without any domain knowledge. In this work, we propose PSCS, a path-based neural model for semantic code search. Our model encodes both the semantics and structures of code represented by AST paths. We train and evaluate our model over 330k-19k query-function pairs, respectively. The evaluation results demonstrate that PSCS achieves a SuccessRate of 47.6% and a Mean Reciprocal Rank (MRR) of 30.4% when considering the top-10 results with a match. The proposed approach significantly outperforms both DeepCS, the first approach that applies deep learning to code search task, and CARLCS, a state-of-the-art approach that introduces a co-attentive representation learning model on the basis of DeepCS. The importance of code structure is demonstrated with an ablation study on code features, which enlightens model design for further studies.

</details>

<details>

<summary>2020-08-17 08:33:50 - Binary-level Directed Fuzzing for Use-After-Free Vulnerabilities</summary>

- *Manh-Dung Nguyen, Sébastien Bardin, Richard Bonichon, Roland Groz, Matthieu Lemerre*

- `2002.10751v2` - [abs](http://arxiv.org/abs/2002.10751v2) - [pdf](http://arxiv.org/pdf/2002.10751v2)

> Directed fuzzing focuses on automatically testing specific parts of the code by taking advantage of additional information such as (partial) bug stack trace, patches or risky operations. Key applications include bug reproduction, patch testing and static analysis report verification. Although directed fuzzing has received a lot of attention recently, hard-to-detect vulnerabilities such as Use-After-Free (UAF) are still not well addressed, especially at the binary level. We propose UAFuzz, the first (binary-level) directed greybox fuzzer dedicated to UAF bugs. The technique features a fuzzing engine tailored to UAF specifics, a lightweight code instrumentation and an efficient bug triage step. Experimental evaluation for bug reproduction on real cases demonstrates that UAFuzz significantly outperforms state-of-the-art directed fuzzers in terms of fault detection rate, time to exposure and bug triaging. UAFuzz has also been proven effective in patch testing, leading to the discovery of 30 new bugs (7 CVEs) in programs such as Perl, GPAC and GNU Patch. Finally, we provide to the community a large fuzzing benchmark dedicated to UAF, built on both real codes and real bugs.

</details>

<details>

<summary>2020-08-17 12:51:14 - Temporal Conformance Checking at Runtime based on Time-infused Process Models</summary>

- *Florian Stertz, Juergen Mangler, Stefanie Rinderle-Ma*

- `2008.07262v1` - [abs](http://arxiv.org/abs/2008.07262v1) - [pdf](http://arxiv.org/pdf/2008.07262v1)

> Conformance checking quantifies the deviations between a set of traces in a given process log and a set of possible traces defined by a process model. Current approaches mostly focus on added or missing events. Lately, multi-perspective mining has provided means to check for conformance with time and resource constraints encoded as data elements. This paper presents an approach for quantifying temporal deviations in conformance checking based on infusing the input process model with a temporal profile. The temporal profile is calculated based on an associated process log considering task durations and the temporal distance between events. Moreover, a simple semantic annotation on tasks in the process model signifies their importance with respect to time. During runtime, deviations between an event stream and the process model with the temporal profile are quantified through a cost function for temporal deviations. The evaluation of the approach shows that the results for two real-world data sets from the financial and a manufacturing domain hold the promise to improve runtime process monitoring and control capabilities.

</details>

<details>

<summary>2020-08-17 13:48:32 - scikit-dyn2sel -- A Dynamic Selection Framework for Data Streams</summary>

- *Lucca Portes Cavalheiro, Jean Paul Barddal, Alceu de Souza Britto Jr, Laurent Heutte*

- `2008.08920v1` - [abs](http://arxiv.org/abs/2008.08920v1) - [pdf](http://arxiv.org/pdf/2008.08920v1)

> Mining data streams is a challenge per se. It must be ready to deal with an enormous amount of data and with problems not present in batch machine learning, such as concept drift. Therefore, applying a batch-designed technique, such as dynamic selection of classifiers (DCS) also presents a challenge. The dynamic characteristic of ensembles that deal with streams presents barriers to the application of traditional DCS techniques in such classifiers. scikit-dyn2sel is an open-source python library tailored for dynamic selection techniques in streaming data. scikit-dyn2sel's development follows code quality and testing standards, including PEP8 compliance and automated high test coverage using codecov.io and circleci.com. Source code, documentation, and examples are made available on GitHub at https://github.com/luccaportes/Scikit-DYN2SEL.

</details>

<details>

<summary>2020-08-17 15:39:11 - Towards Smart Sustainable Cities: Addressing semantic heterogeneity in building management systems using discriminative models</summary>

- *Chidubem Iddianozie, Paulito Palmes*

- `2008.07414v1` - [abs](http://arxiv.org/abs/2008.07414v1) - [pdf](http://arxiv.org/pdf/2008.07414v1)

> Building Management Systems (BMS) are crucial in the drive towards smart sustainable cities. This is due to the fact that they have been effective in significantly reducing the energy consumption of buildings. A typical BMS is composed of smart devices that communicate with one another in order to achieve their purpose. However, the heterogeneity of these devices and their associated meta-data impede the deployment of solutions that depend on the interactions among these devices. Nonetheless, automatically inferring the semantics of these devices using data-driven methods provides an ideal solution to the problems brought about by this heterogeneity. In this paper, we undertake a multi-dimensional study to address the problem of inferring the semantics of IoT devices using machine learning models. Using two datasets with over 67 million data points collected from IoT devices, we developed discriminative models that produced competitive results. Particularly, our study highlights the potential of Image Encoded Time Series (IETS) as a robust alternative to statistical feature-based inference methods. Leveraging just a fraction of the data required by feature-based methods, our evaluations show that this encoding competes with and even outperforms traditional methods in many cases.

</details>

<details>

<summary>2020-08-17 15:49:30 - Siloed Federated Learning for Multi-Centric Histopathology Datasets</summary>

- *Mathieu Andreux, Jean Ogier du Terrail, Constance Beguier, Eric W. Tramel*

- `2008.07424v1` - [abs](http://arxiv.org/abs/2008.07424v1) - [pdf](http://arxiv.org/pdf/2008.07424v1)

> While federated learning is a promising approach for training deep learning models over distributed sensitive datasets, it presents new challenges for machine learning, especially when applied in the medical domain where multi-centric data heterogeneity is common. Building on previous domain adaptation works, this paper proposes a novel federated learning approach for deep learning architectures via the introduction of local-statistic batch normalization (BN) layers, resulting in collaboratively-trained, yet center-specific models. This strategy improves robustness to data heterogeneity while also reducing the potential for information leaks by not sharing the center-specific layer activation statistics. We benchmark the proposed method on the classification of tumorous histopathology image patches extracted from the Camelyon16 and Camelyon17 datasets. We show that our approach compares favorably to previous state-of-the-art methods, especially for transfer learning across datasets.

</details>

<details>

<summary>2020-08-17 16:04:03 - Zero Shot Domain Generalization</summary>

- *Udit Maniyar, Joseph K J, Aniket Anand Deshmukh, Urun Dogan, Vineeth N Balasubramanian*

- `2008.07443v1` - [abs](http://arxiv.org/abs/2008.07443v1) - [pdf](http://arxiv.org/pdf/2008.07443v1)

> Standard supervised learning setting assumes that training data and test data come from the same distribution (domain). Domain generalization (DG) methods try to learn a model that when trained on data from multiple domains, would generalize to a new unseen domain. We extend DG to an even more challenging setting, where the label space of the unseen domain could also change. We introduce this problem as Zero-Shot Domain Generalization (to the best of our knowledge, the first such effort), where the model generalizes across new domains and also across new classes in those domains. We propose a simple strategy which effectively exploits semantic information of classes, to adapt existing DG methods to meet the demands of Zero-Shot Domain Generalization. We evaluate the proposed methods on CIFAR-10, CIFAR-100, F-MNIST and PACS datasets, establishing a strong baseline to foster interest in this new research direction.

</details>

<details>

<summary>2020-08-17 16:13:46 - Extending and Analyzing Self-Supervised Learning Across Domains</summary>

- *Bram Wallace, Bharath Hariharan*

- `2004.11992v2` - [abs](http://arxiv.org/abs/2004.11992v2) - [pdf](http://arxiv.org/pdf/2004.11992v2)

> Self-supervised representation learning has achieved impressive results in recent years, with experiments primarily coming on ImageNet or other similarly large internet imagery datasets. There has been little to no work with these methods on other smaller domains, such as satellite, textural, or biological imagery. We experiment with several popular methods on an unprecedented variety of domains. We discover, among other findings, that Rotation is by far the most semantically meaningful task, with much of the performance of Jigsaw and Instance Discrimination being attributable to the nature of their induced distribution rather than semantic understanding. Additionally, there are several areas, such as fine-grain classification, where all tasks underperform. We quantitatively and qualitatively diagnose the reasons for these failures and successes via novel experiments studying pretext generalization, random labelings, and implicit dimensionality. Code and models are available at https://github.com/BramSW/Extending_SSRL_Across_Domains/.

</details>

<details>

<summary>2020-08-18 17:54:08 - AssembleNet++: Assembling Modality Representations via Attention Connections</summary>

- *Michael S. Ryoo, AJ Piergiovanni, Juhana Kangaspunta, Anelia Angelova*

- `2008.08072v1` - [abs](http://arxiv.org/abs/2008.08072v1) - [pdf](http://arxiv.org/pdf/2008.08072v1)

> We create a family of powerful video models which are able to: (i) learn interactions between semantic object information and raw appearance and motion features, and (ii) deploy attention in order to better learn the importance of features at each convolutional block of the network. A new network component named peer-attention is introduced, which dynamically learns the attention weights using another block or input modality. Even without pre-training, our models outperform the previous work on standard public activity recognition datasets with continuous videos, establishing new state-of-the-art. We also confirm that our findings of having neural connections from the object modality and the use of peer-attention is generally applicable for different existing architectures, improving their performances. We name our model explicitly as AssembleNet++. The code will be available at: https://sites.google.com/corp/view/assemblenet/

</details>

<details>

<summary>2020-08-19 00:16:40 - Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval</summary>

- *Tao Wu, Ellie Ka-In Chio, Heng-Tze Cheng, Yu Du, Steffen Rendle, Dima Kuzmin, Ritesh Agarwal, Li Zhang, John Anderson, Sarvjeet Singh, Tushar Chandra, Ed H. Chi, Wen Li, Ankit Kumar, Xiang Ma, Alex Soares, Nitin Jindal, Pei Cao*

- `2008.02930v2` - [abs](http://arxiv.org/abs/2008.02930v2) - [pdf](http://arxiv.org/pdf/2008.02930v2)

> Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.

</details>

<details>

<summary>2020-08-19 01:53:53 - Score-Based Explanations in Data Management and Machine Learning</summary>

- *Leopoldo Bertossi*

- `2007.12799v2` - [abs](http://arxiv.org/abs/2007.12799v2) - [pdf](http://arxiv.org/pdf/2007.12799v2)

> We describe some approaches to explanations for observed outcomes in data management and machine learning. They are based on the assignment of numerical scores to predefined and potentially relevant inputs. More specifically, we consider explanations for query answers in databases, and for results from classification models. The described approaches are mostly of a causal and counterfactual nature. We argue for the need to bring domain and semantic knowledge into score computations; and suggest some ways to do this.

</details>

<details>

<summary>2020-08-19 05:35:27 - Towards Deep Clustering of Human Activities from Wearables</summary>

- *Alireza Abedin, Farbod Motlagh, Qinfeng Shi, Seyed Hamid Rezatofighi, Damith Chinthana Ranasinghe*

- `2008.01659v2` - [abs](http://arxiv.org/abs/2008.01659v2) - [pdf](http://arxiv.org/pdf/2008.01659v2)

> Our ability to exploit low-cost wearable sensing modalities for critical human behaviour and activity monitoring applications in health and wellness is reliant on supervised learning regimes; here, deep learning paradigms have proven extremely successful in learning activity representations from annotated data. However, the costly work of gathering and annotating sensory activity datasets is labor-intensive, time consuming and not scalable to large volumes of data. While existing unsupervised remedies of deep clustering leverage network architectures and optimization objectives that are tailored for static image datasets, deep architectures to uncover cluster structures from raw sequence data captured by on-body sensors remains largely unexplored. In this paper, we develop an unsupervised end-to-end learning strategy for the fundamental problem of human activity recognition (HAR) from wearables. Through extensive experiments, including comparisons with existing methods, we show the effectiveness of our approach to jointly learn unsupervised representations for sensory data and generate cluster assignments with strong semantic correspondence to distinct human activities.

</details>

<details>

<summary>2020-08-19 09:17:32 - Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes</summary>

- *Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun*

- `1910.08910v2` - [abs](http://arxiv.org/abs/1910.08910v2) - [pdf](http://arxiv.org/pdf/1910.08910v2)

> Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at https://github.com/thunlp/SememeRNN.

</details>

<details>

<summary>2020-08-19 13:31:07 - Generating Categories for Sets of Entities</summary>

- *Shuo Zhang, Krisztian Balog, Jamie Callan*

- `2008.08428v1` - [abs](http://arxiv.org/abs/2008.08428v1) - [pdf](http://arxiv.org/pdf/2008.08428v1)

> Category systems are central components of knowledge bases, as they provide a hierarchical grouping of semantically related concepts and entities. They are a unique and valuable resource that is utilized in a broad range of information access tasks. To aid knowledge editors in the manual process of expanding a category system, this paper presents a method of generating categories for sets of entities. First, we employ neural abstractive summarization models to generate candidate categories. Next, the location within the hierarchy is identified for each candidate. Finally, structure-, content-, and hierarchy-based features are used to rank candidates to identify by the most promising ones (measured in terms of specificity, hierarchy, and importance). We develop a test collection based on Wikipedia categories and demonstrate the effectiveness of the proposed approach.

</details>

<details>

<summary>2020-08-19 13:36:57 - Unsupervised Cross-domain Image Classification by Distance Metric Guided Feature Alignment</summary>

- *Qingjie Meng, Daniel Rueckert, Bernhard Kainz*

- `2008.08433v1` - [abs](http://arxiv.org/abs/2008.08433v1) - [pdf](http://arxiv.org/pdf/2008.08433v1)

> Learning deep neural networks that are generalizable across different domains remains a challenge due to the problem of domain shift. Unsupervised domain adaptation is a promising avenue which transfers knowledge from a source domain to a target domain without using any labels in the target domain. Contemporary techniques focus on extracting domain-invariant features using domain adversarial training. However, these techniques neglect to learn discriminative class boundaries in the latent representation space on a target domain and yield limited adaptation performance. To address this problem, we propose distance metric guided feature alignment (MetFA) to extract discriminative as well as domain-invariant features on both source and target domains. The proposed MetFA method explicitly and directly learns the latent representation without using domain adversarial training. Our model integrates class distribution alignment to transfer semantic knowledge from a source domain to a target domain. We evaluate the proposed method on fetal ultrasound datasets for cross-device image classification. Experimental results demonstrate that the proposed method outperforms the state-of-the-art and enables model generalization.

</details>

<details>

<summary>2020-08-19 13:40:11 - Neighborhood Sensitive Mapping for Zero-Shot Classification using Independently Learned Semantic Embeddings</summary>

- *Gaurav Singh, Fabrizio Silvestri, John Shawe-Taylor*

- `1605.08242v3` - [abs](http://arxiv.org/abs/1605.08242v3) - [pdf](http://arxiv.org/pdf/1605.08242v3)

> In a traditional setting, classifiers are trained to approximate a target function $f:X \rightarrow Y$ where at least a sample for each $y \in Y$ is presented to the training algorithm. In a zero-shot setting we have a subset of the labels $\hat{Y} \subset Y$ for which we do not observe any corresponding training instance. Still, the function $f$ that we train must be able to correctly assign labels also on $\hat{Y}$. In practice, zero-shot problems are very important especially when the label set is large and the cost of editorially label samples for all possible values in the label set might be prohibitively high. Most recent approaches to zero-shot learning are based on finding and exploiting relationships between labels using semantic embeddings. We show in this paper that semantic embeddings, despite being very good at capturing relationships between labels, are not very good at capturing the relationships among labels in a data-dependent manner. For this reason, we propose a novel two-step process for learning a zero-shot classifier. In the first step, we learn what we call a \emph{property embedding space} capturing the "\emph{learnable}" features of the label set. Then, we exploit the learned properties in order to reduce the generalization error for a linear nearest neighbor-based classifier.

</details>

<details>

<summary>2020-08-19 14:30:00 - SentiQ: A Probabilistic Logic Approach to Enhance Sentiment Analysis Tool Quality</summary>

- *Wissam Maamar Kouadri, Salima Benbernou, Mourad Ouziri, Themis Palpanas, Iheb Ben Amor*

- `2008.08919v1` - [abs](http://arxiv.org/abs/2008.08919v1) - [pdf](http://arxiv.org/pdf/2008.08919v1)

> The opinion expressed in various Web sites and social-media is an essential contributor to the decision making process of several organizations. Existing sentiment analysis tools aim to extract the polarity (i.e., positive, negative, neutral) from these opinionated contents. Despite the advance of the research in the field, sentiment analysis tools give \textit{inconsistent} polarities, which is harmful to business decisions. In this paper, we propose SentiQ, an unsupervised Markov logic Network-based approach that injects the semantic dimension in the tools through rules. It allows to detect and solve inconsistencies and then improves the overall accuracy of the tools. Preliminary experimental results demonstrate the usefulness of SentiQ.

</details>

<details>

<summary>2020-08-19 16:47:15 - UoB at SemEval-2020 Task 12: Boosting BERT with Corpus Level Information</summary>

- *Wah Meng Lim, Harish Tayyar Madabushi*

- `2008.08547v1` - [abs](http://arxiv.org/abs/2008.08547v1) - [pdf](http://arxiv.org/pdf/2008.08547v1)

> Pre-trained language model word representation, such as BERT, have been extremely successful in several Natural Language Processing tasks significantly improving on the state-of-the-art. This can largely be attributed to their ability to better capture semantic information contained within a sentence. Several tasks, however, can benefit from information available at a corpus level, such as Term Frequency-Inverse Document Frequency (TF-IDF). In this work we test the effectiveness of integrating this information with BERT on the task of identifying abuse on social media and show that integrating this information with BERT does indeed significantly improve performance. We participate in Sub-Task A (abuse detection) wherein we achieve a score within two points of the top performing team and in Sub-Task B (target detection) wherein we are ranked 4 of the 44 participating teams.

</details>

<details>

<summary>2020-08-19 17:37:02 - Toward Automated Quest Generation in Text-Adventure Games</summary>

- *Prithviraj Ammanabrolu, William Broniec, Alex Mueller, Jeremy Paul, Mark O. Riedl*

- `1909.06283v4` - [abs](http://arxiv.org/abs/1909.06283v4) - [pdf](http://arxiv.org/pdf/1909.06283v4)

> Interactive fictions, or text-adventures, are games in which a player interacts with a world entirely through textual descriptions and text actions. Text-adventure games are typically structured as puzzles or quests wherein the player must execute certain actions in a certain order to succeed. In this paper, we consider the problem of procedurally generating a quest, defined as a series of actions required to progress towards a goal, in a text-adventure game. Quest generation in text environments is challenging because they must be semantically coherent. We present and evaluate two quest generation techniques: (1) a Markov model, and (2) a neural generative model. We specifically look at generating quests about cooking and train our models on recipe data. We evaluate our techniques with human participant studies looking at perceived creativity and coherence.

</details>

<details>

<summary>2020-08-19 17:38:36 - MIMIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for MIMIC-III</summary>

- *Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Michael C. Hughes, Tristan Naumann, Marzyeh Ghassemi*

- `1907.08322v2` - [abs](http://arxiv.org/abs/1907.08322v2) - [pdf](http://arxiv.org/pdf/1907.08322v2)

> Robust machine learning relies on access to data that can be used with standardized frameworks in important tasks and the ability to develop models whose performance can be reasonably reproduced. In machine learning for healthcare, the community faces reproducibility challenges due to a lack of publicly accessible data and a lack of standardized data processing frameworks. We present MIMIC-Extract, an open-source pipeline for transforming raw electronic health record (EHR) data for critical care patients contained in the publicly-available MIMIC-III database into dataframes that are directly usable in common machine learning pipelines. MIMIC-Extract addresses three primary challenges in making complex health records data accessible to the broader machine learning community. First, it provides standardized data processing functions, including unit conversion, outlier detection, and aggregating semantically equivalent features, thus accounting for duplication and reducing missingness. Second, it preserves the time series nature of clinical data and can be easily integrated into clinically actionable prediction tasks in machine learning for health. Finally, it is highly extensible so that other researchers with related questions can easily use the same pipeline. We demonstrate the utility of this pipeline by showcasing several benchmark tasks and baseline results.

</details>

<details>

<summary>2020-08-19 19:58:14 - Information Leakage in Embedding Models</summary>

- *Congzheng Song, Ananth Raghunathan*

- `2004.00053v2` - [abs](http://arxiv.org/abs/2004.00053v2) - [pdf](http://arxiv.org/pdf/2004.00053v2)

> Embeddings are functions that map raw input data to low-dimensional vector representations, while preserving important semantic information about the inputs. Pre-training embeddings on a large amount of unlabeled data and fine-tuning them for downstream tasks is now a de facto standard in achieving state of the art learning in many domains.   We demonstrate that embeddings, in addition to encoding generic semantics, often also present a vector that leaks sensitive information about the input data. We develop three classes of attacks to systematically study information that might be leaked by embeddings. First, embedding vectors can be inverted to partially recover some of the input data. As an example, we show that our attacks on popular sentence embeddings recover between 50\%--70\% of the input words (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive attributes inherent in inputs and independent of the underlying semantic task at hand. Attributes such as authorship of text can be easily extracted by training an inference model on just a handful of labeled embedding vectors. Third, embedding models leak moderate amount of membership information for infrequent training data inputs. We extensively evaluate our attacks on various state-of-the-art embedding models in the text domain. We also propose and evaluate defenses that can prevent the leakage to some extent at a minor cost in utility.

</details>

<details>

<summary>2020-08-19 20:58:27 - Top2Vec: Distributed Representations of Topics</summary>

- *Dimo Angelov*

- `2008.09470v1` - [abs](http://arxiv.org/abs/2008.09470v1) - [pdf](http://arxiv.org/pdf/2008.09470v1)

> Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present $\texttt{top2vec}$, which leverages joint document and word semantic embedding to find $\textit{topic vectors}$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that $\texttt{top2vec}$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.

</details>

<details>

<summary>2020-08-20 15:11:11 - A stabilized finite element method for delamination analysis of composites using cohesive elements</summary>

- *Gourab Ghosh, Ravindra Duddu, Chandrasekhar Annavarapu*

- `2008.09015v1` - [abs](http://arxiv.org/abs/2008.09015v1) - [pdf](http://arxiv.org/pdf/2008.09015v1)

> We demonstrate the ability of a stabilized finite element method, inspired by the weighted Nitsche approach, to alleviate spurious traction oscillations at interlaminar interfaces in multi-ply multi-directional composite laminates. In contrast with the standard (penalty-like) method, the stabilized method allows the use of arbitrarily large values of cohesive stiffness and obviates the need for engineering approaches to estimate minimum cohesive stiffness necessary for accurate delamination analysis. This is achieved by defining a weighted interface traction in the stabilized method, which allows a gradual transition from penalty-like method for soft elastic contact to Nitsche-like method for rigid contact. We conducted several simulation studies involving constant strain patch tests and benchmark delamination tests under mode-I, mode-II and mixed-mode loadings. Our results show clear evidence of traction oscillations with the standard method with structured and perturbed finite element meshes, and that the stabilized method alleviates these oscillations, thus illustrating its robustness.

</details>

<details>

<summary>2020-08-20 15:14:18 - LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification</summary>

- *Sopan Khosla, Rishabh Joshi, Ritam Dutt, Alan W Black, Yulia Tsvetkov*

- `2008.04820v2` - [abs](http://arxiv.org/abs/2008.04820v2) - [pdf](http://arxiv.org/pdf/2008.04820v2)

> In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The "multi-granular" model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge and attains $4^{th}$ position on the test leaderboard. Our final model and code is released at https://github.com/sopu/PropagandaSemEval2020.

</details>

<details>

<summary>2020-08-20 17:33:08 - Contrastive Learning for Unpaired Image-to-Image Translation</summary>

- *Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu*

- `2007.15651v3` - [abs](http://arxiv.org/abs/2007.15651v3) - [pdf](http://arxiv.org/pdf/2007.15651v3)

> In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each "domain" is only a single image.

</details>

<details>

<summary>2020-08-21 01:21:59 - Describing Console I/O Behavior for Testing Student Submissions in Haskell</summary>

- *Oliver Westphal, Janis Voigtländer*

- `2008.09253v1` - [abs](http://arxiv.org/abs/2008.09253v1) - [pdf](http://arxiv.org/pdf/2008.09253v1)

> We present a small, formal language for specifying the behavior of simple console I/O programs. The design is driven by the concrete application case of testing interactive Haskell programs written by students. Specifications are structurally similar to lexical analysis regular expressions, but are augmented with features like global variables that track state and history of program runs, enabling expression of an interesting range of dynamic behavior. We give a semantics for our specification language based on acceptance of execution traces. From this semantics we derive a definition of the set of all traces valid for a given specification. Sampling that set enables us to mechanically check program behavior against specifications in a probabilistic fashion. Beyond testing, other possible uses of the specification language in an education context include related activities like providing more helpful feedback, generating sample solutions, and even generating random exercise tasks.

</details>

<details>

<summary>2020-08-21 02:12:32 - Adapting Event Extractors to Medical Data: Bridging the Covariate Shift</summary>

- *Aakanksha Naik, Jill Lehman, Carolyn Rose*

- `2008.09266v1` - [abs](http://arxiv.org/abs/2008.09266v1) - [pdf](http://arxiv.org/pdf/2008.09266v1)

> We tackle the task of adapting event extractors to new domains without labeled data, by aligning the marginal distributions of source and target domains. As a testbed, we create two new event extraction datasets using English texts from two medical domains: (i) clinical notes, and (ii) doctor-patient conversations. We test the efficacy of three marginal alignment techniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive fine-tuning (DAFT), and (iii) a novel instance weighting technique based on language model likelihood scores (LIW). LIW and DAFT improve over a no-transfer BERT baseline on both domains, but ADA only improves on clinical notes. Deeper analysis of performance under different types of shifts (e.g., lexical shift, semantic shift) reveals interesting variations among models. Our best-performing models reach F1 scores of 70.0 and 72.9 on notes and conversations respectively, using no labeled data from target domains.

</details>

<details>

<summary>2020-08-21 05:30:48 - Explainable Recommender Systems via Resolving Learning Representations</summary>

- *Ninghao Liu, Yong Ge, Li Li, Xia Hu, Rui Chen, Soo-Hyun Choi*

- `2008.09316v1` - [abs](http://arxiv.org/abs/2008.09316v1) - [pdf](http://arxiv.org/pdf/2008.09316v1)

> Recommender systems play a fundamental role in web applications in filtering massive information and matching user interests. While many efforts have been devoted to developing more effective models in various scenarios, the exploration on the explainability of recommender systems is running behind. Explanations could help improve user experience and discover system defects. In this paper, after formally introducing the elements that are related to model explainability, we propose a novel explainable recommendation model through improving the transparency of the representation learning process. Specifically, to overcome the representation entangling problem in traditional models, we revise traditional graph convolution to discriminate information from different layers. Also, each representation vector is factorized into several segments, where each segment relates to one semantic aspect in data. Different from previous work, in our model, factor discovery and representation learning are simultaneously conducted, and we are able to handle extra attribute information and knowledge. In this way, the proposed model can learn interpretable and meaningful representations for users and items. Unlike traditional methods that need to make a trade-off between explainability and effectiveness, the performance of our proposed explainable model is not negatively affected after considering explainability. Finally, comprehensive experiments are conducted to validate the performance of our model as well as explanation faithfulness.

</details>

<details>

<summary>2020-08-21 07:26:55 - Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs</summary>

- *Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, Odej Kao*

- `2008.09340v1` - [abs](http://arxiv.org/abs/2008.09340v1) - [pdf](http://arxiv.org/pdf/2008.09340v1)

> The detection of anomalies is essential mining task for the security and reliability in computer systems. Logs are a common and major data source for anomaly detection methods in almost every computer system. They collect a range of significant events describing the runtime system status. Recent studies have focused predominantly on one-class deep learning methods on predefined non-learnable numerical log representations. The main limitation is that these models are not able to learn log representations describing the semantic differences between normal and anomaly logs, leading to a poor generalization of unseen logs. We propose Logsy, a classification-based method to learn log representations in a way to distinguish between normal data from the system of interest and anomaly samples from auxiliary log datasets, easily accessible via the internet. The idea behind such an approach to anomaly detection is that the auxiliary dataset is sufficiently informative to enhance the representation of the normal data, yet diverse to regularize against overfitting and improve generalization. We propose an attention-based encoder model with a new hyperspherical loss function. This enables learning compact log representations capturing the intrinsic differences between normal and anomaly logs. Empirically, we show an average improvement of 0.25 in the F1 score, compared to the previous methods. To investigate the properties of Logsy, we perform additional experiments including evaluation of the effect of the auxiliary data size, the influence of expert knowledge, and the quality of the learned log representations. The results show that the learned representation boost the performance of the previous methods such as PCA with a relative improvement of 28.2%.

</details>

<details>

<summary>2020-08-21 09:52:58 - Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation</summary>

- *Wenxian Shi, Hao Zhou, Ning Miao, Lei Li*

- `1906.06719v4` - [abs](http://arxiv.org/abs/1906.06719v4) - [pdf](http://arxiv.org/pdf/1906.06719v4)

> Deep generative models are commonly used for generating images and text. Interpretability of these models is one important pursuit, other than the generation quality. Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable. To enhance the controllability and interpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GM-VAE), whose mixture components could be related to hidden semantic aspects of data. In this paper, we generalize the practice and introduce DEM-VAE, a class of models for text generation using VAEs with a mixture distribution of exponential family. Unfortunately, a standard variational training algorithm fails due to the mode-collapse problem. We theoretically identify the root cause of the problem and propose an effective algorithm to train DEM-VAE. Our method penalizes the training with an extra dispersion term to induce a well-structured latent space. Experimental results show that our approach does obtain a meaningful space, and it outperforms strong baselines in text generation benchmarks. The code is available at https://github.com/wenxianxian/demvae.

</details>

<details>

<summary>2020-08-21 10:43:18 - KPRNet: Improving projection-based LiDAR semantic segmentation</summary>

- *Deyvid Kochanov, Fatemeh Karimi Nejadasl, Olaf Booij*

- `2007.12668v2` - [abs](http://arxiv.org/abs/2007.12668v2) - [pdf](http://arxiv.org/pdf/2007.12668v2)

> Semantic segmentation is an important component in the perception systems of autonomous vehicles. In this work, we adopt recent advances in both image and point cloud segmentation to achieve a better accuracy in the task of segmenting LiDAR scans. KPRNet improves the convolutional neural network architecture of 2D projection methods and utilizes KPConv to replace the commonly used post-processing techniques with a learnable point-wise component which allows us to obtain more accurate 3D labels. With these improvements our model outperforms the current best method on the SemanticKITTI benchmark, reaching an mIoU of 63.1.

</details>

<details>

<summary>2020-08-21 14:43:43 - BLONDiE: Blockchain Ontology with Dynamic Extensibility</summary>

- *Ugarte-Rojas Hector, Chullo-Llave Boris*

- `2008.09518v1` - [abs](http://arxiv.org/abs/2008.09518v1) - [pdf](http://arxiv.org/pdf/2008.09518v1)

> There are thousands of projects worldwide based primarily on blockchain technology. These have a large number of users and hundreds of use cases. One of the most popular is the use of cryptocurrencies and their benefits against money without intrinsic value (fiat money) and centralized financial solutions. However, although thousands of new transactions are carried out daily in different platforms, uniform and standardized information does not exist to be able to manage the large amount of data that is generated and exchanged between users through transactions and the generation of new blocks. This research reports the development of BLONDiE, an ontology that allows the semantic representation of knowledge to describe the native structure and related information of the three most relevant blockchain projects to date: Bitcoin, Ethereum and in the recent 1.0 version extends its definitions to include Hyperledger, specifically the Hyperledger Fabric infrastructure. Its use allows having common data formats of different platforms for further processing, such as the execution of semantic queries.

</details>

<details>

<summary>2020-08-22 01:35:25 - Seasonal-adjustment Based Feature Selection Method for Large-scale Search Engine Logs</summary>

- *Thien Q. Tran, Jun Sakuma*

- `2008.09727v1` - [abs](http://arxiv.org/abs/2008.09727v1) - [pdf](http://arxiv.org/pdf/2008.09727v1)

> Search engine logs have a great potential in tracking and predicting outbreaks of infectious disease. More precisely, one can use the search volume of some search terms to predict the infection rate of an infectious disease in nearly real-time. However, conducting accurate and stable prediction of outbreaks using search engine logs is a challenging task due to the following two-way instability characteristics of the search logs. First, the search volume of a search term may change irregularly in the short-term, for example, due to environmental factors such as the amount of media or news. Second, the search volume may also change in the long-term due to the demographic change of the search engine. That is to say, if a model is trained with such search logs with ignoring such characteristic, the resulting prediction would contain serious mispredictions when these changes occur.   In this work, we proposed a novel feature selection method to overcome this instability problem. In particular, we employ a seasonal-adjustment method that decomposes each time series into three components: seasonal, trend and irregular component and build prediction models for each component individually. We also carefully design a feature selection method to select proper search terms to predict each component. We conducted comprehensive experiments on ten different kinds of infectious diseases. The experimental results show that the proposed method outperforms all comparative methods in prediction accuracy for seven of ten diseases, in both now-casting and forecasting setting. Also, the proposed method is more successful in selecting search terms that are semantically related to target diseases.

</details>

<details>

<summary>2020-08-22 03:46:12 - Multidomain Multimodal Fusion For Human Action Recognition Using Inertial Sensors</summary>

- *Zeeshan Ahmad, Naimul Khan*

- `2008.09748v1` - [abs](http://arxiv.org/abs/2008.09748v1) - [pdf](http://arxiv.org/pdf/2008.09748v1)

> One of the major reasons for misclassification of multiplex actions during action recognition is the unavailability of complementary features that provide the semantic information about the actions. In different domains these features are present with different scales and intensities. In existing literature, features are extracted independently in different domains, but the benefits from fusing these multidomain features are not realized. To address this challenge and to extract complete set of complementary information, in this paper, we propose a novel multidomain multimodal fusion framework that extracts complementary and distinct features from different domains of the input modality. We transform input inertial data into signal images, and then make the input modality multidomain and multimodal by transforming spatial domain information into frequency and time-spectrum domain using Discrete Fourier Transform (DFT) and Gabor wavelet transform (GWT) respectively. Features in different domains are extracted by Convolutional Neural networks (CNNs) and then fused by Canonical Correlation based Fusion (CCF) for improving the accuracy of human action recognition. Experimental results on three inertial datasets show the superiority of the proposed method when compared to the state-of-the-art.

</details>

<details>

<summary>2020-08-22 08:04:21 - FAT ALBERT: Finding Answers in Large Texts using Semantic Similarity Attention Layer based on BERT</summary>

- *Omar Mossad, Amgad Ahmed, Anandharaju Raju, Hari Karthikeyan, Zayed Ahmed*

- `2009.01004v1` - [abs](http://arxiv.org/abs/2009.01004v1) - [pdf](http://arxiv.org/pdf/2009.01004v1)

> Machine based text comprehension has always been a significant research field in natural language processing. Once a full understanding of the text context and semantics is achieved, a deep learning model can be trained to solve a large subset of tasks, e.g. text summarization, classification and question answering. In this paper we focus on the question answering problem, specifically the multiple choice type of questions. We develop a model based on BERT, a state-of-the-art transformer network. Moreover, we alleviate the ability of BERT to support large text corpus by extracting the highest influence sentences through a semantic similarity model. Evaluations of our proposed model demonstrate that it outperforms the leading models in the MovieQA challenge and we are currently ranked first in the leader board with test accuracy of 87.79%. Finally, we discuss the model shortcomings and suggest possible improvements to overcome these limitations.

</details>

<details>

<summary>2020-08-23 01:43:46 - Graph Convolutional Networks Reveal Neural Connections Encoding Prosthetic Sensation</summary>

- *Vivek Subramanian, Joshua Khani*

- `2009.03272v1` - [abs](http://arxiv.org/abs/2009.03272v1) - [pdf](http://arxiv.org/pdf/2009.03272v1)

> Extracting stimulus features from neuronal ensembles is of great interest to the development of neuroprosthetics that project sensory information directly to the brain via electrical stimulation. Machine learning strategies that optimize stimulation parameters as the subject learns to interpret the artificial input could improve device efficacy, increase prosthetic performance, ensure stability of evoked sensations, and improve power consumption by eliminating extraneous input. Recent advances extending deep learning techniques to non-Euclidean graph data provide a novel approach to interpreting neuronal spiking activity. For this study, we apply graph convolutional networks (GCNs) to infer the underlying functional relationship between neurons that are involved in the processing of artificial sensory information. Data was collected from a freely behaving rat using a four infrared (IR) sensor, ICMS-based neuroprosthesis to localize IR light sources. We use GCNs to predict the stimulation frequency across four stimulating channels in the prosthesis, which encode relative distance and directional information to an IR-emitting reward port. Our GCN model is able to achieve a peak performance of 73.5% on a modified ordinal regression performance metric in a multiclass classification problem consisting of 7 classes, where chance is 14.3%. Additionally, the inferred adjacency matrix provides a adequate representation of the underlying neural circuitry encoding the artificial sensation.

</details>

<details>

<summary>2020-08-23 21:41:43 - Robust Vision Challenge 2020 -- 1st Place Report for Panoptic Segmentation</summary>

- *Rohit Mohan, Abhinav Valada*

- `2008.10112v1` - [abs](http://arxiv.org/abs/2008.10112v1) - [pdf](http://arxiv.org/pdf/2008.10112v1)

> In this technical report, we present key details of our winning panoptic segmentation architecture EffPS_b1bs4_RVC. Our network is a lightweight version of our state-of-the-art EfficientPS architecture that consists of our proposed shared backbone with a modified EfficientNet-B5 model as the encoder, followed by the 2-way FPN to learn semantically rich multi-scale features. It consists of two task-specific heads, a modified Mask R-CNN instance head and our novel semantic segmentation head that processes features of different scales with specialized modules for coherent feature refinement. Finally, our proposed panoptic fusion module adaptively fuses logits from each of the heads to yield the panoptic segmentation output. The Robust Vision Challenge 2020 benchmarking results show that our model is ranked #1 on Microsoft COCO, VIPER and WildDash, and is ranked #2 on Cityscapes and Mapillary Vistas, thereby achieving the overall rank #1 for the panoptic segmentation task.

</details>

<details>

<summary>2020-08-24 09:37:45 - Cross-lingual Semantic Role Labeling with Model Transfer</summary>

- *Hao Fei, Meishan Zhang, Fei Li, Donghong Ji*

- `2008.10284v1` - [abs](http://arxiv.org/abs/2008.10284v1) - [pdf](http://arxiv.org/pdf/2008.10284v1)

> Prior studies show that cross-lingual semantic role labeling (SRL) can be achieved by model transfer under the help of universal features. In this paper, we fill the gap of cross-lingual SRL by proposing an end-to-end SRL model that incorporates a variety of universal features and transfer methods. We study both the bilingual transfer and multi-source transfer, under gold or machine-generated syntactic inputs, pre-trained high-order abstract features, and contextualized multilingual word representations. Experimental results on the Universal Proposition Bank corpus indicate that performances of the cross-lingual SRL can vary by leveraging different cross-lingual features. In addition, whether the features are gold-standard also has an impact on performances. Precisely, we find that gold syntax features are much more crucial for cross-lingual SRL, compared with the automatically-generated ones. Moreover, universal dependency structure features are able to give the best help, and both pre-trained high-order features and contextualized word representations can further bring significant improvements.

</details>

<details>

<summary>2020-08-24 13:02:09 - Universal Semantic Segmentation for Fisheye Urban Driving Images</summary>

- *Yaozu Ye, Kailun Yang, Kaite Xiang, Juan Wang, Kaiwei Wang*

- `2002.03736v2` - [abs](http://arxiv.org/abs/2002.03736v2) - [pdf](http://arxiv.org/pdf/2002.03736v2)

> Semantic segmentation is a critical method in the field of autonomous driving. When performing semantic image segmentation, a wider field of view (FoV) helps to obtain more information about the surrounding environment, making automatic driving safer and more reliable, which could be offered by fisheye cameras. However, large public fisheye datasets are not available, and the fisheye images captured by the fisheye camera with large FoV comes with large distortion, so commonly-used semantic segmentation model cannot be directly utilized. In this paper, a seven degrees of freedom (DoF) augmentation method is proposed to transform rectilinear image to fisheye image in a more comprehensive way. In the training process, rectilinear images are transformed into fisheye images in seven DoF, which simulates the fisheye images taken by cameras of different positions, orientations and focal lengths. The result shows that training with the seven-DoF augmentation can improve the model's accuracy and robustness against different distorted fisheye data. This seven-DoF augmentation provides a universal semantic segmentation solution for fisheye cameras in different autonomous driving applications. Also, we provide specific parameter settings of the augmentation for autonomous driving. At last, we tested our universal semantic segmentation model on real fisheye images and obtained satisfactory results. The code and configurations are released at https://github.com/Yaozhuwa/FisheyeSeg.

</details>

<details>

<summary>2020-08-24 19:26:54 - Countering Language Drift with Seeded Iterated Learning</summary>

- *Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, Aaron Courville*

- `2003.12694v3` - [abs](http://arxiv.org/abs/2003.12694v3) - [pdf](http://arxiv.org/pdf/2003.12694v3)

> Pretraining on human corpus and then finetuning in a simulator has become a standard pipeline for training a goal-oriented dialogue agent. Nevertheless, as soon as the agents are finetuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we propose a generic approach to counter language drift called Seeded iterated learning (SIL). We periodically refine a pretrained student agent by imitating data sampled from a newly generated teacher agent. At each time step, the teacher is created by copying the student agent, before being finetuned to maximize task completion. SIL does not require external syntactic constraint nor semantic knowledge, making it a valuable task-agnostic finetuning protocol. We evaluate SIL in a toy-setting Lewis Game, and then scale it up to the translation game with natural language. In both settings, SIL helps counter language drift as well as it improves the task completion compared to baselines.

</details>

<details>

<summary>2020-08-24 22:32:30 - LULC Segmentation of RGB Satellite Image Using FCN-8</summary>

- *Abu Bakar Siddik Nayem, Anis Sarker, Ovi Paul, Amin Ali, Md. Ashraful Amin, AKM Mahbubur Rahman*

- `2008.10736v1` - [abs](http://arxiv.org/abs/2008.10736v1) - [pdf](http://arxiv.org/pdf/2008.10736v1)

> This work presents use of Fully Convolutional Network (FCN-8) for semantic segmentation of high-resolution RGB earth surface satel-lite images into land use land cover (LULC) categories. Specically, we propose a non-overlapping grid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16 weights to segment satellite im-ages into four (forest, built-up, farmland and water) classes. The FCN-8 semantically projects the discriminating features in lower resolution learned by the encoder onto the pixel space in higher resolution to get a dense classi cation. We experimented the proposed system with Gaofen-2 image dataset, that contains 150 images of over 60 di erent cities in china. For comparison, we used available ground-truth along with images segmented using a widely used commeriial GIS software called eCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8 obtains signi cantly improved performance, than the eCognition soft-ware. Our model achieves average accuracy of 91.0% and average Inter-section over Union (IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is 0.60. This paper also reports a detail analysis of errors occurred at the LULC boundary.

</details>

<details>

<summary>2020-08-25 07:32:09 - TabSim: A Siamese Neural Network for Accurate Estimation of Table Similarity</summary>

- *Maryam Habibi, Johannes Starlinger, Ulf Leser*

- `2008.10856v1` - [abs](http://arxiv.org/abs/2008.10856v1) - [pdf](http://arxiv.org/pdf/2008.10856v1)

> Tables are a popular and efficient means of presenting structured information. They are used extensively in various kinds of documents including web pages. Tables display information as a two-dimensional matrix, the semantics of which is conveyed by a mixture of structure (rows, columns), headers, caption, and content. Recent research has started to consider tables as first class objects, not just as an addendum to texts, yielding interesting results for problems like table matching, table completion, or value imputation. All of these problems inherently rely on an accurate measure for the semantic similarity of two tables. We present TabSim, a novel method to compute table similarity scores using deep neural networks. Conceptually, TabSim represents a table as a learned concatenation of embeddings of its caption, its content, and its structure. Given two tables in this representation, a Siamese neural network is trained to compute a score correlating with the tables' semantic similarity. To train and evaluate our method, we created a gold standard corpus consisting of 1500 table pairs extracted from biomedical articles and manually scored regarding their degree of similarity, and adopted two other corpora originally developed for a different yet similar task. Our evaluation shows that TabSim outperforms other table similarity measures on average by app. 7% pp F1-score in a binary similarity classification setting and by app. 1.5% pp in a ranking scenario.

</details>

<details>

<summary>2020-08-25 15:46:13 - Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</summary>

- *Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, Zheng Zhang*

- `1909.01315v2` - [abs](http://arxiv.org/abs/1909.01315v2) - [pdf](http://arxiv.org/pdf/1909.01315v2)

> Advancing research in the emerging field of deep graph learning requires new tools to support tensor computation over graphs. In this paper, we present the design principles and implementation of Deep Graph Library (DGL). DGL distills the computational patterns of GNNs into a few generalized sparse tensor operations suitable for extensive parallelization. By advocating graph as the central programming abstraction, DGL can perform optimizations transparently. By cautiously adopting a framework-neutral design, DGL allows users to easily port and leverage the existing components across multiple deep learning frameworks. Our evaluation shows that DGL significantly outperforms other popular GNN-oriented frameworks in both speed and memory consumption over a variety of benchmarks and has little overhead for small scale workloads.

</details>

<details>

<summary>2020-08-25 17:59:53 - Learning to Learn in a Semi-Supervised Fashion</summary>

- *Yun-Chun Chen, Chao-Te Chou, Yu-Chiang Frank Wang*

- `2008.11203v1` - [abs](http://arxiv.org/abs/2008.11203v1) - [pdf](http://arxiv.org/pdf/2008.11203v1)

> To address semi-supervised learning from both labeled and unlabeled data, we present a novel meta-learning scheme. We particularly consider that labeled and unlabeled data share disjoint ground truth label sets, which can be seen tasks like in person re-identification or image retrieval. Our learning scheme exploits the idea of leveraging information from labeled to unlabeled data. Instead of fitting the associated class-wise similarity scores as most meta-learning algorithms do, we propose to derive semantics-oriented similarity representations from labeled data, and transfer such representation to unlabeled ones. Thus, our strategy can be viewed as a self-supervised learning scheme, which can be applied to fully supervised learning tasks for improved performance. Our experiments on various tasks and settings confirm the effectiveness of our proposed approach and its superiority over the state-of-the-art methods.

</details>

<details>

<summary>2020-08-25 22:28:14 - Concept Extraction Using Pointer-Generator Networks</summary>

- *Alexander Shvets, Leo Wanner*

- `2008.11295v1` - [abs](http://arxiv.org/abs/2008.11295v1) - [pdf](http://arxiv.org/pdf/2008.11295v1)

> Concept extraction is crucial for a number of downstream applications. However, surprisingly enough, straightforward single token/nominal chunk-concept alignment or dictionary lookup techniques such as DBpedia Spotlight still prevail. We propose a generic open-domain OOV-oriented extractive model that is based on distant supervision of a pointer-generator network leveraging bidirectional LSTMs and a copy mechanism. The model has been trained on a large annotated corpus compiled specifically for this task from 250K Wikipedia pages, and tested on regular pages, where the pointers to other pages are considered as ground truth concepts. The outcome of the experiments shows that our model significantly outperforms standard techniques and, when used on top of DBpedia Spotlight, further improves its performance. The experiments furthermore show that the model can be readily ported to other datasets on which it equally achieves a state-of-the-art performance.

</details>

<details>

<summary>2020-08-25 22:33:24 - CODIT: Code Editing with Tree-Based Neural Models</summary>

- *Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, Baishakhi Ray*

- `1810.00314v3` - [abs](http://arxiv.org/abs/1810.00314v3) - [pdf](http://arxiv.org/pdf/1810.00314v3)

> The way developers edit day-to-day code tends to be repetitive, often using existing code elements. Many researchers have tried to automate repetitive code changes by learning from specific change templates which are applied to limited scope. The advancement of deep neural networks and the availability of vast open-source evolutionary data opens up the possibility of automatically learning those templates from the wild. However, deep neural network based modeling for code changes and code in general introduces some specific problems that needs specific attention from research community. For instance, compared to natural language, source code vocabulary can be significantly larger. Further, good changes in code do not break its syntactic structure. Thus, deploying state-of-the-art neural network models without adapting the methods to the source code domain yields sub-optimal results. To this end, we propose a novel tree-based neural network system to model source code changes and learn code change patterns from the wild. Specifically, we propose a tree-based neural machine translation model to learn the probability distribution of changes in code. We realize our model with a change suggestion engine, CODIT, and train the model with more than 24k real-world changes and evaluate it on 5k patches. Our evaluation shows the effectiveness of CODITin learning and suggesting patches. CODIT can also learn specific bug fix pattern from bug fixing patches and can fix 25 bugs out of 80 bugs in Defects4J.

</details>

<details>

<summary>2020-08-25 23:30:57 - Neuro-Symbolic Visual Reasoning: Disentangling "Visual" from "Reasoning"</summary>

- *Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang, Kazuhito Koishida*

- `2006.11524v3` - [abs](http://arxiv.org/abs/2006.11524v3) - [pdf](http://arxiv.org/pdf/2006.11524v3)

> Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. However, recent advances in this area are still primarily driven by perception improvements (e.g. scene graph generation) rather than reasoning. Neuro-symbolic models such as Neural Module Networks bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own. To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this framework is used to perform in-depth, disentangled comparisons between well-known VQA models leading to informative insights regarding the participating models as well as the task.

</details>

<details>

<summary>2020-08-26 06:37:43 - Joint Modelling of Cyber Activities and Physical Context to Improve Prediction of Visitor Behaviors</summary>

- *Manpreet Kaur, Flora D. Salim, Yongli Ren, Jeffrey Chan, Martin Tomko, Mark Sanderson*

- `2008.11400v1` - [abs](http://arxiv.org/abs/2008.11400v1) - [pdf](http://arxiv.org/pdf/2008.11400v1)

> This paper investigates the Cyber-Physical behavior of users in a large indoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and browsing logs recorded by the mall operators. Our analysis shows that many users exhibit a high correlation between their cyber activities and their physical context. To find this correlation, we propose a mechanism to semantically label a physical space with rich categorical information from DBPedia concepts and compute a contextual similarity that represents a user's activities with the mall context. We demonstrate the application of cyber-physical contextual similarity in two situations: user visit intent classification and future location prediction. The experimental results demonstrate that exploitation of contextual similarity significantly improves the accuracy of such applications.

</details>

<details>

<summary>2020-08-26 09:05:40 - Training Multimodal Systems for Classification with Multiple Objectives</summary>

- *Jason Armitage, Shramana Thakur, Rishi Tripathi, Jens Lehmann, Maria Maleshkova*

- `2008.11450v1` - [abs](http://arxiv.org/abs/2008.11450v1) - [pdf](http://arxiv.org/pdf/2008.11450v1)

> We learn about the world from a diverse range of sensory information. Automated systems lack this ability as investigation has centred on processing information presented in a single form. Adapting architectures to learn from multiple modalities creates the potential to learn rich representations of the world - but current multimodal systems only deliver marginal improvements on unimodal approaches. Neural networks learn sampling noise during training with the result that performance on unseen data is degraded. This research introduces a second objective over the multimodal fusion process learned with variational inference. Regularisation methods are implemented in the inner training loop to control variance and the modular structure stabilises performance as additional neurons are added to layers. This framework is evaluated on a multilabel classification task with textual and visual inputs to demonstrate the potential for multiple objectives and probabilistic methods to lower variance and improve generalisation.

</details>

<details>

<summary>2020-08-26 11:01:13 - Machine learning approach of Japanese composition scoring and writing aided system's design</summary>

- *Wanhong Huang*

- `2008.11488v1` - [abs](http://arxiv.org/abs/2008.11488v1) - [pdf](http://arxiv.org/pdf/2008.11488v1)

> Automatic scoring system is extremely complex for any language. Because natural language itself is a complex model. When we evaluate articles generated by natural language, we need to view the articles from many dimensions such as word features, grammatical features, semantic features, text structure and so on. Even human beings sometimes can't accurately grade a composition because different people have different opinions about the same article. But a composition scoring system can greatly assist language learners. It can make language leaner improve themselves in the process of output something. Though it is still difficult for machines to directly evaluate a composition at the semantic and pragmatic levels, especially for Japanese, Chinese and other language in high context cultures, we can make machine evaluate a passage in word and grammar levels, which can as an assistance of composition rater or language learner. Especially for foreign language learners, lexical and syntactic content are usually what they are more concerned about. In our experiments, we did the follows works: 1) We use word segmentation tools and dictionaries to achieve word segmentation of an article, and extract word features, as well as generate a words' complexity feature of an article. And Bow technique are used to extract the theme features. 2) We designed a Turing-complete automata model and create 300+ automatons for the grammars that appear in the JLPT examination. And extract grammars features by using these automatons. 3) We propose a statistical approach for scoring a specify theme of composition, the final score will depend on all the writings that submitted to the system. 4) We design an grammar hint function for language leaner, so that they can know currently what grammars they can use.

</details>

<details>

<summary>2020-08-26 20:02:40 - Visual Concept Reasoning Networks</summary>

- *Taesup Kim, Sungwoong Kim, Yoshua Bengio*

- `2008.11783v1` - [abs](http://arxiv.org/abs/2008.11783v1) - [pdf](http://arxiv.org/pdf/2008.11783v1)

> A split-transform-merge strategy has been broadly used as an architectural constraint in convolutional neural networks for visual recognition tasks. It approximates sparsely connected networks by explicitly defining multiple branches to simultaneously learn representations with different visual concepts or properties. Dependencies or interactions between these representations are typically defined by dense and local operations, however, without any adaptiveness or high-level reasoning. In this work, we propose to exploit this strategy and combine it with our Visual Concept Reasoning Networks (VCRNet) to enable reasoning between high-level visual concepts. We associate each branch with a visual concept and derive a compact concept state by selecting a few local descriptors through an attention module. These concept states are then updated by graph-based interaction and used to adaptively modulate the local descriptors. We describe our proposed model by split-transform-attend-interact-modulate-merge stages, which are implemented by opting for a highly modularized architecture. Extensive experiments on visual recognition tasks such as image classification, semantic segmentation, object detection, scene recognition, and action recognition show that our proposed model, VCRNet, consistently improves the performance by increasing the number of parameters by less than 1%.

</details>

<details>

<summary>2020-08-27 05:55:43 - Relation Extraction with Self-determined Graph Convolutional Network</summary>

- *Sunil Kumar Sahu, Derek Thomas, Billy Chiu, Neha Sengupta, Mohammady Mahdy*

- `2008.00441v2` - [abs](http://arxiv.org/abs/2008.00441v2) - [pdf](http://arxiv.org/pdf/2008.00441v2)

> Relation Extraction is a way of obtaining the semantic relationship between entities in text. The state-of-the-art methods use linguistic tools to build a graph for the text in which the entities appear and then a Graph Convolutional Network (GCN) is employed to encode the pre-built graphs. Although their performance is promising, the reliance on linguistic tools results in a non end-to-end process. In this work, we propose a novel model, the Self-determined Graph Convolutional Network (SGCN), which determines a weighted graph using a self-attention mechanism, rather using any linguistic tool. Then, the self-determined graph is encoded using a GCN. We test our model on the TACRED dataset and achieve the state-of-the-art result. Our experiments show that SGCN outperforms the traditional GCN, which uses dependency parsing tools to build the graph.

</details>

<details>

<summary>2020-08-27 06:42:18 - Relation/Entity-Centric Reading Comprehension</summary>

- *Takeshi Onishi*

- `2008.11940v1` - [abs](http://arxiv.org/abs/2008.11940v1) - [pdf](http://arxiv.org/pdf/2008.11940v1)

> Constructing a machine that understands human language is one of the most elusive and long-standing challenges in artificial intelligence. This thesis addresses this challenge through studies of reading comprehension with a focus on understanding entities and their relationships. More specifically, we focus on question answering tasks designed to measure reading comprehension. We focus on entities and relations because they are typically used to represent the semantics of natural language.

</details>

<details>

<summary>2020-08-27 08:32:51 - Cloze Test Helps: Effective Video Anomaly Detection via Learning to Complete Video Events</summary>

- *Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin, Marius Kloft*

- `2008.11988v1` - [abs](http://arxiv.org/abs/2008.11988v1) - [pdf](http://arxiv.org/pdf/2008.11988v1)

> As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner. (2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-of-the-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.

</details>

<details>

<summary>2020-08-27 13:55:54 - M3: Semantic API Migrations</summary>

- *Bruce Collie, Philip Ginsbach, Jackson Woodruff, Ajitha Rajan, Michael O'Boyle*

- `2008.12118v1` - [abs](http://arxiv.org/abs/2008.12118v1) - [pdf](http://arxiv.org/pdf/2008.12118v1)

> Library migration is a challenging problem, where most existing approaches rely on prior knowledge. This can be, for example, information derived from changelogs or statistical models of API usage.   This paper addresses a different API migration scenario where there is no prior knowledge of the target library. We have no historical changelogs and no access to its internal representation. To tackle this problem, this paper proposes a novel approach (M$^3$), where probabilistic program synthesis is used to semantically model the behavior of library functions. Then, we use an SMT-based code search engine to discover similar code in user applications. These discovered instances provide potential locations for API migrations.   We evaluate our approach against 7 well-known libraries from varied application domains, learning correct implementations for 94 functions. Our approach is integrated with standard compiler tooling, and we use this integration to evaluate migration opportunities in 9 existing C/C++ applications with over 1MLoC. We discover over 7,000 instances of these functions, of which more than 2,000 represent migration opportunities.

</details>

<details>

<summary>2020-08-27 15:49:08 - Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation</summary>

- *Yu Chen, Lingfei Wu, Mohammed J. Zaki*

- `1908.04942v4` - [abs](http://arxiv.org/abs/1908.04942v4) - [pdf](http://arxiv.org/pdf/1908.04942v4)

> Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark.

</details>

<details>

<summary>2020-08-27 17:04:46 - DeepFake Detection Based on the Discrepancy Between the Face and its Context</summary>

- *Yuval Nirkin, Lior Wolf, Yosi Keller, Tal Hassner*

- `2008.12262v1` - [abs](http://arxiv.org/abs/2008.12262v1) - [pdf](http://arxiv.org/pdf/2008.12262v1)

> We propose a method for detecting face swapping and other identity manipulations in single images. Face swapping methods, such as DeepFake, manipulate the face region, aiming to adjust the face to the appearance of its context, while leaving the context unchanged. We show that this modus operandi produces discrepancies between the two regions. These discrepancies offer exploitable telltale signs of manipulation. Our approach involves two networks: (i) a face identification network that considers the face region bounded by a tight semantic segmentation, and (ii) a context recognition network that considers the face context (e.g., hair, ears, neck). We describe a method which uses the recognition signals from our two networks to detect such discrepancies, providing a complementary detection signal that improves conventional real vs. fake classifiers commonly used for detecting fake images. Our method achieves state of the art results on the FaceForensics++, Celeb-DF-v2, and DFDC benchmarks for face manipulation detection, and even generalizes to detect fakes produced by unseen methods.

</details>

<details>

<summary>2020-08-27 18:14:38 - Frame-To-Frame Consistent Semantic Segmentation</summary>

- *Manuel Rebol, Patrick Knöbelreiter*

- `2008.00948v3` - [abs](http://arxiv.org/abs/2008.00948v3) - [pdf](http://arxiv.org/pdf/2008.00948v3)

> In this work, we aim for temporally consistent semantic segmentation throughout frames in a video. Many semantic segmentation algorithms process images individually which leads to an inconsistent scene interpretation due to illumination changes, occlusions and other variations over time. To achieve a temporally consistent prediction, we train a convolutional neural network (CNN) which propagates features through consecutive frames in a video using a convolutional long short term memory (ConvLSTM) cell. Besides the temporal feature propagation, we penalize inconsistencies in our loss function. We show in our experiments that the performance improves when utilizing video information compared to single frame prediction. The mean intersection over union (mIoU) metric on the Cityscapes validation set increases from 45.2 % for the single frames to 57.9 % for video data after implementing the ConvLSTM to propagate features trough time on the ESPNet. Most importantly, inconsistency decreases from 4.5 % to 1.3 % which is a reduction by 71.1 %. Our results indicate that the added temporal information produces a frame-to-frame consistent and more accurate image understanding compared to single frame processing. Code and videos are available at https://github.com/mrebol/f2f-consistent-semantic-segmentation

</details>

<details>

<summary>2020-08-27 19:44:43 - A Federated Approach for Fine-Grained Classification of Fashion Apparel</summary>

- *Tejaswini Mallavarapu, Luke Cranfill, Junggab Son, Eun Hye Kim, Reza M. Parizi, John Morris*

- `2008.12350v1` - [abs](http://arxiv.org/abs/2008.12350v1) - [pdf](http://arxiv.org/pdf/2008.12350v1)

> As online retail services proliferate and are pervasive in modern lives, applications for classifying fashion apparel features from image data are becoming more indispensable. Online retailers, from leading companies to start-ups, can leverage such applications in order to increase profit margin and enhance the consumer experience. Many notable schemes have been proposed to classify fashion items, however, the majority of which focused upon classifying basic-level categories, such as T-shirts, pants, skirts, shoes, bags, and so forth. In contrast to most prior efforts, this paper aims to enable an in-depth classification of fashion item attributes within the same category. Beginning with a single dress, we seek to classify the type of dress hem, the hem length, and the sleeve length. The proposed scheme is comprised of three major stages: (a) localization of a target item from an input image using semantic segmentation, (b) detection of human key points (e.g., point of shoulder) using a pre-trained CNN and a bounding box, and (c) three phases to classify the attributes using a combination of algorithmic approaches and deep neural networks. The experimental results demonstrate that the proposed scheme is highly effective, with all categories having average precision of above 93.02%, and outperforms existing Convolutional Neural Networks (CNNs)-based schemes.

</details>

<details>

<summary>2020-08-28 02:03:18 - Topic, Sentiment and Impact Analysis: COVID19 Information Seeking on Social Media</summary>

- *Md Abul Bashar, Richi Nayak, Thirunavukarasu Balasubramaniam*

- `2008.12435v1` - [abs](http://arxiv.org/abs/2008.12435v1) - [pdf](http://arxiv.org/pdf/2008.12435v1)

> When people notice something unusual, they discuss it on social media. They leave traces of their emotions via text expressions. A systematic collection, analysis, and interpretation of social media data across time and space can give insights on local outbreaks, mental health, and social issues. Such timely insights can help in developing strategies and resources with an appropriate and efficient response. This study analysed a large Spatio-temporal tweet dataset of the Australian sphere related to COVID19. The methodology included a volume analysis, dynamic topic modelling, sentiment detection, and semantic brand score to obtain an insight on the COVID19 pandemic outbreak and public discussion in different states and cities of Australia over time. The obtained insights are compared with independently observed phenomena such as government reported instances.

</details>

<details>

<summary>2020-08-28 07:39:45 - An Intelligent CNN-VAE Text Representation Technology Based on Text Semantics for Comprehensive Big Data</summary>

- *Genggeng Liu, Canyang Guo, Lin Xie, Wenxi Liu, Naixue Xiong, Guolong Chen*

- `2008.12522v1` - [abs](http://arxiv.org/abs/2008.12522v1) - [pdf](http://arxiv.org/pdf/2008.12522v1)

> In the era of big data, a large number of text data generated by the Internet has given birth to a variety of text representation methods. In natural language processing (NLP), text representation transforms text into vectors that can be processed by computer without losing the original semantic information. However, these methods are difficult to effectively extract the semantic features among words and distinguish polysemy in language. Therefore, a text feature representation model based on convolutional neural network (CNN) and variational autoencoder (VAE) is proposed to extract the text features and apply the obtained text feature representation on the text classification tasks. CNN is used to extract the features of text vector to get the semantics among words and VAE is introduced to make the text feature space more consistent with Gaussian distribution. In addition, the output of the improved word2vec model is employed as the input of the proposed model to distinguish different meanings of the same word in different contexts. The experimental results show that the proposed model outperforms in k-nearest neighbor (KNN), random forest (RF) and support vector machine (SVM) classification algorithms.

</details>

<details>

<summary>2020-08-28 09:09:26 - Predicting conversions in display advertising based on URL embeddings</summary>

- *Yang Qiu, Nikolaos Tziortziotis, Martial Hue, Michalis Vazirgiannis*

- `2008.12003v2` - [abs](http://arxiv.org/abs/2008.12003v2) - [pdf](http://arxiv.org/pdf/2008.12003v2)

> Online display advertising is growing rapidly in recent years thanks to the automation of the ad buying process. Real-time bidding (RTB) allows the automated trading of ad impressions between advertisers and publishers through real-time auctions. In order to increase the effectiveness of their campaigns, advertisers should deliver ads to the users who are highly likely to be converted (i.e., purchase, registration, website visit, etc.) in the near future. In this study, we introduce and examine different models for estimating the probability of a user converting, given their history of visited URLs. Inspired by natural language processing, we introduce three URL embedding models to compute semantically meaningful URL representations. To demonstrate the effectiveness of the different proposed representation and conversion prediction models, we have conducted experiments on real logged events collected from an advertising platform.

</details>

<details>

<summary>2020-08-28 12:19:12 - Mapping Topic Evolution Across Poetic Traditions</summary>

- *Petr Plechac, Thomas N. Haider*

- `2006.15732v2` - [abs](http://arxiv.org/abs/2006.15732v2) - [pdf](http://arxiv.org/pdf/2006.15732v2)

> Poetic traditions across languages evolved differently, but we find that certain semantic topics occur in several of them, albeit sometimes with temporal delay, or with diverging trajectories over time. We apply Latent Dirichlet Allocation (LDA) to poetry corpora of four languages, i.e. German (52k poems), English (85k poems), Russian (18k poems), and Czech (80k poems). We align and interpret salient topics, their trend over time (1600--1925 A.D.), showing similarities and disparities across poetic traditions with a few select topics, and use their trajectories over time to pinpoint specific literary epochs.

</details>

<details>

<summary>2020-08-28 16:55:43 - Linked Credibility Reviews for Explainable Misinformation Detection</summary>

- *Ronald Denaux, Jose Manuel Gomez-Perez*

- `2008.12742v1` - [abs](http://arxiv.org/abs/2008.12742v1) - [pdf](http://arxiv.org/pdf/2008.12742v1)

> In recent years, misinformation on the Web has become increasingly rampant. The research community has responded by proposing systems and challenges, which are beginning to be useful for (various subtasks of) detecting misinformation. However, most proposed systems are based on deep learning techniques which are fine-tuned to specific domains, are difficult to interpret and produce results which are not machine readable. This limits their applicability and adoption as they can only be used by a select expert audience in very specific settings. In this paper we propose an architecture based on a core concept of Credibility Reviews (CRs) that can be used to build networks of distributed bots that collaborate for misinformation detection. The CRs serve as building blocks to compose graphs of (i) web content, (ii) existing credibility signals --fact-checked claims and reputation reviews of websites--, and (iii) automatically computed reviews. We implement this architecture on top of lightweight extensions to Schema.org and services providing generic NLP tasks for semantic similarity and stance detection. Evaluations on existing datasets of social-media posts, fake news and political speeches demonstrates several advantages over existing systems: extensibility, domain-independence, composability, explainability and transparency via provenance. Furthermore, we obtain competitive results without requiring finetuning and establish a new state of the art on the Clef'18 CheckThat! Factuality task.

</details>

<details>

<summary>2020-08-28 18:49:48 - One Weight Bitwidth to Rule Them All</summary>

- *Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chandra, Diana Marculescu*

- `2008.09916v2` - [abs](http://arxiv.org/abs/2008.09916v2) - [pdf](http://arxiv.org/pdf/2008.09916v2)

> Weight quantization for deep ConvNets has shown promising results for applications such as image classification and semantic segmentation and is especially important for applications where memory storage is limited. However, when aiming for quantization without accuracy degradation, different tasks may end up with different bitwidths. This creates complexity for software and hardware support and the complexity accumulates when one considers mixed-precision quantization, in which case each layer's weights use a different bitwidth. Our key insight is that optimizing for the least bitwidth subject to no accuracy degradation is not necessarily an optimal strategy. This is because one cannot decide optimality between two bitwidths if one has a smaller model size while the other has better accuracy. In this work, we take the first step to understand if some weight bitwidth is better than others by aligning all to the same model size using a width-multiplier. Under this setting, somewhat surprisingly, we show that using a single bitwidth for the whole network can achieve better accuracy compared to mixed-precision quantization targeting zero accuracy degradation when both have the same model size. In particular, our results suggest that when the number of channels becomes a target hyperparameter, a single weight bitwidth throughout the network shows superior results for model compression.

</details>

<details>

<summary>2020-08-28 23:06:23 - SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection</summary>

- *Dominik Schlechtweg, Barbara McGillivray, Simon Hengchen, Haim Dubossarsky, Nina Tahmasebi*

- `2007.11464v2` - [abs](http://arxiv.org/abs/2007.11464v2) - [pdf](http://arxiv.org/pdf/2007.11464v2)

> Lexical Semantic Change detection, i.e., the task of identifying words that change meaning over time, is a very active research area, with applications in NLP, lexicography, and linguistics. Evaluation is currently the most pressing problem in Lexical Semantic Change detection, as no gold standards are available to the community, which hinders progress. We present the results of the first shared task that addresses this gap by providing researchers with an evaluation framework and manually annotated, high-quality datasets for English, German, Latin, and Swedish. 33 teams submitted 186 systems, which were evaluated on two subtasks.

</details>

<details>

<summary>2020-08-29 12:10:23 - Driving Through Ghosts: Behavioral Cloning with False Positives</summary>

- *Andreas Bühler, Adrien Gaidon, Andrei Cramariuc, Rares Ambrus, Guy Rosman, Wolfram Burgard*

- `2008.12969v1` - [abs](http://arxiv.org/abs/2008.12969v1) - [pdf](http://arxiv.org/pdf/2008.12969v1)

> Safe autonomous driving requires robust detection of other traffic participants. However, robust does not mean perfect, and safe systems typically minimize missed detections at the expense of a higher false positive rate. This results in conservative and yet potentially dangerous behavior such as avoiding imaginary obstacles. In the context of behavioral cloning, perceptual errors at training time can lead to learning difficulties or wrong policies, as expert demonstrations might be inconsistent with the perceived world state. In this work, we propose a behavioral cloning approach that can safely leverage imperfect perception without being conservative. Our core contribution is a novel representation of perceptual uncertainty for learning to plan. We propose a new probabilistic birds-eye-view semantic grid to encode the noisy output of object perception systems. We then leverage expert demonstrations to learn an imitative driving policy using this probabilistic representation. Using the CARLA simulator, we show that our approach can safely overcome critical false positives that would otherwise lead to catastrophic failures or conservative behavior.

</details>

<details>

<summary>2020-08-29 17:49:01 - Dual Attention GANs for Semantic Image Synthesis</summary>

- *Hao Tang, Song Bai, Nicu Sebe*

- `2008.13024v1` - [abs](http://arxiv.org/abs/2008.13024v1) - [pdf](http://arxiv.org/pdf/2008.13024v1)

> In this paper, we focus on the semantic image synthesis task that aims at transferring semantic label maps to photo-realistic images. Existing methods lack effective semantic constraints to preserve the semantic information and ignore the structural correlations in both spatial and channel dimensions, leading to unsatisfactory blurry and artifact-prone results. To address these limitations, we propose a novel Dual Attention GAN (DAGAN) to synthesize photo-realistic and semantically-consistent images with fine details from the input layouts without imposing extra training overhead or modifying the network architectures of existing methods. We also propose two novel modules, i.e., position-wise Spatial Attention Module (SAM) and scale-wise Channel Attention Module (CAM), to capture semantic structure attention in spatial and channel dimensions, respectively. Specifically, SAM selectively correlates the pixels at each position by a spatial attention map, leading to pixels with the same semantic label being related to each other regardless of their spatial distances. Meanwhile, CAM selectively emphasizes the scale-wise features at each channel by a channel attention map, which integrates associated features among all channel maps regardless of their scales. We finally sum the outputs of SAM and CAM to further improve feature representation. Extensive experiments on four challenging datasets show that DAGAN achieves remarkably better results than state-of-the-art methods, while using fewer model parameters. The source code and trained models are available at https://github.com/Ha0Tang/DAGAN.

</details>

<details>

<summary>2020-08-30 13:52:24 - LIMSI_UPV at SemEval-2020 Task 9: Recurrent Convolutional Neural Network for Code-mixed Sentiment Analysis</summary>

- *Somnath Banerjee, Sahar Ghannay, Sophie Rosset, Anne Vilnat, Paolo Rosso*

- `2008.13173v1` - [abs](http://arxiv.org/abs/2008.13173v1) - [pdf](http://arxiv.org/pdf/2008.13173v1)

> This paper describes the participation of LIMSI UPV team in SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text. The proposed approach competed in SentiMix Hindi-English subtask, that addresses the problem of predicting the sentiment of a given Hindi-English code-mixed tweet. We propose Recurrent Convolutional Neural Network that combines both the recurrent neural network and the convolutional network to better capture the semantics of the text, for code-mixed sentiment analysis. The proposed system obtained 0.69 (best run) in terms of F1 score on the given test data and achieved the 9th place (Codalab username: somban) in the SentiMix Hindi-English subtask.

</details>

<details>

<summary>2020-08-30 13:58:47 - Action similarity judgment based on kinematic primitives</summary>

- *Vipul Nair, Paul Hemeren, Alessia Vignolo, Nicoletta Noceti, Elena Nicora, Alessandra Sciutti, Francesco Rea, Erik Billing, Francesca Odone, Giulio Sandini*

- `2008.13176v1` - [abs](http://arxiv.org/abs/2008.13176v1) - [pdf](http://arxiv.org/pdf/2008.13176v1)

> Understanding which features humans rely on -- in visually recognizing action similarity is a crucial step towards a clearer picture of human action perception from a learning and developmental perspective. In the present work, we investigate to which extent a computational model based on kinematics can determine action similarity and how its performance relates to human similarity judgments of the same actions. To this aim, twelve participants perform an action similarity task, and their performances are compared to that of a computational model solving the same task. The chosen model has its roots in developmental robotics and performs action classification based on learned kinematic primitives. The comparative experiment results show that both the model and human participants can reliably identify whether two actions are the same or not. However, the model produces more false hits and has a greater selection bias than human participants. A possible reason for this is the particular sensitivity of the model towards kinematic primitives of the presented actions. In a second experiment, human participants' performance on an action identification task indicated that they relied solely on kinematic information rather than on action semantics. The results show that both the model and human performance are highly accurate in an action similarity task based on kinematic-level features, which can provide an essential basis for classifying human actions.

</details>

<details>

<summary>2020-08-30 21:06:06 - On a plausible concept-wise multipreference semantics and its relations with self-organising maps</summary>

- *Laura Giordano, Valentina Gliozzi, Daniele Theseider Dupré*

- `2008.13278v1` - [abs](http://arxiv.org/abs/2008.13278v1) - [pdf](http://arxiv.org/pdf/2008.13278v1)

> Inthispaperwedescribeaconcept-wisemulti-preferencesemantics for description logic which has its root in the preferential approach for modeling defeasible reasoning in knowledge representation. We argue that this proposal, beside satisfying some desired properties, such as KLM postulates, and avoiding the drowning problem, also defines a plausible notion of semantics. We motivate the plausibility of the concept-wise multi-preference semantics by developing a logical semantics of self-organising maps, which have been proposed as possible candidates to explain the psychological mechanisms underlying category generalisation, in terms of multi-preference interpretations.

</details>

<details>

<summary>2020-08-30 23:51:41 - SEEC: Semantic Vector Federation across Edge Computing Environments</summary>

- *Shalisha Witherspoon, Dean Steuer, Graham Bent, Nirmit Desai*

- `2008.13298v1` - [abs](http://arxiv.org/abs/2008.13298v1) - [pdf](http://arxiv.org/pdf/2008.13298v1)

> Semantic vector embedding techniques have proven useful in learning semantic representations of data across multiple domains. A key application enabled by such techniques is the ability to measure semantic similarity between given data samples and find data most similar to a given sample. State-of-the-art embedding approaches assume all data is available on a single site. However, in many business settings, data is distributed across multiple edge locations and cannot be aggregated due to a variety of constraints. Hence, the applicability of state-of-the-art embedding approaches is limited to freely shared datasets, leaving out applications with sensitive or mission-critical data. This paper addresses this gap by proposing novel unsupervised algorithms called \emph{SEEC} for learning and applying semantic vector embedding in a variety of distributed settings. Specifically, for scenarios where multiple edge locations can engage in joint learning, we adapt the recently proposed federated learning techniques for semantic vector embedding. Where joint learning is not possible, we propose novel semantic vector translation algorithms to enable semantic query across multiple edge locations, each with its own semantic vector-space. Experimental results on natural language as well as graph datasets show that this may be a promising new direction.

</details>

<details>

<summary>2020-08-31 03:57:50 - Discovering Bilingual Lexicons in Polyglot Word Embeddings</summary>

- *Ashiqur R. KhudaBukhsh, Shriphani Palakodety, Tom M. Mitchell*

- `2008.13347v1` - [abs](http://arxiv.org/abs/2008.13347v1) - [pdf](http://arxiv.org/pdf/2008.13347v1)

> Bilingual lexicons and phrase tables are critical resources for modern Machine Translation systems. Although recent results show that without any seed lexicon or parallel data, highly accurate bilingual lexicons can be learned using unsupervised methods, such methods rely on the existence of large, clean monolingual corpora. In this work, we utilize a single Skip-gram model trained on a multilingual corpus yielding polyglot word embeddings, and present a novel finding that a surprisingly simple constrained nearest-neighbor sampling technique in this embedding space can retrieve bilingual lexicons, even in harsh social media data sets predominantly written in English and Romanized Hindi and often exhibiting code switching. Our method does not require monolingual corpora, seed lexicons, or any other such resources. Additionally, across three European language pairs, we observe that polyglot word embeddings indeed learn a rich semantic representation of words and substantial bilingual lexicons can be retrieved using our constrained nearest neighbor sampling. We investigate potential reasons and downstream applications in settings spanning both clean texts and noisy social media data sets, and in both resource-rich and under-resourced language pairs.

</details>

<details>

<summary>2020-08-31 04:11:24 - Learning Adaptive Embedding Considering Incremental Class</summary>

- *Yang Yang, Zhen-Qiang Sun, HengShu Zhu, Yanjie Fu, Hui Xiong, Jian Yang*

- `2008.13351v1` - [abs](http://arxiv.org/abs/2008.13351v1) - [pdf](http://arxiv.org/pdf/2008.13351v1)

> Class-Incremental Learning (CIL) aims to train a reliable model with the streaming data, which emerges unknown classes sequentially. Different from traditional closed set learning, CIL has two main challenges: 1) Novel class detection. The initial training data only contains incomplete classes, and streaming test data will accept unknown classes. Therefore, the model needs to not only accurately classify known classes, but also effectively detect unknown classes; 2) Model expansion. After the novel classes are detected, the model needs to be updated without re-training using entire previous data. However, traditional CIL methods have not fully considered these two challenges, first, they are always restricted to single novel class detection each phase and embedding confusion caused by unknown classes. Besides, they also ignore the catastrophic forgetting of known categories in model update. To this end, we propose a Class-Incremental Learning without Forgetting (CILF) framework, which aims to learn adaptive embedding for processing novel class detection and model update in a unified framework. In detail, CILF designs to regularize classification with decoupled prototype based loss, which can improve the intra-class and inter-class structure significantly, and acquire a compact embedding representation for novel class detection in result. Then, CILF employs a learnable curriculum clustering operator to estimate the number of semantic clusters via fine-tuning the learned network, in which curriculum operator can adaptively learn the embedding in self-taught form. Therefore, CILF can detect multiple novel classes and mitigate the embedding confusion problem. Last, with the labeled streaming test data, CILF can update the network with robust regularization to mitigate the catastrophic forgetting. Consequently, CILF is able to iteratively perform novel class detection and model update.

</details>

<details>

<summary>2020-08-31 04:14:46 - Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization</summary>

- *Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng Zhang*

- `1909.02742v3` - [abs](http://arxiv.org/abs/1909.02742v3) - [pdf](http://arxiv.org/pdf/1909.02742v3)

> Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this paper, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception; one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches, such as Neural Cleanse and TABOR.

</details>

<details>

<summary>2020-08-31 15:21:50 - Adversarial Patch Camouflage against Aerial Detection</summary>

- *Ajaya Adhikari, Richard den Hollander, Ioannis Tolios, Michael van Bekkum, Anneloes Bal, Stijn Hendriks, Maarten Kruithof, Dennis Gross, Nils Jansen, Guillermo Pérez, Kit Buurman, Stephan Raaijmakers*

- `2008.13671v1` - [abs](http://arxiv.org/abs/2008.13671v1) - [pdf](http://arxiv.org/pdf/2008.13671v1)

> Detection of military assets on the ground can be performed by applying deep learning-based object detectors on drone surveillance footage. The traditional way of hiding military assets from sight is camouflage, for example by using camouflage nets. However, large assets like planes or vessels are difficult to conceal by means of traditional camouflage nets. An alternative type of camouflage is the direct misleading of automatic object detectors. Recently, it has been observed that small adversarial changes applied to images of the object can produce erroneous output by deep learning-based detectors. In particular, adversarial attacks have been successfully demonstrated to prohibit person detections in images, requiring a patch with a specific pattern held up in front of the person, thereby essentially camouflaging the person for the detector. Research into this type of patch attacks is still limited and several questions related to the optimal patch configuration remain open.   This work makes two contributions. First, we apply patch-based adversarial attacks for the use case of unmanned aerial surveillance, where the patch is laid on top of large military assets, camouflaging them from automatic detectors running over the imagery. The patch can prevent automatic detection of the whole object while only covering a small part of it. Second, we perform several experiments with different patch configurations, varying their size, position, number and saliency. Our results show that adversarial patch attacks form a realistic alternative to traditional camouflage activities, and should therefore be considered in the automated analysis of aerial surveillance imagery.

</details>

<details>

<summary>2020-08-31 15:55:24 - SemEval-2020 Task 6: Definition extraction from free text with the DEFT corpus</summary>

- *Sasha Spala, Nicholas A Miller, Franck Dernoncourt, Carl Dockhorn*

- `2008.13694v1` - [abs](http://arxiv.org/abs/2008.13694v1) - [pdf](http://arxiv.org/pdf/2008.13694v1)

> Research on definition extraction has been conducted for well over a decade, largely with significant constraints on the type of definitions considered. In this work, we present DeftEval, a SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. Definitions and glosses in free text often appear without explicit indicators, across sentences boundaries, or in an otherwise complex linguistic manner. DeftEval involved 3 distinct subtasks: 1)Sentence classification, 2) sequence labeling, and 3) relation extraction.

</details>

<details>

<summary>2020-08-31 17:40:20 - A3Ident: A Two-phased Approach to Identify the Leading Authors of Android Apps</summary>

- *Wei Wang, Guozhu Meng, Haoyu Wang, Kai Chen, Weimin Ge, Xiaohong Li*

- `2008.13768v1` - [abs](http://arxiv.org/abs/2008.13768v1) - [pdf](http://arxiv.org/pdf/2008.13768v1)

> Authorship identification is the process of identifying and classifying authors through given codes. Authorship identification can be used in a wide range of software domains, e.g., code authorship disputes, plagiarism detection, exposure of attackers' identity. Besides the inherent challenges from legacy software development, framework programming and crowdsourcing mode in Android raise the difficulties of authorship identification significantly. More specifically, widespread third party libraries and inherited components (e.g., classes, methods, and variables) dilute the primary code within the entire Android app and blur the boundaries of code written by different authors. However, prior research has not well addressed these challenges.   To this end, we design a two-phased approach to attribute the primary code of an Android app to the specific developer. In the first phase, we put forward three types of strategies to identify the relationships between Java packages in an app, which consist of context, semantic and structural relationships. A package aggregation algorithm is developed to cluster all packages that are of high probability written by the same authors. In the second phase, we develop three types of features to capture authors' coding habits and code stylometry. Based on that, we generate fingerprints for an author from its developed Android apps and employ several machine learning algorithms for authorship classification. We evaluate our approach in three datasets that contain 15,666 apps from 257 distinct developers and achieve a 92.5% accuracy rate on average. Additionally, we test it on 2,900 obfuscated apps and our approach can classify apps with an accuracy rate of 80.4%.

</details>

<details>

<summary>2020-08-31 23:25:01 - Cross-modal Knowledge Reasoning for Knowledge-based Visual Question Answering</summary>

- *Jing Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu, Jianlong Tan*

- `2009.00145v1` - [abs](http://arxiv.org/abs/2009.00145v1) - [pdf](http://arxiv.org/pdf/2009.00145v1)

> Knowledge-based Visual Question Answering (KVQA) requires external knowledge beyond the visible content to answer questions about an image. This ability is challenging but indispensable to achieve general VQA. One limitation of existing KVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the correct answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. Inspired by the human cognition theory, in this paper, we depict an image by multiple knowledge graphs from the visual, semantic and factual views. Thereinto, the visual graph and semantic graph are regarded as image-conditioned instantiation of the factual graph. On top of these new representations, we re-formulate Knowledge-based Visual Question Answering as a recurrent reasoning process for obtaining complementary evidence from multimodal information. To this end, we decompose the model into a series of memory-based reasoning steps, each performed by a G raph-based R ead, U pdate, and C ontrol ( GRUC ) module that conducts parallel reasoning over both visual and semantic information. By stacking the modules multiple times, our model performs transitive reasoning and obtains question-oriented concept representations under the constrain of different modalities. Finally, we perform graph neural networks to infer the global-optimal answer by jointly considering all the concepts. We achieve a new state-of-the-art performance on three popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and demonstrate the effectiveness and interpretability of our model with extensive experiments.

</details>


## 2020-09

<details>

<summary>2020-09-01 02:33:19 - Patching as Translation: the Data and the Metaphor</summary>

- *Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu, Vincent J. Hellendoorn*

- `2008.10707v2` - [abs](http://arxiv.org/abs/2008.10707v2) - [pdf](http://arxiv.org/pdf/2008.10707v2)

> Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that "software patching is like language translation". We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as "proof-of-concept" tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation.

</details>

<details>

<summary>2020-09-01 09:24:01 - IR2Vec: LLVM IR based Scalable Program Embeddings</summary>

- *S. VenkataKeerthy, Rohit Aggarwal, Shalini Jain, Maunendra Sankar Desarkar, Ramakrishna Upadrasta, Y. N. Srikant*

- `1909.06228v3` - [abs](http://arxiv.org/abs/1909.06228v3) - [pdf](http://arxiv.org/pdf/1909.06228v3)

> We propose IR2Vec, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings:Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information.   We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2Vec outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the Thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has betterOut-Of-Vocabulary (OOV) characteristics.

</details>

<details>

<summary>2020-09-01 17:47:15 - A reconstruction of the multipreference closure</summary>

- *Laura Giordano, Valentina Gliozzi*

- `1905.03855v2` - [abs](http://arxiv.org/abs/1905.03855v2) - [pdf](http://arxiv.org/pdf/1905.03855v2)

> The paper describes a preferential approach for dealing with exceptions in KLM preferential logics, based on the rational closure. It is well known that the rational closure does not allow an independent handling of the inheritance of different defeasible properties of concepts. Several solutions have been proposed to face this problem and the lexicographic closure is the most notable one. In this work, we consider an alternative closure construction, called the Multi Preference closure (MP-closure), that has been first considered for reasoning with exceptions in DLs. Here, we reconstruct the notion of MP-closure in the propositional case and we show that it is a natural variant of Lehmann's lexicographic closure. Abandoning Maximal Entropy (an alternative route already considered but not explored by Lehmann) leads to a construction which exploits a different lexicographic ordering w.r.t. the lexicographic closure, and determines a preferential consequence relation rather than a rational consequence relation. We show that, building on the MP-closure semantics, rationality can be recovered, at least from the semantic point of view, resulting in a rational consequence relation which is stronger than the rational closure, but incomparable with the lexicographic closure. We also show that the MP-closure is stronger than the Relevant Closure.

</details>

<details>

<summary>2020-09-01 19:28:51 - Document Similarity from Vector Space Densities</summary>

- *Ilia Rushkin*

- `2009.00672v1` - [abs](http://arxiv.org/abs/2009.00672v1) - [pdf](http://arxiv.org/pdf/2009.00672v1)

> We propose a computationally light method for estimating similarities between text documents, which we call the density similarity (DS) method. The method is based on a word embedding in a high-dimensional Euclidean space and on kernel regression, and takes into account semantic relations among words. We find that the accuracy of this method is virtually the same as that of a state-of-the-art method, while the gain in speed is very substantial. Additionally, we introduce generalized versions of the top-k accuracy metric and of the Jaccard metric of agreement between similarity models.

</details>

<details>

<summary>2020-09-01 22:24:44 - What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge</summary>

- *Kyle Richardson, Ashish Sabharwal*

- `1912.13337v2` - [abs](http://arxiv.org/abs/1912.13337v2) - [pdf](http://arxiv.org/pdf/1912.13337v2)

> Open-domain question answering (QA) is known to involve several underlying knowledge and reasoning challenges, but are models actually learning such knowledge when trained on benchmark tasks? To investigate this, we introduce several new challenge tasks that probe whether state-of-the-art QA models have general knowledge about word definitions and general taxonomic reasoning, both of which are fundamental to more complex forms of reasoning and are widespread in benchmark datasets. As an alternative to expensive crowd-sourcing, we introduce a methodology for automatically building datasets from various types of expert knowledge (e.g., knowledge graphs and lexical taxonomies), allowing for systematic control over the resulting probes and for a more comprehensive evaluation. We find automatically constructing probes to be vulnerable to annotation artifacts, which we carefully control for. Our evaluation confirms that transformer-based QA models are already predisposed to recognize certain types of structural lexical knowledge. However, it also reveals a more nuanced picture: their performance degrades substantially with even a slight increase in the number of hops in the underlying taxonomic hierarchy, or as more challenging distractor candidate answers are introduced. Further, even when these models succeed at the standard instance-level evaluation, they leave much room for improvement when assessed at the level of clusters of semantically connected probes (e.g., all Isa questions about a concept).

</details>

<details>

<summary>2020-09-02 03:16:48 - Heterogeneous Graph Neural Network for Recommendation</summary>

- *Jinghan Shi, Houye Ji, Chuan Shi, Xiao Wang, Zhiqiang Zhang, Jun Zhou*

- `2009.00799v1` - [abs](http://arxiv.org/abs/2009.00799v1) - [pdf](http://arxiv.org/pdf/2009.00799v1)

> The prosperous development of e-commerce has spawned diverse recommendation systems. As a matter of fact, there exist rich and complex interactions among various types of nodes in real-world recommendation systems, which can be constructed as heterogeneous graphs. How learn representative node embedding is the basis and core of the personalized recommendation system. Meta-path is a widely used structure to capture the semantics beneath such interactions and show potential ability in improving node embedding. In this paper, we propose Heterogeneous Graph neural network for Recommendation (HGRec) which injects high-order semantic into node embedding via aggregating multi-hops meta-path based neighbors and fuses rich semantics via multiple meta-paths based on attention mechanism to get comprehensive node embedding. Experimental results demonstrate the importance of rich high-order semantics and also show the potentially good interpretability of HGRec.

</details>

<details>

<summary>2020-09-02 08:22:29 - MultiSegVA: Using Visual Analytics to Segment Biologging Time Series on Multiple Scales</summary>

- *Philipp Meschenmoser, Juri F. Buchmüller, Daniel Seebacher, Martin Wikelski, Daniel A. Keim*

- `2009.00548v2` - [abs](http://arxiv.org/abs/2009.00548v2) - [pdf](http://arxiv.org/pdf/2009.00548v2)

> Segmenting biologging time series of animals on multiple temporal scales is an essential step that requires complex techniques with careful parameterization and possibly cross-domain expertise. Yet, there is a lack of visual-interactive tools that strongly support such multi-scale segmentation. To close this gap, we present our MultiSegVA platform for interactively defining segmentation techniques and parameters on multiple temporal scales. MultiSegVA primarily contributes tailored, visual-interactive means and visual analytics paradigms for segmenting unlabeled time series on multiple scales. Further, to flexibly compose the multi-scale segmentation, the platform contributes a new visual query language that links a variety of segmentation techniques. To illustrate our approach, we present a domain-oriented set of segmentation techniques derived in collaboration with movement ecologists. We demonstrate the applicability and usefulness of MultiSegVA in two real-world use cases from movement ecology, related to behavior analysis after environment-aware segmentation, and after progressive clustering. Expert feedback from movement ecologists shows the effectiveness of tailored visual-interactive means and visual analytics paradigms at segmenting multi-scale data, enabling them to perform semantically meaningful analyses. A third use case demonstrates that MultiSegVA is generalizable to other domains.

</details>

<details>

<summary>2020-09-02 09:14:27 - W-Net: Dense Semantic Segmentation of Subcutaneous Tissue in Ultrasound Images by Expanding U-Net to Incorporate Ultrasound RF Waveform Data</summary>

- *Gautam Rajendrakumar Gare, Jiayuan Li, Rohan Joshi, Mrunal Prashant Vaze, Rishikesh Magar, Michael Yousefpour, Ricardo Luis Rodriguez, John Micheal Galeotti*

- `2008.12413v2` - [abs](http://arxiv.org/abs/2008.12413v2) - [pdf](http://arxiv.org/pdf/2008.12413v2)

> We present W-Net, a novel Convolution Neural Network (CNN) framework that employs raw ultrasound waveforms from each A-scan, typically referred to as ultrasound Radio Frequency (RF) data, in addition to the gray ultrasound image to semantically segment and label tissues. Unlike prior work, we seek to label every pixel in the image, without the use of a background class. To the best of our knowledge, this is also the first deep-learning or CNN approach for segmentation that analyses ultrasound raw RF data along with the gray image. International patent(s) pending [PCT/US20/37519]. We chose subcutaneous tissue (SubQ) segmentation as our initial clinical goal since it has diverse intermixed tissues, is challenging to segment, and is an underrepresented research area. SubQ potential applications include plastic surgery, adipose stem-cell harvesting, lymphatic monitoring, and possibly detection/treatment of certain types of tumors. A custom dataset consisting of hand-labeled images by an expert clinician and trainees are used for the experimentation, currently labeled into the following categories: skin, fat, fat fascia/stroma, muscle and muscle fascia. We compared our results with U-Net and Attention U-Net. Our novel \emph{W-Net}'s RF-Waveform input and architecture increased mIoU accuracy (averaged across all tissue classes) by 4.5\% and 4.9\% compared to regular U-Net and Attention U-Net, respectively. We present analysis as to why the Muscle fascia and Fat fascia/stroma are the most difficult tissues to label. Muscle fascia in particular, the most difficult anatomic class to recognize for both humans and AI algorithms, saw mIoU improvements of 13\% and 16\% from our W-Net vs U-Net and Attention U-Net respectively.

</details>

<details>

<summary>2020-09-02 11:00:42 - Unified Generative Adversarial Networks for Controllable Image-to-Image Translation</summary>

- *Hao Tang, Hong Liu, Nicu Sebe*

- `1912.06112v2` - [abs](http://arxiv.org/abs/1912.06112v2) - [pdf](http://arxiv.org/pdf/1912.06112v2)

> We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps. The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fr\'echet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN.

</details>

<details>

<summary>2020-09-02 13:45:01 - Video Captioning Using Weak Annotation</summary>

- *Jingyi Hou, Yunde Jia, Xinxiao wu, Yayun Qi*

- `2009.01067v1` - [abs](http://arxiv.org/abs/2009.01067v1) - [pdf](http://arxiv.org/pdf/2009.01067v1)

> Video captioning has shown impressive progress in recent years. One key reason of the performance improvements made by existing methods lie in massive paired video-sentence data, but collecting such strong annotation, i.e., high-quality sentences, is time-consuming and laborious. It is the fact that there now exist an amazing number of videos with weak annotation that only contains semantic concepts such as actions and objects. In this paper, we investigate using weak annotation instead of strong annotation to train a video captioning model. To this end, we propose a progressive visual reasoning method that progressively generates fine sentences from weak annotations by inferring more semantic concepts and their dependency relationships for video captioning. To model concept relationships, we use dependency trees that are spanned by exploiting external knowledge from large sentence corpora. Through traversing the dependency trees, the sentences are generated to train the captioning model. Accordingly, we develop an iterative refinement algorithm that refines sentences via spanning dependency trees and fine-tunes the captioning model using the refined sentences in an alternative training manner. Experimental results demonstrate that our method using weak annotation is very competitive to the state-of-the-art methods using strong annotation.

</details>

<details>

<summary>2020-09-02 19:07:34 - Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short Answer Grading</summary>

- *Sasi Kiran Gaddipati, Deebul Nair, Paul G. Plöger*

- `2009.01303v1` - [abs](http://arxiv.org/abs/2009.01303v1) - [pdf](http://arxiv.org/pdf/2009.01303v1)

> Automatic Short Answer Grading (ASAG) is the process of grading the student answers by computational approaches given a question and the desired answer. Previous works implemented the methods of concept mapping, facet mapping, and some used the conventional word embeddings for extracting semantic features. They extracted multiple features manually to train on the corresponding datasets. We use pretrained embeddings of the transfer learning models, ELMo, BERT, GPT, and GPT-2 to assess their efficiency on this task. We train with a single feature, cosine similarity, extracted from the embeddings of these models. We compare the RMSE scores and correlation measurements of the four models with previous works on Mohler dataset. Our work demonstrates that ELMo outperformed the other three models. We also, briefly describe the four transfer learning models and conclude with the possible causes of poor results of transfer learning models.

</details>

<details>

<summary>2020-09-02 20:45:04 - Gender Stereotype Reinforcement: Measuring the Gender Bias Conveyed by Ranking Algorithms</summary>

- *Alessandro Fabris, Alberto Purpura, Gianmaria Silvello, Gian Antonio Susto*

- `2009.01334v1` - [abs](http://arxiv.org/abs/2009.01334v1) - [pdf](http://arxiv.org/pdf/2009.01334v1)

> Search Engines (SE) have been shown to perpetuate well-known gender stereotypes identified in psychology literature and to influence users accordingly. Similar biases were found encoded in Word Embeddings (WEs) learned from large online corpora. In this context, we propose the Gender Stereotype Reinforcement (GSR) measure, which quantifies the tendency of a SE to support gender stereotypes, leveraging gender-related information encoded in WEs. Through the critical lens of construct validity, we validate the proposed measure on synthetic and real collections. Subsequently, we use GSR to compare widely-used Information Retrieval ranking algorithms, including lexical, semantic, and neural models. We check if and how ranking algorithms based on WEs inherit the biases of the underlying embeddings. We also consider the most common debiasing approaches for WEs proposed in the literature and test their impact in terms of GSR and common performance measures. To the best of our knowledge, GSR is the first specifically tailored measure for IR, capable of quantifying representational harms.

</details>

<details>

<summary>2020-09-03 01:14:34 - A survey of loss functions for semantic segmentation</summary>

- *Shruti Jadon*

- `2006.14822v4` - [abs](http://arxiv.org/abs/2006.14822v4) - [pdf](http://arxiv.org/pdf/2006.14822v4)

> Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self-driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on the NBFS skull-segmentation open-source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios. Our code is available at Github: https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions.

</details>

<details>

<summary>2020-09-03 05:23:56 - NLPContributions: An Annotation Scheme for Machine Reading of Scholarly Contributions in Natural Language Processing Literature</summary>

- *Jennifer D'Souza, Sören Auer*

- `2006.12870v3` - [abs](http://arxiv.org/abs/2006.12870v3) - [pdf](http://arxiv.org/pdf/2006.12870v3)

> We describe an annotation initiative to capture the scholarly contributions in natural language processing (NLP) articles, particularly, for the articles that discuss machine learning (ML) approaches for various information extraction tasks. We develop the annotation task based on a pilot annotation exercise on 50 NLP-ML scholarly articles presenting contributions to five information extraction tasks 1. machine translation, 2. named entity recognition, 3. question answering, 4. relation classification, and 5. text classification. In this article, we describe the outcomes of this pilot annotation phase. Through the exercise we have obtained an annotation methodology; and found ten core information units that reflect the contribution of the NLP-ML scholarly investigations. The resulting annotation scheme we developed based on these information units is called NLPContributions.   The overarching goal of our endeavor is four-fold: 1) to find a systematic set of patterns of subject-predicate-object statements for the semantic structuring of scholarly contributions that are more or less generically applicable for NLP-ML research articles; 2) to apply the discovered patterns in the creation of a larger annotated dataset for training machine readers of research contributions; 3) to ingest the dataset into the Open Research Knowledge Graph (ORKG) infrastructure as a showcase for creating user-friendly state-of-the-art overviews; 4) to integrate the machine readers into the ORKG to assist users in the manual curation of their respective article contributions. We envision that the NLPContributions methodology engenders a wider discussion on the topic toward its further refinement and development. Our pilot annotated dataset of 50 NLP-ML scholarly articles according to the NLPContributions scheme is openly available to the research community at https://doi.org/10.25835/0019761.

</details>

<details>

<summary>2020-09-03 14:43:04 - The ADAPT Enhanced Dependency Parser at the IWPT 2020 Shared Task</summary>

- *James Barry, Joachim Wagner, Jennifer Foster*

- `2009.01712v1` - [abs](http://arxiv.org/abs/2009.01712v1) - [pdf](http://arxiv.org/pdf/2009.01712v1)

> We describe the ADAPT system for the 2020 IWPT Shared Task on parsing enhanced Universal Dependencies in 17 languages. We implement a pipeline approach using UDPipe and UDPipe-future to provide initial levels of annotation. The enhanced dependency graph is either produced by a graph-based semantic dependency parser or is built from the basic tree using a small set of heuristics. Our results show that, for the majority of languages, a semantic dependency parser can be successfully applied to the task of parsing enhanced dependencies.   Unfortunately, we did not ensure a connected graph as part of our pipeline approach and our competition submission relied on a last-minute fix to pass the validation script which harmed our official evaluation scores significantly. Our submission ranked eighth in the official evaluation with a macro-averaged coarse ELAS F1 of 67.23 and a treebank average of 67.49. We later implemented our own graph-connecting fix which resulted in a score of 79.53 (language average) or 79.76 (treebank average), which would have placed fourth in the competition evaluation.

</details>

<details>

<summary>2020-09-03 15:07:02 - Solving reachability problems on data-aware workflows</summary>

- *Riccardo De Masellis, Chiara Di Francescomarino, Chiara Ghidini, Sergio Tessaris*

- `1909.12738v2` - [abs](http://arxiv.org/abs/1909.12738v2) - [pdf](http://arxiv.org/pdf/1909.12738v2)

> Recent advances in the field of Business Process Management have brought about several suites able to model complex data objects along with the traditional control flow perspective. Nonetheless, when it comes to formal verification there is still the lack of effective verification tools on imperative data-aware process models and executions: the data perspective is often abstracted away and verification tools are often missing. In this paper we provide a concrete framework for formal verification of reachability properties on imperative data-aware business processes. We start with an expressive, yet empirically tractable class of data-aware process models, an extension of Workflow Nets, and we provide a rigorous mapping between the semantics of such models and that of three important paradigms for reasoning about dynamic systems: Action Languages, Classical Planning, and Model Checking. Then we perform a comprehensive assessment of the performance of three popular tools supporting the above paradigms in solving reachability problems for imperative data-aware business processes, which paves the way for a theoretically well founded and practically viable exploitation of formal verification techniques on data-aware business processes.

</details>

<details>

<summary>2020-09-03 21:56:38 - Multi-Perspective Semantic Information Retrieval</summary>

- *Samarth Rawal, Chitta Baral*

- `2009.01938v1` - [abs](http://arxiv.org/abs/2009.01938v1) - [pdf](http://arxiv.org/pdf/2009.01938v1)

> Information Retrieval (IR) is the task of obtaining pieces of data (such as documents or snippets of text) that are relevant to a particular query or need from a large repository of information. While a combination of traditional keyword- and modern BERT-based approaches have been shown to be effective in recent work, there are often nuances in identifying what information is "relevant" to a particular query, which can be difficult to properly capture using these systems. This work introduces the concept of a Multi-Perspective IR system, a novel methodology that combines multiple deep learning and traditional IR models to better predict the relevance of a query-sentence pair, along with a standardized framework for tuning this system. This work is evaluated on the BioASQ Biomedical IR + QA challenges.

</details>

<details>

<summary>2020-09-03 22:00:42 - Accurate TLS Fingerprinting using Destination Context and Knowledge Bases</summary>

- *Blake Anderson, David McGrew*

- `2009.01939v1` - [abs](http://arxiv.org/abs/2009.01939v1) - [pdf](http://arxiv.org/pdf/2009.01939v1)

> Network fingerprinting is used to identify applications, provide insight into network traffic, and detect malicious activity. With the broad adoption of TLS, traditional fingerprinting techniques that rely on clear-text data are no longer viable. TLS-specific techniques have been introduced that create a fingerprint string from carefully selected data features in the client_hello to facilitate process identification before data is exchanged. Unfortunately, this approach fails in practice because hundreds of processes can map to the same fingerprint string. We solve this problem by presenting a TLS fingerprinting system that makes use of the destination address, port, and server name in addition to a carefully constructed fingerprint string. The destination context is used to disambiguate the set of processes that match a fingerprint string by applying a weighted naive Bayes classifier, resulting in far greater performance.

</details>

<details>

<summary>2020-09-03 23:38:52 - CoNCRA: A Convolutional Neural Network Code Retrieval Approach</summary>

- *Marcelo de Rezende Martins, Marco A. Gerosa*

- `2009.01959v1` - [abs](http://arxiv.org/abs/2009.01959v1) - [pdf](http://arxiv.org/pdf/2009.01959v1)

> Software developers routinely search for code using general-purpose search engines. However, these search engines cannot find code semantically unless it has an accompanying description. We propose a technique for semantic code search: A Convolutional Neural Network approach to code retrieval (CoNCRA). Our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. We evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from Stack Overflow. Our preliminary results showed that our technique, which prioritizes local interactions (words nearby), improved the state-of-the-art (SOTA) by 5% on average, retrieving the most relevant code snippets in the top 3 (three) positions by almost 80% of the time. Therefore, our technique is promising and can improve the efficacy of semantic code retrieval.

</details>

<details>

<summary>2020-09-04 03:41:50 - DeepSun: Machine-Learning-as-a-Service for Solar Flare Prediction</summary>

- *Yasser Abduallah, Jason T. L. Wang, Yang Nie, Chang Liu, Haimin Wang*

- `2009.04238v1` - [abs](http://arxiv.org/abs/2009.04238v1) - [pdf](http://arxiv.org/pdf/2009.04238v1)

> Solar flare prediction plays an important role in understanding and forecasting space weather. The main goal of the Helioseismic and Magnetic Imager (HMI), one of the instruments on NASA's Solar Dynamics Observatory, is to study the origin of solar variability and characterize the Sun's magnetic activity. HMI provides continuous full-disk observations of the solar vector magnetic field with high cadence data that lead to reliable predictive capability; yet, solar flare prediction effort utilizing these data is still limited. In this paper, we present a machine-learning-as-a-service (MLaaS) framework, called DeepSun, for predicting solar flares on the Web based on HMI's data products. Specifically, we construct training data by utilizing the physical parameters provided by the Space-weather HMI Active Region Patches (SHARP) and categorize solar flares into four classes, namely B, C, M, X, according to the X-ray flare catalogs available at the National Centers for Environmental Information (NCEI). Thus, the solar flare prediction problem at hand is essentially a multi-class (i.e., four-class) classification problem. The DeepSun system employs several machine learning algorithms to tackle this multi-class prediction problem and provides an application programming interface (API) for remote programming users. To our knowledge, DeepSun is the first MLaaS tool capable of predicting solar flares through the Internet.

</details>

<details>

<summary>2020-09-04 05:19:53 - A framework for a modular multi-concept lexicographic closure semantics</summary>

- *Laura Giordano, Daniele Theseider Dupré*

- `2009.00964v2` - [abs](http://arxiv.org/abs/2009.00964v2) - [pdf](http://arxiv.org/pdf/2009.00964v2)

> We define a modular multi-concept extension of the lexicographic closure semantics for defeasible description logics with typicality. The idea is that of distributing the defeasible properties of concepts into different modules, according to their subject, and of defining a notion of preference for each module based on the lexicographic closure semantics. The preferential semantics of the knowledge base can then be defined as a combination of the preferences of the single modules. The range of possibilities, from fine grained to coarse grained modules, provides a spectrum of alternative semantics.

</details>

<details>

<summary>2020-09-04 06:18:24 - Dynamic Context-guided Capsule Network for Multimodal Machine Translation</summary>

- *Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, Zhengyuan Yang, Yubin Ge, Jie Zhou, Jiebo Luo*

- `2009.02016v1` - [abs](http://arxiv.org/abs/2009.02016v1) - [pdf](http://arxiv.org/pdf/2009.02016v1)

> Multimodal machine translation (MMT), which mainly focuses on enhancing text-only translation with visual features, has attracted considerable attention from both computer vision and natural language processing communities. Most current MMT models resort to attention mechanism, global context modeling or multimodal joint representation learning to utilize visual features. However, the attention mechanism lacks sufficient semantic interactions between modalities while the other two provide fixed visual context, which is unsuitable for modeling the observed variability when generating translation. To address the above issues, in this paper, we propose a novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at each timestep of decoding, we first employ the conventional source-target attention to produce a timestep-specific source-side context vector. Next, DCCN takes this vector as input and uses it to guide the iterative extraction of related visual features via a context-guided dynamic routing mechanism. Particularly, we represent the input image with global and regional visual features, we introduce two parallel DCCNs to model multimodal context vectors with visual features at different granularities. Finally, we obtain two multimodal context vectors, which are fused and incorporated into the decoder for the prediction of the target word. Experimental results on the Multi30K dataset of English-to-German and English-to-French translation demonstrate the superiority of DCCN. Our code is available on https://github.com/DeepLearnXMU/MM-DCCN.

</details>

<details>

<summary>2020-09-04 08:37:58 - A Framework and DataSet for Bugs in Ethereum Smart Contracts</summary>

- *Pengcheng Zhang, Feng Xiao, Xiapu Luo*

- `2009.02066v1` - [abs](http://arxiv.org/abs/2009.02066v1) - [pdf](http://arxiv.org/pdf/2009.02066v1)

> Ethereum is the largest blockchain platform that supports smart contracts. Users deploy smart contracts by publishing the smart contract's bytecode to the blockchain. Since the data in the blockchain cannot be modified, even if these contracts contain bugs, it is not possible to patch deployed smart contracts with code updates. Moreover, there is currently neither a comprehensive classification framework for Ethereum smart contract bugs, nor detailed criteria for detecting bugs in smart contracts, making it difficult for developers to fully understand the negative effects of bugs and design new approaches to detect bugs. In this paper, to fill the gap, we first collect as many smart contract bugs as possible from multiple sources and divide these bugs into 9 categories by extending the IEEE Standard Classification for Software Anomalies. Then, we design the criteria for detecting each kind of bugs, and construct a dataset of smart contracts covering all kinds of bugs. With our framework and dataset, developers can learn smart contract bugs and develop new tools to detect and locate bugs in smart contracts. Moreover, we evaluate the state-of-the-art tools for smart contract analysis with our dataset and obtain some interesting findings: 1) Mythril, Slither and Remix are the most worthwhile combination of analysis tools. 2) There are still 10 kinds of bugs that cannot be detected by any analysis tool.

</details>

<details>

<summary>2020-09-04 13:46:07 - Phenotypical Ontology Driven Framework for Multi-Task Learning</summary>

- *Mohamed Ghalwash, Zijun Yao, Prithwish Chakraborty, James Codella, Daby Sow*

- `2009.02188v1` - [abs](http://arxiv.org/abs/2009.02188v1) - [pdf](http://arxiv.org/pdf/2009.02188v1)

> Despite the large number of patients in Electronic Health Records (EHRs), the subset of usable data for modeling outcomes of specific phenotypes are often imbalanced and of modest size. This can be attributed to the uneven coverage of medical concepts in EHRs. In this paper, we propose OMTL, an Ontology-driven Multi-Task Learning framework, that is designed to overcome such data limitations. The key contribution of our work is the effective use of knowledge from a predefined well-established medical relationship graph (ontology) to construct a novel deep learning network architecture that mirrors this ontology. It can effectively leverage knowledge from a well-established medical relationship graph (ontology) by constructing a deep learning network architecture that mirrors this graph. This enables common representations to be shared across related phenotypes, and was found to improve the learning performance. The proposed OMTL naturally allows for multitask learning of different phenotypes on distinct predictive tasks. These phenotypes are tied together by their semantic distance according to the external medical ontology. Using the publicly available MIMIC-III database, we evaluate OMTL and demonstrate its efficacy on several real patient outcome predictions over state-of-the-art multi-task learning schemes.

</details>

<details>

<summary>2020-09-04 14:20:46 - SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis</summary>

- *Noa Fish, Lilach Perry, Amit Bermano, Daniel Cohen-Or*

- `2009.02216v1` - [abs](http://arxiv.org/abs/2009.02216v1) - [pdf](http://arxiv.org/pdf/2009.02216v1)

> The paradigm of image-to-image translation is leveraged for the benefit of sketch stylization via transfer of geometric textural details. Lacking the necessary volumes of data for standard training of translation systems, we advocate for operation at the patch level, where a handful of stylized sketches provide ample mining potential for patches featuring basic geometric primitives. Operating at the patch level necessitates special consideration of full sketch translation, as individual translation of patches with no regard to neighbors is likely to produce visible seams and artifacts at patch borders. Aligned pairs of styled and plain primitives are combined to form input hybrids containing styled elements around the border and plain elements within, and given as input to a seamless translation (ST) generator, whose output patches are expected to reconstruct the fully styled patch. An adversarial addition promotes generalization and robustness to diverse geometries at inference time, forming a simple and effective system for arbitrary sketch stylization, as demonstrated upon a variety of styles and sketches.

</details>

<details>

<summary>2020-09-04 14:27:49 - Extracting Semantic Concepts and Relations from Scientific Publications by Using Deep Learning</summary>

- *Fatima N. AL-Aswadi, Huah Yong Chan, Keng Hoon Gan*

- `2009.00331v2` - [abs](http://arxiv.org/abs/2009.00331v2) - [pdf](http://arxiv.org/pdf/2009.00331v2)

> With the large volume of unstructured data that increases constantly on the web, the motivation of representing the knowledge in this data in the machine-understandable form is increased. Ontology is one of the major cornerstones of representing the information in a more meaningful way on the semantic Web. The current ontology repositories are quite limited either for their scope or for currentness. In addition, the current ontology extraction systems have many shortcomings and drawbacks, such as using a small dataset, depending on a large amount predefined patterns to extract semantic relations, and extracting a very few types of relations. The aim of this paper is to introduce a proposal of automatically extracting semantic concepts and relations from scientific publications. This paper suggests new types of semantic relations and points out of using deep learning (DL) models for semantic relation extraction.

</details>

<details>

<summary>2020-09-04 16:10:28 - MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities</summary>

- *Jason Armitage, Endri Kacupaj, Golsa Tahmasebzadeh, Swati, Maria Maleshkova, Ralph Ewerth, Jens Lehmann*

- `2008.06376v3` - [abs](http://arxiv.org/abs/2008.06376v3) - [pdf](http://arxiv.org/pdf/2008.06376v3)

> In this paper, we introduce the MLM (Multiple Languages and Modalities) dataset - a new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. The generation process and inclusion of semantic data provide a resource that further tests the ability for multitask systems to learn relationships between entities. The dataset is designed for researchers and developers who build applications that perform multiple tasks on data encountered on the web and in digital archives. A second version of MLM provides a geo-representative subset of the data with weighted samples for countries of the European Union. We demonstrate the value of the resource in developing novel applications in the digital humanities with a motivating use case and specify a benchmark set of tasks to retrieve modalities and locate entities in the dataset. Evaluation of baseline multitask and single task systems on the full and geo-representative versions of MLM demonstrate the challenges of generalising on diverse data. In addition to the digital humanities, we expect the resource to contribute to research in multimodal representation learning, location estimation, and scene understanding.

</details>

<details>

<summary>2020-09-04 17:55:18 - NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis For Code-Mixed Social Media Text Using an Ensemble Model</summary>

- *Subhra Jyoti Baroi, Nivedita Singh, Ringki Das, Thoudam Doren Singh*

- `2007.12081v2` - [abs](http://arxiv.org/abs/2007.12081v2) - [pdf](http://arxiv.org/pdf/2007.12081v2)

> Sentiment Analysis is the process of deciphering what a sentence emotes and classifying them as either positive, negative, or neutral. In recent times, India has seen a huge influx in the number of active social media users and this has led to a plethora of unstructured text data. Since the Indian population is generally fluent in both Hindi and English, they end up generating code-mixed Hinglish social media text i.e. the expressions of Hindi language, written in the Roman script alongside other English words. The ability to adequately comprehend the notions in these texts is truly necessary. Our team, rns2020 participated in Task 9 at SemEval2020 intending to design a system to carry out the sentiment analysis of code-mixed social media text. This work proposes a system named NITS-Hinglish-SentiMix to viably complete the sentiment analysis of such code-mixed Hinglish text. The proposed framework has recorded an F-Score of 0.617 on the test data.

</details>

<details>

<summary>2020-09-05 04:24:58 - User-Guided Domain Adaptation for Rapid Annotation from User Interactions: A Study on Pathological Liver Segmentation</summary>

- *Ashwin Raju, Zhanghexuan Ji, Chi Tung Cheng, Jinzheng Cai, Junzhou Huang, Jing Xiao, Le Lu, ChienHung Liao, Adam P. Harrison*

- `2009.02455v1` - [abs](http://arxiv.org/abs/2009.02455v1) - [pdf](http://arxiv.org/pdf/2009.02455v1)

> Mask-based annotation of medical images, especially for 3D data, is a bottleneck in developing reliable machine learning models. Using minimal-labor user interactions (UIs) to guide the annotation is promising, but challenges remain on best harmonizing the mask prediction with the UIs. To address this, we propose the user-guided domain adaptation (UGDA) framework, which uses prediction-based adversarial domain adaptation (PADA) to model the combined distribution of UIs and mask predictions. The UIs are then used as anchors to guide and align the mask prediction. Importantly, UGDA can both learn from unlabelled data and also model the high-level semantic meaning behind different UIs. We test UGDA on annotating pathological livers using a clinically comprehensive dataset of 927 patient studies. Using only extreme-point UIs, we achieve a mean (worst-case) performance of 96.1%(94.9%), compared to 93.0% (87.0%) for deep extreme points (DEXTR). Furthermore, we also show UGDA can retain this state-of-the-art performance even when only seeing a fraction of available UIs, demonstrating an ability for robust and reliable UI-guided segmentation with extremely minimal labor demands.

</details>

<details>

<summary>2020-09-05 17:24:23 - Max-Fusion U-Net for Multi-Modal Pathology Segmentation with Attention and Dynamic Resampling</summary>

- *Haochuan Jiang, Chengjia Wang, Agisilaos Chartsias, Sotirios A. Tsaftaris*

- `2009.02569v1` - [abs](http://arxiv.org/abs/2009.02569v1) - [pdf](http://arxiv.org/pdf/2009.02569v1)

> Automatic segmentation of multi-sequence (multi-modal) cardiac MR (CMR) images plays a significant role in diagnosis and management for a variety of cardiac diseases. However, the performance of relevant algorithms is significantly affected by the proper fusion of the multi-modal information. Furthermore, particular diseases, such as myocardial infarction, display irregular shapes on images and occupy small regions at random locations. These facts make pathology segmentation of multi-modal CMR images a challenging task. In this paper, we present the Max-Fusion U-Net that achieves improved pathology segmentation performance given aligned multi-modal images of LGE, T2-weighted, and bSSFP modalities. Specifically, modality-specific features are extracted by dedicated encoders. Then they are fused with the pixel-wise maximum operator. Together with the corresponding encoding features, these representations are propagated to decoding layers with U-Net skip-connections. Furthermore, a spatial-attention module is applied in the last decoding layer to encourage the network to focus on those small semantically meaningful pathological regions that trigger relatively high responses by the network neurons. We also use a simple image patch extraction strategy to dynamically resample training examples with varying spacial and batch sizes. With limited GPU memory, this strategy reduces the imbalance of classes and forces the model to focus on regions around the interested pathology. It further improves segmentation accuracy and reduces the mis-classification of pathology. We evaluate our methods using the Myocardial pathology segmentation (MyoPS) combining the multi-sequence CMR dataset which involves three modalities. Extensive experiments demonstrate the effectiveness of the proposed model which outperforms the related baselines.

</details>

<details>

<summary>2020-09-05 19:07:17 - Analysis and representation of Igbo text document for a text-based system</summary>

- *Ifeanyi-Reuben Nkechi J., Ugwu Chidiebere, Adegbola Tunde*

- `2009.06376v1` - [abs](http://arxiv.org/abs/2009.06376v1) - [pdf](http://arxiv.org/pdf/2009.06376v1)

> The advancement in Information Technology (IT) has assisted in inculcating the three Nigeria major languages in text-based application such as text mining, information retrieval and natural language processing. The interest of this paper is the Igbo language, which uses compounding as a common type of word formation and as well has many vocabularies of compound words. The issues of collocation, word ordering and compounding play high role in Igbo language. The ambiguity in dealing with these compound words has made the representation of Igbo language text document very difficult because this cannot be addressed using the most common and standard approach of the Bag-Of-Words (BOW) model of text representation, which ignores the word order and relation. However, this cause for a concern and the need to develop an improved model to capture this situation. This paper presents the analysis of Igbo language text document, considering its compounding nature and describes its representation with the Word-based N-gram model to properly prepare it for any text-based application. The result shows that Bigram and Trigram n-gram text representation models provide more semantic information as well addresses the issues of compounding, word ordering and collocations which are the major language peculiarities in Igbo. They are likely to give better performance when used in any Igbo text-based system.

</details>

<details>

<summary>2020-09-06 10:05:43 - SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles</summary>

- *G. Da San Martino, A. Barrón-Cedeño, H. Wachsmuth, R. Petrov, P. Nakov*

- `2009.02696v1` - [abs](http://arxiv.org/abs/2009.02696v1) - [pdf](http://arxiv.org/pdf/2009.02696v1)

> We present the results and the main findings of SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. The task featured two subtasks. Subtask SI is about Span Identification: given a plain-text document, spot the specific text fragments containing propaganda. Subtask TC is about Technique Classification: given a specific text fragment, in the context of a full document, determine the propaganda technique it uses, choosing from an inventory of 14 possible propaganda techniques. The task attracted a large number of participants: 250 teams signed up to participate and 44 made a submission on the test set. In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For both subtasks, the best systems used pre-trained Transformers and ensembles.

</details>

<details>

<summary>2020-09-06 14:47:12 - Classifier Combination Approach for Question Classification for Bengali Question Answering System</summary>

- *Somnath Banerjee, Sudip Kumar Naskar, Paolo Rosso, Sivaji Bandyopadhyay*

- `2008.13597v2` - [abs](http://arxiv.org/abs/2008.13597v2) - [pdf](http://arxiv.org/pdf/2008.13597v2)

> Question classification (QC) is a prime constituent of automated question answering system. The work presented here demonstrates that the combination of multiple models achieve better classification performance than those obtained with existing individual models for the question classification task in Bengali. We have exploited state-of-the-art multiple model combination techniques, i.e., ensemble, stacking and voting, to increase QC accuracy. Lexical, syntactic and semantic features of Bengali questions are used for four well-known classifiers, namely Na\"{\i}ve Bayes, kernel Na\"{\i}ve Bayes, Rule Induction, and Decision Tree, which serve as our base learners. Single-layer question-class taxonomy with 8 coarse-grained classes is extended to two-layer taxonomy by adding 69 fine-grained classes. We carried out the experiments both on single-layer and two-layer taxonomies. Experimental results confirmed that classifier combination approaches outperform single classifier classification approaches by 4.02% for coarse-grained question classes. Overall, the stacking approach produces the best results for fine-grained classification and achieves 87.79% of accuracy. The approach presented here could be used in other Indo-Aryan or Indic languages to develop a question answering system.

</details>

<details>

<summary>2020-09-06 18:34:54 - Duluth at SemEval-2020 Task 7: Using Surprise as a Key to Unlock Humorous Headlines</summary>

- *Shuning Jin, Yue Yin, XianE Tang, Ted Pedersen*

- `2009.02795v1` - [abs](http://arxiv.org/abs/2009.02795v1) - [pdf](http://arxiv.org/pdf/2009.02795v1)

> We use pretrained transformer-based language models in SemEval-2020 Task 7: Assessing the Funniness of Edited News Headlines. Inspired by the incongruity theory of humor, we use a contrastive approach to capture the surprise in the edited headlines. In the official evaluation, our system gets 0.531 RMSE in Subtask 1, 11th among 49 submissions. In Subtask 2, our system gets 0.632 accuracy, 9th among 32 submissions.

</details>

<details>

<summary>2020-09-06 23:36:32 - A Metamodel and Framework for AGI</summary>

- *Hugo Latapie, Ozkan Kilic*

- `2008.12879v2` - [abs](http://arxiv.org/abs/2008.12879v2) - [pdf](http://arxiv.org/pdf/2008.12879v2)

> Can artificial intelligence systems exhibit superhuman performance, but in critical ways, lack the intelligence of even a single-celled organism? The answer is clearly 'yes' for narrow AI systems. Animals, plants, and even single-celled organisms learn to reliably avoid danger and move towards food. This is accomplished via a physical knowledge preserving metamodel that autonomously generates useful models of the world. We posit that preserving the structure of knowledge is critical for higher intelligences that manage increasingly higher levels of abstraction, be they human or artificial. This is the key lesson learned from applying AGI subsystems to complex real-world problems that require continuous learning and adaptation. In this paper, we introduce the Deep Fusion Reasoning Engine (DFRE), which implements a knowledge-preserving metamodel and framework for constructing applied AGI systems. The DFRE metamodel exhibits some important fundamental knowledge preserving properties such as clear distinctions between symmetric and antisymmetric relations, and the ability to create a hierarchical knowledge representation that clearly delineates between levels of abstraction. The DFRE metamodel, which incorporates these capabilities, demonstrates how this approach benefits AGI in specific ways such as managing combinatorial explosion and enabling cumulative, distributed and federated learning. Our experiments show that the proposed framework achieves 94% accuracy on average on unsupervised object detection and recognition. This work is inspired by the state-of-the-art approaches to AGI, recent AGI-aspiring work, the granular computing community, as well as Alfred Korzybski's general semantics.

</details>

<details>

<summary>2020-09-07 02:14:51 - A Study of the Learnability of Relational Properties: Model Counting Meets Machine Learning (MCML)</summary>

- *Muhammad Usman, Wenxi Wang, Kaiyuan Wang, Marko Vasic, Haris Vikalo, Sarfraz Khurshid*

- `1912.11580v2` - [abs](http://arxiv.org/abs/1912.11580v2) - [pdf](http://arxiv.org/pdf/1912.11580v2)

> This paper introduces the MCML approach for empirically studying the learnability of relational properties that can be expressed in the well-known software design language Alloy. A key novelty of MCML is quantification of the performance of and semantic differences among trained machine learning (ML) models, specifically decision trees, with respect to entire (bounded) input spaces, and not just for given training and test datasets (as is the common practice). MCML reduces the quantification problems to the classic complexity theory problem of model counting, and employs state-of-the-art model counters. The results show that relatively simple ML models can achieve surprisingly high performance (accuracy and F1-score) when evaluated in the common setting of using training and test datasets - even when the training dataset is much smaller than the test dataset - indicating the seeming simplicity of learning relational properties. However, MCML metrics based on model counting show that the performance can degrade substantially when tested against the entire (bounded) input space, indicating the high complexity of precisely learning these properties, and the usefulness of model counting in quantifying the true performance.

</details>

<details>

<summary>2020-09-07 13:32:07 - Robust Spoken Language Understanding with RL-based Value Error Recovery</summary>

- *Chen Liu, Su Zhu, Lu Chen, Kai Yu*

- `2009.03095v1` - [abs](http://arxiv.org/abs/2009.03095v1) - [pdf](http://arxiv.org/pdf/2009.03095v1)

> Spoken Language Understanding (SLU) aims to extract structured semantic representations (e.g., slot-value pairs) from speech recognized texts, which suffers from errors of Automatic Speech Recognition (ASR). To alleviate the problem caused by ASR-errors, previous works may apply input adaptations to the speech recognized texts, or correct ASR errors in predicted values by searching the most similar candidates in pronunciation. However, these two methods are applied separately and independently. In this work, we propose a new robust SLU framework to guide the SLU input adaptation with a rule-based value error recovery module. The framework consists of a slot tagging model and a rule-based value error recovery module. We pursue on an adapted slot tagging model which can extract potential slot-value pairs mentioned in ASR hypotheses and is suitable for the existing value error recovery module. After the value error recovery, we can achieve a supervision signal (reward) by comparing refined slot-value pairs with annotations. Since operations of the value error recovery are non-differentiable, we exploit policy gradient based Reinforcement Learning (RL) to optimize the SLU model. Extensive experiments on the public CATSLU dataset show the effectiveness of our proposed approach, which can improve the robustness of SLU and outperform the baselines by significant margins.

</details>

<details>

<summary>2020-09-07 17:26:18 - Improving Problem Identification via Automated Log Clustering using Dimensionality Reduction</summary>

- *Carl Martin Rosenberg, Leon Moonen*

- `2009.03257v1` - [abs](http://arxiv.org/abs/2009.03257v1) - [pdf](http://arxiv.org/pdf/2009.03257v1)

> Goal: We consider the problem of automatically grouping logs of runs that failed for the same underlying reasons, so that they can be treated more effectively, and investigate the following questions: (1) Does an approach developed to identify problems in system logs generalize to identifying problems in continuous deployment logs? (2) How does dimensionality reduction affect the quality of automated log clustering? (3) How does the criterion used for merging clusters in the clustering algorithm affect clustering quality?   Method: We replicate and extend earlier work on clustering system log files to assess its generalization to continuous deployment logs. We consider the optional inclusion of one of these dimensionality reduction techniques: Principal Component Analysis (PCA), Latent Semantic Indexing (LSI), and Non-negative Matrix Factorization (NMF). Moreover, we consider three alternative cluster merge criteria (Single Linkage, Average Linkage, and Weighted Linkage), in addition to the Complete Linkage criterion used in earlier work. We empirically evaluate the 16 resulting configurations on continuous deployment logs provided by our industrial collaborator.   Results: Our study shows that (1) identifying problems in continuous deployment logs via clustering is feasible, (2) including NMF significantly improves overall accuracy and robustness, and (3) Complete Linkage performs best of all merge criteria analyzed.   Conclusions: We conclude that problem identification via automated log clustering is improved by including dimensionality reduction, as it decreases the pipeline's sensitivity to parameter choice, thereby increasing its robustness for handling different inputs.

</details>

<details>

<summary>2020-09-08 02:45:43 - Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding</summary>

- *Chen Liu, Su Zhu, Zijian Zhao, Ruisheng Cao, Lu Chen, Kai Yu*

- `2005.11640v3` - [abs](http://arxiv.org/abs/2005.11640v3) - [pdf](http://arxiv.org/pdf/2005.11640v3)

> Spoken Language Understanding (SLU) converts hypotheses from automatic speech recognizer (ASR) into structured semantic representations. ASR recognition errors can severely degenerate the performance of the subsequent SLU module. To address this issue, word confusion networks (WCNs) have been used to encode the input for SLU, which contain richer information than 1-best or n-best hypotheses list. To further eliminate ambiguity, the last system act of dialogue context is also utilized as additional input. In this paper, a novel BERT based SLU model (WCN-BERT SLU) is proposed to encode WCNs and the dialogue context jointly. It can integrate both structural information and ASR posterior probabilities of WCNs in the BERT architecture. Experiments on DSTC2, a benchmark of SLU, show that the proposed method is effective and can outperform previous state-of-the-art models significantly.

</details>

<details>

<summary>2020-09-08 14:12:09 - Disentangled Non-Local Neural Networks</summary>

- *Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu*

- `2006.06668v2` - [abs](http://arxiv.org/abs/2006.06668v2) - [pdf](http://arxiv.org/pdf/2006.06668v2)

> The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics.

</details>

<details>

<summary>2020-09-08 15:53:55 - Towards Analyzing Semantic Robustness of Deep Neural Networks</summary>

- *Abdullah Hamdi, Bernard Ghanem*

- `1904.04621v4` - [abs](http://arxiv.org/abs/1904.04621v4) - [pdf](http://arxiv.org/pdf/1904.04621v4)

> Despite the impressive performance of Deep Neural Networks (DNNs) on various vision tasks, they still exhibit erroneous high sensitivity toward semantic primitives (e.g. object pose). We propose a theoretically grounded analysis for DNN robustness in the semantic space. We qualitatively analyze different DNNs' semantic robustness by visualizing the DNN global behavior as semantic maps and observe interesting behavior of some DNNs. Since generating these semantic maps does not scale well with the dimensionality of the semantic space, we develop a bottom-up approach to detect robust regions of DNNs. To achieve this, we formalize the problem of finding robust semantic regions of the network as optimizing integral bounds and we develop expressions for update directions of the region bounds. We use our developed formulations to quantitatively evaluate the semantic robustness of different popular network architectures. We show through extensive experimentation that several networks, while trained on the same dataset and enjoying comparable accuracy, do not necessarily perform similarly in semantic robustness. For example, InceptionV3 is more accurate despite being less semantically robust than ResNet50. We hope that this tool will serve as a milestone towards understanding the semantic robustness of DNNs.

</details>

<details>

<summary>2020-09-08 16:29:25 - LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets</summary>

- *Abhilasha Sancheti, Kushal Chawla, Gaurav Verma*

- `2009.03849v1` - [abs](http://arxiv.org/abs/2009.03849v1) - [pdf](http://arxiv.org/pdf/2009.03849v1)

> We describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing model achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.

</details>

<details>

<summary>2020-09-08 22:54:03 - Brown University at TREC Deep Learning 2019</summary>

- *George Zerveas, Ruochen Zhang, Leila Kim, Carsten Eickhoff*

- `2009.04016v1` - [abs](http://arxiv.org/abs/2009.04016v1) - [pdf](http://arxiv.org/pdf/2009.04016v1)

> This paper describes Brown University's submission to the TREC 2019 Deep Learning track. We followed a 2-phase method for producing a ranking of passages for a given input query: In the the first phase, the user's query is expanded by appending 3 queries generated by a transformer model which was trained to rephrase an input query into semantically similar queries. The expanded query can exhibit greater similarity in surface form and vocabulary overlap with the passages of interest and can therefore serve as enriched input to any downstream information retrieval method. In the second phase, we use a BERT-based model pre-trained for language modeling but fine-tuned for query - document relevance prediction to compute relevance scores for a set of 1000 candidate passages per query and subsequently obtain a ranking of passages by sorting them based on the predicted relevance scores. According to the results published in the official Overview of the TREC Deep Learning Track 2019, our team ranked 3rd in the passage retrieval task (including full ranking and re-ranking), and 2nd when considering only re-ranking submissions.

</details>

<details>

<summary>2020-09-08 23:34:18 - QED: A Framework and Dataset for Explanations in Question Answering</summary>

- *Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, Michael Collins*

- `2009.06354v1` - [abs](http://arxiv.org/abs/2009.06354v1) - [pdf](http://arxiv.org/pdf/2009.06354v1)

> A question answering system that in addition to providing an answer provides an explanation of the reasoning that leads to that answer has potential advantages in terms of debuggability, extensibility and trust. To this end, we propose QED, a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. We describe and publicly release an expert-annotated dataset of QED explanations built upon a subset of the Google Natural Questions dataset, and report baseline models on two tasks -- post-hoc explanation generation given an answer, and joint question answering and explanation generation. In the joint setting, a promising result suggests that training on a relatively small amount of QED data can improve question answering. In addition to describing the formal, language-theoretic motivations for the QED approach, we describe a large user study showing that the presence of QED explanations significantly improves the ability of untrained raters to spot errors made by a strong neural QA baseline.

</details>

<details>

<summary>2020-09-09 05:00:02 - Rule-Guided Graph Neural Networks for Recommender Systems</summary>

- *Xinze Lyu, Guangyao Li, Jiacheng Huang, Wei Hu*

- `2009.04104v1` - [abs](http://arxiv.org/abs/2009.04104v1) - [pdf](http://arxiv.org/pdf/2009.04104v1)

> To alleviate the cold start problem caused by collaborative filtering in recommender systems, knowledge graphs (KGs) are increasingly employed by many methods as auxiliary resources. However, existing work incorporated with KGs cannot capture the explicit long-range semantics between users and items meanwhile consider various connectivity between items. In this paper, we propose RGRec, which combines rule learning and graph neural networks (GNNs) for recommendation. RGRec first maps items to corresponding entities in KGs and adds users as new entities. Then, it automatically learns rules to model the explicit long-range semantics, and captures the connectivity between entities by aggregation to better encode various information. We show the effectiveness of RGRec on three real-world datasets. Particularly, the combination of rule learning and GNNs achieves substantial improvement compared to methods only using either of them.

</details>

<details>

<summary>2020-09-09 05:19:24 - Language Guided Networks for Cross-modal Moment Retrieval</summary>

- *Kun Liu, Huadong Ma, Chuang Gan*

- `2006.10457v2` - [abs](http://arxiv.org/abs/2006.10457v2) - [pdf](http://arxiv.org/pdf/2006.10457v2)

> We address the challenging task of cross-modal moment retrieval, which aims to localize a temporal segment from an untrimmed video described by a natural language query. It poses great challenges over the proper semantic alignment between vision and linguistic domains. Existing methods independently extract the features of videos and sentences and purely utilize the sentence embedding in the multi-modal fusion stage, which do not make full use of the potential of language. In this paper, we present Language Guided Networks (LGN), a new framework that leverages the sentence embedding to guide the whole process of moment retrieval. In the first feature extraction stage, we propose to jointly learn visual and language features to capture the powerful visual information which can cover the complex semantics in the sentence query. Specifically, the early modulation unit is designed to modulate the visual feature extractor's feature maps by a linguistic embedding. Then we adopt a multi-modal fusion module in the second fusion stage. Finally, to get a precise localizer, the sentence information is utilized to guide the process of predicting temporal positions. Specifically, the late guidance module is developed to linearly transform the output of localization networks via the channel attention mechanism. The experimental results on two popular datasets demonstrate the superior performance of our proposed method on moment retrieval (improving by 5.8\% in terms of Rank1@IoU0.5 on Charades-STA and 5.2\% on TACoS). The source code for the complete system will be publicly available.

</details>

<details>

<summary>2020-09-09 14:12:13 - Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review</summary>

- *Yaodong Cui, Ren Chen, Wenbo Chu, Long Chen, Daxin Tian, Ying Li, Dongpu Cao*

- `2004.05224v2` - [abs](http://arxiv.org/abs/2004.05224v2) - [pdf](http://arxiv.org/pdf/2004.05224v2)

> Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this paper devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.

</details>

<details>

<summary>2020-09-09 16:16:06 - Privacy-Preserving Machine Learning in Untrusted Clouds Made Simple</summary>

- *Dayeol Lee, Dmitrii Kuvaiskii, Anjo Vahldiek-Oberwagner, Mona Vij*

- `2009.04390v1` - [abs](http://arxiv.org/abs/2009.04390v1) - [pdf](http://arxiv.org/pdf/2009.04390v1)

> We present a practical framework to deploy privacy-preserving machine learning (PPML) applications in untrusted clouds based on a trusted execution environment (TEE). Specifically, we shield unmodified PyTorch ML applications by running them in Intel SGX enclaves with encrypted model parameters and encrypted input data to protect the confidentiality and integrity of these secrets at rest and during runtime. We use the open-source Graphene library OS with transparent file encryption and SGX-based remote attestation to minimize porting effort and seamlessly provide file protection and attestation. Our approach is completely transparent to the machine learning application: the developer and the end-user do not need to modify the ML application in any way.

</details>

<details>

<summary>2020-09-09 18:00:15 - Aspect Classification for Legal Depositions</summary>

- *Saurabh Chakravarty, Satvik Chekuri, Maanav Mehrotra, Edward A. Fox*

- `2009.04485v1` - [abs](http://arxiv.org/abs/2009.04485v1) - [pdf](http://arxiv.org/pdf/2009.04485v1)

> Attorneys and others have a strong interest in having a digital library with suitable services (e.g., summarizing, searching, and browsing) to help them work with large corpora of legal depositions. Their needs often involve understanding the semantics of such documents. That depends in part on the role of the deponent, e.g., plaintiff, defendant, law enforcement personnel, expert, etc. In the case of tort litigation associated with property and casualty insurance claims, such as relating to an injury, it is important to know not only about liability, but also about events, accidents, physical conditions, and treatments.   We hypothesize that a legal deposition consists of various aspects that are discussed as part of the deponent testimony. Accordingly, we developed an ontology of aspects in a legal deposition for accident and injury cases. Using that, we have developed a classifier that can identify portions of text for each of the aspects of interest. Doing so was complicated by the peculiarities of this genre, e.g., that deposition transcripts generally consist of data in the form of question-answer (QA) pairs. Accordingly, our automated system starts with pre-processing, and then transforms the QA pairs into a canonical form made up of declarative sentences. Classifying the declarative sentences that are generated, according to the aspect, can then help with downstream tasks such as summarization, segmentation, question-answering, and information retrieval.   Our methods have achieved a classification F1 score of 0.83. Having the aspects classified with a good accuracy will help in choosing QA pairs that can be used as candidate summary sentences, and to generate an informative summary for legal professionals or insurance claim agents. Our methodology could be extended to legal depositions of other kinds, and to aid services like searching.

</details>

<details>

<summary>2020-09-10 09:04:42 - Improving Maritime Traffic Emission Estimations on Missing Data with CRBMs</summary>

- *Alberto Gutierrez-Torre, Josep Ll. Berral, David Buchaca, Marc Guevara, Albert Soret, David Carrera*

- `2009.03001v2` - [abs](http://arxiv.org/abs/2009.03001v2) - [pdf](http://arxiv.org/pdf/2009.03001v2)

> Maritime traffic emissions are a major concern to governments as they heavily impact the Air Quality in coastal cities. Ships use the Automatic Identification System (AIS) to continuously report position and speed among other features, and therefore this data is suitable to be used to estimate emissions, if it is combined with engine data. However, important ship features are often inaccurate or missing. State-of-the-art complex systems, like CALIOPE at the Barcelona Supercomputing Center, are used to model Air Quality. These systems can benefit from AIS based emission models as they are very precise in positioning the pollution. Unfortunately, these models are sensitive to missing or corrupted data, and therefore they need data curation techniques to significantly improve the estimation accuracy. In this work, we propose a methodology for treating ship data using Conditional Restricted Boltzmann Machines (CRBMs) plus machine learning methods to improve the quality of data passed to emission models. Results show that we can improve the default methods proposed to cover missing data. In our results, we observed that using our method the models boosted their accuracy to detect otherwise undetectable emissions. In particular, we used a real data-set of AIS data, provided by the Spanish Port Authority, to estimate that thanks to our method, the model was able to detect 45% of additional emissions, of additional emissions, representing 152 tonnes of pollutants per week in Barcelona and propose new features that may enhance emission modeling.

</details>

<details>

<summary>2020-09-10 13:49:38 - Semantic Segmentation of Histopathological Slides for the Classification of Cutaneous Lymphoma and Eczema</summary>

- *Jérémy Scheurer, Claudio Ferrari, Luis Berenguer Todo Bom, Michaela Beer, Werner Kempf, Luis Haug*

- `2009.05403v1` - [abs](http://arxiv.org/abs/2009.05403v1) - [pdf](http://arxiv.org/pdf/2009.05403v1)

> Mycosis fungoides (MF) is a rare, potentially life threatening skin disease, which in early stages clinically and histologically strongly resembles Eczema, a very common and benign skin condition. In order to increase the survival rate, one needs to provide the appropriate treatment early on. To this end, one crucial step for specialists is the evaluation of histopathological slides (glass slides), or Whole Slide Images (WSI), of the patients' skin tissue. We introduce a deep learning aided diagnostics tool that brings a two-fold value to the decision process of pathologists. First, our algorithm accurately segments WSI into regions that are relevant for an accurate diagnosis, achieving a Mean-IoU of 69% and a Matthews Correlation score of 83% on a novel dataset. Additionally, we also show that our model is competitive with the state of the art on a reference dataset. Second, using the segmentation map and the original image, we are able to predict if a patient has MF or Eczema. We created two models that can be applied in different stages of the diagnostic pipeline, potentially eliminating life-threatening mistakes. The classification outcome is considerably more interpretable than using only the WSI as the input, since it is also based on the segmentation map. Our segmentation model, which we call EU-Net, extends a classical U-Net with an EfficientNet-B7 encoder which was pre-trained on the Imagenet dataset.

</details>

<details>

<summary>2020-09-10 14:09:02 - Efficient Binary-Level Coverage Analysis</summary>

- *M. Ammar Ben Khadra, Dominik Stoffel, Wolfgang Kunz*

- `2004.14191v3` - [abs](http://arxiv.org/abs/2004.14191v3) - [pdf](http://arxiv.org/pdf/2004.14191v3)

> Code coverage analysis plays an important role in the software testing process. More recently, the remarkable effectiveness of coverage feedback has triggered a broad interest in feedback-guided fuzzing. In this work, we introduce bcov, a tool for binary-level coverage analysis. Our tool statically instruments x86-64 binaries in the ELF format without compiler support. We implement several techniques to improve efficiency and scale to large real-world software. First, we bring Agrawal's probe pruning technique to binary-level instrumentation and effectively leverage its superblocks to reduce overhead. Second, we introduce sliced microexecution, a robust technique for jump table analysis which improves CFG precision and enables us to instrument jump table entries. Additionally, smaller instructions in x86-64 pose a challenge for inserting detours. To address this challenge, we aggressively exploit padding bytes and systematically host detours in neighboring basic blocks. We evaluate bcov on a corpus of 95 binaries compiled from eight popular and well-tested packages like FFmpeg and LLVM. Two instrumentation policies, with different edge-level precision, are used to patch all functions in this corpus - over 1.6 million functions. Our precise policy has average performance and memory overheads of 14% and 22% respectively. Instrumented binaries do not introduce any test regressions. The reported coverage is highly accurate with an average F-score of 99.86%. Finally, our jump table analysis is comparable to that of IDA Pro on gcc binaries and outperforms it on clang binaries.

</details>

<details>

<summary>2020-09-10 14:50:53 - Possible Controllability of Control Argumentation Frameworks -- Extended Version</summary>

- *Jean-Guy Mailly*

- `2009.04903v1` - [abs](http://arxiv.org/abs/2009.04903v1) - [pdf](http://arxiv.org/pdf/2009.04903v1)

> The recent Control Argumentation Framework (CAF) is a generalization of Dung's Argumentation Framework which handles argumentation dynamics under uncertainty; especially it can be used to model the behavior of an agent which can anticipate future changes in the environment. Here we provide new insights on this model by defining the notion of possible controllability of a CAF. We study the complexity of this new form of reasoning for the four classical semantics, and we provide a logical encoding for reasoning with this framework.

</details>

<details>

<summary>2020-09-10 17:38:32 - Investigating Gender Bias in BERT</summary>

- *Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria*

- `2009.05021v1` - [abs](http://arxiv.org/abs/2009.05021v1) - [pdf](http://arxiv.org/pdf/2009.05021v1)

> Contextual language models (CLMs) have pushed the NLP benchmarks to a new height. It has become a new norm to utilize CLM provided word embeddings in downstream tasks such as text classification. However, unless addressed, CLMs are prone to learn intrinsic gender-bias in the dataset. As a result, predictions of downstream NLP models can vary noticeably by varying gender words, such as replacing "he" to "she", or even gender-neutral words. In this paper, we focus our analysis on a popular CLM, i.e., BERT. We analyse the gender-bias it induces in five downstream tasks related to emotion and sentiment intensity prediction. For each task, we train a simple regressor utilizing BERT's word embeddings. We then evaluate the gender-bias in regressors using an equity evaluation corpus. Ideally and from the specific design, the models should discard gender informative features from the input. However, the results show a significant dependence of the system's predictions on gender-particular words and phrases. We claim that such biases can be reduced by removing genderspecific features from word embedding. Hence, for each layer in BERT, we identify directions that primarily encode gender information. The space formed by such directions is referred to as the gender subspace in the semantic space of word embeddings. We propose an algorithm that finds fine-grained gender directions, i.e., one primary direction for each BERT layer. This obviates the need of realizing gender subspace in multiple dimensions and prevents other crucial information from being omitted. Experiments show that removing embedding components in such directions achieves great success in reducing BERT-induced bias in the downstream tasks.

</details>

<details>

<summary>2020-09-10 17:41:03 - Learning Shape Features and Abstractions in 3D Convolutional Neural Networks for Detecting Alzheimer's Disease</summary>

- *Md Motiur Rahman Sagar, Martin Dyrba*

- `2009.05023v1` - [abs](http://arxiv.org/abs/2009.05023v1) - [pdf](http://arxiv.org/pdf/2009.05023v1)

> Deep Neural Networks - especially Convolutional Neural Network (ConvNet) has become the state-of-the-art for image classification, pattern recognition and various computer vision tasks. ConvNet has a huge potential in medical domain for analyzing medical data to diagnose diseases in an efficient way. Based on extracted features by ConvNet model from MRI data, early diagnosis is very crucial for preventing progress and treating the Alzheimer's disease. Despite having the ability to deliver great performance, absence of interpretability of the model's decision can lead to misdiagnosis which can be life threatening. In this thesis, learned shape features and abstractions by 3D ConvNets for detecting Alzheimer's disease were investigated using various visualization techniques. How changes in network structures, used filters sizes and filters shapes affects the overall performance and learned features of the model were also inspected. LRP relevance map of different models revealed which parts of the brain were more relevant for the classification decision. Comparing the learned filters by Activation Maximization showed how patterns were encoded in different layers of the network. Finally, transfer learning from a convolutional autoencoder was implemented to check whether increasing the number of training samples with patches of input to extract the low-level features improves learned features and the model performance.

</details>

<details>

<summary>2020-09-10 17:53:19 - GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of Graph, Raster and Vector Data -- Technical Report</summary>

- *Timo Homburg, Steffen Staab, Daniel Janke*

- `2009.05032v1` - [abs](http://arxiv.org/abs/2009.05032v1) - [pdf](http://arxiv.org/pdf/2009.05032v1)

> We introduce an approach to semantically represent and query raster data in a Semantic Web graph. We extend the GeoSPARQL vocabulary and query language to support raster data as a new type of geospatial data. We define new filter functions and illustrate our approach using several use cases on real-world data sets. Finally, we describe a prototypical implementation and validate the feasibility of our approach.

</details>

<details>

<summary>2020-09-11 01:01:34 - An Argumentation-based Approach for Identifying and Dealing with Incompatibilities among Procedural Goals</summary>

- *Mariela Morveli-Espinoza, Juan Carlos Nieves, Ayslan Possebom, Josep Puyol-Gruart, Cesar Augusto Tacla*

- `2009.05186v1` - [abs](http://arxiv.org/abs/2009.05186v1) - [pdf](http://arxiv.org/pdf/2009.05186v1)

> During the first step of practical reasoning, i.e. deliberation, an intelligent agent generates a set of pursuable goals and then selects which of them he commits to achieve. An intelligent agent may in general generate multiple pursuable goals, which may be incompatible among them. In this paper, we focus on the definition, identification and resolution of these incompatibilities. The suggested approach considers the three forms of incompatibility introduced by Castelfranchi and Paglieri, namely the terminal incompatibility, the instrumental or resources incompatibility and the superfluity. We characterise computationally these forms of incompatibility by means of arguments that represent the plans that allow an agent to achieve his goals. Thus, the incompatibility among goals is defined based on the conflicts among their plans, which are represented by means of attacks in an argumentation framework. We also work on the problem of goals selection; we propose to use abstract argumentation theory to deal with this problem, i.e. by applying argumentation semantics. We use a modified version of the "cleaner world" scenario in order to illustrate the performance of our proposal.

</details>

<details>

<summary>2020-09-11 19:33:14 - A Principled Approach to GraphQL Query Cost Analysis</summary>

- *Alan Cha, Erik Wittern, Guillaume Baudart, James C. Davis, Louis Mandel, Jim A. Laredo*

- `2009.05632v1` - [abs](http://arxiv.org/abs/2009.05632v1) - [pdf](http://arxiv.org/pdf/2009.05632v1)

> The landscape of web APIs is evolving to meet new client requirements and to facilitate how providers fulfill them. A recent web API model is GraphQL, which is both a query language and a runtime. Using GraphQL, client queries express the data they want to retrieve or mutate, and servers respond with exactly those data or changes. GraphQL's expressiveness is risky for service providers because clients can succinctly request stupendous amounts of data, and responding to overly complex queries can be costly or disrupt service availability. Recent empirical work has shown that many service providers are at risk. Using traditional API management methods is not sufficient, and practitioners lack principled means of estimating and measuring the cost of the GraphQL queries they receive. In this work, we present a linear-time GraphQL query analysis that can measure the cost of a query without executing it. Our approach can be applied in a separate API management layer and used with arbitrary GraphQL backends. In contrast to existing static approaches, our analysis supports common GraphQL conventions that affect query cost, and our analysis is provably correct based on our formal specification of GraphQL semantics. We demonstrate the potential of our approach using a novel GraphQL query-response corpus for two commercial GraphQL APIs. Our query analysis consistently obtains upper cost bounds, tight enough relative to the true response sizes to be actionable for service providers. In contrast, existing static GraphQL query analyses exhibit over-estimates and under-estimates because they fail to support GraphQL conventions.

</details>

<details>

<summary>2020-09-11 19:35:09 - Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers</summary>

- *Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Sundaresan*

- `2009.05634v1` - [abs](http://arxiv.org/abs/2009.05634v1) - [pdf](http://arxiv.org/pdf/2009.05634v1)

> Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task. In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semi-supervised fashion on a large corpus of source code. Finally, we finetune this model on the task of generating assert statements for unit tests. The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62% of the cases in the first attempt. The results show 80% relative improvement for top-1 accuracy over the previous RNN-based approach in the literature. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment EvoSuite test cases, with additional asserts leading to improved test coverage.

</details>

<details>

<summary>2020-09-12 07:01:12 - Syntax Role for Neural Semantic Role Labeling</summary>

- *Zuchao Li, Hai Zhao, Shexia He, Jiaxun Cai*

- `2009.05737v1` - [abs](http://arxiv.org/abs/2009.05737v1) - [pdf](http://arxiv.org/pdf/2009.05737v1)

> Semantic role labeling (SRL) is dedicated to recognizing the semantic predicate-argument structure of a sentence. Previous studies in terms of traditional models have shown syntactic information can make remarkable contributions to SRL performance; however, the necessity of syntactic information was challenged by a few recent neural SRL studies that demonstrate impressive performance without syntactic backbones and suggest that syntax information becomes much less important for neural semantic role labeling, especially when paired with recent deep neural network and large-scale pre-trained language models. Despite this notion, the neural SRL field still lacks a systematic and full investigation on the relevance of syntactic information in SRL, for both dependency and both monolingual and multilingual settings. This paper intends to quantify the importance of syntactic information for neural SRL in the deep learning framework. We introduce three typical SRL frameworks (baselines), sequence-based, tree-based, and graph-based, which are accompanied by two categories of exploiting syntactic information: syntax pruning-based and syntax feature-based. Experiments are conducted on the CoNLL-2005, 2009, and 2012 benchmarks for all languages available, and results show that neural SRL models can still benefit from syntactic information under certain conditions. Furthermore, we show the quantitative significance of syntax to neural SRL models together with a thorough empirical survey using existing models.

</details>

<details>

<summary>2020-09-12 15:18:48 - Learning semantic Image attributes using Image recognition and knowledge graph embeddings</summary>

- *Ashutosh Tiwari, Sandeep Varma*

- `2009.05812v1` - [abs](http://arxiv.org/abs/2009.05812v1) - [pdf](http://arxiv.org/pdf/2009.05812v1)

> Extracting structured knowledge from texts has traditionally been used for knowledge base generation. However, other sources of information, such as images can be leveraged into this process to build more complete and richer knowledge bases. Structured semantic representation of the content of an image and knowledge graph embeddings can provide a unique representation of semantic relationships between image entities. Linking known entities in knowledge graphs and learning open-world images using language models has attracted lots of interest over the years. In this paper, we propose a shared learning approach to learn semantic attributes of images by combining a knowledge graph embedding model with the recognized attributes of images. The proposed model premises to help us understand the semantic relationship between the entities of an image and implicitly provide a link for the extracted entities through a knowledge graph embedding model. Under the limitation of using a custom user-defined knowledge base with limited data, the proposed model presents significant accuracy and provides a new alternative to the earlier approaches. The proposed approach is a step towards bridging the gap between frameworks which learn from large amounts of data and frameworks which use a limited set of predicates to infer new knowledge.

</details>

<details>

<summary>2020-09-12 17:36:53 - Exploring the Hierarchy in Relation Labels for Scene Graph Generation</summary>

- *Yi Zhou, Shuyang Sun, Chao Zhang, Yikang Li, Wanli Ouyang*

- `2009.05834v1` - [abs](http://arxiv.org/abs/2009.05834v1) - [pdf](http://arxiv.org/pdf/2009.05834v1)

> By assigning each relationship a single label, current approaches formulate the relationship detection as a classification problem. Under this formulation, predicate categories are treated as completely different classes. However, different from the object labels where different classes have explicit boundaries, predicates usually have overlaps in their semantic meanings. For example, sit\_on and stand\_on have common meanings in vertical relationships but different details of how these two objects are vertically placed. In order to leverage the inherent structures of the predicate categories, we propose to first build the language hierarchy and then utilize the Hierarchy Guided Feature Learning (HGFL) strategy to learn better region features of both the coarse-grained level and the fine-grained level. Besides, we also propose the Hierarchy Guided Module (HGM) to utilize the coarse-grained level to guide the learning of fine-grained level features. Experiments show that the proposed simple yet effective method can improve several state-of-the-art baselines by a large margin (up to $33\%$ relative gain) in terms of Recall@50 on the task of Scene Graph Generation in different datasets.

</details>

<details>

<summary>2020-09-12 18:58:32 - Understanding the Role of Individual Units in a Deep Neural Network</summary>

- *David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, Antonio Torralba*

- `2009.05041v2` - [abs](http://arxiv.org/abs/2009.05041v2) - [pdf](http://arxiv.org/pdf/2009.05041v2)

> Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.

</details>

<details>

<summary>2020-09-12 19:53:55 - Semantic Photo Manipulation with a Generative Image Prior</summary>

- *David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba*

- `2005.07727v2` - [abs](http://arxiv.org/abs/2005.07727v2) - [pdf](http://arxiv.org/pdf/2005.07727v2)

> Despite the recent success of GANs in synthesizing images conditioned on inputs such as a user sketch, text, or semantic labels, manipulating the high-level attributes of an existing natural photograph with GANs is challenging for two reasons. First, it is hard for GANs to precisely reproduce an input image. Second, after manipulation, the newly synthesized pixels often do not fit the original image. In this paper, we address these issues by adapting the image prior learned by GANs to image statistics of an individual image. Our method can accurately reconstruct the input image and synthesize new content, consistent with the appearance of the input image. We demonstrate our interactive system on several semantic image editing tasks, including synthesizing new objects consistent with background, removing unwanted objects, and changing the appearance of an object. Quantitative and qualitative comparisons against several existing methods demonstrate the effectiveness of our method.

</details>

<details>

<summary>2020-09-12 19:56:10 - Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview</summary>

- *Deven Shah, H. Andrew Schwartz, Dirk Hovy*

- `1912.11078v2` - [abs](http://arxiv.org/abs/1912.11078v2) - [pdf](http://arxiv.org/pdf/1912.11078v2)

> An increasing number of works in natural language processing have addressed the effect of bias on the predicted outcomes, introducing mitigation techniques that act on different parts of the standard NLP pipeline (data and models). However, these works have been conducted in isolation, without a unifying framework to organize efforts within the field. This leads to repetitive approaches, and puts an undue focus on the effects of bias, rather than on their origins. Research focused on bias symptoms rather than the underlying origins could limit the development of effective countermeasures. In this paper, we propose a unifying conceptualization: the predictive bias framework for NLP. We summarize the NLP literature and propose a general mathematical definition of predictive bias in NLP along with a conceptual framework, differentiating four main origins of biases: label bias, selection bias, model overamplification, and semantic bias. We discuss how past work has countered each bias origin. Our framework serves to guide an introductory overview of predictive bias in NLP, integrating existing work into a single structure and opening avenues for future research.

</details>

<details>

<summary>2020-09-13 02:08:10 - Argumentation-based Agents that Explain their Decisions</summary>

- *Mariela Morveli-Espinoza, Ayslan Possebom, Cesar Augusto Tacla*

- `2009.05897v1` - [abs](http://arxiv.org/abs/2009.05897v1) - [pdf](http://arxiv.org/pdf/2009.05897v1)

> Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots.

</details>

<details>

<summary>2020-09-13 14:11:00 - A Review of Visual Descriptors and Classification Techniques Used in Leaf Species Identification</summary>

- *K. K. Thyagharajan, I. Kiruba Raji*

- `2009.06001v1` - [abs](http://arxiv.org/abs/2009.06001v1) - [pdf](http://arxiv.org/pdf/2009.06001v1)

> Plants are fundamentally important to life. Key research areas in plant science include plant species identification, weed classification using hyper spectral images, monitoring plant health and tracing leaf growth, and the semantic interpretation of leaf information. Botanists easily identify plant species by discriminating between the shape of the leaf, tip, base, leaf margin and leaf vein, as well as the texture of the leaf and the arrangement of leaflets of compound leaves. Because of the increasing demand for experts and calls for biodiversity, there is a need for intelligent systems that recognize and characterize leaves so as to scrutinize a particular species, the diseases that affect them, the pattern of leaf growth, and so on. We review several image processing methods in the feature extraction of leaves, given that feature extraction is a crucial technique in computer vision. As computers cannot comprehend images, they are required to be converted into features by individually analysing image shapes, colours, textures and moments. Images that look the same may deviate in terms of geometric and photometric variations. In our study, we also discuss certain machine learning classifiers for an analysis of different species of leaves.

</details>

<details>

<summary>2020-09-13 22:41:01 - Tax Knowledge Graph for a Smarter and More Personalized TurboTax</summary>

- *Jay Yu, Kevin McCluskey, Saikat Mukherjee*

- `2009.06103v1` - [abs](http://arxiv.org/abs/2009.06103v1) - [pdf](http://arxiv.org/pdf/2009.06103v1)

> Most knowledge graph use cases are data-centric, focusing on representing data entities and their semantic relationships. There are no published success stories to represent large-scale complicated business logic with knowledge graph technologies. In this paper, we will share our innovative and practical approach to representing complicated U.S. and Canadian income tax compliance logic (calculations and rules) via a large-scale knowledge graph. We will cover how the Tax Knowledge Graph is constructed and automated, how it is used to calculate tax refunds, reasoned to find missing info, and navigated to explain the calculated results. The Tax Knowledge Graph has helped transform Intuit's flagship TurboTax product into a smart and personalized experience, accelerating and automating the tax preparation process while instilling confidence for millions of customers.

</details>

<details>

<summary>2020-09-14 05:04:19 - Learning from Multimodal and Multitemporal Earth Observation Data for Building Damage Mapping</summary>

- *Bruno Adriano, Naoto Yokoya, Junshi Xia, Hiroyuki Miura, Wen Liu, Masashi Matsuoka, Shunichi Koshimura*

- `2009.06200v1` - [abs](http://arxiv.org/abs/2009.06200v1) - [pdf](http://arxiv.org/pdf/2009.06200v1)

> Earth observation technologies, such as optical imaging and synthetic aperture radar (SAR), provide excellent means to monitor ever-growing urban environments continuously. Notably, in the case of large-scale disasters (e.g., tsunamis and earthquakes), in which a response is highly time-critical, images from both data modalities can complement each other to accurately convey the full damage condition in the disaster's aftermath. However, due to several factors, such as weather and satellite coverage, it is often uncertain which data modality will be the first available for rapid disaster response efforts. Hence, novel methodologies that can utilize all accessible EO datasets are essential for disaster management. In this study, we have developed a global multisensor and multitemporal dataset for building damage mapping. We included building damage characteristics from three disaster types, namely, earthquakes, tsunamis, and typhoons, and considered three building damage categories. The global dataset contains high-resolution optical imagery and high-to-moderate-resolution multiband SAR data acquired before and after each disaster. Using this comprehensive dataset, we analyzed five data modality scenarios for damage mapping: single-mode (optical and SAR datasets), cross-modal (pre-disaster optical and post-disaster SAR datasets), and mode fusion scenarios. We defined a damage mapping framework for the semantic segmentation of damaged buildings based on a deep convolutional neural network algorithm. We compare our approach to another state-of-the-art baseline model for damage mapping. The results indicated that our dataset, together with a deep learning network, enabled acceptable predictions for all the data modality scenarios.

</details>

<details>

<summary>2020-09-14 09:18:20 - REXUP: I REason, I EXtract, I UPdate with Structured Compositional Reasoning for Visual Question Answering</summary>

- *Siwen Luo, Soyeon Caren Han, Kaiyuan Sun, Josiah Poon*

- `2007.13262v2` - [abs](http://arxiv.org/abs/2007.13262v2) - [pdf](http://arxiv.org/pdf/2007.13262v2)

> Visual question answering (VQA) is a challenging multi-modal task that requires not only the semantic understanding of both images and questions, but also the sound perception of a step-by-step reasoning process that would lead to the correct answer. So far, most successful attempts in VQA have been focused on only one aspect, either the interaction of visual pixel features of images and word features of questions, or the reasoning process of answering the question in an image with simple objects. In this paper, we propose a deep reasoning VQA model with explicit visual structure-aware textual information, and it works well in capturing step-by-step reasoning process and detecting a complex object-relationship in photo-realistic images. REXUP network consists of two branches, image object-oriented and scene graph oriented, which jointly works with super-diagonal fusion compositional attention network. We quantitatively and qualitatively evaluate REXUP on the GQA dataset and conduct extensive ablation studies to explore the reasons behind REXUP's effectiveness. Our best model significantly outperforms the precious state-of-the-art, which delivers 92.7% on the validation set and 73.1% on the test-dev set.

</details>

<details>

<summary>2020-09-14 15:16:25 - At your Command! An Empirical Study on How LaypersonsTeach Robots New Functions</summary>

- *Sebastian Weigelt, Vanessa Steurer, Walter F. Tichy*

- `2009.06510v1` - [abs](http://arxiv.org/abs/2009.06510v1) - [pdf](http://arxiv.org/pdf/2009.06510v1)

> Even though intelligent systems such as Siri or Google Assistant are enjoyable (and useful) dialog partners, users can only access predefined functionality. Enabling end-users to extend the functionality of intelligent systems will be the next big thing. To promote research in this area we carried out an empirical study on how laypersons teach robots new functions by means of natural language instructions. The result is a labeled corpus consisting of 3168 submissions given by 870 subjects. The analysis of the dataset revealed that many participants used certain wordings to express their wish to teach new functionality; two corresponding trigrams are among the most frequent. On the contrary, more than one third (36.93%) did not verbalize the teaching intent at all. We labeled the semantic constituents in the utterances: declaration (including the name of the function) and intermediate steps. The full corpus is publicly available: http://dx.doi.org/10.21227/zecn-6c61

</details>

<details>

<summary>2020-09-14 17:51:52 - Knowledge-Based Legal Document Assembly</summary>

- *Marko Marković, Stevan Gostojić*

- `2009.06611v1` - [abs](http://arxiv.org/abs/2009.06611v1) - [pdf](http://arxiv.org/pdf/2009.06611v1)

> This paper proposes a knowledge-based legal document assembly method that uses a machine-readable representation of knowledge of legal professionals. This knowledgebase has two components - the formal knowledge of legal norms represented as a rule-base and the tacit knowledge represented by a document template. A document assembly system is developed as a proof of concept. It collects input data in the form of an interactive interview, performs legal reasoning over input data, and generates the output document. The system also creates an argument graph as an explanation of the reasoning process providing the user with an interpretation of how the input data and the rule-base influence the content of the output document. The system also semantically marks up data in the output document, facilitating its further processing and providing support to the interoperability of information systems in the legal domain.

</details>

<details>

<summary>2020-09-15 02:13:15 - Autonomization of Monoidal Categories</summary>

- *Antonin Delpeuch*

- `1411.3827v4` - [abs](http://arxiv.org/abs/1411.3827v4) - [pdf](http://arxiv.org/pdf/1411.3827v4)

> We show that contrary to common belief in the DisCoCat community, a monoidal category is all that is needed to define a categorical compositional model of natural language. This relies on a construction which freely adds adjoints to a monoidal category. In the case of distributional semantics, this broadens the range of available models, to include non-linear maps and cartesian products for instance. We illustrate the applications of this principle to various distributional models of meaning.

</details>

<details>

<summary>2020-09-15 02:14:18 - Functorial Question Answering</summary>

- *Giovanni de Felice, Konstantinos Meichanetzidis, Alexis Toumi*

- `1905.07408v3` - [abs](http://arxiv.org/abs/1905.07408v3) - [pdf](http://arxiv.org/pdf/1905.07408v3)

> Distributional compositional (DisCo) models are functors that compute the meaning of a sentence from the meaning of its words. We show that DisCo models in the category of sets and relations correspond precisely to relational databases. As a consequence, we get complexity-theoretic reductions from semantics and entailment of a fragment of natural language to evaluation and containment of conjunctive queries, respectively. Finally, we define question answering as an NP-complete problem.

</details>

<details>

<summary>2020-09-15 02:16:08 - A Compositional Framework for Scientific Model Augmentation</summary>

- *Micah Halter, Christine Herlihy, James Fairbanks*

- `1907.03536v2` - [abs](http://arxiv.org/abs/1907.03536v2) - [pdf](http://arxiv.org/pdf/1907.03536v2)

> Scientists construct and analyze computational models to understand the world. That understanding comes from efforts to augment, combine, and compare models of related phenomena. We propose SemanticModels.jl, a system that leverages techniques from static and dynamic program analysis to process executable versions of scientific models to perform such metamodeling tasks. By framing these metamodeling tasks as metaprogramming problems, SemanticModels.jl enables writing programs that generate and expand models. To this end, we present a category theory-based framework for defining metamodeling tasks, and extracting semantic information from model implementations, and show how this framework can be used to enhance scientific workflows in a working case study.

</details>

<details>

<summary>2020-09-15 03:07:53 - PRF: A Framework for Building Automatic Program Repair Prototypes for JVM-Based Languages</summary>

- *Ali Ghanbari, Andrian Marcus*

- `2009.06848v1` - [abs](http://arxiv.org/abs/2009.06848v1) - [pdf](http://arxiv.org/pdf/2009.06848v1)

> PRF is a Java-based framework that allows researchers to build prototypes of test-based generate-and-validate automatic program repair techniques for JVM languages by simply extending it with their patch generation plugins. The framework also provides other useful components for constructing automatic program repair tools, e.g., a fault localization component that provides spectrum-based fault localization information at different levels of granularity, a configurable and safe patch validation component that is 11+X faster than vanilla testing, and a customizable post-processing component to generate fix reports. A demo video of PRF is available at https://bit.ly/3ehduSS.

</details>

<details>

<summary>2020-09-15 08:03:41 - Multi-scale Attention U-Net (MsAUNet): A Modified U-Net Architecture for Scene Segmentation</summary>

- *Soham Chattopadhyay, Hritam Basak*

- `2009.06911v1` - [abs](http://arxiv.org/abs/2009.06911v1) - [pdf](http://arxiv.org/pdf/2009.06911v1)

> Despite the growing success of Convolution neural networks (CNN) in the recent past in the task of scene segmentation, the standard models lack some of the important features that might result in sub-optimal segmentation outputs. The widely used encoder-decoder architecture extracts and uses several redundant and low-level features at different steps and different scales. Also, these networks fail to map the long-range dependencies of local features, which results in discriminative feature maps corresponding to each semantic class in the resulting segmented image. In this paper, we propose a novel multi-scale attention network for scene segmentation purposes by using the rich contextual information from an image. Different from the original UNet architecture we have used attention gates which take the features from the encoder and the output of the pyramid pool as input and produced out-put is further concatenated with the up-sampled output of the previous pyramid-pool layer and mapped to the next subsequent layer. This network can map local features with their global counterparts with improved accuracy and emphasize on discriminative image regions by focusing on relevant local features only. We also propose a compound loss function by optimizing the IoU loss and fusing Dice Loss and Weighted Cross-entropy loss with it to achieve an optimal solution at a faster convergence rate. We have evaluated our model on two standard datasets named PascalVOC2012 and ADE20k and was able to achieve mean IoU of 79.88% and 44.88% on the two datasets respectively, and compared our result with the widely known models to prove the superiority of our model over them.

</details>

<details>

<summary>2020-09-15 08:17:22 - S2IGAN: Speech-to-Image Generation via Adversarial Learning</summary>

- *Xinsheng Wang, Tingting Qiao, Jihua Zhu, Alan Hanjalic, Odette Scharenborg*

- `2005.06968v2` - [abs](http://arxiv.org/abs/2005.06968v2) - [pdf](http://arxiv.org/pdf/2005.06968v2)

> An estimated half of the world's languages do not have a written form, making it impossible for these languages to benefit from any existing text-based technologies. In this paper, a speech-to-image generation (S2IG) framework is proposed which translates speech descriptions to photo-realistic images without using any text information, thus allowing unwritten languages to potentially benefit from this technology. The proposed S2IG framework, named S2IGAN, consists of a speech embedding network (SEN) and a relation-supervised densely-stacked generative model (RDG). SEN learns the speech embedding with the supervision of the corresponding visual information. Conditioned on the speech embedding produced by SEN, the proposed RDG synthesizes images that are semantically consistent with the corresponding speech descriptions. Extensive experiments on two public benchmark datasets CUB and Oxford-102 demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and a solid baseline for the S2IG task.

</details>

<details>

<summary>2020-09-15 08:23:06 - Consistency Regularization with Generative Adversarial Networks for Semi-Supervised Learning</summary>

- *Zexi Chen, Bharathkumar Ramachandra, Ranga Raju Vatsavai*

- `2007.03844v2` - [abs](http://arxiv.org/abs/2007.03844v2) - [pdf](http://arxiv.org/pdf/2007.03844v2)

> Generative Adversarial Networks (GANs) based semi-supervised learning (SSL) approaches are shown to improve classification performance by utilizing a large number of unlabeled samples in conjunction with limited labeled samples. However, their performance still lags behind the state-of-the-art non-GAN based SSL approaches. We identify that the main reason for this is the lack of consistency in class probability predictions on the same image under local perturbations. Following the general literature, we address this issue via label consistency regularization, which enforces the class probability predictions for an input image to be unchanged under various semantic-preserving perturbations. In this work, we introduce consistency regularization into the vanilla semi-GAN to address this critical limitation. In particular, we present a new composite consistency regularization method which, in spirit, leverages both local consistency and interpolation consistency. We demonstrate the efficacy of our approach on two SSL image classification benchmark datasets, SVHN and CIFAR-10. Our experiments show that this new composite consistency regularization based semi-GAN significantly improves its performance and achieves new state-of-the-art performance among GAN-based SSL approaches.

</details>

<details>

<summary>2020-09-15 10:01:27 - High-order Refining for End-to-end Chinese Semantic Role Labeling</summary>

- *Hao Fei, Yafeng Ren, Donghong Ji*

- `2009.06957v1` - [abs](http://arxiv.org/abs/2009.06957v1) - [pdf](http://arxiv.org/pdf/2009.06957v1)

> Current end-to-end semantic role labeling is mostly accomplished via graph-based neural models. However, these all are first-order models, where each decision for detecting any predicate-argument pair is made in isolation with local features. In this paper, we present a high-order refining mechanism to perform interaction between all predicate-argument pairs. Based on the baseline graph model, our high-order refining module learns higher-order features between all candidate pairs via attention calculation, which are later used to update the original token representations. After several iterations of refinement, the underlying token representations can be enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues.

</details>

<details>

<summary>2020-09-15 10:35:17 - Harness the Power of DERs for Secure Communications in Electric Energy Systems</summary>

- *Ioannis Zografopoulos, Juan Ospina, Charalambos Konstantinou*

- `2009.06975v1` - [abs](http://arxiv.org/abs/2009.06975v1) - [pdf](http://arxiv.org/pdf/2009.06975v1)

> Electric energy systems are undergoing significant changes to improve system reliability and accommodate increasing power demands. The penetration of distributed energy resources (DERs) including roof-top solar panels, energy storage, electric vehicles, etc., enables the on-site generation of economically dispatchable power curtailing operational costs. The effective control of DERs requires communication between utilities and DER system operators. The communication protocols employed for DER management and control lack sophisticated cybersecurity features and can compromise power systems secure operation if malicious control commands are issued to DERs. To overcome authentication-related protocol issues, we present a bolt-on security extension that can be implemented on Distributed Network Protocol v3 (DNP3). We port an authentication framework, DERauth, into DNP3, and utilize real-time measurements from a simulated DER battery energy storage system to enhance communication security. We evaluate our framework in a testbed setup using DNP3 master and outstation devices performing secure authentication by leveraging the entropy of DERs.

</details>

<details>

<summary>2020-09-15 11:09:04 - Contrastive Cross-site Learning with Redesigned Net for COVID-19 CT Classification</summary>

- *Zhao Wang, Quande Liu, Qi Dou*

- `2009.07652v1` - [abs](http://arxiv.org/abs/2009.07652v1) - [pdf](http://arxiv.org/pdf/2009.07652v1)

> The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global public health crisis spreading hundreds of countries. With the continuous growth of new infections, developing automated tools for COVID-19 identification with CT image is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. To enlarge the datasets for developing machine learning methods, it is essentially helpful to aggregate the cases from different medical systems for learning robust and generalizable models. This paper proposes a novel joint learning framework to perform accurate COVID-19 identification by effectively learning with heterogeneous datasets with distribution discrepancy. We build a powerful backbone by redesigning the recently proposed COVID-Net in aspects of network architecture and learning strategy to improve the prediction accuracy and learning efficiency. On top of our improved backbone, we further explicitly tackle the cross-site domain shift by conducting separate feature normalization in latent space. Moreover, we propose to use a contrastive training objective to enhance the domain invariance of semantic embeddings for boosting the classification performance on each dataset. We develop and evaluate our method with two public large-scale COVID-19 diagnosis datasets made up of CT images. Extensive experiments show that our approach consistently improves the performances on both datasets, outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.

</details>

<details>

<summary>2020-09-15 11:55:45 - Optimal Use of Multi-spectral Satellite Data with Convolutional Neural Networks</summary>

- *Sagar Vaze, James Foley, Mohamed Seddiq, Alexey Unagaev, Natalia Efremova*

- `2009.07000v1` - [abs](http://arxiv.org/abs/2009.07000v1) - [pdf](http://arxiv.org/pdf/2009.07000v1)

> The analysis of satellite imagery will prove a crucial tool in the pursuit of sustainable development. While Convolutional Neural Networks (CNNs) have made large gains in natural image analysis, their application to multi-spectral satellite images (wherein input images have a large number of channels) remains relatively unexplored. In this paper, we compare different methods of leveraging multi-band information with CNNs, demonstrating the performance of all compared methods on the task of semantic segmentation of agricultural vegetation (vineyards). We show that standard industry practice of using bands selected by a domain expert leads to a significantly worse test accuracy than the other methods compared. Specifically, we compare: using bands specified by an expert; using all available bands; learning attention maps over the input bands; and leveraging Bayesian optimisation to dictate band choice. We show that simply using all available band information already increases test time performance, and show that the Bayesian optimisation, first applied to band selection in this work, can be used to further boost accuracy.

</details>

<details>

<summary>2020-09-15 17:51:41 - LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation</summary>

- *Wentao Zhu, Can Zhao, Wenqi Li, Holger Roth, Ziyue Xu, Daguang Xu*

- `2006.12575v3` - [abs](http://arxiv.org/abs/2006.12575v3) - [pdf](http://arxiv.org/pdf/2006.12575v3)

> Deep Learning (DL) models are becoming larger, because the increase in model size might offer significant accuracy gain. To enable the training of large deep networks, data parallelism and model parallelism are two well-known approaches for parallel training. However, data parallelism does not help reduce memory footprint per device. In this work, we introduce Large deep 3D ConvNets with Automated Model Parallelism (LAMP) and investigate the impact of both input's and deep 3D ConvNets' size on segmentation accuracy. Through automated model parallelism, it is feasible to train large deep 3D ConvNets with a large input patch, even the whole image. Extensive experiments demonstrate that, facilitated by the automated model parallelism, the segmentation accuracy can be improved through increasing model size and input context size, and large input yields significant inference speedup compared with sliding window of small patches in the inference. Code is available\footnote{https://monai.io/research/lamp-automated-model-parallelism}.

</details>

<details>

<summary>2020-09-15 19:23:53 - Augmented Natural Language for Generative Sequence Labeling</summary>

- *Ben Athiwaratkun, Cicero Nogueira dos Santos, Jason Krone, Bing Xiang*

- `2009.13272v1` - [abs](http://arxiv.org/abs/2009.13272v1) - [pdf](http://arxiv.org/pdf/2009.13272v1)

> We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework is general purpose, performing well on few-shot, low-resource, and high-resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot ($75.0\% \rightarrow 90.9\%$) and 1-shot ($70.4\% \rightarrow 81.0\%$) state-of-the-art results. Furthermore, our model generates large improvements ($46.27\% \rightarrow 63.83\%$) in low-resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high-resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.

</details>

<details>

<summary>2020-09-15 22:44:04 - Data Programming by Demonstration: A Framework for Interactively Learning Labeling Functions</summary>

- *Sara Evensen, Chang Ge, Dongjin Choi, Çağatay Demiralp*

- `2009.01444v3` - [abs](http://arxiv.org/abs/2009.01444v3) - [pdf](http://arxiv.org/pdf/2009.01444v3)

> Data programming is a programmatic weak supervision approach to efficiently curate large-scale labeled training data. Writing data programs (labeling functions) requires, however, both programming literacy and domain expertise. Many subject matter experts have neither programming proficiency nor time to effectively write data programs. Furthermore, regardless of one's expertise in coding or machine learning, transferring domain expertise into labeling functions by enumerating rules and thresholds is not only time consuming but also inherently difficult. Here we propose a new framework, data programming by demonstration (DPBD), to generate labeling rules using interactive demonstrations of users. DPBD aims to relieve the burden of writing labeling functions from users, enabling them to focus on higher-level semantics such as identifying relevant signals for labeling tasks. We operationalize our framework with Ruler, an interactive system that synthesizes labeling rules for document classification by using span-level annotations of users on document examples. We compare Ruler with conventional data programming through a user study conducted with 10 data scientists creating labeling functions for sentiment and spam classification tasks. We find that Ruler is easier to use and learn and offers higher overall satisfaction, while providing discriminative model performances comparable to ones achieved by conventional data programming.

</details>

<details>

<summary>2020-09-15 23:59:06 - Bio-inspired Structure Identification in Language Embeddings</summary>

- *Hongwei, Zhou, Oskar Elek, Pranav Anand, Angus G. Forbes*

- `2009.02459v2` - [abs](http://arxiv.org/abs/2009.02459v2) - [pdf](http://arxiv.org/pdf/2009.02459v2)

> Word embeddings are a popular way to improve downstream performances in contemporary language modeling. However, the underlying geometric structure of the embedding space is not well understood. We present a series of explorations using bio-inspired methodology to traverse and visualize word embeddings, demonstrating evidence of discernible structure. Moreover, our model also produces word similarity rankings that are plausible yet very different from common similarity metrics, mainly cosine similarity and Euclidean distance. We show that our bio-inspired model can be used to investigate how different word embedding techniques result in different semantic outputs, which can emphasize or obscure particular interpretations in textual data.

</details>

<details>

<summary>2020-09-16 00:30:49 - Asking Complex Questions with Multi-hop Answer-focused Reasoning</summary>

- *Xiyao Ma, Qile Zhu, Yanlin Zhou, Xiaolin Li, Dapeng Wu*

- `2009.07402v1` - [abs](http://arxiv.org/abs/2009.07402v1) - [pdf](http://arxiv.org/pdf/2009.07402v1)

> Asking questions from natural language text has attracted increasing attention recently, and several schemes have been proposed with promising results by asking the right question words and copy relevant words from the input to the question. However, most state-of-the-art methods focus on asking simple questions involving single-hop relations. In this paper, we propose a new task called multihop question generation that asks complex and semantically relevant questions by additionally discovering and modeling the multiple entities and their semantic relations given a collection of documents and the corresponding answer 1. To solve the problem, we propose multi-hop answer-focused reasoning on the grounded answer-centric entity graph to include different granularity levels of semantic information including the word-level and document-level semantics of the entities and their semantic relations. Through extensive experiments on the HOTPOTQA dataset, we demonstrate the superiority and effectiveness of our proposed model that serves as a baseline to motivate future work.

</details>

<details>

<summary>2020-09-16 00:52:18 - An Imprecise Probability Approach for Abstract Argumentation based on Credal Sets</summary>

- *Mariela Morveli-Espinoza, Juan Carlos Nieves, Cesar Augusto Tacla*

- `2009.07405v1` - [abs](http://arxiv.org/abs/2009.07405v1) - [pdf](http://arxiv.org/pdf/2009.07405v1)

> Some abstract argumentation approaches consider that arguments have a degree of uncertainty, which impacts on the degree of uncertainty of the extensions obtained from a abstract argumentation framework (AAF) under a semantics. In these approaches, both the uncertainty of the arguments and of the extensions are modeled by means of precise probability values. However, in many real life situations the exact probabilities values are unknown and sometimes there is a need for aggregating the probability values of different sources. In this paper, we tackle the problem of calculating the degree of uncertainty of the extensions considering that the probability values of the arguments are imprecise. We use credal sets to model the uncertainty values of arguments and from these credal sets, we calculate the lower and upper bounds of the extensions. We study some properties of the suggested approach and illustrate it with an scenario of decision making.

</details>

<details>

<summary>2020-09-16 01:07:07 - Retrofitting Structure-aware Transformer Language Model for End Tasks</summary>

- *Hao Fei, Yafeng Ren, Donghong Ji*

- `2009.07408v1` - [abs](http://arxiv.org/abs/2009.07408v1) - [pdf](http://arxiv.org/pdf/2009.07408v1)

> We consider retrofitting structure-aware Transformer-based language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.

</details>

<details>

<summary>2020-09-16 02:33:32 - Job2Vec: Job Title Benchmarking with Collective Multi-View Representation Learning</summary>

- *Denghui Zhang, Junming Liu, Hengshu Zhu, Yanchi Liu, Lichen Wang, Pengyang Wang, Hui Xiong*

- `2009.07429v1` - [abs](http://arxiv.org/abs/2009.07429v1) - [pdf](http://arxiv.org/pdf/2009.07429v1)

> Job Title Benchmarking (JTB) aims at matching job titles with similar expertise levels across various companies. JTB could provide precise guidance and considerable convenience for both talent recruitment and job seekers for position and salary calibration/prediction. Traditional JTB approaches mainly rely on manual market surveys, which is expensive and labor-intensive. Recently, the rapid development of Online Professional Graph has accumulated a large number of talent career records, which provides a promising trend for data-driven solutions. However, it is still a challenging task since (1) the job title and job transition (job-hopping) data is messy which contains a lot of subjective and non-standard naming conventions for the same position (e.g., Programmer, Software Development Engineer, SDE, Implementation Engineer), (2) there is a large amount of missing title/transition information, and (3) one talent only seeks limited numbers of jobs which brings the incompleteness and randomness modeling job transition patterns. To overcome these challenges, we aggregate all the records to construct a large-scale Job Title Benchmarking Graph (Job-Graph), where nodes denote job titles affiliated with specific companies and links denote the correlations between jobs. We reformulate the JTB as the task of link prediction over the Job-Graph that matched job titles should have links. Along this line, we propose a collective multi-view representation learning method (Job2Vec) by examining the Job-Graph jointly in (1) graph topology view, (2)semantic view, (3) job transition balance view, and (4) job transition duration view. We fuse the multi-view representations in the encode-decode paradigm to obtain a unified optimal representation for the task of link prediction. Finally, we conduct extensive experiments to validate the effectiveness of our proposed method.

</details>

<details>

<summary>2020-09-16 09:14:59 - Brain tumour segmentation using cascaded 3D densely-connected U-net</summary>

- *Mina Ghaffari, Arcot Sowmya, Ruth Oliver*

- `2009.07563v1` - [abs](http://arxiv.org/abs/2009.07563v1) - [pdf](http://arxiv.org/pdf/2009.07563v1)

> Accurate brain tumour segmentation is a crucial step towards improving disease diagnosis and proper treatment planning. In this paper, we propose a deep-learning based method to segment a brain tumour into its subregions: whole tumour, tumour core and enhancing tumour. The proposed architecture is a 3D convolutional neural network based on a variant of the U-Net architecture of Ronneberger et al. [17] with three main modifications: (i) a heavy encoder, light decoder structure using residual blocks (ii) employment of dense blocks instead of skip connections, and (iii) utilization of self-ensembling in the decoder part of the network. The network was trained and tested using two different approaches: a multitask framework to segment all tumour subregions at the same time and a three-stage cascaded framework to segment one sub-region at a time. An ensemble of the results from both frameworks was also computed. To address the class imbalance issue, appropriate patch extraction was employed in a pre-processing step. The connected component analysis was utilized in the post-processing step to reduce false positive predictions. Experimental results on the BraTS20 validation dataset demonstrates that the proposed model achieved average Dice Scores of 0.90, 0.82, and 0.78 for whole tumour, tumour core and enhancing tumour respectively.

</details>

<details>

<summary>2020-09-16 11:01:46 - UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation</summary>

- *Jian Guan, Minlie Huang*

- `2009.07602v1` - [abs](http://arxiv.org/abs/2009.07602v1) - [pdf](http://arxiv.org/pdf/2009.07602v1)

> Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable unreferenced metric for evaluating open-ended story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.

</details>

<details>

<summary>2020-09-16 13:01:05 - Adoption of Twitter's New Length Limit: Is 280 the New 140?</summary>

- *Kristina Gligorić, Ashton Anderson, Robert West*

- `2009.07661v1` - [abs](http://arxiv.org/abs/2009.07661v1) - [pdf](http://arxiv.org/pdf/2009.07661v1)

> In November 2017, Twitter doubled the maximum allowed tweet length from 140 to 280 characters, a drastic switch on one of the world's most influential social media platforms. In the first long-term study of how the new length limit was adopted by Twitter users, we ask: Does the effect of the new length limit resemble that of the old one? Or did the doubling of the limit fundamentally change how Twitter is shaped by the limited length of posted content? By analyzing Twitter's publicly available 1% sample over a period of around 3 years, we find that, when the length limit was raised from 140 to 280 characters, the prevalence of tweets around 140 characters dropped immediately, while the prevalence of tweets around 280 characters rose steadily for about 6 months. Despite this rise, tweets approaching the length limit have been far less frequent after than before the switch. We find widely different adoption rates across languages and client-device types. The prevalence of tweets around 140 characters before the switch in a given language is strongly correlated with the prevalence of tweets around 280 characters after the switch in the same language, and very long tweets are vastly more popular on Web clients than on mobile clients. Moreover, tweets of around 280 characters after the switch are syntactically and semantically similar to tweets of around 140 characters before the switch, manifesting patterns of message squeezing in both cases. Taken together, these findings suggest that the new 280-character limit constitutes a new, less intrusive version of the old 140-character limit. The length limit remains an important factor that should be considered in all studies using Twitter data.

</details>

<details>

<summary>2020-09-16 14:56:11 - Leveraging Semantic Parsing for Relation Linking over Knowledge Bases</summary>

- *Nandana Mihindukulasooriya, Gaetano Rossiello, Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Mo Yu, Alfio Gliozzo, Salim Roukos, Alexander Gray*

- `2009.07726v1` - [abs](http://arxiv.org/abs/2009.07726v1) - [pdf](http://arxiv.org/pdf/2009.07726v1)

> Knowledgebase question answering systems are heavily dependent on relation extraction and linking modules. However, the task of extracting and linking relations from text to knowledgebases faces two primary challenges; the ambiguity of natural language and lack of training data. To overcome these challenges, we present SLING, a relation linking framework which leverages semantic parsing using Abstract Meaning Representation (AMR) and distant supervision. SLING integrates multiple relation linking approaches that capture complementary signals such as linguistic cues, rich semantic representation, and information from the knowledgebase. The experiments on relation linking using three KBQA datasets; QALD-7, QALD-9, and LC-QuAD 1.0 demonstrate that the proposed approach achieves state-of-the-art performance on all benchmarks.

</details>

<details>

<summary>2020-09-16 18:42:11 - Fake News Early Detection: An Interdisciplinary Study</summary>

- *Xinyi Zhou, Atishay Jain, Vir V. Phoha, Reza Zafarani*

- `1904.11679v2` - [abs](http://arxiv.org/abs/1904.11679v2) - [pdf](http://arxiv.org/pdf/1904.11679v2)

> Massive dissemination of fake news and its potential to erode democracy has increased the demand for accurate fake news detection. Recent advancements in this area have proposed novel techniques that aim to detect fake news by exploring how it propagates on social networks. Nevertheless, to detect fake news at an early stage, i.e., when it is published on a news outlet but not yet spread on social media, one cannot rely on news propagation information as it does not exist. Hence, there is a strong need to develop approaches that can detect fake news by focusing on news content. In this paper, a theory-driven model is proposed for fake news detection. The method investigates news content at various levels: lexicon-level, syntax-level, semantic-level and discourse-level. We represent news at each level, relying on well-established theories in social and forensic psychology. Fake news detection is then conducted within a supervised machine learning framework. As an interdisciplinary research, our work explores potential fake news patterns, enhances the interpretability in fake news feature engineering, and studies the relationships among fake news, deception/disinformation, and clickbaits. Experiments conducted on two real-world datasets indicate the proposed method can outperform the state-of-the-art and enable fake news early detection when there is limited content information.

</details>

<details>

<summary>2020-09-16 23:14:13 - A Network-Based High-Level Data Classification Algorithm Using Betweenness Centrality</summary>

- *Esteban Vilca, Liang Zhao*

- `2009.07971v1` - [abs](http://arxiv.org/abs/2009.07971v1) - [pdf](http://arxiv.org/pdf/2009.07971v1)

> Data classification is a major machine learning paradigm, which has been widely applied to solve a large number of real-world problems. Traditional data classification techniques consider only physical features (e.g., distance, similarity, or distribution) of the input data. For this reason, those are called \textit{low-level} classification. On the other hand, the human (animal) brain performs both low and high orders of learning and it has a facility in identifying patterns according to the semantic meaning of the input data. Data classification that considers not only physical attributes but also the pattern formation is referred to as \textit{high-level} classification. Several high-level classification techniques have been developed, which make use of complex networks to characterize data patterns and have obtained promising results. In this paper, we propose a pure network-based high-level classification technique that uses the betweenness centrality measure. We test this model in nine different real datasets and compare it with other nine traditional and well-known classification models. The results show us a competent classification performance.

</details>

<details>

<summary>2020-09-17 00:56:45 - Dealing with Incompatibilities among Procedural Goals under Uncertainty</summary>

- *Mariela Morveli-Espinoza, Juan Carlos Nieves, Ayslan Trevizan Possebom, Cesar Augusto Tacla*

- `2009.08776v1` - [abs](http://arxiv.org/abs/2009.08776v1) - [pdf](http://arxiv.org/pdf/2009.08776v1)

> By considering rational agents, we focus on the problem of selecting goals out of a set of incompatible ones. We consider three forms of incompatibility introduced by Castelfranchi and Paglieri, namely the terminal, the instrumental (or based on resources), and the superfluity. We represent the agent's plans by means of structured arguments whose premises are pervaded with uncertainty. We measure the strength of these arguments in order to determine the set of compatible goals. We propose two novel ways for calculating the strength of these arguments, depending on the kind of incompatibility that exists between them. The first one is the logical strength value, it is denoted by a three-dimensional vector, which is calculated from a probabilistic interval associated with each argument. The vector represents the precision of the interval, the location of it, and the combination of precision and location. This type of representation and treatment of the strength of a structured argument has not been defined before by the state of the art. The second way for calculating the strength of the argument is based on the cost of the plans (regarding the necessary resources) and the preference of the goals associated with the plans. Considering our novel approach for measuring the strength of structured arguments, we propose a semantics for the selection of plans and goals that is based on Dung's abstract argumentation theory. Finally, we make a theoretical evaluation of our proposal.

</details>

<details>

<summary>2020-09-17 02:36:52 - Cross-Modal Alignment with Mixture Experts Neural Network for Intral-City Retail Recommendation</summary>

- *Po Li, Lei Li, Yan Fu, Jun Rong, Yu Zhang*

- `2009.09926v1` - [abs](http://arxiv.org/abs/2009.09926v1) - [pdf](http://arxiv.org/pdf/2009.09926v1)

> In this paper, we introduce Cross-modal Alignment with mixture experts Neural Network (CameNN) recommendation model for intral-city retail industry, which aims to provide fresh foods and groceries retailing within 5 hours delivery service arising for the outbreak of Coronavirus disease (COVID-19) pandemic around the world. We propose CameNN, which is a multi-task model with three tasks including Image to Text Alignment (ITA) task, Text to Image Alignment (TIA) task and CVR prediction task. We use pre-trained BERT to generate the text embedding and pre-trained InceptionV4 to generate image patch embedding (each image is split into small patches with the same pixels and treat each patch as an image token). Softmax gating networks follow to learn the weight of each transformer expert output and choose only a subset of experts conditioned on the input. Then transformer encoder is applied as the share-bottom layer to learn all input features' shared interaction. Next, mixture of transformer experts (MoE) layer is implemented to model different aspects of tasks. At top of the MoE layer, we deploy a transformer layer for each task as task tower to learn task-specific information. On the real word intra-city dataset, experiments demonstrate CameNN outperform baselines and achieve significant improvements on the image and text representation. In practice, we applied CameNN on CVR prediction in our intra-city recommender system which is one of the leading intra-city platforms operated in China.

</details>

<details>

<summary>2020-09-17 05:13:41 - Layer-stacked Attention for Heterogeneous Network Embedding</summary>

- *Nhat Tran, Jean Gao*

- `2009.08072v1` - [abs](http://arxiv.org/abs/2009.08072v1) - [pdf](http://arxiv.org/pdf/2009.08072v1)

> The heterogeneous network is a robust data abstraction that can model entities of different types interacting in various ways. Such heterogeneity brings rich semantic information but presents nontrivial challenges in aggregating the heterogeneous relationships between objects - especially those of higher-order indirect relations. Recent graph neural network approaches for representation learning on heterogeneous networks typically employ the attention mechanism, which is often only optimized for predictions based on direct links. Furthermore, even though most deep learning methods can aggregate higher-order information by building deeper models, such a scheme can diminish the degree of interpretability. To overcome these challenges, we explore an architecture - Layer-stacked ATTention Embedding (LATTE) - that automatically decomposes higher-order meta relations at each layer to extract the relevant heterogeneous neighborhood structures for each node. Additionally, by successively stacking layer representations, the learned node embedding offers a more interpretable aggregation scheme for nodes of different types at different neighborhood ranges. We conducted experiments on several benchmark heterogeneous network datasets. In both transductive and inductive node classification tasks, LATTE can achieve state-of-the-art performance compared to existing approaches, all while offering a lightweight model. With extensive experimental analyses and visualizations, the framework can demonstrate the ability to extract informative insights on heterogeneous networks.

</details>

<details>

<summary>2020-09-17 09:00:59 - End-to-End Neural Event Coreference Resolution</summary>

- *Yaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, Le Sun*

- `2009.08153v1` - [abs](http://arxiv.org/abs/2009.08153v1) - [pdf](http://arxiv.org/pdf/2009.08153v1)

> Traditional event coreference systems usually rely on pipeline framework and hand-crafted features, which often face error propagation problem and have poor generalization ability. In this paper, we propose an End-to-End Event Coreference approach -- E3C neural network, which can jointly model event detection and event coreference resolution tasks, and learn to extract features from raw text automatically. Furthermore, because event mentions are highly diversified and event coreference is intricately governed by long-distance, semantic-dependent decisions, a type-guided event coreference mechanism is further proposed in our E3C neural network. Experiments show that our method achieves new state-of-the-art performance on two standard datasets.

</details>

<details>

<summary>2020-09-17 10:50:42 - Generating Label Cohesive and Well-Formed Adversarial Claims</summary>

- *Pepa Atanasova, Dustin Wright, Isabelle Augenstein*

- `2009.08205v1` - [abs](http://arxiv.org/abs/2009.08205v1) - [pdf](http://arxiv.org/pdf/2009.08205v1)

> Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimising the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.

</details>

<details>

<summary>2020-09-17 11:53:12 - RDF2Vec Light -- A Lightweight Approach for Knowledge Graph Embeddings</summary>

- *Jan Portisch, Michael Hladik, Heiko Paulheim*

- `2009.07659v2` - [abs](http://arxiv.org/abs/2009.07659v2) - [pdf](http://arxiv.org/pdf/2009.07659v2)

> Knowledge graph embedding approaches represent nodes and edges of graphs as mathematical vectors. Current approaches focus on embedding complete knowledge graphs, i.e. all nodes and edges. This leads to very high computational requirements on large graphs such as DBpedia or Wikidata. However, for most downstream application scenarios, only a small subset of concepts is of actual interest. In this paper, we present RDF2Vec Light, a lightweight embedding approach based on RDF2Vec which generates vectors for only a subset of entities. To that end, RDF2Vec Light only traverses and processes a subgraph of the knowledge graph. Our method allows the application of embeddings of very large knowledge graphs in scenarios where such embeddings were not possible before due to a significantly lower runtime and significantly reduced hardware requirements.

</details>

<details>

<summary>2020-09-17 13:00:13 - Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA</summary>

- *Ieva Staliūnaitė, Ignacio Iacobacci*

- `2009.08257v1` - [abs](http://arxiv.org/abs/2009.08257v1) - [pdf](http://arxiv.org/pdf/2009.08257v1)

> Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.

</details>

<details>

<summary>2020-09-17 14:37:31 - Defeasible reasoning in Description Logics: an overview on DL^N</summary>

- *Piero A. Bonatti, Iliana M. Petrova, Luigi Sauro*

- `2009.04978v2` - [abs](http://arxiv.org/abs/2009.04978v2) - [pdf](http://arxiv.org/pdf/2009.04978v2)

> DL^N is a recent approach that extends description logics with defeasible reasoning capabilities. In this paper we provide an overview on DL^N, illustrating the underlying knowledge engineering requirements as well as the characteristic features that preserve DL^N from some recurrent semantic and computational drawbacks. We also compare DL^N with some alternative nonmonotonic semantics, enlightening the relationships between the KLM postulates and DL^N.

</details>

<details>

<summary>2020-09-17 15:59:57 - A novel highly efficient Lagrangian model for massively multidomain simulations: parallel context</summary>

- *Sebastian Florez, Julien Fausty, Karen Alvarado, Brayan Murgas, Marc Bernacki*

- `2009.04424v2` - [abs](http://arxiv.org/abs/2009.04424v2) - [pdf](http://arxiv.org/pdf/2009.04424v2)

> A new method for the simulation of evolving multi-domains problems has been introduced in a previous work (RealIMotion), Florez et al. (2020). In this article further developments of the model will be presented. The main focus here is a robust parallel implementation using a distributed-memory approach with the Message Passing Interface (MPI) library OpenMPI. The original 2D sequential methodology consists in a modified front-tracking approach where the main originality is that not only interfaces between domains are discretized but their interiors are also meshed. The interfaces are tracked based on the topological degree of each node on the mesh and the remeshing and topological changes of the domains are driven by selective local operations performed over an element patch. The accuracy and the performance of the sequential method has proven very promising in Florez et al. (2020). In this article a parallel implementation will be discussed and tested in context of motion by curvature flow for polycrystals, i.e. by considering Grain Growth (GG) mechanism. Results of the performance of the model are given and comparisons with other approaches in the literature are discussed.

</details>

<details>

<summary>2020-09-17 17:28:08 - A Semantic Web Framework for Automated Smart Assistants: COVID-19 Case Study</summary>

- *Yusuf Sermet, Ibrahim Demir*

- `2007.00747v2` - [abs](http://arxiv.org/abs/2007.00747v2) - [pdf](http://arxiv.org/pdf/2007.00747v2)

> COVID-19 pandemic elucidated that knowledge systems will be instrumental in cases where accurate information needs to be communicated to a substantial group of people with different backgrounds and technological resources. However, several challenges and obstacles hold back the wide adoption of virtual assistants by public health departments and organizations. This paper presents the Instant Expert, an open-source semantic web framework to build and integrate voice-enabled smart assistants (i.e. chatbots) for any web platform regardless of the underlying domain and technology. The component allows non-technical domain experts to effortlessly incorporate an operational assistant with voice recognition capability into their websites. Instant Expert is capable of automatically parsing, processing, and modeling Frequently Asked Questions pages as an information resource as well as communicating with an external knowledge engine for ontology-powered inference and dynamic data utilization. The presented framework utilizes advanced web technologies to ensure reusability and reliability, and an inference engine for natural language understanding powered by deep learning and heuristic algorithms. A use case for creating an informatory assistant for COVID-19 based on the Centers for Disease Control and Prevention (CDC) data is presented to demonstrate the framework's usage and benefits.

</details>

<details>

<summary>2020-09-17 22:49:32 - Semantic Loss Application to Entity Relation Recognition</summary>

- *Venkata Sasank Pagolu*

- `2006.04031v2` - [abs](http://arxiv.org/abs/2006.04031v2) - [pdf](http://arxiv.org/pdf/2006.04031v2)

> Usually, entity relation recognition systems either use a pipe-lined model that treats the entity tagging and relation identification as separate tasks or a joint model that simultaneously identifies the relation and entities. This paper compares these two general approaches for the entity relation recognition. State-of-the-art entity relation recognition systems are built using deep recurrent neural networks which often does not capture the symbolic knowledge or the logical constraints in the problem. The main contribution of this paper is an end-to-end neural model for joint entity relation extraction which incorporates a novel loss function. This novel loss function encodes the constraint information in the problem to guide the model training effectively. We show that addition of this loss function to the existing typical loss functions has a positive impact over the performance of the models. This model is truly end-to-end, requires no feature engineering and easily extensible. Extensive experimentation has been conducted to evaluate the significance of capturing symbolic knowledge for natural language understanding. Models using this loss function are observed to be outperforming their counterparts and converging faster. Experimental results in this work suggest the use of this methodology for other language understanding applications.

</details>

<details>

<summary>2020-09-18 04:17:23 - A Knowledge Graph based Approach for Mobile Application Recommendation</summary>

- *Mingwei Zhang, Jiawei Zhao, Hai Dong, Ke Deng, Ying Liu*

- `2009.08621v1` - [abs](http://arxiv.org/abs/2009.08621v1) - [pdf](http://arxiv.org/pdf/2009.08621v1)

> With the rapid prevalence of mobile devices and the dramatic proliferation of mobile applications (apps), app recommendation becomes an emergent task that would benefit both app users and stockholders. How to effectively organize and make full use of rich side information of users and apps is a key challenge to address the sparsity issue for traditional approaches. To meet this challenge, we proposed a novel end-to-end Knowledge Graph Convolutional Embedding Propagation Model (KGEP) for app recommendation. Specifically, we first designed a knowledge graph construction method to model the user and app side information, then adopted KG embedding techniques to capture the factual triplet-focused semantics of the side information related to the first-order structure of the KG, and finally proposed a relation-weighted convolutional embedding propagation model to capture the recommendation-focused semantics related to high-order structure of the KG. Extensive experiments conducted on a real-world dataset validate the effectiveness of the proposed approach compared to the state-of-the-art recommendation approaches.

</details>

<details>

<summary>2020-09-18 07:20:44 - Multi-species Seagrass Detection and Classification from Underwater Images</summary>

- *Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett Kettle, Brano Kusy*

- `2009.09924v1` - [abs](http://arxiv.org/abs/2009.09924v1) - [pdf](http://arxiv.org/pdf/2009.09924v1)

> Underwater surveys conducted using divers or robots equipped with customized camera payloads can generate a large number of images. Manual review of these images to extract ecological data is prohibitive in terms of time and cost, thus providing strong incentive to automate this process using machine learning solutions. In this paper, we introduce a multi-species detector and classifier for seagrasses based on a deep convolutional neural network (achieved an overall accuracy of 92.4%). We also introduce a simple method to semi-automatically label image patches and therefore minimize manual labelling requirement. We describe and release publicly the dataset collected in this study as well as the code and pre-trained models to replicate our experiments at: https://github.com/csiro-robotics/deepseagrass

</details>

<details>

<summary>2020-09-18 09:47:05 - Contextual Semantic Interpretability</summary>

- *Diego Marcos, Ruth Fong, Sylvain Lobry, Remi Flamary, Nicolas Courty, Devis Tuia*

- `2009.08720v1` - [abs](http://arxiv.org/abs/2009.08720v1) - [pdf](http://arxiv.org/pdf/2009.08720v1)

> Convolutional neural networks (CNN) are known to learn an image representation that captures concepts relevant to the task, but do so in an implicit way that hampers model interpretability. However, one could argue that such a representation is hidden in the neurons and can be made explicit by teaching the model to recognize semantically interpretable attributes that are present in the scene. We call such an intermediate layer a \emph{semantic bottleneck}. Once the attributes are learned, they can be re-combined to reach the final decision and provide both an accurate prediction and an explicit reasoning behind the CNN decision. In this paper, we look into semantic bottlenecks that capture context: we want attributes to be in groups of a few meaningful elements and participate jointly to the final decision. We use a two-layer semantic bottleneck that gathers attributes into interpretable, sparse groups, allowing them contribute differently to the final output depending on the context. We test our contextual semantic interpretable bottleneck (CSIB) on the task of landscape scenicness estimation and train the semantic interpretable bottleneck using an auxiliary database (SUN Attributes). Our model yields in predictions as accurate as a non-interpretable baseline when applied to a real-world test set of Flickr images, all while providing clear and interpretable explanations for each prediction.

</details>

<details>

<summary>2020-09-18 13:40:39 - Using Neural Networks for Relation Extraction from Biomedical Literature</summary>

- *Diana Sousa, Andre Lamurias, Francisco M. Couto*

- `1905.11391v2` - [abs](http://arxiv.org/abs/1905.11391v2) - [pdf](http://arxiv.org/pdf/1905.11391v2)

> Using different sources of information to support automated extracting of relations between biomedical concepts contributes to the development of our understanding of biological systems. The primary comprehensive source of these relations is biomedical literature. Several relation extraction approaches have been proposed to identify relations between concepts in biomedical literature, namely, using neural networks algorithms. The use of multichannel architectures composed of multiple data representations, as in deep neural networks, is leading to state-of-the-art results. The right combination of data representations can eventually lead us to even higher evaluation scores in relation extraction tasks. Thus, biomedical ontologies play a fundamental role by providing semantic and ancestry information about an entity. The incorporation of biomedical ontologies has already been proved to enhance previous state-of-the-art results.

</details>

<details>

<summary>2020-09-18 21:59:12 - Amazon Fake Reviews</summary>

- *Seung Ah Choi*

- `2009.09102v1` - [abs](http://arxiv.org/abs/2009.09102v1) - [pdf](http://arxiv.org/pdf/2009.09102v1)

> Often, there are suspicious Amazon reviews that seem to be excessively positive or have been created through a repeating algorithm. I moved to detect fake reviews on Amazon through semantic analysis in conjunction with meta data such as time, word choice, and the user who posted. I first came up with several instances that may indicate a review isn't genuine and constructed what the algorithm would look like. Then I coded the algorithm and tested the accuracy of it using statistical analysis and analyzed it based on the six qualities of code.

</details>

<details>

<summary>2020-09-18 23:07:39 - Out of Sight, Out of Place: Detecting and Assessing Swapped Arguments</summary>

- *Roger Scott, Joseph Ranieri, Lucja Kot, Vineeth Kashyap*

- `2009.09117v1` - [abs](http://arxiv.org/abs/2009.09117v1) - [pdf](http://arxiv.org/pdf/2009.09117v1)

> Programmers often add meaningful information about program semantics when naming program entities such as variables, functions, and macros. However, static analysis tools typically discount this information when they look for bugs in a program. In this work, we describe the design and implementation of a static analysis checker called SwapD, which uses the natural language information in programs to warn about mistakenly-swapped arguments at call sites. SwapD combines two independent detection strategies to improve the effectiveness of the overall checker. We present the results of a comprehensive evaluation of SwapD over a large corpus of C and C++ programs totaling 417 million lines of code. In this evaluation, SwapD found 154 manually-vetted real-world cases of mistakenly-swapped arguments, suggesting that such errors, while not pervasive in released code, are a real problem and a worthwhile target for static analysis.

</details>

<details>

<summary>2020-09-18 23:59:15 - Will it Unblend?</summary>

- *Yuval Pinter, Cassandra L. Jacobs, Jacob Eisenstein*

- `2009.09123v1` - [abs](http://arxiv.org/abs/2009.09123v1) - [pdf](http://arxiv.org/pdf/2009.09123v1)

> Natural language processing systems often struggle with out-of-vocabulary (OOV) terms, which do not appear in training data. Blends, such as "innoventor", are one particularly challenging class of OOV, as they are formed by fusing together two or more bases that relate to the intended meaning in unpredictable manners and degrees. In this work, we run experiments on a novel dataset of English OOV blends to quantify the difficulty of interpreting the meanings of blends by large-scale contextual language models such as BERT. We first show that BERT's processing of these blends does not fully access the component meanings, leaving their contextual representations semantically impoverished. We find this is mostly due to the loss of characters resulting from blend formation. Then, we assess how easily different models can recognize the structure and recover the origin of blends, and find that context-aware embedding systems outperform character-level and context-free embeddings, although their results are still far from satisfactory.

</details>

<details>

<summary>2020-09-19 04:18:41 - Proceedings 36th International Conference on Logic Programming (Technical Communications)</summary>

- *Francesco Ricca, Alessandra Russo, Sergio Greco, Nicola Leone, Alexander Artikis, Gerhard Friedrich, Paul Fodor, Angelika Kimmig, Francesca Lisi, Marco Maratea, Alessandra Mileo, Fabrizio Riguzzi*

- `2009.09158v1` - [abs](http://arxiv.org/abs/2009.09158v1) - [pdf](http://arxiv.org/pdf/2009.09158v1)

> Since the first conference held in Marseille in 1982, ICLP has been the premier international event for presenting research in logic programming. Contributions are solicited in all areas of logic programming and related areas, including but not restricted to:   - Foundations: Semantics, Formalisms, Answer-Set Programming, Non-monotonic Reasoning, Knowledge Representation.   - Declarative Programming: Inference engines, Analysis, Type and mode inference, Partial evaluation, Abstract interpretation, Transformation, Validation, Verification, Debugging, Profiling, Testing, Logic-based domain-specific languages, constraint handling rules.   - Related Paradigms and Synergies: Inductive and Co-inductive Logic Programming, Constraint Logic Programming, Interaction with SAT, SMT and CSP solvers, Logic programming techniques for type inference and theorem proving, Argumentation, Probabilistic Logic Programming, Relations to object-oriented and Functional programming, Description logics, Neural-Symbolic Machine Learning, Hybrid Deep Learning and Symbolic Reasoning.   - Implementation: Concurrency and distribution, Objects, Coordination, Mobility, Virtual machines, Compilation, Higher Order, Type systems, Modules, Constraint handling rules, Meta-programming, Foreign interfaces, User interfaces.   - Applications: Databases, Big Data, Data Integration and Federation, Software Engineering, Natural Language Processing, Web and Semantic Web, Agents, Artificial Intelligence, Bioinformatics, Education, Computational life sciences, Education, Cybersecurity, and Robotics.

</details>

<details>

<summary>2020-09-19 06:20:37 - Nominal Compound Chain Extraction: A New Task for Semantic-enriched Lexical Chain</summary>

- *Bobo Li, Hao Fei, Yafeng Ren, Donghong Ji*

- `2009.09173v1` - [abs](http://arxiv.org/abs/2009.09173v1) - [pdf](http://arxiv.org/pdf/2009.09173v1)

> Lexical chain consists of cohesion words in a document, which implies the underlying structure of a text, and thus facilitates downstream NLP tasks. Nevertheless, existing work focuses on detecting the simple surface lexicons with shallow syntax associations, ignoring the semantic-aware lexical compounds as well as the latent semantic frames, (e.g., topic), which can be much more crucial for real-world NLP applications. In this paper, we introduce a novel task, Nominal Compound Chain Extraction (NCCE), extracting and clustering all the nominal compounds that share identical semantic topics. In addition, we model the task as a two-stage prediction (i.e., compound extraction and chain detection), which is handled via a proposed joint framework. The model employs the BERT encoder to yield contextualized document representation. Also, HowNet is exploited as external resources for offering rich sememe information. The experiments are based on our manually annotated corpus, and the results prove the necessity of the NCCE task as well as the effectiveness of our joint approach.

</details>

<details>

<summary>2020-09-19 14:04:54 - Shimon the Rapper: A Real-Time System for Human-Robot Interactive Rap Battles</summary>

- *Richard Savery, Lisa Zahray, Gil Weinberg*

- `2009.09234v1` - [abs](http://arxiv.org/abs/2009.09234v1) - [pdf](http://arxiv.org/pdf/2009.09234v1)

> We present a system for real-time lyrical improvisation between a human and a robot in the style of hip hop. Our system takes vocal input from a human rapper, analyzes the semantic meaning, and generates a response that is rapped back by a robot over a musical groove. Previous work with real-time interactive music systems has largely focused on instrumental output, and vocal interactions with robots have been explored, but not in a musical context. Our generative system includes custom methods for censorship, voice, rhythm, rhyming and a novel deep learning pipeline based on phoneme embeddings. The rap performances are accompanied by synchronized robotic gestures and mouth movements. Key technical challenges that were overcome in the system are developing rhymes, performing with low-latency and dataset censorship. We evaluated several aspects of the system through a survey of videos and sample text output. Analysis of comments showed that the overall perception of the system was positive. The model trained on our hip hop dataset was rated significantly higher than our metal dataset in coherence, rhyme quality, and enjoyment. Participants preferred outputs generated by a given input phrase over outputs generated from unknown keywords, indicating that the system successfully relates its output to its input.

</details>

<details>

<summary>2020-09-19 14:41:50 - Word class flexibility: A deep contextualized approach</summary>

- *Bai Li, Guillaume Thomas, Yang Xu, Frank Rudzicz*

- `2009.09241v1` - [abs](http://arxiv.org/abs/2009.09241v1) - [pdf](http://arxiv.org/pdf/2009.09241v1)

> Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages. We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.

</details>

<details>

<summary>2020-09-19 18:43:59 - Cascaded Semantic and Positional Self-Attention Network for Document Classification</summary>

- *Juyong Jiang, Jie Zhang, Kai Zhang*

- `2009.07148v2` - [abs](http://arxiv.org/abs/2009.07148v2) - [pdf](http://arxiv.org/pdf/2009.07148v2)

> Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network (CSPAN) in the context of document classification. The CSPAN uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residue connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between semantics and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high convergence rate. We evaluate the CSPAN model on several benchmark data sets for document classification with careful ablation studies, and demonstrate the encouraging results compared with state of the art.

</details>

<details>

<summary>2020-09-19 18:54:01 - Reducing false-positive biopsies with deep neural networks that utilize local and global information in screening mammograms</summary>

- *Nan Wu, Zhe Huang, Yiqiu Shen, Jungkyu Park, Jason Phang, Taro Makino, S. Gene Kim, Kyunghyun Cho, Laura Heacock, Linda Moy, Krzysztof J. Geras*

- `2009.09282v1` - [abs](http://arxiv.org/abs/2009.09282v1) - [pdf](http://arxiv.org/pdf/2009.09282v1)

> Breast cancer is the most common cancer in women, and hundreds of thousands of unnecessary biopsies are done around the world at a tremendous cost. It is crucial to reduce the rate of biopsies that turn out to be benign tissue. In this study, we build deep neural networks (DNNs) to classify biopsied lesions as being either malignant or benign, with the goal of using these networks as second readers serving radiologists to further reduce the number of false positive findings. We enhance the performance of DNNs that are trained to learn from small image patches by integrating global context provided in the form of saliency maps learned from the entire image into their reasoning, similar to how radiologists consider global context when evaluating areas of interest. Our experiments are conducted on a dataset of 229,426 screening mammography exams from 141,473 patients. We achieve an AUC of 0.8 on a test set consisting of 464 benign and 136 malignant lesions.

</details>

<details>

<summary>2020-09-19 21:10:57 - Covid-Transformer: Detecting COVID-19 Trending Topics on Twitter Using Universal Sentence Encoder</summary>

- *Meysam Asgari-Chenaghlu, Narjes Nikzad-Khasmakhi, Shervin Minaee*

- `2009.03947v3` - [abs](http://arxiv.org/abs/2009.03947v3) - [pdf](http://arxiv.org/pdf/2009.03947v3)

> The novel corona-virus disease (also known as COVID-19) has led to a pandemic, impacting more than 200 countries across the globe. With its global impact, COVID-19 has become a major concern of people almost everywhere, and therefore there are a large number of tweets coming out from every corner of the world, about COVID-19 related topics. In this work, we try to analyze the tweets and detect the trending topics and major concerns of people on Twitter, which can enable us to better understand the situation, and devise better planning. More specifically we propose a model based on the universal sentence encoder to detect the main topics of Tweets in recent months. We used universal sentence encoder in order to derive the semantic representation and the similarity of tweets. We then used the sentence similarity and their embeddings, and feed them to K-means clustering algorithm to group similar tweets (in semantic sense). After that, the cluster summary is obtained using a text summarization algorithm based on deep learning, which can uncover the underlying topics of each cluster. Through experimental results, we show that our model can detect very informative topics, by processing a large number of tweets on sentence level (which can preserve the overall meaning of the tweets). Since this framework has no restriction on specific data distribution, it can be used to detect trending topics from any other social media and any other context rather than COVID-19. Experimental results show superiority of our proposed approach to other baselines, including TF-IDF, and latent Dirichlet allocation (LDA).

</details>

<details>

<summary>2020-09-19 22:02:44 - What is the Best Grid-Map for Self-Driving Cars Localization? An Evaluation under Diverse Types of Illumination, Traffic, and Environment</summary>

- *Filipe Mutz, Thiago Oliveira-Santos, Avelino Forechi, Karin S. Komati, Claudine Badue, Felipe M. G. França, Alberto F. De Souza*

- `2009.09308v1` - [abs](http://arxiv.org/abs/2009.09308v1) - [pdf](http://arxiv.org/pdf/2009.09308v1)

> The localization of self-driving cars is needed for several tasks such as keeping maps updated, tracking objects, and planning. Localization algorithms often take advantage of maps for estimating the car pose. Since maintaining and using several maps is computationally expensive, it is important to analyze which type of map is more adequate for each application. In this work, we provide data for such analysis by comparing the accuracy of a particle filter localization when using occupancy, reflectivity, color, or semantic grid maps. To the best of our knowledge, such evaluation is missing in the literature. For building semantic and colour grid maps, point clouds from a Light Detection and Ranging (LiDAR) sensor are fused with images captured by a front-facing camera. Semantic information is extracted from images with a deep neural network. Experiments are performed in varied environments, under diverse conditions of illumination and traffic. Results show that occupancy grid maps lead to more accurate localization, followed by reflectivity grid maps. In most scenarios, the localization with semantic grid maps kept the position tracking without catastrophic losses, but with errors from 2 to 3 times bigger than the previous. Colour grid maps led to inaccurate and unstable localization even using a robust metric, the entropy correlation coefficient, for comparing online data and the map.

</details>

<details>

<summary>2020-09-20 02:06:36 - Factorized Deep Generative Models for Trajectory Generation with Spatiotemporal-Validity Constraints</summary>

- *Liming Zhang, Liang Zhao, Dieter Pfoser*

- `2009.09333v1` - [abs](http://arxiv.org/abs/2009.09333v1) - [pdf](http://arxiv.org/pdf/2009.09333v1)

> Trajectory data generation is an important domain that characterizes the generative process of mobility data. Traditional methods heavily rely on predefined heuristics and distributions and are weak in learning unknown mechanisms. Inspired by the success of deep generative neural networks for images and texts, a fast-developing research topic is deep generative models for trajectory data which can learn expressively explanatory models for sophisticated latent patterns. This is a nascent yet promising domain for many applications. We first propose novel deep generative models factorizing time-variant and time-invariant latent variables that characterize global and local semantics, respectively. We then develop new inference strategies based on variational inference and constrained optimization to encapsulate the spatiotemporal validity. New deep neural network architectures have been developed to implement the inference and generation models with newly-generalized latent variable priors. The proposed methods achieved significant improvements in quantitative and qualitative evaluations in extensive experiments.

</details>

<details>

<summary>2020-09-20 05:47:23 - A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining</summary>

- *Chenguang Zhu, Ruochen Xu, Michael Zeng, Xuedong Huang*

- `2004.02016v4` - [abs](http://arxiv.org/abs/2004.02016v4) - [pdf](http://arxiv.org/pdf/2004.02016v4)

> With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep neural models for text summarization and dialogue systems. However, the semantic structure and styles of meeting transcripts are quite different from articles and conversations. In this paper, we propose a novel abstractive summary network that adapts to the meeting scenario. We design a hierarchical structure to accommodate long meeting transcripts and a role vector to depict the difference among speakers. Furthermore, due to the inadequacy of meeting summary data, we pretrain the model on large-scale news summary data. Empirical results show that our model outperforms previous approaches in both automatic metrics and human evaluation. For example, on ICSI dataset, the ROUGE-1 score increases from 34.66% to 46.28%.

</details>

<details>

<summary>2020-09-20 08:26:38 - Deriving Visual Semantics from Spatial Context: An Adaptation of LSA and Word2Vec to generate Object and Scene Embeddings from Images</summary>

- *Matthias S. Treder, Juan Mayor-Torres, Christoph Teufel*

- `2009.09384v1` - [abs](http://arxiv.org/abs/2009.09384v1) - [pdf](http://arxiv.org/pdf/2009.09384v1)

> Embeddings are an important tool for the representation of word meaning. Their effectiveness rests on the distributional hypothesis: words that occur in the same context carry similar semantic information. Here, we adapt this approach to index visual semantics in images of scenes. To this end, we formulate a distributional hypothesis for objects and scenes: Scenes that contain the same objects (object context) are semantically related. Similarly, objects that appear in the same spatial context (within a scene or subregions of a scene) are semantically related. We develop two approaches for learning object and scene embeddings from annotated images. In the first approach, we adapt LSA and Word2vec's Skipgram and CBOW models to generate two sets of embeddings from object co-occurrences in whole images, one for objects and one for scenes. The representational space spanned by these embeddings suggests that the distributional hypothesis holds for images. In an initial application of this approach, we show that our image-based embeddings improve scene classification models such as ResNet18 and VGG-11 (3.72\% improvement on Top5 accuracy, 4.56\% improvement on Top1 accuracy). In the second approach, rather than analyzing whole images of scenes, we focus on co-occurrences of objects within subregions of an image. We illustrate that this method yields a sensible hierarchical decomposition of a scene into collections of semantically related objects. Overall, these results suggest that object and scene embeddings from object co-occurrences and spatial context yield semantically meaningful representations as well as computational improvements for downstream applications such as scene classification.

</details>

<details>

<summary>2020-09-20 14:42:33 - Supervised Ontology and Instance Matching with MELT</summary>

- *Sven Hertling, Jan Portisch, Heiko Paulheim*

- `2009.11102v1` - [abs](http://arxiv.org/abs/2009.11102v1) - [pdf](http://arxiv.org/pdf/2009.11102v1)

> In this paper, we present MELT-ML, a machine learning extension to the Matching and EvaLuation Toolkit (MELT) which facilitates the application of supervised learning for ontology and instance matching. Our contributions are twofold: We present an open source machine learning extension to the matching toolkit as well as two supervised learning use cases demonstrating the capabilities of the new extension.

</details>

<details>

<summary>2020-09-20 18:42:13 - Learning Soft Labels via Meta Learning</summary>

- *Nidhi Vyas, Shreyas Saxena, Thomas Voice*

- `2009.09496v1` - [abs](http://arxiv.org/abs/2009.09496v1) - [pdf](http://arxiv.org/pdf/2009.09496v1)

> One-hot labels do not represent soft decision boundaries among concepts, and hence, models trained on them are prone to overfitting. Using soft labels as targets provide regularization, but different soft labels might be optimal at different stages of optimization. Also, training with fixed labels in the presence of noisy annotations leads to worse generalization. To address these limitations, we propose a framework, where we treat the labels as learnable parameters, and optimize them along with model parameters. The learned labels continuously adapt themselves to the model's state, thereby providing dynamic regularization. When applied to the task of supervised image-classification, our method leads to consistent gains across different datasets and architectures. For instance, dynamically learned labels improve ResNet18 by 2.1% on CIFAR100. When applied to dataset containing noisy labels, the learned labels correct the annotation mistakes, and improves over state-of-the-art by a significant margin. Finally, we show that learned labels capture semantic relationship between classes, and thereby improve teacher models for the downstream task of distillation.

</details>

<details>

<summary>2020-09-20 19:50:28 - Relation Extraction from Biomedical and Clinical Text: Unified Multitask Learning Framework</summary>

- *Shweta Yadav, Srivatsa Ramesh, Sriparna Saha, Asif Ekbal*

- `2009.09509v1` - [abs](http://arxiv.org/abs/2009.09509v1) - [pdf](http://arxiv.org/pdf/2009.09509v1)

> To minimize the accelerating amount of time invested in the biomedical literature search, numerous approaches for automated knowledge extraction have been proposed. Relation extraction is one such task where semantic relations between the entities are identified from the free text. In the biomedical domain, extraction of regulatory pathways, metabolic processes, adverse drug reaction or disease models necessitates knowledge from the individual relations, for example, physical or regulatory interactions between genes, proteins, drugs, chemical, disease or phenotype. In this paper, we study the relation extraction task from three major biomedical and clinical tasks, namely drug-drug interaction, protein-protein interaction, and medical concept relation extraction. Towards this, we model the relation extraction problem in multi-task learning (MTL) framework and introduce for the first time the concept of structured self-attentive network complemented with the adversarial learning approach for the prediction of relationships from the biomedical and clinical text. The fundamental notion of MTL is to simultaneously learn multiple problems together by utilizing the concepts of the shared representation. Additionally, we also generate the highly efficient single task model which exploits the shortest dependency path embedding learned over the attentive gated recurrent unit to compare our proposed MTL models. The framework we propose significantly improves overall the baselines (deep learning techniques) and single-task models for predicting the relationships, without compromising on the performance of all the tasks.

</details>

<details>

<summary>2020-09-20 23:51:37 - Check_square at CheckThat! 2020: Claim Detection in Social Media via Fusion of Transformer and Syntactic Features</summary>

- *Gullal S. Cheema, Sherzod Hakimov, Ralph Ewerth*

- `2007.10534v2` - [abs](http://arxiv.org/abs/2007.10534v2) - [pdf](http://arxiv.org/pdf/2007.10534v2)

> In this digital age of news consumption, a news reader has the ability to react, express and share opinions with others in a highly interactive and fast manner. As a consequence, fake news has made its way into our daily life because of very limited capacity to verify news on the Internet by large companies as well as individuals. In this paper, we focus on solving two problems which are part of the fact-checking ecosystem that can help to automate fact-checking of claims in an ever increasing stream of content on social media. For the first problem, claim check-worthiness prediction, we explore the fusion of syntactic features and deep transformer Bidirectional Encoder Representations from Transformers (BERT) embeddings, to classify check-worthiness of a tweet, i.e. whether it includes a claim or not. We conduct a detailed feature analysis and present our best performing models for English and Arabic tweets. For the second problem, claim retrieval, we explore the pre-trained embeddings from a Siamese network transformer model (sentence-transformers) specifically trained for semantic textual similarity, and perform KD-search to retrieve verified claims with respect to a query tweet.

</details>

<details>

<summary>2020-09-21 07:59:38 - Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange</summary>

- *Steven Y. Feng, Aaron W. Li, Jesse Hoey*

- `1909.00088v2` - [abs](http://arxiv.org/abs/1909.00088v2) - [pdf](http://arxiv.org/pdf/1909.00088v2)

> In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipeline's success by its Semantic Text Exchange Score (STES): the ability to preserve the original text's sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.

</details>

<details>

<summary>2020-09-21 10:48:07 - Measurement Dependence Inducing Latent Causal Models</summary>

- *Alex Markham, Moritz Grosse-Wentrup*

- `1910.08778v3` - [abs](http://arxiv.org/abs/1910.08778v3) - [pdf](http://arxiv.org/pdf/1910.08778v3)

> We consider the task of causal structure learning over measurement dependence inducing latent (MeDIL) causal models. We show that this task can be framed in terms of the graph theoretic problem of finding edge clique covers,resulting in an algorithm for returning minimal MeDIL causal models (minMCMs). This algorithm is non-parametric, requiring no assumptions about linearity or Gaussianity. Furthermore, despite rather weak assumptions aboutthe class of MeDIL causal models, we show that minimality in minMCMs implies some rather specific and interesting properties. By establishing MeDIL causal models as a semantics for edge clique covers, we also provide a starting point for future work further connecting causal structure learning to developments in graph theory and network science.

</details>

<details>

<summary>2020-09-21 13:42:59 - A Dataset of Laryngeal Endoscopic Images with Comparative Study on Convolution Neural Network Based Semantic Segmentation</summary>

- *Max-Heinrich Laves, Jens Bicker, Lüder A. Kahrs, Tobias Ortmaier*

- `1807.06081v4` - [abs](http://arxiv.org/abs/1807.06081v4) - [pdf](http://arxiv.org/pdf/1807.06081v4)

> Purpose Automated segmentation of anatomical structures in medical image analysis is a prerequisite for autonomous diagnosis as well as various computer and robot aided interventions. Recent methods based on deep convolutional neural networks (CNN) have outperformed former heuristic methods. However, those methods were primarily evaluated on rigid, real-world environments. In this study, existing segmentation methods were evaluated for their use on a new dataset of transoral endoscopic exploration. Methods Four machine learning based methods SegNet, UNet, ENet and ErfNet were trained with supervision on a novel 7-class dataset of the human larynx. The dataset contains 536 manually segmented images from two patients during laser incisions. The Intersection-over-Union (IoU) evaluation metric was used to measure the accuracy of each method. Data augmentation and network ensembling were employed to increase segmentation accuracy. Stochastic inference was used to show uncertainties of the individual models. Patient-to-patient transfer was investigated using patient-specific fine-tuning. Results In this study, a weighted average ensemble network of UNet and ErfNet was best suited for the segmentation of laryngeal soft tissue with a mean IoU of 84.7 %. The highest efficiency was achieved by ENet with a mean inference time of 9.22 ms per image. It is shown that 10 additional images from a new patient are sufficient for patient-specific fine-tuning. Conclusion CNN-based methods for semantic segmentation are applicable to endoscopic images of laryngeal soft tissue. The segmentation can be used for active constraints or to monitor morphological changes and autonomously detect pathologies. Further improvements could be achieved by using a larger dataset or training the models in a self-supervised manner on additional unlabeled data.

</details>

<details>

<summary>2020-09-21 14:14:05 - NABU $\mathrm{-}$ Multilingual Graph-based Neural RDF Verbalizer</summary>

- *Diego Moussallem, Dwaraknath Gnaneshwar, Thiago Castro Ferreira, Axel-Cyrille Ngonga Ngomo*

- `2009.07728v2` - [abs](http://arxiv.org/abs/2009.07728v2) - [pdf](http://arxiv.org/pdf/2009.07728v2)

> The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU.

</details>

<details>

<summary>2020-09-21 14:46:17 - Feature Distillation With Guided Adversarial Contrastive Learning</summary>

- *Tao Bai, Jinnan Chen, Jun Zhao, Bihan Wen, Xudong Jiang, Alex Kot*

- `2009.09922v1` - [abs](http://arxiv.org/abs/2009.09922v1) - [pdf](http://arxiv.org/pdf/2009.09922v1)

> Deep learning models are shown to be vulnerable to adversarial examples. Though adversarial training can enhance model robustness, typical approaches are computationally expensive. Recent works proposed to transfer the robustness to adversarial attacks across different tasks or models with soft labels.Compared to soft labels, feature contains rich semantic information and holds the potential to be applied to different downstream tasks. In this paper, we propose a novel approach called Guided Adversarial Contrastive Distillation (GACD), to effectively transfer adversarial robustness from teacher to student with features. We first formulate this objective as contrastive learning and connect it with mutual information. With a well-trained teacher model as an anchor, students are expected to extract features similar to the teacher. Then considering the potential errors made by teachers, we propose sample reweighted estimation to eliminate the negative effects from teachers. With GACD, the student not only learns to extract robust features, but also captures structural knowledge from the teacher. By extensive experiments evaluating over popular datasets such as CIFAR-10, CIFAR-100 and STL-10, we demonstrate that our approach can effectively transfer robustness across different models and even different tasks, and achieve comparable or better results than existing methods. Besides, we provide a detailed analysis of various methods, showing that students produced by our approach capture more structural knowledge from teachers and learn more robust features under adversarial attacks.

</details>

<details>

<summary>2020-09-21 17:04:32 - Visual-Semantic Embedding Model Informed by Structured Knowledge</summary>

- *Mirantha Jayathilaka, Tingting Mu, Uli Sattler*

- `2009.10026v1` - [abs](http://arxiv.org/abs/2009.10026v1) - [pdf](http://arxiv.org/pdf/2009.10026v1)

> We propose a novel approach to improve a visual-semantic embedding model by incorporating concept representations captured from an external structured knowledge base. We investigate its performance on image classification under both standard and zero-shot settings. We propose two novel evaluation frameworks to analyse classification errors with respect to the class hierarchy indicated by the knowledge base. The approach is tested using the ILSVRC 2012 image dataset and a WordNet knowledge base. With respect to both standard and zero-shot image classification, our approach shows superior performance compared with the original approach, which uses word embeddings.

</details>

<details>

<summary>2020-09-21 17:47:44 - Latin BERT: A Contextual Language Model for Classical Philology</summary>

- *David Bamman, Patrick J. Burns*

- `2009.10053v1` - [abs](http://arxiv.org/abs/2009.10053v1) - [pdf](http://arxiv.org/pdf/2009.10053v1)

> We present Latin BERT, a contextual language model for the Latin language, trained on 642.7 million words from a variety of sources spanning the Classical era to the 21st century. In a series of case studies, we illustrate the affordances of this language-specific model both for work in natural language processing for Latin and in using computational methods for traditional scholarship: we show that Latin BERT achieves a new state of the art for part-of-speech tagging on all three Universal Dependency datasets for Latin and can be used for predicting missing text (including critical emendations); we create a new dataset for assessing word sense disambiguation for Latin and demonstrate that Latin BERT outperforms static word embeddings; and we show that it can be used for semantically-informed search by querying contextual nearest neighbors. We publicly release trained models to help drive future work in this space.

</details>

<details>

<summary>2020-09-21 20:55:05 - Recommending Stack Overflow Posts for Fixing Runtime Exceptions using Failure Scenario Matching</summary>

- *Sonal Mahajan, Negarsadat Abolhassani, Mukul R. Prasad*

- `2009.10174v1` - [abs](http://arxiv.org/abs/2009.10174v1) - [pdf](http://arxiv.org/pdf/2009.10174v1)

> Using online Q&A forums, such as Stack Overflow (SO), for guidance to resolve program bugs, among other development issues, is commonplace in modern software development practice. Runtime exceptions (RE) is one such important class of bugs that is actively discussed on SO. In this work we present a technique and prototype tool called MAESTRO that can automatically recommend an SO post that is most relevant to a given Java RE in a developer's code. MAESTRO compares the exception-generating program scenario in the developer's code with that discussed in an SO post and returns the post with the closest match. To extract and compare the exception scenario effectively, MAESTRO first uses the answer code snippets in a post to implicate a subset of lines in the post's question code snippet as responsible for the exception and then compares these lines with the developer's code in terms of their respective Abstract Program Graph (APG) representations. The APG is a simplified and abstracted derivative of an abstract syntax tree, proposed in this work, that allows an effective comparison of the functionality embodied in the high-level program structure, while discarding many of the low-level syntactic or semantic differences. We evaluate MAESTRO on a benchmark of 78 instances of Java REs extracted from the top 500 Java projects on GitHub and show that MAESTRO can return either a highly relevant or somewhat relevant SO post corresponding to the exception instance in 71% of the cases, compared to relevant posts returned in only 8% - 44% instances, by four competitor tools based on state-of-the-art techniques. We also conduct a user experience study of MAESTRO with 10 Java developers, where the participants judge MAESTRO reporting a highly relevant or somewhat relevant post in 80% of the instances. In some cases the post is judged to be even better than the one manually found by the participant.

</details>

<details>

<summary>2020-09-22 00:48:18 - SQuARE: Semantics-based Question Answering and Reasoning Engine</summary>

- *Kinjal Basu, Sarat Chandra Varanasi, Farhad Shakerin, Gopal Gupta*

- `2009.10239v1` - [abs](http://arxiv.org/abs/2009.10239v1) - [pdf](http://arxiv.org/pdf/2009.10239v1)

> Understanding the meaning of a text is a fundamental challenge of natural language understanding (NLU) and from its early days, it has received significant attention through question answering (QA) tasks. We introduce a general semantics-based framework for natural language QA and also describe the SQuARE system, an application of this framework. The framework is based on the denotational semantics approach widely used in programming language research. In our framework, valuation function maps syntax tree of the text to its commonsense meaning represented using basic knowledge primitives (the semantic algebra) coded using answer set programming (ASP). We illustrate an application of this framework by using VerbNet primitives as our semantic algebra and a novel algorithm based on partial tree matching that generates an answer set program that represents the knowledge in the text. A question posed against that text is converted into an ASP query using the same framework and executed using the s(CASP) goal-directed ASP system. Our approach is based purely on (commonsense) reasoning. SQuARE achieves 100% accuracy on all the five datasets of bAbI QA tasks that we have tested. The significance of our work is that, unlike other machine learning based approaches, ours is based on "understanding" the text and does not require any training. SQuARE can also generate an explanation for an answer while maintaining high accuracy.

</details>

<details>

<summary>2020-09-22 00:51:13 - A Machine Learning guided Rewriting Approach for ASP Logic Programs</summary>

- *Elena Mastria, Jessica Zangari, Simona Perri, Francesco Calimeri*

- `2009.10252v1` - [abs](http://arxiv.org/abs/2009.10252v1) - [pdf](http://arxiv.org/pdf/2009.10252v1)

> Answer Set Programming (ASP) is a declarative logic formalism that allows to encode computational problems via logic programs. Despite the declarative nature of the formalism, some advanced expertise is required, in general, for designing an ASP encoding that can be efficiently evaluated by an actual ASP system. A common way for trying to reduce the burden of manually tweaking an ASP program consists in automatically rewriting the input encoding according to suitable techniques, for producing alternative, yet semantically equivalent, ASP programs. However, rewriting does not always grant benefits in terms of performance; hence, proper means are needed for predicting their effects with this respect. In this paper we describe an approach based on Machine Learning (ML) to automatically decide whether to rewrite. In particular, given an ASP program and a set of input facts, our approach chooses whether and how to rewrite input rules based on a set of features measuring their structural properties and domain information. To this end, a Multilayer Perceptrons model has then been trained to guide the ASP grounder I-DLV on rewriting input rules. We report and discuss the results of an experimental evaluation over a prototypical implementation.

</details>

<details>

<summary>2020-09-22 01:02:07 - ALICE: Active Learning with Contrastive Natural Language Explanations</summary>

- *Weixin Liang, James Zou, Zhou Yu*

- `2009.10259v1` - [abs](http://arxiv.org/abs/2009.10259v1) - [pdf](http://arxiv.org/pdf/2009.10259v1)

> Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides several bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points.

</details>

<details>

<summary>2020-09-22 01:30:29 - Semantic Workflows and Machine Learning for the Assessment of Carbon Storage by Urban Trees</summary>

- *Juan Carrillo, Daniel Garijo, Mark Crowley, Rober Carrillo, Yolanda Gil, Katherine Borda*

- `2009.10263v1` - [abs](http://arxiv.org/abs/2009.10263v1) - [pdf](http://arxiv.org/pdf/2009.10263v1)

> Climate science is critical for understanding both the causes and consequences of changes in global temperatures and has become imperative for decisive policy-making. However, climate science studies commonly require addressing complex interoperability issues between data, software, and experimental approaches from multiple fields. Scientific workflow systems provide unparalleled advantages to address these issues, including reproducibility of experiments, provenance capture, software reusability and knowledge sharing. In this paper, we introduce a novel workflow with a series of connected components to perform spatial data preparation, classification of satellite imagery with machine learning algorithms, and assessment of carbon stored by urban trees. To the best of our knowledge, this is the first study that estimates carbon storage for a region in Africa following the guidelines from the Intergovernmental Panel on Climate Change (IPCC).

</details>

<details>

<summary>2020-09-22 07:30:19 - Global-to-Local Neural Networks for Document-Level Relation Extraction</summary>

- *Difeng Wang, Wei Hu, Ermei Cao, Weijian Sun*

- `2009.10359v1` - [abs](http://arxiv.org/abs/2009.10359v1) - [pdf](http://arxiv.org/pdf/2009.10359v1)

> Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.

</details>

<details>

<summary>2020-09-22 08:58:24 - Unsupervised Translation of Programming Languages</summary>

- *Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample*

- `2006.03511v3` - [abs](http://arxiv.org/abs/2006.03511v3) - [pdf](http://arxiv.org/pdf/2006.03511v3)

> A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.

</details>

<details>

<summary>2020-09-22 13:31:37 - Context-theoretic Semantics for Natural Language: an Algebraic Framework</summary>

- *Daoud Clarke*

- `2009.10542v1` - [abs](http://arxiv.org/abs/2009.10542v1) - [pdf](http://arxiv.org/pdf/2009.10542v1)

> Techniques in which words are represented as vectors have proved useful in many applications in computational linguistics, however there is currently no general semantic formalism for representing meaning in terms of vectors. We present a framework for natural language semantics in which words, phrases and sentences are all represented as vectors, based on a theoretical analysis which assumes that meaning is determined by context.   In the theoretical analysis, we define a corpus model as a mathematical abstraction of a text corpus. The meaning of a string of words is assumed to be a vector representing the contexts it occurs in in the corpus model. Based on this assumption, we can show that the vector representations of words can be considered as elements of an algebra over a field. We note that in applications of vector spaces to representing meanings of words there is an underlying lattice structure; we interpret the partial ordering of the lattice as describing entailment between meanings. We also define the context-theoretic probability of a string, and, based on this and the lattice structure, a degree of entailment between strings.   Together these properties form guidelines as to how to construct semantic representations within the framework. A context theory is an implementation of the framework; in an implementation strings are represented as vectors with the properties deduced from the theoretical analysis.   We show how to incorporate logical semantics into context theories; this enables us to represent statistical information about uncertainty by taking weighted sums of individual representations. We also use the framework to analyse approaches to the task of recognising textual entailment, to ontological representations of meaning and to representing syntactic structure. For the latter, we give new algebraic descriptions of link grammar.

</details>

<details>

<summary>2020-09-22 14:24:12 - A Deep Learning Approach to Geographical Candidate Selection through Toponym Matching</summary>

- *Mariona Coll Ardanuy, Kasra Hosseini, Katherine McDonough, Amrey Krause, Daniel van Strien, Federico Nanni*

- `2009.08114v2` - [abs](http://arxiv.org/abs/2009.08114v2) - [pdf](http://arxiv.org/pdf/2009.08114v2)

> Recognizing toponyms and resolving them to their real-world referents is required for providing advanced semantic access to textual data. This process is often hindered by the high degree of variation in toponyms. Candidate selection is the task of identifying the potential entities that can be referred to by a toponym previously recognized. While it has traditionally received little attention in the research community, it has been shown that candidate selection has a significant impact on downstream tasks (i.e. entity resolution), especially in noisy or non-standard text. In this paper, we introduce a flexible deep learning method for candidate selection through toponym matching, using state-of-the-art neural network architectures. We perform an intrinsic toponym matching evaluation based on several new realistic datasets, which cover various challenging scenarios (cross-lingual and regional variations, as well as OCR errors). We report its performance on candidate selection in the context of the downstream task of toponym resolution, both on existing datasets and on a new manually-annotated resource of nineteenth-century English OCR'd text.

</details>

<details>

<summary>2020-09-22 15:45:45 - ThingML+ Augmenting Model-Driven Software Engineering for the Internet of Things with Machine Learning</summary>

- *Armin Moin, Stephan Rössler, Stephan Günnemann*

- `2009.10633v1` - [abs](http://arxiv.org/abs/2009.10633v1) - [pdf](http://arxiv.org/pdf/2009.10633v1)

> In this paper, we present the current position of the research project ML-Quadrat, which aims to extend the methodology, modeling language and tool support of ThingML - an open source modeling tool for IoT/CPS - to address Machine Learning needs for the IoT applications. Currently, ThingML offers a modeling language and tool support for modeling the components of the system, their communication interfaces as well as their behaviors. The latter is done through state machines. However, we argue that in many cases IoT/CPS services involve system components and physical processes, whose behaviors are not well understood in order to be modeled using state machines. Hence, quite often a data-driven approach that enables inference based on the observed data, e.g., using Machine Learning is preferred. To this aim, ML-Quadrat integrates the necessary Machine Learning concepts into ThingML both on the modeling level (syntax and semantics of the modeling language) and on the code generators level. We plan to support two target platforms for code generation regarding Stream Processing and Complex Event Processing, namely Apache SAMOA and Apama.

</details>

<details>

<summary>2020-09-22 16:05:16 - ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds</summary>

- *Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis, Siddhartha Chaudhuri, Radomír Měch*

- `2003.12181v5` - [abs](http://arxiv.org/abs/2003.12181v5) - [pdf](http://arxiv.org/pdf/2003.12181v5)

> We propose a novel, end-to-end trainable, deep network called ParSeNet that decomposes a 3D point cloud into parametric surface patches, including B-spline patches as well as basic geometric primitives. ParSeNet is trained on a large-scale dataset of man-made 3D shapes and captures high-level semantic priors for shape decomposition. It handles a much richer class of primitives than prior work, and allows us to represent surfaces with higher fidelity. It also produces repeatable and robust parametrizations of a surface compared to purely geometric approaches. We present extensive experiments to validate our approach against analytical and learning-based alternatives. Our source code is publicly available at: https://hippogriff.github.io/parsenet.

</details>

<details>

<summary>2020-09-22 18:46:05 - Role of Orthogonality Constraints in Improving Properties of Deep Networks for Image Classification</summary>

- *Hongjun Choi, Anirudh Som, Pavan Turaga*

- `2009.10762v1` - [abs](http://arxiv.org/abs/2009.10762v1) - [pdf](http://arxiv.org/pdf/2009.10762v1)

> Standard deep learning models that employ the categorical cross-entropy loss are known to perform well at image classification tasks. However, many standard models thus obtained often exhibit issues like feature redundancy, low interpretability, and poor calibration. A body of recent work has emerged that has tried addressing some of these challenges by proposing the use of new regularization functions in addition to the cross-entropy loss. In this paper, we present some surprising findings that emerge from exploring the role of simple orthogonality constraints as a means of imposing physics-motivated constraints common in imaging. We propose an Orthogonal Sphere (OS) regularizer that emerges from physics-based latent-representations under simplifying assumptions. Under further simplifying assumptions, the OS constraint can be written in closed-form as a simple orthonormality term and be used along with the cross-entropy loss function. The findings indicate that orthonormality loss function results in a) rich and diverse feature representations, b) robustness to feature sub-selection, c) better semantic localization in the class activation maps, and d) reduction in model calibration error. We demonstrate the effectiveness of the proposed OS regularization by providing quantitative and qualitative results on four benchmark datasets - CIFAR10, CIFAR100, SVHN and tiny ImageNet.

</details>

<details>

<summary>2020-09-22 19:09:17 - Deep Generalized Convolutional Sum-Product Networks</summary>

- *Jos van de Wolfshaar, Andrzej Pronobis*

- `1902.06155v4` - [abs](http://arxiv.org/abs/1902.06155v4) - [pdf](http://arxiv.org/pdf/1902.06155v4)

> Sum-Product Networks (SPNs) are hierarchical, graphical models that combine benefits of deep learning and probabilistic modeling. SPNs offer unique advantages to applications demanding exact probabilistic inference over high-dimensional, noisy inputs. Yet, compared to convolutional neural nets, they struggle with capturing complex spatial relationships in image data. To alleviate this issue, we introduce Deep Generalized Convolutional Sum-Product Networks (DGC-SPNs), which encode spatial features in a way similar to CNNs, while preserving the validity of the probabilistic SPN model. As opposed to existing SPN-based image representations, DGC-SPNs allow for overlapping convolution patches through a novel parameterization of dilations and strides, resulting in significantly improved feature coverage and feature resolution. DGC-SPNs substantially outperform other SPN architectures across several visual datasets and for both generative and discriminative tasks, including image inpainting and classification. These contributions are reinforced by the first simple, scalable, and GPU-optimized implementation of SPNs, integrated with the widely used Keras/TensorFlow framework. The resulting model is fully probabilistic and versatile, yet efficient and straightforward to apply in practical applications in place of traditional deep nets.

</details>

<details>

<summary>2020-09-22 22:38:54 - Message Passing for Hyper-Relational Knowledge Graphs</summary>

- *Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, Jens Lehmann*

- `2009.10847v1` - [abs](http://arxiv.org/abs/2009.10847v1) - [pdf](http://arxiv.org/pdf/2009.10847v1)

> Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.

</details>

<details>

<summary>2020-09-23 07:42:14 - Semantics-Preserving Adversarial Training</summary>

- *Wonseok Lee, Hanbit Lee, Sang-goo Lee*

- `2009.10978v1` - [abs](http://arxiv.org/abs/2009.10978v1) - [pdf](http://arxiv.org/pdf/2009.10978v1)

> Adversarial training is a defense technique that improves adversarial robustness of a deep neural network (DNN) by including adversarial examples in the training data. In this paper, we identify an overlooked problem of adversarial training in that these adversarial examples often have different semantics than the original data, introducing unintended biases into the model. We hypothesize that such non-semantics-preserving (and resultingly ambiguous) adversarial data harm the robustness of the target models. To mitigate such unintended semantic changes of adversarial examples, we propose semantics-preserving adversarial training (SPAT) which encourages perturbation on the pixels that are shared among all classes when generating adversarial examples in the training stage. Experiment results show that SPAT improves adversarial robustness and achieves state-of-the-art results in CIFAR-10 and CIFAR-100.

</details>

<details>

<summary>2020-09-23 08:00:56 - Towards a Flexible Embedding Learning Framework</summary>

- *Chin-Chia Michael Yeh, Dhruv Gelda, Zhongfang Zhuang, Yan Zheng, Liang Gou, Wei Zhang*

- `2009.10989v1` - [abs](http://arxiv.org/abs/2009.10989v1) - [pdf](http://arxiv.org/pdf/2009.10989v1)

> Representation learning is a fundamental building block for analyzing entities in a database. While the existing embedding learning methods are effective in various data mining problems, their applicability is often limited because these methods have pre-determined assumptions on the type of semantics captured by the learned embeddings, and the assumptions may not well align with specific downstream tasks. In this work, we propose an embedding learning framework that 1) uses an input format that is agnostic to input data type, 2) is flexible in terms of the relationships that can be embedded into the learned representations, and 3) provides an intuitive pathway to incorporate domain knowledge into the embedding learning process. Our proposed framework utilizes a set of entity-relation-matrices as the input, which quantifies the affinities among different entities in the database. Moreover, a sampling mechanism is carefully designed to establish a direct connection between the input and the information captured by the output embeddings. To complete the representation learning toolbox, we also outline a simple yet effective post-processing technique to properly visualize the learned embeddings. Our empirical results demonstrate that the proposed framework, in conjunction with a set of relevant entity-relation-matrices, outperforms the existing state-of-the-art approaches in various data mining tasks.

</details>

<details>

<summary>2020-09-23 11:10:12 - Testing the Quantitative Spacetime Hypothesis using Artificial Narrative Comprehension (I) : Bootstrapping Meaning from Episodic Narrative viewed as a Feature Landscape</summary>

- *Mark Burgess*

- `2010.08126v1` - [abs](http://arxiv.org/abs/2010.08126v1) - [pdf](http://arxiv.org/pdf/2010.08126v1)

> The problem of extracting important and meaningful parts of a sensory data stream, without prior training, is studied for symbolic sequences, by using textual narrative as a test case. This is part of a larger study concerning the extraction of concepts from spacetime processes, and their knowledge representations within hybrid symbolic-learning `Artificial Intelligence'. Most approaches to text analysis make extensive use of the evolved human sense of language and semantics. In this work, streams are parsed without knowledge of semantics, using only measurable patterns (size and time) within the changing stream of symbols -- as an event `landscape'. This is a form of interferometry. Using lightweight procedures that can be run in just a few seconds on a single CPU, this work studies the validity of the Semantic Spacetime Hypothesis, for the extraction of concepts as process invariants. This `semantic preprocessor' may then act as a front-end for more sophisticated long-term graph-based learning techniques. The results suggest that what we consider important and interesting about sensory experience is not solely based on higher reasoning, but on simple spacetime process cues, and this may be how cognitive processing is bootstrapped in the beginning.

</details>

<details>

<summary>2020-09-23 11:19:17 - Testing the Quantitative Spacetime Hypothesis using Artificial Narrative Comprehension (II) : Establishing the Geometry of Invariant Concepts, Themes, and Namespaces</summary>

- *Mark Burgess*

- `2010.08125v1` - [abs](http://arxiv.org/abs/2010.08125v1) - [pdf](http://arxiv.org/pdf/2010.08125v1)

> Given a pool of observations selected from a sensor stream, input data can be robustly represented, via a multiscale process, in terms of invariant concepts, and themes. Applying this to episodic natural language data, one may obtain a graph geometry associated with the decomposition, which is a direct encoding of spacetime relationships for the events. This study contributes to an ongoing application of the Semantic Spacetime Hypothesis, and demonstrates the unsupervised analysis of narrative texts using inexpensive computational methods without knowledge of linguistics. Data streams are parsed and fractionated into small constituents, by multiscale interferometry, in the manner of bioinformatic analysis. Fragments may then be recombined to construct original sensory episodes---or form new narratives by a chemistry of association and pattern reconstruction, based only on the four fundamental spacetime relationships. There is a straightforward correspondence between bioinformatic processes and this cognitive representation of natural language. Features identifiable as `concepts' and `narrative themes' span three main scales (micro, meso, and macro). Fragments of the input act as symbols in a hierarchy of alphabets that define new effective languages at each scale.

</details>

<details>

<summary>2020-09-23 13:12:30 - Cosine Similarity of Multimodal Content Vectors for TV Programmes</summary>

- *Saba Nazir, Taner Cagali, Chris Newell, Mehrnoosh Sadrzadeh*

- `2009.11129v1` - [abs](http://arxiv.org/abs/2009.11129v1) - [pdf](http://arxiv.org/pdf/2009.11129v1)

> Multimodal information originates from a variety of sources: audiovisual files, textual descriptions, and metadata. We show how one can represent the content encoded by each individual source using vectors, how to combine the vectors via middle and late fusion techniques, and how to compute the semantic similarities between the contents. Our vectorial representations are built from spectral features and Bags of Audio Words, for audio, LSI topics and Doc2vec embeddings for subtitles, and the categorical features, for metadata. We implement our model on a dataset of BBC TV programmes and evaluate the fused representations to provide recommendations. The late fused similarity matrices significantly improve the precision and diversity of recommendations.

</details>

<details>

<summary>2020-09-23 13:41:27 - Evolution of Part-of-Speech in Classical Chinese</summary>

- *Bai Li*

- `2009.11144v1` - [abs](http://arxiv.org/abs/2009.11144v1) - [pdf](http://arxiv.org/pdf/2009.11144v1)

> Classical Chinese is a language notable for its word class flexibility: the same word may often be used as a noun or a verb. Bisang (2008) claimed that Classical Chinese is a precategorical language, where the syntactic position of a word determines its part-of-speech category. In this paper, we apply entropy-based metrics to evaluate these claims on historical corpora. We further explore differences between nouns and verbs in Classical Chinese: using psycholinguistic norms, we find a positive correlation between concreteness and noun usage. Finally, we align character embeddings from Classical and Modern Chinese, and find that verbs undergo more semantic change than nouns.

</details>

<details>

<summary>2020-09-23 14:33:47 - Bootstrapping a Crosslingual Semantic Parser</summary>

- *Tom Sherborne, Yumo Xu, Mirella Lapata*

- `2004.02585v4` - [abs](http://arxiv.org/abs/2004.02585v4) - [pdf](http://arxiv.org/pdf/2004.02585v4)

> Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple domains with minimal annotation. We query if machine translation is an adequate substitute for training data, and extend this to investigate bootstrapping using joint training with English, paraphrasing, and multilingual pre-trained models. We develop a Transformer-based parser combining paraphrases by ensembling attention over multiple encoders and present new versions of ATIS and Overnight in German and Chinese for evaluation. Experimental results indicate that MT can approximate training data in a new language for accurate parsing when augmented with paraphrasing through multiple MT engines. Considering when MT is inadequate, we also find that using our approach achieves parsing accuracy within 2% of complete translation using only 50% of training data.

</details>

<details>

<summary>2020-09-23 15:45:32 - A Comparative Study on Structural and Semantic Properties of Sentence Embeddings</summary>

- *Alexander Kalinowski, Yuan An*

- `2009.11226v1` - [abs](http://arxiv.org/abs/2009.11226v1) - [pdf](http://arxiv.org/pdf/2009.11226v1)

> Sentence embeddings encode natural language sentences as low-dimensional dense vectors. A great deal of effort has been put into using sentence embeddings to improve several important natural language processing tasks. Relation extraction is such an NLP task that aims at identifying structured relations defined in a knowledge base from unstructured text. A promising and more efficient approach would be to embed both the text and structured knowledge in low-dimensional spaces and discover semantic alignments or mappings between them. Although a number of techniques have been proposed in the literature for embedding both sentences and knowledge graphs, little is known about the structural and semantic properties of these embedding spaces in terms of relation extraction. In this paper, we investigate the aforementioned properties by evaluating the extent to which sentences carrying similar senses are embedded in close proximity sub-spaces, and if we can exploit that structure to align sentences to a knowledge graph. We propose a set of experiments using a widely-used large-scale data set for relation extraction and focusing on a set of key sentence embedding methods. We additionally provide the code for reproducing these experiments at https://github.com/akalino/semantic-structural-sentences. These embedding methods cover a wide variety of techniques ranging from simple word embedding combination to transformer-based BERT-style model. Our experimental results show that different embedding spaces have different degrees of strength for the structural and semantic properties. These results provide useful information for developing embedding-based relation extraction methods.

</details>

<details>

<summary>2020-09-23 17:45:17 - X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</summary>

- *Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi*

- `2009.11278v1` - [abs](http://arxiv.org/abs/2009.11278v1) - [pdf](http://arxiv.org/pdf/2009.11278v1)

> Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.

</details>

<details>

<summary>2020-09-23 17:53:00 - Region Growing with Convolutional Neural Networks for Biomedical Image Segmentation</summary>

- *John Lagergren, Erica Rutter, Kevin Flores*

- `2009.11717v1` - [abs](http://arxiv.org/abs/2009.11717v1) - [pdf](http://arxiv.org/pdf/2009.11717v1)

> In this paper we present a methodology that uses convolutional neural networks (CNNs) for segmentation by iteratively growing predicted mask regions in each coordinate direction. The CNN is used to predict class probability scores in a small neighborhood of the center pixel in a tile of an image. We use a threshold on the CNN probability scores to determine whether pixels are added to the region and the iteration continues until no new pixels are added to the region. Our method is able to achieve high segmentation accuracy and preserve biologically realistic morphological features while leveraging small amounts of training data and maintaining computational efficiency. Using retinal blood vessel images from the DRIVE database we found that our method is more accurate than a fully convolutional semantic segmentation CNN for several evaluation metrics.

</details>

<details>

<summary>2020-09-23 18:11:47 - Generative Modelling of 3D in-silico Spongiosa with Controllable Micro-Structural Parameters</summary>

- *Emmanuel Iarussi, Felix Thomsen, Claudio Delrieux*

- `2009.11327v1` - [abs](http://arxiv.org/abs/2009.11327v1) - [pdf](http://arxiv.org/pdf/2009.11327v1)

> Research in vertebral bone micro-structure generally requires costly procedures to obtain physical scans of real bone with a specific pathology under study, since no methods are available yet to generate realistic bone structures in-silico. Here we propose to apply recent advances in generative adversarial networks (GANs) to develop such a method. We adapted style-transfer techniques, which have been largely used in other contexts, in order to transfer style between image pairs while preserving its informational content. In a first step, we trained a volumetric generative model in a progressive manner using a Wasserstein objective and gradient penalty (PWGAN-GP) to create patches of realistic bone structure in-silico. The training set contained 7660 purely spongeous bone samples from twelve human vertebrae (T12 or L1) with isotropic resolution of 164um and scanned with a high resolution peripheral quantitative CT (Scanco XCT). After training, we generated new samples with tailored micro-structure properties by optimizing a vector z in the learned latent space. To solve this optimization problem, we formulated a differentiable goal function that leads to valid samples while compromising the appearance (content) with target 3D properties (style). Properties of the learned latent space effectively matched the data distribution. Furthermore, we were able to simulate the resulting bone structure after deterioration or treatment effects of osteoporosis therapies based only on expected changes of micro-structural parameters. Our method allows to generate a virtually infinite number of patches of realistic bone micro-structure, and thereby likely serves for the development of bone-biomarkers and to simulate bone therapies in advance.

</details>

<details>

<summary>2020-09-24 01:29:08 - Logic2Text: High-Fidelity Natural Language Generation from Logical Forms</summary>

- *Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Yang Wang*

- `2004.14579v2` - [abs](http://arxiv.org/abs/2004.14579v2) - [pdf](http://arxiv.org/pdf/2004.14579v2)

> Previous works on Natural Language Generation (NLG) from structured data have primarily focused on surface-level descriptions of record sequences. However, for complex structured data, e.g., multi-row tables, it is often desirable for an NLG system to describe interesting facts from logical inferences across records. If only provided with the table, it is hard for existing models to produce controllable and high-fidelity logical generations. In this work, we formulate logical level NLG as generation from logical forms in order to obtain controllable, high-fidelity, and faithful generations. We present a new large-scale dataset, \textsc{Logic2Text}, with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which poses great challenges on the model's ability to understand the semantics. We experiment on (1) Fully-supervised training with the full datasets, and (2) Few-shot setting, provided with hundreds of paired examples; We compare several popular generation models and analyze their performances. We hope our dataset can encourage research towards building an advanced NLG system capable of natural, faithful, and human-like generation. The dataset and code are available at https://github.com/czyssrs/Logic2Text.

</details>

<details>

<summary>2020-09-24 02:13:21 - Towards Causal Explanation Detection with Pyramid Salient-Aware Network</summary>

- *Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao*

- `2009.10288v2` - [abs](http://arxiv.org/abs/2009.10288v2) - [pdf](http://arxiv.org/pdf/2009.10288v2)

> Causal explanation analysis (CEA) can assist us to understand the reasons behind daily events, which has been found very helpful for understanding the coherence of messages. In this paper, we focus on Causal Explanation Detection, an important subtask of causal explanation analysis, which determines whether a causal explanation exists in one message. We design a Pyramid Salient-Aware Network (PSAN) to detect causal explanations on messages. PSAN can assist in causal explanation detection via capturing the salient semantics of discourses contained in their keywords with a bottom graph-based word-level salient network. Furthermore, PSAN can modify the dominance of discourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. The experiments on the commonly used dataset of CEA shows that the PSAN outperforms the state-of-the-art method by 1.8% F1 value on the Causal Explanation Detection task.

</details>

<details>

<summary>2020-09-24 03:35:41 - Disentangled Neural Architecture Search</summary>

- *Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi*

- `2009.13266v1` - [abs](http://arxiv.org/abs/2009.13266v1) - [pdf](http://arxiv.org/pdf/2009.13266v1)

> Neural architecture search has shown its great potential in various areas recently. However, existing methods rely heavily on a black-box controller to search architectures, which suffers from the serious problem of lacking interpretability. In this paper, we propose disentangled neural architecture search (DNAS) which disentangles the hidden representation of the controller into semantically meaningful concepts, making the neural architecture search process interpretable. Based on systematical study, we discover the correlation between network architecture and its performance, and propose a dense-sampling strategy to conduct a targeted search in promising regions that may generate well-performing architectures. We show that: 1) DNAS successfully disentangles the architecture representations, including operation selection, skip connections, and number of layers. 2) Benefiting from interpretability, DNAS can find excellent architectures under different FLOPS restrictions flexibly. 3) Dense-sampling leads to neural architecture search with higher efficiency and better performance. On the NASBench-101 dataset, DNAS achieves state-of-the-art performance of 94.21% using less than 1/13 computational cost of baseline methods. On ImageNet dataset, DNAS discovers the competitive architectures that achieves 22.7% test error. our method provides a new perspective of understanding neural architecture search.

</details>

<details>

<summary>2020-09-24 06:04:56 - ThreatZoom: CVE2CWE using Hierarchical Neural Network</summary>

- *Ehsan Aghaei, Waseem Shadid, Ehab Al-Shaer*

- `2009.11501v1` - [abs](http://arxiv.org/abs/2009.11501v1) - [pdf](http://arxiv.org/pdf/2009.11501v1)

> The Common Vulnerabilities and Exposures (CVE) represent standard means for sharing publicly known information security vulnerabilities. One or more CVEs are grouped into the Common Weakness Enumeration (CWE) classes for the purpose of understanding the software or configuration flaws and potential impacts enabled by these vulnerabilities and identifying means to detect or prevent exploitation. As the CVE-to-CWE classification is mostly performed manually by domain experts, thousands of critical and new CVEs remain unclassified, yet they are unpatchable. This significantly limits the utility of CVEs and slows down proactive threat mitigation. This paper presents the first automatic tool to classify CVEs to CWEs. ThreatZoom uses a novel learning algorithm that employs an adaptive hierarchical neural network which adjusts its weights based on text analytic scores and classification errors. It automatically estimates the CWE classes corresponding to a CVE instance using both statistical and semantic features extracted from the description of a CVE. This tool is rigorously tested by various datasets provided by MITRE and the National Vulnerability Database (NVD). The accuracy of classifying CVE instances to their correct CWE classes are 92% (fine-grain) and 94% (coarse-grain) for NVD dataset, and 75% (fine-grain) and 90% (coarse-grain) for MITRE dataset, despite the small corpus.

</details>

<details>

<summary>2020-09-24 06:21:35 - Tag2Vec: Learning Tag Representations in Tag Networks</summary>

- *Junshan Wang, Zhicong Lu, Guojie Song, Yue Fan, Lun Du, Wei Lin*

- `1905.03041v2` - [abs](http://arxiv.org/abs/1905.03041v2) - [pdf](http://arxiv.org/pdf/1905.03041v2)

> Network embedding is a method to learn low-dimensional representation vectors for nodes in complex networks. In real networks, nodes may have multiple tags but existing methods ignore the abundant semantic and hierarchical information of tags. This information is useful to many network applications and usually very stable. In this paper, we propose a tag representation learning model, Tag2Vec, which mixes nodes and tags into a hybrid network. Firstly, for tag networks, we define semantic distance as the proximity between tags and design a novel strategy, parameterized random walk, to generate context with semantic and hierarchical information of tags adaptively. Then, we propose hyperbolic Skip-gram model to express the complex hierarchical structure better with lower output dimensions. We evaluate our model on the NBER U.S. patent dataset and WordNet dataset. The results show that our model can learn tag representations with rich semantic information and it outperforms other baselines.

</details>

<details>

<summary>2020-09-24 10:50:12 - Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation</summary>

- *Laura Mora Ballestar, Veronica Vilaplana*

- `2009.12188v1` - [abs](http://arxiv.org/abs/2009.12188v1) - [pdf](http://arxiv.org/pdf/2009.12188v1)

> Automation of brain tumors in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task. However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is specially critical in medical diagnosis. This work proposes a 3D encoder-decoder architecture, based on V-Net \cite{vnet} which is trained with patching techniques to reduce memory consumption and decrease the effect of unbalanced data. We also introduce voxel-wise uncertainty, both epistemic and aleatoric using test-time dropout and data-augmentation respectively. Uncertainty maps can provide extra information to expert neurologists, useful for detecting when the model is not confident on the provided segmentation.

</details>

<details>

<summary>2020-09-24 13:55:32 - Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph</summary>

- *Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang*

- `2009.11692v1` - [abs](http://arxiv.org/abs/2009.11692v1) - [pdf](http://arxiv.org/pdf/2009.11692v1)

> Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.

</details>

<details>

<summary>2020-09-24 16:53:40 - Attribute Propagation Network for Graph Zero-shot Learning</summary>

- *Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang*

- `2009.11816v1` - [abs](http://arxiv.org/abs/2009.11816v1) - [pdf](http://arxiv.org/pdf/2009.11816v1)

> The goal of zero-shot learning (ZSL) is to train a model to classify samples of classes that were not seen during training. To address this challenging task, most ZSL methods relate unseen test classes to seen(training) classes via a pre-defined set of attributes that can describe all classes in the same semantic space, so the knowledge learned on the training classes can be adapted to unseen classes. In this paper, we aim to optimize the attribute space for ZSL by training a propagation mechanism to refine the semantic attributes of each class based on its neighbors and related classes on a graph of classes. We show that the propagated attributes can produce classifiers for zero-shot classes with significantly improved performance in different ZSL settings. The graph of classes is usually free or very cheap to acquire such as WordNet or ImageNet classes. When the graph is not provided, given pre-defined semantic embeddings of the classes, we can learn a mechanism to generate the graph in an end-to-end manner along with the propagation mechanism. However, this graph-aided technique has not been well-explored in the literature. In this paper, we introduce the attribute propagation network (APNet), which is composed of 1) a graph propagation model generating attribute vector for each class and 2) a parameterized nearest neighbor (NN) classifier categorizing an image to the class with the nearest attribute vector to the image's embedding. For better generalization over unseen classes, different from previous methods, we adopt a meta-learning strategy to train the propagation mechanism and the similarity metric for the NN classifier on multiple sub-graphs, each associated with a classification task over a subset of training classes. In experiments with two zero-shot learning settings and five benchmark datasets, APNet achieves either compelling performance or new state-of-the-art results.

</details>

<details>

<summary>2020-09-24 21:56:02 - Toward a Thermodynamics of Meaning</summary>

- *Jonathan Scott Enderle*

- `2009.11963v1` - [abs](http://arxiv.org/abs/2009.11963v1) - [pdf](http://arxiv.org/pdf/2009.11963v1)

> As language models such as GPT-3 become increasingly successful at generating realistic text, questions about what purely text-based modeling can learn about the world have become more urgent. Is text purely syntactic, as skeptics argue? Or does it in fact contain some semantic information that a sufficiently sophisticated language model could use to learn about the world without any additional inputs? This paper describes a new model that suggests some qualified answers to those questions. By theorizing the relationship between text and the world it describes as an equilibrium relationship between a thermodynamic system and a much larger reservoir, this paper argues that even very simple language models do learn structural facts about the world, while also proposing relatively precise limits on the nature and extent of those facts. This perspective promises not only to answer questions about what language models actually learn, but also to explain the consistent and surprising success of cooccurrence prediction as a meaning-making strategy in AI.

</details>

<details>

<summary>2020-09-25 03:09:54 - Learning to Match Jobs with Resumes from Sparse Interaction Data using Multi-View Co-Teaching Network</summary>

- *Shuqing Bian, Xu Chen, Wayne Xin Zhao, Kun Zhou, Yupeng Hou, Yang Song, Tao Zhang, Ji-Rong Wen*

- `2009.13299v1` - [abs](http://arxiv.org/abs/2009.13299v1) - [pdf](http://arxiv.org/pdf/2009.13299v1)

> With the ever-increasing growth of online recruitment data, job-resume matching has become an important task to automatically match jobs with suitable resumes. This task is typically casted as a supervised text matching problem. Supervised learning is powerful when the labeled data is sufficient. However, on online recruitment platforms, job-resume interaction data is sparse and noisy, which affects the performance of job-resume match algorithms. To alleviate these problems, in this paper, we propose a novel multi-view co-teaching network from sparse interaction data for job-resume matching. Our network consists of two major components, namely text-based matching model and relation-based matching model. The two parts capture semantic compatibility in two different views, and complement each other. In order to address the challenges from sparse and noisy data, we design two specific strategies to combine the two components. First, two components share the learned parameters or representations, so that the original representations of each component can be enhanced. More importantly, we adopt a co-teaching mechanism to reduce the influence of noise in training data. The core idea is to let the two components help each other by selecting more reliable training instances. The two strategies focus on representation enhancement and data enhancement, respectively. Compared with pure text-based matching models, the proposed approach is able to learn better data representations from limited or even sparse interaction data, which is more resistible to noise in training data. Experiment results have demonstrated that our model is able to outperform state-of-the-art methods for job-resume matching.

</details>

<details>

<summary>2020-09-25 04:12:39 - ARMA Nets: Expanding Receptive Field for Dense Prediction</summary>

- *Jiahao Su, Shiqi Wang, Furong Huang*

- `2002.11609v2` - [abs](http://arxiv.org/abs/2002.11609v2) - [pdf](http://arxiv.org/pdf/2002.11609v2)

> Global information is essential for dense prediction problems, whose goal is to compute a discrete or continuous label for each pixel in the images. Traditional convolutional layers in neural networks, initially designed for image classification, are restrictive in these problems since the filter size limits their receptive fields. In this work, we propose to replace any traditional convolutional layer with an autoregressive moving-average (ARMA) layer, a novel module with an adjustable receptive field controlled by the learnable autoregressive coefficients. Compared with traditional convolutional layers, our ARMA layer enables explicit interconnections of the output neurons and learns its receptive field by adapting the autoregressive coefficients of the interconnections. ARMA layer is adjustable to different types of tasks: for tasks where global information is crucial, it is capable of learning relatively large autoregressive coefficients to allow for an output neuron's receptive field covering the entire input; for tasks where only local information is required, it can learn small or near zero autoregressive coefficients and automatically reduces to a traditional convolutional layer. We show both theoretically and empirically that the effective receptive field of networks with ARMA layers (named as ARMA networks) expands with larger autoregressive coefficients. We also provably solve the instability problem of learning and prediction in the ARMA layer through a re-parameterization mechanism. Additionally, we demonstrate that ARMA networks substantially improve their baselines on challenging dense prediction tasks including video prediction and semantic segmentation.

</details>

<details>

<summary>2020-09-25 08:34:46 - Vector symbolic architectures for context-free grammars</summary>

- *Peter beim Graben, Markus Huber, Werner Meyer, Ronald Römer, Matthias Wolff*

- `2003.05171v2` - [abs](http://arxiv.org/abs/2003.05171v2) - [pdf](http://arxiv.org/pdf/2003.05171v2)

> Background / introduction. Vector symbolic architectures (VSA) are a viable approach for the hyperdimensional representation of symbolic data, such as documents, syntactic structures, or semantic frames. Methods. We present a rigorous mathematical framework for the representation of phrase structure trees and parse trees of context-free grammars (CFG) in Fock space, i.e. infinite-dimensional Hilbert space as being used in quantum field theory. We define a novel normal form for CFG by means of term algebras. Using a recently developed software toolbox, called FockBox, we construct Fock space representations for the trees built up by a CFG left-corner (LC) parser. Results. We prove a universal representation theorem for CFG term algebras in Fock space and illustrate our findings through a low-dimensional principal component projection of the LC parser states. Conclusions. Our approach could leverage the development of VSA for explainable artificial intelligence (XAI) by means of hyperdimensional deep neural computation. It could be of significance for the improvement of cognitive user interfaces and other applications of VSA in machine learning.

</details>

<details>

<summary>2020-09-25 09:38:59 - Focus-Constrained Attention Mechanism for CVAE-based Response Generation</summary>

- *Zhi Cui, Yanran Li, Jiayi Zhang, Jianwei Cui, Chen Wei, Bin Wang*

- `2009.12102v1` - [abs](http://arxiv.org/abs/2009.12102v1) - [pdf](http://arxiv.org/pdf/2009.12102v1)

> To model diverse responses for a given post, one promising way is to introduce a latent variable into Seq2Seq models. The latent variable is supposed to capture the discourse-level information and encourage the informativeness of target responses. However, such discourse-level information is often too coarse for the decoder to be utilized. To tackle it, our idea is to transform the coarse-grained discourse-level information into fine-grained word-level information. Specifically, we firstly measure the semantic concentration of corresponding target response on the post words by introducing a fine-grained focus signal. Then, we propose a focus-constrained attention mechanism to take full advantage of focus in well aligning the input to the target response. The experimental results demonstrate that by exploiting the fine-grained signal, our model can generate more diverse and informative responses compared with several state-of-the-art models.

</details>

<details>

<summary>2020-09-25 09:45:26 - An enhanced Tree-LSTM architecture for sentence semantic modeling using typed dependencies</summary>

- *Jeena Kleenankandy, K. A. Abdul Nazeer*

- `2002.07775v2` - [abs](http://arxiv.org/abs/2002.07775v2) - [pdf](http://arxiv.org/pdf/2002.07775v2)

> Tree-based Long short term memory (LSTM) network has become state-of-the-art for modeling the meaning of language texts as they can effectively exploit the grammatical syntax and thereby non-linear dependencies among words of the sentence. However, most of these models cannot recognize the difference in meaning caused by a change in semantic roles of words or phrases because they do not acknowledge the type of grammatical relations, also known as typed dependencies, in sentence structure. This paper proposes an enhanced LSTM architecture, called relation gated LSTM, which can model the relationship between two inputs of a sequence using a control input. We also introduce a Tree-LSTM model called Typed Dependency Tree-LSTM that uses the sentence dependency parse structure as well as the dependency type to embed sentence meaning into a dense vector. The proposed model outperformed its type-unaware counterpart in two typical NLP tasks - Semantic Relatedness Scoring and Sentiment Analysis, in a lesser number of training epochs. The results were comparable or competitive with other state-of-the-art models. Qualitative analysis showed that changes in the voice of sentences had little effect on the model's predicted scores, while changes in nominal (noun) words had a more significant impact. The model recognized subtle semantic relationships in sentence pairs. The magnitudes of learned typed dependencies embeddings were also in agreement with human intuitions. The research findings imply the significance of grammatical relations in sentence modeling. The proposed models would serve as a base for future researches in this direction.

</details>

<details>

<summary>2020-09-25 13:38:29 - Explainable Link Prediction for Emerging Entities in Knowledge Graphs</summary>

- *Rajarshi Bhowmik, Gerard de Melo*

- `2005.00637v2` - [abs](http://arxiv.org/abs/2005.00637v2) - [pdf](http://arxiv.org/pdf/2005.00637v2)

> Despite their large-scale coverage, cross-domain knowledge graphs invariably suffer from inherent incompleteness and sparsity. Link prediction can alleviate this by inferring a target entity, given a source entity and a query relation. Recent embedding-based approaches operate in an uninterpretable latent semantic vector space of entities and relations, while path-based approaches operate in the symbolic space, making the inference process explainable. However, these approaches typically consider static snapshots of the knowledge graphs, severely restricting their applicability for evolving knowledge graphs with newly emerging entities. To overcome this issue, we propose an inductive representation learning framework that is able to learn representations of previously unseen entities. Our method finds reasoning paths between source and target entities, thereby making the link prediction for unseen entities interpretable and providing support evidence for the inferred link.

</details>

<details>

<summary>2020-09-25 15:51:40 - Certified Defenses for Adversarial Patches</summary>

- *Ping-Yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, Tom Goldstein*

- `2003.06693v2` - [abs](http://arxiv.org/abs/2003.06693v2) - [pdf](http://arxiv.org/pdf/2003.06693v2)

> Adversarial patch attacks are among one of the most practical threat models against real-world computer vision systems. This paper studies certified and empirical defenses against patch attacks. We begin with a set of experiments showing that most existing defenses, which work by pre-processing input images to mitigate adversarial patches, are easily broken by simple white-box adversaries. Motivated by this finding, we propose the first certified defense against patch attacks, and propose faster methods for its training. Furthermore, we experiment with different patch shapes for testing, obtaining surprisingly good robustness transfer across shapes, and present preliminary results on certified defense against sparse attacks. Our complete implementation can be found on: https://github.com/Ping-C/certifiedpatchdefense.

</details>

<details>

<summary>2020-09-25 17:20:25 - Managing Machine Learning Workflow Components</summary>

- *Marcio Moreno, Vítor Lourenço, Sandro Rama Fiorini, Polyana Costa, Rafael Brandão, Daniel Civitarese, Renato Cerqueira*

- `1912.05665v2` - [abs](http://arxiv.org/abs/1912.05665v2) - [pdf](http://arxiv.org/pdf/1912.05665v2)

> Machine Learning Workflows (MLWfs) have become essential and a disruptive approach in problem-solving over several industries. However, the development process of MLWfs may be complicated, hard to achieve, time-consuming, and error-prone. To handle this problem, in this paper, we introduce machine learning workflow management (MLWfM) as a technique to aid the development and reuse of MLWfs and their components through three aspects: representation, execution, and creation. More precisely, we discuss our approach to structure the MLWfs' components and their metadata to aid retrieval and reuse of components in new MLWfs. Also, we consider the execution of these components within a tool. The hybrid knowledge representation, called Hyperknowledge, frames our methodology, supporting the three MLWfM's aspects. To validate our approach, we show a practical use case in the Oil & Gas industry.

</details>

<details>

<summary>2020-09-25 20:49:07 - XTE: Explainable Text Entailment</summary>

- *Vivian S. Silva, André Freitas, Siegfried Handschuh*

- `2009.12431v1` - [abs](http://arxiv.org/abs/2009.12431v1) - [pdf](http://arxiv.org/pdf/2009.12431v1)

> Text entailment, the task of determining whether a piece of text logically follows from another piece of text, is a key component in NLP, providing input for many semantic applications such as question answering, text summarization, information extraction, and machine translation, among others. Entailment scenarios can range from a simple syntactic variation to more complex semantic relationships between pieces of text, but most approaches try a one-size-fits-all solution that usually favors some scenario to the detriment of another. Furthermore, for entailments requiring world knowledge, most systems still work as a "black box", providing a yes/no answer that does not explain the underlying reasoning process. In this work, we introduce XTE - Explainable Text Entailment - a novel composite approach for recognizing text entailment which analyzes the entailment pair to decide whether it must be resolved syntactically or semantically. Also, if a semantic matching is involved, we make the answer interpretable, using external knowledge bases composed of structured lexical definitions to generate natural language justifications that explain the semantic relationship holding between the pieces of text. Besides outperforming well-established entailment algorithms, our composite approach gives an important step towards Explainable AI, allowing the inference model interpretation, making the semantic reasoning process explicit and understandable.

</details>

<details>

<summary>2020-09-26 11:12:06 - DT-Net: A novel network based on multi-directional integrated convolution and threshold convolution</summary>

- *Hongfeng You, Long Yu, Shengwei Tian, Xiang Ma, Yan Xing, Xiaojie Ma*

- `2009.12569v1` - [abs](http://arxiv.org/abs/2009.12569v1) - [pdf](http://arxiv.org/pdf/2009.12569v1)

> Since medical image data sets contain few samples and singular features, lesions are viewed as highly similar to other tissues. The traditional neural network has a limited ability to learn features. Even if a host of feature maps is expanded to obtain more semantic information, the accuracy of segmenting the final medical image is slightly improved, and the features are excessively redundant. To solve the above problems, in this paper, we propose a novel end-to-end semantic segmentation algorithm, DT-Net, and use two new convolution strategies to better achieve end-to-end semantic segmentation of medical images. 1. In the feature mining and feature fusion stage, we construct a multi-directional integrated convolution (MDIC). The core idea is to use the multi-scale convolution to enhance the local multi-directional feature maps to generate enhanced feature maps and to mine the generated features that contain more semantics without increasing the number of feature maps. 2. We also aim to further excavate and retain more meaningful deep features reduce a host of noise features in the training process. Therefore, we propose a convolution thresholding strategy. The central idea is to set a threshold to eliminate a large number of redundant features and reduce computational complexity. Through the two strategies proposed above, the algorithm proposed in this paper produces state-of-the-art results on two public medical image datasets. We prove in detail that our proposed strategy plays an important role in feature mining and eliminating redundant features. Compared with the existing semantic segmentation algorithms, our proposed algorithm has better robustness.

</details>

<details>

<summary>2020-09-26 12:56:08 - Unsupervised Transfer of Semantic Role Models from Verbal to Nominal Domain</summary>

- *Yanpeng Zhao, Ivan Titov*

- `2005.00278v2` - [abs](http://arxiv.org/abs/2005.00278v2) - [pdf](http://arxiv.org/pdf/2005.00278v2)

> Semantic role labeling (SRL) is an NLP task involving the assignment of predicate arguments to types, called semantic roles. Though research on SRL has primarily focused on verbal predicates and many resources available for SRL provide annotations only for verbs, semantic relations are often triggered by other linguistic constructions, e.g., nominalizations. In this work, we investigate a transfer scenario where we assume role-annotated data for the source verbal domain but only unlabeled data for the target nominal domain. Our key assumption, enabling the transfer between the two domains, is that selectional preferences of a role (i.e., preferences or constraints on the admissible arguments) do not strongly depend on whether the relation is triggered by a verb or a noun. For example, the same set of arguments can fill the Acquirer role for the verbal predicate `acquire' and its nominal form `acquisition'. We approach the transfer task from the variational autoencoding perspective. The labeler serves as an encoder (predicting role labels given a sentence), whereas selectional preferences are captured in the decoder component (generating arguments for the predicting roles). Nominal roles are not labeled in the training data, and the learning objective instead pushes the labeler to assign roles predictive of the arguments. Sharing the decoder parameters across the domains encourages consistency between labels predicted for both domains and facilitates the transfer. The method substantially outperforms baselines, such as unsupervised and `direct transfer' methods, on the English CoNLL-2009 dataset.

</details>

<details>

<summary>2020-09-26 14:32:13 - Deep Learning-based Four-region Lung Segmentation in Chest Radiography for COVID-19 Diagnosis</summary>

- *Young-Gon Kim, Kyungsang Kim, Dufan Wu, Hui Ren, Won Young Tak, Soo Young Park, Yu Rim Lee, Min Kyu Kang, Jung Gil Park, Byung Seok Kim, Woo Jin Chung, Mannudeep K. Kalra, Quanzheng Li*

- `2009.12610v1` - [abs](http://arxiv.org/abs/2009.12610v1) - [pdf](http://arxiv.org/pdf/2009.12610v1)

> Purpose. Imaging plays an important role in assessing severity of COVID 19 pneumonia. However, semantic interpretation of chest radiography (CXR) findings does not include quantitative description of radiographic opacities. Most current AI assisted CXR image analysis framework do not quantify for regional variations of disease. To address these, we proposed a four region lung segmentation method to assist accurate quantification of COVID 19 pneumonia. Methods. A segmentation model to separate left and right lung is firstly applied, and then a carina and left hilum detection network is used, which are the clinical landmarks to separate the upper and lower lungs. To improve the segmentation performance of COVID 19 images, ensemble strategy incorporating five models is exploited. Using each region, we evaluated the clinical relevance of the proposed method with the Radiographic Assessment of the Quality of Lung Edema (RALE). Results. The proposed ensemble strategy showed dice score of 0.900, which is significantly higher than conventional methods (0.854 0.889). Mean intensities of segmented four regions indicate positive correlation to the extent and density scores of pulmonary opacities under the RALE framework. Conclusion. A deep learning based model in CXR can accurately segment and quantify regional distribution of pulmonary opacities in patients with COVID 19 pneumonia.

</details>

<details>

<summary>2020-09-26 14:56:57 - ARPA: Armenian Paraphrase Detection Corpus and Models</summary>

- *Arthur Malajyan, Karen Avetisyan, Tsolak Ghukasyan*

- `2009.12615v1` - [abs](http://arxiv.org/abs/2009.12615v1) - [pdf](http://arxiv.org/pdf/2009.12615v1)

> In this work, we employ a semi-automatic method based on back translation to generate a sentential paraphrase corpus for the Armenian language. The initial collection of sentences is translated from Armenian to English and back twice, resulting in pairs of lexically distant but semantically similar sentences. The generated paraphrases are then manually reviewed and annotated. Using the method train and test datasets are created, containing 2360 paraphrases in total. In addition, the datasets are used to train and evaluate BERTbased models for detecting paraphrase in Armenian, achieving results comparable to the state-of-the-art of other languages.

</details>

<details>

<summary>2020-09-27 00:58:32 - Differentially Private Adversarial Robustness Through Randomized Perturbations</summary>

- *Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu, Nathanael Teissier*

- `2009.12718v1` - [abs](http://arxiv.org/abs/2009.12718v1) - [pdf](http://arxiv.org/pdf/2009.12718v1)

> Deep Neural Networks, despite their great success in diverse domains, are provably sensitive to small perturbations on correctly classified examples and lead to erroneous predictions. Recently, it was proposed that this behavior can be combatted by optimizing the worst case loss function over all possible substitutions of training examples. However, this can be prone to weighing unlikely substitutions higher, limiting the accuracy gain. In this paper, we study adversarial robustness through randomized perturbations, which has two immediate advantages: (1) by ensuring that substitution likelihood is weighted by the proximity to the original word, we circumvent optimizing the worst case guarantees and achieve performance gains; and (2) the calibrated randomness imparts differentially-private model training, which additionally improves robustness against adversarial attacks on the model outputs. Our approach uses a novel density-based mechanism based on truncated Gumbel noise, which ensures training on substitutions of both rare and dense words in the vocabulary while maintaining semantic similarity for model robustness.

</details>

<details>

<summary>2020-09-27 04:07:11 - CodeBLEU: a Method for Automatic Evaluation of Code Synthesis</summary>

- *Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, Shuai Ma*

- `2009.10297v2` - [abs](http://arxiv.org/abs/2009.10297v2) - [pdf](http://arxiv.org/pdf/2009.10297v2)

> Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate the natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that our proposed CodeBLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.

</details>

<details>

<summary>2020-09-27 07:44:02 - An Attention-Guided Deep Regression Model for Landmark Detection in Cephalograms</summary>

- *Zhusi Zhong, Jie Li, Zhenxi Zhang, Zhicheng Jiao, Xinbo Gao*

- `1906.07549v3` - [abs](http://arxiv.org/abs/1906.07549v3) - [pdf](http://arxiv.org/pdf/1906.07549v3)

> Cephalometric tracing method is usually used in orthodontic diagnosis and treatment planning. In this paper, we propose a deep learning based framework to automatically detect anatomical landmarks in cephalometric X-ray images. We train the deep encoder-decoder for landmark detection, and combine global landmark configuration with local high-resolution feature responses. The proposed frame-work is based on 2-stage u-net, regressing the multi-channel heatmaps for land-mark detection. In this framework, we embed attention mechanism with global stage heatmaps, guiding the local stage inferring, to regress the local heatmap patches in a high resolution. Besides, the Expansive Exploration strategy improves robustness while inferring, expanding the searching scope without increasing model complexity. We have evaluated our framework in the most widely-used public dataset of landmark detection in cephalometric X-ray images. With less computation and manually tuning, our framework achieves state-of-the-art results.

</details>

<details>

<summary>2020-09-28 00:19:42 - Uncertain Linear Logic via Fibring of Probabilistic and Fuzzy Logic</summary>

- *Ben Goertzel*

- `2009.12990v1` - [abs](http://arxiv.org/abs/2009.12990v1) - [pdf](http://arxiv.org/pdf/2009.12990v1)

> Beginning with a simple semantics for propositions, based on counting observations, it is shown that probabilistic and fuzzy logic correspond to two different heuristic assumptions regarding the combination of propositions whose evidence bases are not currently available. These two different heuristic assumptions lead to two different sets of formulas for propagating quantitative truth values through lattice operations. It is shown that these two sets of formulas provide a natural grounding for the multiplicative and additive operator-sets in linear logic. The standard rules of linear logic then emerge as consequences of the underlying semantics. The concept of linear logic as a ``logic of resources" is manifested here via the principle of ``conservation of evidence" -- the restrictions to weakening and contraction in linear logic serve to avoid double-counting of evidence (beyond any double-counting incurred via use of heuristic truth value functions).

</details>

<details>

<summary>2020-09-28 04:32:23 - Gotta Catch 'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks</summary>

- *Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, Ben Y. Zhao*

- `1904.08554v6` - [abs](http://arxiv.org/abs/1904.08554v6) - [pdf](http://arxiv.org/pdf/1904.08554v6)

> Deep neural networks (DNN) are known to be vulnerable to adversarial attacks. Numerous efforts either try to patch weaknesses in trained models, or try to make it difficult or costly to compute adversarial examples that exploit them. In our work, we explore a new "honeypot" approach to protect DNN models. We intentionally inject trapdoors, honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples. Attackers' optimization algorithms gravitate towards trapdoors, leading them to produce attacks similar to trapdoors in the feature space. Our defense then identifies attacks by comparing neuron activation signatures of inputs to those of trapdoors. In this paper, we introduce trapdoors and describe an implementation of a trapdoor-enabled defense. First, we analytically prove that trapdoors shape the computation of adversarial attacks so that attack inputs will have feature representations very similar to those of trapdoors. Second, we experimentally show that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks (PGD, optimization-based CW, Elastic Net, BPDA), with negligible impact on normal classification. These results generalize across classification domains, including image, facial, and traffic-sign recognition. We also present significant results measuring trapdoors' robustness against customized adaptive attacks (countermeasures).

</details>

<details>

<summary>2020-09-28 09:29:49 - Incomplete Utterance Rewriting as Semantic Segmentation</summary>

- *Qian Liu, Bei Chen, Jian-Guang Lou, Bin Zhou, Dongmei Zhang*

- `2009.13166v1` - [abs](http://arxiv.org/abs/2009.13166v1) - [pdf](http://arxiv.org/pdf/2009.13166v1)

> Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.

</details>

<details>

<summary>2020-09-28 10:25:24 - A Comprehensive Study of Automatic Program Repair on the QuixBugs Benchmark</summary>

- *He Ye, Matias Martinez, Thomas Durieux, Martin Monperrus*

- `1805.03454v4` - [abs](http://arxiv.org/abs/1805.03454v4) - [pdf](http://arxiv.org/pdf/1805.03454v4)

> Automatic program repair papers tend to repeatedly use the same benchmarks. This poses a threat to the external validity of the findings of the program repair research community. In this paper, we perform an empirical study of automatic repair on a benchmark of bugs called QuixBugs, which has been little studied. In this paper, 1) We report on the characteristics of QuixBugs; 2) We study the effectiveness of 10 program repair tools on it; 3) We apply three patch correctness assessment techniques to comprehensively study the presence of overfitting patches in QuixBugs. Our key results are: 1) 16/40 buggy programs in QuixBugs can be repaired with at least a test suite adequate patch; 2) A total of 338 plausible patches are generated on the QuixBugs by the considered tools, and 53.3% of them are overfitting patches according to our manual assessment; 3) The three automated patch correctness assessment techniques, RGT_Evosuite, RGT_InputSampling and GT_Invariants, achieve an accuracy of 98.2%, 80.8% and 58.3% in overfitting detection, respectively. To our knowledge, this is the largest empirical study of automatic repair on QuixBugs, combining both quantitative and qualitative insights. All our empirical results are publicly available on GitHub in order to facilitate future research on automatic program repair.

</details>

<details>

<summary>2020-09-28 12:47:59 - Graph-based Multi-hop Reasoning for Long Text Generation</summary>

- *Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu Sun*

- `2009.13282v1` - [abs](http://arxiv.org/abs/2009.13282v1) - [pdf](http://arxiv.org/pdf/2009.13282v1)

> Long text generation is an important but challenging task.The main problem lies in learning sentence-level semantic dependencies which traditional generative models often suffer from. To address this problem, we propose a Multi-hop Reasoning Generation (MRG) approach that incorporates multi-hop reasoning over a knowledge graph to learn semantic dependencies among sentences. MRG consists of twoparts, a graph-based multi-hop reasoning module and a path-aware sentence realization module. The reasoning module is responsible for searching skeleton paths from a knowledge graph to imitate the imagination process in the human writing for semantic transfer. Based on the inferred paths, the sentence realization module then generates a complete sentence. Unlike previous black-box models, MRG explicitly infers the skeleton path, which provides explanatory views tounderstand how the proposed model works. We conduct experiments on three representative tasks, including story generation, review generation, and product description generation. Automatic and manual evaluation show that our proposed method can generate more informative and coherentlong text than strong baselines, such as pre-trained models(e.g. GPT-2) and knowledge-enhanced models.

</details>

<details>

<summary>2020-09-28 19:26:52 - Specifying and Computing Causes for Query Answers in Databases via Database Repairs and Repair Programs</summary>

- *Leopoldo Bertossi*

- `1712.01001v7` - [abs](http://arxiv.org/abs/1712.01001v7) - [pdf](http://arxiv.org/pdf/1712.01001v7)

> A correspondence between database tuples as causes for query answers in databases and tuple-based repairs of inconsistent databases with respect to denial constraints has already been established. In this work, answer-set programs that specify repairs of databases are used as a basis for solving computational and reasoning problems about causes. Here, causes are also introduced at the attribute level by appealing to a both null-based and attribute-based repair semantics. The corresponding repair programs are presented, and they are used as a basis for computation and reasoning about attribute-level causes. They are extended to deal with the case of causality under integrity constraints.

</details>

<details>

<summary>2020-09-28 21:22:22 - MPG-Net: Multi-Prediction Guided Network for Segmentation of Retinal Layers in OCT Images</summary>

- *Zeyu Fu, Yang Sun, Xiangyu Zhang, Scott Stainton, Shaun Barney, Jeffry Hogg, William Innes, Satnam Dlay*

- `2009.13634v1` - [abs](http://arxiv.org/abs/2009.13634v1) - [pdf](http://arxiv.org/pdf/2009.13634v1)

> Optical coherence tomography (OCT) is a commonly-used method of extracting high resolution retinal information. Moreover there is an increasing demand for the automated retinal layer segmentation which facilitates the retinal disease diagnosis. In this paper, we propose a novel multiprediction guided attention network (MPG-Net) for automated retinal layer segmentation in OCT images. The proposed method consists of two major steps to strengthen the discriminative power of a U-shape Fully convolutional network (FCN) for reliable automated segmentation. Firstly, the feature refinement module which adaptively re-weights the feature channels is exploited in the encoder to capture more informative features and discard information in irrelevant regions. Furthermore, we propose a multi-prediction guided attention mechanism which provides pixel-wise semantic prediction guidance to better recover the segmentation mask at each scale. This mechanism which transforms the deep supervision to supervised attention is able to guide feature aggregation with more semantic information between intermediate layers. Experiments on the publicly available Duke OCT dataset confirm the effectiveness of the proposed method as well as an improved performance over other state-of-the-art approaches.

</details>

<details>

<summary>2020-09-28 22:08:00 - Conversational Semantic Parsing</summary>

- *Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick, Mike Haeger, Haoran Li, Yashar Mehdad, Ves Stoyanov, Anuj Kumar, Mike Lewis, Sonal Gupta*

- `2009.13655v1` - [abs](http://arxiv.org/abs/2009.13655v1) - [pdf](http://arxiv.org/pdf/2009.13655v1)

> The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.

</details>

<details>

<summary>2020-09-28 23:31:44 - A Dictionary Approach to Domain-Invariant Learning in Deep Networks</summary>

- *Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu*

- `1909.11285v2` - [abs](http://arxiv.org/abs/1909.11285v2) - [pdf](http://arxiv.org/pdf/1909.11285v2)

> In this paper, we consider domain-invariant deep learning by explicitly modeling domain shifts with only a small amount of domain-specific parameters in a Convolutional Neural Network (CNN). By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of dictionary atoms, we show for the first time, both empirically and theoretically, that domain shifts can be effectively handled by decomposing a convolutional layer into a domain-specific atom layer and a domain-shared coefficient layer, while both remain convolutional. An input channel will now first convolve spatially only with each respective domain-specific dictionary atom to "absorb" domain variations, and then output channels are linearly combined using common decomposition coefficients trained to promote shared semantics across domains. We use toy examples, rigorous analysis, and real-world examples with diverse datasets and architectures, to show the proposed plug-in framework's effectiveness in cross and joint domain performance and domain adaptation. With the proposed architecture, we need only a small set of dictionary atoms to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred.

</details>

<details>

<summary>2020-09-29 02:28:34 - Pixel-Semantic Revise of Position Learning A One-Stage Object Detector with A Shared Encoder-Decoder</summary>

- *Qian Li, Nan Guo, Xiaochun Ye, Dongrui Fan, Zhimin Tang*

- `2001.01057v2` - [abs](http://arxiv.org/abs/2001.01057v2) - [pdf](http://arxiv.org/pdf/2001.01057v2)

> Recently, many methods have been proposed for object detection. They cannot detect objects by semantic features, adaptively. In this work, according to channel and spatial attention mechanisms, we mainly analyze that different methods detect objects adaptively. Some state-of-the-art detectors combine different feature pyramids with many mechanisms to enhance multi-level semantic information. However, they require more cost. This work addresses that by an anchor-free detector with shared encoder-decoder with attention mechanism, extracting shared features. We consider features of different levels from backbone (e.g., ResNet-50) as the basis features. Then, we feed the features into a simple module, followed by a detector header to detect objects. Meantime, we use the semantic features to revise geometric locations, and the detector is a pixel-semantic revising of position. More importantly, this work analyzes the impact of different pooling strategies (e.g., mean, maximum or minimum) on multi-scale objects, and finds the minimum pooling improve detection performance on small objects better. Compared with state-of-the-art MNC based on ResNet-101 for the standard MSCOCO 2014 baseline, our method improves detection AP of 3.8%.

</details>

<details>

<summary>2020-09-29 05:38:13 - Feature Robust Optimal Transport for High-dimensional Data</summary>

- *Mathis Petrovich, Chao Liang, Ryoma Sato, Yanbin Liu, Yao-Hung Hubert Tsai, Linchao Zhu, Yi Yang, Ruslan Salakhutdinov, Makoto Yamada*

- `2005.12123v4` - [abs](http://arxiv.org/abs/2005.12123v4) - [pdf](http://arxiv.org/pdf/2005.12123v4)

> Optimal transport is a machine learning problem with applications including distribution comparison, feature selection, and generative adversarial networks. In this paper, we propose feature-robust optimal transport (FROT) for high-dimensional data, which solves high-dimensional OT problems using feature selection to avoid the curse of dimensionality. Specifically, we find a transport plan with discriminative features. To this end, we formulate the FROT problem as a min--max optimization problem. We then propose a convex formulation of the FROT problem and solve it using a Frank--Wolfe-based optimization algorithm, whereby the subproblem can be efficiently solved using the Sinkhorn algorithm. Since FROT finds the transport plan from selected features, it is robust to noise features. To show the effectiveness of FROT, we propose using the FROT algorithm for the layer selection problem in deep neural networks for semantic correspondence. By conducting synthetic and benchmark experiments, we demonstrate that the proposed method can find a strong correspondence by determining important layers. We show that the FROT algorithm achieves state-of-the-art performance in real-world semantic correspondence datasets.

</details>

<details>

<summary>2020-09-29 07:32:17 - SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery</summary>

- *Jiaming Shen, Wenda Qiu, Jingbo Shang, Michelle Vanni, Xiang Ren, Jiawei Han*

- `2009.13827v1` - [abs](http://arxiv.org/abs/2009.13827v1) - [pdf](http://arxiv.org/pdf/2009.13827v1)

> Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have similar likelihoods of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities' infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. To facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks.

</details>

<details>

<summary>2020-09-29 12:41:27 - Neural Topic Modeling with Cycle-Consistent Adversarial Training</summary>

- *Xuemeng Hu, Rui Wang, Deyu Zhou, Yuxuan Xiong*

- `2009.13971v1` - [abs](http://arxiv.org/abs/2009.13971v1) - [pdf](http://arxiv.org/pdf/2009.13971v1)

> Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.

</details>

<details>

<summary>2020-09-29 12:42:09 - NXNSAttack: Recursive DNS Inefficiencies and Vulnerabilities</summary>

- *Yehuda Afek, Anat Bremler-Barr, Lior Shafir*

- `2005.09107v2` - [abs](http://arxiv.org/abs/2005.09107v2) - [pdf](http://arxiv.org/pdf/2005.09107v2)

> This paper exposes a new vulnerability and introduces a corresponding attack, the NoneXistent Name Server Attack (NXNSAttack), that disrupts and may paralyze the DNS system, making it difficult or impossible for Internet users to access websites, web e-mail, online video chats, or any other online resource. The NXNSAttack generates a storm of packets between DNS resolvers and DNS authoritative name servers. The storm is produced by the response of resolvers to unrestricted referral response messages of authoritative name servers. The attack is significantly more destructive than NXDomain attacks (e.g., the Mirai attack): i) It reaches an amplification factor of more than 1620x on the number of packets exchanged by the recursive resolver. ii) In addition to the negative cache, the attack also saturates the 'NS' section of the resolver caches. To mitigate the attack impact, we propose an enhancement to the recursive resolver algorithm, MaxFetch(k), that prevents unnecessary proactive fetches. We implemented the MaxFetch(1) mitigation enhancement on a BIND resolver and tested it on real-world DNS query datasets. Our results show that MaxFetch(1) degrades neither the recursive resolver throughput nor its latency. Following the discovery of the attack, a responsible disclosure procedure was carried out, and several DNS vendors and public providers have issued a CVE and patched their systems.

</details>

<details>

<summary>2020-09-29 13:11:40 - The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning</summary>

- *Nuobei Shi, Qin Zeng, Raymond Lee*

- `2009.13984v1` - [abs](http://arxiv.org/abs/2009.13984v1) - [pdf](http://arxiv.org/pdf/2009.13984v1)

> In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by GPT-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of XAI (Explainable Artificial Intelligence) to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer learning as back-end to interpret the responses by ontology graph.

</details>

<details>

<summary>2020-09-29 16:03:13 - A Survey on Semantic Parsing from the perspective of Compositionality</summary>

- *Pawan Kumar, Srikanta Bedathur*

- `2009.14116v1` - [abs](http://arxiv.org/abs/2009.14116v1) - [pdf](http://arxiv.org/pdf/2009.14116v1)

> Different from previous surveys in semantic parsing (Kamath and Das, 2018) and knowledge base question answering(KBQA)(Chakraborty et al., 2019; Zhu et al., 2019; Hoffner et al., 2017) we try to takes a different perspective on the study of semantic parsing. Specifically, we will focus on (a)meaning composition from syntactical structure(Partee, 1975), and (b) the ability of semantic parsers to handle lexical variation given the context of a knowledge base (KB). In the following section after an introduction of the field of semantic parsing and its uses in KBQA, we will describe meaning representation using grammar formalism CCG (Steedman, 1996). We will discuss semantic composition using formal languages in Section 2. In section 3 we will consider systems that uses formal languages e.g. $\lambda$-calculus (Steedman, 1996), $\lambda$-DCS (Liang, 2013). Section 4 and 5 consider semantic parser using structured-language for logical form. Section 6 is on different benchmark datasets ComplexQuestions (Bao et al.,2016) and GraphQuestions (Su et al., 2016) that can be used to evaluate semantic parser on their ability to answer complex questions that are highly compositional in nature.

</details>

<details>

<summary>2020-09-29 17:31:34 - Contextualized Spoken Word Representations from Convolutional Autoencoders</summary>

- *Prakamya Mishra, Pranav Mathur*

- `2007.02880v2` - [abs](http://arxiv.org/abs/2007.02880v2) - [pdf](http://arxiv.org/pdf/2007.02880v2)

> A lot of work has been done to build text-based language models for performing different NLP tasks, but not much research has been done in the case of audio-based language models. This paper proposes a Convolutional Autoencoder based neural architecture to model syntactically and semantically adequate contextualized representations of varying length spoken words. The use of such representations can not only lead to great advances in the audio-based NLP tasks but can also curtail the loss of information like tone, expression, accent, etc while converting speech to text to perform these tasks. The performance of the proposed model is validated by (1) examining the generated vector space, and (2) evaluating its performance on three benchmark datasets for measuring word similarities, against existing widely used text-based language models that are trained on the transcriptions. The proposed model was able to demonstrate its robustness when compared to the other two language-based models.

</details>

<details>

<summary>2020-09-29 18:48:19 - On the Language Neutrality of Pre-trained Multilingual Representations</summary>

- *Jindřich Libovický, Rudolf Rosa, Alexander Fraser*

- `2004.05160v4` - [abs](http://arxiv.org/abs/2004.05160v4) - [pdf](http://arxiv.org/pdf/2004.05160v4)

> Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data.

</details>

<details>

<summary>2020-09-29 20:13:58 - Improving Device Directedness Classification of Utterances with Semantic Lexical Features</summary>

- *Kellen Gillespie, Ioannis C. Konstantakopoulos, Xingzhi Guo, Vishal Thanvantri Vasudevan, Abhinav Sethy*

- `2010.01949v1` - [abs](http://arxiv.org/abs/2010.01949v1) - [pdf](http://arxiv.org/pdf/2010.01949v1)

> User interactions with personal assistants like Alexa, Google Home and Siri are typically initiated by a wake term or wakeword. Several personal assistants feature "follow-up" modes that allow users to make additional interactions without the need of a wakeword. For the system to only respond when appropriate, and to ignore speech not intended for it, utterances must be classified as device-directed or non-device-directed. State-of-the-art systems have largely used acoustic features for this task, while others have used only lexical features or have added LM-based lexical features. We propose a directedness classifier that combines semantic lexical features with a lightweight acoustic feature and show it is effective in classifying directedness. The mixed-domain lexical and acoustic feature model is able to achieve 14% relative reduction of EER over a state-of-the-art acoustic-only baseline model. Finally, we successfully apply transfer learning and semi-supervised learning to the model to improve accuracy even further.

</details>

<details>

<summary>2020-09-29 20:56:57 - Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</summary>

- *Saurabh Kulshreshtha, José Luis Redondo-García, Ching-Yun Chang*

- `2009.14304v1` - [abs](http://arxiv.org/abs/2009.14304v1) - [pdf](http://arxiv.org/pdf/2009.14304v1)

> Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work proposes several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs. dictionary-based supervision and rotational vs. fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3% F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.

</details>

<details>

<summary>2020-09-30 01:54:13 - End-to-End Spoken Language Understanding Without Full Transcripts</summary>

- *Hong-Kwang J. Kuo, Zoltán Tüske, Samuel Thomas, Yinghui Huang, Kartik Audhkhasi, Brian Kingsbury, Gakuto Kurata, Zvi Kons, Ron Hoory, Luis Lastras*

- `2009.14386v1` - [abs](http://arxiv.org/abs/2009.14386v1) - [pdf](http://arxiv.org/pdf/2009.14386v1)

> An essential component of spoken language understanding (SLU) is slot filling: representing the meaning of a spoken utterance using semantic entity labels. In this paper, we develop end-to-end (E2E) spoken language understanding systems that directly convert speech input to semantic entities and investigate if these E2E SLU models can be trained solely on semantic entity annotations without word-for-word transcripts. Training such models is very useful as they can drastically reduce the cost of data collection. We created two types of such speech-to-entities models, a CTC model and an attention-based encoder-decoder model, by adapting models trained originally for speech recognition. Given that our experiments involve speech input, these systems need to recognize both the entity label and words representing the entity value correctly. For our speech-to-entities experiments on the ATIS corpus, both the CTC and attention models showed impressive ability to skip non-entity words: there was little degradation when trained on just entities versus full transcripts. We also explored the scenario where the entities are in an order not necessarily related to spoken order in the utterance. With its ability to do re-ordering, the attention model did remarkably well, achieving only about 2% degradation in speech-to-bag-of-entities F1 score.

</details>

<details>

<summary>2020-09-30 08:21:08 - Temporal Context Aggregation for Video Retrieval with Contrastive Learning</summary>

- *Jie Shao, Xin Wen, Bingchen Zhao, Xiangyang Xue*

- `2008.01334v2` - [abs](http://arxiv.org/abs/2008.01334v2) - [pdf](http://arxiv.org/pdf/2008.01334v2)

> The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates long-range temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features.

</details>

<details>

<summary>2020-09-30 09:22:23 - Visual Semantic Multimedia Event Model for Complex Event Detection in Video Streams</summary>

- *Piyush Yadav, Edward Curry*

- `2009.14525v1` - [abs](http://arxiv.org/abs/2009.14525v1) - [pdf](http://arxiv.org/pdf/2009.14525v1)

> Multimedia data is highly expressive and has traditionally been very difficult for a machine to interpret. Middleware systems such as complex event processing (CEP) mine patterns from data streams and send notifications to users in a timely fashion. Presently, CEP systems have inherent limitations to process multimedia streams due to its data complexity and the lack of an underlying structured data model. In this work, we present a visual event specification method to enable complex multimedia event processing by creating a semantic knowledge representation derived from low-level media streams. The method enables the detection of high-level semantic concepts from the media streams using an ensemble of pattern detection capabilities. The semantic model is aligned with a multimedia CEP engine deep learning models to give flexibility to end-users to build rules using spatiotemporal event calculus. This enhances CEP capability to detect patterns from media streams and bridge the semantic gap between highly expressive knowledge-centric user queries to the low-level features of the multi-media data. We have built a small traffic event ontology prototype to validate the approach and performance. The paper contribution is threefold: i) we present a knowledge graph representation for multimedia streams, ii) a hierarchical event network to detect visual patterns from media streams and iii) define complex pattern rules for complex multimedia event reasoning using event calculus

</details>

<details>

<summary>2020-09-30 09:26:58 - Scaling Distributed Machine Learning with In-Network Aggregation</summary>

- *Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, Peter Richtárik*

- `1903.06701v2` - [abs](http://arxiv.org/abs/1903.06701v2) - [pdf](http://arxiv.org/pdf/1903.06701v2)

> Training machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide an efficient solution that speeds up training by up to 5.5$\times$ for a number of real-world benchmark models.

</details>

<details>

<summary>2020-09-30 13:59:59 - RCM: Requirement Capturing Model for Automated Requirements Formalisation</summary>

- *Aya Zaki-Ismail, Mohamed Osama, Mohamed Abdelrazek, John Grundy, Amani Ibrahim*

- `2009.14683v1` - [abs](http://arxiv.org/abs/2009.14683v1) - [pdf](http://arxiv.org/pdf/2009.14683v1)

> Most existing automated requirements formalisation techniques require system engineers to (re)write their requirements using a set of predefined requirement templates with a fixed structure and known semantics to simplify the formalisation process. However, these techniques require understanding and memorising requirement templates, which are usually fixed format, limit requirements captured, and do not allow capture of more diverse requirements. To address these limitations, we need a reference model that captures key requirement details regardless of their structure, format or order. Then, using NLP techniques we can transform textual requirements into the reference model. Finally, using a suite of transformation rules we can then convert these requirements into formal notations. In this paper, we introduce the first and key step in this process, a Requirement Capturing Model (RCM) - as a reference model - to model the key elements of a system requirement regardless of their format, or order. We evaluated the robustness of the RCM model compared to 15 existing requirements representation approaches and a benchmark of 162 requirements. Our evaluation shows that RCM breakdowns support a wider range of requirements formats compared to the existing approaches. We also implemented a suite of transformation rules that transforms RCM-based requirements into temporal logic(s). In the future, we will develop NLP-based RCM extraction technique to provide end-to-end solution.

</details>

<details>

<summary>2020-09-30 15:46:44 - SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020)</summary>

- *Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, Çağrı Çöltekin*

- `2006.07235v2` - [abs](http://arxiv.org/abs/2006.07235v2) - [pdf](http://arxiv.org/pdf/2006.07235v2)

> We present the results and main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The task involves three subtasks corresponding to the hierarchical taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The task featured five languages: English, Arabic, Danish, Greek, and Turkish for Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020 was one of the most popular tasks at SemEval-2020 attracting a large number of participants across all subtasks and also across all languages. A total of 528 teams signed up to participate in the task, 145 teams submitted systems during the evaluation period, and 70 submitted system description papers.

</details>

<details>

<summary>2020-09-30 20:00:11 - The Importance of Balanced Data Sets: Analyzing a Vehicle Trajectory Prediction Model based on Neural Networks and Distributed Representations</summary>

- *Florian Mirus, Terrence C. Stewart, Jorg Conradt*

- `2010.00084v1` - [abs](http://arxiv.org/abs/2010.00084v1) - [pdf](http://arxiv.org/pdf/2010.00084v1)

> Predicting future behavior of other traffic participants is an essential task that needs to be solved by automated vehicles and human drivers alike to achieve safe and situationaware driving. Modern approaches to vehicles trajectory prediction typically rely on data-driven models like neural networks, in particular LSTMs (Long Short-Term Memorys), achieving promising results. However, the question of optimal composition of the underlying training data has received less attention. In this paper, we expand on previous work on vehicle trajectory prediction based on neural network models employing distributed representations to encode automotive scenes in a semantic vector substrate. We analyze the influence of variations in the training data on the performance of our prediction models. Thereby, we show that the models employing our semantic vector representation outperform the numerical model when trained on an adequate data set and thereby, that the composition of training data in vehicle trajectory prediction is crucial for successful training. We conduct our analysis on challenging real-world driving data.

</details>

<details>

<summary>2020-09-30 22:19:40 - Self-Guided Multiple Instance Learning for Weakly Supervised Disease Classification and Localization in Chest Radiographs</summary>

- *Constantin Seibold, Jens Kleesiek, Heinz-Peter Schlemmer, Rainer Stiefelhagen*

- `2010.00127v1` - [abs](http://arxiv.org/abs/2010.00127v1) - [pdf](http://arxiv.org/pdf/2010.00127v1)

> The lack of fine-grained annotations hinders the deployment of automated diagnosis systems, which require human-interpretable justification for their decision process. In this paper, we address the problem of weakly supervised identification and localization of abnormalities in chest radiographs. To that end, we introduce a novel loss function for training convolutional neural networks increasing the \emph{localization confidence} and assisting the overall \emph{disease identification}. The loss leverages both image- and patch-level predictions to generate auxiliary supervision. Rather than forming strictly binary from the predictions as done in previous loss formulations, we create targets in a more customized manner, which allows the loss to account for possible misclassification. We show that the supervision provided within the proposed learning scheme leads to better performance and more precise predictions on prevalent datasets for multiple-instance learning as well as on the NIH~ChestX-Ray14 benchmark for disease recognition than previously used losses.

</details>

<details>

<summary>2020-09-30 22:56:11 - Semantic Segmentation With Multi Scale Spatial Attention For Self Driving Cars</summary>

- *Abhinav Sagar, RajKumar Soundrapandiyan*

- `2007.12685v3` - [abs](http://arxiv.org/abs/2007.12685v3) - [pdf](http://arxiv.org/pdf/2007.12685v3)

> In this paper, we present a novel neural network using multi scale feature fusion at various scales for accurate and efficient semantic image segmentation. We used ResNet based feature extractor, dilated convolutional layers in downsampling part, atrous convolutional layers in the upsampling part and used concat operation to merge them. A new attention module is proposed to encode more contextual information and enhance the receptive field of the network. We present an in depth theoretical analysis of our network with training and optimization details. Our network was trained and tested on the Camvid dataset and Cityscapes dataset using mean accuracy per class and Intersection Over Union (IOU) as the evaluation metrics. Our model outperforms previous state of the art methods on semantic segmentation achieving mean IOU value of 74.12 while running at >100 FPS.

</details>

<details>

<summary>2020-09-30 23:54:38 - Learning from Mistakes: Combining Ontologies via Self-Training for Dialogue Generation</summary>

- *Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur, Marilyn Walker*

- `2010.00150v1` - [abs](http://arxiv.org/abs/2010.00150v1) - [pdf](http://arxiv.org/pdf/2010.00150v1)

> Natural language generators (NLGs) for task-oriented dialogue typically take a meaning representation (MR) as input. They are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor-intensive and time-consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies. Here we explore, for the first time, whether it is possible to train an NLG for a new larger ontology using existing training sets for the restaurant domain, where each set is based on a different ontology. We create a new, larger combined ontology, and then train an NLG to produce utterances covering it. For example, if one dataset has attributes for family-friendly and rating information, and the other has attributes for decor and service, our aim is an NLG for the combined ontology that can produce utterances that realize values for family-friendly, rating, decor and service. Initial experiments with a baseline neural sequence-to-sequence model show that this task is surprisingly challenging. We then develop a novel self-training method that identifies (errorful) model outputs, automatically constructs a corrected MR input to form a new (MR, utterance) training pair, and then repeatedly adds these new instances back into the training data. We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4% improvement over the baseline model. We also report a human qualitative evaluation of the final model showing that it achieves high naturalness, semantic coherence and grammaticality

</details>


## 2020-10

<details>

<summary>2020-10-01 01:15:28 - Compiling ONNX Neural Network Models Using MLIR</summary>

- *Tian Jin, Gheorghe-Teodor Bercea, Tung D. Le, Tong Chen, Gong Su, Haruki Imai, Yasushi Negishi, Anh Leu, Kevin O'Brien, Kiyokuni Kawachiya, Alexandre E. Eichenberger*

- `2008.08272v2` - [abs](http://arxiv.org/abs/2008.08272v2) - [pdf](http://arxiv.org/pdf/2008.08272v2)

> Deep neural network models are becoming increasingly popular and have been used in various tasks such as computer vision, speech recognition, and natural language processing. Machine learning models are commonly trained in a resource-rich environment and then deployed in a distinct environment such as high availability machines or edge devices. To assist the portability of models, the open-source community has proposed the Open Neural Network Exchange (ONNX) standard. In this paper, we present a high-level, preliminary report on our onnx-mlir compiler, which generates code for the inference of deep neural network models described in the ONNX format. Onnx-mlir is an open-source compiler implemented using the Multi-Level Intermediate Representation (MLIR) infrastructure recently integrated in the LLVM project. Onnx-mlir relies on the MLIR concept of dialects to implement its functionality. We propose here two new dialects: (1) an ONNX specific dialect that encodes the ONNX standard semantics, and (2) a loop-based dialect to provide for a common lowering point for all ONNX dialect operations. Each intermediate representation facilitates its own characteristic set of graph-level and loop-based optimizations respectively. We illustrate our approach by following several models through the proposed representations and we include some early optimization work and performance results.

</details>

<details>

<summary>2020-10-01 05:27:51 - RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble</summary>

- *Michael Bendersky, Honglei Zhuang, Ji Ma, Shuguang Han, Keith Hall, Ryan McDonald*

- `2010.00200v1` - [abs](http://arxiv.org/abs/2010.00200v1) - [pdf](http://arxiv.org/pdf/2010.00200v1)

> In this paper, we report the results of our participation in the TREC-COVID challenge. To meet the challenge of building a search engine for rapidly evolving biomedical collection, we propose a simple yet effective weighted hierarchical rank fusion approach, that ensembles together 102 runs from (a) lexical and semantic retrieval systems, (b) pre-trained and fine-tuned BERT rankers, and (c) relevance feedback runs. Our ablation studies demonstrate the contributions of each of these systems to the overall ensemble. The submitted ensemble runs achieved state-of-the-art performance in rounds 4 and 5 of the TREC-COVID challenge.

</details>

<details>

<summary>2020-10-01 10:44:31 - Open-Set Hypothesis Transfer with Semantic Consistency</summary>

- *Zeyu Feng, Chang Xu, Dacheng Tao*

- `2010.00292v1` - [abs](http://arxiv.org/abs/2010.00292v1) - [pdf](http://arxiv.org/pdf/2010.00292v1)

> Unsupervised open-set domain adaptation (UODA) is a realistic problem where unlabeled target data contain unknown classes. Prior methods rely on the coexistence of both source and target domain data to perform domain alignment, which greatly limits their applications when source domain data are restricted due to privacy concerns. This paper addresses the challenging hypothesis transfer setting for UODA, where data from source domain are no longer available during adaptation on target domain. We introduce a method that focuses on the semantic consistency under transformation of target data, which is rarely appreciated by previous domain adaptation methods. Specifically, our model first discovers confident predictions and performs classification with pseudo-labels. Then we enforce the model to output consistent and definite predictions on semantically similar inputs. As a result, unlabeled data can be classified into discriminative classes coincided with either source classes or unknown classes. Experimental results show that our model outperforms state-of-the-art methods on UODA benchmarks.

</details>

<details>

<summary>2020-10-01 17:15:20 - Elaboration Tolerant Representation of Markov Decision Process via Decision-Theoretic Extension of Probabilistic Action Language pBC+</summary>

- *Yi Wang, Joohyung Lee*

- `1904.00512v2` - [abs](http://arxiv.org/abs/1904.00512v2) - [pdf](http://arxiv.org/pdf/1904.00512v2)

> We extend probabilistic action language pBC+ with the notion of utility as in decision theory. The semantics of the extended pBC+ can be defined as a shorthand notation for a decision-theoretic extension of the probabilistic answer set programming language LPMLN. Alternatively, the semantics of pBC+ can also be defined in terms of Markov Decision Process (MDP), which in turn allows for representing MDP in a succinct and elaboration tolerant way as well as to leverage an MDP solver to compute pBC+. The idea led to the design of the system pbcplus2mdp, which can find an optimal policy of a pBC+ action description using an MDP solver. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2020-10-01 19:41:11 - Follow the Leader: Documents on the Leading Edge of Semantic Change Get More Citations</summary>

- *Sandeep Soni, Kristina Lerman, Jacob Eisenstein*

- `1909.04189v2` - [abs](http://arxiv.org/abs/1909.04189v2) - [pdf](http://arxiv.org/pdf/1909.04189v2)

> Diachronic word embeddings -- vector representations of words over time -- offer remarkable insights into the evolution of language and provide a tool for quantifying sociocultural change from text documents. Prior work has used such embeddings to identify shifts in the meaning of individual words. However, simply knowing that a word has changed in meaning is insufficient to identify the instances of word usage that convey the historical or the newer meaning. In this paper, we link diachronic word embeddings to documents, by situating those documents as leaders or laggards with respect to ongoing semantic changes. Specifically, we propose a novel method to quantify the degree of semantic progressiveness in each word usage, and then show how these usages can be aggregated to obtain scores for each document. We analyze two large collections of documents, representing legal opinions and scientific articles. Documents that are scored as semantically progressive receive a larger number of citations, indicating that they are especially influential. Our work thus provides a new technique for identifying lexical semantic leaders and demonstrates a new link between progressive use of language and influence in a citation network.

</details>

<details>

<summary>2020-10-01 20:48:37 - Beyond The Text: Analysis of Privacy Statements through Syntactic and Semantic Role Labeling</summary>

- *Yan Shvartzshnaider, Ananth Balashankar, Vikas Patidar, Thomas Wies, Lakshminarayanan Subramanian*

- `2010.00678v1` - [abs](http://arxiv.org/abs/2010.00678v1) - [pdf](http://arxiv.org/pdf/2010.00678v1)

> This paper formulates a new task of extracting privacy parameters from a privacy policy, through the lens of Contextual Integrity, an established social theory framework for reasoning about privacy norms. Privacy policies, written by lawyers, are lengthy and often comprise incomplete and vague statements. In this paper, we show that traditional NLP tasks, including the recently proposed Question-Answering based solutions, are insufficient to address the privacy parameter extraction problem and provide poor precision and recall. We describe 4 different types of conventional methods that can be partially adapted to address the parameter extraction task with varying degrees of success: Hidden Markov Models, BERT fine-tuned models, Dependency Type Parsing (DP) and Semantic Role Labeling (SRL). Based on a detailed evaluation across 36 real-world privacy policies of major enterprises, we demonstrate that a solution combining syntactic DP coupled with type-specific SRL tasks provides the highest accuracy for retrieving contextual privacy parameters from privacy statements. We also observe that incorporating domain-specific knowledge is critical to achieving high precision and recall, thus inspiring new NLP research to address this important problem in the privacy domain.

</details>

<details>

<summary>2020-10-01 21:47:58 - Automatic and Efficient Variability-Aware Lifting of Functional Programs</summary>

- *Ramy Shahin, Marsha Chechik*

- `2010.00697v1` - [abs](http://arxiv.org/abs/2010.00697v1) - [pdf](http://arxiv.org/pdf/2010.00697v1)

> A software analysis is a computer program that takes some representation of a software product as input and produces some useful information about that product as output. A software product line encompasses \emph{many} software product variants, and thus existing analyses can be applied to each of the product variations individually, but not to the entire product line as a whole. Enumerating all product variants and analyzing them one by one is usually intractable due to the combinatorial explosion of the number of product variants with respect to product line features. Several software analyses (e.g., type checkers, model checkers, data flow analyses) have been redesigned/re-implemented to support variability. This usually requires a lot of time and effort, and the variability-aware version of the analysis might have new errors/bugs that do not exist in the original one.   Given an analysis program written in a functional language based on PCF, in this paper we present two approaches to transforming (lifting) it into a semantically equivalent variability-aware analysis. A light-weight approach (referred to as \emph{shallow lifting}) wraps the analysis program into a variability-aware version, exploring all combinations of its input arguments. Deep lifting, on the other hand, is a program rewriting mechanism where the syntactic constructs of the input program are rewritten into their variability-aware counterparts. Compositionally this results in an efficient program semantically equivalent to the input program, modulo variability. We present the correctness criteria for functional program lifting, together with correctness proof sketches of our program transformations. We evaluate our approach on a set of program analyses applied to the BusyBox C-language product line.

</details>

<details>

<summary>2020-10-02 02:54:06 - Deep Learning for Earth Image Segmentation based on Imperfect Polyline Labels with Annotation Errors</summary>

- *Zhe Jiang, Marcus Stephen Kirby, Wenchong He, Arpan Man Sainju*

- `2010.00757v1` - [abs](http://arxiv.org/abs/2010.00757v1) - [pdf](http://arxiv.org/pdf/2010.00757v1)

> In recent years, deep learning techniques (e.g., U-Net, DeepLab) have achieved tremendous success in image segmentation. The performance of these models heavily relies on high-quality ground truth segment labels. Unfortunately, in many real-world problems, ground truth segment labels often have geometric annotation errors due to manual annotation mistakes, GPS errors, or visually interpreting background imagery at a coarse resolution. Such location errors will significantly impact the training performance of existing deep learning algorithms. Existing research on label errors either models ground truth errors in label semantics (assuming label locations to be correct) or models label location errors with simple square patch shifting. These methods cannot fully incorporate the geometric properties of label location errors. To fill the gap, this paper proposes a generic learning framework based on the EM algorithm to update deep learning model parameters and infer hidden true label locations simultaneously. Evaluations on a real-world hydrological dataset in the streamline refinement application show that the proposed framework outperforms baseline methods in classification accuracy (reducing the number of false positives by 67% and reducing the number of false negatives by 55%).

</details>

<details>

<summary>2020-10-02 03:08:04 - BERT-ATTACK: Adversarial Attack Against BERT Using BERT</summary>

- *Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu*

- `2004.09984v3` - [abs](http://arxiv.org/abs/2004.09984v3) - [pdf](http://arxiv.org/pdf/2004.09984v3)

> Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.

</details>

<details>

<summary>2020-10-02 03:15:03 - Enriching Word Embeddings with Temporal and Spatial Information</summary>

- *Hongyu Gong, Suma Bhat, Pramod Viswanath*

- `2010.00761v1` - [abs](http://arxiv.org/abs/2010.00761v1) - [pdf](http://arxiv.org/pdf/2010.00761v1)

> The meaning of a word is closely linked to sociocultural factors that can change over time and location, resulting in corresponding meaning changes. Taking a global view of words and their meanings in a widely used language, such as English, may require us to capture more refined semantics for use in time-specific or location-aware situations, such as the study of cultural trends or language use. However, popular vector representations for words do not adequately include temporal or spatial information. In this work, we present a model for learning word representation conditioned on time and location. In addition to capturing meaning changes over time and location, we require that the resulting word embeddings retain salient semantic and geometric properties. We train our model on time- and location-stamped corpora, and show using both quantitative and qualitative evaluations that it can capture semantics across time and locations. We note that our model compares favorably with the state-of-the-art for time-specific embedding, and serves as a new benchmark for location-specific embeddings.

</details>

<details>

<summary>2020-10-02 04:22:54 - Learning Structured Latent Factors from Dependent Data:A Generative Model Framework from Information-Theoretic Perspective</summary>

- *Ruixiang Zhang, Masanori Koyama, Katsuhiko Ishiguro*

- `2007.10623v2` - [abs](http://arxiv.org/abs/2007.10623v2) - [pdf](http://arxiv.org/pdf/2007.10623v2)

> Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck to enforce it. Our model provides a principled approach to learn a set of semantically meaningful latent factors that reflect various types of desired structures like capturing correlation or encoding invariance, while also offering the flexibility to automatically estimate the dependency structure from data. We show that our framework unifies many existing generative models and can be applied to a variety of tasks including multi-modal data modeling, algorithmic fairness, and invariant risk minimization.

</details>

<details>

<summary>2020-10-02 06:46:02 - Public Announcement Logic in HOL</summary>

- *Sebastian Reiche, Christoph Benzmüller*

- `2010.00810v1` - [abs](http://arxiv.org/abs/2010.00810v1) - [pdf](http://arxiv.org/pdf/2010.00810v1)

> A shallow semantical embedding for public announcement logic with relativized common knowledge is presented. This embedding enables the first-time automation of this logic with off-the-shelf theorem provers for classical higher-order logic. It is demonstrated (i) how meta-theoretical studies can be automated this way, and (ii) how non-trivial reasoning in the target logic (public announcement logic), required e.g. to obtain a convincing encoding and automation of the wise men puzzle, can be realized. Key to the presented semantical embedding -- in contrast, e.g., to related work on the semantical embedding of normal modal logics -- is that evaluation domains are modeled explicitly and treated as additional parameter in the encodings of the constituents of the embedded target logic, while they were previously implicitly shared between meta logic and target logic.

</details>

<details>

<summary>2020-10-02 08:38:40 - SST-BERT at SemEval-2020 Task 1: Semantic Shift Tracing by Clustering in BERT-based Embedding Spaces</summary>

- *K Vani, Sandra Mitrovic, Alessandro Antonucci, Fabio Rinaldi*

- `2010.00857v1` - [abs](http://arxiv.org/abs/2010.00857v1) - [pdf](http://arxiv.org/pdf/2010.00857v1)

> Lexical semantic change detection (also known as semantic shift tracing) is a task of identifying words that have changed their meaning over time. Unsupervised semantic shift tracing, focal point of SemEval2020, is particularly challenging. Given the unsupervised setup, in this work, we propose to identify clusters among different occurrences of each target word, considering these as representatives of different word meanings. As such, disagreements in obtained clusters naturally allow to quantify the level of semantic shift per each target word in four target languages. To leverage this idea, clustering is performed on contextualized (BERT-based) embeddings of word occurrences. The obtained results show that our approach performs well both measured separately (per language) and overall, where we surpass all provided SemEval baselines.

</details>

<details>

<summary>2020-10-02 08:42:56 - Building Large Lexicalized Ontologies from Text: a Use Case in Automatic Indexing of Biotechnology Patents</summary>

- *Claire Nédellec, Wiktoria Golik, Sophie Aubin, Robert Bossy*

- `2010.00860v1` - [abs](http://arxiv.org/abs/2010.00860v1) - [pdf](http://arxiv.org/pdf/2010.00860v1)

> This paper presents a tool, TyDI, and methods experimented in the building of a termino-ontology, i.e. a lexicalized ontology aimed at fine-grained indexation for semantic search applications. TyDI provides facilities for knowledge engineers and domain experts to efficiently collaborate to validate, organize and conceptualize corpus extracted terms. A use case on biotechnology patent search demonstrates TyDI's potential.

</details>

<details>

<summary>2020-10-02 08:48:07 - DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution</summary>

- *Marcel C. Bühler, Andrés Romero, Radu Timofte*

- `2004.04433v3` - [abs](http://arxiv.org/abs/2004.04433v3) - [pdf](http://arxiv.org/pdf/2004.04433v3)

> Super-resolution (SR) is by definition ill-posed. There are infinitely many plausible high-resolution variants for a given low-resolution natural image. Most of the current literature aims at a single deterministic solution of either high reconstruction fidelity or photo-realistic perceptual quality. In this work, we propose an explorative facial super-resolution framework, DeepSEE, for Deep disentangled Semantic Explorative Extreme super-resolution. To the best of our knowledge, DeepSEE is the first method to leverage semantic maps for explorative super-resolution. In particular, it provides control of the semantic regions, their disentangled appearance and it allows a broad range of image manipulations. We validate DeepSEE on faces, for up to 32x magnification and exploration of the space of super-resolution. Our code and models are available at: https://mcbuehler.github.io/DeepSEE/

</details>

<details>

<summary>2020-10-02 10:31:02 - Point-of-Interest Type Inference from Social Media Text</summary>

- *Danae Sánchez Villegas, Daniel Preoţiuc-Pietro, Nikolaos Aletras*

- `2009.14734v2` - [abs](http://arxiv.org/abs/2009.14734v2) - [pdf](http://arxiv.org/pdf/2009.14734v2)

> Physical places help shape how we perceive the experiences we have there. For the first time, we study the relationship between social media text and the type of the place from where it was posted, whether a park, restaurant, or someplace else. To facilitate this, we introduce a novel data set of $\sim$200,000 English tweets published from 2,761 different points-of-interest in the U.S., enriched with place type information. We train classifiers to predict the type of the location a tweet was sent from that reach a macro F1 of 43.67 across eight classes and uncover the linguistic markers associated with each type of place. The ability to predict semantic place information from a tweet has applications in recommendation systems, personalization services and cultural geography.

</details>

<details>

<summary>2020-10-02 11:18:57 - GPU acceleration of CaNS for massively-parallel direct numerical simulations of canonical fluid flows</summary>

- *Pedro Costa, Everett Phillips, Luca Brandt, Massimiliano Fatica*

- `2001.05234v3` - [abs](http://arxiv.org/abs/2001.05234v3) - [pdf](http://arxiv.org/pdf/2001.05234v3)

> This work presents the GPU acceleration of the open-source code CaNS for very fast massively-parallel simulations of canonical fluid flows. The distinct feature of the many-CPU Navier-Stokes solver in CaNS is its fast direct solver for the second-order finite-difference Poisson equation, based on the method of eigenfunction expansions. The solver implements all the boundary conditions valid for this type of problems in a unified framework. Here, we extend the solver for GPU-accelerated clusters using CUDA Fortran. The porting makes extensive use of CUF kernels and has been greatly simplified by the unified memory feature of CUDA Fortran, which handles the data migration between host (CPU) and device (GPU) without defining new arrays in the source code. The overall implementation has been validated against benchmark data for turbulent channel flow and its performance assessed on a NVIDIA DGX-2 system (16 Tesla V100 32Gb, connected with NVLink via NVSwitch). The wall-clock time per time step of the GPU-accelerated implementation is impressively small when compared to its CPU implementation on state-of-the-art many-CPU clusters, as long as the domain partitioning is sufficiently small that the data resides mostly on the GPUs. The implementation has been made freely available and open-source under the terms of an MIT license.

</details>

<details>

<summary>2020-10-02 11:38:36 - EVMPatch: Timely and Automated Patching of Ethereum Smart Contracts</summary>

- *Michael Rodler, Wenting Li, Ghassan O. Karame, Lucas Davi*

- `2010.00341v2` - [abs](http://arxiv.org/abs/2010.00341v2) - [pdf](http://arxiv.org/pdf/2010.00341v2)

> Recent attacks exploiting errors in smart contract code had devastating consequences thereby questioning the benefits of this technology. It is currently highly challenging to fix errors and deploy a patched contract in time. Instant patching is especially important since smart contracts are always online due to the distributed nature of blockchain systems. They also manage considerable amounts of assets, which are at risk and often beyond recovery after an attack. Existing solutions to upgrade smart contracts depend on manual and error-prone processes. This paper presents a framework, called EVMPatch, to instantly and automatically patch faulty smart contracts. EVMPatch features a bytecode rewriting engine for the popular Ethereum blockchain, and transparently/automatically rewrites common off-the-shelf contracts to upgradable contracts. The proof-of-concept implementation of EVMPatch automatically hardens smart contracts that are vulnerable to integer over/underflows and access control errors, but can be easily extended to cover more bug classes. Our extensive evaluation on 14,000 real-world (vulnerable) contracts demonstrate that our approach successfully blocks attack transactions launched on these contracts, while keeping the intended functionality of the contract intact. We perform a study with experienced software developers, showing that EVMPatch is practical, and reduces the time for converting a given Solidity smart contract to an upgradable contract by 97.6 %, while ensuring functional equivalence to the original contract.

</details>

<details>

<summary>2020-10-02 12:09:09 - Generative View Synthesis: From Single-view Semantics to Novel-view Images</summary>

- *Tewodros Habtegebrial, Varun Jampani, Orazio Gallo, Didier Stricker*

- `2008.09106v2` - [abs](http://arxiv.org/abs/2008.09106v2) - [pdf](http://arxiv.org/pdf/2008.09106v2)

> Content creation, central to applications such as virtual reality, can be a tedious and time-consuming. Recent image synthesis methods simplify this task by offering tools to generate new views from as little as a single input image, or by converting a semantic map into a photorealistic image. We propose to push the envelope further, and introduce Generative View Synthesis (GVS), which can synthesize multiple photorealistic views of a scene given a single semantic map. We show that the sequential application of existing techniques, e.g., semantics-to-image translation followed by monocular view synthesis, fail at capturing the scene's structure. In contrast, we solve the semantics-to-image translation in concert with the estimation of the 3D layout of the scene, thus producing geometrically consistent novel views that preserve semantic structures. We first lift the input 2D semantic map onto a 3D layered representation of the scene in feature space, thereby preserving the semantic labels of 3D geometric structures. We then project the layered features onto the target views to generate the final novel-view images. We verify the strengths of our method and compare it with several advanced baselines on three different datasets. Our approach also allows for style manipulation and image editing operations, such as the addition or removal of objects, with simple manipulations of the input style images and semantic maps respectively. Visit the project page at https://gvsnet.github.io.

</details>

<details>

<summary>2020-10-02 12:23:16 - Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation</summary>

- *Giuseppe Russo, Nora Hollenstein, Claudiu Musat, Ce Zhang*

- `2004.14983v2` - [abs](http://arxiv.org/abs/2004.14983v2) - [pdf](http://arxiv.org/pdf/2004.14983v2)

> We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining adversarial learning with a context-aware loss and a cyclical word dropout routine. We demonstrate the value of the individual model components in an ablation study. The scalability of our approach is ensured through a single discriminator, independently of the number of attributes. We show high quality, diversity and attribute control in the generated sentences through a series of automatic and human assessments. As the main application of our work, we test the potential of this new NLG model in a data augmentation scenario. In a downstream NLP task, the sentences generated by our CGA model show significant improvements over a strong baseline, and a classification performance often comparable to adding same amount of additional real data.

</details>

<details>

<summary>2020-10-02 22:06:31 - Human brain activity for machine attention</summary>

- *Lukas Muttenthaler, Nora Hollenstein, Maria Barrett*

- `2006.05113v2` - [abs](http://arxiv.org/abs/2006.05113v2) - [pdf](http://arxiv.org/pdf/2006.05113v2)

> Cognitively inspired NLP leverages human-derived data to teach machines about language processing mechanisms. Recently, neural networks have been augmented with behavioral data to solve a range of NLP tasks spanning syntax and semantics. We are the first to exploit neuroscientific data, namely electroencephalography (EEG), to inform a neural attention model about language processing of the human brain. The challenge in working with EEG data is that features are exceptionally rich and need extensive pre-processing to isolate signals specific to text processing. We devise a method for finding such EEG features to supervise machine attention through combining theoretically motivated cropping with random forest tree splits. After this dimensionality reduction, the pre-processed EEG features are capable of distinguishing two reading tasks retrieved from a publicly available EEG corpus. We apply these features to regularise attention on relation classification and show that EEG is more informative than strong baselines. This improvement depends on both the cognitive load of the task and the EEG frequency domain. Hence, informing neural attention models with EEG signals is beneficial but requires further investigation to understand which dimensions are the most useful across NLP tasks.

</details>

<details>

<summary>2020-10-02 23:16:17 - The Hetero-functional Graph Theory Toolbox</summary>

- *Dakota Thompson, Prabhat Hegde, Wester C. H. Schoonenberg, Inas Khayal, Amro M. Farid*

- `2005.10006v2` - [abs](http://arxiv.org/abs/2005.10006v2) - [pdf](http://arxiv.org/pdf/2005.10006v2)

> In the 20th century, newly invented technical artifacts were connected to form large-scale complex engineering systems. Furthermore, the interactions found within these networked systems has grown in both degree as well as heterogeneity. Consequently, these already complex engineering systems have converged in what is now called systems-of-systems. The analysis, design, planning, and operation of these engineering systems from a holistic perspective has necessitated ever-more sophisticated modeling techniques. Despite significant advancements in model-based systems engineering and network science, these seemingly disparate fields have experienced similar limitations in addressing the complexity of engineering systems. Hetero-Functional Graph Theory (HFGT) has emerged as a means to address some of these limitations. This paper serves as a user guide to a recently developed Hetero-functional Graph Theory Toolbox which facilitates the computation of HFGT mathematical models. It is written in the MATLAB language and has been tested with v9.6 (R2019a). It is openly available on GitHub together with a sample input file for straightforward re-use. The paper details the syntax and semantics of the input file, the principal data structure of the toolbox, and the functions used to construct and populate this data structure. The toolbox has been fully validated against several peer-review HFGT publications.

</details>

<details>

<summary>2020-10-03 00:25:39 - Exploring Contextualized Neural Language Models for Temporal Dependency Parsing</summary>

- *Hayley Ross, Jonathon Cai, Bonan Min*

- `2004.14577v2` - [abs](http://arxiv.org/abs/2004.14577v2) - [pdf](http://arxiv.org/pdf/2004.14577v2)

> Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.

</details>

<details>

<summary>2020-10-03 06:14:09 - Randomly Weighted, Untrained Neural Tensor Networks Achieve Greater Relational Expressiveness</summary>

- *Jinyung Hong, Theodore P. Pavlic*

- `2006.12392v2` - [abs](http://arxiv.org/abs/2006.12392v2) - [pdf](http://arxiv.org/pdf/2006.12392v2)

> Neural Tensor Networks (NTNs), which are structured to encode the degree of relationship among pairs of entities, are used in Logic Tensor Networks (LTNs) to facilitate Statistical Relational Learning (SRL) in first-order logic. In this paper, we propose Randomly Weighted Tensor Networks (RWTNs), which incorporate randomly drawn, untrained tensors into an NTN encoder network with a trained decoder network. We show that RWTNs meet or surpass the performance of traditionally trained LTNs for Semantic Image Interpretation (SII) tasks that have been used as a representative example of how LTNs utilize reasoning over first-order logic to exceed the performance of solely data-driven methods. We demonstrate that RWTNs outperform LTNs for the detection of the relevant part-of relations between objects, and we show that RWTNs can achieve similar performance as LTNs for object classification while using fewer parameters for learning. Furthermore, we demonstrate that because the randomized weights do not depend on the data, several decoder networks can share a single NTN, giving RWTNs a unique economy of spatial scale for simultaneous classification tasks.

</details>

<details>

<summary>2020-10-03 07:31:30 - Deep matrix factorizations</summary>

- *Pierre De Handschutter, Nicolas Gillis, Xavier Siebert*

- `2010.00380v2` - [abs](http://arxiv.org/abs/2010.00380v2) - [pdf](http://arxiv.org/pdf/2010.00380v2)

> Constrained low-rank matrix approximations have been known for decades as powerful linear dimensionality reduction techniques to be able to extract the information contained in large data sets in a relevant way. However, such low-rank approaches are unable to mine complex, interleaved features that underlie hierarchical semantics. Recently, deep matrix factorization (deep MF) was introduced to deal with the extraction of several layers of features and has been shown to reach outstanding performances on unsupervised tasks. Deep MF was motivated by the success of deep learning, as it is conceptually close to some neural networks paradigms. In this paper, we present the main models, algorithms, and applications of deep MF through a comprehensive literature review. We also discuss theoretical questions and perspectives of research.

</details>

<details>

<summary>2020-10-03 19:50:04 - Semantic Role Labeling Guided Multi-turn Dialogue ReWriter</summary>

- *Kun Xu, Haochen Tan, Linfeng Song, Han Wu, Haisong Zhang, Linqi Song, Dong Yu*

- `2010.01417v1` - [abs](http://arxiv.org/abs/2010.01417v1) - [pdf](http://arxiv.org/pdf/2010.01417v1)

> For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting rid of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.

</details>

<details>

<summary>2020-10-03 20:56:16 - MagGAN: High-Resolution Face Attribute Editing with Mask-Guided Generative Adversarial Network</summary>

- *Yi Wei, Zhe Gan, Wenbo Li, Siwei Lyu, Ming-Ching Chang, Lei Zhang, Jianfeng Gao, Pengchuan Zhang*

- `2010.01424v1` - [abs](http://arxiv.org/abs/2010.01424v1) - [pdf](http://arxiv.org/pdf/2010.01424v1)

> We present Mask-guided Generative Adversarial Network (MagGAN) for high-resolution face attribute editing, in which semantic facial masks from a pre-trained face parser are used to guide the fine-grained image editing process. With the introduction of a mask-guided reconstruction loss, MagGAN learns to only edit the facial parts that are relevant to the desired attribute changes, while preserving the attribute-irrelevant regions (e.g., hat, scarf for modification `To Bald'). Further, a novel mask-guided conditioning strategy is introduced to incorporate the influence region of each attribute change into the generator. In addition, a multi-level patch-wise discriminator structure is proposed to scale our model for high-resolution ($1024 \times 1024$) face editing. Experiments on the CelebA benchmark show that the proposed method significantly outperforms prior state-of-the-art approaches in terms of both image quality and editing performance.

</details>

<details>

<summary>2020-10-04 00:04:40 - GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems</summary>

- *Shiquan Yang, Rui Zhang, Sarah Erfani*

- `2010.01447v1` - [abs](http://arxiv.org/abs/2010.01447v1) - [pdf](http://arxiv.org/pdf/2010.01447v1)

> End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.

</details>

<details>

<summary>2020-10-04 01:23:17 - Sentence Constituent-Aware Aspect-Category Sentiment Analysis with Graph Attention Networks</summary>

- *Yuncong Li, Cunxiang Yin, Sheng-hua Zhong*

- `2010.01461v1` - [abs](http://arxiv.org/abs/2010.01461v1) - [pdf](http://arxiv.org/pdf/2010.01461v1)

> Aspect category sentiment analysis (ACSA) aims to predict the sentiment polarities of the aspect categories discussed in sentences. Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate the appropriate sentiment words for the given aspect category and obtain promising results. However, most of these methods directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. To mitigate this problem, we propose a Sentence Constituent-Aware Network (SCAN) for aspect-category sentiment analysis. SCAN contains two graph attention modules and an interactive loss function. The graph attention modules generate representations of the nodes in sentence constituency parse trees for the aspect category detection (ACD) task and the ACSA task, respectively. ACD aims to detect aspect categories discussed in sentences and is a auxiliary task. For a given aspect category, the interactive loss function helps the ACD task to find the nodes which can predict the aspect category but can't predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. The experimental results on five public datasets demonstrate the effectiveness of SCAN.

</details>

<details>

<summary>2020-10-04 03:52:39 - Explanation Ontology in Action: A Clinical Use-Case</summary>

- *Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman, Amar K. Das, Deborah L. McGuinness*

- `2010.01478v1` - [abs](http://arxiv.org/abs/2010.01478v1) - [pdf](http://arxiv.org/pdf/2010.01478v1)

> We addressed the problem of a lack of semantic representation for user-centric explanations and different explanation types in our Explanation Ontology (https://purl.org/heals/eo). Such a representation is increasingly necessary as explainability has become an important problem in Artificial Intelligence with the emergence of complex methods and an uptake in high-precision and user-facing settings. In this submission, we provide step-by-step guidance for system designers to utilize our ontology, introduced in our resource track paper, to plan and model for explanations during the design of their Artificial Intelligence systems. We also provide a detailed example with our utilization of this guidance in a clinical setting.

</details>

<details>

<summary>2020-10-04 03:53:35 - Explanation Ontology: A Model of Explanations for User-Centered AI</summary>

- *Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman, Amar K. Das, Deborah L. McGuinness*

- `2010.01479v1` - [abs](http://arxiv.org/abs/2010.01479v1) - [pdf](http://arxiv.org/pdf/2010.01479v1)

> Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.

</details>

<details>

<summary>2020-10-04 19:03:39 - Inquisitive Question Generation for High Level Text Comprehension</summary>

- *Wei-Jen Ko, Te-Yuan Chen, Yiyan Huang, Greg Durrett, Junyi Jessy Li*

- `2010.01657v1` - [abs](http://arxiv.org/abs/2010.01657v1) - [pdf](http://arxiv.org/pdf/2010.01657v1)

> Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets.   We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.

</details>

<details>

<summary>2020-10-04 19:06:16 - Generating Dialogue Responses from a Semantic Latent Space</summary>

- *Wei-Jen Ko, Avik Ray, Yilin Shen, Hongxia Jin*

- `2010.01658v1` - [abs](http://arxiv.org/abs/2010.01658v1) - [pdf](http://arxiv.org/pdf/2010.01658v1)

> Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.

</details>

<details>

<summary>2020-10-04 19:30:13 - Multi-Resolution Fusion and Multi-scale Input Priors Based Crowd Counting</summary>

- *Usman Sajid, Wenchi Ma, Guanghui Wang*

- `2010.01664v1` - [abs](http://arxiv.org/abs/2010.01664v1) - [pdf](http://arxiv.org/pdf/2010.01664v1)

> Crowd counting in still images is a challenging problem in practice due to huge crowd-density variations, large perspective changes, severe occlusion, and variable lighting conditions. The state-of-the-art patch rescaling module (PRM) based approaches prove to be very effective in improving the crowd counting performance. However, the PRM module requires an additional and compromising crowd-density classification process. To address these issues and challenges, the paper proposes a new multi-resolution fusion based end-to-end crowd counting network. It employs three deep-layers based columns/branches, each catering the respective crowd-density scale. These columns regularly fuse (share) the information with each other. The network is divided into three phases with each phase containing one or more columns. Three input priors are introduced to serve as an efficient and effective alternative to the PRM module, without requiring any additional classification operations. Along with the final crowd count regression head, the network also contains three auxiliary crowd estimation regression heads, which are strategically placed at each phase end to boost the overall performance. Comprehensive experiments on three benchmark datasets demonstrate that the proposed approach outperforms all the state-of-the-art models under the RMSE evaluation metric. The proposed approach also has better generalization capability with the best results during the cross-dataset experiments.

</details>

<details>

<summary>2020-10-04 19:34:20 - Multi-Modal Retrieval using Graph Neural Networks</summary>

- *Aashish Kumar Misraa, Ajinkya Kale, Pranav Aggarwal, Ali Aminian*

- `2010.01666v1` - [abs](http://arxiv.org/abs/2010.01666v1) - [pdf](http://arxiv.org/pdf/2010.01666v1)

> Most real world applications of image retrieval such as Adobe Stock, which is a marketplace for stock photography and illustrations, need a way for users to find images which are both visually (i.e. aesthetically) and conceptually (i.e. containing the same salient objects) as a query image. Learning visual-semantic representations from images is a well studied problem for image retrieval. Filtering based on image concepts or attributes is traditionally achieved with index-based filtering (e.g. on textual tags) or by re-ranking after an initial visual embedding based retrieval. In this paper, we learn a joint vision and concept embedding in the same high-dimensional space. This joint model gives the user fine-grained control over the semantics of the result set, allowing them to explore the catalog of images more rapidly. We model the visual and concept relationships as a graph structure, which captures the rich information through node neighborhood. This graph structure helps us learn multi-modal node embeddings using Graph Neural Networks. We also introduce a novel inference time control, based on selective neighborhood connectivity allowing the user control over the retrieval algorithm. We evaluate these multi-modal embeddings quantitatively on the downstream relevance task of image retrieval on MS-COCO dataset and qualitatively on MS-COCO and an Adobe Stock dataset.

</details>

<details>

<summary>2020-10-04 20:59:57 - Systematic Generalization on gSCAN with Language Conditioned Embedding</summary>

- *Tong Gao, Qi Huang, Raymond J. Mooney*

- `2009.05552v2` - [abs](http://arxiv.org/abs/2009.05552v2) - [pdf](http://arxiv.org/pdf/2009.05552v2)

> Systematic Generalization refers to a learning algorithm's ability to extrapolate learned behavior to unseen situations that are distinct but semantically similar to its training data. As shown in recent work, state-of-the-art deep learning models fail dramatically even on tasks for which they are designed when the test set is systematically different from the training data. We hypothesize that explicitly modeling the relations between objects in their contexts while learning their representations will help achieve systematic generalization. Therefore, we propose a novel method that learns objects' contextualized embeddings with dynamic message passing conditioned on the input natural language and end-to-end trainable with other downstream deep learning modules. To our knowledge, this model is the first one that significantly outperforms the provided baseline and reaches state-of-the-art performance on grounded-SCAN (gSCAN), a grounded natural language navigation dataset designed to require systematic generalization in its test splits.

</details>

<details>

<summary>2020-10-04 21:44:15 - On Losses for Modern Language Models</summary>

- *Stephane Aroca-Ouellette, Frank Rudzicz*

- `2010.01694v1` - [abs](http://arxiv.org/abs/2010.01694v1) - [pdf](http://arxiv.org/pdf/2010.01694v1)

> BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT Base on the GLUE benchmark using fewer than a quarter of the training tokens.

</details>

<details>

<summary>2020-10-04 22:50:59 - Reading Comprehension as Natural Language Inference: A Semantic Analysis</summary>

- *Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Li, Pavan Kapanipathi, Kartik Talamadupula*

- `2010.01713v1` - [abs](http://arxiv.org/abs/2010.01713v1) - [pdf](http://arxiv.org/pdf/2010.01713v1)

> In the recent past, Natural language Inference (NLI) has gained significant attention, particularly given its promise for downstream NLP tasks. However, its true impact is limited and has not been well studied. Therefore, in this paper, we explore the utility of NLI for one of the most prominent downstream tasks, viz. Question Answering (QA). We transform the one of the largest available MRC dataset (RACE) to an NLI form, and compare the performances of a state-of-the-art model (RoBERTa) on both these forms. We propose new characterizations of questions, and evaluate the performance of QA and NLI models on these categories. We highlight clear categories for which the model is able to perform better when the data is presented in a coherent entailment form, and a structured question-answer concatenation form, respectively.

</details>

<details>

<summary>2020-10-04 22:52:29 - Pynsett: A programmable relation extractor</summary>

- *Alberto Cetoli*

- `2007.02100v2` - [abs](http://arxiv.org/abs/2007.02100v2) - [pdf](http://arxiv.org/pdf/2007.02100v2)

> This paper proposes a programmable relation extraction method for the English language by parsing texts into semantic graphs. A person can define rules in plain English that act as matching patterns onto the graph representation. These rules are designed to capture the semantic content of the documents, allowing for flexibility and ad-hoc entities. Relation extraction is a complex task that typically requires sizable training corpora. The method proposed here is ideal for extracting specialized ontologies in a limited collection of documents.

</details>

<details>

<summary>2020-10-05 00:23:49 - Attention Guided Semantic Relationship Parsing for Visual Question Answering</summary>

- *Moshiur Farazi, Salman Khan, Nick Barnes*

- `2010.01725v1` - [abs](http://arxiv.org/abs/2010.01725v1) - [pdf](http://arxiv.org/pdf/2010.01725v1)

> Humans explain inter-object relationships with semantic labels that demonstrate a high-level understanding required to perform complex Vision-Language tasks such as Visual Question Answering (VQA). However, existing VQA models represent relationships as a combination of object-level visual features which constrain a model to express interactions between objects in a single domain, while the model is trying to solve a multi-modal task. In this paper, we propose a general purpose semantic relationship parser which generates a semantic feature vector for each subject-predicate-object triplet in an image, and a Mutual and Self Attention (MSA) mechanism that learns to identify relationship triplets that are important to answer the given question. To motivate the significance of semantic relationships, we show an oracle setting with ground-truth relationship triplets, where our model achieves a ~25% accuracy gain over the closest state-of-the-art model on the challenging GQA dataset. Further, with our semantic parser, we show that our model outperforms other comparable approaches on VQA and GQA datasets.

</details>

<details>

<summary>2020-10-05 01:33:58 - Transformer-Based Neural Text Generation with Syntactic Guidance</summary>

- *Yinghao Li, Rui Feng, Isaac Rehg, Chao Zhang*

- `2010.01737v1` - [abs](http://arxiv.org/abs/2010.01737v1) - [pdf](http://arxiv.org/pdf/2010.01737v1)

> We study the problem of using (partial) constituency parse trees as syntactic guidance for controlled text generation. Existing approaches to this problem use recurrent structures, which not only suffer from the long-term dependency problem but also falls short in modeling the tree structure of the syntactic guidance. We propose to leverage the parallelism of Transformer to better incorporate parse trees. Our method first expands a partial template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the expanded tree to guide text generation. The effectiveness of our model in this process hinges upon two new attention mechanisms: 1) a path attention mechanism that forces one node to attend to only other nodes located in its path in the syntax tree to better incorporate syntax guidance; 2) a multi-encoder attention mechanism that allows the decoder to dynamically attend to information from multiple encoders. Our experiments in the controlled paraphrasing task show that our method outperforms SOTA models both semantically and syntactically, improving the best baseline's BLEU score from 11.83 to 26.27.

</details>

<details>

<summary>2020-10-05 02:14:13 - On the Effects of Knowledge-Augmented Data in Word Embeddings</summary>

- *Diego Ramirez-Echavarria, Antonis Bikakis, Luke Dickens, Rob Miller, Andreas Vlachidis*

- `2010.01745v1` - [abs](http://arxiv.org/abs/2010.01745v1) - [pdf](http://arxiv.org/pdf/2010.01745v1)

> This paper investigates techniques for knowledge injection into word embeddings learned from large corpora of unannotated data. These representations are trained with word cooccurrence statistics and do not commonly exploit syntactic and semantic information from linguistic knowledge bases, which potentially limits their transferability to domains with differing language distributions or usages. We propose a novel approach for linguistic knowledge injection through data augmentation to learn word embeddings that enforce semantic relationships from the data, and systematically evaluate the impact it has on the resulting representations. We show our knowledge augmentation approach improves the intrinsic characteristics of the learned embeddings while not significantly altering their results on a downstream text classification task.

</details>

<details>

<summary>2020-10-05 05:04:14 - Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning</summary>

- *Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, Shouling Ji*

- `2010.01781v1` - [abs](http://arxiv.org/abs/2010.01781v1) - [pdf](http://arxiv.org/pdf/2010.01781v1)

> Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.

</details>

<details>

<summary>2020-10-05 07:39:02 - TabEAno: Table to Knowledge Graph Entity Annotation</summary>

- *Phuc Nguyen, Natthawut Kertkeidkachorn, Ryutaro Ichise, Hideaki Takeda*

- `2010.01829v1` - [abs](http://arxiv.org/abs/2010.01829v1) - [pdf](http://arxiv.org/pdf/2010.01829v1)

> In the Open Data era, a large number of table resources have been made available on the Web and data portals. However, it is difficult to directly utilize such data due to the ambiguity of entities, name variations, heterogeneous schema, missing, or incomplete metadata. To address these issues, we propose a novel approach, namely TabEAno, to semantically annotate table rows toward knowledge graph entities. Specifically, we introduce a "two-cells" lookup strategy bases on the assumption that there is an existing logical relation occurring in the knowledge graph between the two closed cells in the same row of the table. Despite the simplicity of the approach, TabEAno outperforms the state of the art approaches in the two standard datasets e.g, T2D, Limaye with, and in the large-scale Wikipedia tables dataset.

</details>

<details>

<summary>2020-10-05 09:28:51 - KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding</summary>

- *Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, Hyungjoon Soh*

- `2004.03289v3` - [abs](http://arxiv.org/abs/2004.03289v3) - [pdf](http://arxiv.org/pdf/2004.03289v3)

> Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in natural language understanding (NLU). Although several benchmark datasets for those tasks have been released in English and a few other languages, there are no publicly available NLI or STS datasets in the Korean language. Motivated by this, we construct and release new datasets for Korean NLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous approaches, we machine-translate existing English training sets and manually translate development and test sets into Korean. To accelerate research on Korean NLU, we also establish baselines on KorNLI and KorSTS. Our datasets are publicly available at https://github.com/kakaobrain/KorNLUDatasets.

</details>

<details>

<summary>2020-10-05 09:37:54 - Time-Dynamic Estimates of the Reliability of Deep Semantic Segmentation Networks</summary>

- *Kira Maag, Matthias Rottmann, Hanno Gottschalk*

- `1911.05075v2` - [abs](http://arxiv.org/abs/1911.05075v2) - [pdf](http://arxiv.org/pdf/1911.05075v2)

> In the semantic segmentation of street scenes with neural networks, the reliability of predictions is of highest interest. The assessment of neural networks by means of uncertainties is a common ansatz to prevent safety issues. As in applications like automated driving, video streams of images are available, we present a time-dynamic approach to investigating uncertainties and assessing the prediction quality of neural networks. We track segments over time and gather aggregated metrics per segment, thus obtaining time series of metrics from which we assess prediction quality. This is done by either classifying between intersection over union equal to 0 and greater than 0 or predicting the intersection over union directly. We study different models for these two tasks and analyze the influence of the time series length on the predictive power of our metrics.

</details>

<details>

<summary>2020-10-05 09:54:51 - A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese</summary>

- *Anh Tuan Nguyen, Mai Hoang Dao, Dat Quoc Nguyen*

- `2010.01891v1` - [abs](http://arxiv.org/abs/2010.01891v1) - [pdf](http://arxiv.org/pdf/2010.01891v1)

> Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020).

</details>

<details>

<summary>2020-10-05 10:25:29 - PUM at SemEval-2020 Task 12: Aggregation of Transformer-based models' features for offensive language recognition</summary>

- *Piotr Janiszewski, Mateusz Skiba, Urszula Walińska*

- `2010.01897v1` - [abs](http://arxiv.org/abs/2010.01897v1) - [pdf](http://arxiv.org/pdf/2010.01897v1)

> In this paper, we describe the PUM team's entry to the SemEval-2020 Task 12. Creating our solution involved leveraging two well-known pretrained models used in natural language processing: BERT and XLNet, which achieve state-of-the-art results in multiple NLP tasks. The models were fine-tuned for each subtask separately and features taken from their hidden layers were combined and fed into a fully connected neural network. The model using aggregated Transformer features can serve as a powerful tool for offensive language identification problem. Our team was ranked 7th out of 40 in Sub-task C - Offense target identification with 64.727% macro F1-score and 64th out of 85 in Sub-task A - Offensive language identification (89.726% F1-score).

</details>

<details>

<summary>2020-10-05 10:26:36 - Exploring Semantic Capacity of Terms</summary>

- *Jie Huang, Zilong Wang, Kevin Chen-Chuan Chang, Wen-mei Hwu, Jinjun Xiong*

- `2010.01898v1` - [abs](http://arxiv.org/abs/2010.01898v1) - [pdf](http://arxiv.org/pdf/2010.01898v1)

> We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.

</details>

<details>

<summary>2020-10-05 11:44:09 - Improving Reconstructive Surgery Design using Gaussian Process Surrogates to Capture Material Behavior Uncertainty</summary>

- *Casey Stowers, Taeksang Lee, Ilias Bilionis, Arun Gosain, Adrian Buganza Tepole*

- `2010.02800v1` - [abs](http://arxiv.org/abs/2010.02800v1) - [pdf](http://arxiv.org/pdf/2010.02800v1)

> Excessive loads near wounds produce pathological scarring and other complications. Presently, stress cannot easily be measured by surgeons in the operating room. Instead, surgeons rely on intuition and experience. Predictive computational tools are ideal candidates for surgery planning. Finite element (FE) simulations have shown promise in predicting stress fields on large skin patches and complex cases, helping to identify potential regions of complication. Unfortunately, these simulations are computationally expensive and deterministic. However, running a few, well-selected FE simulations allows us to create Gaussian process (GP) surrogate models of local cutaneous flaps that are computationally efficient and able to predict stress and strain for arbitrary material parameters. Here, we create GP surrogates for the advancement, rotation, and transposition flaps. We then use the predictive capability of these surrogates to perform a global sensitivity analysis, ultimately showing that fiber direction has the most significant impact on strain field variations. We then perform an optimization to determine the optimal fiber direction for each flap for three different objectives driven by clinical guidelines. While material properties are not controlled by the surgeon and are actually a source of uncertainty, the surgeon can in fact control the orientation of the flap. Therefore, fiber direction is the only material parameter that can be optimized clinically. The optimization task relies on the efficiency of the GP surrogates to calculate the expected cost of different strategies when the uncertainty of other material parameters is included. We propose optimal flap orientations for the three cost functions and that can help in reducing stress resulting from the surgery and ultimately reduce complications associated with excessive mechanical loading near wounds.

</details>

<details>

<summary>2020-10-05 12:42:39 - LEAPME: Learning-based Property Matching with Embeddings</summary>

- *Daniel Ayala, Inma Hernández, David Ruiz, Erhard Rahm*

- `2010.01951v1` - [abs](http://arxiv.org/abs/2010.01951v1) - [pdf](http://arxiv.org/pdf/2010.01951v1)

> Data integration tasks such as the creation and extension of knowledge graphs involve the fusion of heterogeneous entities from many sources. Matching and fusion of such entities require to also match and combine their properties (attributes). However, previous schema matching approaches mostly focus on two sources only and often rely on simple similarity measurements. They thus face problems in challenging use cases such as the integration of heterogeneous product entities from many sources.   We therefore present a new machine learning-based property matching approach called LEAPME (LEArning-based Property Matching with Embeddings) that utilizes numerous features of both property names and instance values. The approach heavily makes use of word embeddings to better utilize the domain-specific semantics of both property names and instance values. The use of supervised machine learning helps exploit the predictive power of word embeddings.   Our comparative evaluation against five baselines for several multi-source datasets with real-world data shows the high effectiveness of LEAPME. We also show that our approach is even effective when training data from another domain (transfer learning) is used.

</details>

<details>

<summary>2020-10-05 13:18:38 - A Spherical Hidden Markov Model for Semantics-Rich Human Mobility Modeling</summary>

- *Wanzheng Zhu, Chao Zhang, Shuochao Yao, Xiaobin Gao, Jiawei Han*

- `2010.01986v1` - [abs](http://arxiv.org/abs/2010.01986v1) - [pdf](http://arxiv.org/pdf/2010.01986v1)

> We study the problem of modeling human mobility from semantic trace data, wherein each GPS record in a trace is associated with a text message that describes the user's activity. Existing methods fall short in unveiling human movement regularities, because they either do not model the text data at all or suffer from text sparsity severely. We propose SHMM, a multi-modal spherical hidden Markov model for semantics-rich human mobility modeling. Under the hidden Markov assumption, SHMM models the generation process of a given trace by jointly considering the observed location, time, and text at each step of the trace. The distinguishing characteristic of SHMM is the text modeling part. We use fixed-size vector representations to encode the semantics of the text messages, and model the generation of the l2-normalized text embeddings on a unit sphere with the von Mises-Fisher (vMF) distribution. Compared with other alternatives like multi-variate Gaussian, our choice of the vMF distribution not only incurs much fewer parameters, but also better leverages the discriminative power of text embeddings in a directional metric space. The parameter inference for the vMF distribution is non-trivial since it involves functional inversion of ratios of Bessel functions. We theoretically prove that: 1) the classical Expectation-Maximization algorithm can work with vMF distributions; and 2) while closed-form solutions are hard to be obtained for the M-step, Newton's method is guaranteed to converge to the optimal solution with quadratic convergence rate. We have performed extensive experiments on both synthetic and real-life data. The results on synthetic data verify our theoretical analysis; while the results on real-life data demonstrate that SHMM learns meaningful semantics-rich mobility models, outperforms state-of-the-art mobility models for next location prediction, and incurs lower training cost.

</details>

<details>

<summary>2020-10-05 13:34:20 - X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset</summary>

- *Angel Daza, Anette Frank*

- `2010.01998v1` - [abs](http://arxiv.org/abs/2010.01998v1) - [pdf](http://arxiv.org/pdf/2010.01998v1)

> Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work, we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.

</details>

<details>

<summary>2020-10-05 13:35:02 - A Novel Actor Dual-Critic Model for Remote Sensing Image Captioning</summary>

- *Ruchika Chavhan, Biplab Banerjee, Xiao Xiang Zhu, Subhasis Chaudhuri*

- `2010.01999v1` - [abs](http://arxiv.org/abs/2010.01999v1) - [pdf](http://arxiv.org/pdf/2010.01999v1)

> We deal with the problem of generating textual captions from optical remote sensing (RS) images using the notion of deep reinforcement learning. Due to the high inter-class similarity in reference sentences describing remote sensing data, jointly encoding the sentences and images encourages prediction of captions that are semantically more precise than the ground truth in many cases. To this end, we introduce an Actor Dual-Critic training strategy where a second critic model is deployed in the form of an encoder-decoder RNN to encode the latent information corresponding to the original and generated captions. While all actor-critic methods use an actor to predict sentences for an image and a critic to provide rewards, our proposed encoder-decoder RNN guarantees high-level comprehension of images by sentence-to-image translation. We observe that the proposed model generates sentences on the test data highly similar to the ground truth and is successful in generating even better captions in many critical cases. Extensive experiments on the benchmark Remote Sensing Image Captioning Dataset (RSICD) and the UCM-captions dataset confirm the superiority of the proposed approach in comparison to the previous state-of-the-art where we obtain a gain of sharp increments in both the ROUGE-L and CIDEr measures.

</details>

<details>

<summary>2020-10-05 16:51:14 - Generative Patch Priors for Practical Compressive Image Recovery</summary>

- *Rushil Anirudh, Suhas Lohit, Pavan Turaga*

- `2006.10873v2` - [abs](http://arxiv.org/abs/2006.10873v2) - [pdf](http://arxiv.org/pdf/2006.10873v2)

> In this paper, we propose the generative patch prior (GPP) that defines a generative prior for compressive image recovery, based on patch-manifold models. Unlike learned, image-level priors that are restricted to the range space of a pre-trained generator, GPP can recover a wide variety of natural images using a pre-trained patch generator. Additionally, GPP retains the benefits of generative priors like high reconstruction quality at extremely low sensing rates, while also being much more generally applicable. We show that GPP outperforms several unsupervised and supervised techniques on three different sensing models -- linear compressive sensing with known, and unknown calibration settings, and the non-linear phase retrieval problem. Finally, we propose an alternating optimization strategy using GPP for joint calibration-and-reconstruction which performs favorably against several baselines on a real world, un-calibrated compressive sensing dataset.

</details>

<details>

<summary>2020-10-05 18:00:15 - Acrostic Poem Generation</summary>

- *Rajat Agarwal, Katharina Kann*

- `2010.02239v1` - [abs](http://arxiv.org/abs/2010.02239v1) - [pdf](http://arxiv.org/pdf/2010.02239v1)

> We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem's semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.

</details>

<details>

<summary>2020-10-05 18:26:38 - Grounded Compositional Outputs for Adaptive Language Modeling</summary>

- *Nikolaos Pappas, Phoebe Mulcaire, Noah A. Smith*

- `2009.11523v2` - [abs](http://arxiv.org/abs/2009.11523v2) - [pdf](http://arxiv.org/pdf/2009.11523v2)

> Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's vocabulary$-$typically selected before training and permanently fixed later$-$affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.

</details>

<details>

<summary>2020-10-05 18:49:48 - Accelerating Reinforcement Learning through GPU Atari Emulation</summary>

- *Steven Dalton, Iuri Frosio, Michael Garland*

- `1907.08467v2` - [abs](http://arxiv.org/abs/1907.08467v2) - [pdf](http://arxiv.org/pdf/1907.08467v2)

> We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari Learning Environment (ALE) which is used for the development of deep reinforcement algorithms. CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a finding previously achieved only through a cluster of CPUs. Beyond highlighting the differences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace. CuLE is available at https://github.com/NVLabs/cule .

</details>

<details>

<summary>2020-10-06 01:20:53 - Second-Order NLP Adversarial Examples</summary>

- *John X. Morris*

- `2010.01770v2` - [abs](http://arxiv.org/abs/2010.01770v2) - [pdf](http://arxiv.org/pdf/2010.01770v2)

> Adversarial example generation methods in NLP rely on models like language models or sentence encoders to determine if potential adversarial examples are valid. In these methods, a valid adversarial example fools the model being attacked, and is determined to be semantically or syntactically valid by a second model. Research to date has counted all such examples as errors by the attacked model. We contend that these adversarial examples may not be flaws in the attacked model, but flaws in the model that determines validity. We term such invalid inputs second-order adversarial examples. We propose the constraint robustness curve and associated metric ACCS as tools for evaluating the robustness of a constraint to second-order adversarial examples. To generate this curve, we design an adversarial attack to run directly on the semantic similarity models. We test on two constraints, the Universal Sentence Encoder (USE) and BERTScore. Our findings indicate that such second-order examples exist, but are typically less common than first-order adversarial examples in state-of-the-art models. They also indicate that USE is effective as constraint on NLP adversarial examples, while BERTScore is nearly ineffectual. Code for running the experiments in this paper is available at https://github.com/jxmorris12/second-order-adversarial-examples.

</details>

<details>

<summary>2020-10-06 02:51:02 - Multi-Fact Correction in Abstractive Text Summarization</summary>

- *Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, Jingjing Liu*

- `2010.02443v1` - [abs](http://arxiv.org/abs/2010.02443v1) - [pdf](http://arxiv.org/pdf/2010.02443v1)

> Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.

</details>

<details>

<summary>2020-10-06 03:11:55 - LIMIT-BERT : Linguistic Informed Multi-Task BERT</summary>

- *Junru Zhou, Zhuosheng Zhang, Hai Zhao, Shuailiang Zhang*

- `1910.14296v2` - [abs](http://arxiv.org/abs/1910.14296v2) - [pdf](http://arxiv.org/pdf/1910.14296v2)

> In this paper, we present a Linguistic Informed Multi-Task BERT (LIMIT-BERT) for learning language representations across multiple linguistic tasks by Multi-Task Learning (MTL). LIMIT-BERT includes five key linguistic syntax and semantics tasks: Part-Of-Speech (POS) tags, constituent and dependency syntactic parsing, span and dependency semantic role labeling (SRL). Besides, LIMIT-BERT adopts linguistics mask strategy: Syntactic and Semantic Phrase Masking which mask all of the tokens corresponding to a syntactic/semantic phrase. Different from recent Multi-Task Deep Neural Networks (MT-DNN) (Liu et al., 2019), our LIMIT-BERT is linguistically motivated and learning in a semi-supervised method which provides large amounts of linguistic-task data as same as BERT learning corpus. As a result, LIMIT-BERT not only improves linguistic tasks performance but also benefits from a regularization effect and linguistic information that leads to more general representations to help adapt to new tasks and domains. LIMIT-BERT obtains new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.

</details>

<details>

<summary>2020-10-06 03:30:01 - Parsing All: Syntax and Semantics, Dependencies and Spans</summary>

- *Junru Zhou, Zuchao Li, Hai Zhao*

- `1908.11522v3` - [abs](http://arxiv.org/abs/1908.11522v3) - [pdf](http://arxiv.org/pdf/1908.11522v3)

> Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As linguistic representation formalisms, both syntax and semantics may be represented in either span (constituent/phrase) or dependency, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of neural network and benefits from two representation formalisms in a uniform way. The experiments show that semantics and syntax can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.

</details>

<details>

<summary>2020-10-06 03:32:17 - Collaboratively boosting data-driven deep learning and knowledge-guided ontological reasoning for semantic segmentation of remote sensing imagery</summary>

- *Yansheng Li, Song Ouyang, Yongjun Zhang*

- `2010.02451v1` - [abs](http://arxiv.org/abs/2010.02451v1) - [pdf](http://arxiv.org/pdf/2010.02451v1)

> As one kind of architecture from the deep learning family, deep semantic segmentation network (DSSN) achieves a certain degree of success on the semantic segmentation task and obviously outperforms the traditional methods based on hand-crafted features. As a classic data-driven technique, DSSN can be trained by an end-to-end mechanism and competent for employing the low-level and mid-level cues (i.e., the discriminative image structure) to understand images, but lacks the high-level inference ability. By contrast, human beings have an excellent inference capacity and can be able to reliably interpret the RS imagery only when human beings master the basic RS domain knowledge. In literature, ontological modeling and reasoning is an ideal way to imitate and employ the domain knowledge of human beings, but is still rarely explored and adopted in the RS domain. To remedy the aforementioned critical limitation of DSSN, this paper proposes a collaboratively boosting framework (CBF) to combine data-driven deep learning module and knowledge-guided ontological reasoning module in an iterative way.

</details>

<details>

<summary>2020-10-06 03:49:22 - Identifying Spurious Correlations for Robust Text Classification</summary>

- *Zhao Wang, Aron Culotta*

- `2010.02458v1` - [abs](http://arxiv.org/abs/2010.02458v1) - [pdf](http://arxiv.org/pdf/2010.02458v1)

> The predictions of text classifiers are often driven by spurious correlations -- e.g., the term `Spielberg' correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine correlations in text classification. We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from "genuine" ones. Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains. Experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification, as measured by improved worst-case accuracy on the samples affected by spurious correlations.

</details>

<details>

<summary>2020-10-06 04:03:42 - STP-UDGAT: Spatial-Temporal-Preference User Dimensional Graph Attention Network for Next POI Recommendation</summary>

- *Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh, Renrong Weng, Jagannadan Varadarajan*

- `2010.07024v1` - [abs](http://arxiv.org/abs/2010.07024v1) - [pdf](http://arxiv.org/pdf/2010.07024v1)

> Next Point-of-Interest (POI) recommendation is a longstanding problem across the domains of Location-Based Social Networks (LBSN) and transportation. Recent Recurrent Neural Network (RNN) based approaches learn POI-POI relationships in a local view based on independent user visit sequences. This limits the model's ability to directly connect and learn across users in a global view to recommend semantically trained POIs. In this work, we propose a Spatial-Temporal-Preference User Dimensional Graph Attention Network (STP-UDGAT), a novel explore-exploit model that concurrently exploits personalized user preferences and explores new POIs in global spatial-temporal-preference (STP) neighbourhoods, while allowing users to selectively learn from other users. In addition, we propose random walks as a masked self-attention option to leverage the STP graphs' structures and find new higher-order POI neighbours during exploration. Experimental results on six real-world datasets show that our model significantly outperforms baseline and state-of-the-art methods.

</details>

<details>

<summary>2020-10-06 04:18:18 - Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays</summary>

- *Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley*

- `2010.02467v1` - [abs](http://arxiv.org/abs/2010.02467v1) - [pdf](http://arxiv.org/pdf/2010.02467v1)

> Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists' workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g.~label imbalance) and face common issues inherent in text generation models (e.g.~repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.

</details>

<details>

<summary>2020-10-06 05:59:25 - GRUEN for Evaluating Linguistic Quality of Generated Text</summary>

- *Wanzheng Zhu, Suma Bhat*

- `2010.02498v1` - [abs](http://arxiv.org/abs/2010.02498v1) - [pdf](http://arxiv.org/pdf/2010.02498v1)

> Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.

</details>

<details>

<summary>2020-10-06 08:00:02 - Multi-typed Objects Multi-view Multi-instance Multi-label Learning</summary>

- *Yuanlin Yang, Guoxian Yu, Jun Wang, Carlotta Domeniconi, Xiangliang Zhang*

- `2010.02539v1` - [abs](http://arxiv.org/abs/2010.02539v1) - [pdf](http://arxiv.org/pdf/2010.02539v1)

> Multi-typed objects Multi-view Multi-instance Multi-label Learning (M4L) deals with interconnected multi-typed objects (or bags) that are made of diverse instances, represented with heterogeneous feature views and annotated with a set of non-exclusive but semantically related labels. M4L is more general and powerful than the typical Multi-view Multi-instance Multi-label Learning (M3L), which only accommodates single-typed bags and lacks the power to jointly model the naturally interconnected multi-typed objects in the physical world. To combat with this novel and challenging learning task, we develop a joint matrix factorization based solution (M4L-JMF). Particularly, M4L-JMF firstly encodes the diverse attributes and multiple inter(intra)-associations among multi-typed bags into respective data matrices, and then jointly factorizes these matrices into low-rank ones to explore the composite latent representation of each bag and its instances (if any). In addition, it incorporates a dispatch and aggregation term to distribute the labels of bags to individual instances and reversely aggregate the labels of instances to their affiliated bags in a coherent manner. Experimental results on benchmark datasets show that M4L-JMF achieves significantly better results than simple adaptions of existing M3L solutions on this novel problem.

</details>

<details>

<summary>2020-10-06 08:43:27 - Memory-efficient GAN-based Domain Translation of High Resolution 3D Medical Images</summary>

- *Hristina Uzunova, Jan Ehrhardt, Heinz Handels*

- `2010.03396v1` - [abs](http://arxiv.org/abs/2010.03396v1) - [pdf](http://arxiv.org/pdf/2010.03396v1)

> Generative adversarial networks (GANs) are currently rarely applied on 3D medical images of large size, due to their immense computational demand. The present work proposes a multi-scale patch-based GAN approach for establishing unpaired domain translation by generating 3D medical image volumes of high resolution in a memory-efficient way. The key idea to enable memory-efficient image generation is to first generate a low-resolution version of the image followed by the generation of patches of constant sizes but successively growing resolutions. To avoid patch artifacts and incorporate global information, the patch generation is conditioned on patches from previous resolution scales. Those multi-scale GANs are trained to generate realistically looking images from image sketches in order to perform an unpaired domain translation. This allows to preserve the topology of the test data and generate the appearance of the training domain data. The evaluation of the domain translation scenarios is performed on brain MRIs of size 155x240x240 and thorax CTs of size up to 512x512x512. Compared to common patch-based approaches, the multi-resolution scheme enables better image quality and prevents patch artifacts. Also, it ensures constant GPU memory demand independent from the image size, allowing for the generation of arbitrarily large images.

</details>

<details>

<summary>2020-10-06 09:07:57 - Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem</summary>

- *Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu, Sheng Zhong*

- `2004.13781v2` - [abs](http://arxiv.org/abs/2004.13781v2) - [pdf](http://arxiv.org/pdf/2004.13781v2)

> The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural information for encoding, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our model for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.

</details>

<details>

<summary>2020-10-06 09:29:51 - Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution</summary>

- *Yordan Yordanov, Oana-Maria Camburu, Vid Kocijan, Thomas Lukasiewicz*

- `2010.02570v1` - [abs](http://arxiv.org/abs/2010.02570v1) - [pdf](http://arxiv.org/pdf/2010.02570v1)

> Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.

</details>

<details>

<summary>2020-10-06 10:06:01 - Semantically Driven Sentence Fusion: Modeling and Evaluation</summary>

- *Eyal Ben-David, Orgad Keller, Eric Malmi, Idan Szpektor, Roi Reichart*

- `2010.02592v1` - [abs](http://arxiv.org/abs/2010.02592v1) - [pdf](http://arxiv.org/pdf/2010.02592v1)

> Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders models from robustly capturing the semantic relationship between input sentences. To alleviate this, we present an approach in which ground-truth solutions are automatically expanded into multiple references via curated equivalence classes of connective phrases. We apply this method to a large-scale dataset and use the augmented dataset for both model training and evaluation. To improve the learning of semantic representation using multiple references, we enrich the model with auxiliary discourse classification tasks under a multi-tasking framework. Our experiments highlight the improvements of our approach over state-of-the-art models.

</details>

<details>

<summary>2020-10-06 10:24:45 - Joint Semantics and Data-Driven Path Representation for Knowledge Graph Inference</summary>

- *Guanglin Niu, Bo Li, Yongfei Zhang, Yongpan Sheng, Chuan Shi, Jingyang Li, Shiliang Pu*

- `2010.02602v1` - [abs](http://arxiv.org/abs/2010.02602v1) - [pdf](http://arxiv.org/pdf/2010.02602v1)

> Inference on a large-scale knowledge graph (KG) is of great importance for KG applications like question answering. The path-based reasoning models can leverage much information over paths other than pure triples in the KG, which face several challenges: all the existing path-based methods are data-driven, lacking explainability for path representation. Besides, some methods either consider only relational paths or ignore the heterogeneity between entities and relations both contained in paths, which cannot capture the rich semantics of paths well. To address the above challenges, in this work, we propose a novel joint semantics and data-driven path representation that balances explainability and generalization in the framework of KG embedding. More specifically, we inject horn rules to obtain the condensed paths by the transparent and explainable path composition procedure. The entity converter is designed to transform the entities along paths into the representations in the semantic level similar to relations for reducing the heterogeneity between entities and relations, in which the KGs both with and without type information are considered. Our proposed model is evaluated on two classes of tasks: link prediction and path query answering task. The experimental results show that it has a significant performance gain over several different state-of-the-art baselines.

</details>

<details>

<summary>2020-10-06 12:03:47 - Extracting Implicitly Asserted Propositions in Argumentation</summary>

- *Yohan Jo, Jacky Visser, Chris Reed, Eduard Hovy*

- `2010.02654v1` - [abs](http://arxiv.org/abs/2010.02654v1) - [pdf](http://arxiv.org/pdf/2010.02654v1)

> Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.

</details>

<details>

<summary>2020-10-06 12:37:53 - Coreferential Reasoning Learning for Language Representation</summary>

- *Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, Zhiyuan Liu*

- `2004.06870v2` - [abs](http://arxiv.org/abs/2004.06870v2) - [pdf](http://arxiv.org/pdf/2004.06870v2)

> Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.

</details>

<details>

<summary>2020-10-06 12:38:02 - Incorporating Behavioral Hypotheses for Query Generation</summary>

- *Ruey-Cheng Chen, Chia-Jung Lee*

- `2010.02667v1` - [abs](http://arxiv.org/abs/2010.02667v1) - [pdf](http://arxiv.org/pdf/2010.02667v1)

> Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-$k$ word error rate and Bert F1 Score compared to a recent BART model.

</details>

<details>

<summary>2020-10-06 13:05:47 - BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations</summary>

- *Aina Garí Soler, Marianna Apidianaki*

- `2010.02686v1` - [abs](http://arxiv.org/abs/2010.02686v1) - [pdf](http://arxiv.org/pdf/2010.02686v1)

> Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.

</details>

<details>

<summary>2020-10-06 13:17:38 - Analyzing Individual Neurons in Pre-trained Language Models</summary>

- *Nadir Durrani, Hassan Sajjad, Fahim Dalvi, Yonatan Belinkov*

- `2010.02695v1` - [abs](http://arxiv.org/abs/2010.02695v1) - [pdf](http://arxiv.org/pdf/2010.02695v1)

> While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.

</details>

<details>

<summary>2020-10-06 13:50:47 - CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and Context-Dependent Word Representations</summary>

- *Martin Pömsl, Roman Lyapin*

- `2005.06602v3` - [abs](http://arxiv.org/abs/2005.06602v3) - [pdf](http://arxiv.org/pdf/2005.06602v3)

> This paper describes the winning contribution to SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection (Subtask 2) handed in by team UG Student Intern. We present an ensemble model that makes predictions based on context-free and context-dependent word representations. The key findings are that (1) context-free word representations are a powerful and robust baseline, (2) a sentence classification objective can be used to obtain useful context-dependent word representations, and (3) combining those representations increases performance on some datasets while decreasing performance on others.

</details>

<details>

<summary>2020-10-06 13:51:02 - Vec2Instance: Parameterization for Deep Instance Segmentation</summary>

- *N. Lakmal Deshapriya, Matthew N. Dailey, Manzul Kumar Hazarika, Hiroyuki Miyazaki*

- `2010.02725v1` - [abs](http://arxiv.org/abs/2010.02725v1) - [pdf](http://arxiv.org/pdf/2010.02725v1)

> Current advances in deep learning is leading to human-level accuracy in computer vision tasks such as object classification, localization, semantic segmentation, and instance segmentation. In this paper, we describe a new deep convolutional neural network architecture called Vec2Instance for instance segmentation. Vec2Instance provides a framework for parametrization of instances, allowing convolutional neural networks to efficiently estimate the complex shapes of instances around their centroids. We demonstrate the feasibility of the proposed architecture with respect to instance segmentation tasks on satellite images, which have a wide range of applications. Moreover, we demonstrate the usefulness of the new method for extracting building foot-prints from satellite images. Total pixel-wise accuracy of our approach is 89\%, near the accuracy of the state-of-the-art Mask RCNN (91\%). Vec2Instance is an alternative approach to complex instance segmentation pipelines, offering simplicity and intuitiveness. The code developed under this study is available in the Vec2Instance GitHub repository, https://github.com/lakmalnd/Vec2Instance

</details>

<details>

<summary>2020-10-06 14:49:04 - Fast semantic parsing with well-typedness guarantees</summary>

- *Matthias Lindemann, Jonas Groschwitz, Alexander Koller*

- `2009.07365v2` - [abs](http://arxiv.org/abs/2009.07365v2) - [pdf](http://arxiv.org/pdf/2009.07365v2)

> AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.

</details>

<details>

<summary>2020-10-06 14:59:16 - An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks</summary>

- *Lifu Tu, Tianyu Liu, Kevin Gimpel*

- `2010.02789v1` - [abs](http://arxiv.org/abs/2010.02789v1) - [pdf](http://arxiv.org/pdf/2010.02789v1)

> Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions.

</details>

<details>

<summary>2020-10-06 15:25:15 - QADiscourse -- Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines</summary>

- *Valentina Pyatkin, Ayal Klein, Reut Tsarfaty, Ido Dagan*

- `2010.02815v1` - [abs](http://arxiv.org/abs/2010.02815v1) - [pdf](http://arxiv.org/pdf/2010.02815v1)

> Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs, which in turn allows us to crowd-source wide-coverage data annotated with discourse relations, via an intuitively appealing interface for composing such questions and answers. Based on our proposed representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations.

</details>

<details>

<summary>2020-10-06 16:04:12 - Semantic Evaluation for Text-to-SQL with Distilled Test Suites</summary>

- *Ruiqi Zhong, Tao Yu, Dan Klein*

- `2010.02840v1` - [abs](http://arxiv.org/abs/2010.02840v1) - [pdf](http://arxiv.org/pdf/2010.02840v1)

> We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.

</details>

<details>

<summary>2020-10-06 16:08:28 - Safety Aware Reinforcement Learning (SARL)</summary>

- *Santiago Miret, Somdeb Majumdar, Carroll Wainwright*

- `2010.02846v1` - [abs](http://arxiv.org/abs/2010.02846v1) - [pdf](http://arxiv.org/pdf/2010.02846v1)

> As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability.

</details>

<details>

<summary>2020-10-06 17:06:50 - COD3S: Diverse Generation with Discrete Semantic Signatures</summary>

- *Nathaniel Weir, João Sedoc, Benjamin Van Durme*

- `2010.02882v1` - [abs](http://arxiv.org/abs/2010.02882v1) - [pdf](http://arxiv.org/pdf/2010.02882v1)

> We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seq models typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply COD3S to causal generation, the task of predicting a proposition's plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.

</details>

<details>

<summary>2020-10-06 20:30:59 - Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic Priming</summary>

- *Kanishka Misra, Allyson Ettinger, Julia Taylor Rayz*

- `2010.03010v1` - [abs](http://arxiv.org/abs/2010.03010v1) - [pdf](http://arxiv.org/pdf/2010.03010v1)

> Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows "priming," predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.

</details>

<details>

<summary>2020-10-06 22:23:00 - A Survey on Recognizing Textual Entailment as an NLP Evaluation</summary>

- *Adam Poliak*

- `2010.03061v1` - [abs](http://arxiv.org/abs/2010.03061v1) - [pdf](http://arxiv.org/pdf/2010.03061v1)

> Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.

</details>

<details>

<summary>2020-10-06 22:56:22 - Adversarial Patch Attacks on Monocular Depth Estimation Networks</summary>

- *Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii*

- `2010.03072v1` - [abs](http://arxiv.org/abs/2010.03072v1) - [pdf](http://arxiv.org/pdf/2010.03072v1)

> Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.

</details>

<details>

<summary>2020-10-06 22:56:31 - Beyond [CLS] through Ranking by Generation</summary>

- *Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, Bing Xiang*

- `2010.03073v1` - [abs](http://arxiv.org/abs/2010.03073v1) - [pdf](http://arxiv.org/pdf/2010.03073v1)

> Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.

</details>

<details>

<summary>2020-10-07 00:51:40 - Multi-View Attention Network for Visual Dialog</summary>

- *Sungjin Park, Taesun Whang, Yeochan Yoon, Heuiseok Lim*

- `2004.14025v3` - [abs](http://arxiv.org/abs/2004.14025v3) - [pdf](http://arxiv.org/pdf/2004.14025v3)

> Visual dialog is a challenging vision-language task in which a series of questions visually grounded by a given image are answered. To resolve the visual dialog task, a high-level understanding of various multimodal inputs (e.g., question, dialog history, and image) is required. Specifically, it is necessary for an agent to 1) determine the semantic intent of question and 2) align question-relevant textual and visual contents among heterogeneous modality inputs. In this paper, we propose Multi-View Attention Network (MVAN), which leverages multiple views about heterogeneous inputs based on attention mechanisms. MVAN effectively captures the question-relevant information from the dialog history with two complementary modules (i.e., Topic Aggregation and Context Matching), and builds multimodal representations through sequential alignment processes (i.e., Modality Alignment). Experimental results on VisDial v1.0 dataset show the effectiveness of our proposed model, which outperforms the previous state-of-the-art methods with respect to all evaluation metrics.

</details>

<details>

<summary>2020-10-07 02:23:58 - Structure Aware Negative Sampling in Knowledge Graphs</summary>

- *Kian Ahrabian, Aarash Feizi, Yasmin Salehi, William L. Hamilton, Avishek Joey Bose*

- `2009.11355v2` - [abs](http://arxiv.org/abs/2009.11355v2) - [pdf](http://arxiv.org/pdf/2009.11355v2)

> Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node's k-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.

</details>

<details>

<summary>2020-10-07 02:48:44 - VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling</summary>

- *Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo*

- `2010.03124v1` - [abs](http://arxiv.org/abs/2010.03124v1) - [pdf](http://arxiv.org/pdf/2010.03124v1)

> In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, "Cambridge" and the first non-English corpus "Robert", which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.

</details>

<details>

<summary>2020-10-07 07:47:54 - Integrating Inter-Object Scenarios with Intra-object Statecharts for Developing Reactive Systems</summary>

- *David Harel, Rami Marelly, Assaf Marron, Smadar Szekely*

- `1911.10691v2` - [abs](http://arxiv.org/abs/1911.10691v2) - [pdf](http://arxiv.org/pdf/1911.10691v2)

> In all software development projects, engineers face the challenge of translating the requirements layer into a design layer, then into an implementation-code layer, and then validating the correctness of the result. Many methodologies, languages and tools exist for facilitating the process, including multiple back-and-forth `refinement trips' across the requirements, design and implementation layers, by focusing on formalizing the artifacts involved and on automating a variety of tasks throughout. In this paper, we introduce a novel and unique development environment, which integrates scenario-based programming (SBP) via the LSC language and the object-oriented, visual Statecharts formalism, for the development of reactive systems. LSC targets creation of models and systems directly from requirement specifications, and Statecharts is used mainly for specifying final component behavior. Our integration enables semantically-rich joint execution, with the sharing and interfacing of objects and events, and can be used for creating and then gradually enhancing testable models from early in requirements elicitation through detailed design. In some cases, it can be used for generating final system code. We describe the technical details of the integration and its semantics and discuss its significance for future development methodologies.

</details>

<details>

<summary>2020-10-07 07:48:12 - Textual Supervision for Visually Grounded Spoken Language Understanding</summary>

- *Bertrand Higy, Desmond Elliott, Grzegorz Chrupała*

- `2010.02806v2` - [abs](http://arxiv.org/abs/2010.02806v2) - [pdf](http://arxiv.org/pdf/2010.02806v2)

> Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end approach compares to a traditional pipeline-based approach when one has access to transcriptions. Comparing different strategies, we find that the pipeline approach works better when enough text is available. With low-resource languages in mind, we also show that translations can be effectively used in place of transcriptions but more data is needed to obtain similar results.

</details>

<details>

<summary>2020-10-07 08:36:44 - Learning Binary Semantic Embedding for Histology Image Classification and Retrieval</summary>

- *Xiao Kang, Xingbo Liu, Xiushan Nie, Yilong Yin*

- `2010.03266v1` - [abs](http://arxiv.org/abs/2010.03266v1) - [pdf](http://arxiv.org/pdf/2010.03266v1)

> With the development of medical imaging technology and machine learning, computer-assisted diagnosis which can provide impressive reference to pathologists, attracts extensive research interests. The exponential growth of medical images and uninterpretability of traditional classification models have hindered the applications of computer-assisted diagnosis. To address these issues, we propose a novel method for Learning Binary Semantic Embedding (LBSE). Based on the efficient and effective embedding, classification and retrieval are performed to provide interpretable computer-assisted diagnosis for histology images. Furthermore, double supervision, bit uncorrelation and balance constraint, asymmetric strategy and discrete optimization are seamlessly integrated in the proposed method for learning binary embedding. Experiments conducted on three benchmark datasets validate the superiority of LBSE under various scenarios.

</details>

<details>

<summary>2020-10-07 15:34:00 - ELMo and BERT in semantic change detection for Russian</summary>

- *Julia Rodina, Yuliya Trofimova, Andrey Kutuzov, Ekaterina Artemova*

- `2010.03481v1` - [abs](http://arxiv.org/abs/2010.03481v1) - [pdf](http://arxiv.org/pdf/2010.03481v1)

> We study the effectiveness of contextualized embeddings for the task of diachronic semantic change detection for Russian language data. Evaluation test sets consist of Russian nouns and adjectives annotated based on their occurrences in texts created in pre-Soviet, Soviet and post-Soviet time periods. ELMo and BERT architectures are compared on the task of ranking Russian words according to the degree of their semantic change over time. We use several methods for aggregation of contextualized embeddings from these architectures and evaluate their performance. Finally, we compare unsupervised and supervised techniques in this task.

</details>

<details>

<summary>2020-10-07 15:40:36 - CATBERT: Context-Aware Tiny BERT for Detecting Social Engineering Emails</summary>

- *Younghoo Lee, Joshua Saxe, Richard Harang*

- `2010.03484v1` - [abs](http://arxiv.org/abs/2010.03484v1) - [pdf](http://arxiv.org/pdf/2010.03484v1)

> Targeted phishing emails are on the rise and facilitate the theft of billions of dollars from organizations a year. While malicious signals from attached files or malicious URLs in emails can be detected by conventional malware signatures or machine learning technologies, it is challenging to identify hand-crafted social engineering emails which don't contain any malicious code and don't share word choices with known attacks. To tackle this problem, we fine-tune a pre-trained BERT model by replacing the half of Transformer blocks with simple adapters to efficiently learn sophisticated representations of the syntax and semantics of the natural language. Our Context-Aware network also learns the context representations between email's content and context features from email headers. Our CatBERT(Context-Aware Tiny Bert) achieves a 87% detection rate as compared to DistilBERT, LSTM, and logistic regression baselines which achieve 83%, 79%, and 54% detection rates at false positive rates of 1%, respectively. Our model is also faster than competing transformer approaches and is resilient to adversarial attacks which deliberately replace keywords with typos or synonyms.

</details>

<details>

<summary>2020-10-07 17:40:19 - Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive Language Identification using Pre-trained Language Models</summary>

- *Shuohuan Wang, Jiaxiang Liu, Xuan Ouyang, Yu Sun*

- `2010.03542v1` - [abs](http://arxiv.org/abs/2010.03542v1) - [pdf](http://arxiv.org/pdf/2010.03542v1)

> This paper describes Galileo's performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages. We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.

</details>

<details>

<summary>2020-10-07 17:43:55 - A Self-supervised Approach for Semantic Indexing in the Context of COVID-19 Pandemic</summary>

- *Nima Ebadi, Peyman Najafirad*

- `2010.03544v1` - [abs](http://arxiv.org/abs/2010.03544v1) - [pdf](http://arxiv.org/pdf/2010.03544v1)

> The pandemic has accelerated the pace at which COVID-19 scientific papers are published. In addition, the process of manually assigning semantic indexes to these papers by experts is even more time-consuming and overwhelming in the current health crisis. Therefore, there is an urgent need for automatic semantic indexing models which can effectively scale-up to newly introduced concepts and rapidly evolving distributions of the hyperfocused related literature. In this research, we present a novel semantic indexing approach based on the state-of-the-art self-supervised representation learning and transformer encoding exclusively suitable for pandemic crises. We present a case study on a novel dataset that is based on COVID-19 papers published and manually indexed in PubMed. Our study shows that our self-supervised model outperforms the best performing models of BioASQ Task 8a by micro-F1 score of 0.1 and LCA-F score of 0.08 on average. Our model also shows superior performance on detecting the supplementary concepts which is quite important when the focus of the literature has drastically shifted towards specific concepts related to the pandemic. Our study sheds light on the main challenges confronting semantic indexing models during a pandemic, namely new domains and drastic changes of their distributions, and as a superior alternative for such situations, propose a model founded on approaches which have shown auspicious performance in improving generalization and data efficiency in various NLP tasks. We also show the joint indexing of major Medical Subject Headings (MeSH) and supplementary concepts improves the overall performance.

</details>

<details>

<summary>2020-10-07 17:47:53 - Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing</summary>

- *Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, Sonal Gupta*

- `2010.03546v1` - [abs](http://arxiv.org/abs/2010.03546v1) - [pdf](http://arxiv.org/pdf/2010.03546v1)

> Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user's intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music).   In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.

</details>

<details>

<summary>2020-10-07 18:30:47 - Representing Point Clouds with Generative Conditional Invertible Flow Networks</summary>

- *Michał Stypułkowski, Kacper Kania, Maciej Zamorski, Maciej Zięba, Tomasz Trzciński, Jan Chorowski*

- `2010.11087v1` - [abs](http://arxiv.org/abs/2010.11087v1) - [pdf](http://arxiv.org/pdf/2010.11087v1)

> In this paper, we propose a simple yet effective method to represent point clouds as sets of samples drawn from a cloud-specific probability distribution. This interpretation matches intrinsic characteristics of point clouds: the number of points and their ordering within a cloud is not important as all points are drawn from the proximity of the object boundary. We postulate to represent each cloud as a parameterized probability distribution defined by a generative neural network. Once trained, such a model provides a natural framework for point cloud manipulation operations, such as aligning a new cloud into a default spatial orientation. To exploit similarities between same-class objects and to improve model performance, we turn to weight sharing: networks that model densities of points belonging to objects in the same family share all parameters with the exception of a small, object-specific embedding vector. We show that these embedding vectors capture semantic relationships between objects. Our method leverages generative invertible flow networks to learn embeddings as well as to generate point clouds. Thanks to this formulation and contrary to similar approaches, we are able to train our model in an end-to-end fashion. As a result, our model offers competitive or superior quantitative results on benchmark datasets, while enabling unprecedented capabilities to perform cloud manipulation tasks, such as point cloud registration and regeneration, by a generative network.

</details>

<details>

<summary>2020-10-07 19:19:42 - MuSeM: Detecting Incongruent News Headlines using Mutual Attentive Semantic Matching</summary>

- *Rahul Mishra, Piyush Yadav, Remi Calizzano, Markus Leippold*

- `2010.03617v1` - [abs](http://arxiv.org/abs/2010.03617v1) - [pdf](http://arxiv.org/pdf/2010.03617v1)

> Measuring the congruence between two texts has several useful applications, such as detecting the prevalent deceptive and misleading news headlines on the web. Many works have proposed machine learning based solutions such as text similarity between the headline and body text to detect the incongruence. Text similarity based methods fail to perform well due to different inherent challenges such as relative length mismatch between the news headline and its body content and non-overlapping vocabulary. On the other hand, more recent works that use headline guided attention to learn a headline derived contextual representation of the news body also result in convoluting overall representation due to the news body's lengthiness. This paper proposes a method that uses inter-mutual attention-based semantic matching between the original and synthetically generated headlines, which utilizes the difference between all pairs of word embeddings of words involved. The paper also investigates two more variations of our method, which use concatenation and dot-products of word embeddings of the words of original and synthetic headlines. We observe that the proposed method outperforms prior arts significantly for two publicly available datasets.

</details>

<details>

<summary>2020-10-07 21:26:20 - Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank</summary>

- *Eleftheria Briakou, Marine Carpuat*

- `2010.03662v1` - [abs](http://arxiv.org/abs/2010.03662v1) - [pdf](http://arxiv.org/pdf/2010.03662v1)

> Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.

</details>

<details>

<summary>2020-10-08 00:41:43 - BAE: BERT-based Adversarial Examples for Text Classification</summary>

- *Siddhant Garg, Goutham Ramakrishnan*

- `2004.01970v3` - [abs](http://arxiv.org/abs/2004.01970v3) - [pdf](http://arxiv.org/pdf/2004.01970v3)

> Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.

</details>

<details>

<summary>2020-10-08 01:18:42 - Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding</summary>

- *Qile Zhu, Haidar Khan, Saleh Soltan, Stephen Rawls, Wael Hamza*

- `2010.03714v1` - [abs](http://arxiv.org/abs/2010.03714v1) - [pdf](http://arxiv.org/pdf/2010.03714v1)

> Semantic parsing is one of the key components of natural language understanding systems. A successful parse transforms an input utterance to an action that is easily understood by the system. Many algorithms have been proposed to solve this problem, from conventional rulebased or statistical slot-filling systems to shiftreduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on autoregressive sequence to sequence models to generate the parse directly. This model is slow at inference time, generating parses in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the autoregressive model and 2) significantly improves cross-lingual transfer in the low-resource setting by 37% compared to autoregressive baseline. We test our approach on three well-known monolingual datasets: ATIS, SNIPS and TOP. For cross lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.

</details>

<details>

<summary>2020-10-08 02:01:31 - PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge</summary>

- *Yun He, Zhuoer Wang, Yin Zhang, Ruihong Huang, James Caverlee*

- `2010.03725v1` - [abs](http://arxiv.org/abs/2010.03725v1) - [pdf](http://arxiv.org/pdf/2010.03725v1)

> We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.

</details>

<details>

<summary>2020-10-08 03:14:38 - Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition</summary>

- *Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, James Caverlee*

- `2010.03746v1` - [abs](http://arxiv.org/abs/2010.03746v1) - [pdf](http://arxiv.org/pdf/2010.03746v1)

> Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.

</details>

<details>

<summary>2020-10-08 12:36:24 - Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines</summary>

- *Łukasz Borchmann, Dawid Wiśniewski, Andrzej Gretkowski, Izabela Kosmala, Dawid Jurkiewicz, Łukasz Szałkiewicz, Gabriela Pałka, Karol Kaczmarek, Agnieszka Kaliska, Filip Graliński*

- `1911.03911v2` - [abs](http://arxiv.org/abs/1911.03911v2) - [pdf](http://arxiv.org/pdf/1911.03911v2)

> We propose a new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts. The task differs substantially from conventional NLI and shared tasks on legal information extraction (e.g., one has to identify text span instead of a single document, page, or paragraph). The specification of the proposed task is followed by an evaluation of multiple solutions within the unified framework proposed for this branch of methods. It is shown that state-of-the-art pretrained encoders fail to provide satisfactory results on the task proposed. In contrast, Language Model-based solutions perform better, especially when unsupervised fine-tuning is applied. Besides the ablation studies, we addressed questions regarding detection accuracy for relevant text fragments depending on the number of examples available. In addition to the dataset and reference results, LMs specialized in the legal domain were made publicly available.

</details>

<details>

<summary>2020-10-08 14:07:32 - GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems</summary>

- *Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, Xiaodan Liang*

- `2010.03994v1` - [abs](http://arxiv.org/abs/2010.03994v1) - [pdf](http://arxiv.org/pdf/2010.03994v1)

> Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgements. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.

</details>

<details>

<summary>2020-10-08 14:52:01 - SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects</summary>

- *Evangelos Ntavelis, Andrés Romero, Iason Kastanis, Luc Van Gool, Radu Timofte*

- `2004.04977v2` - [abs](http://arxiv.org/abs/2004.04977v2) - [pdf](http://arxiv.org/pdf/2004.04977v2)

> Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels.

</details>

<details>

<summary>2020-10-08 17:58:35 - Reducing Sentiment Bias in Language Models via Counterfactual Evaluation</summary>

- *Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli*

- `1911.03064v3` - [abs](http://arxiv.org/abs/1911.03064v3) - [pdf](http://arxiv.org/pdf/1911.03064v3)

> Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model's latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.

</details>

<details>

<summary>2020-10-08 20:26:34 - Evaluating the Effectiveness of Efficient Neural Architecture Search for Sentence-Pair Tasks</summary>

- *Ansel MacLaughlin, Jwala Dhamala, Anoop Kumar, Sriram Venkatapathy, Ragav Venkatesan, Rahul Gupta*

- `2010.04249v1` - [abs](http://arxiv.org/abs/2010.04249v1) - [pdf](http://arxiv.org/pdf/2010.04249v1)

> Neural Architecture Search (NAS) methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art (SOTA) performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search (ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and two sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS to NLP tasks, our results are mixed -- we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.

</details>

<details>

<summary>2020-10-08 22:45:08 - Deep or Simple Models for Semantic Tagging? It Depends on your Data [Experiments]</summary>

- *Jinfeng Li, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan*

- `2007.05651v2` - [abs](http://arxiv.org/abs/2007.05651v2) - [pdf](http://arxiv.org/pdf/2007.05651v2)

> Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.   We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against "simple models" over datasets with varying characteristics. Specifically, we select three prevalent deep models (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and compare their performance on the semantic tagging task over 21 datasets. Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task.

</details>

<details>

<summary>2020-10-09 00:28:21 - Asking without Telling: Exploring Latent Ontologies in Contextual Representations</summary>

- *Julian Michael, Jan A. Botha, Ian Tenney*

- `2004.14513v2` - [abs](http://arxiv.org/abs/2004.14513v2) - [pdf](http://arxiv.org/pdf/2004.14513v2)

> The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to existing classifier-based probing methods that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.

</details>

<details>

<summary>2020-10-09 03:04:01 - An ensemble learning approach for software semantic clone detection</summary>

- *Min Fu, Gang Luo, Xi Zheng, Tianyi Zhang, Dongjin Yu, Miryung Kim*

- `2010.04336v1` - [abs](http://arxiv.org/abs/2010.04336v1) - [pdf](http://arxiv.org/pdf/2010.04336v1)

> Code clone is a serious problem in software and has the potential to software defects, maintenance overhead, and licensing violations. Therefore, clone detection is important for reducing maintenance effort and improving code quality during software evolution. A variety of clone detection techniques have been proposed to identify similar code in software. However, few of them can efficiently detect semantic clones (functionally similar code without any syntactic resemblance). Recently, several deep learning based clone detectors are proposed to detect semantic clones. However, these approaches have high cost in data labelling and model training. In this paper, we propose a novel approach that leverages word embedding and ensemble learning techniques to detect semantic clones. Our evaluation on a commonly used clone benchmark, BigCloneBench, shows that our approach significantly improves the precision and recall of semantic clone detection, in comparison to a token-based clone detector, SourcererCC, and another deep learning based clone detector, CDLH.

</details>

<details>

<summary>2020-10-09 05:55:05 - Token-level Adaptive Training for Neural Machine Translation</summary>

- *Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie Zhou, Dong Yu*

- `2010.04380v1` - [abs](http://arxiv.org/abs/2010.04380v1) - [pdf](http://arxiv.org/pdf/2010.04380v1)

> There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.

</details>

<details>

<summary>2020-10-09 06:58:40 - Latent Dirichlet Allocation Model Training with Differential Privacy</summary>

- *Fangyuan Zhao, Xuebin Ren, Shusen Yang, Qing Han, Peng Zhao, Xinyu Yang*

- `2010.04391v1` - [abs](http://arxiv.org/abs/2010.04391v1) - [pdf](http://arxiv.org/pdf/2010.04391v1)

> Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for hidden semantic discovery of text data and serves as a fundamental tool for text analysis in various applications. However, the LDA model as well as the training process of LDA may expose the text information in the training data, thus bringing significant privacy concerns. To address the privacy issue in LDA, we systematically investigate the privacy protection of the main-stream LDA training algorithm based on Collapsed Gibbs Sampling (CGS) and propose several differentially private LDA algorithms for typical training scenarios. In particular, we present the first theoretical analysis on the inherent differential privacy guarantee of CGS based LDA training and further propose a centralized privacy-preserving algorithm (HDP-LDA) that can prevent data inference from the intermediate statistics in the CGS training. Also, we propose a locally private LDA training algorithm (LP-LDA) on crowdsourced data to provide local differential privacy for individual data contributors. Furthermore, we extend LP-LDA to an online version as OLP-LDA to achieve LDA training on locally private mini-batches in a streaming setting. Extensive analysis and experiment results validate both the effectiveness and efficiency of our proposed privacy-preserving LDA training algorithms.

</details>

<details>

<summary>2020-10-09 07:07:04 - gundapusunil at SemEval-2020 Task 9: Syntactic Semantic LSTM Architecture for SENTIment Analysis of Code-MIXed Data</summary>

- *Sunil Gundapu, Radhika Mamidi*

- `2010.04395v1` - [abs](http://arxiv.org/abs/2010.04395v1) - [pdf](http://arxiv.org/pdf/2010.04395v1)

> The phenomenon of mixing the vocabulary and syntax of multiple languages within the same utterance is called Code-Mixing. This is more evident in multilingual societies. In this paper, we have developed a system for SemEval 2020: Task 9 on Sentiment Analysis for Code-Mixed Social Media Text. Our system first generates two types of embeddings for the social media text. In those, the first one is character level embeddings to encode the character level information and to handle the out-of-vocabulary entries and the second one is FastText word embeddings for capturing morphology and semantics. These two embeddings were passed to the LSTM network and the system outperformed the baseline model.

</details>

<details>

<summary>2020-10-09 07:48:09 - Uncertainty-Aware Semantic Augmentation for Neural Machine Translation</summary>

- *Xiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng, Luxi Xing, Weihua Luo*

- `2010.04411v1` - [abs](http://arxiv.org/abs/2010.04411v1) - [pdf](http://arxiv.org/pdf/2010.04411v1)

> As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.

</details>

<details>

<summary>2020-10-09 07:48:37 - Continual learning using hash-routed convolutional neural networks</summary>

- *Ahmad Berjaoui*

- `2010.05880v1` - [abs](http://arxiv.org/abs/2010.05880v1) - [pdf](http://arxiv.org/pdf/2010.05880v1)

> Continual learning could shift the machine learning paradigm from data centric to model centric. A continual learning model needs to scale efficiently to handle semantically different datasets, while avoiding unnecessary growth. We introduce hash-routed convolutional neural networks: a group of convolutional units where data flows dynamically. Feature maps are compared using feature hashing and similar data is routed to the same units. A hash-routed network provides excellent plasticity thanks to its routed nature, while generating stable features through the use of orthogonal feature hashing. Each unit evolves separately and new units can be added (to be used only when necessary). Hash-routed networks achieve excellent performance across a variety of typical continual learning benchmarks without storing raw data and train using only gradient descent. Besides providing a continual learning framework for supervised tasks with encouraging results, our model can be used for unsupervised or reinforcement learning.

</details>

<details>

<summary>2020-10-09 09:33:10 - Retrieve and Refine: Exemplar-based Neural Comment Generation</summary>

- *Bolin Wei, Yongmin Li, Ge Li, Xin Xia, Zhi Jin*

- `2010.04459v1` - [abs](http://arxiv.org/abs/2010.04459v1) - [pdf](http://arxiv.org/pdf/2010.04459v1)

> Code comment generation which aims to automatically generate natural language descriptions for source code, is a crucial task in the field of automatic software development. Traditional comment generation methods use manually-crafted templates or information retrieval (IR) techniques to generate summaries for source code. In recent years, neural network-based methods which leveraged acclaimed encoder-decoder deep learning framework to learn comment generation patterns from a large-scale parallel code corpus, have achieved impressive results. However, these emerging methods only take code-related information as input. Software reuse is common in the process of software development, meaning that comments of similar code snippets are helpful for comment generation. Inspired by the IR-based and template-based approaches, in this paper, we propose a neural comment generation approach where we use the existing comments of similar code snippets as exemplars to guide comment generation. Specifically, given a piece of code, we first use an IR technique to retrieve a similar code snippet and treat its comment as an exemplar. Then we design a novel seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to assist in the target comment generation based on the semantic similarity between the source code and the similar code. We evaluate our approach on a large-scale Java corpus, which contains about 2M samples, and experimental results demonstrate that our model outperforms the state-of-the-art methods by a substantial margin.

</details>

<details>

<summary>2020-10-09 10:15:06 - Word Level Language Identification in English Telugu Code Mixed Data</summary>

- *Sunil Gundapu, Radhika Mamidi*

- `2010.04482v1` - [abs](http://arxiv.org/abs/2010.04482v1) - [pdf](http://arxiv.org/pdf/2010.04482v1)

> In a multilingual or sociolingual configuration Intra-sentential Code Switching (ICS) or Code Mixing (CM) is frequently observed nowadays. In the world, most of the people know more than one language. CM usage is especially apparent in social media platforms. Moreover, ICS is particularly significant in the context of technology, health, and law where conveying the upcoming developments are difficult in one's native language. In applications like dialog systems, machine translation, semantic parsing, shallow parsing, etc. CM and Code Switching pose serious challenges. To do any further advancement in code-mixed data, the necessary step is Language Identification. In this paper, we present a study of various models - Nave Bayes Classifier, Random Forest Classifier, Conditional Random Field (CRF), and Hidden Markov Model (HMM) for Language Identification in English - Telugu Code Mixed Data. Considering the paucity of resources in code mixed languages, we proposed the CRF model and HMM model for word level language identification. Our best performing system is CRF-based with an f1-score of 0.91.

</details>

<details>

<summary>2020-10-09 10:15:26 - Contralaterally Enhanced Networks for Thoracic Disease Detection</summary>

- *Gangming Zhao, Chaowei Fang, Guanbin Li, Licheng Jiao, Yizhou Yu*

- `2010.04483v1` - [abs](http://arxiv.org/abs/2010.04483v1) - [pdf](http://arxiv.org/pdf/2010.04483v1)

> Identifying and locating diseases in chest X-rays are very challenging, due to the low visual contrast between normal and abnormal regions, and distortions caused by other overlapping tissues. An interesting phenomenon is that there exist many similar structures in the left and right parts of the chest, such as ribs, lung fields and bronchial tubes. This kind of similarities can be used to identify diseases in chest X-rays, according to the experience of broad-certificated radiologists. Aimed at improving the performance of existing detection methods, we propose a deep end-to-end module to exploit the contralateral context information for enhancing feature representations of disease proposals. First of all, under the guidance of the spine line, the spatial transformer network is employed to extract local contralateral patches, which can provide valuable context information for disease proposals. Then, we build up a specific module, based on both additive and subtractive operations, to fuse the features of the disease proposal and the contralateral patch. Our method can be integrated into both fully and weakly supervised disease detection frameworks. It achieves 33.17 AP50 on a carefully annotated private chest X-ray dataset which contains 31,000 images. Experiments on the NIH chest X-ray dataset indicate that our method achieves state-of-the-art performance in weakly-supervised disease localization.

</details>

<details>

<summary>2020-10-09 10:20:58 - Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models</summary>

- *Pierangelo Lombardo, Alessio Boiardi, Luca Colombo, Angelo Schiavone, Nicolò Tamagnone*

- `2010.04486v1` - [abs](http://arxiv.org/abs/2010.04486v1) - [pdf](http://arxiv.org/pdf/2010.04486v1)

> The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks. In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks. Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.

</details>

<details>

<summary>2020-10-09 12:39:33 - What Have We Achieved on Text Summarization?</summary>

- *Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, Yue Zhang*

- `2010.04529v1` - [abs](http://arxiv.org/abs/2010.04529v1) - [pdf](http://arxiv.org/pdf/2010.04529v1)

> Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric(MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.

</details>

<details>

<summary>2020-10-09 13:54:27 - A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos</summary>

- *Frank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neubig, Yonatan Bisk, Nan Duan*

- `2005.00706v2` - [abs](http://arxiv.org/abs/2005.00706v2) - [pdf](http://arxiv.org/pdf/2005.00706v2)

> Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.

</details>

<details>

<summary>2020-10-09 15:33:54 - High-order Semantic Role Labeling</summary>

- *Zuchao Li, Hai Zhao, Rui Wang, Kevin Parnow*

- `2010.04641v1` - [abs](http://arxiv.org/abs/2010.04641v1) - [pdf](http://arxiv.org/pdf/2010.04641v1)

> Semantic role labeling is primarily used to identify predicates, arguments, and their semantic relationships. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships between predicates and arguments and the correlations between arguments at most, while the correlations between predicates have been neglected for a long time. High-order features and structure learning were very common in modeling such correlations before the neural network era. In this paper, we introduce a high-order graph structure for the neural semantic role labeling model, which enables the model to explicitly consider not only the isolated predicate-argument pairs but also the interaction between the predicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009 benchmark show that the high-order structural learning techniques are beneficial to the strong performing SRL models and further boost our baseline to achieve new state-of-the-art results.

</details>

<details>

<summary>2020-10-09 18:33:18 - Structured Attention for Unsupervised Dialogue Structure Induction</summary>

- *Liang Qiu, Yizhou Zhao, Weiyan Shi, Yuan Liang, Feng Shi, Tao Yuan, Zhou Yu, Song-Chun Zhu*

- `2009.08552v2` - [abs](http://arxiv.org/abs/2009.08552v2) - [pdf](http://arxiv.org/pdf/2009.08552v2)

> Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.

</details>

<details>

<summary>2020-10-09 22:42:38 - Paying down metadata debt: learning the representation of concepts using topic models</summary>

- *Jiahao Chen, Manuela Veloso*

- `2010.04836v1` - [abs](http://arxiv.org/abs/2010.04836v1) - [pdf](http://arxiv.org/pdf/2010.04836v1)

> We introduce a data management problem called metadata debt, to identify the mapping between data concepts and their logical representations. We describe how this mapping can be learned using semisupervised topic models based on low-rank matrix factorizations that account for missing and noisy labels, coupled with sparsity penalties to improve localization and interpretability. We introduce a gauge transformation approach that allows us to construct explicit associations between topics and concept labels, and thus assign meaning to topics. We also show how to use this topic model for semisupervised learning tasks like extrapolating from known labels, evaluating possible errors in existing labels, and predicting missing features. We show results from this topic model in predicting subject tags on over 25,000 datasets from Kaggle.com, demonstrating the ability to learn semantically meaningful features.

</details>

<details>

<summary>2020-10-10 06:49:25 - Structured Self-Attention Weights Encode Semantics in Sentiment Analysis</summary>

- *Zhengxuan Wu, Thanh-Son Nguyen, Desmond C. Ong*

- `2010.04922v1` - [abs](http://arxiv.org/abs/2010.04922v1) - [pdf](http://arxiv.org/pdf/2010.04922v1)

> Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing (NLP) models. Very recent work suggests that the self-attention in the Transformer encodes syntactic information; Here, we show that self-attention scores encode semantics by considering sentiment analysis tasks. In contrast to gradient-based feature attribution methods, we propose a simple and effective Layer-wise Attention Tracing (LAT) method to analyze structured attention weights. We apply our method to Transformer models trained on two tasks that have surface dissimilarities, but share common semantics---sentiment analysis of movie reviews and time-series valence prediction in life story narratives. Across both tasks, words with high aggregated attention weights were rich in emotional semantics, as quantitatively validated by an emotion lexicon labeled by human annotators. Our results show that structured attention weights encode rich semantics in sentiment analysis, and match human interpretations of semantics.

</details>

<details>

<summary>2020-10-10 07:13:32 - Cue-word Driven Neural Response Generation with a Shrinking Vocabulary</summary>

- *Qiansheng Wang, Yuxin Liu, Chengguo Lv, Zhen Wang, Guohong Fu*

- `2010.04927v1` - [abs](http://arxiv.org/abs/2010.04927v1) - [pdf](http://arxiv.org/pdf/2010.04927v1)

> Open-domain response generation is the task of generating sensible and informative re-sponses to the source sentence. However, neural models tend to generate safe and mean-ingless responses. While cue-word introducing approaches encourage responses with concrete semantics and have shown tremendous potential, they still fail to explore di-verse responses during decoding. In this paper, we propose a novel but natural approach that can produce multiple cue-words during decoding, and then uses the produced cue-words to drive decoding and shrinks the decoding vocabulary. Thus the neural genera-tion model can explore the full space of responses and discover informative ones with efficiency. Experimental results show that our approach significantly outperforms several strong baseline models with much lower decoding complexity. Especially, our approach can converge to concrete semantics more efficiently during decoding.

</details>

<details>

<summary>2020-10-10 13:47:55 - Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings</summary>

- *Prafull Prakash, Saurabh Kumar Shashidhar, Wenlong Zhao, Subendhu Rongali, Haidar Khan, Michael Kayser*

- `2010.05002v1` - [abs](http://arxiv.org/abs/2010.05002v1) - [pdf](http://arxiv.org/pdf/2010.05002v1)

> The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google Assistant on edge devices with limited memory budgets. We propose to learn compositional code embeddings to greatly reduce the sizes of BERT-base and RoBERTa-base. We also apply the technique to DistilBERT, ALBERT-base, and ALBERT-large, three already compressed BERT variants which attain similar state-of-the-art performances on semantic parsing with much smaller model sizes. We observe 95.15% ~ 98.46% embedding compression rates and 20.47% ~ 34.22% encoder compression rates, while preserving greater than 97.5% semantic parsing performances. We provide the recipe for training and analyze the trade-off between code embedding sizes and downstream performances.

</details>

<details>

<summary>2020-10-10 22:03:58 - Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation</summary>

- *Mehrad Moradshahi, Giovanni Campagna, Sina J. Semnani, Silei Xu, Monica S. Lam*

- `2010.05106v1` - [abs](http://arxiv.org/abs/2010.05106v1) - [pdf](http://arxiv.org/pdf/2010.05106v1)

> We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators.   We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our models achieve an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested.   Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.

</details>

<details>

<summary>2020-10-11 05:26:32 - Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance</summary>

- *Ahmed El-Kishky, Francisco Guzmán*

- `2002.00761v2` - [abs](http://arxiv.org/abs/2002.00761v2) - [pdf](http://arxiv.org/pdf/2002.00761v2)

> Document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Such aligned data can be used for a variety of NLP tasks from training cross-lingual representations to mining parallel data for machine translation. In this paper we develop an unsupervised scoring function that leverages cross-lingual sentence embeddings to compute the semantic distance between documents in different languages. These semantic distances are then used to guide a document alignment algorithm to properly pair cross-lingual web documents across a variety of low, mid, and high-resource language pairs. Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents, we utilize a more tractable greedy algorithm that performs comparably. We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7% on high-resource language pairs, 15% on mid-resource language pairs, and 22% on low-resource language pairs.

</details>

<details>

<summary>2020-10-11 06:07:12 - Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection</summary>

- *Hoang Nguyen, Chenwei Zhang, Congying Xia, Philip S. Yu*

- `2010.02481v2` - [abs](http://arxiv.org/abs/2010.02481v2) - [pdf](http://arxiv.org/pdf/2010.02481v2)

> Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available.

</details>

<details>

<summary>2020-10-11 08:27:07 - Learning Adaptive Language Interfaces through Decomposition</summary>

- *Siddharth Karamcheti, Dorsa Sadigh, Percy Liang*

- `2010.05190v1` - [abs](http://arxiv.org/abs/2010.05190v1) - [pdf](http://arxiv.org/pdf/2010.05190v1)

> Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.

</details>

<details>

<summary>2020-10-11 10:22:03 - GuCNet: A Guided Clustering-based Network for Improved Classification</summary>

- *Ushasi Chaudhuri, Syomantak Chaudhuri, Subhasis Chaudhuri*

- `2010.05212v1` - [abs](http://arxiv.org/abs/2010.05212v1) - [pdf](http://arxiv.org/pdf/2010.05212v1)

> We deal with the problem of semantic classification of challenging and highly-cluttered dataset. We present a novel, and yet a very simple classification technique by leveraging the ease of classifiability of any existing well separable dataset for guidance. Since the guide dataset which may or may not have any semantic relationship with the experimental dataset, forms well separable clusters in the feature set, the proposed network tries to embed class-wise features of the challenging dataset to those distinct clusters of the guide set, making them more separable. Depending on the availability, we propose two types of guide sets: one using texture (image) guides and another using prototype vectors representing cluster centers. Experimental results obtained on the challenging benchmark RSSCN, LSUN, and TU-Berlin datasets establish the efficacy of the proposed method as we outperform the existing state-of-the-art techniques by a considerable margin.

</details>

<details>

<summary>2020-10-11 10:22:10 - Exploiting Knowledge Graphs for Facilitating Product/Service Discovery</summary>

- *Sarika Jain*

- `2010.05213v1` - [abs](http://arxiv.org/abs/2010.05213v1) - [pdf](http://arxiv.org/pdf/2010.05213v1)

> Most of the existing techniques to product discovery rely on syntactic approaches, thus ignoring valuable and specific semantic information of the underlying standards during the process. The product data comes from different heterogeneous sources and formats giving rise to the problem of interoperability. Above all, due to the continuously increasing influx of data, the manual labeling is getting costlier. Integrating the descriptions of different products into a single representation requires organizing all the products across vendors in a single taxonomy. Practically relevant and quality product categorization standards are still limited in number; and that too in academic research projects where we can majorly see only prototypes as compared to industry. This work presents a cost-effective solution for e-commerce on the Data Web by employing an unsupervised approach for data classification and exploiting the knowledge graphs for matching. The proposed architecture describes available products in web ontology language OWL and stores them in a triple store. User input specifications for certain products are matched against the available product categories to generate a knowledge graph. This mullti-phased top-down approach to develop and improve existing, if any, tailored product recommendations will be able to connect users with the exact product/service of their choice.

</details>

<details>

<summary>2020-10-11 12:26:20 - Quantifying the Contextualization of Word Representations with Semantic Class Probing</summary>

- *Mengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh, Hinrich Schütze*

- `2004.12198v2` - [abs](http://arxiv.org/abs/2004.12198v2) - [pdf](http://arxiv.org/pdf/2004.12198v2)

> Pretrained language models have achieved a new state of the art on many NLP tasks, but there are still many open questions about how and why they work so well. We investigate the contextualization of words in BERT. We quantify the amount of contextualization, i.e., how well words are interpreted in context, by studying the extent to which semantic classes of a word can be inferred from its contextualized embeddings. Quantifying contextualization helps in understanding and utilizing pretrained language models. We show that top layer representations achieve high accuracy inferring semantic classes; that the strongest contextualization effects occur in the lower layers; that local context is mostly sufficient for semantic class inference; and that top layer representations are more task-specific after finetuning while lower layer representations are more transferable. Finetuning uncovers task related features, but pretrained knowledge is still largely preserved.

</details>

<details>

<summary>2020-10-11 18:35:03 - H2O-Net: Self-Supervised Flood Segmentation via Adversarial Domain Adaptation and Label Refinement</summary>

- *Peri Akiva, Matthew Purri, Kristin Dana, Beth Tellman, Tyler Anderson*

- `2010.05309v1` - [abs](http://arxiv.org/abs/2010.05309v1) - [pdf](http://arxiv.org/pdf/2010.05309v1)

> Accurate flood detection in near real time via high resolution, high latency satellite imagery is essential to prevent loss of lives by providing quick and actionable information. Instruments and sensors useful for flood detection are only available in low resolution, low latency satellites with region re-visit periods of up to 16 days, making flood alerting systems that use such satellites unreliable. This work presents H2O-Network, a self supervised deep learning method to segment floods from satellites and aerial imagery by bridging domain gap between low and high latency satellite and coarse-to-fine label refinement. H2O-Net learns to synthesize signals highly correlative with water presence as a domain adaptation step for semantic segmentation in high resolution satellite imagery. Our work also proposes a self-supervision mechanism, which does not require any hand annotation, used during training to generate high quality ground truth data. We demonstrate that H2O-Net outperforms the state-of-the-art semantic segmentation methods on satellite imagery by 10% and 12% pixel accuracy and mIoU respectively for the task of flood segmentation. We emphasize the generalizability of our model by transferring model weights trained on satellite imagery to drone imagery, a highly different sensor and domain.

</details>

<details>

<summary>2020-10-12 02:19:16 - VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles</summary>

- *Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, Rui Yan*

- `2010.05406v1` - [abs](http://arxiv.org/abs/2010.05406v1) - [pdf](http://arxiv.org/pdf/2010.05406v1)

> A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.

</details>

<details>

<summary>2020-10-12 03:34:44 - It's not a Non-Issue: Negation as a Source of Error in Machine Translation</summary>

- *Md Mosharaf Hossain, Antonios Anastasopoulos, Eduardo Blanco, Alexis Palmer*

- `2010.05432v1` - [abs](http://arxiv.org/abs/2010.05432v1) - [pdf](http://arxiv.org/pdf/2010.05432v1)

> As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigate whether translating negation is an issue for modern MT systems using 17 translation directions as test bed. Through thorough analysis, we find that indeed the presence of negation can significantly impact downstream quality, in some cases resulting in quality reductions of more than 60%. We also provide a linguistically motivated analysis that directly explains the majority of our findings. We release our annotations and code to replicate our analysis here: https://github.com/mosharafhossain/negation-mt.

</details>

<details>

<summary>2020-10-12 04:26:46 - Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation</summary>

- *Fahimeh Saleh, Wray Buntine, Gholamreza Haffari*

- `2010.05445v1` - [abs](http://arxiv.org/abs/2010.05445v1) - [pdf](http://arxiv.org/pdf/2010.05445v1)

> Scarcity of parallel sentence-pairs poses a significant hurdle for training high-quality Neural Machine Translation (NMT) models in bilingually low-resource scenarios. A standard approach is transfer learning, which involves taking a model trained on a high-resource language-pair and fine-tuning it on the data of the low-resource MT condition of interest. However, it is not clear generally which high-resource language-pair offers the best transfer learning for the target MT setting. Furthermore, different transferred models may have complementary semantic and/or syntactic strengths, hence using only one model may be sub-optimal. In this paper, we tackle this problem using knowledge distillation, where we propose to distill the knowledge of ensemble of teacher models to a single student model. As the quality of these teacher models varies, we propose an effective adaptive knowledge distillation approach to dynamically adjust the contribution of the teacher models during the distillation process. Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvement compared to strong baselines.

</details>

<details>

<summary>2020-10-12 05:45:44 - COGS: A Compositional Generalization Challenge Based on Semantic Interpretation</summary>

- *Najoung Kim, Tal Linzen*

- `2010.05465v1` - [abs](http://arxiv.org/abs/2010.05465v1) - [pdf](http://arxiv.org/pdf/2010.05465v1)

> Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99%), but generalization accuracy was substantially lower (16--35%) and showed high sensitivity to random seed ($\pm$6--8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.

</details>

<details>

<summary>2020-10-12 05:58:09 - TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation</summary>

- *Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben Swift, Hanna Suominen, Hongdong Li*

- `2010.05468v1` - [abs](http://arxiv.org/abs/2010.05468v1) - [pdf](http://arxiv.org/pdf/2010.05468v1)

> Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of signvideos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.

</details>

<details>

<summary>2020-10-12 06:07:39 - Evaluation of Siamese Networks for Semantic Code Search</summary>

- *Raunak Sinha, Utkarsh Desai, Srikanth Tamilselvam, Senthil Mani*

- `2011.01043v1` - [abs](http://arxiv.org/abs/2011.01043v1) - [pdf](http://arxiv.org/pdf/2011.01043v1)

> With the increase in the number of open repositories and discussion forums, the use of natural language for semantic code search has become increasingly common. The accuracy of the results returned by such systems, however, can be low due to 1) limited shared vocabulary between code and user query and 2) inadequate semantic understanding of user query and its relation to code syntax. Siamese networks are well suited to learning such joint relations between data, but have not been explored in the context of code search. In this work, we evaluate Siamese networks for this task by exploring multiple extraction network architectures. These networks independently process code and text descriptions before passing them to a Siamese network to learn embeddings in a common space. We experiment on two different datasets and discover that Siamese networks can act as strong regularizers on networks that extract rich information from code and text, which in turn helps achieve impressive performance on code search beating previous baselines on $2$ programming languages. We also analyze the embedding space of these networks and provide directions to fully leverage the power of Siamese networks for semantic code search.

</details>

<details>

<summary>2020-10-12 08:06:12 - A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge Graph</summary>

- *Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou*

- `2010.05511v1` - [abs](http://arxiv.org/abs/2010.05511v1) - [pdf](http://arxiv.org/pdf/2010.05511v1)

> Generating a vivid, novel, and diverse essay with only several given topic words is a challenging task of natural language generation. In previous work, there are two problems left unsolved: neglect of sentiment beneath the text and insufficient utilization of topic-related knowledge. Therefore, we propose a novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational autoencoder (CVAE) framework. We firstly inject the sentiment information into the generator for controlling sentiment for each sentence, which leads to various generated essays. Then we design a Topic Knowledge Graph enhanced decoder. Unlike existing models that use knowledge entities separately, our model treats the knowledge graph as a whole and encodes more structured, connected semantic information in the graph to generate a more relevant essay. Experimental results show that our SCTKG can generate sentiment controllable essays and outperform the state-of-the-art approach in terms of topic relevance, fluency, and diversity on both automatic and human evaluation.

</details>

<details>

<summary>2020-10-12 08:06:33 - Automatic Quantification of Settlement Damage using Deep Learning of Satellite Images</summary>

- *Lili Lu, Weisi Guo*

- `2010.05512v1` - [abs](http://arxiv.org/abs/2010.05512v1) - [pdf](http://arxiv.org/pdf/2010.05512v1)

> Humanitarian disasters and political violence cause significant damage to our living space. The reparation cost to homes, infrastructure, and the ecosystem is often difficult to quantify in real-time. Real-time quantification is critical to both informing relief operations, but also planning ahead for rebuilding. Here, we use satellite images before and after major crisis around the world to train a robust baseline Residual Network (ResNet) and a disaster quantification Pyramid Scene Parsing Network (PSPNet). ResNet offers robustness to poor image quality and can identify areas of destruction with high accuracy (92\%), whereas PSPNet offers contextualised quantification of built environment damage with good accuracy (84\%). As there are multiple damage dimensions to consider (e.g. economic loss and fatalities), we fit a multi-linear regression model to quantify the overall damage. To validate our combined system of deep learning and regression modeling, we successfully match our prediction to the ongoing recovery in the 2020 Beirut port explosion. These innovations provide a better quantification of overall disaster magnitude and inform intelligent humanitarian systems of unfolding disasters.

</details>

<details>

<summary>2020-10-12 08:17:56 - Unsupervised Semantic Aggregation and Deformable Template Matching for Semi-Supervised Learning</summary>

- *Tao Han, Junyu Gao, Yuan Yuan, Qi Wang*

- `2010.05517v1` - [abs](http://arxiv.org/abs/2010.05517v1) - [pdf](http://arxiv.org/pdf/2010.05517v1)

> Unlabeled data learning has attracted considerable attention recently. However, it is still elusive to extract the expected high-level semantic feature with mere unsupervised learning. In the meantime, semi-supervised learning (SSL) demonstrates a promising future in leveraging few samples. In this paper, we combine both to propose an Unsupervised Semantic Aggregation and Deformable Template Matching (USADTM) framework for SSL, which strives to improve the classification performance with few labeled data and then reduce the cost in data annotating. Specifically, unsupervised semantic aggregation based on Triplet Mutual Information (T-MI) loss is explored to generate semantic labels for unlabeled data. Then the semantic labels are aligned to the actual class by the supervision of labeled data. Furthermore, a feature pool that stores the labeled samples is dynamically updated to assign proxy labels for unlabeled data, which are used as targets for cross-entropy minimization. Extensive experiments and analysis across four standard semi-supervised learning benchmarks validate that USADTM achieves top performance (e.g., 90.46$\%$ accuracy on CIFAR-10 with 40 labels and 95.20$\%$ accuracy with 250 labels). The code is released at https://github.com/taohan10200/USADTM.

</details>

<details>

<summary>2020-10-12 09:36:24 - Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards</summary>

- *Rahul Aralikatte, Mostafa Abdou, Heather Lent, Daniel Hershcovich, Anders Søgaard*

- `2010.05567v1` - [abs](http://arxiv.org/abs/2010.05567v1) - [pdf](http://arxiv.org/pdf/2010.05567v1)

> Coreference resolution and semantic role labeling are NLP tasks that capture different aspects of semantics, indicating respectively, which expressions refer to the same entity, and what semantic roles expressions serve in the sentence. However, they are often closely interdependent, and both generally necessitate natural language understanding. Do they form a coherent abstract representation of documents? We present a neural network architecture for joint coreference resolution and semantic role labeling for English, and train graph neural networks to model the 'coherence' of the combined shallow semantic graph. Using the resulting coherence score as a reward for our joint semantic analyzer, we use reinforcement learning to encourage global coherence over the document and between semantic annotations. This leads to improvements on both tasks in multiple datasets from different domains, and across a range of encoders of different expressivity, calling, we believe, for a more holistic approach to semantics in NLP.

</details>

<details>

<summary>2020-10-12 09:49:27 - Meta-Context Transformers for Domain-Specific Response Generation</summary>

- *Debanjana Kar, Suranjana Samanta, Amar Prakash Azad*

- `2010.05572v1` - [abs](http://arxiv.org/abs/2010.05572v1) - [pdf](http://arxiv.org/pdf/2010.05572v1)

> Despite the tremendous success of neural dialogue models in recent years, it suffers a lack of relevance, diversity, and some times coherence in generated responses. Lately, transformer-based models, such as GPT-2, have revolutionized the landscape of dialogue generation by capturing the long-range structures through language modeling. Though these models have exhibited excellent language coherence, they often lack relevance and terms when used for domain-specific response generation. In this paper, we present DSRNet (Domain Specific Response Network), a transformer-based model for dialogue response generation by reinforcing domain-specific attributes. In particular, we extract meta attributes from context and infuse them with the context utterances for better attention over domain-specific key terms and relevance. We study the use of DSRNet in a multi-turn multi-interlocutor environment for domain-specific response generation. In our experiments, we evaluate DSRNet on Ubuntu dialogue datasets, which are mainly composed of various technical domain related dialogues for IT domain issue resolutions and also on CamRest676 dataset, which contains restaurant domain conversations. Trained with maximum likelihood objective, our model shows significant improvement over the state-of-the-art for multi-turn dialogue systems supported by better BLEU and semantic similarity (BertScore) scores. Besides, we also observe that the responses produced by our model carry higher relevance due to the presence of domain-specific key attributes that exhibit better overlap with the attributes of the context. Our analysis shows that the performance improvement is mostly due to the infusion of key terms along with dialogues which result in better attention over domain-relevant terms. Other contributing factors include joint modeling of dialogue context with the domain-specific meta attributes and topics.

</details>

<details>

<summary>2020-10-12 10:59:37 - Rethinking Experience Replay: a Bag of Tricks for Continual Learning</summary>

- *Pietro Buzzega, Matteo Boschini, Angelo Porrello, Simone Calderara*

- `2010.05595v1` - [abs](http://arxiv.org/abs/2010.05595v1) - [pdf](http://arxiv.org/pdf/2010.05595v1)

> In Continual Learning, a Neural Network is trained on a stream of data whose distribution shifts over time. Under these assumptions, it is especially challenging to improve on classes appearing later in the stream while remaining accurate on previous ones. This is due to the infamous problem of catastrophic forgetting, which causes a quick performance degradation when the classifier focuses on learning new categories. Recent literature proposed various approaches to tackle this issue, often resorting to very sophisticated techniques. In this work, we show that naive rehearsal can be patched to achieve similar performance. We point out some shortcomings that restrain Experience Replay (ER) and propose five tricks to mitigate them. Experiments show that ER, thus enhanced, displays an accuracy gain of 51.2 and 26.9 percentage points on the CIFAR-10 and CIFAR-100 datasets respectively (memory buffer size 1000). As a result, it surpasses current state-of-the-art rehearsal-based methods.

</details>

<details>

<summary>2020-10-12 12:14:05 - Thieves on Sesame Street! Model Extraction of BERT-based APIs</summary>

- *Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, Mohit Iyyer*

- `1910.12366v3` - [abs](http://arxiv.org/abs/1910.12366v3) - [pdf](http://arxiv.org/pdf/1910.12366v3)

> We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.

</details>

<details>

<summary>2020-10-12 12:34:58 - Improving Compositional Generalization in Semantic Parsing</summary>

- *Inbar Oren, Jonathan Herzig, Nitish Gupta, Matt Gardner, Jonathan Berant*

- `2010.05647v1` - [abs](http://arxiv.org/abs/2010.05647v1) - [pdf](http://arxiv.org/pdf/2010.05647v1)

> Generalization of models to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in semantic parsing, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional generalization. We find that the following factors improve compositional generalization: (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.

</details>

<details>

<summary>2020-10-12 13:31:01 - Reformulating Unsupervised Style Transfer as Paraphrase Generation</summary>

- *Kalpesh Krishna, John Wieting, Mohit Iyyer*

- `2010.05700v1` - [abs](http://arxiv.org/abs/2010.05700v1) - [pdf](http://arxiv.org/pdf/2010.05700v1)

> Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.

</details>

<details>

<summary>2020-10-12 14:24:01 - Probing Pretrained Language Models for Lexical Semantics</summary>

- *Ivan Vulić, Edoardo Maria Ponti, Robert Litschko, Goran Glavaš, Anna Korhonen*

- `2010.05731v1` - [abs](http://arxiv.org/abs/2010.05731v1) - [pdf](http://arxiv.org/pdf/2010.05731v1)

> The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.

</details>

<details>

<summary>2020-10-12 14:32:39 - Using Type Information to Improve Entity Coreference Resolution</summary>

- *Sopan Khosla, Carolyn Rose*

- `2010.05738v1` - [abs](http://arxiv.org/abs/2010.05738v1) - [pdf](http://arxiv.org/pdf/2010.05738v1)

> Coreference resolution (CR) is an essential part of discourse analysis. Most recently, neural approaches have been proposed to improve over SOTA models from earlier paradigms. So far none of the published neural models leverage external semantic knowledge such as type information. This paper offers the first such model and evaluation, demonstrating modest gains in accuracy by introducing either gold standard or predicted types. In the proposed approach, type information serves both to (1) improve mention representation and (2) create a soft type consistency check between coreference candidate mentions. Our evaluation covers two different grain sizes of types over four different benchmark corpora.

</details>

<details>

<summary>2020-10-12 18:36:43 - Adversarial Examples for Models of Code</summary>

- *Noam Yefet, Uri Alon, Eran Yahav*

- `1910.07517v5` - [abs](http://arxiv.org/abs/1910.07517v5) - [pdf](http://arxiv.org/pdf/1910.07517v5)

> Neural models of code have shown impressive results when performing tasks such as predicting method names and identifying certain kinds of bugs. We show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code using adversarial examples. The main idea of our approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program's semantics, thereby creating an adversarial example. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model's inputs, while holding the model weights constant, and following the gradients to slightly modify the input code. We show that our DAMP attack is effective across three neural architectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#. Our evaluations demonstrate that DAMP has up to 89% success rate in changing a prediction to the adversary's choice (a targeted attack) and a success rate of up to 94% in changing a given prediction to any incorrect prediction (a non-targeted attack). To defend a model against such attacks, we empirically examine a variety of possible defenses and discuss their trade-offs. We show that some of these defenses can dramatically drop the success rate of the attacker, with a minor penalty of 2% relative degradation in accuracy when they are not performing under attack. Our code, data, and trained models are available at https://github.com/tech-srl/adversarial-examples .

</details>

<details>

<summary>2020-10-12 18:55:22 - Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation</summary>

- *Rajiv Movva, Jason Y. Zhao*

- `2009.13270v2` - [abs](http://arxiv.org/abs/2009.13270v2) - [pdf](http://arxiv.org/pdf/2009.13270v2)

> Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model's learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.

</details>

<details>

<summary>2020-10-13 01:14:14 - FILM: A Fast, Interpretable, and Low-rank Metric Learning Approach for Sentence Matching</summary>

- *Xiangru Tang, Alan Aw*

- `2010.05523v2` - [abs](http://arxiv.org/abs/2010.05523v2) - [pdf](http://arxiv.org/pdf/2010.05523v2)

> Detection of semantic similarity plays a vital role in sentence matching. It requires to learn discriminative representations of natural language. Recently, owing to more and more sophisticated model architecture, impressive progress has been made, along with a time-consuming training process and not-interpretable inference. To alleviate this problem, we explore a metric learning approach, named FILM (Fast, Interpretable, and Low-rank Metric learning) to efficiently find a high discriminative projection of the high-dimensional data. We construct this metric learning problem as a manifold optimization problem and solve it with the Cayley transformation method with the Barzilai-Borwein step size. In experiments, we apply FILM with triplet loss minimization objective to the Quora Challenge and Semantic Textual Similarity (STS) Task. The results demonstrate that the FILM method achieves superior performance as well as the fastest computation speed, which is consistent with our theoretical analysis of time complexity.

</details>

<details>

<summary>2020-10-13 01:48:52 - Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching</summary>

- *Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork*

- `2004.12297v2` - [abs](http://arxiv.org/abs/2004.12297v2) - [pdf](http://arxiv.org/pdf/2004.12297v2)

> Many natural language processing and information retrieval problems can be formalized as the task of semantic matching. Existing work in this area has been largely focused on matching between short texts (e.g., question answering), or between a short and a long text (e.g., ad-hoc retrieval). Semantic matching between long-form documents, which has many important applications like news recommendation, related article recommendation and document clustering, is relatively less explored and needs more research effort. In recent years, self-attention based models like Transformers and BERT have achieved state-of-the-art performance in the task of text matching. These models, however, are still limited to short text like a few sentences or one paragraph due to the quadratic computational complexity of self-attention with respect to input text length. In this paper, we address the issue by proposing the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for long-form document matching. Our model contains several innovations to adapt self-attention models for longer text input. In order to better capture sentence level semantic relations within a document, we pre-train the model with a novel masked sentence block language modeling task in addition to the masked word language modeling task used by BERT. Our experimental results on several benchmark datasets for long-form document matching show that our proposed SMITH model outperforms the previous state-of-the-art models including hierarchical attention, multi-depth attention-based hierarchical recurrent neural network, and BERT. Comparing to BERT based baselines, our model is able to increase maximum input text length from 512 to 2048. We will open source a Wikipedia based benchmark dataset, code and a pre-trained checkpoint to accelerate future research on long-form document matching.

</details>

<details>

<summary>2020-10-13 02:27:05 - Asking Crowdworkers to Write Entailment Examples: The Best of Bad Options</summary>

- *Clara Vania, Ruijie Chen, Samuel R. Bowman*

- `2010.06122v1` - [abs](http://arxiv.org/abs/2010.06122v1) - [pdf](http://arxiv.org/pdf/2010.06122v1)

> Large-scale natural language inference (NLI) datasets such as SNLI or MNLI have been created by asking crowdworkers to read a premise and write three new hypotheses, one for each possible semantic relationships (entailment, contradiction, and neutral). While this protocol has been used to create useful benchmark data, it remains unclear whether the writing-based annotation protocol is optimal for any purpose, since it has not been evaluated directly. Furthermore, there is ample evidence that crowdworker writing can introduce artifacts in the data. We investigate two alternative protocols which automatically create candidate (premise, hypothesis) pairs for annotators to label. Using these protocols and a writing-based baseline, we collect several new English NLI datasets of over 3k examples each, each using a fixed amount of annotator time, but a varying number of examples to fit that time budget. Our experiments on NLI and transfer learning show negative results: None of the alternative protocols outperforms the baseline in evaluations of generalization within NLI or on transfer to outside target tasks. We conclude that crowdworker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes.

</details>

<details>

<summary>2020-10-13 02:53:52 - BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance</summary>

- *Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang, Yaohong Jin*

- `2010.06133v1` - [abs](http://arxiv.org/abs/2010.06133v1) - [pdf](http://arxiv.org/pdf/2010.06133v1)

> Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for various NLP tasks. %motivated by the intuition that different NLP tasks require different levels of linguistic knowledge contained in the intermediate layers of BERT. In addition, we leverage Earth Mover's Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables the effective matching for many-to-many layer mapping. %EMD can be applied to network layers with different sizes and effectively measures semantic distance between the teacher network and student network. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model's performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression.

</details>

<details>

<summary>2020-10-13 02:58:25 - Matching Guided Distillation</summary>

- *Kaiyu Yue, Jiangfan Deng, Feng Zhou*

- `2008.09958v2` - [abs](http://arxiv.org/abs/2008.09958v2) - [pdf](http://arxiv.org/pdf/2008.09958v2)

> Feature distillation is an effective way to improve the performance for a smaller student model, which has fewer parameters and lower computation cost compared to the larger teacher model. Unfortunately, there is a common obstacle - the gap in semantic feature structure between the intermediate features of teacher and student. The classic scheme prefers to transform intermediate features by adding the adaptation module, such as naive convolutional, attention-based or more complicated one. However, this introduces two problems: a) The adaptation module brings more parameters into training. b) The adaptation module with random initialization or special transformation isn't friendly for distilling a pre-trained student. In this paper, we present Matching Guided Distillation (MGD) as an efficient and parameter-free manner to solve these problems. The key idea of MGD is to pose matching the teacher channels with students' as an assignment problem. We compare three solutions of the assignment problem to reduce channels from teacher features with partial distillation loss. The overall training takes a coordinate-descent approach between two optimization objects - assignments update and parameters update. Since MGD only contains normalization or pooling operations with negligible computation cost, it is flexible to plug into network with other distillation methods.

</details>

<details>

<summary>2020-10-13 10:09:15 - SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning</summary>

- *Gencer Sumbul, Sonali Nayak, Begüm Demir*

- `2006.08432v2` - [abs](http://arxiv.org/abs/2006.08432v2) - [pdf](http://arxiv.org/pdf/2006.08432v2)

> Deep neural networks (DNNs) have been recently found popular for image captioning problems in remote sensing (RS). Existing DNN based approaches rely on the availability of a training set made up of a high number of RS images with their captions. However, captions of training images may contain redundant information (they can be repetitive or semantically similar to each other), resulting in information deficiency while learning a mapping from the image domain to the language domain. To overcome this limitation, in this paper, we present a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC) approach. The proposed approach consists of three main steps. The first step obtains the standard image captions by jointly exploiting convolutional neural networks (CNNs) with long short-term memory (LSTM) networks. The second step, unlike the existing RS image captioning methods, summarizes the ground-truth captions of each training image into a single caption by exploiting sequence to sequence neural networks and eliminates the redundancy present in the training set. The third step automatically defines the adaptive weights associated to each RS image to combine the standard captions with the summarized captions based on the semantic content of the image. This is achieved by a novel adaptive weighting strategy defined in the context of LSTM networks. Experimental results obtained on the RSCID, UCM-Captions and Sydney-Captions datasets show the effectiveness of the proposed approach compared to the state-of-the-art RS image captioning approaches. The code of the proposed approach is publicly available at https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.

</details>

<details>

<summary>2020-10-13 10:39:14 - BRUMS at SemEval-2020 Task 12 : Transformer based Multilingual Offensive Language Identification in Social Media</summary>

- *Tharindu Ranasinghe, Hansi Hettiarachchi*

- `2010.06278v1` - [abs](http://arxiv.org/abs/2010.06278v1) - [pdf](http://arxiv.org/pdf/2010.06278v1)

> In this paper, we describe the team \textit{BRUMS} entry to OffensEval 2: Multilingual Offensive Language Identification in Social Media in SemEval-2020. The OffensEval organizers provided participants with annotated datasets containing posts from social media in Arabic, Danish, English, Greek and Turkish. We present a multilingual deep learning model to identify offensive language in social media. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility between languages.

</details>

<details>

<summary>2020-10-13 10:42:41 - S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via Multi-View Consistency</summary>

- *Mel Vecerik, Jean-Baptiste Regli, Oleg Sushkov, David Barker, Rugile Pevceviciute, Thomas Rothörl, Christopher Schuster, Raia Hadsell, Lourdes Agapito, Jonathan Scholz*

- `2009.14711v2` - [abs](http://arxiv.org/abs/2009.14711v2) - [pdf](http://arxiv.org/pdf/2009.14711v2)

> A robot's ability to act is fundamentally constrained by what it can perceive. Many existing approaches to visual representation learning utilize general-purpose training criteria, e.g. image reconstruction, smoothness in latent space, or usefulness for control, or else make use of large datasets annotated with specific features (bounding boxes, segmentations, etc.). However, both approaches often struggle to capture the fine-detail required for precision tasks on specific objects, e.g. grasping and mating a plug and socket. We argue that these difficulties arise from a lack of geometric structure in these models. In this work we advocate semantic 3D keypoints as a visual representation, and present a semi-supervised training objective that can allow instance or category-level keypoints to be trained to 1-5 millimeter-accuracy with minimal supervision. Furthermore, unlike local texture-based approaches, our model integrates contextual information from a large area and is therefore robust to occlusion, noise, and lack of discernible texture. We demonstrate that this ability to locate semantic keypoints enables high level scripting of human understandable behaviours. Finally we show that these keypoints provide a good way to define reward functions for reinforcement learning and are a good representation for training agents.

</details>

<details>

<summary>2020-10-13 10:48:15 - RGCL at SemEval-2020 Task 6: Neural Approaches to Definition Extraction</summary>

- *Tharindu Ranasinghe, Alistair Plum, Constantin Orasan, Ruslan Mitkov*

- `2010.06281v1` - [abs](http://arxiv.org/abs/2010.06281v1) - [pdf](http://arxiv.org/pdf/2010.06281v1)

> This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection.

</details>

<details>

<summary>2020-10-13 10:56:09 - Land Cover Semantic Segmentation Using ResUNet</summary>

- *Vasilis Pollatos, Loukas Kouvaras, Eleni Charou*

- `2010.06285v1` - [abs](http://arxiv.org/abs/2010.06285v1) - [pdf](http://arxiv.org/pdf/2010.06285v1)

> In this paper we present our work on developing an automated system for land cover classification. This system takes a multiband satellite image of an area as input and outputs the land cover map of the area at the same resolution as the input. For this purpose convolutional machine learning models were trained in the task of predicting the land cover semantic segmentation of satellite images. This is a case of supervised learning. The land cover label data were taken from the CORINE Land Cover inventory and the satellite images were taken from the Copernicus hub. As for the model, U-Net architecture variations were applied. Our area of interest are the Ionian islands (Greece). We created a dataset from scratch covering this particular area. In addition, transfer learning from the BigEarthNet dataset [1] was performed. In [1] simple classification of satellite images into the classes of CLC is performed but not segmentation as we do. However, their models have been trained into a dataset much bigger than ours, so we applied transfer learning using their pretrained models as the first part of out network, utilizing the ability these networks have developed to extract useful features from the satellite images (we transferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer learning other techniques were applied in order to overcome the limitations set by the small size of our area of interest. We used data augmentation (cutting images into overlapping patches, applying random transformations such as rotations and flips) and cross validation. The results are tested on the 3 CLC class hierarchy levels and a comparative study is made on the results of different approaches.

</details>

<details>

<summary>2020-10-13 11:34:57 - Automating App Review Response Generation Based on Contextual Knowledge</summary>

- *Cuiyun Gao, Wenjie Zhou, Xin Xia, David Lo, Qi Xie, Michael R. Lyu*

- `2010.06301v1` - [abs](http://arxiv.org/abs/2010.06301v1) - [pdf](http://arxiv.org/pdf/2010.06301v1)

> User experience of mobile apps is an essential ingredient that can influence the audience volumes and app revenue. To ensure good user experience and assist app development, several prior studies resort to analysis of app reviews, a type of app repository that directly reflects user opinions about the apps. Accurately responding to the app reviews is one of the ways to relieve user concerns and thus improve user experience. However, the response quality of the existing method relies on the pre-extracted features from other tools, including manually-labelled keywords and predicted review sentiment, which may hinder the generalizability and flexibility of the method. In this paper, we propose a novel end-to-end neural network approach, named CoRe, with the contextual knowledge naturally incorporated and without involving external tools. Specifically, CoRe integrates two types of contextual knowledge in the training corpus, including official app descriptions from app store and responses of the retrieved semantically similar reviews, for enhancing the relevance and accuracy of the generated review responses. Experiments on practical review data show that CoRe can outperform the state-of-the-art method by 11.53% in terms of BLEU-4, an accuracy metric that is widely used to evaluate text generation systems.

</details>

<details>

<summary>2020-10-13 13:21:18 - Deep Evolution for Facial Emotion Recognition</summary>

- *Emmanuel Dufourq, Bruce A. Bassett*

- `2009.14194v2` - [abs](http://arxiv.org/abs/2009.14194v2) - [pdf](http://arxiv.org/pdf/2009.14194v2)

> Deep facial expression recognition faces two challenges that both stem from the large number of trainable parameters: long training times and a lack of interpretability. We propose a novel method based on evolutionary algorithms, that deals with both challenges by massively reducing the number of trainable parameters, whilst simultaneously retaining classification performance, and in some cases achieving superior performance. We are robustly able to reduce the number of parameters on average by 95% (e.g. from 2M to 100k parameters) with no loss in classification accuracy. The algorithm learns to choose small patches from the image, relative to the nose, which carry the most important information about emotion, and which coincide with typical human choices of important features. Our work implements a novel form attention and shows that evolutionary algorithms are a valuable addition to machine learning in the deep learning era, both for reducing the number of parameters for facial expression recognition and for providing interpretable features that can help reduce bias.

</details>

<details>

<summary>2020-10-13 13:33:38 - MALA: Cross-Domain Dialogue Generation with Action Learning</summary>

- *Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang*

- `1912.08442v2` - [abs](http://arxiv.org/abs/1912.08442v2) - [pdf](http://arxiv.org/pdf/1912.08442v2)

> Response generation for task-oriented dialogues involves two basic components: dialogue planning and surface realization. These two components, however, have a discrepancy in their objectives, i.e., task completion and language quality. To deal with such discrepancy, conditioned response generation has been introduced where the generation process is factorized into action decision and language generation via explicit action representations. To obtain action representations, recent studies learn latent actions in an unsupervised manner based on the utterance lexical similarity. Such an action learning approach is prone to diversities of language surfaces, which may impinge task completion and language quality. To address this issue, we propose multi-stage adaptive latent action learning (MALA) that learns semantic latent actions by distinguishing the effects of utterances on dialogue progress. We model the utterance effect using the transition of dialogue states caused by the utterance and develop a semantic similarity measurement that estimates whether utterances have similar effects. For learning semantic actions on domains without dialogue states, MsALA extends the semantic similarity measurement across domains progressively, i.e., from aligning shared actions to learning domain-specific actions. Experiments using multi-domain datasets, SMD and MultiWOZ, show that our proposed model achieves consistent improvements over the baselines models in terms of both task completion and language quality.

</details>

<details>

<summary>2020-10-13 14:54:05 - RuSemShift: a dataset of historical lexical semantic change in Russian</summary>

- *Julia Rodina, Andrey Kutuzov*

- `2010.06436v1` - [abs](http://arxiv.org/abs/2010.06436v1) - [pdf](http://arxiv.org/pdf/2010.06436v1)

> We present RuSemShift, a large-scale manually annotated test set for the task of semantic change modeling in Russian for two long-term time period pairs: from the pre-Soviet through the Soviet times and from the Soviet through the post-Soviet times. Target words were annotated by multiple crowd-source workers. The annotation process was organized following the DURel framework and was based on sentence contexts extracted from the Russian National Corpus. Additionally, we report the performance of several distributional approaches on RuSemShift, achieving promising results, which at the same time leave room for other researchers to improve.

</details>

<details>

<summary>2020-10-13 14:59:46 - Automatic Extraction of Urban Outdoor Perception from Geolocated Free-Texts</summary>

- *Frances Santos, Thiago H Silva, Antonio A F Loureiro, Leandro Villas*

- `2010.06444v1` - [abs](http://arxiv.org/abs/2010.06444v1) - [pdf](http://arxiv.org/pdf/2010.06444v1)

> The automatic extraction of urban perception shared by people on location-based social networks (LBSNs) is an important multidisciplinary research goal. One of the reasons is because it facilitates the understanding of the intrinsic characteristics of urban areas in a scalable way, helping to leverage new services. However, content shared on LBSNs is diverse, encompassing several topics, such as politics, sports, culture, religion, and urban perceptions, making the task of content extraction regarding a particular topic very challenging. Considering free-text messages shared on LBSNs, we propose an automatic and generic approach to extract people's perceptions. For that, our approach explores opinions that are spatial-temporal and semantically similar. We exemplify our approach in the context of urban outdoor areas in Chicago, New York City and London. Studying those areas, we found evidence that LBSN data brings valuable information about urban regions. To analyze and validate our outcomes, we conducted a temporal analysis to measure the results' robustness over time. We show that our approach can be helpful to better understand urban areas considering different perspectives. We also conducted a comparative analysis based on a public dataset, which contains volunteers' perceptions regarding urban areas expressed in a controlled experiment. We observe that both results yield a very similar level of agreement.

</details>

<details>

<summary>2020-10-13 15:21:14 - Making Every Label Count: Handling Semantic Imprecision by Integrating Domain Knowledge</summary>

- *Clemens-Alexander Brust, Björn Barz, Joachim Denzler*

- `2010.06469v1` - [abs](http://arxiv.org/abs/2010.06469v1) - [pdf](http://arxiv.org/pdf/2010.06469v1)

> Noisy data, crawled from the web or supplied by volunteers such as Mechanical Turkers or citizen scientists, is considered an alternative to professionally labeled data. There has been research focused on mitigating the effects of label noise. It is typically modeled as inaccuracy, where the correct label is replaced by an incorrect label from the same set. We consider an additional dimension of label noise: imprecision. For example, a non-breeding snow bunting is labeled as a bird. This label is correct, but not as precise as the task requires.   Standard softmax classifiers cannot learn from such a weak label because they consider all classes mutually exclusive, which non-breeding snow bunting and bird are not. We propose CHILLAX (Class Hierarchies for Imprecise Label Learning and Annotation eXtrapolation), a method based on hierarchical classification, to fully utilize labels of any precision.   Experiments on noisy variants of NABirds and ILSVRC2012 show that our method outperforms strong baselines by as much as 16.4 percentage points, and the current state of the art by up to 3.9 percentage points.

</details>

<details>

<summary>2020-10-13 15:32:00 - XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization</summary>

- *Alessandro Raganato, Tommaso Pasini, Jose Camacho-Collados, Mohammad Taher Pilehvar*

- `2010.06478v1` - [abs](http://arxiv.org/abs/2010.06478v1) - [pdf](http://arxiv.org/pdf/2010.06478v1)

> The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.

</details>

<details>

<summary>2020-10-13 17:00:38 - Towards Visually Explaining Similarity Models</summary>

- *Meng Zheng, Srikrishna Karanam, Terrence Chen, Richard J. Radke, Ziyan Wu*

- `2008.06035v2` - [abs](http://arxiv.org/abs/2008.06035v2) - [pdf](http://arxiv.org/pdf/2008.06035v2)

> We consider the problem of visually explaining similarity models, i.e., explaining why a model predicts two images to be similar in addition to producing a scalar score. While much recent work in visual model interpretability has focused on gradient-based attention, these methods rely on a classification module to generate visual explanations. Consequently, they cannot readily explain other kinds of models that do not use or need classification-like loss functions (e.g., similarity models trained with a metric learning loss). In this work, we bridge this crucial gap, presenting a method to generate gradient-based visual attention for image similarity predictors. By relying solely on the learned feature embedding, we show that our approach can be applied to any kind of CNN-based similarity architecture, an important step towards generic visual explainability. We show that our resulting attention maps serve more than just interpretability; they can be infused into the model learning process itself with new trainable constraints. We show that the resulting similarity models perform, and can be visually explained, better than the corresponding baseline models trained without these constraints. We demonstrate our approach using extensive experiments on three different kinds of tasks: generic image retrieval, person re-identification, and low-shot semantic segmentation.

</details>

<details>

<summary>2020-10-13 22:00:31 - CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring</summary>

- *Jiaxin Huang, Yiqing Xie, Yu Meng, Yunyi Zhang, Jiawei Han*

- `2010.06714v1` - [abs](http://arxiv.org/abs/2010.06714v1) - [pdf](http://arxiv.org/pdf/2010.06714v1)

> Taxonomy is not only a fundamental form of knowledge representation, but also crucial to vast knowledge-rich applications, such as question answering and web search. Most existing taxonomy construction methods extract hypernym-hyponym entity pairs to organize a "universal" taxonomy. However, these generic taxonomies cannot satisfy user's specific interest in certain areas and relations. Moreover, the nature of instance taxonomy treats each node as a single word, which has low semantic coverage. In this paper, we propose a method for seed-guided topical taxonomy construction, which takes a corpus and a seed taxonomy described by concept names as input, and constructs a more complete taxonomy based on user's interest, wherein each node is represented by a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill this goal. A relation transferring module learns and transfers the user's interested relation along multiple paths to expand the seed taxonomy structure in width and depth. A concept learning module enriches the semantics of each concept node by jointly embedding the taxonomy and text. Comprehensive experiments conducted on real-world datasets show that Corel generates high-quality topical taxonomies and outperforms all the baselines significantly.

</details>

<details>

<summary>2020-10-13 22:35:42 - On the relationship between class selectivity, dimensionality, and robustness</summary>

- *Matthew L. Leavitt, Ari S. Morcos*

- `2007.04440v2` - [abs](http://arxiv.org/abs/2007.04440v2) - [pdf](http://arxiv.org/pdf/2007.04440v2)

> While the relative trade-offs between sparse and distributed representations in deep neural networks (DNNs) are well-studied, less is known about how these trade-offs apply to representations of semantically-meaningful information. Class selectivity, the variability of a unit's responses across data classes or dimensions, is one way of quantifying the sparsity of semantic representations. Given recent evidence showing that class selectivity can impair generalization, we sought to investigate whether it also confers robustness (or vulnerability) to perturbations of input data. We found that mean class selectivity predicts vulnerability to naturalistic corruptions; networks regularized to have lower levels of class selectivity are more robust to corruption, while networks with higher class selectivity are more vulnerable to corruption, as measured using Tiny ImageNetC and CIFAR10C. In contrast, we found that class selectivity increases robustness to multiple types of gradient-based adversarial attacks. To examine this difference, we studied the dimensionality of the change in the representation due to perturbation, finding that decreasing class selectivity increases the dimensionality of this change for both corruption types, but with a notably larger increase for adversarial attacks. These results demonstrate the causal relationship between selectivity and robustness and provide new insights into the mechanisms of this relationship.

</details>

<details>

<summary>2020-10-13 22:37:29 - "What Are You Trying to Do?" Semantic Typing of Event Processes</summary>

- *Muhao Chen, Hongming Zhang, Haoyu Wang, Dan Roth*

- `2010.06724v1` - [abs](http://arxiv.org/abs/2010.06724v1) - [pdf](http://arxiv.org/pdf/2010.06724v1)

> This paper studies a new cognitively motivated semantic typing task, multi-axis event process typing, that, given an event process, attempts to infer free-form type labels describing (i) the type of action made by the process and (ii) the type of object the process seeks to affect. This task is inspired by computational and cognitive studies of event understanding, which suggest that understanding processes of events is often directed by recognizing the goals, plans or intentions of the protagonist(s). We develop a large dataset containing over 60k event processes, featuring ultra fine-grained typing on both the action and object type axes with very large ($10^3\sim 10^4$) label vocabularies. We then propose a hybrid learning framework, P2GT, which addresses the challenging typing problem with indirect supervision from glosses1and a joint learning-to-rank framework. As our experiments indicate, P2GT supports identifying the intent of processes, as well as the fine semantic type of the affected object. It also demonstrates the capability of handling few-shot cases, and strong generalizability on out-of-domain event processes.

</details>

<details>

<summary>2020-10-14 04:01:54 - A Graph Representation of Semi-structured Data for Web Question Answering</summary>

- *Xingyao Zhang, Linjun Shou, Jian Pei, Ming Gong, Lijie Wen, Daxin Jiang*

- `2010.06801v1` - [abs](http://arxiv.org/abs/2010.06801v1) - [pdf](http://arxiv.org/pdf/2010.06801v1)

> The abundant semi-structured data on the Web, such as HTML-based tables and lists, provide commercial search engines a rich information source for question answering (QA). Different from plain text passages in Web documents, Web tables and lists have inherent structures, which carry semantic correlations among various elements in tables and lists. Many existing studies treat tables and lists as flat documents with pieces of text and do not make good use of semantic information hidden in structures. In this paper, we propose a novel graph representation of Web tables and lists based on a systematic categorization of the components in semi-structured data as well as their relations. We also develop pre-training and reasoning techniques on the graph model for the QA task. Extensive experiments on several real datasets collected from a commercial engine verify the effectiveness of our approach. Our method improves F1 score by 3.90 points over the state-of-the-art baselines.

</details>

<details>

<summary>2020-10-14 06:27:07 - Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems</summary>

- *Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, Liang Lin*

- `2010.06823v1` - [abs](http://arxiv.org/abs/2010.06823v1) - [pdf](http://arxiv.org/pdf/2010.06823v1)

> A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols' semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.

</details>

<details>

<summary>2020-10-14 10:23:48 - PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training</summary>

- *An Tran, Ali Zonoozi, Jagannadan Varadarajan, Hannes Kruppa*

- `2010.06932v1` - [abs](http://arxiv.org/abs/2010.06932v1) - [pdf](http://arxiv.org/pdf/2010.06932v1)

> Road network and building footprint extraction is essential for many applications such as updating maps, traffic regulations, city planning, ride-hailing, disaster response \textit{etc}. Mapping road networks is currently both expensive and labor-intensive. Recently, improvements in image segmentation through the application of deep neural networks has shown promising results in extracting road segments from large scale, high resolution satellite imagery. However, significant challenges remain due to lack of enough labeled training data needed to build models for industry grade applications. In this paper, we propose a two-stage transfer learning technique to improve robustness of semantic segmentation for satellite images that leverages noisy pseudo ground truth masks obtained automatically (without human labor) from crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation that uses focal loss, poly learning rate, and context module. We demonstrate the strengths of our approach through evaluations done on three popular datasets over two tasks, namely, road extraction and building foot-print detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet and DeepGlobe road extraction dataset, respectively.

</details>

<details>

<summary>2020-10-14 12:54:15 - 3D Segmentation Networks for Excessive Numbers of Classes: Distinct Bone Segmentation in Upper Bodies</summary>

- *Eva Schnider, Antal Horváth, Georg Rauter, Azhar Zam, Magdalena Müller-Gerbl, Philippe C. Cattin*

- `2010.07045v1` - [abs](http://arxiv.org/abs/2010.07045v1) - [pdf](http://arxiv.org/pdf/2010.07045v1)

> Segmentation of distinct bones plays a crucial role in diagnosis, planning, navigation, and the assessment of bone metastasis. It supplies semantic knowledge to visualisation tools for the planning of surgical interventions and the education of health professionals. Fully supervised segmentation of 3D data using Deep Learning methods has been extensively studied for many tasks but is usually restricted to distinguishing only a handful of classes. With 125 distinct bones, our case includes many more labels than typical 3D segmentation tasks. For this reason, the direct adaptation of most established methods is not possible. This paper discusses the intricacies of training a 3D segmentation network in a many-label setting and shows necessary modifications in network architecture, loss function, and data augmentation. As a result, we demonstrate the robustness of our method by automatically segmenting over one hundred distinct bones simultaneously in an end-to-end learnt fashion from a CT-scan.

</details>

<details>

<summary>2020-10-14 14:32:38 - Explaining Creative Artifacts</summary>

- *Lav R. Varshney, Nazneen Fatema Rajani, Richard Socher*

- `2010.07126v1` - [abs](http://arxiv.org/abs/2010.07126v1) - [pdf](http://arxiv.org/pdf/2010.07126v1)

> Human creativity is often described as the mental process of combining associative elements into a new form, but emerging computational creativity algorithms may not operate in this manner. Here we develop an inverse problem formulation to deconstruct the products of combinatorial and compositional creativity into associative chains as a form of post-hoc interpretation that matches the human creative process. In particular, our formulation is structured as solving a traveling salesman problem through a knowledge graph of associative elements. We demonstrate our approach using an example in explaining culinary computational creativity where there is an explicit semantic structure, and two examples in language generation where we either extract explicit concepts that map to a knowledge graph or we consider distances in a word embedding space. We close by casting the length of an optimal traveling salesman path as a measure of novelty in creativity.

</details>

<details>

<summary>2020-10-14 17:06:41 - Text Classification Using Label Names Only: A Language Model Self-Training Approach</summary>

- *Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, Jiawei Han*

- `2010.07245v1` - [abs](http://arxiv.org/abs/2010.07245v1) - [pdf](http://arxiv.org/pdf/2010.07245v1)

> Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.

</details>

<details>

<summary>2020-10-14 21:22:45 - Improved Approximation Algorithms for Stochastic-Matching Problems</summary>

- *Marek Adamczyk, Brian Brubach, Fabrizio Grandoni, Karthik A. Sankararaman, Aravind Srinivasan, Pan Xu*

- `2010.08142v1` - [abs](http://arxiv.org/abs/2010.08142v1) - [pdf](http://arxiv.org/pdf/2010.08142v1)

> We consider the Stochastic Matching problem, which is motivated by applications in kidney exchange and online dating. In this problem, we are given an undirected graph. Each edge is assigned a known, independent probability of existence and a positive weight (or profit). We must probe an edge to discover whether or not it exists. Each node is assigned a positive integer called a timeout (or a patience). On this random graph we are executing a process, which probes the edges one-by-one and gradually constructs a matching. The process is constrained in two ways. First, if a probed edge exists, it must be added irrevocably to the matching (the query-commit model). Second, the timeout of a node $v$ upper-bounds the number of edges incident to $v$ that can be probed. The goal is to maximize the expected weight of the constructed matching.   For this problem, Bansal et al. (Algorithmica 2012) provided a $0.33$-approximation algorithm for bipartite graphs and a $0.25$-approximation for general graphs. We improve the approximation factors to $0.39$ and $0.269$, respectively.   The main technical ingredient in our result is a novel way of probing edges according to a not-uniformly-random permutation. Patching this method with an algorithm that works best for large-probability edges (plus additional ideas) leads to our improved approximation factors.

</details>

<details>

<summary>2020-10-14 23:26:28 - From Language to Language-ish: How Brain-Like is an LSTM's Representation of Nonsensical Language Stimuli?</summary>

- *Maryam Hashemzadeh, Greta Kaufeld, Martha White, Andrea E. Martin, Alona Fyshe*

- `2010.07435v1` - [abs](http://arxiv.org/abs/2010.07435v1) - [pdf](http://arxiv.org/pdf/2010.07435v1)

> The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain's reaction to syntactically and semantically sound language stimuli. In this study, we asked: how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain's reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain's activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.

</details>

<details>

<summary>2020-10-14 23:50:25 - A new approach for extracting the conceptual schema of texts based on the linguistic Thematic Progression theory</summary>

- *Elena del Olmo Suárez, Ana María Fernández-Pampillón Cesteros*

- `2010.07440v1` - [abs](http://arxiv.org/abs/2010.07440v1) - [pdf](http://arxiv.org/pdf/2010.07440v1)

> The purpose of this article is to present a new approach for the discovery and labelling of the implicit conceptual schema of texts through the application of the Thematic Progression theory. The underlying conceptual schema is the core component for the generation of summaries that are genuinely consistent with the semantics of the text.

</details>

<details>

<summary>2020-10-15 00:31:15 - Semantic Label Smoothing for Sequence to Sequence Problems</summary>

- *Michal Lukasik, Himanshu Jain, Aditya Krishna Menon, Seungyeon Kim, Srinadh Bhojanapalli, Felix Yu, Sanjiv Kumar*

- `2010.07447v1` - [abs](http://arxiv.org/abs/2010.07447v1) - [pdf](http://arxiv.org/pdf/2010.07447v1)

> Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs. Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence. Unlike these works, in this paper, we propose a technique that smooths over \emph{well formed} relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also \emph{semantically similar}. Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.

</details>

<details>

<summary>2020-10-15 01:15:43 - Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs</summary>

- *Jueqing Lu, Lan Du, Ming Liu, Joanna Dipnall*

- `2010.07459v1` - [abs](http://arxiv.org/abs/2010.07459v1) - [pdf](http://arxiv.org/pdf/2010.07459v1)

> Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations. Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.

</details>

<details>

<summary>2020-10-15 02:10:48 - Dual Inference for Improving Language Understanding and Generation</summary>

- *Shang-Yu Su, Yung-Sung Chuang, Yun-Nung Chen*

- `2010.04246v2` - [abs](http://arxiv.org/abs/2010.04246v2) - [pdf](http://arxiv.org/pdf/2010.04246v2)

> Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance. However, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models. To better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining. The experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.

</details>

<details>

<summary>2020-10-15 05:08:56 - Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs</summary>

- *Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A. Smith, Yejin Choi*

- `2010.07526v1` - [abs](http://arxiv.org/abs/2010.07526v1) - [pdf](http://arxiv.org/pdf/2010.07526v1)

> Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks.

</details>

<details>

<summary>2020-10-15 08:08:16 - Approximate Manifold Defense Against Multiple Adversarial Perturbations</summary>

- *Jay Nandy, Wynne Hsu, Mong Li Lee*

- `2004.02183v2` - [abs](http://arxiv.org/abs/2004.02183v2) - [pdf](http://arxiv.org/pdf/2004.02183v2)

> Existing defenses against adversarial attacks are typically tailored to a specific perturbation type. Using adversarial training to defend against multiple types of perturbation requires expensive adversarial examples from different perturbation types at each training step. In contrast, manifold-based defense incorporates a generative network to project an input sample onto the clean data manifold. This approach eliminates the need to generate expensive adversarial examples while achieving robustness against multiple perturbation types. However, the success of this approach relies on whether the generative network can capture the complete clean data manifold, which remains an open problem for complex input domain. In this work, we devise an approximate manifold defense mechanism, called RBF-CNN, for image classification. Instead of capturing the complete data manifold, we use an RBF layer to learn the density of small image patches. RBF-CNN also utilizes a reconstruction layer that mitigates any minor adversarial perturbations. Further, incorporating our proposed reconstruction process for training improves the adversarial robustness of our RBF-CNN models. Experiment results on MNIST and CIFAR-10 datasets indicate that RBF-CNN offers robustness for multiple perturbations without the need for expensive adversarial training.

</details>

<details>

<summary>2020-10-15 08:54:17 - DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding</summary>

- *Zilong Wang, Mingjie Zhan, Xuebo Liu, Ding Liang*

- `2010.11685v1` - [abs](http://arxiv.org/abs/2010.11685v1) - [pdf](http://arxiv.org/pdf/2010.11685v1)

> Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well, it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detection and handcrafted features in previous works cannot apply to all forms because of their requirements on formats. Therefore, we concentrate on the most elementary components, the key-value pairs, and adopt multimodal methods to extract features. We consider the form structure as a tree-like or graph-like hierarchy of text fragments. The parent-child relation corresponds to the key-value pairs in forms. We utilize the state-of-the-art models and design targeted extraction modules to extract multimodal features from semantic contents, layout information, and visual images. A hybrid fusion method of concatenation and feature shifting is designed to fuse the heterogeneous features and provide an informative joint representation. We adopt an asymmetric algorithm and negative sampling in our model as well. We validate our method on two benchmarks, MedForm and FUNSD, and extensive experiments demonstrate the effectiveness of our method.

</details>

<details>

<summary>2020-10-15 09:18:27 - Normal Forms for (Semantically) Witness-Based Learners in Inductive Inference</summary>

- *Vanja Doskoč, Timo Kötzing*

- `2010.09461v1` - [abs](http://arxiv.org/abs/2010.09461v1) - [pdf](http://arxiv.org/pdf/2010.09461v1)

> We study learners (computable devices) inferring formal languages, a setting referred to as language learning in the limit or inductive inference. In particular, we require the learners we investigate to be witness-based, that is, to justify each of their mind changes. Besides being a natural requirement for a learning task, this restriction deserves special attention as it is a specialization of various important learning paradigms. In particular, with the help of witness-based learning, explanatory learners are shown to be equally powerful under these seemingly incomparable paradigms. Nonetheless, until now, witness-based learners have only been studied sparsely.   In this work, we conduct a thorough study of these learners both when requiring syntactic and semantic convergence and obtain normal forms thereof. In the former setting, we extend known results such that they include witness-based learning and generalize these to hold for a variety of learners. Transitioning to behaviourally correct learning, we also provide normal forms for semantically witness-based learners. Most notably, we show that set-driven globally semantically witness-based learners are equally powerful as their Gold-style semantically conservative counterpart. Such results are key to understanding the, yet undiscovered, mutual relation between various important learning paradigms when learning behaviourally correctly.

</details>

<details>

<summary>2020-10-15 09:27:47 - Learning Languages with Decidable Hypotheses</summary>

- *Julian Berger, Maximilian Böther, Vanja Doskoč, Jonathan Gadea Harder, Nicolas Klodt, Timo Kötzing, Winfried Lötzsch, Jannik Peters, Leon Schiller, Lars Seifert, Armin Wells, Simon Wietheger*

- `2011.09866v1` - [abs](http://arxiv.org/abs/2011.09866v1) - [pdf](http://arxiv.org/pdf/2011.09866v1)

> In language learning in the limit, the most common type of hypothesis is to give an enumerator for a language. This so-called $W$-index allows for naming arbitrary computably enumerable languages, with the drawback that even the membership problem is undecidable. In this paper we use a different system which allows for naming arbitrary decidable languages, namely programs for characteristic functions (called $C$-indices). These indices have the drawback that it is now not decidable whether a given hypothesis is even a legal $C$-index.   In this first analysis of learning with $C$-indices, we give a structured account of the learning power of various restrictions employing $C$-indices, also when compared with $W$-indices. We establish a hierarchy of learning power depending on whether $C$-indices are required (a) on all outputs; (b) only on outputs relevant for the class to be learned and (c) only in the limit as final, correct hypotheses. Furthermore, all these settings are weaker than learning with $W$-indices (even when restricted to classes of computable languages). We analyze all these questions also in relation to the mode of data presentation.   Finally, we also ask about the relation of semantic versus syntactic convergence and derive the map of pairwise relations for these two kinds of convergence coupled with various forms of data presentation.

</details>

<details>

<summary>2020-10-15 09:52:40 - Hierarchical Text Interaction for Rating Prediction</summary>

- *Jiahui Wen, Jingwei Ma, Hongkui Tu, Wei Yin, Jian Fang*

- `2010.07628v1` - [abs](http://arxiv.org/abs/2010.07628v1) - [pdf](http://arxiv.org/pdf/2010.07628v1)

> Traditional recommender systems encounter several challenges such as data sparsity and unexplained recommendation. To address these challenges, many works propose to exploit semantic information from review data. However, these methods have two major limitations in terms of the way to model textual features and capture textual interaction. For textual modeling, they simply concatenate all the reviews of a user/item into a single review. However, feature extraction at word/phrase level can violate the meaning of the original reviews. As for textual interaction, they defer the interactions to the prediction layer, making them fail to capture complex correlations between users and items. To address those limitations, we propose a novel Hierarchical Text Interaction model(HTI) for rating prediction. In HTI, we propose to model low-level word semantics and high-level review representations hierarchically. The hierarchy allows us to exploit textual features at different granularities. To further capture complex user-item interactions, we propose to exploit semantic correlations between each user-item pair at different hierarchies. At word level, we propose an attention mechanism specialized to each user-item pair, and capture the important words for representing each review. At review level, we mutually propagate textual features between the user and item, and capture the informative reviews. The aggregated review representations are integrated into a collaborative filtering framework for rating prediction. Experiments on five real-world datasets demonstrate that HTI outperforms state-of-the-art models by a large margin. Further case studies provide a deep insight into HTI's ability to capture semantic correlations at different levels of granularities for rating prediction.

</details>

<details>

<summary>2020-10-15 12:40:56 - Does Chinese BERT Encode Word Structure?</summary>

- *Yile Wang, Leyang Cui, Yue Zhang*

- `2010.07711v1` - [abs](http://arxiv.org/abs/2010.07711v1) - [pdf](http://arxiv.org/pdf/2010.07711v1)

> Contextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the features captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in BERT. However, little work has investigated word features for character-based languages such as Chinese. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT; (2) word-level features are mostly in the middle representation layers; (3) downstream tasks make different use of word features in BERT, with POS tagging and chunking relying the most on word features, and natural language inference relying the least on such features.

</details>

<details>

<summary>2020-10-15 14:09:02 - NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of Code-Mixed Dravidian text using XLNet</summary>

- *Shubhanker Banerjee, Arun Jayapal, Sajeetha Thavareesan*

- `2010.07773v1` - [abs](http://arxiv.org/abs/2010.07773v1) - [pdf](http://arxiv.org/pdf/2010.07773v1)

> Social media has penetrated into multilingual societies, however most of them use English to be a preferred language for communication. So it looks natural for them to mix their cultural language with English during conversations resulting in abundance of multilingual data, call this code-mixed data, available in todays' world.Downstream NLP tasks using such data is challenging due to the semantic nature of it being spread across multiple languages.One such Natural Language Processing task is sentiment analysis, for this we use an auto-regressive XLNet model to perform sentiment analysis on code-mixed Tamil-English and Malayalam-English datasets.

</details>

<details>

<summary>2020-10-15 14:34:26 - Hierarchical Poset Decoding for Compositional Generalization in Language</summary>

- *Yinuo Guo, Zeqi Lin, Jian-Guang Lou, Dongmei Zhang*

- `2010.07792v1` - [abs](http://arxiv.org/abs/2010.07792v1) - [pdf](http://arxiv.org/pdf/2010.07792v1)

> We formalize human language understanding as a structured prediction task where the output is a partially ordered set (poset). Current encoder-decoder architectures do not take the poset structure of semantics into account properly, thus suffering from poor compositional generalization ability. In this paper, we propose a novel hierarchical poset decoding paradigm for compositional generalization in language. Intuitively: (1) the proposed paradigm enforces partial permutation invariance in semantics, thus avoiding overfitting to bias ordering information; (2) the hierarchical mechanism allows to capture high-level structures of posets. We evaluate our proposed decoder on Compositional Freebase Questions (CFQ), a large and realistic natural language question answering dataset that is specifically designed to measure compositional generalization. Results show that it outperforms current decoders.

</details>

<details>

<summary>2020-10-15 18:31:26 - An Imitation Game for Learning Semantic Parsers from User Interaction</summary>

- *Ziyu Yao, Yiqi Tang, Wen-tau Yih, Huan Sun, Yu Su*

- `2005.00689v3` - [abs](http://arxiv.org/abs/2005.00689v3) - [pdf](http://arxiv.org/pdf/2005.00689v3)

> Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstration when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstration, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and re-trains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at https://github.com/sunlab-osu/MISP.

</details>

<details>

<summary>2020-10-15 20:08:59 - P2L: Predicting Transfer Learning for Images and Semantic Relations</summary>

- *Bishwaranjan Bhattacharjee, John R. Kender, Matthew Hill, Parijat Dube, Siyu Huo, Michael R. Glass, Brian Belgodere, Sharath Pankanti, Noel Codella, Patrick Watson*

- `1908.07630v2` - [abs](http://arxiv.org/abs/1908.07630v2) - [pdf](http://arxiv.org/pdf/1908.07630v2)

> Transfer learning enhances learning across tasks, by leveraging previously learned representations -- if they are properly chosen. We describe an efficient method to accurately estimate the appropriateness of a previously trained model for use in a new learning task. We use this measure, which we call "Predict To Learn" ("P2L"), in the two very different domains of images and semantic relations, where it predicts, from a set of "source" models, the one model most likely to produce effective transfer for training a given "target" model. We validate our approach thoroughly, by assembling a collection of candidate source models, then fine-tuning each candidate to perform each of a collection of target tasks, and finally measuring how well transfer has been enhanced. Across 95 tasks within multiple domains (images classification and semantic relations), the P2L approach was able to select the best transfer learning model on average, while the heuristic of choosing model trained with the largest data set selected the best model in only 55 cases. These results suggest that P2L captures important information in common between source and target tasks, and that this shared informational structure contributes to successful transfer learning more than simple data size.

</details>

<details>

<summary>2020-10-15 23:25:01 - Montague Grammar Induction</summary>

- *Gene Louis Kim, Aaron Steven White*

- `2010.08067v1` - [abs](http://arxiv.org/abs/2010.08067v1) - [pdf](http://arxiv.org/pdf/2010.08067v1)

> We propose a computational modeling framework for inducing combinatory categorial grammars from arbitrary behavioral data. This framework provides the analyst fine-grained control over the assumptions that the induced grammar should conform to: (i) what the primitive types are; (ii) how complex types are constructed; (iii) what set of combinators can be used to combine types; and (iv) whether (and to what) the types of some lexical items should be fixed. In a proof-of-concept experiment, we deploy our framework for use in distributional analysis. We focus on the relationship between s(emantic)-selection and c(ategory)-selection, using as input a lexicon-scale acceptability judgment dataset focused on English verbs' syntactic distribution (the MegaAcceptability dataset) and enforcing standard assumptions from the semantics literature on the induced grammar.

</details>

<details>

<summary>2020-10-16 01:53:08 - MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering</summary>

- *Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang*

- `2009.08566v2` - [abs](http://arxiv.org/abs/2009.08566v2) - [pdf](http://arxiv.org/pdf/2009.08566v2)

> While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a $10.57\%$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.

</details>

<details>

<summary>2020-10-16 02:12:30 - Modeling Token-level Uncertainty to Learn Unknown Concepts in SLU via Calibrated Dirichlet Prior RNN</summary>

- *Yilin Shen, Wenhu Chen, Hongxia Jin*

- `2010.08101v1` - [abs](http://arxiv.org/abs/2010.08101v1) - [pdf](http://arxiv.org/pdf/2010.08101v1)

> One major task of spoken language understanding (SLU) in modern personal assistants is to extract semantic concepts from an utterance, called slot filling. Although existing slot filling models attempted to improve extracting new concepts that are not seen in training data, the performance in practice is still not satisfied. Recent research collected question and answer annotated data to learn what is unknown and should be asked, yet not practically scalable due to the heavy data collection effort. In this paper, we incorporate softmax-based slot filling neural architectures to model the sequence uncertainty without question supervision. We design a Dirichlet Prior RNN to model high-order uncertainty by degenerating as softmax layer for RNN model training. To further enhance the uncertainty modeling robustness, we propose a novel multi-task training to calibrate the Dirichlet concentration parameters. We collect unseen concepts to create two test datasets from SLU benchmark datasets Snips and ATIS. On these two and another existing Concept Learning benchmark datasets, we show that our approach significantly outperforms state-of-the-art approaches by up to 8.18%. Our method is generic and can be applied to any RNN or Transformer based slot filling models with a softmax layer.

</details>

<details>

<summary>2020-10-16 03:12:26 - Semantic Editing On Segmentation Map Via Multi-Expansion Loss</summary>

- *Jianfeng He, Xuchao Zhang, Shuo Lei, Shuhui Wang, Qingming Huang, Chang-Tien Lu, Bei Xiao*

- `2010.08128v1` - [abs](http://arxiv.org/abs/2010.08128v1) - [pdf](http://arxiv.org/pdf/2010.08128v1)

> Semantic editing on segmentation map has been proposed as an intermediate interface for image generation, because it provides flexible and strong assistance in various image generation tasks. This paper aims to improve quality of edited segmentation map conditioned on semantic inputs. Even though recent studies apply global and local adversarial losses extensively to generate images for higher image quality, we find that they suffer from the misalignment of the boundary area in the mask area. To address this, we propose MExGAN for semantic editing on segmentation map, which uses a novel Multi-Expansion (MEx) loss implemented by adversarial losses on MEx areas. Each MEx area has the mask area of the generation as the majority and the boundary of original context as the minority. To boost convenience and stability of MEx loss, we further propose an Approximated MEx (A-MEx) loss. Besides, in contrast to previous model that builds training data for semantic editing on segmentation map with part of the whole image, which leads to model performance degradation, MExGAN applies the whole image to build the training data. Extensive experiments on semantic editing on segmentation map and natural image inpainting show competitive results on four datasets.

</details>

<details>

<summary>2020-10-16 04:48:24 - DPAttack: Diffused Patch Attacks against Universal Object Detection</summary>

- *Shudeng Wu, Tao Dai, Shu-Tao Xia*

- `2010.11679v1` - [abs](http://arxiv.org/abs/2010.11679v1) - [pdf](http://arxiv.org/pdf/2010.11679v1)

> Recently, deep neural networks (DNNs) have been widely and successfully used in Object Detection, e.g. Faster RCNN, YOLO, CenterNet. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. Adversarial attacks against object detection can be divided into two categories, whole-pixel attacks and patch attacks. While these attacks add perturbations to a large number of pixels in images, we proposed a diffused patch attack (\textbf{DPAttack}) to successfully fool object detectors by diffused patches of asteroid-shaped or grid-shape, which only change a small number of pixels. Experiments show that our DPAttack can successfully fool most object detectors with diffused patches and we get the second place in the Alibaba Tianchi competition: Alibaba-Tsinghua Adversarial Challenge on Object Detection. Our code can be obtained from https://github.com/Wu-Shudeng/DPAttack.

</details>

<details>

<summary>2020-10-16 10:52:54 - Peer-Assisted Robotic Learning: A Data-Driven Collaborative Learning Approach for Cloud Robotic Systems</summary>

- *Boyi Liu, Lujia Wang, Xinquan Chen, Lexiong Huang, Cheng-Zhong Xu*

- `2010.08303v1` - [abs](http://arxiv.org/abs/2010.08303v1) - [pdf](http://arxiv.org/pdf/2010.08303v1)

> A technological revolution is occurring in the field of robotics with the data-driven deep learning technology. However, building datasets for each local robot is laborious. Meanwhile, data islands between local robots make data unable to be utilized collaboratively. To address this issue, the work presents Peer-Assisted Robotic Learning (PARL) in robotics, which is inspired by the peer-assisted learning in cognitive psychology and pedagogy. PARL implements data collaboration with the framework of cloud robotic systems. Both data and models are shared by robots to the cloud after semantic computing and training locally. The cloud converges the data and performs augmentation, integration, and transferring. Finally, fine tune this larger shared dataset in the cloud to local robots. Furthermore, we propose the DAT Network (Data Augmentation and Transferring Network) to implement the data processing in PARL. DAT Network can realize the augmentation of data from multi-local robots. We conduct experiments on a simplified self-driving task for robots (cars). DAT Network has a significant improvement in the augmentation in self-driving scenarios. Along with this, the self-driving experimental results also demonstrate that PARL is capable of improving learning effects with data collaboration of local robots.

</details>

<details>

<summary>2020-10-16 10:56:27 - A Knowledge Graph for Assessing Aggressive Tax Planning Strategies</summary>

- *Niklas Lüdemann, Ageda Shiba, Nikolaos Thymianis, Nicolas Heist, Christopher Ludwig, Heiko Paulheim*

- `2008.05239v3` - [abs](http://arxiv.org/abs/2008.05239v3) - [pdf](http://arxiv.org/pdf/2008.05239v3)

> The taxation of multi-national companies is a complex field, since it is influenced by the legislation of several states. Laws in different states may have unforeseen interaction effects, which can be exploited by allowing multinational companies to minimize taxes, a concept known as tax planning. In this paper, we present a knowledge graph of multinational companies and their relationships, comprising almost 1.5M business entities. We show that commonly known tax planning strategies can be formulated as subgraph queries to that graph, which allows for identifying companies using certain strategies. Moreover, we demonstrate that we can identify anomalies in the graph which hint at potential tax planning strategies, and we show how to enhance those analyses by incorporating information from Wikidata using federated queries.

</details>

<details>

<summary>2020-10-16 11:10:26 - Hierarchical Quantized Autoencoders</summary>

- *Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty*

- `2002.08111v3` - [abs](http://arxiv.org/abs/2002.08111v3) - [pdf](http://arxiv.org/pdf/2002.08111v3)

> Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.

</details>

<details>

<summary>2020-10-16 11:32:12 - QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph</summary>

- *Saeedeh Shekarpour, Abhishek Nadgeri, Kuldeep Singh*

- `2010.08323v1` - [abs](http://arxiv.org/abs/2010.08323v1) - [pdf](http://arxiv.org/pdf/2010.08323v1)

> In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their "black box" nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.

</details>

<details>

<summary>2020-10-17 01:32:08 - Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains</summary>

- *Weijie Yu, Chen Xu, Jun Xu, Liang Pang, Xiaopeng Gao, Xiaozhao Wang, Ji-Rong Wen*

- `2010.07717v2` - [abs](http://arxiv.org/abs/2010.07717v2) - [pdf](http://arxiv.org/pdf/2010.07717v2)

> One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.

</details>

<details>

<summary>2020-10-17 08:18:59 - RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling</summary>

- *Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong*

- `2010.08738v1` - [abs](http://arxiv.org/abs/2010.08738v1) - [pdf](http://arxiv.org/pdf/2010.08738v1)

> In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labeled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, we especially provide linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. A series of benchmark models and results are reported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue context-to-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.

</details>

<details>

<summary>2020-10-17 13:51:37 - Variational Encoder-based Reliable Classification</summary>

- *Chitresh Bhushan, Zhaoyuan Yang, Nurali Virani, Naresh Iyer*

- `2002.08289v2` - [abs](http://arxiv.org/abs/2002.08289v2) - [pdf](http://arxiv.org/pdf/2002.08289v2)

> Machine learning models provide statistically impressive results which might be individually unreliable. To provide reliability, we propose an Epistemic Classifier (EC) that can provide justification of its belief using support from the training dataset as well as quality of reconstruction. Our approach is based on modified variational auto-encoders that can identify a semantically meaningful low-dimensional space where perceptually similar instances are close in $\ell_2$-distance too. Our results demonstrate improved reliability of predictions and robust identification of samples with adversarial attacks as compared to baseline of softmax-based thresholding.

</details>

<details>

<summary>2020-10-17 18:44:33 - Lifelong update of semantic maps in dynamic environments</summary>

- *Manjunath Narayana, Andreas Kolling, Lucio Nardelli, Phil Fong*

- `2010.08846v1` - [abs](http://arxiv.org/abs/2010.08846v1) - [pdf](http://arxiv.org/pdf/2010.08846v1)

> A robot understands its world through the raw information it senses from its surroundings. This raw information is not suitable as a shared representation between the robot and its user. A semantic map, containing high-level information that both the robot and user understand, is better suited to be a shared representation. We use the semantic map as the user-facing interface on our fleet of floor-cleaning robots. Jitter in the robot's sensed raw map, dynamic objects in the environment, and exploration of new space by the robot are common challenges for robots. Solving these challenges effectively in the context of semantic maps is key to enabling semantic maps for lifelong mapping. First, as a robot senses new changes and alters its raw map in successive runs, the semantics must be updated appropriately. We update the map using a spatial transfer of semantics. Second, it is important to keep semantics and their relative constraints consistent even in the presence of dynamic objects. Inconsistencies are automatically determined and resolved through the introduction of a map layer of meta-semantics. Finally, a discovery phase allows the semantic map to be updated with new semantics whenever the robot uncovers new information. Deployed commercially on thousands of floor-cleaning robots in real homes, our user-facing semantic maps provide a intuitive user experience through a lifelong mapping robot.

</details>

<details>

<summary>2020-10-18 03:43:34 - Learn to Segment Retinal Lesions and Beyond</summary>

- *Qijie Wei, Xirong Li, Weihong Yu, Xiao Zhang, Yongpeng Zhang, Bojie Hu, Bin Mo, Di Gong, Ning Chen, Dayong Ding, Youxin Chen*

- `1912.11619v3` - [abs](http://arxiv.org/abs/1912.11619v3) - [pdf](http://arxiv.org/pdf/1912.11619v3)

> Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path re-designed to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a side-attention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this large-scale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading

</details>

<details>

<summary>2020-10-18 14:26:10 - Construction and Application of Teaching System Based on Crowdsourcing Knowledge Graph</summary>

- *Jinta Weng, Ying Gao, Jing Qiu, Guozhu Ding, Huanqin Zheng*

- `2010.08995v1` - [abs](http://arxiv.org/abs/2010.08995v1) - [pdf](http://arxiv.org/pdf/2010.08995v1)

> Through the combination of crowdsourcing knowledge graph and teaching system, research methods to generate knowledge graph and its applications. Using two crowdsourcing approaches, crowdsourcing task distribution and reverse captcha generation, to construct knowledge graph in the field of teaching system. Generating a complete hierarchical knowledge graph of the teaching domain by nodes of school, student, teacher, course, knowledge point and exercise type. The knowledge graph constructed in a crowdsourcing manner requires many users to participate collaboratively with fully consideration of teachers' guidance and users' mobilization issues. Based on the three subgraphs of knowledge graph, prominent teacher, student learning situation and suitable learning route could be visualized. Personalized exercises recommendation model is used to formulate the personalized exercise by algorithm based on the knowledge graph. Collaborative creation model is developed to realize the crowdsourcing construction mechanism. Though unfamiliarity with the learning mode of knowledge graph and learners' less attention to the knowledge structure, system based on Crowdsourcing Knowledge Graph can still get high acceptance around students and teachers

</details>

<details>

<summary>2020-10-18 16:52:35 - Distribution-Based Invariant Deep Networks for Learning Meta-Features</summary>

- *Gwendoline De Bie, Herilalaina Rakotoarison, Gabriel Peyré, Michèle Sebag*

- `2006.13708v2` - [abs](http://arxiv.org/abs/2006.13708v2) - [pdf](http://arxiv.org/pdf/2006.13708v2)

> Recent advances in deep learning from probability distributions successfully achieve classification or regression from distribution samples, thus invariant under permutation of the samples. The first contribution of the paper is to extend these neural architectures to achieve invariance under permutation of the features, too. The proposed architecture, called Dida, inherits the NN properties of universal approximation, and its robustness w.r.t. Lipschitz-bounded transformations of the input distribution is established. The second contribution is to empirically and comparatively demonstrate the merits of the approach on two tasks defined at the dataset level. On both tasks, Dida learns meta-features supporting the characterization of a (labelled) dataset. The first task consists of predicting whether two dataset patches are extracted from the same initial dataset. The second task consists of predicting whether the learning performance achieved by a hyper-parameter configuration under a fixed algorithm (ranging in k-NN, SVM, logistic regression and linear classifier with SGD) dominates that of another configuration, for a dataset extracted from the OpenML benchmarking suite. On both tasks, Dida outperforms the state of the art: DSS (Maron et al., 2020) and Dataset2Vec (Jomaa et al., 2019) architectures, as well as the models based on the hand-crafted meta-features of the literature.

</details>

<details>

<summary>2020-10-18 18:31:11 - Motif Learning in Knowledge Graphs Using Trajectories Of Differential Equations</summary>

- *Mojtaba Nayyeri, Chengjin Xu, Jens Lehmann, Sahar Vahdati*

- `2010.06684v2` - [abs](http://arxiv.org/abs/2010.06684v2) - [pdf](http://arxiv.org/pdf/2010.06684v2)

> Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space (usually a vector space). Ultimately, the plausibility of the predicted links is measured by using a scoring function over the learned embeddings (vectors). Therefore, the capability in preserving graph characteristics including structural aspects and semantics highly depends on the design of the KGE, as well as the inherited abilities from the underlying geometry. Many KGEs use the flat geometry which renders them incapable of preserving complex structures and consequently causes wrong inferences by the models. To address this problem, we propose a neuro differential KGE that embeds nodes of a KG on the trajectories of Ordinary Differential Equations (ODEs). To this end, we represent each relation (edge) in a KG as a vector field on a smooth Riemannian manifold. We specifically parameterize ODEs by a neural network to represent various complex shape manifolds and more importantly complex shape vector fields on the manifold. Therefore, the underlying embedding space is capable of getting various geometric forms to encode complexity in subgraph structures with different motifs. Experiments on synthetic and benchmark dataset as well as social network KGs justify the ODE trajectories as a means to structure preservation and consequently avoiding wrong inferences over state-of-the-art KGE models.

</details>

<details>

<summary>2020-10-18 19:27:06 - UoB at SemEval-2020 Task 1: Automatic Identification of Novel Word Senses</summary>

- *Eleri Sarsfield, Harish Tayyar Madabushi*

- `2010.09072v1` - [abs](http://arxiv.org/abs/2010.09072v1) - [pdf](http://arxiv.org/pdf/2010.09072v1)

> Much as the social landscape in which languages are spoken shifts, language too evolves to suit the needs of its users. Lexical semantic change analysis is a burgeoning field of semantic analysis which aims to trace changes in the meanings of words over time. This paper presents an approach to lexical semantic change detection based on Bayesian word sense induction suitable for novel word sense identification. This approach is used for a submission to SemEval-2020 Task 1, which shows the approach to be capable of the SemEval task. The same approach is also applied to a corpus gleaned from 15 years of Twitter data, the results of which are then used to identify words which may be instances of slang.

</details>

<details>

<summary>2020-10-18 19:41:09 - Graphite: GRAPH-Induced feaTure Extraction for Point Cloud Registration</summary>

- *Mahdi Saleh, Shervin Dehghani, Benjamin Busam, Nassir Navab, Federico Tombari*

- `2010.09079v1` - [abs](http://arxiv.org/abs/2010.09079v1) - [pdf](http://arxiv.org/pdf/2010.09079v1)

> 3D Point clouds are a rich source of information that enjoy growing popularity in the vision community. However, due to the sparsity of their representation, learning models based on large point clouds is still a challenge. In this work, we introduce Graphite, a GRAPH-Induced feaTure Extraction pipeline, a simple yet powerful feature transform and keypoint detector. Graphite enables intensive down-sampling of point clouds with keypoint detection accompanied by a descriptor. We construct a generic graph-based learning scheme to describe point cloud regions and extract salient points. To this end, we take advantage of 6D pose information and metric learning to learn robust descriptions and keypoints across different scans. We Reformulate the 3D keypoint pipeline with graph neural networks which allow efficient processing of the point set while boosting its descriptive power which ultimately results in more accurate 3D registrations. We demonstrate our lightweight descriptor on common 3D descriptor matching and point cloud registration benchmarks and achieve comparable results with the state of the art. Describing 100 patches of a point cloud and detecting their keypoints takes only ~0.018 seconds with our proposed network.

</details>

<details>

<summary>2020-10-19 05:22:54 - comp-syn: Perceptually Grounded Word Embeddings with Color</summary>

- *Bhargav Srinivasa Desikan, Tasker Hull, Ethan O. Nadler, Douglas Guilbeault, Aabir Abubaker Kar, Mark Chu, Donald Ruggiero Lo Sardo*

- `2010.04292v2` - [abs](http://arxiv.org/abs/2010.04292v2) - [pdf](http://arxiv.org/pdf/2010.04292v2)

> Popular approaches to natural language processing create word embeddings based on textual co-occurrence patterns, but often ignore embodied, sensory aspects of language. Here, we introduce the Python package comp-syn, which provides grounded word embeddings based on the perceptually uniform color distributions of Google Image search results. We demonstrate that comp-syn significantly enriches models of distributional semantics. In particular, we show that (1) comp-syn predicts human judgments of word concreteness with greater accuracy and in a more interpretable fashion than word2vec using low-dimensional word-color embeddings, and (2) comp-syn performs comparably to word2vec on a metaphorical vs. literal word-pair classification task. comp-syn is open-source on PyPi and is compatible with mainstream machine-learning Python packages. Our package release includes word-color embeddings for over 40,000 English words, each associated with crowd-sourced word concreteness judgments.

</details>

<details>

<summary>2020-10-19 06:33:12 - Improving Company Valuations with Automated Knowledge Discovery, Extraction and Fusion</summary>

- *Albert Weichselbraun, Philipp Kuntschik, Sandro Hörler*

- `2010.09249v1` - [abs](http://arxiv.org/abs/2010.09249v1) - [pdf](http://arxiv.org/pdf/2010.09249v1)

> Performing company valuations within the domain of biotechnology, pharmacy and medical technology is a challenging task, especially when considering the unique set of risks biotech start-ups face when entering new markets. Companies specialized in global valuation services, therefore, combine valuation models and past experience with heterogeneous metrics and indicators that provide insights into a company's performance. This paper illustrates how automated knowledge discovery, extraction and data fusion can be used to (i) obtain additional indicators that provide insights into the success of a company's product development efforts, and (ii) support labor-intensive data curation processes. We apply deep web knowledge acquisition methods to identify and harvest data on clinical trials that is hidden behind proprietary search interfaces and integrate the extracted data into the industry partner's company valuation ontology. In addition, focused Web crawls and shallow semantic parsing yield information on the company's key personnel and respective contact data, notifying domain experts of relevant changes that get then incorporated into the industry partner's company data.

</details>

<details>

<summary>2020-10-19 07:11:18 - FiSSA at SemEval-2020 Task 9: Fine-tuned For Feelings</summary>

- *Bertelt Braaksma, Richard Scholtens, Stan van Suijlekom, Remy Wang, Ahmet Üstün*

- `2007.12544v3` - [abs](http://arxiv.org/abs/2007.12544v3) - [pdf](http://arxiv.org/pdf/2007.12544v3)

> In this paper, we present our approach for sentiment classification on Spanish-English code-mixed social media data in the SemEval-2020 Task 9. We investigate performance of various pre-trained Transformer models by using different fine-tuning strategies. We explore both monolingual and multilingual models with the standard fine-tuning method. Additionally, we propose a custom model that we fine-tune in two steps: once with a language modeling objective, and once with a task-specific objective. Although two-step fine-tuning improves sentiment classification performance over the base model, the large multilingual XLM-RoBERTa model achieves best weighted F1-score with 0.537 on development data and 0.739 on test data. With this score, our team jupitter placed tenth overall in the competition.

</details>

<details>

<summary>2020-10-19 10:13:56 - CONFIG: Controllable Neural Face Image Generation</summary>

- *Marek Kowalski, Stephan J. Garbin, Virginia Estellers, Tadas Baltrušaitis, Matthew Johnson, Jamie Shotton*

- `2005.02671v3` - [abs](http://arxiv.org/abs/2005.02671v3) - [pdf](http://arxiv.org/pdf/2005.02671v3)

> Our ability to sample realistic natural images, particularly faces, has advanced by leaps and bounds in recent years, yet our ability to exert fine-tuned control over the generative process has lagged behind. If this new technology is to find practical uses, we need to achieve a level of control over generative networks which, without sacrificing realism, is on par with that seen in computer graphics and character animation. To this end we propose ConfigNet, a neural face model that allows for controlling individual aspects of output images in semantically meaningful ways and that is a significant step on the path towards finely-controllable neural rendering. ConfigNet is trained on real face images as well as synthetic face renders. Our novel method uses synthetic data to factorize the latent space into elements that correspond to the inputs of a traditional rendering pipeline, separating aspects such as head pose, facial expression, hair style, illumination, and many others which are very hard to annotate in real data. The real images, which are presented to the network without labels, extend the variety of the generated images and encourage realism. Finally, we propose an evaluation criterion using an attribute detection network combined with a user study and demonstrate state-of-the-art individual control over attributes in the output images.

</details>

<details>

<summary>2020-10-19 10:22:16 - Understanding Unnatural Questions Improves Reasoning over Text</summary>

- *Xiao-Yu Guo, Yuan-Fang Li, Gholamreza Haffari*

- `2010.09366v1` - [abs](http://arxiv.org/abs/2010.09366v1) - [pdf](http://arxiv.org/pdf/2010.09366v1)

> Complex question answering (CQA) over raw text is a challenging task. A prominent approach to this task is based on the programmer-interpreter framework, where the programmer maps the question into a sequence of reasoning actions which is then executed on the raw text by the interpreter. Learning an effective CQA model requires large amounts of human-annotated data,consisting of the ground-truth sequence of reasoning actions, which is time-consuming and expensive to collect at scale. In this paper, we address the challenge of learning a high-quality programmer (parser) by projecting natural human-generated questions into unnatural machine-generated questions which are more convenient to parse. We firstly generate synthetic (question,action sequence) pairs by a data generator, and train a semantic parser that associates synthetic questions with their corresponding action sequences. To capture the diversity when applied tonatural questions, we learn a projection model to map natural questions into their most similar unnatural questions for which the parser can work well. Without any natural training data, our projection model provides high-quality action sequences for the CQA task. Experimental results show that the QA model trained exclusively with synthetic data generated by our method outperforms its state-of-the-art counterpart trained on human-labeled data.

</details>

<details>

<summary>2020-10-19 13:53:38 - COSEA: Convolutional Code Search with Layer-wise Attention</summary>

- *Hao Wang, Jia Zhang, Yingce Xia, Jiang Bian, Chao Zhang, Tie-Yan Liu*

- `2010.09520v1` - [abs](http://arxiv.org/abs/2010.09520v1) - [pdf](http://arxiv.org/pdf/2010.09520v1)

> Semantic code search, which aims to retrieve code snippets relevant to a given natural language query, has attracted many research efforts with the purpose of accelerating software development. The huge amount of online publicly available code repositories has prompted the employment of deep learning techniques to build state-of-the-art code search models. Particularly, they leverage deep neural networks to embed codes and queries into a unified semantic vector space and then use the similarity between code's and query's vectors to approximate the semantic correlation between code and the query. However, most existing studies overlook the code's intrinsic structural logic, which indeed contains a wealth of semantic information, and fails to capture intrinsic features of codes. In this paper, we propose a new deep learning architecture, COSEA, which leverages convolutional neural networks with layer-wise attention to capture the valuable code's intrinsic structural logic. To further increase the learning efficiency of COSEA, we propose a variant of contrastive loss for training the code search model, where the ground-truth code should be distinguished from the most similar negative sample. We have implemented a prototype of COSEA. Extensive experiments over existing public datasets of Python and SQL have demonstrated that COSEA can achieve significant improvements over state-of-the-art methods on code search tasks.

</details>

<details>

<summary>2020-10-19 14:03:28 - Gastric histopathology image segmentation using a hierarchical conditional random field</summary>

- *Changhao Sun, Chen Li, Jinghua Zhang, Muhammad Rahaman, Shiliang Ai, Hao Chen, Frank Kulwa, Yixin Li, Xiaoyan Li, Tao Jiang*

- `2003.01302v5` - [abs](http://arxiv.org/abs/2003.01302v5) - [pdf](http://arxiv.org/pdf/2003.01302v5)

> For the Convolutional Neural Networks (CNNs) applied in the intelligent diagnosis of gastric cancer, existing methods mostly focus on individual characteristics or network frameworks without a policy to depict the integral information. Mainly, Conditional Random Field (CRF), an efficient and stable algorithm for analyzing images containing complicated contents, can characterize spatial relation in images. In this paper, a novel Hierarchical Conditional Random Field (HCRF) based Gastric Histopathology Image Segmentation (GHIS) method is proposed, which can automatically localize abnormal (cancer) regions in gastric histopathology images obtained by an optical microscope to assist histopathologists in medical work. This HCRF model is built up with higher order potentials, including pixel-level and patch-level potentials, and graph-based post-processing is applied to further improve its segmentation performance. Especially, a CNN is trained to build up the pixel-level potentials and another three CNNs are fine-tuned to build up the patch-level potentials for sufficient spatial segmentation information. In the experiment, a hematoxylin and eosin (H&E) stained gastric histopathological dataset with 560 abnormal images are divided into training, validation and test sets with a ratio of 1 : 1 : 2. Finally, segmentation accuracy, recall and specificity of 78.91%, 65.59%, and 81.33% are achieved on the test set. Our HCRF model demonstrates high segmentation performance and shows its effectiveness and future potential in the GHIS field.

</details>

<details>

<summary>2020-10-19 14:53:06 - Against All Odds: Winning the Defense Challenge in an Evasion Competition with Diversification</summary>

- *Erwin Quiring, Lukas Pirch, Michael Reimsbach, Daniel Arp, Konrad Rieck*

- `2010.09569v1` - [abs](http://arxiv.org/abs/2010.09569v1) - [pdf](http://arxiv.org/pdf/2010.09569v1)

> Machine learning-based systems for malware detection operate in a hostile environment. Consequently, adversaries will also target the learning system and use evasion attacks to bypass the detection of malware. In this paper, we outline our learning-based system PEberus that got the first place in the defender challenge of the Microsoft Evasion Competition, resisting a variety of attacks from independent attackers. Our system combines multiple, diverse defenses: we address the semantic gap, use various classification models, and apply a stateful defense. This competition gives us the unique opportunity to examine evasion attacks under a realistic scenario. It also highlights that existing machine learning methods can be hardened against attacks by thoroughly analyzing the attack surface and implementing concepts from adversarial learning. Our defense can serve as an additional baseline in the future to strengthen the research on secure learning.

</details>

<details>

<summary>2020-10-19 15:09:04 - Learning to Reconstruct and Segment 3D Objects</summary>

- *Bo Yang*

- `2010.09582v1` - [abs](http://arxiv.org/abs/2010.09582v1) - [pdf](http://arxiv.org/pdf/2010.09582v1)

> To endow machines with the ability to perceive the real-world in a three dimensional representation as we do as humans is a fundamental and long-standing topic in Artificial Intelligence. Given different types of visual inputs such as images or point clouds acquired by 2D/3D sensors, one important goal is to understand the geometric structure and semantics of the 3D environment. Traditional approaches usually leverage hand-crafted features to estimate the shape and semantics of objects or scenes. However, they are difficult to generalize to novel objects and scenarios, and struggle to overcome critical issues caused by visual occlusions. By contrast, we aim to understand scenes and the objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. To achieve these aims, this thesis makes three core contributions from object-level 3D shape estimation from single or multiple views to scene-level semantic understanding.

</details>

<details>

<summary>2020-10-19 15:23:24 - Better Distractions: Transformer-based Distractor Generation and Multiple Choice Question Filtering</summary>

- *Jeroen Offerijns, Suzan Verberne, Tessa Verhoef*

- `2010.09598v1` - [abs](http://arxiv.org/abs/2010.09598v1) - [pdf](http://arxiv.org/pdf/2010.09598v1)

> For the field of education, being able to generate semantically correct and educationally relevant multiple choice questions (MCQs) could have a large impact. While question generation itself is an active research topic, generating distractors (the incorrect multiple choice options) receives much less attention. A missed opportunity, since there is still a lot of room for improvement in this area. In this work, we train a GPT-2 language model to generate three distractors for a given question and text context, using the RACE dataset. Next, we train a BERT language model to answer MCQs, and use this model as a filter, to select only questions that can be answered and therefore presumably make sense. To evaluate our work, we start by using text generation metrics, which show that our model outperforms earlier work on distractor generation (DG) and achieves state-of-the-art performance. Also, by calculating the question answering ability, we show that larger base models lead to better performance. Moreover, we conducted a human evaluation study, which confirmed the quality of the generated questions, but showed no statistically significant effect of the QA filter.

</details>

<details>

<summary>2020-10-19 16:27:48 - Adaptive Attentional Network for Few-Shot Knowledge Graph Completion</summary>

- *Jiawei Sheng, Shu Guo, Zhenyu Chen, Juwei Yue, Lihong Wang, Tingwen Liu, Hongbo Xu*

- `2010.09638v1` - [abs](http://arxiv.org/abs/2010.09638v1) - [pdf](http://arxiv.org/pdf/2010.09638v1)

> Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes.

</details>

<details>

<summary>2020-10-19 16:56:03 - PySBD: Pragmatic Sentence Boundary Disambiguation</summary>

- *Nipun Sadvilkar, Mark Neumann*

- `2010.09657v1` - [abs](http://arxiv.org/abs/2010.09657v1) - [pdf](http://arxiv.org/pdf/2010.09657v1)

> In this paper, we present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language-specific set of sentence boundary exemplars) originally implemented as a ruby gem - pragmatic_segmenter - which we ported to Python with additional improvements and functionality. PySBD passes 97.92% of the Golden Rule Set exemplars for English, an improvement of 25% over the next best open-source Python tool.

</details>

<details>

<summary>2020-10-19 23:29:50 - Context-Oriented Behavioral Programming</summary>

- *Achiya Elyasaf*

- `2005.02373v2` - [abs](http://arxiv.org/abs/2005.02373v2) - [pdf](http://arxiv.org/pdf/2005.02373v2)

> Modern systems require programmers to develop code that dynamically adapts to different contexts, leading to the evolution of new context-oriented programming languages. These languages introduce new software-engineering challenges, such as: how to maintain and keep the separation of concerns of the codebase? how to model the changing behaviors? how to verify the system behavior? and more.   This paper introduces Context-Oriented Behavioral Programming(COBP) - a novel paradigm for developing context-aware systems, centered on natural and incremental specification of context-dependent behaviors. As the name suggests, we combine behavioral-programming(BP) - a scenario-based modeling paradigm - with context idioms that explicitly specify when scenarios are relevant and what information they need. The core idea is to connect the behavioral model with a data model that represents the context, allowing an intuitive connection between the models via update and select queries. Combining behavioral-programming with context-oriented programming brings the best of the two worlds, solving issues that arise when using each of the approaches in separation.   We begin with providing abstract semantics for COBP, laying the foundations for applying reasoning algorithms to context-aware behavioral programs. We then exemplify the semantics with formal specifications of systems, including a variant of Conway's Game of Life. Finally, we present a JavaScript-based implementation of the paradigm and provide two case studies of real-life context-aware systems (one in robotics and another in IoT) that were developed using this tool. Throughout the examples and case studies, we provide design patterns and a methodology for coping with the above challenges.

</details>

<details>

<summary>2020-10-19 23:53:17 - ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries</summary>

- *Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin*

- `2010.09927v1` - [abs](http://arxiv.org/abs/2010.09927v1) - [pdf](http://arxiv.org/pdf/2010.09927v1)

> Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on textual input that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a search engine. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL's superior performance extends to well-formed text, achieving 84.9% (logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding.

</details>

<details>

<summary>2020-10-20 07:43:00 - BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues</summary>

- *Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi*

- `2010.10095v1` - [abs](http://arxiv.org/abs/2010.10095v1) - [pdf](http://arxiv.org/pdf/2010.10095v1)

> Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.

</details>

<details>

<summary>2020-10-20 10:18:20 - Individual corpora predict fast memory retrieval during reading</summary>

- *Markus J. Hofmann, Lara Müller, Andre Rölke, Ralph Radach, Chris Biemann*

- `2010.10176v1` - [abs](http://arxiv.org/abs/2010.10176v1) - [pdf](http://arxiv.org/pdf/2010.10176v1)

> The corpus, from which a predictive language model is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our regression analyses revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual's longterm memory structure can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly.

</details>

<details>

<summary>2020-10-20 12:24:39 - Image Captioning with Visual Object Representations Grounded in the Textual Modality</summary>

- *Dušan Variš, Katsuhito Sudoh, Satoshi Nakamura*

- `2010.09413v2` - [abs](http://arxiv.org/abs/2010.09413v2) - [pdf](http://arxiv.org/pdf/2010.09413v2)

> We present our work in progress exploring the possibilities of a shared embedding space between textual and visual modality. Leveraging the textual nature of object detection labels and the hypothetical expressiveness of extracted visual object representations, we propose an approach opposite to the current trend, grounding of the representations in the word embedding space of the captioning system instead of grounding words or sentences in their associated images. Based on the previous work, we apply additional grounding losses to the image captioning training objective aiming to force visual object representations to create more heterogeneous clusters based on their class label and copy a semantic structure of the word embedding space. In addition, we provide an analysis of the learned object vector space projection and its impact on the IC system performance. With only slight change in performance, grounded models reach the stopping criterion during training faster than the unconstrained model, needing about two to three times less training updates. Additionally, an improvement in structural correlation between the word embeddings and both original and projected object vectors suggests that the grounding is actually mutual.

</details>

<details>

<summary>2020-10-20 13:05:48 - Adversarial Item Promotion: Vulnerabilities at the Core of Top-N Recommenders that Use Images to Address Cold Start</summary>

- *Zhuoran Liu, Martha Larson*

- `2006.01888v3` - [abs](http://arxiv.org/abs/2006.01888v3) - [pdf](http://arxiv.org/pdf/2006.01888v3)

> E-commerce platforms provide their customers with ranked lists of recommended items matching the customers' preferences. Merchants on e-commerce platforms would like their items to appear as high as possible in the top-N of these ranked lists. In this paper, we demonstrate how unscrupulous merchants can create item images that artificially promote their products, improving their rankings. Recommender systems that use images to address the cold start problem are vulnerable to this security risk. We describe a new type of attack, Adversarial Item Promotion (AIP), that strikes directly at the core of Top-N recommenders: the ranking mechanism itself. Existing work on adversarial images in recommender systems investigates the implications of conventional attacks, which target deep learning classifiers. In contrast, our AIP attacks are embedding attacks that seek to push features representations in a way that fools the ranker (not a classifier) and directly lead to item promotion. We introduce three AIP attacks insider attack, expert attack, and semantic attack, which are defined with respect to three successively more realistic attack models. Our experiments evaluate the danger of these attacks when mounted against three representative visually-aware recommender algorithms in a framework that uses images to address cold start. We also evaluate potential defenses, including adversarial training and find that common, currently-existing, techniques do not eliminate the danger of AIP attacks. In sum, we show that using images to address cold start opens recommender systems to potential threats with clear practical implications.

</details>

<details>

<summary>2020-10-20 13:15:59 - Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech</summary>

- *William N. Havard, Jean-Pierre Chevrot, Laurent Besacier*

- `2006.08387v2` - [abs](http://arxiv.org/abs/2006.08387v2) - [pdf](http://arxiv.org/pdf/2006.08387v2)

> The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.

</details>

<details>

<summary>2020-10-20 13:56:15 - Leveraging Technology for Healthcare and Retaining Access to Personal Health Data to Enhance Personal Health and Well-being</summary>

- *Ayan Chatterjee, Ali Shahaab, Martin W. Gerdes, Santiago Martinez, Pankaj Khatiwada*

- `2010.10285v1` - [abs](http://arxiv.org/abs/2010.10285v1) - [pdf](http://arxiv.org/pdf/2010.10285v1)

> Health data is a sensitive category of personal data. It might result in a high risk to individual and health information handling rights and opportunities unless there is a palatable defense. Reasonable security standards are needed to protect electronic health records (EHR). All personal data handling needs adequate explanation. Maintaining access to medical data even in the developing world would favor health and well-being across the world. Unfortunately, there are still countries that hinder the portability of medical records. Numerous occurrences have shown that it still takes weeks for the medical data to be ported from one general physician (GP) to another. Cross border portability is nearly impossible due to the lack of technical infrastructure and standardization. We demonstrate the difficulty of the portability of medical records with some example case studies as a collaborative engagement exercise through a data mapping process to describe how different people and datapoints interact and evaluate EHR portability techniques. We then propose a blockchain-based EHR system that allows secure, and cross border sharing of medical data. The ethical and technical challenges around having such a system have also been discussed in this study.

</details>

<details>

<summary>2020-10-20 16:21:57 - Hermes Attack: Steal DNN Models with Lossless Inference Accuracy</summary>

- *Yuankun Zhu, Yueqiang Cheng, Husheng Zhou, Yantao Lu*

- `2006.12784v2` - [abs](http://arxiv.org/abs/2006.12784v2) - [pdf](http://arxiv.org/pdf/2006.12784v2)

> Deep Neural Networks (DNNs) models become one of the most valuable enterprise assets due to their critical roles in all aspects of applications. With the trend of privatization deployment of DNN models, the data leakage of the DNN models is becoming increasingly serious and widespread. All existing model-extraction attacks can only leak parts of targeted DNN models with low accuracy or high overhead. In this paper, we first identify a new attack surface -- unencrypted PCIe traffic, to leak DNN models. Based on this new attack surface, we propose a novel model-extraction attack, namely Hermes Attack, which is the first attack to fully steal the whole victim DNN model. The stolen DNN models have the same hyper-parameters, parameters, and semantically identical architecture as the original ones. It is challenging due to the closed-source CUDA runtime, driver, and GPU internals, as well as the undocumented data structures and the loss of some critical semantics in the PCIe traffic. Additionally, there are millions of PCIe packets with numerous noises and chaos orders. Our Hermes Attack addresses these issues by huge reverse engineering efforts and reliable semantic reconstruction, as well as skillful packet selection and order correction. We implement a prototype of the Hermes Attack, and evaluate two sequential DNN models (i.e., MINIST and VGG) and one consequential DNN model (i.e., ResNet) on three NVIDIA GPU platforms, i.e., NVIDIA Geforce GT 730, NVIDIA Geforce GTX 1080 Ti, and NVIDIA Geforce RTX 2080 Ti. The evaluation results indicate that our scheme is able to efficiently and completely reconstruct ALL of them with making inferences on any one image. Evaluated with Cifar10 test dataset that contains 10,000 images, the experiment results show that the stolen models have the same inference accuracy as the original ones (i.e., lossless inference accuracy).

</details>

<details>

<summary>2020-10-20 17:54:16 - Natural Language Inference with Mixed Effects</summary>

- *William Gantt, Benjamin Kane, Aaron Steven White*

- `2010.10501v1` - [abs](http://arxiv.org/abs/2010.10501v1) - [pdf](http://arxiv.org/pdf/2010.10501v1)

> There is growing evidence that the prevalence of disagreement in the raw annotations used to construct natural language inference datasets makes the common practice of aggregating those annotations to a single label problematic. We propose a generic method that allows one to skip the aggregation step and train on the raw annotations directly without subjecting the model to unwanted noise that can arise from annotator response biases. We demonstrate that this method, which generalizes the notion of a \textit{mixed effects model} by incorporating \textit{annotator random effects} into any existing neural model, improves performance over models that do not incorporate such effects.

</details>

<details>

<summary>2020-10-21 04:20:13 - FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents</summary>

- *Bill Yuchen Lin, Ying Sheng, Nguyen Vo, Sandeep Tata*

- `2010.10755v1` - [abs](http://arxiv.org/abs/2010.10755v1) - [pdf](http://arxiv.org/pdf/2010.10755v1)

> Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations. The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average without requiring features over rendered pages or expensive hand-crafted features.

</details>

<details>

<summary>2020-10-21 05:49:22 - Deep learning algorithms out-perform veterinary pathologists in detecting the mitotically most active tumor region</summary>

- *Marc Aubreville, Christof A. Bertram, Christian Marzahl, Corinne Gurtner, Martina Dettwiler, Anja Schmidt, Florian Bartenschlager, Sophie Merz, Marco Fragoso, Olivia Kershaw, Robert Klopfleisch, Andreas Maier*

- `1902.05414v3` - [abs](http://arxiv.org/abs/1902.05414v3) - [pdf](http://arxiv.org/pdf/1902.05414v3)

> Manual count of mitotic figures, which is determined in the tumor region with the highest mitotic activity, is a key parameter of most tumor grading schemes. It can be, however, strongly dependent on the area selection due to uneven mitotic figure distribution in the tumor section.We aimed to assess the question, how significantly the area selection could impact the mitotic count, which has a known high inter-rater disagreement. On a data set of 32 whole slide images of H&E-stained canine cutaneous mast cell tumor, fully annotated for mitotic figures, we asked eight veterinary pathologists (five board-certified, three in training) to select a field of interest for the mitotic count. To assess the potential difference on the mitotic count, we compared the mitotic count of the selected regions to the overall distribution on the slide.Additionally, we evaluated three deep learning-based methods for the assessment of highest mitotic density: In one approach, the model would directly try to predict the mitotic count for the presented image patches as a regression task. The second method aims at deriving a segmentation mask for mitotic figures, which is then used to obtain a mitotic density. Finally, we evaluated a two-stage object-detection pipeline based on state-of-the-art architectures to identify individual mitotic figures. We found that the predictions by all models were, on average, better than those of the experts. The two-stage object detector performed best and outperformed most of the human pathologists on the majority of tumor cases. The correlation between the predicted and the ground truth mitotic count was also best for this approach (0.963 to 0.979). Further, we found considerable differences in position selection between pathologists, which could partially explain the high variance that has been reported for the manual mitotic count.

</details>

<details>

<summary>2020-10-21 06:39:24 - Debiased Contrastive Learning</summary>

- *Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, Stefanie Jegelka*

- `2007.00224v3` - [abs](http://arxiv.org/abs/2007.00224v3) - [pdf](http://arxiv.org/pdf/2007.00224v3)

> A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.

</details>

<details>

<summary>2020-10-21 09:10:20 - Universal Adversarial Attack on Attention and the Resulting Dataset DAmageNet</summary>

- *Sizhe Chen, Zhengbao He, Chengjin Sun, Jie Yang, Xiaolin Huang*

- `2001.06325v3` - [abs](http://arxiv.org/abs/2001.06325v3) - [pdf](http://arxiv.org/pdf/2001.06325v3)

> Adversarial attacks on deep neural networks (DNNs) have been found for several years. However, the existing adversarial attacks have high success rates only when the information of the victim DNN is well-known or could be estimated by the structure similarity or massive queries. In this paper, we propose to Attack on Attention (AoA), a semantic property commonly shared by DNNs. AoA enjoys a significant increase in transferability when the traditional cross entropy loss is replaced with the attention loss. Since AoA alters the loss function only, it could be easily combined with other transferability-enhancement techniques and then achieve SOTA performance. We apply AoA to generate 50000 adversarial samples from ImageNet validation set to defeat many neural networks, and thus name the dataset as DAmageNet. 13 well-trained DNNs are tested on DAmageNet, and all of them have an error rate over 85%. Even with defenses or adversarial training, most models still maintain an error rate over 70% on DAmageNet. DAmageNet is the first universal adversarial dataset. It could be downloaded freely and serve as a benchmark for robustness testing and adversarial training.

</details>

<details>

<summary>2020-10-21 09:58:00 - Explaining black-box text classifiers for disease-treatment information extraction</summary>

- *Milad Moradi, Matthias Samwald*

- `2010.10873v1` - [abs](http://arxiv.org/abs/2010.10873v1) - [pdf](http://arxiv.org/pdf/2010.10873v1)

> Deep neural networks and other intricate Artificial Intelligence (AI) models have reached high levels of accuracy on many biomedical natural language processing tasks. However, their applicability in real-world use cases may be limited due to their vague inner working and decision logic. A post-hoc explanation method can approximate the behavior of a black-box AI model by extracting relationships between feature values and outcomes. In this paper, we introduce a post-hoc explanation method that utilizes confident itemsets to approximate the behavior of black-box classifiers for medical information extraction. Incorporating medical concepts and semantics into the explanation process, our explanator finds semantic relations between inputs and outputs in different parts of the decision space of a black-box classifier. The experimental results show that our explanation method can outperform perturbation and decision set based explanators in terms of fidelity and interpretability of explanations produced for predictions on a disease-treatment information extraction task.

</details>

<details>

<summary>2020-10-21 11:12:01 - Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition</summary>

- *Anand Panchbhai, Tommaso Soru, Edgard Marx*

- `2010.10900v1` - [abs](http://arxiv.org/abs/2010.10900v1) - [pdf](http://arxiv.org/pdf/2010.10900v1)

> A booming amount of information is continuously added to the Internet as structured and unstructured data, feeding knowledge bases such as DBpedia and Wikidata with billions of statements describing millions of entities. The aim of Question Answering systems is to allow lay users to access such data using natural language without needing to write formal queries. However, users often submit questions that are complex and require a certain level of abstraction and reasoning to decompose them into basic graph patterns. In this short paper, we explore the use of architectures based on Neural Machine Translation called Neural SPARQL Machines to learn pattern compositions. We show that sequence-to-sequence models are a viable and promising option to transform long utterances into complex SPARQL queries.

</details>

<details>

<summary>2020-10-21 15:16:20 - Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News</summary>

- *Reuben Tan, Bryan A. Plummer, Kate Saenko*

- `2009.07698v5` - [abs](http://arxiv.org/abs/2009.07698v5) - [pdf](http://arxiv.org/pdf/2009.07698v5)

> Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study experiments, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation.

</details>

<details>

<summary>2020-10-21 17:46:11 - Semantic Role Labeling as Syntactic Dependency Parsing</summary>

- *Tianze Shi, Igor Malioutov, Ozan İrsoy*

- `2010.11170v1` - [abs](http://arxiv.org/abs/2010.11170v1) - [pdf](http://arxiv.org/pdf/2010.11170v1)

> We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.

</details>

<details>

<summary>2020-10-21 19:01:00 - On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries</summary>

- *Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daumé III, Lillian Lee*

- `2010.11246v1` - [abs](http://arxiv.org/abs/2010.11246v1) - [pdf](http://arxiv.org/pdf/2010.11246v1)

> Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce Squall, a dataset that enriches 11,276 WikiTableQuestions English-language questions with manually created SQL equivalents plus alignments between SQL and question fragments. Our annotation enables new training possibilities for encoder-decoder models, including approaches from machine translation previously precluded by the absence of alignments. We propose and test two methods: (1) supervised attention; (2) adopting an auxiliary objective of disambiguating references in the input queries to table columns. In 5-fold cross validation, these strategies improve over strong baselines by 4.4% execution accuracy. Oracle experiments suggest that annotated alignments can support further accuracy gains of up to 23.9%.

</details>

<details>

<summary>2020-10-21 20:43:47 - Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training</summary>

- *Xiaofeng Liu, Yuzhuo Han, Song Bai, Yi Ge, Tianxing Wang, Xu Han, Site Li, Jane You, Ju Lu*

- `2010.12440v1` - [abs](http://arxiv.org/abs/2010.12440v1) - [pdf](http://arxiv.org/pdf/2010.12440v1)

> Semantic segmentation (SS) is an important perception manner for self-driving cars and robotics, which classifies each pixel into a pre-determined class. The widely-used cross entropy (CE) loss-based deep networks has achieved significant progress w.r.t. the mean Intersection-over Union (mIoU). However, the cross entropy loss can not take the different importance of each class in an self-driving system into account. For example, pedestrians in the image should be much more important than the surrounding buildings when make a decisions in the driving, so their segmentation results are expected to be as accurate as possible. In this paper, we propose to incorporate the importance-aware inter-class correlation in a Wasserstein training framework by configuring its ground distance matrix. The ground distance matrix can be pre-defined following a priori in a specific task, and the previous importance-ignored methods can be the particular cases. From an optimization perspective, we also extend our ground metric to a linear, convex or concave increasing function $w.r.t.$ pre-defined ground distance. We evaluate our method on CamVid and Cityscapes datasets with different backbones (SegNet, ENet, FCN and Deeplab) in a plug and play fashion. In our extenssive experiments, Wasserstein loss demonstrates superior segmentation performance on the predefined critical classes for safe-driving.

</details>

<details>

<summary>2020-10-21 21:48:39 - Probing and Fine-tuning Reading Comprehension Models for Few-shot Event Extraction</summary>

- *Rui Feng, Jie Yuan, Chao Zhang*

- `2010.11325v1` - [abs](http://arxiv.org/abs/2010.11325v1) - [pdf](http://arxiv.org/pdf/2010.11325v1)

> We study the problem of event extraction from text data, which requires both detecting target event types and their arguments. Typically, both the event detection and argument detection subtasks are formulated as supervised sequence labeling problems. We argue that the event extraction models so trained are inherently label-hungry, and can generalize poorly across domains and text genres.We propose a reading comprehension framework for event extraction.Specifically, we formulate event detection as a textual entailment prediction problem, and argument detection as a question answer-ing problem. By constructing proper query templates, our approach can effectively distill rich knowledge about tasks and label semantics from pretrained reading comprehension models. Moreover, our model can be fine-tuned with a small amount of data to boost its performance. Our experiment results show that our method performs strongly for zero-shot and few-shot event extraction, and it achieves state-of-the-art performance on the ACE 2005 benchmark when trained with full supervision.

</details>

<details>

<summary>2020-10-21 21:49:00 - ConjNLI: Natural Language Inference Over Conjunctive Sentences</summary>

- *Swarnadeep Saha, Yixin Nie, Mohit Bansal*

- `2010.10418v2` - [abs](http://arxiv.org/abs/2010.10418v2) - [pdf](http://arxiv.org/pdf/2010.10418v2)

> Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions ("and", "or", "but", "nor") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions. Our data and code are publicly available at: https://github.com/swarnaHub/ConjNLI

</details>

<details>

<summary>2020-10-21 23:45:18 - Latte-Mix: Measuring Sentence Semantic Similarity with Latent Categorical Mixtures</summary>

- *M. Li, H. Bai, L. Tan, K. Xiong, M. Li, J. Lin*

- `2010.11351v1` - [abs](http://arxiv.org/abs/2010.11351v1) - [pdf](http://arxiv.org/pdf/2010.11351v1)

> Measuring sentence semantic similarity using pre-trained language models such as BERT generally yields unsatisfactory zero-shot performance, and one main reason is ineffective token aggregation methods such as mean pooling. In this paper, we demonstrate under a Bayesian framework that distance between primitive statistics such as the mean of word embeddings are fundamentally flawed for capturing sentence-level semantic similarity. To remedy this issue, we propose to learn a categorical variational autoencoder (VAE) based on off-the-shelf pre-trained language models. We theoretically prove that measuring the distance between the latent categorical mixtures, namely Latte-Mix, can better reflect the true sentence semantic similarity. In addition, our Bayesian framework provides explanations for why models finetuned on labelled sentence pairs have better zero-shot performance. We also empirically demonstrate that these finetuned models could be further improved by Latte-Mix. Our method not only yields the state-of-the-art zero-shot performance on semantic similarity datasets such as STS, but also enjoy the benefits of fast training and having small memory footprints.

</details>

<details>

<summary>2020-10-22 02:14:27 - Exploit Multiple Reference Graphs for Semi-supervised Relation Extraction</summary>

- *Wanli Li, Tieyun Qian*

- `2010.11383v1` - [abs](http://arxiv.org/abs/2010.11383v1) - [pdf](http://arxiv.org/pdf/2010.11383v1)

> Manual annotation of the labeled data for relation extraction is time-consuming and labor-intensive. Semi-supervised methods can offer helping hands for this problem and have aroused great research interests. Existing work focuses on mapping the unlabeled samples to the classes to augment the labeled dataset. However, it is hard to find an overall good mapping function, especially for the samples with complicated syntactic components in one sentence.   To tackle this limitation, we propose to build the connection between the unlabeled data and the labeled ones rather than directly mapping the unlabeled samples to the classes. Specifically, we first use three kinds of information to construct reference graphs, including entity reference, verb reference, and semantics reference. The goal is to semantically or lexically connect the unlabeled sample(s) to the labeled one(s). Then, we develop a Multiple Reference Graph (MRefG) model to exploit the reference information for better recognizing high-quality unlabeled samples. The effectiveness of our method is demonstrated by extensive comparison experiments with the state-of-the-art baselines on two public datasets.

</details>

<details>

<summary>2020-10-22 03:38:19 - Incorporate Semantic Structures into Machine Translation Evaluation via UCCA</summary>

- *Jin Xu, Yinuo Guo, Junfeng Hu*

- `2010.08728v2` - [abs](http://arxiv.org/abs/2010.08728v2) - [pdf](http://arxiv.org/pdf/2010.08728v2)

> Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in machine translation, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on lexical similarity.

</details>

<details>

<summary>2020-10-22 03:48:56 - Rethinking pooling in graph neural networks</summary>

- *Diego Mesquita, Amauri H. Souza, Samuel Kaski*

- `2010.11418v1` - [abs](http://arxiv.org/abs/2010.11418v1) - [pdf](http://arxiv.org/pdf/2010.11418v1)

> Graph pooling is a central component of a myriad of graph neural network (GNN) architectures. As an inheritance from traditional CNNs, most approaches formulate graph pooling as a cluster assignment problem, extending the idea of local patches in regular grids to graphs. Despite the wide adherence to this design choice, no work has rigorously evaluated its influence on the success of GNNs. In this paper, we build upon representative GNNs and introduce variants that challenge the need for locality-preserving representations, either using randomization or clustering on the complement graph. Strikingly, our experiments demonstrate that using these variants does not result in any decrease in performance. To understand this phenomenon, we study the interplay between convolutional layers and the subsequent pooling ones. We show that the convolutions play a leading role in the learned representations. In contrast to the common belief, local pooling is not responsible for the success of GNNs on relevant and widely-used benchmarks.

</details>

<details>

<summary>2020-10-22 03:52:00 - Basket Recommendation with Multi-Intent Translation Graph Neural Network</summary>

- *Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, Philip S. Yu*

- `2010.11419v1` - [abs](http://arxiv.org/abs/2010.11419v1) - [pdf](http://arxiv.org/pdf/2010.11419v1)

> The problem of basket recommendation~(BR) is to recommend a ranking list of items to the current basket. Existing methods solve this problem by assuming the items within the same basket are correlated by one semantic relation, thus optimizing the item embeddings. However, this assumption breaks when there exist multiple intents within a basket. For example, assuming a basket contains \{\textit{bread, cereal, yogurt, soap, detergent}\} where \{\textit{bread, cereal, yogurt}\} are correlated through the "breakfast" intent, while \{\textit{soap, detergent}\} are of "cleaning" intent, ignoring multiple relations among the items spoils the ability of the model to learn the embeddings. To resolve this issue, it is required to discover the intents within the basket. However, retrieving a multi-intent pattern is rather challenging, as intents are latent within the basket. Additionally, intents within the basket may also be correlated. Moreover, discovering a multi-intent pattern requires modeling high-order interactions, as the intents across different baskets are also correlated. To this end, we propose a new framework named as \textbf{M}ulti-\textbf{I}ntent \textbf{T}ranslation \textbf{G}raph \textbf{N}eural \textbf{N}etwork~({\textbf{MITGNN}}). MITGNN models $T$ intents as tail entities translated from one corresponding basket embedding via $T$ relation vectors. The relation vectors are learned through multi-head aggregators to handle user and item information. Additionally, MITGNN propagates multiple intents across our defined basket graph to learn the embeddings of users and items by aggregating neighbors. Extensive experiments on two real-world datasets prove the effectiveness of our proposed model on both transductive and inductive BR. The code is available online at https://github.com/JimLiu96/MITGNN.

</details>

<details>

<summary>2020-10-22 03:59:51 - Efficient Scale-Permuted Backbone with Learned Resource Distribution</summary>

- *Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Yin Cui, Mingxing Tan, Quoc Le, Xiaodan Song*

- `2010.11426v1` - [abs](http://arxiv.org/abs/2010.11426v1) - [pdf](http://arxiv.org/pdf/2010.11426v1)

> Recently, SpineNet has demonstrated promising results on object detection and image classification over ResNet model. However, it is unclear if the improvement adds up when combining scale-permuted backbone with advanced efficient operations and compound scaling. Furthermore, SpineNet is built with a uniform resource distribution over operations. While this strategy seems to be prevalent for scale-decreased models, it may not be an optimal design for scale-permuted models. In this work, we propose a simple technique to combine efficient operations and compound scaling with a previously learned scale-permuted architecture. We demonstrate the efficiency of scale-permuted model can be further improved by learning a resource distribution over the entire network. The resulting efficient scale-permuted models outperform state-of-the-art EfficientNet-based models on object detection and achieve competitive performance on image classification and semantic segmentation. Code and models will be open-sourced soon.

</details>

<details>

<summary>2020-10-22 04:35:37 - Task-Adaptive Feature Transformer for Few-Shot Segmentation</summary>

- *Jun Seo, Young-Hyun Park, Sung-Whan Yoon, Jaekyun Moon*

- `2010.11437v1` - [abs](http://arxiv.org/abs/2010.11437v1) - [pdf](http://arxiv.org/pdf/2010.11437v1)

> Few-shot learning allows machines to classify novel classes using only a few labeled samples. Recently, few-shot segmentation aiming at semantic segmentation on low sample data has also seen great interest. In this paper, we propose a learnable module for few-shot segmentation, the task-adaptive feature transformer (TAFT). TAFT linearly transforms task-specific high-level features to a set of task-agnostic features well-suited to the segmentation job. Using this task-conditioned feature transformation, the model is shown to effectively utilize the semantic information in novel classes to generate tight segmentation masks. The proposed TAFT module can be easily plugged into existing semantic segmentation algorithms to achieve few-shot segmentation capability with only a few added parameters. We combine TAFT with Deeplab V3+, a well-known segmentation architecture; experiments on the PASCAL-$5^i$ dataset confirm that this combination successfully adds few-shot learning capability to the segmentation algorithm, achieving the state-of-the-art few-shot segmentation performance in some key representative cases.

</details>

<details>

<summary>2020-10-22 06:35:58 - Evaluating Factuality in Generation with Dependency-level Entailment</summary>

- *Tanya Goyal, Greg Durrett*

- `2010.05478v2` - [abs](http://arxiv.org/abs/2010.05478v2) - [pdf](http://arxiv.org/pdf/2010.05478v2)

> Despite significant progress in text generation models, a serious limitation is their tendency to produce text that is factually inconsistent with information in the input. Recent work has studied whether textual entailment systems can be used to identify factual errors; however, these sentence-level entailment models are trained to solve a different problem than generation filtering and they do not localize which part of a generation is non-factual. In this paper, we propose a new formulation of entailment that decomposes it at the level of dependency arcs. Rather than focusing on aggregate decisions, we instead ask whether the semantic relationship manifested by individual dependency arcs in the generated output is supported by the input. Human judgments on this task are difficult to obtain; we therefore propose a method to automatically create data based on existing entailment or paraphrase corpora. Experiments show that our dependency arc entailment model trained on this data can identify factual inconsistencies in paraphrasing and summarization better than sentence-level methods or those based on question generation, while additionally localizing the erroneous parts of the generation.

</details>

<details>

<summary>2020-10-22 07:21:17 - On the Effects of Using word2vec Representations in Neural Networks for Dialogue Act Recognition</summary>

- *Christophe Cerisara, Pavel Kral, Ladislav Lenc*

- `2010.11490v1` - [abs](http://arxiv.org/abs/2010.11490v1) - [pdf](http://arxiv.org/pdf/2010.11490v1)

> Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However , and this is more surprising, we also found that standard word2vec em-beddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.

</details>

<details>

<summary>2020-10-22 07:23:09 - Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis</summary>

- *Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, Dawn Song*

- `2007.08095v2` - [abs](http://arxiv.org/abs/2007.08095v2) - [pdf](http://arxiv.org/pdf/2007.08095v2)

> The use of deep learning techniques has achieved significant progress for program synthesis from input-output examples. However, when the program semantics become more complex, it still remains a challenge to synthesize programs that are consistent with the specification. In this work, we propose SED, a neural program generation framework that incorporates synthesis, execution, and debugging stages. Instead of purely relying on the neural program synthesizer to generate the final program, SED first produces initial programs using the neural program synthesizer component, then utilizes a neural program debugger to iteratively repair the generated programs. The integration of the debugger component enables SED to modify the programs based on the execution results and specification, which resembles the coding process of human programmers. On Karel, a challenging input-output program synthesis benchmark, SED reduces the error rate of the neural program synthesizer itself by a considerable margin, and outperforms the standard beam search for decoding.

</details>

<details>

<summary>2020-10-22 07:50:31 - FAQ-based Question Answering via Knowledge Anchors</summary>

- *Ruobing Xie, Yanan Lu, Fen Lin, Leyu Lin*

- `1911.05930v2` - [abs](http://arxiv.org/abs/1911.05930v2) - [pdf](http://arxiv.org/pdf/1911.05930v2)

> Question answering (QA) aims to understand questions and find appropriate answers. In real-world QA systems, Frequently Asked Question (FAQ) based QA is usually a practical and effective solution, especially for some complicated questions (e.g., How and Why). Recent years have witnessed the great successes of knowledge graphs (KGs) in KBQA systems, while there are still few works focusing on making full use of KGs in FAQ-based QA. In this paper, we propose a novel Knowledge Anchor based Question Answering (KAQA) framework for FAQ-based QA to better understand questions and retrieve more appropriate answers. More specifically, KAQA mainly consists of three modules: knowledge graph construction, query anchoring and query-document matching. We consider entities and triples of KGs in texts as knowledge anchors to precisely capture the core semantics, which brings in higher precision and better interpretability. The multi-channel matching strategy also enables most sentence matching models to be flexibly plugged in our KAQA framework to fit different real-world computation limitations. In experiments, we evaluate our models on both offline and online query-document matching tasks on a real-world FAQ-based QA system in WeChat Search, with detailed analysis, ablation tests and case studies. The significant improvements confirm the effectiveness and robustness of the KAQA framework in real-world FAQ-based QA.

</details>

<details>

<summary>2020-10-22 09:10:41 - Discrete Signal Processing with Set Functions</summary>

- *Markus Püschel, Chris Wendler*

- `2001.10290v2` - [abs](http://arxiv.org/abs/2001.10290v2) - [pdf](http://arxiv.org/pdf/2001.10290v2)

> Set functions are functions (or signals) indexed by the powerset (set of all subsets) of a finite set N. They are fundamental and ubiquitous in many application domains and have been used, for example, to formally describe or quantify loss functions for semantic image segmentation, the informativeness of sensors in sensor networks the utility of sets of items in recommender systems, cooperative games in game theory, or bidders in combinatorial auctions. In particular, the subclass of submodular functions occurs in many optimization and machine learning problems. In this paper, we derive discrete-set signal processing (SP), a novel shift-invariant linear signal processing framework for set functions. Discrete-set SP considers different notions of shift obtained from set union and difference operations. For each shift it provides associated notions of shift-invariant filters, convolution, Fourier transform, and frequency response. We provide intuition for our framework using the concept of generalized coverage function that we define, identify multivariate mutual information as a special case of a discrete-set spectrum, and motivate frequency ordering. Our work brings a new set of tools for analyzing and processing set functions, and, in particular, for dealing with their exponential nature. We show two prototypical applications and experiments: compression in submodular function optimization and sampling for preference elicitation in combinatorial auctions.

</details>

<details>

<summary>2020-10-22 11:38:19 - Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample</summary>

- *Shir Gur, Sagie Benaim, Lior Wolf*

- `2006.12226v3` - [abs](http://arxiv.org/abs/2006.12226v3) - [pdf](http://arxiv.org/pdf/2006.12226v3)

> We consider the task of generating diverse and novel videos from a single video sample. Recently, new hierarchical patch-GAN based approaches were proposed for generating diverse images, given only a single sample at training time. Moving to videos, these approaches fail to generate diverse samples, and often collapse into generating samples similar to the training video. We introduce a novel patch-based variational autoencoder (VAE) which allows for a much greater diversity in generation. Using this tool, a new hierarchical video generation scheme is constructed: at coarse scales, our patch-VAE is employed, ensuring samples are of high diversity. Subsequently, at finer scales, a patch-GAN renders the fine details, resulting in high quality videos. Our experiments show that the proposed method produces diverse samples in both the image domain, and the more challenging video domain.

</details>

<details>

<summary>2020-10-22 13:08:04 - HRFA: High-Resolution Feature-based Attack</summary>

- *Zhixing Ye, Sizhe Chen, Peidong Zhang, Chengjin Sun, Xiaolin Huang*

- `2001.07631v2` - [abs](http://arxiv.org/abs/2001.07631v2) - [pdf](http://arxiv.org/pdf/2001.07631v2)

> Adversarial attacks have long been developed for revealing the vulnerability of Deep Neural Networks (DNNs) by adding imperceptible perturbations to the input. Most methods generate perturbations like normal noise, which is not interpretable and without semantic meaning. In this paper, we propose High-Resolution Feature-based Attack (HRFA), yielding authentic adversarial examples with up to $1024 \times 1024$ resolution. HRFA exerts attack by modifying the latent feature representation of the image, i.e., the gradients back propagate not only through the victim DNN, but also through the generative model that maps the feature space to the image space. In this way, HRFA generates adversarial examples that are in high-resolution, realistic, noise-free, and hence is able to evade several denoising-based defenses. In the experiment, the effectiveness of HRFA is validated by attacking the object classification and face verification tasks with BigGAN and StyleGAN, respectively. The advantages of HRFA are verified from the high quality, high authenticity, and high attack success rate faced with defenses.

</details>

<details>

<summary>2020-10-22 15:57:14 - Posterior Re-calibration for Imbalanced Datasets</summary>

- *Junjiao Tian, Yen-Cheng Liu, Nathan Glaser, Yen-Chang Hsu, Zsolt Kira*

- `2010.11820v1` - [abs](http://arxiv.org/abs/2010.11820v1) - [pdf](http://arxiv.org/pdf/2010.11820v1)

> Neural Networks can perform poorly when the training label distribution is heavily imbalanced, as well as when the testing data differs from the training distribution. In order to deal with shift in the testing label distribution, which imbalance causes, we motivate the problem from the perspective of an optimal Bayes classifier and derive a post-training prior rebalancing technique that can be solved through a KL-divergence based optimization. This method allows a flexible post-training hyper-parameter to be efficiently tuned on a validation set and effectively modify the classifier margin to deal with this imbalance. We further combine this method with existing likelihood shift methods, re-interpreting them from the same Bayesian perspective, and demonstrating that our method can deal with both problems in a unified way. The resulting algorithm can be conveniently used on probabilistic classification problems agnostic to underlying architectures. Our results on six different datasets and five different architectures show state of art accuracy, including on large-scale imbalanced datasets such as iNaturalist for classification and Synthia for semantic segmentation. Please see https://github.com/GT-RIPL/UNO-IC.git for implementation.

</details>

<details>

<summary>2020-10-22 16:53:26 - Fine-grained Synthesis of Unrestricted Adversarial Examples</summary>

- *Omid Poursaeed, Tianxing Jiang, Yordanos Goshu, Harry Yang, Serge Belongie, Ser-Nam Lim*

- `1911.09058v2` - [abs](http://arxiv.org/abs/1911.09058v2) - [pdf](http://arxiv.org/pdf/1911.09058v2)

> We propose a novel approach for generating unrestricted adversarial examples by manipulating fine-grained aspects of image generation. Unlike existing unrestricted attacks that typically hand-craft geometric transformations, we learn stylistic and stochastic modifications leveraging state-of-the-art generative models. This allows us to manipulate an image in a controlled, fine-grained manner without being bounded by a norm threshold. Our approach can be used for targeted and non-targeted unrestricted attacks on classification, semantic segmentation and object detection models. Our attacks can bypass certified defenses, yet our adversarial images look indistinguishable from natural images as verified by human evaluation. Moreover, we demonstrate that adversarial training with our examples improves performance of the model on clean images without requiring any modifications to the architecture. We perform experiments on LSUN, CelebA-HQ and COCO-Stuff as high resolution datasets to validate efficacy of our proposed approach.

</details>

<details>

<summary>2020-10-22 17:55:44 - Neural-Symbolic Integration: A Compositional Perspective</summary>

- *Efthymia Tsamoura, Loizos Michael*

- `2010.11926v1` - [abs](http://arxiv.org/abs/2010.11926v1) - [pdf](http://arxiv.org/pdf/2010.11926v1)

> Despite significant progress in the development of neural-symbolic frameworks, the question of how to integrate a neural and a symbolic system in a \emph{compositional} manner remains open. Our work seeks to fill this gap by treating these two systems as black boxes to be integrated as modules into a single architecture, without making assumptions on their internal structure and semantics. Instead, we expect only that each module exposes certain methods for accessing the functions that the module implements: the symbolic module exposes a deduction method for computing the function's output on a given input, and an abduction method for computing the function's inputs for a given output; the neural module exposes a deduction method for computing the function's output on a given input, and an induction method for updating the function given input-output training instances. We are, then, able to show that a symbolic module -- with any choice for syntax and semantics, as long as the deduction and abduction methods are exposed -- can be cleanly integrated with a neural module, and facilitate the latter's efficient training, achieving empirical performance that exceeds that of previous work.

</details>

<details>

<summary>2020-10-22 21:05:24 - RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</summary>

- *Oindrila Saha, Aditya Kusupati, Harsha Vardhan Simhadri, Manik Varma, Prateek Jain*

- `2002.11921v2` - [abs](http://arxiv.org/abs/2002.11921v2) - [pdf](http://arxiv.org/pdf/2002.11921v2)

> Standard Convolutional Neural Networks (CNNs) designed for computer vision tasks tend to have large intermediate activation maps. These require large working memory and are thus unsuitable for deployment on resource-constrained devices typically used for inference on the edge. Aggressively downsampling the images via pooling or strided convolutions can address the problem but leads to a significant decrease in accuracy due to gross aggregation of the feature map by standard pooling operators. In this paper, we introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregates features over large patches of an image and rapidly downsamples activation maps. Empirical evaluation indicates that an RNNPool layer can effectively replace multiple blocks in a variety of architectures such as MobileNets, DenseNet when applied to standard vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak memory usage for inference while retaining comparable accuracy. We use RNNPool with the standard S3FD architecture to construct a face detection method that achieves state-of-the-art MAP for tiny ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released at https://github.com/Microsoft/EdgeML.

</details>

<details>

<summary>2020-10-22 21:17:04 - Translating Recursive Probabilistic Programs to Factor Graph Grammars</summary>

- *David Chiang, Chung-chieh Shan*

- `2010.12071v1` - [abs](http://arxiv.org/abs/2010.12071v1) - [pdf](http://arxiv.org/pdf/2010.12071v1)

> It is natural for probabilistic programs to use conditionals to express alternative substructures in models, and loops (recursion) to express repeated substructures in models. Thus, probabilistic programs with conditionals and recursion motivate ongoing interest in efficient and general inference. A factor graph grammar (FGG) generates a set of factor graphs that do not all need to be enumerated in order to perform inference. We provide a semantics-preserving translation from first-order probabilistic programs with conditionals and recursion to FGGs.

</details>

<details>

<summary>2020-10-22 21:40:50 - Getting Passive Aggressive About False Positives: Patching Deployed Malware Detectors</summary>

- *Edward Raff, Bobby Filar, James Holt*

- `2010.12080v1` - [abs](http://arxiv.org/abs/2010.12080v1) - [pdf](http://arxiv.org/pdf/2010.12080v1)

> False positives (FPs) have been an issue of extreme importance for anti-virus (AV) systems for decades. As more security vendors turn to machine learning, alert deluge has hit critical mass with over 20% of all alerts resulting in FPs and, in some organizations, the number reaches half of all alerts. This increase has resulted in fatigue, frustration, and, worst of all, neglect from security workers on SOC teams. A foundational cause for FPs is that vendors must build one global system to try and satisfy all customers, but have no method to adjust to individual local environments. This leads to outrageous, albeit technically correct, characterization of their platforms being 99.9% effective. Once these systems are deployed the idiosyncrasies of individual, local environments expose blind spots that lead to FPs and uncertainty.   We propose a strategy for fixing false positives in production after a model has already been deployed. For too long the industry has tried to combat these problems with inefficient, and at times, dangerous allowlist techniques and excessive model retraining which is no longer enough. We propose using a technique called passive-aggressive learning to alter a malware detection model to an individual's environment, eliminating false positives without sharing any customer sensitive information. We will show how to use passive-aggressive learning to solve a collection of notoriously difficult false positives from a production environment without compromising the malware model's accuracy, reducing the total number of FP alerts by an average of 23x.

</details>

<details>

<summary>2020-10-22 22:03:41 - Towards falsifiable interpretability research</summary>

- *Matthew L. Leavitt, Ari Morcos*

- `2010.12016v1` - [abs](http://arxiv.org/abs/2010.12016v1) - [pdf](http://arxiv.org/pdf/2010.12016v1)

> Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are "important" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.

</details>

<details>

<summary>2020-10-22 23:11:18 - Zero-Shot Learning from scratch (ZFS): leveraging local compositional representations</summary>

- *Tristan Sylvain, Linda Petrini, R Devon Hjelm*

- `2010.13320v1` - [abs](http://arxiv.org/abs/2010.13320v1) - [pdf](http://arxiv.org/pdf/2010.13320v1)

> Zero-shot classification is a generalization task where no instance from the target classes is seen during training. To allow for test-time transfer, each class is annotated with semantic information, commonly in the form of attributes or text descriptions. While classical zero-shot learning does not explicitly forbid using information from other datasets, the approaches that achieve the best absolute performance on image benchmarks rely on features extracted from encoders pretrained on Imagenet. This approach relies on hyper-optimized Imagenet-relevant parameters from the supervised classification setting, entangling important questions about the suitability of those parameters and how they were learned with more fundamental questions about representation learning and generalization. To remove these distractors, we propose a more challenging setting: Zero-Shot Learning from scratch (ZFS), which explicitly forbids the use of encoders fine-tuned on other datasets. Our analysis on this setting highlights the importance of local information, and compositional representations.

</details>

<details>

<summary>2020-10-23 02:43:02 - Lightweight Generative Adversarial Networks for Text-Guided Image Manipulation</summary>

- *Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz*

- `2010.12136v1` - [abs](http://arxiv.org/abs/2010.12136v1) - [pdf](http://arxiv.org/pdf/2010.12136v1)

> We propose a novel lightweight generative adversarial network for efficient image manipulation using natural language descriptions. To achieve this, a new word-level discriminator is proposed, which provides the generator with fine-grained training feedback at word-level, to facilitate training a lightweight generator that has a small number of parameters, but can still correctly focus on specific visual attributes of an image, and then edit them without affecting other contents that are not described in the text. Furthermore, thanks to the explicit training signal related to each word, the discriminator can also be simplified to have a lightweight structure. Compared with the state of the art, our method has a much smaller number of parameters, but still achieves a competitive manipulation performance. Extensive experimental results demonstrate that our method can better disentangle different visual attributes, then correctly map them to corresponding semantic words, and thus achieve a more accurate image modification using natural language descriptions.

</details>

<details>

<summary>2020-10-23 03:32:38 - Knowledge-enriched, Type-constrained and Grammar-guided Question Generation over Knowledge Bases</summary>

- *Sheng Bi, Xiya Cheng, Yuan-Fang Li, Yongzhen Wang, Guilin Qi*

- `2010.03157v3` - [abs](http://arxiv.org/abs/2010.03157v3) - [pdf](http://arxiv.org/pdf/2010.03157v3)

> Question generation over knowledge bases (KBQG) aims at generating natural-language questions about a subgraph, i.e. a set of (connected) triples. Two main challenges still face the current crop of encoder-decoder-based methods, especially on small subgraphs: (1) low diversity and poor fluency due to the limited information contained in the subgraphs, and (2) semantic drift due to the decoder's oblivion of the semantics of the answer entity. We propose an innovative knowledge-enriched, type-constrained and grammar-guided KBQG model, named KTG, to addresses the above challenges. In our model, the encoder is equipped with auxiliary information from the KB, and the decoder is constrained with word types during QG. Specifically, entity domain and description, as well as relation hierarchy information are considered to construct question contexts, while a conditional copy mechanism is incorporated to modulate question semantics according to current word types. Besides, a novel reward function featuring grammatical similarity is designed to improve both generative richness and syntactic correctness via reinforcement learning. Extensive experiments show that our proposed model outperforms existing methods by a significant margin on two widely-used benchmark datasets SimpleQuestion and PathQuestion.

</details>

<details>

<summary>2020-10-23 06:58:36 - Audio Caption in a Car Setting with a Sentence-Level Loss</summary>

- *Xuenan Xu, Heinrich Dinkel, Mengyue Wu, Kai Yu*

- `1905.13448v2` - [abs](http://arxiv.org/abs/1905.13448v2) - [pdf](http://arxiv.org/pdf/1905.13448v2)

> Captioning has attracted much attention in image and video understanding while a small amount of work examines audio captioning. This paper contributes a Mandarin-annotated dataset for audio captioning within a car scene. A sentence-level loss is proposed to be used in tandem with a GRU encoder-decoder model to generate captions with higher semantic similarity to human annotations. We evaluate the model on the newly-proposed Car dataset, a previously published Mandarin Hospital dataset and the Joint dataset, indicating its generalization capability across different scenes. An improvement in all metrics can be observed, including classical natural language generation (NLG) metrics, sentence richness and human evaluation ratings. However, though detailed audio captions can now be automatically generated, human annotations still outperform model captions on many aspects.

</details>

<details>

<summary>2020-10-23 10:21:45 - Convolutional Generation of Textured 3D Meshes</summary>

- *Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-Francine Moens, Aurelien Lucchi*

- `2006.07660v2` - [abs](http://arxiv.org/abs/2006.07660v2) - [pdf](http://arxiv.org/pdf/2006.07660v2)

> While recent generative models for 2D images achieve impressive visual results, they clearly lack the ability to perform 3D reasoning. This heavily restricts the degree of control over generated objects as well as the possible applications of such models. In this work, we bridge this gap by leveraging recent advances in differentiable rendering. We design a framework that can generate triangle meshes and associated high-resolution texture maps, using only 2D supervision from single-view natural images. A key contribution of our work is the encoding of the mesh and texture as 2D representations, which are semantically aligned and can be easily modeled by a 2D convolutional GAN. We demonstrate the efficacy of our method on Pascal3D+ Cars and CUB, both in an unconditional setting and in settings where the model is conditioned on class labels, attributes, and text. Finally, we propose an evaluation methodology that assesses the mesh and texture quality separately.

</details>

<details>

<summary>2020-10-23 12:16:07 - Exercise Hierarchical Feature Enhanced Knowledge Tracing</summary>

- *Hanshuang Tong, Yun Zhou, Zhen Wang*

- `2011.09867v1` - [abs](http://arxiv.org/abs/2011.09867v1) - [pdf](http://arxiv.org/pdf/2011.09867v1)

> Knowledge tracing is a fundamental task in the computer-aid educational system. In this paper, we propose a hierarchical exercise feature enhanced knowledge tracing framework, which could enhance the ability of knowledge tracing by incorporating knowledge distribution, semantic features, and difficulty features from exercise text. Extensive experiments show the high performance of our framework.

</details>

<details>

<summary>2020-10-23 12:32:26 - Scalable Unsupervised Multi-Criteria Trajectory Segmentation and Driving Preference Mining</summary>

- *Florian Barth, Stefan Funke, Tobias Skovgaard Jepsen, Claudius Proissl*

- `2011.03331v1` - [abs](http://arxiv.org/abs/2011.03331v1) - [pdf](http://arxiv.org/pdf/2011.03331v1)

> We present analysis techniques for large trajectory data sets that aim to provide a semantic understanding of trajectories reaching beyond them being point sequences in time and space. The presented techniques use a driving preference model w.r.t. road segment traversal costs, e.g., travel time and distance, to analyze and explain trajectories.   In particular, we present trajectory mining techniques that can (a) find interesting points within a trajectory indicating, e.g., a via-point, and (b) recover the driving preferences of a driver based on their chosen trajectory. We evaluate our techniques on the tasks of via-point identification and personalized routing using a data set of more than 1 million vehicle trajectories collected throughout Denmark during a 3-year period. Our techniques can be implemented efficiently and are highly parallelizable, allowing them to scale to millions or billions of trajectories.

</details>

<details>

<summary>2020-10-23 15:56:27 - Semantic Text Analysis for Detection of Compromised Accounts on Social Networks</summary>

- *Dominic Seyler, Lunan Li, ChengXiang Zhai*

- `1804.07247v4` - [abs](http://arxiv.org/abs/1804.07247v4) - [pdf](http://arxiv.org/pdf/1804.07247v4)

> Compromised accounts on social networks are regular user accounts that have been taken over by an entity with malicious intent. Since the adversary exploits the already established trust of a compromised account, it is crucial to detect these accounts to limit the damage they can cause. We propose a novel general framework for semantic analysis of text messages coming out from an account to detect compromised accounts. Our framework is built on the observation that normal users will use language that is measurably different from the language that an adversary would use when the account is compromised. We propose to use the difference of language models of users and adversaries to define novel interpretable semantic features for measuring semantic incoherence in a message stream. We study the effectiveness of the proposed semantic features using a Twitter data set. Evaluation results show that the proposed framework is effective for discovering compromised accounts on social networks and a KL-divergence-based language model feature works best.

</details>

<details>

<summary>2020-10-23 16:45:06 - Neural Passage Retrieval with Improved Negative Contrast</summary>

- *Jing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni, Yinfei Yang*

- `2010.12523v1` - [abs](http://arxiv.org/abs/2010.12523v1) - [pdf](http://arxiv.org/pdf/2010.12523v1)

> In this paper we explore the effects of negative sampling in dual encoder models used to retrieve passages for automatic question answering. We explore four negative sampling strategies that complement the straightforward random sampling of negatives, typically used to train dual encoder models. Out of the four strategies, three are based on retrieval and one on heuristics. Our retrieval-based strategies are based on the semantic similarity and the lexical overlap between questions and passages. We train the dual encoder models in two stages: pre-training with synthetic data and fine tuning with domain-specific data. We apply negative sampling to both stages. The approach is evaluated in two passage retrieval tasks. Even though it is not evident that there is one single sampling strategy that works best in all the tasks, it is clear that our strategies contribute to improving the contrast between the response and all the other passages. Furthermore, mixing the negatives from different strategies achieve performance on par with the best performing strategy in all tasks. Our results establish a new state-of-the-art level of performance on two of the open-domain question answering datasets that we evaluated.

</details>

<details>

<summary>2020-10-23 17:00:26 - GiBERT: Introducing Linguistic Knowledge into BERT through a Lightweight Gated Injection Method</summary>

- *Nicole Peinelt, Marek Rei, Maria Liakata*

- `2010.12532v1` - [abs](http://arxiv.org/abs/2010.12532v1) - [pdf](http://arxiv.org/pdf/2010.12532v1)

> Large pre-trained language models such as BERT have been the driving force behind recent improvements across many NLP tasks. However, BERT is only trained to predict missing words - either behind masks or in the next sentence - and has no knowledge of lexical, syntactic or semantic information beyond what it picks up through unsupervised pre-training. We propose a novel method to explicitly inject linguistic knowledge in the form of word embeddings into any layer of a pre-trained BERT. Our performance improvements on multiple semantic similarity datasets when injecting dependency-based and counter-fitted embeddings indicate that such information is beneficial and currently missing from the original model. Our qualitative analysis shows that counter-fitted embedding injection particularly helps with cases involving synonym pairs.

</details>

<details>

<summary>2020-10-23 20:47:58 - Generating Adequate Distractors for Multiple-Choice Questions</summary>

- *Cheng Zhang, Yicheng Sun, Hejia Chen, Jie Wang*

- `2010.12658v1` - [abs](http://arxiv.org/abs/2010.12658v1) - [pdf](http://arxiv.org/pdf/2010.12658v1)

> This paper presents a novel approach to automatic generation of adequate distractors for a given question-answer pair (QAP) generated from a given article to form an adequate multiple-choice question (MCQ). Our method is a combination of part-of-speech tagging, named-entity tagging, semantic-role labeling, regular expressions, domain knowledge bases, word embeddings, word edit distance, WordNet, and other algorithms. We use the US SAT (Scholastic Assessment Test) practice reading tests as a dataset to produce QAPs and generate three distractors for each QAP to form an MCQ. We show that, via experiments and evaluations by human judges, each MCQ has at least one adequate distractor and 84\% of MCQs have three adequate distractors.

</details>

<details>

<summary>2020-10-23 22:00:39 - Reciprocal Adversarial Learning via Characteristic Functions</summary>

- *Shengxi Li, Zeyang Yu, Min Xiang, Danilo Mandic*

- `2006.08413v2` - [abs](http://arxiv.org/abs/2006.08413v2) - [pdf](http://arxiv.org/pdf/2006.08413v2)

> Generative adversarial nets (GANs) have become a preferred tool for tasks involving complicated distributions. To stabilise the training and reduce the mode collapse of GANs, one of their main variants employs the integral probability metric (IPM) as the loss function. This provides extensive IPM-GANs with theoretical support for basically comparing moments in an embedded domain of the \textit{critic}. We generalise this by comparing the distributions rather than their moments via a powerful tool, i.e., the characteristic function (CF), which uniquely and universally comprising all the information about a distribution. For rigour, we first establish the physical meaning of the phase and amplitude in CF, and show that this provides a feasible way of balancing the accuracy and diversity of generation. We then develop an efficient sampling strategy to calculate the CFs. Within this framework, we further prove an equivalence between the embedded and data domains when a reciprocal exists, where we naturally develop the GAN in an auto-encoder structure, in a way of comparing everything in the embedded space (a semantically meaningful manifold). This efficient structure uses only two modules, together with a simple training strategy, to achieve bi-directionally generating clear images, which is referred to as the reciprocal CF GAN (RCF-GAN). Experimental results demonstrate the superior performances of the proposed RCF-GAN in terms of both generation and reconstruction.

</details>

<details>

<summary>2020-10-23 23:37:12 - Open-Domain Frame Semantic Parsing Using Transformers</summary>

- *Aditya Kalyanpur, Or Biran, Tom Breloff, Jennifer Chu-Carroll, Ariel Diertani, Owen Rambow, Mark Sammons*

- `2010.10998v2` - [abs](http://arxiv.org/abs/2010.10998v2) - [pdf](http://arxiv.org/pdf/2010.10998v2)

> Frame semantic parsing is a complex problem which includes multiple underlying subtasks. Recent approaches have employed joint learning of subtasks (such as predicate and argument detection), and multi-task learning of related tasks (such as syntactic and semantic parsing). In this paper, we explore multi-task learning of all subtasks with transformer-based models. We show that a purely generative encoder-decoder architecture handily beats the previous state of the art in FrameNet 1.7 parsing, and that a mixed decoding multi-task approach achieves even better performance. Finally, we show that the multi-task model also outperforms recent state of the art systems for PropBank SRL parsing on the CoNLL 2012 benchmark.

</details>

<details>

<summary>2020-10-24 00:14:04 - Word2vec Conjecture and A Limitative Result</summary>

- *Falcon Z. Dai*

- `2010.12719v1` - [abs](http://arxiv.org/abs/2010.12719v1) - [pdf](http://arxiv.org/pdf/2010.12719v1)

> Being inspired by the success of \texttt{word2vec} \citep{mikolov2013distributed} in capturing analogies, we study the conjecture that analogical relations can be represented by vector spaces. Unlike many previous works that focus on the distributional semantic aspect of \texttt{word2vec}, we study the purely \emph{representational} question: can \emph{all} semantic word-word relations be represented by differences (or directions) of vectors? We call this the word2vec conjecture and point out some of its desirable implications. However, we will exhibit a class of relations that cannot be represented in this way, thus falsifying the conjecture and establishing a limitative result for the representability of semantic relations by vector spaces over fields of characteristic 0, e.g., real or complex numbers.

</details>

<details>

<summary>2020-10-24 15:37:09 - RUArt: A Novel Text-Centered Solution for Text-Based Visual Question Answering</summary>

- *Zan-Xia Jin, Heran Wu, Chun Yang, Fang Zhou, Jingyan Qin, Lei Xiao, Xu-Cheng Yin*

- `2010.12917v1` - [abs](http://arxiv.org/abs/2010.12917v1) - [pdf](http://arxiv.org/pdf/2010.12917v1)

> Text-based visual question answering (VQA) requires to read and understand text in an image to correctly answer a given question. However, most current methods simply add optical character recognition (OCR) tokens extracted from the image into the VQA model without considering contextual information of OCR tokens and mining the relationships between OCR tokens and scene objects. In this paper, we propose a novel text-centered method called RUArt (Reading, Understanding and Answering the Related Text) for text-based VQA. Taking an image and a question as input, RUArt first reads the image and obtains text and scene objects. Then, it understands the question, OCRed text and objects in the context of the scene, and further mines the relationships among them. Finally, it answers the related text for the given question through text semantic matching and reasoning. We evaluate our RUArt on two text-based VQA benchmarks (ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the reasons behind RUArt's effectiveness. Experimental results demonstrate that our method can effectively explore the contextual information of the text and mine the stable relationships between the text and objects.

</details>

<details>

<summary>2020-10-25 04:11:30 - CRAB: Class Representation Attentive BERT for Hate Speech Identification in Social Media</summary>

- *Sayyed M. Zahiri, Ali Ahmadvand*

- `2010.13028v1` - [abs](http://arxiv.org/abs/2010.13028v1) - [pdf](http://arxiv.org/pdf/2010.13028v1)

> In recent years, social media platforms have hosted an explosion of hate speech and objectionable content. The urgent need for effective automatic hate speech detection models have drawn remarkable investment from companies and researchers. Social media posts are generally short and their semantics could drastically be altered by even a single token. Thus, it is crucial for this task to learn context-aware input representations, and consider relevancy scores between input embeddings and class representations as an additional signal. To accommodate these needs, this paper introduces CRAB (Class Representation Attentive BERT), a neural model for detecting hate speech in social media. The model benefits from two semantic representations: (i) trainable token-wise and sentence-wise class representations, and (ii) contextualized input embeddings from state-of-the-art BERT encoder. To investigate effectiveness of CRAB, we train our model on Twitter data and compare it against strong baselines. Our results show that CRAB achieves 1.89% relative improved Macro-averaged F1 over state-of-the-art baseline. The results of this research open an opportunity for the future research on automated abusive behavior detection in social media

</details>

<details>

<summary>2020-10-25 04:27:43 - Towards Medical Knowmetrics: Representing and Computing Medical Knowledge using Semantic Predications as the Knowledge Unit and the Uncertainty as the Knowledge Context</summary>

- *Xiaoying Li, Suyuan Peng, Jian Du*

- `2010.13031v1` - [abs](http://arxiv.org/abs/2010.13031v1) - [pdf](http://arxiv.org/pdf/2010.13031v1)

> In China, Prof. Hongzhou Zhao and Zeyuan Liu are the pioneers of the concept "knowledge unit" and "knowmetrics" for measuring knowledge. However, the definition of "computable knowledge object" remains controversial so far in different fields. For example, it is defined as 1) quantitative scientific concept in natural science and engineering, 2) knowledge point in the field of education research, and 3) semantic predications, i.e., Subject-Predicate-Object (SPO) triples in biomedical fields. The Semantic MEDLINE Database (SemMedDB), a high-quality public repository of SPO triples extracted from medical literature, provides a basic data infrastructure for measuring medical knowledge. In general, the study of extracting SPO triples as computable knowledge unit from unstructured scientific text has been overwhelmingly focusing on scientific knowledge per se. Since the SPO triples would be possibly extracted from hypothetical, speculative statements or even conflicting and contradictory assertions, the knowledge status (i.e., the uncertainty), which serves as an integral and critical part of scientific knowledge has been largely overlooked. This article aims to put forward a framework for Medical Knowmetrics using the SPO triples as the knowledge unit and the uncertainty as the knowledge context. The lung cancer publications dataset is used to validate the proposed framework. The uncertainty of medical knowledge and how its status evolves over time indirectly reflect the strength of competing knowledge claims, and the probability of certainty for a given SPO triple. We try to discuss the new insights using the uncertainty-centric approaches to detect research fronts, and identify knowledge claims with high certainty level, in order to improve the efficacy of knowledge-driven decision support.

</details>

<details>

<summary>2020-10-25 05:21:52 - VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks</summary>

- *Soyeon Caren Han, Siqu Long, Siwen Luo, Kunze Wang, Josiah Poon*

- `2010.03182v3` - [abs](http://arxiv.org/abs/2010.03182v3) - [pdf](http://arxiv.org/pdf/2010.03182v3)

> Text-to-image multimodal tasks, generating/retrieving an image from a given text description, are extremely challenging tasks since raw text descriptions cover quite limited information in order to fully describe visually realistic images. We propose a new visual contextual text representation for text-to-image multimodal tasks, VICTR, which captures rich visual semantic information of objects from the text input. First, we use the text description as initial input and conduct dependency parsing to extract the syntactic structure and analyse the semantic aspect, including object quantities, to extract the scene graph. Then, we train the extracted objects, attributes, and relations in the scene graph and the corresponding geometric relation information using Graph Convolutional Networks, and it generates text representation which integrates textual and visual semantic information. The text representation is aggregated with word-level and sentence-level embedding to generate both visual contextual word and sentence representation. For the evaluation, we attached VICTR to the state-of-the-art models in text-to-image generation.VICTR is easily added to existing models and improves across both quantitative and qualitative aspects.

</details>

<details>

<summary>2020-10-25 08:20:38 - Further Analysis of Outlier Detection with Deep Generative Models</summary>

- *Ziyu Wang, Bin Dai, David Wipf, Jun Zhu*

- `2010.13064v1` - [abs](http://arxiv.org/abs/2010.13064v1) - [pdf](http://arxiv.org/pdf/2010.13064v1)

> The recent, counter-intuitive discovery that deep generative models (DGMs) can frequently assign a higher likelihood to outliers has implications for both outlier detection applications as well as our overall understanding of generative modeling. In this work, we present a possible explanation for this phenomenon, starting from the observation that a model's typical set and high-density region may not conincide. From this vantage point we propose a novel outlier test, the empirical success of which suggests that the failure of existing likelihood-based outlier tests does not necessarily imply that the corresponding generative model is uncalibrated. We also conduct additional experiments to help disentangle the impact of low-level texture versus high-level semantics in differentiating outliers. In aggregate, these results suggest that modifications to the standard evaluation practices and benchmarks commonly applied in the literature are needed.

</details>

<details>

<summary>2020-10-25 08:55:40 - Dynamic Adversarial Patch for Evading Object Detection Models</summary>

- *Shahar Hoory, Tzvika Shapira, Asaf Shabtai, Yuval Elovici*

- `2010.13070v1` - [abs](http://arxiv.org/abs/2010.13070v1) - [pdf](http://arxiv.org/pdf/2010.13070v1)

> Recent research shows that neural networks models used for computer vision (e.g., YOLO and Fast R-CNN) are vulnerable to adversarial evasion attacks. Most of the existing real-world adversarial attacks against object detectors use an adversarial patch which is attached to the target object (e.g., a carefully crafted sticker placed on a stop sign). This method may not be robust to changes in the camera's location relative to the target object; in addition, it may not work well when applied to nonplanar objects such as cars. In this study, we present an innovative attack method against object detectors applied in a real-world setup that addresses some of the limitations of existing attacks. Our method uses dynamic adversarial patches which are placed at multiple predetermined locations on a target object. An adversarial learning algorithm is applied in order to generate the patches used. The dynamic attack is implemented by switching between optimized patches dynamically, according to the camera's position (i.e., the object detection system's position). In order to demonstrate our attack in a real-world setup, we implemented the patches by attaching flat screens to the target object; the screens are used to present the patches and switch between them, depending on the current camera location. Thus, the attack is dynamic and adjusts itself to the situation to achieve optimal results. We evaluated our dynamic patch approach by attacking the YOLOv2 object detector with a car as the target object and succeeded in misleading it in up to 90% of the video frames when filming the car from a wide viewing angle range. We improved the attack by generating patches that consider the semantic distance between the target object and its classification. We also examined the attack's transferability among different car models and were able to mislead the detector 71% of the time.

</details>

<details>

<summary>2020-10-25 11:52:31 - RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs</summary>

- *Zhiwei Xu, Thalaiyasingam Ajanthan, Vibhav Vineet, Richard Hartley*

- `2010.02488v3` - [abs](http://arxiv.org/abs/2010.02488v3) - [pdf](http://arxiv.org/pdf/2010.02488v3)

> Although 3D Convolutional Neural Networks (CNNs) are essential for most learning based applications involving dense 3D data, their applicability is limited due to excessive memory and computational requirements. Compressing such networks by pruning therefore becomes highly desirable. However, pruning 3D CNNs is largely unexplored possibly because of the complex nature of typical pruning algorithms that embeds pruning into an iterative optimization paradigm. In this work, we introduce a Resource Aware Neuron Pruning (RANP) algorithm that prunes 3D CNNs at initialization to high sparsity levels. Specifically, the core idea is to obtain an importance score for each neuron based on their sensitivity to the loss function. This neuron importance is then reweighted according to the neuron resource consumption related to FLOPs or memory. We demonstrate the effectiveness of our pruning method on 3D semantic segmentation with widely used 3D-UNets on ShapeNet and BraTS'18 as well as on video classification with MobileNetV2 and I3D on UCF101 dataset. In these experiments, our RANP leads to roughly 50-95 reduction in FLOPs and 35-80 reduction in memory with negligible loss in accuracy compared to the unpruned networks. This significantly reduces the computational resources required to train 3D CNNs. The pruned network obtained by our algorithm can also be easily scaled up and transferred to another dataset for training.

</details>

<details>

<summary>2020-10-25 14:49:24 - ExplanationLP: Abductive Reasoning for Explainable Science Question Answering</summary>

- *Mokanarangan Thayaparan, Marco Valentino, André Freitas*

- `2010.13128v1` - [abs](http://arxiv.org/abs/2010.13128v1) - [pdf](http://arxiv.org/pdf/2010.13128v1)

> We propose a novel approach for answering and explaining multiple-choice science questions by reasoning on grounding and abstract inference chains. This paper frames question answering as an abductive reasoning problem, constructing plausible explanations for each choice and then selecting the candidate with the best explanation as the final answer. Our system, ExplanationLP, elicits explanations by constructing a weighted graph of relevant facts for each candidate answer and extracting the facts that satisfy certain structural and semantic constraints. To extract the explanations, we employ a linear programming formalism designed to select the optimal subgraph. The graphs' weighting function is composed of a set of parameters, which we fine-tune to optimize answer selection performance. We carry out our experiments on the WorldTree and ARC-Challenge corpus to empirically demonstrate the following conclusions: (1) Grounding-Abstract inference chains provides the semantic control to perform explainable abductive reasoning (2) Efficiency and robustness in learning with a fewer number of parameters by outperforming contemporary explainable and transformer-based approaches in a similar setting (3) Generalisability by outperforming SOTA explainable approaches on general science question sets.

</details>

<details>

<summary>2020-10-25 15:39:26 - Gauntlet: Finding Bugs in Compilers for Programmable Packet Processing</summary>

- *Fabian Ruffy, Tao Wang, Anirudh Sivaraman*

- `2006.01074v2` - [abs](http://arxiv.org/abs/2006.01074v2) - [pdf](http://arxiv.org/pdf/2006.01074v2)

> Programmable packet-processing devices such as programmable switches and network interface cards are becoming mainstream. These devices are configured in a domain-specific language such as P4, using a compiler to translate packet-processing programs into instructions for different targets. As networks with programmable devices become widespread, it is critical that these compilers be dependable.   This paper considers the problem of finding bugs in compilers for packet processing in the context of P4-16. We introduce domain-specific techniques to induce both abnormal termination of the compiler (crash bugs) and miscompilation (semantic bugs). We apply these techniques to (1) the open-source P4 compiler (P4C) infrastructure, which serves as a common base for different P4 back ends; (2) the P4 back end for the P4 reference software switch; and (3) the P4 back end for the Barefoot Tofino switch.   Across the 3 platforms, over 8 months of bug finding, our tool Gauntlet detected 96 new and distinct bugs (62 crash and 34 semantic), which we confirmed with the respective compiler developers. 54 have been fixed (31 crash and 23 semantic); the remaining have been assigned to a developer. Our bug-finding efforts also led to 6 P4 specification changes. We have open sourced Gauntlet at p4gauntlet.github.io and it now runs within P4C's continuous integration pipeline.

</details>

<details>

<summary>2020-10-25 15:43:44 - Are "Undocumented Workers" the Same as "Illegal Aliens"? Disentangling Denotation and Connotation in Vector Spaces</summary>

- *Albert Webson, Zhizhong Chen, Carsten Eickhoff, Ellie Pavlick*

- `2010.02976v2` - [abs](http://arxiv.org/abs/2010.02976v2) - [pdf](http://arxiv.org/pdf/2010.02976v2)

> In politics, neologisms are frequently invented for partisan objectives. For example, "undocumented workers" and "illegal aliens" refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial neural network that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., "immigrants" vs. "aliens", "estate tax" vs. "death tax") move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.

</details>

<details>

<summary>2020-10-25 16:55:39 - Are all negatives created equal in contrastive instance discrimination?</summary>

- *Tiffany Tianhui Cai, Jonathan Frankle, David J. Schwab, Ari S. Morcos*

- `2010.06682v2` - [abs](http://arxiv.org/abs/2010.06682v2) - [pdf](http://arxiv.org/pdf/2010.06682v2)

> Self-supervised learning has recently begun to rival supervised learning on computer vision tasks. Many of the recent approaches have been based on contrastive instance discrimination (CID), in which the network is trained to recognize two augmented versions of the same instance (a query and positive) while discriminating against a pool of other instances (negatives). The learned representation is then used on downstream tasks such as image classification. Using methodology from MoCo v2 (Chen et al., 2020), we divided negatives by their difficulty for a given query and studied which difficulty ranges were most important for learning useful representations. We found a minority of negatives -- the hardest 5% -- were both necessary and sufficient for the downstream task to reach nearly full accuracy. Conversely, the easiest 95% of negatives were unnecessary and insufficient. Moreover, the very hardest 0.1% of negatives were unnecessary and sometimes detrimental. Finally, we studied the properties of negatives that affect their hardness, and found that hard negatives were more semantically similar to the query, and that some negatives were more consistently easy or hard than we would expect by chance. Together, our results indicate that negatives vary in importance and that CID may benefit from more intelligent negative treatment.

</details>

<details>

<summary>2020-10-25 17:31:12 - Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings</summary>

- *Vaibhav Kumar, Tenzin Singhay Bhotia, Vaibhav Kumar*

- `2010.13168v1` - [abs](http://arxiv.org/abs/2010.13168v1) - [pdf](http://arxiv.org/pdf/2010.13168v1)

> Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.

</details>

<details>

<summary>2020-10-25 19:37:56 - Auto-Encoding Variational Bayes for Inferring Topics and Visualization</summary>

- *Dang Pham, Tuan M. V. Le*

- `2010.09233v2` - [abs](http://arxiv.org/abs/2010.09233v2) - [pdf](http://arxiv.org/pdf/2010.09233v2)

> Visualization and topic modeling are widely used approaches for text analysis. Traditional visualization methods find low-dimensional representations of documents in the visualization space (typically 2D or 3D) that can be displayed using a scatterplot. In contrast, topic modeling aims to discover topics from text, but for visualization, one needs to perform a post-hoc embedding using dimensionality reduction methods. Recent approaches propose using a generative model to jointly find topics and visualization, allowing the semantics to be infused in the visualization space for a meaningful interpretation. A major challenge that prevents these methods from being used practically is the scalability of their inference algorithms. We present, to the best of our knowledge, the first fast Auto-Encoding Variational Bayes based inference method for jointly inferring topics and visualization. Since our method is black box, it can handle model changes efficiently with little mathematical rederivation effort. We demonstrate the efficiency and effectiveness of our method on real-world large datasets and compare it with existing baselines.

</details>

<details>

<summary>2020-10-25 23:39:07 - Multimodal Self-Supervised Learning for Medical Image Analysis</summary>

- *Aiham Taleb, Christoph Lippert, Tassilo Klein, Moin Nabi*

- `1912.05396v2` - [abs](http://arxiv.org/abs/1912.05396v2) - [pdf](http://arxiv.org/pdf/1912.05396v2)

> Self-supervised learning approaches leverage unlabeled samples to acquire generic knowledge about different concepts, hence allowing for annotation-efficient downstream task learning. In this paper, we propose a novel self-supervised method that leverages multiple imaging modalities. We introduce the multimodal puzzle task, which facilitates rich representation learning from multiple image modalities. The learned representations allow for subsequent fine-tuning on different downstream tasks. To achieve that, we learn a modality-agnostic feature embedding by confusing image modalities at the data-level. Together with the Sinkhorn operator, with which we formulate the puzzle solving optimization as permutation matrix inference instead of classification, they allow for efficient solving of multimodal puzzles with varying levels of complexity. In addition, we also propose to utilize cross-modal generation techniques for multimodal data augmentation used for training self-supervised tasks. In other words, we exploit synthetic images for self-supervised pretraining, instead of downstream tasks directly, in order to circumvent quality issues associated with synthetic images, while improving data-efficiency and representations quality. Our experimental results, which assess the gains in downstream performance and data-efficiency, show that solving our multimodal puzzles yields better semantic representations, compared to treating each modality independently. Our results also highlight the benefits of exploiting synthetic images for self-supervised pretraining. We showcase our approach on four downstream tasks: Brain tumor segmentation and survival days prediction using four MRI modalities, Prostate segmentation using two MRI modalities, and Liver segmentation using unregistered CT and MRI modalities. We outperform many previous solutions, and achieve results competitive to state-of-the-art.

</details>

<details>

<summary>2020-10-26 07:02:47 - Multimodal Topic Learning for Video Recommendation</summary>

- *Shi Pu, Yijiang He, Zheng Li, Mao Zheng*

- `2010.13373v1` - [abs](http://arxiv.org/abs/2010.13373v1) - [pdf](http://arxiv.org/pdf/2010.13373v1)

> Facilitated by deep neural networks, video recommendation systems have made significant advances. Existing video recommendation systems directly exploit features from different modalities (e.g., user personal data, user behavior data, video titles, video tags, and visual contents) to input deep neural networks, while expecting the networks to online mine user-preferred topics implicitly from these features. However, the features lacking semantic topic information limits accurate recommendation generation. In addition, feature crosses using visual content features generate high dimensionality features that heavily downgrade the online computational efficiency of networks. In this paper, we explicitly separate topic generation from recommendation generation, propose a multimodal topic learning algorithm to exploit three modalities (i.e., tags, titles, and cover images) for generating video topics offline. The topics generated by the proposed algorithm serve as semantic topic features to facilitate preference scope determination and recommendation generation. Furthermore, we use the semantic topic features instead of visual content features to effectively reduce online computational cost. Our proposed algorithm has been deployed in the Kuaibao information streaming platform. Online and offline evaluation results show that our proposed algorithm performs favorably.

</details>

<details>

<summary>2020-10-26 07:41:40 - Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction</summary>

- *Amir Pouran Ben Veyseh, Tuan Ngo Nguyen, Thien Huu Nguyen*

- `2010.13391v1` - [abs](http://arxiv.org/abs/2010.13391v1) - [pdf](http://arxiv.org/pdf/2010.13391v1)

> The goal of Event Argument Extraction (EAE) is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks (GTNs) to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.

</details>

<details>

<summary>2020-10-26 11:31:42 - Multifaceted Context Representation using Dual Attention for Ontology Alignment</summary>

- *Vivek Iyer, Arvind Agarwal, Harshit Kumar*

- `2010.11721v2` - [abs](http://arxiv.org/abs/2010.11721v2) - [pdf](http://arxiv.org/pdf/2010.11721v2)

> Ontology Alignment is an important research problem that finds application in various fields such as data integration, data transfer, data preparation etc. State-of-the-art (SOTA) architectures in Ontology Alignment typically use naive domain-dependent approaches with handcrafted rules and manually assigned values, making them unscalable and inefficient. Deep Learning approaches for ontology alignment use domain-specific architectures that are not only in-extensible to other datasets and domains, but also typically perform worse than rule-based approaches due to various limitations including over-fitting of models, sparsity of datasets etc. In this work, we propose VeeAlign, a Deep Learning based model that uses a dual-attention mechanism to compute the contextualized representation of a concept in order to learn alignments. By doing so, not only does our approach exploit both syntactic and semantic structure of ontologies, it is also, by design, flexible and scalable to different domains with minimal effort. We validate our approach on various datasets from different domains and in multilingual settings, and show its superior performance over SOTA methods.

</details>

<details>

<summary>2020-10-26 13:26:09 - Collaboration vs. choreography conformance in BPMN</summary>

- *Flavio Corradini, Andrea Morichetta, Andrea Polini, Barbara Re, Francesco Tiezzi*

- `2002.04396v7` - [abs](http://arxiv.org/abs/2002.04396v7) - [pdf](http://arxiv.org/pdf/2002.04396v7)

> The BPMN 2.0 standard is a widely used semi-formal notation to model distributed information systems from different perspectives. The standard makes available a set of diagrams to represent such perspectives. Choreography diagrams represent global constraints concerning the interactions among system components without exposing their internal structure. Collaboration diagrams instead permit to depict the internal behaviour of a component, also referred as process, when integrated with others so to represent a possible implementation of the distributed system.   This paper proposes a design methodology and a formal framework for checking conformance of choreographies against collaborations. In particular, the paper presents a direct formal operational semantics for both BPMN choreography and collaboration diagrams. Conformance aspects are proposed through two relations defined on top of the defined semantics. The approach benefits from the availability of a tool we have developed, named C4, that permits to experiment the theoretical framework in practical contexts. The objective here is to make the exploited formal methods transparent to system designers, thus fostering a wider adoption by practitioners.

</details>

<details>

<summary>2020-10-26 15:44:22 - GraphMDN: Leveraging graph structure and deep learning to solve inverse problems</summary>

- *Tuomas P. Oikarinen, Daniel C. Hannah, Sohrob Kazerounian*

- `2010.13668v1` - [abs](http://arxiv.org/abs/2010.13668v1) - [pdf](http://arxiv.org/pdf/2010.13668v1)

> The recent introduction of Graph Neural Networks (GNNs) and their growing popularity in the past few years has enabled the application of deep learning algorithms to non-Euclidean, graph-structured data. GNNs have achieved state-of-the-art results across an impressive array of graph-based machine learning problems. Nevertheless, despite their rapid pace of development, much of the work on GNNs has focused on graph classification and embedding techniques, largely ignoring regression tasks over graph data. In this paper, we develop a Graph Mixture Density Network (GraphMDN), which combines graph neural networks with mixture density network (MDN) outputs. By combining these techniques, GraphMDNs have the advantage of naturally being able to incorporate graph structured information into a neural architecture, as well as the ability to model multi-modal regression targets. As such, GraphMDNs are designed to excel on regression tasks wherein the data are graph structured, and target statistics are better represented by mixtures of densities rather than singular values (so-called ``inverse problems"). To demonstrate this, we extend an existing GNN architecture known as Semantic GCN (SemGCN) to a GraphMDN structure, and show results from the Human3.6M pose estimation task. The extended model consistently outperforms both GCN and MDN architectures on their own, with a comparable number of parameters.

</details>

<details>

<summary>2020-10-26 16:48:07 - Protocol Analysis with Time</summary>

- *Damián Aparicio-Sánchez, Santiago Escobar, Catherine Meadows, Jose Meseguer, Julia Sapiña*

- `2010.13707v1` - [abs](http://arxiv.org/abs/2010.13707v1) - [pdf](http://arxiv.org/pdf/2010.13707v1)

> We present a framework suited to the analysis of cryptographic protocols that make use of time in their execution. We provide a process algebra syntax that makes time information available to processes, and a transition semantics that takes account of fundamental properties of time. Additional properties can be added by the user if desirable. This timed protocol framework can be implemented either as a simulation tool or as a symbolic analysis tool in which time references are represented by logical variables, and in which the properties of time are implemented as constraints on those time logical variables. These constraints are carried along the symbolic execution of the protocol. The satisfiability of these constraints can be evaluated as the analysis proceeds, so attacks that violate the laws of physics can be rejected as impossible. We demonstrate the feasibility of our approach by using the Maude-NPA protocol analyzer together with an SMT solver that is used to evaluate the satisfiability of timing constraints. We provide a sound and complete protocol transformation from our timed process algebra to the Maude-NPA syntax and semantics, and we prove its soundness and completeness. We then use the tool to analyze Mafia fraud and distance hijacking attacks on a suite of distance-bounding protocols.

</details>

<details>

<summary>2020-10-26 17:38:17 - Spin-Weighted Spherical CNNs</summary>

- *Carlos Esteves, Ameesh Makadia, Kostas Daniilidis*

- `2006.10731v2` - [abs](http://arxiv.org/abs/2006.10731v2) - [pdf](http://arxiv.org/pdf/2006.10731v2)

> Learning equivariant representations is a promising way to reduce sample and model complexity and improve the generalization performance of deep neural networks. The spherical CNNs are successful examples, producing SO(3)-equivariant representations of spherical inputs. There are two main types of spherical CNNs. The first type lifts the inputs to functions on the rotation group SO(3) and applies convolutions on the group, which are computationally expensive since SO(3) has one extra dimension. The second type applies convolutions directly on the sphere, which are limited to zonal (isotropic) filters, and thus have limited expressivity. In this paper, we present a new type of spherical CNN that allows anisotropic filters in an efficient way, without ever leaving the spherical domain. The key idea is to consider spin-weighted spherical functions, which were introduced in physics in the study of gravitational waves. These are complex-valued functions on the sphere whose phases change upon rotation. We define a convolution between spin-weighted functions and build a CNN based on it. The spin-weighted functions can also be interpreted as spherical vector fields, allowing applications to tasks where the inputs or outputs are vector fields. Experiments show that our method outperforms previous methods on tasks like classification of spherical images, classification of 3D shapes and semantic segmentation of spherical panoramas.

</details>

<details>

<summary>2020-10-26 17:42:42 - Robust and Verifiable Information Embedding Attacks to Deep Neural Networks via Error-Correcting Codes</summary>

- *Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong*

- `2010.13751v1` - [abs](http://arxiv.org/abs/2010.13751v1) - [pdf](http://arxiv.org/pdf/2010.13751v1)

> In the era of deep learning, a user often leverages a third-party machine learning tool to train a deep neural network (DNN) classifier and then deploys the classifier as an end-user software product or a cloud service. In an information embedding attack, an attacker is the provider of a malicious third-party machine learning tool. The attacker embeds a message into the DNN classifier during training and recovers the message via querying the API of the black-box classifier after the user deploys it. Information embedding attacks have attracted growing attention because of various applications such as watermarking DNN classifiers and compromising user privacy. State-of-the-art information embedding attacks have two key limitations: 1) they cannot verify the correctness of the recovered message, and 2) they are not robust against post-processing of the classifier.   In this work, we aim to design information embedding attacks that are verifiable and robust against popular post-processing methods. Specifically, we leverage Cyclic Redundancy Check to verify the correctness of the recovered message. Moreover, to be robust against post-processing, we leverage Turbo codes, a type of error-correcting codes, to encode the message before embedding it to the DNN classifier. We propose to recover the message via adaptively querying the classifier to save queries. Our adaptive recovery strategy leverages the property of Turbo codes that supports error correcting with a partial code. We evaluate our information embedding attacks using simulated messages and apply them to three applications, where messages have semantic interpretations. We consider 8 popular methods to post-process the classifier. Our results show that our attacks can accurately and verifiably recover the messages in all considered scenarios, while state-of-the-art attacks cannot accurately recover the messages in many scenarios.

</details>

<details>

<summary>2020-10-26 18:21:27 - Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</summary>

- *Cheng-I Lai, Yung-Sung Chuang, Hung-Yi Lee, Shang-Wen Li, James Glass*

- `2010.13826v1` - [abs](http://arxiv.org/abs/2010.13826v1) - [pdf](http://arxiv.org/pdf/2010.13826v1)

> Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.

</details>

<details>

<summary>2020-10-26 19:16:00 - Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions</summary>

- *Peter A. Jansen*

- `2009.14259v2` - [abs](http://arxiv.org/abs/2009.14259v2) - [pdf](http://arxiv.org/pdf/2009.14259v2)

> The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as "put a hot piece of bread on a plate". Currently, the best-performing models are able to complete less than 5% of these tasks successfully. In this work we focus on modeling the translation problem of converting natural language directives into detailed multi-step sequences of actions that accomplish those goals in the virtual environment. We empirically demonstrate that it is possible to generate gold multi-step plans from language directives alone without any visual input in 26% of unseen cases. When a small amount of visual information is incorporated, namely the starting location in the virtual environment, our best-performing GPT-2 model successfully generates gold command sequences in 58% of cases. Our results suggest that contextualized language models may provide strong visual semantic planning modules for grounded virtual agents.

</details>

<details>

<summary>2020-10-26 21:30:25 - Neural Network Architecture for Credibility Assessment of Textual Claims</summary>

- *Nurendra Choudhary, Rajat Singh, Ishita Bindlish, Manish Shrivastava*

- `1803.10547v3` - [abs](http://arxiv.org/abs/1803.10547v3) - [pdf](http://arxiv.org/pdf/1803.10547v3)

> Text articles with false claims, especially news, have recently become aggravating for the Internet users. These articles are in wide circulation and readers face difficulty discerning fact from fiction. Previous work on credibility assessment has focused on factual analysis and linguistic features. The task's main challenge is the distinction between the features of true and false articles. In this paper, we propose a novel approach called Credibility Outcome (CREDO) which aims at scoring the credibility of an article in an open domain setting.   CREDO consists of different modules for capturing various features responsible for the credibility of an article. These features includes credibility of the article's source and author, semantic similarity between the article and related credible articles retrieved from a knowledge base, and sentiments conveyed by the article. A neural network architecture learns the contribution of each of these modules to the overall credibility of an article. Experiments on Snopes dataset reveals that CREDO outperforms the state-of-the-art approaches based on linguistic features.

</details>

<details>

<summary>2020-10-26 23:23:45 - Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing</summary>

- *Edoardo Maria Ponti, Helen O'Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, Anna Korhonen*

- `1807.00914v3` - [abs](http://arxiv.org/abs/1807.00914v3) - [pdf](http://arxiv.org/pdf/1807.00914v3)

> Linguistic typology aims to capture structural and semantic variation across the world's languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-employment of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such approach could be facilitated by recent developments in data-driven induction of typological knowledge.

</details>

<details>

<summary>2020-10-26 23:48:14 - Distributed Representations of Lexical Sets and Prototypes in Causal Alternation Verbs</summary>

- *Edoardo Maria Ponti, Elisabetta Jezek, Bernardo Magnini*

- `1610.00765v2` - [abs](http://arxiv.org/abs/1610.00765v2) - [pdf](http://arxiv.org/pdf/1610.00765v2)

> Lexical sets contain the words filling an argument slot of a verb, and are in part determined by selectional preferences. The purpose of this paper is to unravel the properties of lexical sets through distributional semantics. We investigate 1) whether lexical set behave as prototypical categories with a centre and a periphery; 2) whether they are polymorphic, i.e. composed by subcategories; 3) whether the distance between lexical sets of different arguments is explanatory of verb properties. In particular, our case study are lexical sets of causative-inchoative verbs in Italian. Having studied several vector models, we find that 1) based on spatial distance from the centroid, object fillers are scattered uniformly across the category, whereas intransitive subject fillers lie on its edge; 2) a correlation exists between the amount of verb senses and that of clusters discovered automatically, especially for intransitive subjects; 3) the distance between the centroids of object and intransitive subject is correlated with other properties of verbs, such as their cross-lingual tendency to appear in the intransitive pattern rather than transitive one. This paper is noncommittal with respect to the hypothesis that this connection is underpinned by a semantic reason, namely the spontaneity of the event denoted by the verb.

</details>

<details>

<summary>2020-10-27 05:46:46 - Semantic Visual Navigation by Watching YouTube Videos</summary>

- *Matthew Chang, Arjun Gupta, Saurabh Gupta*

- `2006.10034v2` - [abs](http://arxiv.org/abs/2006.10034v2) - [pdf](http://arxiv.org/pdf/2006.10034v2)

> Semantic cues and statistical regularities in real-world environment layouts can improve efficiency for navigation in novel environments. This paper learns and leverages such semantic cues for navigating to objects of interest in novel environments, by simply watching YouTube videos. This is challenging because YouTube videos don't come with labels for actions or goals, and may not even showcase optimal behavior. Our method tackles these challenges through the use of Q-learning on pseudo-labeled transition quadruples (image, action, next image, reward). We show that such off-policy Q-learning from passive data is able to learn meaningful semantic cues for navigation. These cues, when used in a hierarchical navigation policy, lead to improved efficiency at the ObjectGoal task in visually realistic simulations. We observe a relative improvement of 15-83% over end-to-end RL, behavior cloning, and classical methods, while using minimal direct interaction.

</details>

<details>

<summary>2020-10-27 14:02:00 - Ice Monitoring in Swiss Lakes from Optical Satellites and Webcams using Machine Learning</summary>

- *Manu Tom, Rajanie Prabha, Tianyu Wu, Emmanuel Baltsavias, Laura Leal-Taixe, Konrad Schindler*

- `2010.14300v1` - [abs](http://arxiv.org/abs/2010.14300v1) - [pdf](http://arxiv.org/pdf/2010.14300v1)

> Continuous observation of climate indicators, such as trends in lake freezing, is important to understand the dynamics of the local and global climate system. Consequently, lake ice has been included among the Essential Climate Variables (ECVs) of the Global Climate Observing System (GCOS), and there is a need to set up operational monitoring capabilities. Multi-temporal satellite images and publicly available webcam streams are among the viable data sources to monitor lake ice. In this work we investigate machine learning-based image analysis as a tool to determine the spatio-temporal extent of ice on Swiss Alpine lakes as well as the ice-on and ice-off dates, from both multispectral optical satellite images (VIIRS and MODIS) and RGB webcam images. We model lake ice monitoring as a pixel-wise semantic segmentation problem, i.e., each pixel on the lake surface is classified to obtain a spatially explicit map of ice cover. We show experimentally that the proposed system produces consistently good results when tested on data from multiple winters and lakes. Our satellite-based method obtains mean Intersection-over-Union (mIoU) scores >93%, for both sensors. It also generalises well across lakes and winters with mIoU scores >78% and >80% respectively. On average, our webcam approach achieves mIoU values of 87% (approx.) and generalisation scores of 71% (approx.) and 69% (approx.) across different cameras and winters respectively. Additionally, we put forward a new benchmark dataset of webcam images (Photi-LakeIce) which includes data from two winters and three cameras.

</details>

<details>

<summary>2020-10-28 02:54:13 - Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity</summary>

- *Brian Thompson, Matt Post*

- `2008.04935v2` - [abs](http://arxiv.org/abs/2008.04935v2) - [pdf](http://arxiv.org/pdf/2008.04935v2)

> Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from such a model using standard beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the amount of lexical diversity between the input and output can be controlled at generation time. We conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages.

</details>

<details>

<summary>2020-10-28 03:49:19 - Proof-Carrying Plans: a Resource Logic for AI Planning</summary>

- *Alasdair Hill, Ekaterina Komendantskaya, Ronald P. A. Petrick*

- `2008.04165v2` - [abs](http://arxiv.org/abs/2008.04165v2) - [pdf](http://arxiv.org/pdf/2008.04165v2)

> Recent trends in AI verification and Explainable AI have raised the question of whether AI planning techniques can be verified. In this paper, we present a novel resource logic, the Proof Carrying Plans (PCP) logic that can be used to verify plans produced by AI planners. The PCP logic takes inspiration from existing resource logics (such as Linear logic and Separation logic) as well as Hoare logic when it comes to modelling states and resource-aware plan execution. It also capitalises on the Curry-Howard approach to logics, in its treatment of plans as functions and plan pre- and post-conditions as types. This paper presents two main results. From the theoretical perspective, we show that the PCP logic is sound relative to the standard possible world semantics used in AI planning. From the practical perspective, we present a complete Agda formalisation of the PCP logic and of its soundness proof. Moreover, we showcase the Curry-Howard, or functional, value of this implementation by supplementing it with the library that parses AI plans into Agda's proofs automatically. We provide evaluation of this library and the resulting Agda functions.

</details>

<details>

<summary>2020-10-28 04:40:18 - Detecting Individuals with Depressive Disorder fromPersonal Google Search and YouTube History Logs</summary>

- *Boyu Zhang, Anis Zaman, Rupam Acharyya, Ehsan Hoque, Vincent Silenzio, Henry Kautz*

- `2010.15670v1` - [abs](http://arxiv.org/abs/2010.15670v1) - [pdf](http://arxiv.org/pdf/2010.15670v1)

> Depressive disorder is one of the most prevalent mental illnesses among the global population. However, traditional screening methods require exacting in-person interviews and may fail to provide immediate interventions. In this work, we leverage ubiquitous personal longitudinal Google Search and YouTube engagement logs to detect individuals with depressive disorder. We collected Google Search and YouTube history data and clinical depression evaluation results from $212$ participants ($99$ of them suffered from moderate to severe depressions). We then propose a personalized framework for classifying individuals with and without depression symptoms based on mutual-exciting point process that captures both the temporal and semantic aspects of online activities. Our best model achieved an average F1 score of $0.77 \pm 0.04$ and an AUC ROC of $0.81 \pm 0.02$.

</details>

<details>

<summary>2020-10-28 08:31:40 - Generating Knowledge Graphs by Employing Natural Language Processing and Machine Learning Techniques within the Scholarly Domain</summary>

- *Danilo Dessì, Francesco Osborne, Diego Reforgiato Recupero, Davide Buscaldi, Enrico Motta*

- `2011.01103v1` - [abs](http://arxiv.org/abs/2011.01103v1) - [pdf](http://arxiv.org/pdf/2011.01103v1)

> The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) show the advantage of such an hybrid system over alternative approaches, and vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.

</details>

<details>

<summary>2020-10-28 08:39:30 - A Comparative Analysis of Knowledge-Intensive and Data-Intensive Semantic Parsers</summary>

- *Junjie Cao, Zi Lin, Weiwei Sun, Xiaojun Wan*

- `1907.02298v3` - [abs](http://arxiv.org/abs/1907.02298v3) - [pdf](http://arxiv.org/pdf/1907.02298v3)

> We present a phenomenon-oriented comparative analysis of the two dominant approaches in task-independent semantic parsing: classic, knowledge-intensive and neural, data-intensive models. To reflect state-of-the-art neural NLP technologies, we introduce a new target structure-centric parser that can produce semantic graphs much more accurately than previous data-driven parsers. We then show that, in spite of comparable performance overall, knowledge- and data-intensive models produce different types of errors, in a way that can be explained by their theoretical properties. This analysis leads to new directions for parser development.

</details>

<details>

<summary>2020-10-28 10:07:01 - Find or Classify? Dual Strategy for Slot-Value Predictions on Multi-Domain Dialog State Tracking</summary>

- *Jian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wan, Philip S. Yu, Richard Socher, Caiming Xiong*

- `1910.03544v4` - [abs](http://arxiv.org/abs/1910.03544v4) - [pdf](http://arxiv.org/pdf/1910.03544v4)

> Dialog state tracking (DST) is a core component in task-oriented dialog systems. Existing approaches for DST mainly fall into one of two categories, namely, ontology-based and ontology-free methods. An ontology-based method selects a value from a candidate-value list for each target slot, while an ontology-free method extracts spans from dialog contexts. Recent work introduced a BERT-based model to strike a balance between the two methods by pre-defining categorical and non-categorical slots. However, it is not clear enough which slots are better handled by either of the two slot types, and the way to use the pre-trained model has not been well investigated. In this paper, we propose a simple yet effective dual-strategy model for DST, by adapting a single BERT-style reading comprehension model to jointly handle both the categorical and non-categorical slots. Our experiments on the MultiWOZ datasets show that our method significantly outperforms the BERT-based counterpart, finding that the key is a deep interaction between the domain-slot and context information. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1) settings, our method performs competitively and robustly across the two different settings. Our method sets the new state of the art in the noisy setting, while performing more robustly than the best model in the cleaner setting. We also conduct a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research.

</details>

<details>

<summary>2020-10-28 11:38:51 - Dynamic Bayesian Approach for decision-making in Ego-Things</summary>

- *Divya Kanapram, Damian Campo, Mohamad Baydoun, Lucio Marcenaro, Eliane L. Bodanese, Carlo Regazzoni, Mario Marchese*

- `2010.14900v1` - [abs](http://arxiv.org/abs/2010.14900v1) - [pdf](http://arxiv.org/pdf/2010.14900v1)

> This paper presents a novel approach to detect abnormalities in dynamic systems based on multisensory data and feature selection. The proposed method produces multiple inference models by considering several features of the observed data. This work facilitates the obtainment of the most precise features for predicting future instances and detecting abnormalities. Growing neural gas (GNG) is employed for clustering multisensory data into a set of nodes that provide a semantic interpretation of data and define local linear models for prediction purposes. Our method uses a Markov Jump particle filter (MJPF) for state estimation and abnormality detection. The proposed method can be used for selecting the optimal set features to be shared in networking operations such that state prediction, decision-making, and abnormality detection processes are favored. This work is evaluated by using a real dataset consisting of a moving vehicle performing some tasks in a controlled environment.

</details>

<details>

<summary>2020-10-28 12:33:04 - Bridging the Modality Gap for Speech-to-Text Translation</summary>

- *Yuchen Liu, Junnan Zhu, Jiajun Zhang, Chengqing Zong*

- `2010.14920v1` - [abs](http://arxiv.org/abs/2010.14920v1) - [pdf](http://arxiv.org/pdf/2010.14920v1)

> End-to-end speech translation aims to translate speech in one language into text in another language via an end-to-end way. Most existing methods employ an encoder-decoder structure with a single encoder to learn acoustic representation and semantic information simultaneously, which ignores the speech-and-text modality differences and makes the encoder overloaded, leading to great difficulty in learning such a model. To address these issues, we propose a Speech-to-Text Adaptation for Speech Translation (STAST) model which aims to improve the end-to-end model performance by bridging the modality gap between speech and text. Specifically, we decouple the speech translation encoder into three parts and introduce a shrink mechanism to match the length of speech representation with that of the corresponding text transcription. To obtain better semantic representation, we completely integrate a text-based translation model into the STAST so that two tasks can be trained in the same latent space. Furthermore, we introduce a cross-modal adaptation method to close the distance between speech and text representation. Experimental results on English-French and English-German speech translation corpora have shown that our model significantly outperforms strong baselines, and achieves the new state-of-the-art performance.

</details>

<details>

<summary>2020-10-28 12:53:04 - Dynamic Point Cloud Denoising via Manifold-to-Manifold Distance</summary>

- *Wei Hu, Qianjiang Hu, Zehua Wang, Xiang Gao*

- `2003.08355v3` - [abs](http://arxiv.org/abs/2003.08355v3) - [pdf](http://arxiv.org/pdf/2003.08355v3)

> 3D dynamic point clouds provide a natural discrete representation of real-world objects or scenes in motion, with a wide range of applications in immersive telepresence, autonomous driving, surveillance, \etc. Nevertheless, dynamic point clouds are often perturbed by noise due to hardware, software or other causes. While a plethora of methods have been proposed for static point cloud denoising, few efforts are made for the denoising of dynamic point clouds, which is quite challenging due to the irregular sampling patterns both spatially and temporally. In this paper, we represent dynamic point clouds naturally on spatial-temporal graphs, and exploit the temporal consistency with respect to the underlying surface (manifold). In particular, we define a manifold-to-manifold distance and its discrete counterpart on graphs to measure the variation-based intrinsic distance between surface patches in the temporal domain, provided that graph operators are discrete counterparts of functionals on Riemannian manifolds. Then, we construct the spatial-temporal graph connectivity between corresponding surface patches based on the temporal distance and between points in adjacent patches in the spatial domain. Leveraging the initial graph representation, we formulate dynamic point cloud denoising as the joint optimization of the desired point cloud and underlying graph representation, regularized by both spatial smoothness and temporal consistency. We reformulate the optimization and present an efficient algorithm. Experimental results show that the proposed method significantly outperforms independent denoising of each frame from state-of-the-art static point cloud denoising approaches, on both Gaussian noise and simulated LiDAR noise.

</details>

<details>

<summary>2020-10-28 15:15:13 - A Comprehensive Survey on Word Representation Models: From Classical to State-Of-The-Art Word Representation Language Models</summary>

- *Usman Naseem, Imran Razzak, Shah Khalid Khan, Mukesh Prasad*

- `2010.15036v1` - [abs](http://arxiv.org/abs/2010.15036v1) - [pdf](http://arxiv.org/pdf/2010.15036v1)

> Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP related tasks. In the end, this survey briefly discusses the commonly used ML and DL based classifiers, evaluation metrics and the applications of these word embeddings in different NLP tasks.

</details>

<details>

<summary>2020-10-28 16:08:54 - Self-awareness in Intelligent Vehicles: Experience Based Abnormality Detection</summary>

- *Divya Kanapram, Pablo Marin-Plaza, Lucio Marcenaro, David Martin, Arturo de la Escalera, Carlo Regazzoni*

- `2010.15056v1` - [abs](http://arxiv.org/abs/2010.15056v1) - [pdf](http://arxiv.org/pdf/2010.15056v1)

> The evolution of Intelligent Transportation System in recent times necessitates the development of self-driving agents: the self-awareness consciousness. This paper aims to introduce a novel method to detect abnormalities based on internal cross-correlation parameters of the vehicle. Before the implementation of Machine Learning, the detection of abnormalities were manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. Nowadays, it is possible to train a Dynamic Bayesian Network (DBN) model to automatically evaluate and detect when the vehicle is potentially misbehaving. In this paper, different scenarios have been set in order to train and test a switching DBN for Perimeter Monitoring Task using a semantic segmentation for the DBN model and Hellinger Distance metric for abnormality measurements.

</details>

<details>

<summary>2020-10-28 20:47:13 - A Visuospatial Dataset for Naturalistic Verb Learning</summary>

- *Dylan Ebert, Ellie Pavlick*

- `2010.15225v1` - [abs](http://arxiv.org/abs/2010.15225v1) - [pdf](http://arxiv.org/pdf/2010.15225v1)

> We introduce a new dataset for training and evaluating grounded language models. Our data is collected within a virtual reality environment and is designed to emulate the quality of language data to which a pre-verbal child is likely to have access: That is, naturalistic, spontaneous speech paired with richly grounded visuospatial context. We use the collected data to compare several distributional semantics models for verb learning. We evaluate neural models based on 2D (pixel) features as well as feature-engineered models based on 3D (symbolic, spatial) features, and show that neither modeling approach achieves satisfactory performance. Our results are consistent with evidence from child language acquisition that emphasizes the difficulty of learning verbs from naive distributional data. We discuss avenues for future work on cognitively-inspired grounded language learning, and release our corpus with the intent of facilitating research on the topic.

</details>

<details>

<summary>2020-10-28 23:55:00 - No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting with Adversarial Attacks</summary>

- *Siqi Liu, Arnaud Arindra Adiyoso Setio, Florin C. Ghesu, Eli Gibson, Sasa Grbic, Bogdan Georgescu, Dorin Comaniciu*

- `2003.03824v2` - [abs](http://arxiv.org/abs/2003.03824v2) - [pdf](http://arxiv.org/pdf/2003.03824v2)

> Detecting malignant pulmonary nodules at an early stage can allow medical interventions which may increase the survival rate of lung cancer patients. Using computer vision techniques to detect nodules can improve the sensitivity and the speed of interpreting chest CT for lung cancer screening. Many studies have used CNNs to detect nodule candidates. Though such approaches have been shown to outperform the conventional image processing based methods regarding the detection accuracy, CNNs are also known to be limited to generalize on under-represented samples in the training set and prone to imperceptible noise perturbations. Such limitations can not be easily addressed by scaling up the dataset or the models. In this work, we propose to add adversarial synthetic nodules and adversarial attack samples to the training data to improve the generalization and the robustness of the lung nodule detection systems. To generate hard examples of nodules from a differentiable nodule synthesizer, we use projected gradient descent (PGD) to search the latent code within a bounded neighbourhood that would generate nodules to decrease the detector response. To make the network more robust to unanticipated noise perturbations, we use PGD to search for noise patterns that can trigger the network to give over-confident mistakes. By evaluating on two different benchmark datasets containing consensus annotations from three radiologists, we show that the proposed techniques can improve the detection performance on real CT data. To understand the limitations of both the conventional networks and the proposed augmented networks, we also perform stress-tests on the false positive reduction networks by feeding different types of artificially produced patches. We show that the augmented networks are more robust to both under-represented nodules as well as resistant to noise perturbations.

</details>

<details>

<summary>2020-10-29 00:14:33 - Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks</summary>

- *Masood S. Mortazavi*

- `2010.15288v1` - [abs](http://arxiv.org/abs/2010.15288v1) - [pdf](http://arxiv.org/pdf/2010.15288v1)

> Semantically-aligned $(speech, image)$ datasets can be used to explore "visually-grounded speech". In a majority of existing investigations, features of an image signal are extracted using neural networks "pre-trained" on other tasks (e.g., classification on ImageNet). In still others, pre-trained networks are used to extract audio features prior to semantic embedding. Without "transfer learning" through pre-trained initialization or pre-trained feature extraction, previous results have tended to show low rates of recall in $speech \rightarrow image$ and $image \rightarrow speech$ queries.   Choosing appropriate neural architectures for encoders in the speech and image branches and using large datasets, one can obtain competitive recall rates without any reliance on any pre-trained initialization or feature extraction: $(speech,image)$ semantic alignment and $speech \rightarrow image$ and $image \rightarrow speech$ retrieval are canonical tasks worthy of independent investigation of their own and allow one to explore other questions---e.g., the size of the audio embedder can be reduced significantly with little loss of recall rates in $speech \rightarrow image$ and $image \rightarrow speech$ queries.

</details>

<details>

<summary>2020-10-29 02:07:05 - "where is this relationship going?": Understanding Relationship Trajectories in Narrative Text</summary>

- *Keen You, Dan Goldwasser*

- `2010.15313v1` - [abs](http://arxiv.org/abs/2010.15313v1) - [pdf](http://arxiv.org/pdf/2010.15313v1)

> We examine a new commonsense reasoning task: given a narrative describing a social interaction that centers on two protagonists, systems make inferences about the underlying relationship trajectory. Specifically, we propose two evaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction MCQ. In Relationship Outlook Prediction, a system maps an interaction to a relationship outlook that captures how the interaction is expected to change the relationship. In Resolution Prediction, a system attributes a given relationship outlook to a particular resolution that explains the outcome. These two tasks parallel two real-life questions that people frequently ponder upon as they navigate different social situations: "where is this relationship going?" and "how did we end up here?". To facilitate the investigation of human social relationships through these two tasks, we construct a new dataset, Social Narrative Tree, which consists of 1250 stories documenting a variety of daily social interactions. The narratives encode a multitude of social elements that interweave to give rise to rich commonsense knowledge of how relationships evolve with respect to social interactions. We establish baseline performances using language models and the accuracies are significantly lower than human performance. The results demonstrate that models need to look beyond syntactic and semantic signals to comprehend complex human relationships.

</details>

<details>

<summary>2020-10-29 03:46:04 - Self-Supervised Graph Transformer on Large-Scale Molecular Data</summary>

- *Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, Junzhou Huang*

- `2007.02835v2` - [abs](http://arxiv.org/abs/2007.02835v2) - [pdf](http://arxiv.org/pdf/2007.02835v2)

> How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for molecular representation learning. Nevertheless, two issues impede the usage of GNNs in real scenarios: (1) insufficient labeled molecules for supervised training; (2) poor generalization capability to new-synthesized molecules. To address them both, we propose a novel framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node-, edge- and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks into the Transformer-style architecture to deliver a class of more expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules -- the biggest GNN and the largest training dataset in molecular representation learning. We then leverage the pre-trained GROVER for molecular property prediction followed by task-specific fine-tuning, where we observe a huge improvement (more than 6% on average) from current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the significant potential on performance boosting.

</details>

<details>

<summary>2020-10-29 05:59:14 - Frontiers in Mortar Methods for Isogeometric Analysis</summary>

- *Christian Hesch, Ustim Khristenko, Rolf Krause, Alexander Popp, Alexander Seitz, Wolfgang Wall, Barbara Wohlmuth*

- `2006.06677v3` - [abs](http://arxiv.org/abs/2006.06677v3) - [pdf](http://arxiv.org/pdf/2006.06677v3)

> Complex geometries as common in industrial applications consist of multiple patches, if spline based parametrizations are used. The requirements for the generation of analysis-suitable models are increasing dramatically since isogeometric analysis is directly based on the spline parametrization and nowadays used for the calculation of higher-order partial differential equations. The computational, or more general, the engineering analysis necessitates suitable coupling techniques between the different patches. Mortar methods have been successfully applied for coupling of patches and for contact mechanics in recent years to resolve the arising issues within the interface. We present here current achievements in the design of mortar technologies in isogeometric analysis within the Priority Program SPP 1748, Reliable Simulation Techniques in Solid Mechanics. Development of Non-standard Discretisation Methods, Mechanical and Mathematical Analysis.

</details>

<details>

<summary>2020-10-29 08:36:47 - InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs</summary>

- *Yujun Shen, Ceyuan Yang, Xiaoou Tang, Bolei Zhou*

- `2005.09635v2` - [abs](http://arxiv.org/abs/2005.09635v2) - [pdf](http://arxiv.org/pdf/2005.09635v2)

> Although Generative Adversarial Networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.

</details>

<details>

<summary>2020-10-29 09:14:57 - Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery</summary>

- *Christina Corbane, Vasileios Syrris, Filip Sabo, Panagiotis Politis, Michele Melchiorri, Martino Pesaresi, Pierre Soille, Thomas Kemper*

- `2006.03267v2` - [abs](http://arxiv.org/abs/2006.03267v2) - [pdf](http://arxiv.org/pdf/2006.03267v2)

> Spatially consistent and up-to-date maps of human settlements are crucial for addressing policies related to urbanization and sustainability, especially in the era of an increasingly urbanized world.The availability of open and free Sentinel-2 data of the Copernicus Earth Observation program offers a new opportunity for wall-to-wall mapping of human settlements at a global scale.This paper presents a deep-learning-based framework for a fully automated extraction of built-up areas at a spatial resolution of 10 m from a global composite of Sentinel-2 imagery.A multi-neuro modeling methodology building on a simple Convolution Neural Networks architecture for pixel-wise image classification of built-up areas is developed.The core features of the proposed model are the image patch of size 5 x 5 pixels adequate for describing built-up areas from Sentinel-2 imagery and the lightweight topology with a total number of 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened layers.The deployment of the model on the global Sentinel-2 image composite provides the most detailed and complete map reporting about built-up areas for reference year 2018. The validation of the results with an independent reference data-set of building footprints covering 277 sites across the world establishes the reliability of the built-up layer produced by the proposed framework and the model robustness.

</details>

<details>

<summary>2020-10-29 10:06:46 - Named Entity Recognition for Social Media Texts with Semantic Augmentation</summary>

- *Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, Bo Dai*

- `2010.15458v1` - [abs](http://arxiv.org/abs/2010.15458v1) - [pdf](http://arxiv.org/pdf/2010.15458v1)

> Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.

</details>

<details>

<summary>2020-10-29 10:25:17 - Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information</summary>

- *Yuyang Nie, Yuanhe Tian, Yan Song, Xiang Ao, Xiang Wan*

- `2010.15466v1` - [abs](http://arxiv.org/abs/2010.15466v1) - [pdf](http://arxiv.org/pdf/2010.15466v1)

> Named entity recognition (NER) is highly sensitive to sentential syntactic and semantic properties where entities may be extracted according to how they are used and placed in the running text. To model such properties, one could rely on existing resources to providing helpful knowledge to the NER task; some existing studies proved the effectiveness of doing so, and yet are limited in appropriately leveraging the knowledge such as distinguishing the important ones for particular context. In this paper, we improve NER by leveraging different types of syntactic information through attentive ensemble, which functionalizes by the proposed key-value memory networks, syntax attention, and the gate mechanism for encoding, weighting and aggregating such syntactic information, respectively. Experimental results on six English and Chinese benchmark datasets suggest the effectiveness of the proposed model and show that it outperforms previous studies on all experiment datasets.

</details>

<details>

<summary>2020-10-29 13:45:16 - What it Thinks is Important is Important: Robustness Transfers through Input Gradients</summary>

- *Alvin Chan, Yi Tay, Yew-Soon Ong*

- `1912.05699v3` - [abs](http://arxiv.org/abs/1912.05699v3) - [pdf](http://arxiv.org/pdf/1912.05699v3)

> Adversarial perturbations are imperceptible changes to input pixels that can change the prediction of deep learning models. Learned weights of models robust to such perturbations are previously found to be transferable across different tasks but this applies only if the model architecture for the source and target tasks is the same. Input gradients characterize how small changes at each input pixel affect the model output. Using only natural images, we show here that training a student model's input gradients to match those of a robust teacher model can gain robustness close to a strong baseline that is robustly trained from scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and Tiny-ImageNet, we show that our proposed method, input gradient adversarial matching, can transfer robustness across different tasks and even across different model architectures. This demonstrates that directly targeting the semantics of input gradients is a feasible way towards adversarial robustness.

</details>

<details>

<summary>2020-10-29 19:58:43 - GripNet: Graph Information Propagation on Supergraph for Heterogeneous Graphs</summary>

- *Hao Xu, Shengqi Sang, Peizhen Bai, Laurence Yang, Haiping Lu*

- `2010.15914v1` - [abs](http://arxiv.org/abs/2010.15914v1) - [pdf](http://arxiv.org/pdf/2010.15914v1)

> Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Graph information propagation Network (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration.

</details>

<details>

<summary>2020-10-30 03:04:22 - Semantic Labeling Using a Deep Contextualized Language Model</summary>

- *Mohamed Trabelsi, Jin Cao, Jeff Heflin*

- `2010.16037v1` - [abs](http://arxiv.org/abs/2010.16037v1) - [pdf](http://arxiv.org/pdf/2010.16037v1)

> Generating schema labels automatically for column values of data tables has many data science applications such as schema matching, and data discovery and linking. For example, automatically extracted tables with missing headers can be filled by the predicted schema labels which significantly minimizes human effort. Furthermore, the predicted labels can reduce the impact of inconsistent names across multiple data tables. Understanding the connection between column values and contextual information is an important yet neglected aspect as previously proposed methods treat each column independently. In this paper, we propose a context-aware semantic labeling method using both the column values and context. Our new method is based on a new setting for semantic labeling, where we sequentially predict labels for an input table with missing headers. We incorporate both the values and context of each data column using the pre-trained contextualized language model, BERT, that has achieved significant improvements in multiple natural language processing tasks. To our knowledge, we are the first to successfully apply BERT to solve the semantic labeling task. We evaluate our approach using two real-world datasets from different domains, and we demonstrate substantial improvements in terms of evaluation metrics over state-of-the-art feature-based methods.

</details>

<details>

<summary>2020-10-30 03:47:06 - CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations</summary>

- *Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun*

- `2010.06351v4` - [abs](http://arxiv.org/abs/2010.06351v4) - [pdf](http://arxiv.org/pdf/2010.06351v4)

> Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6\% absolute gain on GLUE benchmarks and 0.8\% absolute increment on $\text{NLVR}^2$.

</details>

<details>

<summary>2020-10-30 04:30:09 - Logic-guided Semantic Representation Learning for Zero-Shot Relation Classification</summary>

- *Juan Li, Ruoxu Wang, Ningyu Zhang, Wen Zhang, Fan Yang, Huajun Chen*

- `2010.16068v1` - [abs](http://arxiv.org/abs/2010.16068v1) - [pdf](http://arxiv.org/pdf/2010.16068v1)

> Relation classification aims to extract semantic relations between entity pairs from the sentences. However, most existing methods can only identify seen relation classes that occurred during training. To recognize unseen relations at test time, we explore the problem of zero-shot relation classification. Previous work regards the problem as reading comprehension or textual entailment, which have to rely on artificial descriptive information to improve the understandability of relation types. Thus, rich semantic knowledge of the relation labels is ignored. In this paper, we propose a novel logic-guided semantic representation learning model for zero-shot relation classification. Our approach builds connections between seen and unseen relations via implicit and explicit semantic representations with knowledge graph embeddings and logic rules. Extensive experimental results demonstrate that our method can generalize to unseen relation types and achieve promising improvements.

</details>

<details>

<summary>2020-10-30 07:08:10 - Keep it Consistent: Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication</summary>

- *Ruize Wang, Zhongyu Wei, Ying Cheng, Piji Li, Haijun Shan, Ji Zhang, Qi Zhang, Xuanjing Huang*

- `1911.04192v2` - [abs](http://arxiv.org/abs/1911.04192v2) - [pdf](http://arxiv.org/pdf/1911.04192v2)

> Visual storytelling aims to generate a narrative paragraph from a sequence of images automatically. Existing approaches construct text description independently for each image and roughly concatenate them as a story, which leads to the problem of generating semantically incoherent content. In this paper, we propose a new way for visual storytelling by introducing a topic description task to detect the global semantic context of an image stream. A story is then constructed with the guidance of the topic description. In order to combine the two generation tasks, we propose a multi-agent communication framework that regards the topic description generator and the story generator as two agents and learn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method's good ability in generating stories with higher quality compared to state-of-the-art methods.

</details>

<details>

<summary>2020-10-30 08:34:35 - Auto-Panoptic: Cooperative Multi-Component Architecture Search for Panoptic Segmentation</summary>

- *Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang, Liang Lin*

- `2010.16119v1` - [abs](http://arxiv.org/abs/2010.16119v1) - [pdf](http://arxiv.org/pdf/2010.16119v1)

> Panoptic segmentation is posed as a new popular test-bed for the state-of-the-art holistic scene understanding methods with the requirement of simultaneously segmenting both foreground things and background stuff. The state-of-the-art panoptic segmentation network exhibits high structural complexity in different network components, i.e. backbone, proposal-based foreground branch, segmentation-based background branch, and feature fusion module across branches, which heavily relies on expert knowledge and tedious trials. In this work, we propose an efficient, cooperative and highly automated framework to simultaneously search for all main components including backbone, segmentation branches, and feature fusion module in a unified panoptic segmentation pipeline based on the prevailing one-shot Network Architecture Search (NAS) paradigm. Notably, we extend the common single-task NAS into the multi-component scenario by taking the advantage of the newly proposed intra-modular search space and problem-oriented inter-modular search space, which helps us to obtain an optimal network architecture that not only performs well in both instance segmentation and semantic segmentation tasks but also be aware of the reciprocal relations between foreground things and background stuff classes. To relieve the vast computation burden incurred by applying NAS to complicated network architectures, we present a novel path-priority greedy search policy to find a robust, transferrable architecture with significantly reduced searching overhead. Our searched architecture, namely Auto-Panoptic, achieves the new state-of-the-art on the challenging COCO and ADE20K benchmarks. Moreover, extensive experiments are conducted to demonstrate the effectiveness of path-priority policy and transferability of Auto-Panoptic across different datasets. Codes and models are available at: https://github.com/Jacobew/AutoPanoptic.

</details>

<details>

<summary>2020-10-30 12:16:45 - Thinking About Causation: A Causal Language with Epistemic Operators</summary>

- *Fausto Barbero, Katrin Schulz, Sonja Smets, Fernando R. Velázquez-Quesada, Kaibo Xie*

- `2010.16217v1` - [abs](http://arxiv.org/abs/2010.16217v1) - [pdf](http://arxiv.org/pdf/2010.16217v1)

> This paper proposes a formal framework for modeling the interaction of causal and (qualitative) epistemic reasoning. To this purpose, we extend the notion of a causal model with a representation of the epistemic state of an agent. On the side of the object language, we add operators to express knowledge and the act of observing new information. We provide a sound and complete axiomatization of the logic, and discuss the relation of this framework to causal team semantics.

</details>

<details>

<summary>2020-10-31 02:20:32 - Multimodal and self-supervised representation learning for automatic gesture recognition in surgical robotics</summary>

- *Aniruddha Tamhane, Jie Ying Wu, Mathias Unberath*

- `2011.00168v1` - [abs](http://arxiv.org/abs/2011.00168v1) - [pdf](http://arxiv.org/pdf/2011.00168v1)

> Self-supervised, multi-modal learning has been successful in holistic representation of complex scenarios. This can be useful to consolidate information from multiple modalities which have multiple, versatile uses. Its application in surgical robotics can lead to simultaneously developing a generalised machine understanding of the surgical process and reduce the dependency on quality, expert annotations which are generally difficult to obtain. We develop a self-supervised, multi-modal representation learning paradigm that learns representations for surgical gestures from video and kinematics. We use an encoder-decoder network configuration that encodes representations from surgical videos and decodes them to yield kinematics. We quantitatively demonstrate the efficacy of our learnt representations for gesture recognition (with accuracy between 69.6 % and 77.8 %), transfer learning across multiple tasks (with accuracy between 44.6 % and 64.8 %) and surgeon skill classification (with accuracy between 76.8 % and 81.2 %). Further, we qualitatively demonstrate that our self-supervised representations cluster in semantically meaningful properties (surgeon skill and gestures).

</details>

<details>

<summary>2020-10-31 02:21:43 - Understanding Pre-trained BERT for Aspect-based Sentiment Analysis</summary>

- *Hu Xu, Lei Shu, Philip S. Yu, Bing Liu*

- `2011.00169v1` - [abs](http://arxiv.org/abs/2011.00169v1) - [pdf](http://arxiv.org/pdf/2011.00169v1)

> This paper analyzes the pre-trained hidden representations learned from reviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work is motivated by the recent progress in BERT-based language models for ABSA. However, it is not clear how the general proxy task of (masked) language model trained on unlabeled corpus without annotations of aspects or opinions can provide important features for downstream tasks in ABSA. By leveraging the annotated datasets in ABSA, we investigate both the attentions and the learned representations of BERT pre-trained on reviews. We found that BERT uses very few self-attention heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the fine-grained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-RRC-ABSA.

</details>

<details>

<summary>2020-10-31 04:41:55 - Structured Convolutions for Efficient Neural Network Design</summary>

- *Yash Bhalgat, Yizhe Zhang, Jamie Lin, Fatih Porikli*

- `2008.02454v2` - [abs](http://arxiv.org/abs/2008.02454v2) - [pdf](http://arxiv.org/pdf/2008.02454v2)

> In this work, we tackle model efficiency by exploiting redundancy in the \textit{implicit structure} of the building blocks of convolutional neural networks. We start our analysis by introducing a general definition of Composite Kernel structures that enable the execution of convolution operations in the form of efficient, scaled, sum-pooling components. As its special case, we propose \textit{Structured Convolutions} and show that these allow decomposition of the convolution operation into a sum-pooling operation followed by a convolution with significantly lower complexity and fewer weights. We show how this decomposition can be applied to 2D and 3D kernels as well as the fully-connected layers. Furthermore, we present a Structural Regularization loss that promotes neural network layers to leverage on this desired structure in a way that, after training, they can be decomposed with negligible performance loss. By applying our method to a wide range of CNN architectures, we demonstrate "structured" versions of the ResNets that are up to 2$\times$ smaller and a new Structured-MobileNetV2 that is more efficient while staying within an accuracy loss of 1% on ImageNet and CIFAR-10 datasets. We also show similar structured versions of EfficientNet on ImageNet and HRNet architecture for semantic segmentation on the Cityscapes dataset. Our method performs equally well or superior in terms of the complexity reduction in comparison to the existing tensor decomposition and channel pruning methods.

</details>

<details>

<summary>2020-10-31 16:48:55 - Method of the coherence evaluation of Ukrainian text</summary>

- *S. D. Pogorilyy, A. A. Kramov*

- `2011.00310v1` - [abs](http://arxiv.org/abs/2011.00310v1) - [pdf](http://arxiv.org/pdf/2011.00310v1)

> Due to the growing role of the SEO technologies, it is necessary to perform an automated analysis of the article's quality. Such approach helps both to return the most intelligible pages for the user's query and to raise the web sites positions to the top of query results. An automated assessment of a coherence is a part of the complex analysis of the text. In this article, main methods for text coherence measurements for Ukrainian language are analyzed. Expediency of using the semantic similarity graph method in comparison with other methods are explained. It is suggested the improvement of that method by the pre-training of the neural network for vector representations of sentences. Experimental examination of the original method and its modifications is made. Training and examination procedures are made on the corpus of Ukrainian texts, which were previously retrieved from abstracts and full texts of Ukrainian scientific articles. The testing procedure is implemented by performing of two typical tasks for the text coherence assessment: document discrimination task and insertion task. Accordingly to the analysis it is defined the most effective combination of method's modification and its parameter for the measurement of the text coherence.

</details>

<details>

<summary>2020-10-31 18:44:07 - Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage</summary>

- *Ella Rabinovich, Hila Gonen, Suzanne Stevenson*

- `2011.00335v1` - [abs](http://arxiv.org/abs/2011.00335v1) - [pdf](http://arxiv.org/pdf/2011.00335v1)

> A large body of research on gender-linked language has established foundations regarding cross-gender differences in lexical, emotional, and topical preferences, along with their sociological underpinnings. We compile a novel, large and diverse corpus of spontaneous linguistic productions annotated with speakers' gender, and perform a first large-scale empirical study of distinctions in the usage of \textit{figurative language} between male and female authors. Our analyses suggest that (1) idiomatic choices reflect gender-specific lexical and semantic preferences in general language, (2) men's and women's idiomatic usages express higher emotion than their literal language, with detectable, albeit more subtle, differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances, and (3) contextual analysis of idiomatic expressions reveals considerable differences, reflecting subtle divergences in usage environments, shaped by cross-gender communication styles and semantic biases.

</details>

<details>

<summary>2020-10-31 19:37:22 - Aspectuality Across Genre: A Distributional Semantics Approach</summary>

- *Thomas Kober, Malihe Alikhani, Matthew Stone, Mark Steedman*

- `2011.00345v1` - [abs](http://arxiv.org/abs/2011.00345v1) - [pdf](http://arxiv.org/pdf/2011.00345v1)

> The interpretation of the lexical aspect of verbs in English plays a crucial role for recognizing textual entailment and learning discourse-level inferences. We show that two elementary dimensions of aspectual class, states vs. events, and telic vs. atelic events, can be modelled effectively with distributional semantics. We find that a verb's local context is most indicative of its aspectual class, and demonstrate that closed class words tend to be stronger discriminating contexts than content words. Our approach outperforms previous work on three datasets. Lastly, we contribute a dataset of human--human conversations annotated with lexical aspect and present experiments that show the correlation of telicity with genre and discourse goals.

</details>


## 2020-11

<details>

<summary>2020-11-01 05:15:37 - Two-layer clustering-based sparsifying transform learning for low-dose CT reconstruction</summary>

- *Xikai Yang, Yong Long, Saiprasad Ravishankar*

- `2011.00428v1` - [abs](http://arxiv.org/abs/2011.00428v1) - [pdf](http://arxiv.org/pdf/2011.00428v1)

> Achieving high-quality reconstructions from low-dose computed tomography (LDCT) measurements is of much importance in clinical settings. Model-based image reconstruction methods have been proven to be effective in removing artifacts in LDCT. In this work, we propose an approach to learn a rich two-layer clustering-based sparsifying transform model (MCST2), where image patches and their subsequent feature maps (filter residuals) are clustered into groups with different learned sparsifying filters per group. We investigate a penalized weighted least squares (PWLS) approach for LDCT reconstruction incorporating learned MCST2 priors. Experimental results show the superior performance of the proposed PWLS-MCST2 approach compared to other related recent schemes.

</details>

<details>

<summary>2020-11-01 11:34:50 - Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems</summary>

- *Vitou Phy, Yang Zhao, Akiko Aizawa*

- `2011.00483v1` - [abs](http://arxiv.org/abs/2011.00483v1) - [pdf](http://arxiv.org/pdf/2011.00483v1)

> Many automatic evaluation metrics have been proposed to score the overall quality of a response in open-domain dialogue. Generally, the overall quality is comprised of various aspects, such as relevancy, specificity, and empathy, and the importance of each aspect differs according to the task. For instance, specificity is mandatory in a food-ordering dialogue task, whereas fluency is preferred in a language-teaching dialogue system. However, existing metrics are not designed to cope with such flexibility. For example, BLEU score fundamentally relies only on word overlapping, whereas BERTScore relies on semantic similarity between reference and candidate response. Thus, they are not guaranteed to capture the required aspects, i.e., specificity. To design a metric that is flexible to a task, we first propose making these qualities manageable by grouping them into three groups: understandability, sensibleness, and likability, where likability is a combination of qualities that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single metric called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy. We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics.

</details>

<details>

<summary>2020-11-01 14:36:49 - Fine-grained Information Status Classification Using Discourse Context-Aware BERT</summary>

- *Yufang Hou*

- `2010.14759v2` - [abs](http://arxiv.org/abs/2010.14759v2) - [pdf](http://arxiv.org/pdf/2010.14759v2)

> Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the problem as a subtask of learning fine-grained information status (IS). However, these systems heavily depend on many hand-crafted linguistic features. In this paper, we propose a simple discourse context-aware BERT model for fine-grained IS classification. On the ISNotes corpus (Markert et al., 2012), our model achieves new state-of-the-art performance on fine-grained IS classification, obtaining a 4.8 absolute overall accuracy improvement compared to Hou et al. (2013a). More importantly, we also show an improvement of 10.5 F1 points for bridging anaphora recognition without using any complex hand-crafted semantic features designed for capturing the bridging phenomenon. We further analyze the trained model and find that the most attended signals for each IS category correspond well to linguistic notions of information status.

</details>

<details>

<summary>2020-11-01 15:59:24 - Semantic coordinates analysis reveals language changes in the AI field</summary>

- *Zining Zhu, Yang Xu, Frank Rudzicz*

- `2011.00543v1` - [abs](http://arxiv.org/abs/2011.00543v1) - [pdf](http://arxiv.org/pdf/2011.00543v1)

> Semantic shifts can reflect changes in beliefs across hundreds of years, but it is less clear whether trends in fast-changing communities across a short time can be detected. We propose semantic coordinates analysis, a method based on semantic shifts, that reveals changes in language within publications of a field (we use AI as example) across a short time span. We use GloVe-style probability ratios to quantify the shifting directions and extents from multiple viewpoints. We show that semantic coordinates analysis can detect shifts echoing changes of research interests (e.g., "deep" shifted further from "rigorous" to "neural"), and developments of research activities (e,g., "collaboration" contains less "competition" than "collaboration"), based on publications spanning as short as 10 years.

</details>

<details>

<summary>2020-11-01 16:04:33 - Enhancing lexical-based approach with external knowledge for Vietnamese multiple-choice machine reading comprehension</summary>

- *Kiet Van Nguyen, Khiem Vinh Tran, Son T. Luu, Anh Gia-Tuan Nguyen, Ngan Luu-Thuy Nguyen*

- `2001.05687v5` - [abs](http://arxiv.org/abs/2001.05687v5) - [pdf](http://arxiv.org/pdf/2001.05687v5)

> Although Vietnamese is the 17th most popular native-speaker language in the world, there are not many research studies on Vietnamese machine reading comprehension (MRC), the task of understanding a text and answering questions about it. One of the reasons is because of the lack of high-quality benchmark datasets for this task. In this work, we construct a dataset which consists of 2,783 pairs of multiple-choice questions and answers based on 417 Vietnamese texts which are commonly used for teaching reading comprehension for elementary school pupils. In addition, we propose a lexical-based MRC method that utilizes semantic similarity measures and external knowledge sources to analyze questions and extract answers from the given text. We compare the performance of the proposed model with several baseline lexical-based and neural network-based models. Our proposed method achieves 61.81% by accuracy, which is 5.51% higher than the best baseline model. We also measure human performance on our dataset and find that there is a big gap between machine-model and human performances. This indicates that significant progress can be made on this task. The dataset is freely available on our website for research purposes.

</details>

<details>

<summary>2020-11-01 16:47:17 - Institution-based Encoding and Verification of Simple UML State Machines in CASL/SPASS</summary>

- *Tobias Rosenberger, Saddek Bensalem, Alexander Knapp, Markus Roggenbach*

- `2011.00556v1` - [abs](http://arxiv.org/abs/2011.00556v1) - [pdf](http://arxiv.org/pdf/2011.00556v1)

> This paper provides the first correct semantical representation of UML state-machines within the logical framework of an institution (previous attempts were flawed). A novel encoding of this representation into first-order logic enables symbolic analyses through a multitude of theorem-provers.   UML state-machines are central to model-based systems-engineering. Till now, state-machine analysis has been mostly restricted to model checking, which for state-machines suffers heavily from the state-space explosion problem. Symbolic reasoning, as enabled and demonstrated here, provides a powerful alternative, which can deal with large or even infinite state spaces.   Full proofs are given.

</details>

<details>

<summary>2020-11-01 18:54:09 - COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning</summary>

- *Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, Thomas Brox*

- `2011.00597v1` - [abs](http://arxiv.org/abs/2011.00597v1) - [pdf](http://arxiv.org/pdf/2011.00597v1)

> Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext

</details>

<details>

<summary>2020-11-02 01:43:40 - Definition Frames: Using Definitions for Hybrid Concept Representations</summary>

- *Evangelia Spiliopoulou, Artidoro Pagnoni, Eduard Hovy*

- `1909.04793v2` - [abs](http://arxiv.org/abs/1909.04793v2) - [pdf](http://arxiv.org/pdf/1909.04793v2)

> Advances in word representations have shown tremendous improvements in downstream NLP tasks, but lack semantic interpretability. In this paper, we introduce Definition Frames (DF), a matrix distributed representation extracted from definitions, where each dimension is semantically interpretable. DF dimensions correspond to the Qualia structure relations: a set of relations that uniquely define a term. Our results show that DFs have competitive performance with other distributional semantic approaches on word similarity tasks.

</details>

<details>

<summary>2020-11-02 01:54:56 - IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP</summary>

- *Fajri Koto, Afshin Rahimi, Jey Han Lau, Timothy Baldwin*

- `2011.00677v1` - [abs](http://arxiv.org/abs/2011.00677v1) - [pdf](http://arxiv.org/pdf/2011.00677v1)

> Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.

</details>

<details>

<summary>2020-11-02 02:06:33 - Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora</summary>

- *Robert Frank, Jackson Petty*

- `2011.00682v1` - [abs](http://arxiv.org/abs/2011.00682v1) - [pdf](http://arxiv.org/pdf/2011.00682v1)

> Reflexive anaphora present a challenge for semantic interpretation: their meaning varies depending on context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. In this paper, we explore this question in the context of a fragment of English that incorporates the relevant sort of contextual variability. We consider sequence-to-sequence architectures with recurrent units and show that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success as measured in two ways: how much lexical support is needed to induce an abstract reflexive meaning (i.e., how many distinct reflexive antecedents must occur during training) and what contexts must a noun phrase occur in to support generalization of reflexive interpretation to this noun phrase?

</details>

<details>

<summary>2020-11-02 02:40:58 - Reference and Document Aware Semantic Evaluation Methods for Korean Language Summarization</summary>

- *Dongyub Lee, Myeongcheol Shin, Taesun Whang, Seungwoo Cho, Byeongil Ko, Daniel Lee, Eunggyun Kim, Jaechoon Jo*

- `2005.03510v2` - [abs](http://arxiv.org/abs/2005.03510v2) - [pdf](http://arxiv.org/pdf/2005.03510v2)

> Text summarization refers to the process that generates a shorter form of text from the source document preserving salient information. Many existing works for text summarization are generally evaluated by using recall-oriented understudy for gisting evaluation (ROUGE) scores. However, as ROUGE scores are computed based on n-gram overlap, they do not reflect semantic meaning correspondences between generated and reference summaries. Because Korean is an agglutinative language that combines various morphemes into a word that express several meanings, ROUGE is not suitable for Korean summarization. In this paper, we propose evaluation metrics that reflect semantic meanings of a reference summary and the original document, Reference and Document Aware Semantic Score (RDASS). We then propose a method for improving the correlation of the metrics with human judgment. Evaluation results show that the correlation with human judgment is significantly higher for our evaluation metrics than for ROUGE scores.

</details>

<details>

<summary>2020-11-02 03:39:41 - Revealing Secrets in SPARQL Session Level</summary>

- *Xinyue Zhang, Meng Wang, Muhammad Saleem, Axel-Cyrille Ngonga Ngomo, Guilin Qi, Haofen Wang*

- `2009.06625v2` - [abs](http://arxiv.org/abs/2009.06625v2) - [pdf](http://arxiv.org/pdf/2009.06625v2)

> Based on Semantic Web technologies, knowledge graphs help users to discover information of interest by using live SPARQL services. Answer-seekers often examine intermediate results iteratively and modify SPARQL queries repeatedly in a search session. In this context, understanding user behaviors is critical for effective intention prediction and query optimization. However, these behaviors have not yet been researched systematically at the SPARQL session level. This paper reveals secrets of session-level user search behaviors by conducting a comprehensive investigation over massive real-world SPARQL query logs. In particular, we thoroughly assess query changes made by users w.r.t. structural and data-driven features of SPARQL queries. To illustrate the potentiality of our findings, we employ an application example of how to use our findings, which might be valuable to devise efficient SPARQL caching, auto-completion, query suggestion, approximation, and relaxation techniques in the future.

</details>

<details>

<summary>2020-11-02 05:47:08 - ÚFAL at MRP 2020: Permutation-invariant Semantic Parsing in PERIN</summary>

- *David Samuel, Milan Straka*

- `2011.00758v1` - [abs](http://arxiv.org/abs/2011.00758v1) - [pdf](http://arxiv.org/pdf/2011.00758v1)

> We present PERIN, a novel permutation-invariant approach to sentence-to-graph semantic parsing. PERIN is a versatile, cross-framework and language independent architecture for universal modeling of semantic structures. Our system participated in the CoNLL 2020 shared task, Cross-Framework Meaning Representation Parsing (MRP 2020), where it was evaluated on five different frameworks (AMR, DRG, EDS, PTG and UCCA) across four languages. PERIN was one of the winners of the shared task. The source code and pretrained models are available at https://github.com/ufal/perin.

</details>

<details>

<summary>2020-11-02 06:52:35 - A Curious New Result of Resolution Strategies in Negation-Limited Inverters Problem</summary>

- *Ruo Ando, Yoshiyasu Takefuji*

- `2011.00775v1` - [abs](http://arxiv.org/abs/2011.00775v1) - [pdf](http://arxiv.org/pdf/2011.00775v1)

> Generally, negation-limited inverters problem is known as a puzzle of constructing an inverter with AND gates and OR gates and a few inverters. In this paper, we introduce a curious new result about the effectiveness of two powerful ATP (Automated Theorem Proving) strategies on tackling negation limited inverter problem. Two resolution strategies are UR (Unit Resulting) resolution and hyper-resolution. In experiment, we come two kinds of automated circuit construction: 3 input/output inverters and 4 input/output BCD Counter Circuit. Both circuits are constructed with a few limited inverters. Curiously, it has been turned out that UR resolution is drastically faster than hyper-resolution in the measurement of the size of SOS (Set of Support). Besides, we discuss the syntactic and semantic criteria which might causes considerable difference of computation cost between UR resolution and hyper-resolution.

</details>

<details>

<summary>2020-11-02 07:51:05 - Context Dependent Semantic Parsing: A Survey</summary>

- *Zhuang Li, Lizhen Qu, Gholamreza Haffari*

- `2011.00797v1` - [abs](http://arxiv.org/abs/2011.00797v1) - [pdf](http://arxiv.org/pdf/2011.00797v1)

> Semantic parsing is the task of translating natural language utterances into machine-readable meaning representations. Currently, most semantic parsing methods are not able to utilize contextual information (e.g. dialogue and comments history), which has a great potential to boost semantic parsing performance. To address this issue, context dependent semantic parsing has recently drawn a lot of attention. In this survey, we investigate progress on the methods for the context dependent semantic parsing, together with the current datasets and tasks. We then point out open problems and challenges for future research in this area. The collected resources for this topic are available at:https://github.com/zhuang-li/Contextual-Semantic-Parsing-Paper-List.

</details>

<details>

<summary>2020-11-02 09:03:46 - Comparison by Conversion: Reverse-Engineering UCCA from Syntax and Lexical Semantics</summary>

- *Daniel Hershcovich, Nathan Schneider, Dotan Dvir, Jakob Prange, Miryam de Lhoneux, Omri Abend*

- `2011.00834v1` - [abs](http://arxiv.org/abs/2011.00834v1) - [pdf](http://arxiv.org/pdf/2011.00834v1)

> Building robust natural language understanding systems will require a clear characterization of whether and how various linguistic meaning representations complement each other. To perform a systematic comparative analysis, we evaluate the mapping between meaning representations from different frameworks using two complementary methods: (i) a rule-based converter, and (ii) a supervised delexicalized parser that parses to one framework using only information from the other as features. We apply these methods to convert the STREUSLE corpus (with syntactic and lexical semantic annotations) to UCCA (a graph-structured full-sentence meaning representation). Both methods yield surprisingly accurate target representations, close to fully supervised UCCA parser quality---indicating that UCCA annotations are partially redundant with STREUSLE annotations. Despite this substantial convergence between frameworks, we find several important areas of divergence.

</details>

<details>

<summary>2020-11-02 10:43:10 - 3D Self-Supervised Methods for Medical Imaging</summary>

- *Aiham Taleb, Winfried Loetzsch, Noel Danz, Julius Severin, Thomas Gaertner, Benjamin Bergner, Christoph Lippert*

- `2006.03829v3` - [abs](http://arxiv.org/abs/2006.03829v3) - [pdf](http://arxiv.org/pdf/2006.03829v3)

> Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.

</details>

<details>

<summary>2020-11-02 13:14:57 - On the Sentence Embeddings from Pre-trained Language Models</summary>

- *Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li*

- `2011.05864v1` - [abs](http://arxiv.org/abs/2011.05864v1) - [pdf](http://arxiv.org/pdf/2011.05864v1)

> Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.

</details>

<details>

<summary>2020-11-02 13:25:39 - A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English</summary>

- *Marius Mosbach, Stefania Degaetano-Ortlieb, Marie-Pauline Krielke, Badr M. Abdullah, Dietrich Klakow*

- `2011.00960v1` - [abs](http://arxiv.org/abs/2011.00960v1) - [pdf](http://arxiv.org/pdf/2011.00960v1)

> Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance. Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models' performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.

</details>

<details>

<summary>2020-11-02 13:27:21 - Combining Event Semantics and Degree Semantics for Natural Language Inference</summary>

- *Izumi Haruta, Koji Mineshima, Daisuke Bekki*

- `2011.00961v1` - [abs](http://arxiv.org/abs/2011.00961v1) - [pdf](http://arxiv.org/pdf/2011.00961v1)

> In formal semantics, there are two well-developed semantic frameworks: event semantics, which treats verbs and adverbial modifiers using the notion of event, and degree semantics, which analyzes adjectives and comparatives using the notion of degree. However, it is not obvious whether these frameworks can be combined to handle cases in which the phenomena in question are interacting with each other. Here, we study this issue by focusing on natural language inference (NLI). We implement a logic-based NLI system that combines event semantics and degree semantics and their interaction with lexical knowledge. We evaluate the system on various NLI datasets containing linguistically challenging problems. The results show that the system achieves high accuracies on these datasets in comparison with previous logic-based systems and deep-learning-based systems. This suggests that the two semantic frameworks can be combined consistently to handle various combinations of linguistic phenomena without compromising the advantage of either framework.

</details>

<details>

<summary>2020-11-02 13:50:59 - DNN-Based Semantic Model for Rescoring N-best Speech Recognition List</summary>

- *Dominique Fohr, Irina Illina*

- `2011.00975v1` - [abs](http://arxiv.org/abs/2011.00975v1) - [pdf](http://arxiv.org/pdf/2011.00975v1)

> The word error rate (WER) of an automatic speech recognition (ASR) system increases when a mismatch occurs between the training and the testing conditions due to the noise, etc. In this case, the acoustic information can be less reliable. This work aims to improve ASR by modeling long-term semantic relations to compensate for distorted acoustic features. We propose to perform this through rescoring of the ASR N-best hypotheses list. To achieve this, we train a deep neural network (DNN). Our DNN rescoring model is aimed at selecting hypotheses that have better semantic consistency and therefore lower WER. We investigate two types of representations as part of input features to our DNN model: static word embeddings (from word2vec) and dynamic contextual embeddings (from BERT). Acoustic and linguistic features are also included. We perform experiments on the publicly available dataset TED-LIUM mixed with real noise. The proposed rescoring approaches give significant improvement of the WER over the ASR system without rescoring models in two noisy conditions and with n-gram and RNNLM.

</details>

<details>

<summary>2020-11-02 14:25:58 - Towards Topic-Guided Conversational Recommender System</summary>

- *Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang, Ji-Rong Wen*

- `2010.04125v2` - [abs](http://arxiv.org/abs/2010.04125v2) - [pdf](http://arxiv.org/pdf/2010.04125v2)

> Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. To develop an effective CRS, the support of high-quality datasets is essential. Existing CRS datasets mainly focus on immediate requests from users, while lack proactive guidance to the recommendation scenario. In this paper, we contribute a new CRS dataset named \textbf{TG-ReDial} (\textbf{Re}commendation through \textbf{T}opic-\textbf{G}uided \textbf{Dial}og). Our dataset has two major features. First, it incorporates topic threads to enforce natural semantic transitions towards the recommendation scenario. Second, it is created in a semi-automatic way, hence human annotation is more reasonable and controllable. Based on TG-ReDial, we present the task of topic-guided conversational recommendation, and propose an effective approach to this task. Extensive experiments have demonstrated the effectiveness of our approach on three sub-tasks, namely topic prediction, item recommendation and response generation. TG-ReDial is available at https://github.com/RUCAIBox/TG-ReDial.

</details>

<details>

<summary>2020-11-02 16:55:40 - Gibbs Sampling with People</summary>

- *Peter M. C. Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn, Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori Jacoby*

- `2008.02595v2` - [abs](http://arxiv.org/abs/2008.02595v2) - [pdf](http://arxiv.org/pdf/2008.02595v2)

> A core problem in cognitive science and machine learning is to understand how humans derive semantic representations from perceptual objects, such as color from an apple, pleasantness from a musical chord, or seriousness from a face. Markov Chain Monte Carlo with People (MCMCP) is a prominent method for studying such representations, in which participants are presented with binary choice trials constructed such that the decisions follow a Markov Chain Monte Carlo acceptance rule. However, while MCMCP has strong asymptotic properties, its binary choice paradigm generates relatively little information per trial, and its local proposal function makes it slow to explore the parameter space and find the modes of the distribution. Here we therefore generalize MCMCP to a continuous-sampling paradigm, where in each iteration the participant uses a slider to continuously manipulate a single stimulus dimension to optimize a given criterion such as 'pleasantness'. We formulate both methods from a utility-theory perspective, and show that the new method can be interpreted as 'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation parameter to the transition step, and show that this parameter can be manipulated to flexibly shift between Gibbs sampling and deterministic optimization. In an initial study, we show GSP clearly outperforming MCMCP; we then show that GSP provides novel and interpretable results in three other domains, namely musical chords, vocal emotions, and faces. We validate these results through large-scale perceptual rating experiments. The final experiments use GSP to navigate the latent space of a state-of-the-art image synthesis network (StyleGAN), a promising approach for applying GSP to high-dimensional perceptual spaces. We conclude by discussing future cognitive applications and ethical implications.

</details>

<details>

<summary>2020-11-02 18:41:32 - The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks</summary>

- *Brihi Joshi, Neil Shah, Francesco Barbieri, Leonardo Neves*

- `2011.01196v1` - [abs](http://arxiv.org/abs/2011.01196v1) - [pdf](http://arxiv.org/pdf/2011.01196v1)

> Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as question answering, sentiment analysis, and textual similarity in recent years. Extensive work shows how accurately such models can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such models' performance on tasks that require a more granular level of representation. We focus on the problem of textual similarity from two perspectives: matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring embeddings to capture overall textual semantics). We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into models that use contextual embeddings, achieving relative improvements of up to 36% on granular tasks.

</details>

<details>

<summary>2020-11-02 19:11:29 - MZET: Memory Augmented Zero-Shot Fine-grained Named Entity Typing</summary>

- *Tao Zhang, Congying Xia, Chun-Ta Lu, Philip Yu*

- `2004.01267v2` - [abs](http://arxiv.org/abs/2004.01267v2) - [pdf](http://arxiv.org/pdf/2004.01267v2)

> Named entity typing (NET) is a classification task of assigning an entity mention in the context with given semantic types. However, with the growing size and granularity of the entity types, rare researches in previous concern with newly emerged entity types. In this paper, we propose MZET, a novel memory augmented FNET (Fine-grained NET) model, to tackle the unseen types in a zero-shot manner. MZET incorporates character-level, word-level, and contextural-level information to learn the entity mention representation. Besides, MZET considers the semantic meaning and the hierarchical structure into the entity type representation. Finally, through the memory component which models the relationship between the entity mention and the entity type, MZET transfer the knowledge from seen entity types to the zero-shot ones. Extensive experiments on three public datasets show prominent performance obtained by MZET, which surpasses the state-of-the-art FNET neural network models with up to 7% gain in Micro-F1 and Macro-F1 score.

</details>

<details>

<summary>2020-11-02 19:19:08 - There's No Trick, Its Just a Simple Trick: A Web-Compat and Privacy Improving Approach to Third-party Web Storage</summary>

- *Jordan Jueckstock, Peter Snyder, Shaown Sarker, Alexandros Kapravelos, Benjamin Livshits*

- `2011.01267v1` - [abs](http://arxiv.org/abs/2011.01267v1) - [pdf](http://arxiv.org/pdf/2011.01267v1)

> While much current web privacy research focuses on browser fingerprinting, the boring fact is that the majority of current third-party web tracking is conducted using traditional, persistent-state identifiers. One possible explanation for the privacy community's focus on fingerprinting is that to date browsers have faced a lose-lose dilemma when dealing with third-party stateful identifiers: block state in third-party frames and break a significant number of webpages, or allow state in third-party frames and enable pervasive tracking. The alternative, middle-ground solutions that have been deployed all trade privacy for compatibility, rely on manually curated lists, or depend on the user to manage state and state-access themselves. This work furthers privacy on the web by presenting a novel system for managing the lifetime of third-party storage, "page-length storage". We compare page-length storage to existing approaches for managing third-party state and find that page-length storage has the privacy protections of the most restrictive current option (i.e., blocking third-party storage) but web-compatibility properties mostly similar to the least restrictive option (i.e., allowing all third-party storage). This work further compares page-length storage to an alternative third-party storage partitioning scheme and finds that page-length storage provides superior privacy protections with comparable web-compatibility. We provide a dataset of the privacy and compatibility behaviors observed when applying the compared third-party storage strategies on a crawl of the Tranco 1k and the quantitative metrics used to demonstrate that page-length storage matches or surpasses existing approaches. Finally, we provide an open-source implementation of our page-length storage approach, implemented as patches against Chromium.

</details>

<details>

<summary>2020-11-02 19:28:15 - CORD19STS: COVID-19 Semantic Textual Similarity Dataset</summary>

- *Xiao Guo, Hengameh Mirzaalian, Ekraam Sabir, Ayush Jaiswal, Wael Abd-Almageed*

- `2007.02461v2` - [abs](http://arxiv.org/abs/2007.02461v2) - [pdf](http://arxiv.org/pdf/2007.02461v2)

> In order to combat the COVID-19 pandemic, society can benefit from various natural language processing applications, such as dialog medical diagnosis systems and information retrieval engines calibrated specifically for COVID-19. These applications rely on the ability to measure semantic textual similarity (STS), making STS a fundamental task that can benefit several downstream applications. However, existing STS datasets and models fail to translate their performance to a domain-specific environment such as COVID-19. To overcome this gap, we introduce CORD19STS dataset which includes 13,710 annotated sentence pairs collected from COVID-19 open research dataset (CORD-19) challenge. To be specific, we generated one million sentence pairs using different sampling strategies. We then used a finetuned BERT-like language model, which we call Sen-SCI-CORD19-BERT, to calculate the similarity scores between sentence pairs to provide a balanced dataset with respect to the different semantic similarity levels, which gives us a total of 32K sentence pairs. Each sentence pair was annotated by five Amazon Mechanical Turk (AMT) crowd workers, where the labels represent different semantic similarity levels between the sentence pairs (i.e. related, somewhat-related, and not-related). After employing a rigorous qualification tasks to verify collected annotations, our final CORD19STS dataset includes 13,710 sentence pairs.

</details>

<details>

<summary>2020-11-02 19:36:24 - Completing and Debugging Ontologies: state of the art and challenges</summary>

- *Patrick Lambrix*

- `1908.03171v2` - [abs](http://arxiv.org/abs/1908.03171v2) - [pdf](http://arxiv.org/pdf/1908.03171v2)

> As semantically-enabled applications require high-quality ontologies, developing and maintaining ontologies that are as correct and complete as possible is an important although difficult task in ontology engineering. A key step is ontology debugging and completion. In general, there are two steps: detecting defects and repairing defects. In this paper we discuss the state of the art regarding the repairing step. We do this by formalizing the repairing step as an abduction problem and situating the state of the art with respect to this framework. We show that there are still many open research problems and show opportunities for further work and advancing the field.

</details>

<details>

<summary>2020-11-03 05:06:41 - Neutralizing Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation</summary>

- *Seungjae Shin, Kyungwoo Song, JoonHo Jang, Hyemi Kim, Weonyoung Joo, Il-Chul Moon*

- `2004.03133v2` - [abs](http://arxiv.org/abs/2004.03133v2) - [pdf](http://arxiv.org/pdf/2004.03133v2)

> Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear subspace for debiasing, we introduce a \textit{Latent Disentanglement} method with a siamese auto-encoder structure with an adapted gradient reversal layer. Our structure enables the separation of the semantic latent information and gender latent information of given word into the disjoint latent dimensions. Afterwards, we introduce a \textit{Counterfactual Generation} to convert the gender information of words, so the original and the modified embeddings can produce a gender-neutralized word embedding after geometric alignment regularization, without loss of semantic information. From the various quantitative and qualitative debiasing experiments, our method shows to be better than existing debiasing methods in debiasing word embeddings. In addition, Our method shows the ability to preserve semantic information during debiasing by minimizing the semantic information losses for extrinsic NLP downstream tasks.

</details>

<details>

<summary>2020-11-03 07:14:44 - Unsupervised Representation Learning by InvariancePropagation</summary>

- *Feng Wang, Huaping Liu, Di Guo, Fuchun Sun*

- `2010.11694v2` - [abs](http://arxiv.org/abs/2010.11694v2) - [pdf](http://arxiv.org/pdf/2010.11694v2)

> Unsupervised learning methods based on contrastive learning have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy fine-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classification on Places205 and Pascal VOC, and transfer learning on small scale datasets.

</details>

<details>

<summary>2020-11-03 10:39:10 - Towards Conceptual Modeling Semantics: Eventizing Tarski's Truth Schema</summary>

- *Sabah Al-Fedaghi*

- `2011.01608v1` - [abs](http://arxiv.org/abs/2011.01608v1) - [pdf](http://arxiv.org/pdf/2011.01608v1)

> Modeling languages in software engineering (e.g., UML) evolved from software systems modeling where denotational and operational kinds of semantics are the traditional subjects of research and practice. According to some authors, although a large portion of the static semantics (e.g., UML) seems to have reached a consensus, the dynamic semantics of activities, interactions, and state machines poses a major challenge. Central to semantics is the relationship between a sentence and the (actual) world. Carefully examining semantics-related issues in the modeling languages field to avoid problems that may affect practical applicability is important. One effort in this direction is OMG s release of a 2020 draft specification for Foundational UML (fUML), with the base semantics specifying executions that are executable in the same sense as a program in a traditional programming language. Additionally, efforts within academia have sought to develop an alternative approach to modeling languages using formal semantics (e.g., using Russell s theory of types and Tarski s declarative semantics). This paper aims at a similar exploratory venture of developing semantics, only for a much more modest diagrammatic modeling language, called the thinging machine model. The model promotes a deep understanding of the scrutinized modeling language and leads to considerably fruitful questions. Constructing the thinging machine model seems to facilitate progress in this direction, and the initial results in this paper indicate the viability of the approach.

</details>

<details>

<summary>2020-11-03 13:11:41 - An exploration of the encoding of grammatical gender in word embeddings</summary>

- *Hartger Veeman, Ali Basirat*

- `2008.01946v2` - [abs](http://arxiv.org/abs/2008.01946v2) - [pdf](http://arxiv.org/pdf/2008.01946v2)

> The vector representation of words, known as word embeddings, has opened a new research approach in linguistic studies. These representations can capture different types of information about words. The grammatical gender of nouns is a typical classification of nouns based on their formal and semantic properties. The study of grammatical gender based on word embeddings can give insight into discussions on how grammatical genders are determined. In this study, we compare different sets of word embeddings according to the accuracy of a neural classifier determining the grammatical gender of nouns. It is found that there is an overlap in how grammatical gender is encoded in Swedish, Danish, and Dutch embeddings. Our experimental results on the contextualized embeddings pointed out that adding more contextual information to embeddings is detrimental to the classifier's performance. We also observed that removing morpho-syntactic features such as articles from the training corpora of embeddings decreases the classification performance dramatically, indicating a large portion of the information is encoded in the relationship between nouns and articles.

</details>

<details>

<summary>2020-11-03 17:18:03 - Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory</summary>

- *Hannah Chen, Yangfeng Ji, David Evans*

- `2011.01856v1` - [abs](http://arxiv.org/abs/2011.01856v1) - [pdf](http://arxiv.org/pdf/2011.01856v1)

> Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an augmented dataset by directly inferring labels from the original sentence pairs using a transitivity property. We use structural balance theory to identify likely mislabelings in the graph, and flip their labels. We evaluate our methods on paraphrase models trained using these datasets starting from a pretrained BERT model, and find that the automatically-enhanced training sets result in more accurate models.

</details>

<details>

<summary>2020-11-03 19:47:06 - Mixing Consistent Deep Clustering</summary>

- *Daniel Lutscher, Ali el Hassouni, Maarten Stol, Mark Hoogendoorn*

- `2011.01977v1` - [abs](http://arxiv.org/abs/2011.01977v1) - [pdf](http://arxiv.org/pdf/2011.01977v1)

> Finding well-defined clusters in data represents a fundamental challenge for many data-driven applications, and largely depends on good data representation. Drawing on literature regarding representation learning, studies suggest that one key characteristic of good latent representations is the ability to produce semantically mixed outputs when decoding linear interpolations of two latent representations. We propose the Mixing Consistent Deep Clustering method which encourages interpolations to appear realistic while adding the constraint that interpolations of two data points must look like one of the two inputs. By applying this training method to various clustering (non-)specific autoencoder models we found that using the proposed training method systematically changed the structure of learned representations of a model and it improved clustering performance for the tested ACAI, IDEC, and VAE models on the MNIST, SVHN, and CIFAR-10 datasets. These outcomes have practical implications for numerous real-world clustering tasks, as it shows that the proposed method can be added to existing autoencoders to further improve clustering performance.

</details>

<details>

<summary>2020-11-03 22:55:40 - Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations</summary>

- *Ke Tran, Ming Tan*

- `2011.02050v1` - [abs](http://arxiv.org/abs/2011.02050v1) - [pdf](http://arxiv.org/pdf/2011.02050v1)

> Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (i.e., BART). Specifically, we first extract masked templates from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset for navigation domain.

</details>

<details>

<summary>2020-11-03 23:49:42 - Exhaustive Entity Recognition for Coptic: Challenges and Solutions</summary>

- *Amir Zeldes, Lance Martin, Sichang Tu*

- `2011.02068v1` - [abs](http://arxiv.org/abs/2011.02068v1) - [pdf](http://arxiv.org/pdf/2011.02068v1)

> Entity recognition provides semantic access to ancient materials in the Digital Humanities: itexposes people and places of interest in texts that cannot be read exhaustively, facilitates linkingresources and can provide a window into text contents, even for texts with no translations. Inthis paper we present entity recognition for Coptic, the language of Hellenistic era Egypt. Weevaluate NLP approaches to the task and lay out difficulties in applying them to a low-resource,morphologically complex language. We present solutions for named and non-named nested en-tity recognition and semi-automatic entity linking to Wikipedia, relying on robust dependencyparsing, feature-based CRF models, and hand-crafted knowledge base resources, enabling highaccuracy NER with orders of magnitude less data than those used for high resource languages.The results suggest avenues for research on other languages in similar settings.

</details>

<details>

<summary>2020-11-04 01:36:36 - Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering</summary>

- *Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, Qi Wu*

- `2006.09073v3` - [abs](http://arxiv.org/abs/2006.09073v3) - [pdf](http://arxiv.org/pdf/2006.09073v3)

> Fact-based Visual Question Answering (FVQA) requires external knowledge beyond visible content to answer questions about an image, which is challenging but indispensable to achieve general VQA. One limitation of existing FVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the final answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. In this paper, we depict an image by a multi-modal heterogeneous graph, which contains multiple layers of information corresponding to the visual, semantic and factual features. On top of the multi-layer graph representations, we propose a modality-aware heterogeneous graph convolutional network to capture evidence from different layers that is most relevant to the given question. Specifically, the intra-modal graph convolution selects evidence from each modality and cross-modal graph convolution aggregates relevant information across different modalities. By stacking this process multiple times, our model performs iterative reasoning and predicts the optimal answer by analyzing all question-oriented evidence. We achieve a new state-of-the-art performance on the FVQA task and demonstrate the effectiveness and interpretability of our model with extensive experiments.

</details>

<details>

<summary>2020-11-04 06:00:40 - Learning Sparse Prototypes for Text Generation</summary>

- *Junxian He, Taylor Berg-Kirkpatrick, Graham Neubig*

- `2006.16336v2` - [abs](http://arxiv.org/abs/2006.16336v2) - [pdf](http://arxiv.org/pdf/2006.16336v2)

> Prototype-driven text generation uses non-parametric models that first choose from a library of sentence "prototypes" and then modify the prototype to generate the output text. While effective, these methods are inefficient at test time as a result of needing to store and index the entire training corpus. Further, existing methods often require heuristics to identify which prototypes to reference at training time. In this paper, we propose a novel generative model that automatically learns a sparse prototype support set that, nonetheless, achieves strong language modeling performance. This is achieved by (1) imposing a sparsity-inducing prior on the prototype selection distribution, and (2) utilizing amortized variational inference to learn a prototype retrieval function. In experiments, our model outperforms previous prototype-driven language models while achieving up to a 1000x memory reduction, as well as a 1000x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics and syntax at different granularity as we vary the sparsity of prototype selection, and that certain sentence attributes can be controlled by specifying the prototype for generation.

</details>

<details>

<summary>2020-11-04 08:05:43 - Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine Learning to Infer the Emotions?</summary>

- *Laura Oberländer, Kevin Reich, Roman Klinger*

- `2011.01599v2` - [abs](http://arxiv.org/abs/2011.01599v2) - [pdf](http://arxiv.org/pdf/2011.01599v2)

> Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: "who is described to feel the emotion?" (experiencer), "what causes this emotion?" (stimulus), and at which entity is it directed?" (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.

</details>

<details>

<summary>2020-11-04 17:12:12 - Necessary and Sufficient Explanations in Abstract Argumentation</summary>

- *AnneMarie Borg, Floris Bex*

- `2011.02414v1` - [abs](http://arxiv.org/abs/2011.02414v1) - [pdf](http://arxiv.org/pdf/2011.02414v1)

> In this paper, we discuss necessary and sufficient explanations for formal argumentation - the question whether and why a certain argument can be accepted (or not) under various extension-based semantics. Given a framework with which explanations for argumentation-based conclusions can be derived, we study necessity and sufficiency: what (sets of) arguments are necessary or sufficient for the (non-)acceptance of an argument?

</details>

<details>

<summary>2020-11-04 20:33:09 - Schema-Guided Natural Language Generation</summary>

- *Yuheng Du, Shereen Oraby, Vittorio Perera, Minmin Shen, Anjali Narayan-Chen, Tagyoung Chung, Anu Venkatesh, Dilek Hakkani-Tur*

- `2005.05480v2` - [abs](http://arxiv.org/abs/2005.05480v2) - [pdf](http://arxiv.org/pdf/2005.05480v2)

> Neural network based approaches to data-to-text natural language generation (NLG) have gained popularity in recent years, with the goal of generating a natural language prompt that accurately realizes an input meaning representation. To facilitate the training of neural network models, researchers created large datasets of paired utterances and their meaning representations. However, the creation of such datasets is an arduous task and they mostly consist of simple meaning representations composed of slot and value tokens to be realized. These representations do not include any contextual information that an NLG system can use when trying to generalize, such as domain information and descriptions of slots and values. In this paper, we present the novel task of Schema-Guided Natural Language Generation (SG-NLG). Here, the goal is still to generate a natural language prompt, but in SG-NLG, the input MRs are paired with rich schemata providing contextual information. To generate a dataset for SG-NLG we re-purpose an existing dataset for another task: dialog state tracking, which includes a large and rich schema spanning multiple different attributes, including information about the domain, user intent, and slot descriptions. We train different state-of-the-art models for neural natural language generation on this dataset and show that in many cases, including rich schema information allows our models to produce higher quality outputs both in terms of semantics and diversity. We also conduct experiments comparing model performance on seen versus unseen domains, and present a human evaluation demonstrating high ratings for overall output quality.

</details>

<details>

<summary>2020-11-05 01:45:18 - Transforming Facial Weight of Real Images by Editing Latent Space of StyleGAN</summary>

- *V N S Rama Krishna Pinnimty, Matt Zhao, Palakorn Achananuparp, Ee-Peng Lim*

- `2011.02606v1` - [abs](http://arxiv.org/abs/2011.02606v1) - [pdf](http://arxiv.org/pdf/2011.02606v1)

> We present an invert-and-edit framework to automatically transform facial weight of an input face image to look thinner or heavier by leveraging semantic facial attributes encoded in the latent space of Generative Adversarial Networks (GANs). Using a pre-trained StyleGAN as the underlying generator, we first employ an optimization-based embedding method to invert the input image into the StyleGAN latent space. Then, we identify the facial-weight attribute direction in the latent space via supervised learning and edit the inverted latent code by moving it positively or negatively along the extracted feature axis. Our framework is empirically shown to produce high-quality and realistic facial-weight transformations without requiring training GANs with a large amount of labeled face images from scratch. Ultimately, our framework can be utilized as part of an intervention to motivate individuals to make healthier food choices by visualizing the future impacts of their behavior on appearance.

</details>

<details>

<summary>2020-11-05 03:07:20 - Understanding the drivers of sustainable land expansion using a patch-generating land use simulation (PLUS) model: A case study in Wuhan, China</summary>

- *Xun Liang, Qingfeng Guan, Keith C. Clarke, Shishi Liu, Bingyu Wang, Yao Yao*

- `2010.11541v3` - [abs](http://arxiv.org/abs/2010.11541v3) - [pdf](http://arxiv.org/pdf/2010.11541v3)

> Cellular Automata (CA) are widely used to model the dynamics within complex land use and land cover (LULC) systems. Past CA model research has focused on improving the technical modeling procedures, and only a few studies have sought to improve our understanding of the nonlinear relationships that underlie LULC change. Many CA models lack the ability to simulate the detailed patch evolution of multiple land use types. This study introduces a patch-generating land use simulation (PLUS) model that integrates a land expansion analysis strategy and a CA model based on multi-type random patch seeds. These were used to understand the drivers of land expansion and to investigate the landscape dynamics in Wuhan, China. The proposed model achieved a higher simulation accuracy and more similar landscape pattern metrics to the true landscape than other CA models tested. The land expansion analysis strategy also uncovered some underlying transition rules, such as that grassland is most likely to be found where it is not strongly impacted by human activities, and that deciduous forest areas tend to grow adjacent to arterial roads. We also projected the structure of land use under different optimizing scenarios for 2035 by combining the proposed model with multi-objective programming. The results indicate that the proposed model can help policymakers to manage future land use dynamics and so to realize more sustainable land use patterns for future development. Software for PLUS has been made available at https://github.com/HPSCIL/Patch-generating_Land_Use_Simulation_Model

</details>

<details>

<summary>2020-11-05 06:04:50 - Reinforcement Learning with Augmented Data</summary>

- *Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas*

- `2004.14990v5` - [abs](http://arxiv.org/abs/2004.14990v5) - [pdf](http://arxiv.org/pdf/2004.14990v5)

> Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.

</details>

<details>

<summary>2020-11-05 08:35:41 - Secure Information Flow Connections</summary>

- *Chandrika Bhardwaj, Sanjiva Prasad*

- `2011.03319v1` - [abs](http://arxiv.org/abs/2011.03319v1) - [pdf](http://arxiv.org/pdf/2011.03319v1)

> Denning's lattice model provided secure information flow analyses with an intuitive mathematical foundation: the lattice ordering determines permitted flows. We examine how this framework may be extended to support the flow of information between autonomous organisations, each employing possibly quite different security lattices and information flow policies. We propose a connection framework that permits different organisations to exchange information while maintaining both security of information flow as well as their autonomy in formulating and maintaining security policies. Our prescriptive framework is based on the rigorous mathematical framework of Lagois connections proposed by Melton, together with a simple operational model for transferring object data between domains. The merit of this formulation is that it is simple, minimal, adaptable and intuitive. We show that our framework is semantically sound, by proving that the connections proposed preserve standard correctness notions such as non-interference. We then illustrate how Lagois theory also provides a robust framework and methodology for negotiating and maintaining secure agreements on information flow between autonomous organisations, even when either or both organisations change their security lattices. Composition and decomposition properties indicate support for a modular approach to secure flow frameworks in complex organisations. We next show that this framework extends naturally and conservatively to the Decentralised Labels Model of Myers et al. - a Lagois connection between the hierarchies of principals in two organisations naturally induces a Lagois connection between the corresponding security label lattices, thus extending the security guarantees ensured by the decentralised model to encompass bidirectional inter-organisational flows.

</details>

<details>

<summary>2020-11-05 10:55:19 - SHACL Satisfiability and Containment (Extended Paper)</summary>

- *Paolo Pareti, George Konstantinidis, Fabio Mogavero, Timothy J. Norman*

- `2009.09806v2` - [abs](http://arxiv.org/abs/2009.09806v2) - [pdf](http://arxiv.org/pdf/2009.09806v2)

> The Shapes Constraint Language (SHACL) is a recent W3C recommendation language for validating RDF data. Specifically, SHACL documents are collections of constraints that enforce particular shapes on an RDF graph. Previous work on the topic has provided theoretical and practical results for the validation problem, but did not consider the standard decision problems of satisfiability and containment, which are crucial for verifying the feasibility of the constraints and important for design and optimization purposes. In this paper, we undertake a thorough study of different features of non-recursive SHACL by providing a translation to a new first-order language, called SCL, that precisely captures the semantics of SHACL w.r.t. satisfiability and containment. We study the interaction of SHACL features in this logic and provide the detailed map of decidability and complexity results of the aforementioned decision problems for different SHACL sublanguages. Notably, we prove that both problems are undecidable for the full language, but we present decidable combinations of interesting features.

</details>

<details>

<summary>2020-11-05 12:16:30 - Fast Object Detection with Latticed Multi-Scale Feature Fusion</summary>

- *Yue Shi, Bo Jiang, Zhengping Che, Jian Tang*

- `2011.02780v1` - [abs](http://arxiv.org/abs/2011.02780v1) - [pdf](http://arxiv.org/pdf/2011.02780v1)

> Scale variance is one of the crucial challenges in multi-scale object detection. Early approaches address this problem by exploiting the image and feature pyramid, which raises suboptimal results with computation burden and constrains from inherent network structures. Pioneering works also propose multi-scale (i.e., multi-level and multi-branch) feature fusions to remedy the issue and have achieved encouraging progress. However, existing fusions still have certain limitations such as feature scale inconsistency, ignorance of level-wise semantic transformation, and coarse granularity. In this work, we present a novel module, the Fluff block, to alleviate drawbacks of current multi-scale fusion methods and facilitate multi-scale object detection. Specifically, Fluff leverages both multi-level and multi-branch schemes with dilated convolutions to have rapid, effective and finer-grained feature fusions. Furthermore, we integrate Fluff to SSD as FluffNet, a powerful real-time single-stage detector for multi-scale object detection. Empirical results on MS COCO and PASCAL VOC have demonstrated that FluffNet obtains remarkable efficiency with state-of-the-art accuracy. Additionally, we indicate the great generality of the Fluff block by showing how to embed it to other widely-used detectors as well.

</details>

<details>

<summary>2020-11-05 12:48:25 - A Multi-Stage Adaptive Sampling Scheme for Passivity Characterization of Large-Scale Macromodels</summary>

- *Marco De Stefano, Stefano Grivet-Talocia, Torben Wendt, Cheng Yang, Christian Schuster*

- `2011.02789v1` - [abs](http://arxiv.org/abs/2011.02789v1) - [pdf](http://arxiv.org/pdf/2011.02789v1)

> This paper proposes a hierarchical adaptive sampling scheme for passivity characterization of large-scale linear lumped macromodels. Here, large-scale is intended both in terms of dynamic order and especially number of input/output ports. Standard passivity characterization approaches based on spectral properties of associated Hamiltonian matrices are either inefficient or non-applicable for large-scale models, due to an excessive computational cost. This paper builds on existing adaptive sampling methods and proposes a hybrid multi-stage algorithm that is able to detect the passivity violations with limited computing resources. Results from extensive testing demonstrate a major reduction in computational requirements with respect to competing approaches.

</details>

<details>

<summary>2020-11-05 14:57:41 - Semantic and Relational Spaces in Science of Science: Deep Learning Models for Article Vectorisation</summary>

- *Diego Kozlowski, Jennifer Dusdal, Jun Pang, Andreas Zilian*

- `2011.02887v1` - [abs](http://arxiv.org/abs/2011.02887v1) - [pdf](http://arxiv.org/pdf/2011.02887v1)

> Over the last century, we observe a steady and exponentially growth of scientific publications globally. The overwhelming amount of available literature makes a holistic analysis of the research within a field and between fields based on manual inspection impossible. Automatic techniques to support the process of literature review are required to find the epistemic and social patterns that are embedded in scientific publications. In computer sciences, new tools have been developed to deal with large volumes of data. In particular, deep learning techniques open the possibility of automated end-to-end models to project observations to a new, low-dimensional space where the most relevant information of each observation is highlighted. Using deep learning to build new representations of scientific publications is a growing but still emerging field of research. The aim of this paper is to discuss the potential and limits of deep learning for gathering insights about scientific research articles. We focus on document-level embeddings based on the semantic and relational aspects of articles, using Natural Language Processing (NLP) and Graph Neural Networks (GNNs). We explore the different outcomes generated by those techniques. Our results show that using NLP we can encode a semantic space of articles, while with GNN we are able to build a relational space where the social practices of a research community are also encoded.

</details>

<details>

<summary>2020-11-05 16:08:50 - Learning Efficient Task-Specific Meta-Embeddings with Word Prisms</summary>

- *Jingyi He, KC Tsiolis, Kian Kenyon-Dean, Jackie Chi Kit Cheung*

- `2011.02944v1` - [abs](http://arxiv.org/abs/2011.02944v1) - [pdf](http://arxiv.org/pdf/2011.02944v1)

> Word embeddings are trained to predict word cooccurrence statistics, which leads them to possess different lexical properties (syntactic, semantic, etc.) depending on the notion of context defined at training time. These properties manifest when querying the embedding space for the most similar vectors, and when used at the input layer of deep neural networks trained to solve downstream NLP problems. Meta-embeddings combine multiple sets of differently trained word embeddings, and have been shown to successfully improve intrinsic and extrinsic performance over equivalent models which use just one set of source embeddings. We introduce word prisms: a simple and efficient meta-embedding method that learns to combine source embeddings according to the task at hand. Word prisms learn orthogonal transformations to linearly combine the input source embeddings, which allows them to be very efficient at inference time. We evaluate word prisms in comparison to other meta-embedding methods on six extrinsic evaluations and observe that word prisms offer improvements in performance on all tasks.

</details>

<details>

<summary>2020-11-05 16:36:58 - Street to Cloud: Improving Flood Maps With Crowdsourcing and Semantic Segmentation</summary>

- *Veda Sunkara, Matthew Purri, Bertrand Le Saux, Jennifer Adams*

- `2011.08010v1` - [abs](http://arxiv.org/abs/2011.08010v1) - [pdf](http://arxiv.org/pdf/2011.08010v1)

> To address the mounting destruction caused by floods in climate-vulnerable regions, we propose Street to Cloud, a machine learning pipeline for incorporating crowdsourced ground truth data into the segmentation of satellite imagery of floods. We propose this approach as a solution to the labor-intensive task of generating high-quality, hand-labeled training data, and demonstrate successes and failures of different plausible crowdsourcing approaches in our model. Street to Cloud leverages community reporting and machine learning to generate novel, near-real time insights into the extent of floods to be used for emergency response.

</details>

<details>

<summary>2020-11-05 21:59:37 - Evaluating the Performance of Twitter-based Exploit Detectors</summary>

- *Daniel Alves de Sousa, Elaine Ribeiro de Faria, Rodrigo Sanches Miani*

- `2011.03113v1` - [abs](http://arxiv.org/abs/2011.03113v1) - [pdf](http://arxiv.org/pdf/2011.03113v1)

> Patch prioritization is a crucial aspect of information systems security, and knowledge of which vulnerabilities were exploited in the wild is a powerful tool to help systems administrators accomplish this task. The analysis of social media for this specific application can enhance the results and bring more agility by collecting data from online discussions and applying machine learning techniques to detect real-world exploits. In this paper, we use a technique that combines Twitter data with public database information to classify vulnerabilities as exploited or not-exploited. We analyze the behavior of different classifying algorithms, investigate the influence of different antivirus data as ground truth, and experiment with various time window sizes. Our findings suggest that using a Light Gradient Boosting Machine (LightGBM) can benefit the results, and for most cases, the statistics related to a tweet and the users who tweeted are more meaningful than the text tweeted. We also demonstrate the importance of using ground-truth data from security companies not mentioned in previous works.

</details>

<details>

<summary>2020-11-06 02:08:32 - Leveraging an Efficient and Semantic Location Embedding to Seek New Ports of Bike Share Services</summary>

- *Yuan Wang, Chenwei Wang, Yinan Ling, Keita Yokoyama, Hsin-Tai Wu, Yi Fang*

- `2011.03158v1` - [abs](http://arxiv.org/abs/2011.03158v1) - [pdf](http://arxiv.org/pdf/2011.03158v1)

> For short distance traveling in crowded urban areas, bike share services are becoming popular owing to the flexibility and convenience. To expand the service coverage, one of the key tasks is to seek new service ports, which requires to well understand the underlying features of the existing service ports. In this paper, we propose a new model, named for Efficient and Semantic Location Embedding (ESLE), which carries both geospatial and semantic information of the geo-locations. To generate ESLE, we first train a multi-label model with a deep Convolutional Neural Network (CNN) by feeding the static map-tile images and then extract location embedding vectors from the model. Compared to most recent relevant literature, ESLE is not only much cheaper in computation, but also easier to interpret via a systematic semantic analysis. Finally, we apply ESLE to seek new service ports for NTT DOCOMO's bike share services operated in Japan. The initial results demonstrate the effectiveness of ESLE, and provide a few insights that might be difficult to discover by using the conventional approaches.

</details>

<details>

<summary>2020-11-06 03:52:54 - Real-Time Uncertainty Estimation in Computer Vision via Uncertainty-Aware Distribution Distillation</summary>

- *Yichen Shen, Zhilu Zhang, Mert R. Sabuncu, Lin Sun*

- `2007.15857v2` - [abs](http://arxiv.org/abs/2007.15857v2) - [pdf](http://arxiv.org/pdf/2007.15857v2)

> Calibrated estimates of uncertainty are critical for many real-world computer vision applications of deep learning. While there are several widely-used uncertainty estimation methods, dropout inference stands out for its simplicity and efficacy. This technique, however, requires multiple forward passes through the network during inference and therefore can be too resource-intensive to be deployed in real-time applications. We propose a simple, easy-to-optimize distillation method for learning the conditional predictive distribution of a pre-trained dropout model for fast, sample-free uncertainty estimation in computer vision tasks. We empirically test the effectiveness of the proposed method on both semantic segmentation and depth estimation tasks and demonstrate our method can significantly reduce the inference time, enabling real-time uncertainty quantification, while achieving improved quality of both the uncertainty estimates and predictive performance over the regular dropout model.

</details>

<details>

<summary>2020-11-06 04:13:36 - I Know What You Asked: Graph Path Learning using AMR for Commonsense Reasoning</summary>

- *Jungwoo Lim, Dongsuk Oh, Yoonna Jang, Kisu Yang, Heuiseok Lim*

- `2011.00766v2` - [abs](http://arxiv.org/abs/2011.00766v2) - [pdf](http://arxiv.org/pdf/2011.00766v2)

> CommonsenseQA is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an AMR-ConceptNet-Pruned (ACP) graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation (AMR) graph generated from input questions and an external commonsense knowledge graph, ConceptNet (CN). Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the CommonsenseQA task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines.

</details>

<details>

<summary>2020-11-06 10:02:12 - OP-IMS @ DIACR-Ita: Back to the Roots: SGNS+OP+CD still rocks Semantic Change Detection</summary>

- *Jens Kaiser, Dominik Schlechtweg, Sabine Schulte im Walde*

- `2011.03258v1` - [abs](http://arxiv.org/abs/2011.03258v1) - [pdf](http://arxiv.org/pdf/2011.03258v1)

> We present the results of our participation in the DIACR-Ita shared task on lexical semantic change detection for Italian. We exploit one of the earliest and most influential semantic change detection models based on Skip-Gram with Negative Sampling, Orthogonal Procrustes alignment and Cosine Distance and obtain the winning submission of the shared task with near to perfect accuracy .94. Our results once more indicate that, within the present task setup in lexical semantic change detection, the traditional type-based approaches yield excellent performance.

</details>

<details>

<summary>2020-11-06 17:08:33 - QMUL-SDS @ DIACR-Ita: Evaluating Unsupervised Diachronic Lexical Semantics Classification in Italian</summary>

- *Rabab Alkhalifa, Adam Tsakalidis, Arkaitz Zubiaga, Maria Liakata*

- `2011.02935v2` - [abs](http://arxiv.org/abs/2011.02935v2) - [pdf](http://arxiv.org/pdf/2011.02935v2)

> In this paper, we present the results and main findings of our system for the DIACR-ITA 2020 Task. Our system focuses on using variations of training sets and different semantic detection methods. The task involves training, aligning and predicting a word's vector change from two diachronic Italian corpora. We demonstrate that using Temporal Word Embeddings with a Compass C-BOW model is more effective compared to different approaches including Logistic Regression and a Feed Forward Neural Network using accuracy. Our model ranked 3rd with an accuracy of 83.3%.

</details>

<details>

<summary>2020-11-06 17:45:15 - Explainable Artificial Intelligence Recommendation System by Leveraging the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype Development</summary>

- *Nariman Ammar, Arash Shaban-Nejad*

- `2011.08090v1` - [abs](http://arxiv.org/abs/2011.08090v1) - [pdf](http://arxiv.org/pdf/2011.08090v1)

> The study of adverse childhood experiences and their consequences has emerged over the past 20 years. In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve surveillance of adverse childhood experiences. We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners ability to provide explanations for the decisions they make.

</details>

<details>

<summary>2020-11-06 18:26:49 - Learning Graph Structure With A Finite-State Automaton Layer</summary>

- *Daniel D. Johnson, Hugo Larochelle, Daniel Tarlow*

- `2007.04929v2` - [abs](http://arxiv.org/abs/2007.04929v2) - [pdf](http://arxiv.org/pdf/2007.04929v2)

> Graph-based neural network models are producing strong results in a number of domains, in part because graphs provide flexibility to encode domain knowledge in the form of relational structure (edges) between nodes in the graph. In practice, edges are used both to represent intrinsic structure (e.g., abstract syntax trees of programs) and more abstract relations that aid reasoning for a downstream task (e.g., results of relevant program analyses). In this work, we study the problem of learning to derive abstract relations from the intrinsic graph structure. Motivated by their power in program analyses, we consider relations defined by paths on the base graph accepted by a finite-state automaton. We show how to learn these relations end-to-end by relaxing the problem into learning finite-state automata policies on a graph-based POMDP and then training these policies using implicit differentiation. The result is a differentiable Graph Finite-State Automaton (GFSA) layer that adds a new edge type (expressed as a weighted adjacency matrix) to a base graph. We demonstrate that this layer can find shortcuts in grid-world graphs and reproduce simple static analyses on Python programs. Additionally, we combine the GFSA layer with a larger graph-based model trained end-to-end on the variable misuse program understanding task, and find that using the GFSA layer leads to better performance than using hand-engineered semantic edges or other baseline methods for adding learned edge types.

</details>

<details>

<summary>2020-11-06 20:20:45 - A Weakly Supervised Convolutional Network for Change Segmentation and Classification</summary>

- *Philipp Andermatt, Radu Timofte*

- `2011.03577v1` - [abs](http://arxiv.org/abs/2011.03577v1) - [pdf](http://arxiv.org/pdf/2011.03577v1)

> Fully supervised change detection methods require difficult to procure pixel-level labels, while weakly supervised approaches can be trained with image-level labels. However, most of these approaches require a combination of changed and unchanged image pairs for training. Thus, these methods can not directly be used for datasets where only changed image pairs are available. We present W-CDNet, a novel weakly supervised change detection network that can be trained with image-level semantic labels. Additionally, W-CDNet can be trained with two different types of datasets, either containing changed image pairs only or a mixture of changed and unchanged image pairs. Since we use image-level semantic labels for training, we simultaneously create a change mask and label the changed object for single-label images. W-CDNet employs a W-shaped siamese U-net to extract feature maps from an image pair which then get compared in order to create a raw change mask. The core part of our model, the Change Segmentation and Classification (CSC) module, learns an accurate change mask at a hidden layer by using a custom Remapping Block and then segmenting the current input image with the change mask. The segmented image is used to predict the image-level semantic label. The correct label can only be predicted if the change mask actually marks relevant change. This forces the model to learn an accurate change mask. We demonstrate the segmentation and classification performance of our approach and achieve top results on AICD and HRSCD, two public aerial imaging change detection datasets as well as on a Food Waste change detection dataset. Our code is available at https://github.com/PhiAbs/W-CDNet .

</details>

<details>

<summary>2020-11-06 20:56:21 - Subgraph Neural Networks</summary>

- *Emily Alsentzer, Samuel G. Finlayson, Michelle M. Li, Marinka Zitnik*

- `2006.10538v3` - [abs](http://arxiv.org/abs/2006.10538v3) - [pdf](http://arxiv.org/pdf/2006.10538v3)

> Deep learning methods for graphs achieve remarkable performance on many node-level and graph-level prediction tasks. However, despite the proliferation of the methods and their success, prevailing Graph Neural Networks (GNNs) neglect subgraphs, rendering subgraph prediction tasks challenging to tackle in many impactful applications. Further, subgraph prediction tasks present several unique challenges: subgraphs can have non-trivial internal topology, but also carry a notion of position and external connectivity information relative to the underlying graph in which they exist. Here, we introduce SubGNN, a subgraph neural network to learn disentangled subgraph representations. We propose a novel subgraph routing mechanism that propagates neural messages between the subgraph's components and randomly sampled anchor patches from the underlying graph, yielding highly accurate subgraph representations. SubGNN specifies three channels, each designed to capture a distinct aspect of subgraph topology, and we provide empirical evidence that the channels encode their intended properties. We design a series of new synthetic and real-world subgraph datasets. Empirical results for subgraph classification on eight datasets show that SubGNN achieves considerable performance gains, outperforming strong baseline methods, including node-level and graph-level GNNs, by 19.8% over the strongest baseline. SubGNN performs exceptionally well on challenging biomedical datasets where subgraphs have complex topology and even comprise multiple disconnected components.

</details>

<details>

<summary>2020-11-07 11:16:07 - Constraint Monotonicity, Epistemic Splitting and Foundedness Could in General Be Too Strong in Answer Set Programming</summary>

- *Yi-Dong Shen, Thomas Eiter*

- `2010.00191v2` - [abs](http://arxiv.org/abs/2010.00191v2) - [pdf](http://arxiv.org/pdf/2010.00191v2)

> Recently, the notions of subjective constraint monotonicity, epistemic splitting, and foundedness have been introduced for epistemic logic programs, with the aim to use them as main criteria respectively intuitions to compare different answer set semantics proposed in the literature on how they comply with these intuitions. In this note, we consider these three notions and demonstrate on some examples that they may be too strong in general and may exclude some desired answer sets respectively world views. In conclusion, these properties should not be regarded as mandatory properties that every answer set semantics must satisfy in general.

</details>

<details>

<summary>2020-11-07 11:27:18 - NLP-CIC @ DIACR-Ita: POS and Neighbor Based Distributional Models for Lexical Semantic Change in Diachronic Italian Corpora</summary>

- *Jason Angel, Carlos A. Rodriguez-Diaz, Alexander Gelbukh, Sergio Jimenez*

- `2011.03755v1` - [abs](http://arxiv.org/abs/2011.03755v1) - [pdf](http://arxiv.org/pdf/2011.03755v1)

> We present our systems and findings on unsupervised lexical semantic change for the Italian language in the DIACR-Ita shared-task at EVALITA 2020. The task is to determine whether a target word has evolved its meaning with time, only relying on raw-text from two time-specific datasets. We propose two models representing the target words across the periods to predict the changing words using threshold and voting schemes. Our first model solely relies on part-of-speech usage and an ensemble of distance measures. The second model uses word embedding representation to extract the neighbor's relative distances across spaces and propose "the average of absolute differences" to estimate lexical semantic change. Our models achieved competent results, ranking third in the DIACR-Ita competition. Furthermore, we experiment with the k_neighbor parameter of our second model to compare the impact of using "the average of absolute differences" versus the cosine distance used in Hamilton et al. (2016).

</details>

<details>

<summary>2020-11-07 11:39:56 - HOI Analysis: Integrating and Decomposing Human-Object Interaction</summary>

- *Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Cewu Lu*

- `2010.16219v2` - [abs](http://arxiv.org/abs/2010.16219v2) - [pdf](http://arxiv.org/pdf/2010.16219v2)

> Human-Object Interaction (HOI) consists of human, object and implicit interaction/verb. Different from previous methods that directly map pixels to HOI semantics, we propose a novel perspective for HOI learning in an analytical manner. In analogy to Harmonic Analysis, whose goal is to study how to represent the signals with the superposition of basic waves, we propose the HOI Analysis. We argue that coherent HOI can be decomposed into isolated human and object. Meanwhile, isolated human and object can also be integrated into coherent HOI again. Moreover, transformations between human-object pairs with the same HOI can also be easier approached with integration and decomposition. As a result, the implicit verb will be represented in the transformation function space. In light of this, we propose an Integration-Decomposition Network (IDN) to implement the above transformations and achieve state-of-the-art performance on widely-used HOI detection benchmarks. Code is available at https://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network).

</details>

<details>

<summary>2020-11-07 13:28:12 - CAggNet: Crossing Aggregation Network for Medical Image Segmentation</summary>

- *Xu Cao, Yanghao Lin*

- `2004.08237v2` - [abs](http://arxiv.org/abs/2004.08237v2) - [pdf](http://arxiv.org/pdf/2004.08237v2)

> In this paper, we present Crossing Aggregation Network (CAggNet), a novel densely connected semantic segmentation approach for medical image analysis. The crossing aggregation network improves the idea from deep layer aggregation and makes significant innovations in semantic and spatial information fusion. In CAggNet, the simple skip connection structure of general U-Net is replaced by aggregations of multi-level down-sampling and up-sampling layers, which is a new form of nested skip connection. This aggregation architecture enables the network to fuse both coarse and fine features interactively in semantic segmentation. It also introduces weighted aggregation module to up-sample multi-scale output at the end of the network. We have evaluated and compared our CAggNet with several advanced U-Net based methods in two public medical image datasets, including the 2018 Data Science Bowl nuclei detection dataset and the 2015 MICCAI gland segmentation competition dataset. Experimental results indicate that CAggNet improves medical object recognition and achieves a more accurate and efficient segmentation compared to existing improved U-Net and UNet++ structure.

</details>

<details>

<summary>2020-11-07 19:19:10 - Unsupervised Monocular Depth Learning in Dynamic Scenes</summary>

- *Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, Anelia Angelova*

- `2010.16404v2` - [abs](http://arxiv.org/abs/2010.16404v2) - [pdf](http://arxiv.org/pdf/2010.16404v2)

> We present a method for jointly training the estimation of depth, ego-motion, and a dense 3D translation field of objects relative to the scene, with monocular photometric consistency being the sole source of supervision. We show that this apparently heavily underdetermined problem can be regularized by imposing the following prior knowledge about 3D translation fields: they are sparse, since most of the scene is static, and they tend to be constant for rigid moving objects. We show that this regularization alone is sufficient to train monocular depth prediction models that exceed the accuracy achieved in prior work for dynamic scenes, including methods that require semantic input. Code is at https://github.com/google-research/google-research/tree/master/depth_and_motion_learning .

</details>

<details>

<summary>2020-11-07 22:41:18 - Online matrix factorization for Markovian data and applications to Network Dictionary Learning</summary>

- *Hanbaek Lyu, Deanna Needell, Laura Balzano*

- `1911.01931v6` - [abs](http://arxiv.org/abs/1911.01931v6) - [pdf](http://arxiv.org/pdf/1911.01931v6)

> Online Matrix Factorization (OMF) is a fundamental tool for dictionary learning problems, giving an approximate representation of complex data sets in terms of a reduced number of extracted features. Convergence guarantees for most of the OMF algorithms in the literature assume independence between data matrices, and the case of dependent data streams remains largely unexplored. In this paper, we show that a non-convex generalization of the well-known OMF algorithm for i.i.d. stream of data in \citep{mairal2010online} converges almost surely to the set of critical points of the expected loss function, even when the data matrices are functions of some underlying Markov chain satisfying a mild mixing condition. This allows one to extract features more efficiently from dependent data streams, as there is no need to subsample the data sequence to approximately satisfy the independence assumption. As the main application, by combining online non-negative matrix factorization and a recent MCMC algorithm for sampling motifs from networks, we propose a novel framework of Network Dictionary Learning, which extracts ``network dictionary patches' from a given network in an online manner that encodes main features of the network. We demonstrate this technique and its application to network denoising problems on real-world network data.

</details>

<details>

<summary>2020-11-08 21:29:10 - A Semantic Framework for Enabling Radio Spectrum Policy Management and Evaluation</summary>

- *H. Santos, A. Mulvehill, J. S. Erickson, J. P. McCusker, M. Gordon, O. Xie, S. Stouffer, G. Capraro, A. Pidwerbetsky, J. Burgess, A. Berlinsky, K. Turck, J. Ashdown, D. L. McGuinness*

- `2011.04085v1` - [abs](http://arxiv.org/abs/2011.04085v1) - [pdf](http://arxiv.org/pdf/2011.04085v1)

> Because radio spectrum is a finite resource, its usage and sharing is regulated by government agencies. These agencies define policies to manage spectrum allocation and assignment across multiple organizations, systems, and devices. With more portions of the radio spectrum being licensed for commercial use, the importance of providing an increased level of automation when evaluating such policies becomes crucial for the efficiency and efficacy of spectrum management. We introduce our Dynamic Spectrum Access Policy Framework for supporting the United States government's mission to enable both federal and non-federal entities to compatibly utilize available spectrum. The DSA Policy Framework acts as a machine-readable policy repository providing policy management features and spectrum access request evaluation. The framework utilizes a novel policy representation using OWL and PROV-O along with a domain-specific reasoning implementation that mixes GeoSPARQL, OWL reasoning, and knowledge graph traversal to evaluate incoming spectrum access requests and explain how applicable policies were used. The framework is currently being used to support live, over-the-air field exercises involving a diverse set of federal and commercial radios, as a component of a prototype spectrum management system.

</details>

<details>

<summary>2020-11-08 21:34:05 - Personalization of learning using adaptive technologies and augmented reality</summary>

- *Maiia Marienko, Yulia Nosenko, Mariya Shyshkina*

- `2011.05802v1` - [abs](http://arxiv.org/abs/2011.05802v1) - [pdf](http://arxiv.org/pdf/2011.05802v1)

> The research is aimed at developing the recommendations for educators on using adaptive technologies and augmented reality in personalized learning implementation. The latest educational technologies related to learning personalization and the adaptation of its content to the individual needs of students and group work are considered. The current state of research is described, the trends of development are determined. Due to a detailed analysis of scientific works, a retrospective of the development of adaptive and, in particular, cloud-oriented systems is shown. The preconditions of their appearance and development, the main scientific ideas that contributed to this are analyzed. The analysis showed that the scientists point to four possible types of semantic interaction of augmented reality and adaptive technologies. The adaptive cloud-based educational systems design is considered as the promising trend of research. It was determined that adaptability can be manifested in one or a combination of several aspects: content, evaluation and consistency. The cloud technology is taken as a platform for integrating adaptive learning with augmented reality as the effective modern tools to personalize learning. The prospects of the adaptive cloud-based systems design in the context of teachers training are evaluated. The essence and place of assistive technologies in adaptive learning systems design are defined. It is shown that augmented reality can be successfully applied in inclusive education. The ways of combining adaptive systems and augmented reality tools to support the process of teachers training are considered. The recommendations on the use of adaptive cloud-based systems in teacher education are given.

</details>

<details>

<summary>2020-11-09 01:37:26 - SSP: Single Shot Future Trajectory Prediction</summary>

- *Isht Dwivedi, Srikanth Malla, Behzad Dariush, Chiho Choi*

- `2004.05846v2` - [abs](http://arxiv.org/abs/2004.05846v2) - [pdf](http://arxiv.org/pdf/2004.05846v2)

> We propose a robust solution to future trajectory forecast, which can be practically applicable to autonomous agents in highly crowded environments. For this, three aspects are particularly addressed in this paper. First, we use composite fields to predict future locations of all road agents in a single-shot, which results in a constant time complexity, regardless of the number of agents in the scene. Second, interactions between agents are modeled as a non-local response, enabling spatial relationships between different locations to be captured temporally as well (i.e., in spatio-temporal interactions). Third, the semantic context of the scene are modeled and take into account the environmental constraints that potentially influence the future motion. To this end, we validate the robustness of the proposed approach using the ETH, UCY, and SDD datasets and highlight its practical functionality compared to the current state-of-the-art methods.

</details>

<details>

<summary>2020-11-09 01:39:52 - CxGBERT: BERT meets Construction Grammar</summary>

- *Harish Tayyar Madabushi, Laurence Romain, Dagmar Divjak, Petar Milin*

- `2011.04134v1` - [abs](http://arxiv.org/abs/2011.04134v1) - [pdf](http://arxiv.org/pdf/2011.04134v1)

> While lexico-semantic elements no doubt capture a large amount of linguistic information, it has been argued that they do not capture all information contained in text. This assumption is central to constructionist approaches to language which argue that language consists of constructions, learned pairings of a form and a function or meaning that are either frequent or have a meaning that cannot be predicted from its component parts. BERT's training objectives give it access to a tremendous amount of lexico-semantic information, and while BERTology has shown that BERT captures certain important linguistic dimensions, there have been no studies exploring the extent to which BERT might have access to constructional information. In this work we design several probes and conduct extensive experiments to answer this question. Our results allow us to conclude that BERT does indeed have access to a significant amount of information, much of which linguists typically call constructional information. The impact of this observation is potentially far-reaching as it provides insights into what deep learning methods learn from text, while also showing that information contained in constructions is redundantly encoded in lexico-semantics.

</details>

<details>

<summary>2020-11-09 04:38:02 - Text Classification through Glyph-aware Disentangled Character Embedding and Semantic Sub-character Augmentation</summary>

- *Takumi Aoki, Shunsuke Kitada, Hitoshi Iyatomi*

- `2011.04184v1` - [abs](http://arxiv.org/abs/2011.04184v1) - [pdf](http://arxiv.org/pdf/2011.04184v1)

> We propose a new character-based text classification framework for non-alphabetic languages, such as Chinese and Japanese. Our framework consists of a variational character encoder (VCE) and character-level text classifier. The VCE is composed of a $\beta$-variational auto-encoder ($\beta$-VAE) that learns the proposed glyph-aware disentangled character embedding (GDCE). Since our GDCE provides zero-mean unit-variance character embeddings that are dimensionally independent, it is applicable for our interpretable data augmentation, namely, semantic sub-character augmentation (SSA). In this paper, we evaluated our framework using Japanese text classification tasks at the document- and sentence-level. We confirmed that our GDCE and SSA not only provided embedding interpretability but also improved the classification performance. Our proposal achieved a competitive result to the state-of-the-art model while also providing model interpretability. Our code is available on https://github.com/IyatomiLab/GDCE-SSA

</details>

<details>

<summary>2020-11-09 10:24:12 - Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT</summary>

- *Rik van Noord, Antonio Toral, Johan Bos*

- `2011.04308v1` - [abs](http://arxiv.org/abs/2011.04308v1) - [pdf](http://arxiv.org/pdf/2011.04308v1)

> We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.

</details>

<details>

<summary>2020-11-09 15:28:53 - Synonym Knowledge Enhanced Reader for Chinese Idiom Reading Comprehension</summary>

- *Siyu Long, Ran Wang, Kun Tao, Jiali Zeng, Xin-Yu Dai*

- `2011.04499v1` - [abs](http://arxiv.org/abs/2011.04499v1) - [pdf](http://arxiv.org/pdf/2011.04499v1)

> Machine reading comprehension (MRC) is the task that asks a machine to answer questions based on a given context. For Chinese MRC, due to the non-literal and non-compositional semantic characteristics, Chinese idioms pose unique challenges for machines to understand. Previous studies tend to treat idioms separately without fully exploiting the relationship among them. In this paper, we first define the concept of literal meaning coverage to measure the consistency between semantics and literal meanings for Chinese idioms. With the definition, we prove that the literal meanings of many idioms are far from their semantics, and we also verify that the synonymic relationship can mitigate this inconsistency, which would be beneficial for idiom comprehension. Furthermore, to fully utilize the synonymic relationship, we propose the synonym knowledge enhanced reader. Specifically, for each idiom, we first construct a synonym graph according to the annotations from a high-quality synonym dictionary or the cosine similarity between the pre-trained idiom embeddings and then incorporate the graph attention network and gate mechanism to encode the graph. Experimental results on ChID, a large-scale Chinese idiom reading comprehension dataset, show that our model achieves state-of-the-art performance.

</details>

<details>

<summary>2020-11-09 15:37:43 - VisBERT: Hidden-State Visualizations for Transformers</summary>

- *Betty van Aken, Benjamin Winter, Alexander Löser, Felix A. Gers*

- `2011.04507v1` - [abs](http://arxiv.org/abs/2011.04507v1) - [pdf](http://arxiv.org/pdf/2011.04507v1)

> Explainability and interpretability are two important concepts, the absence of which can and should impede the application of well-performing neural networks to real-world problems. At the same time, they are difficult to incorporate into the large, black-box models that achieve state-of-the-art results in a multitude of NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one such black-box model. It has become a staple architecture to solve many different NLP tasks and has inspired a number of related Transformer models. Understanding how these models draw conclusions is crucial for both their improvement and application. We contribute to this challenge by presenting VisBERT, a tool for visualizing the contextual token representations within BERT for the task of (multi-hop) Question Answering. Instead of analyzing attention weights, we focus on the hidden states resulting from each encoder block within the BERT model. This way we can observe how the semantic representations are transformed throughout the layers of the model. VisBERT enables users to get insights about the model's internal state and to explore its inference steps or potential shortcomings. The tool allows us to identify distinct phases in BERT's transformations that are similar to a traditional NLP pipeline and offer insights during failed predictions.

</details>

<details>

<summary>2020-11-09 17:56:03 - Learning Generalizable Physiological Representations from Large-scale Wearable Data</summary>

- *Dimitris Spathis, Ignacio Perez-Pozuelo, Soren Brage, Nicholas J. Wareham, Cecilia Mascolo*

- `2011.04601v1` - [abs](http://arxiv.org/abs/2011.04601v1) - [pdf](http://arxiv.org/pdf/2011.04601v1)

> To date, research on sensor-equipped mobile devices has primarily focused on the purely supervised task of human activity recognition (walking, running, etc), demonstrating limited success in inferring high-level health outcomes from low-level signals, such as acceleration. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship.   We evaluate our model in the largest free-living combined-sensing dataset (comprising more than 280,000 hours of wrist accelerometer & wearable ECG data) and show that the resulting embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict (higher than 70 AUC) variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring.

</details>

<details>

<summary>2020-11-09 20:16:57 - CLAR: A Cross-Lingual Argument Regularizer for Semantic Role Labeling</summary>

- *Ishan Jindal, Yunyao Li, Siddhartha Brahma, Huaiyu Zhu*

- `2011.04732v1` - [abs](http://arxiv.org/abs/2011.04732v1) - [pdf](http://arxiv.org/pdf/2011.04732v1)

> Semantic role labeling (SRL) identifies predicate-argument structure(s) in a given sentence. Although different languages have different argument annotations, polyglot training, the idea of training one model on multiple languages, has previously been shown to outperform monolingual baselines, especially for low resource languages. In fact, even a simple combination of data has been shown to be effective with polyglot training by representing the distant vocabularies in a shared representation space. Meanwhile, despite the dissimilarity in argument annotations between languages, certain argument labels do share common semantic meaning across languages (e.g. adjuncts have more or less similar semantic meaning across languages). To leverage such similarity in annotation space across languages, we propose a method called Cross-Lingual Argument Regularizer (CLAR). CLAR identifies such linguistic annotation similarity across languages and exploits this information to map the target language arguments using a transformation of the space on which source language arguments lie. By doing so, our experimental results show that CLAR consistently improves SRL performance on multiple languages over monolingual and polyglot baselines for low resource languages.

</details>

<details>

<summary>2020-11-09 20:42:01 - Adversarial Semantic Collisions</summary>

- *Congzheng Song, Alexander M. Rush, Vitaly Shmatikov*

- `2011.04743v1` - [abs](http://arxiv.org/abs/2011.04743v1) - [pdf](http://arxiv.org/pdf/2011.04743v1)

> We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts-- including paraphrase identification, document retrieval, response suggestion, and extractive summarization-- are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert.

</details>

<details>

<summary>2020-11-09 20:45:39 - Personalized Query Rewriting in Conversational AI Agents</summary>

- *Alireza Roshan-Ghias, Clint Solomon Mathialagan, Pragaash Ponnusamy, Lambert Mathias, Chenlei Guo*

- `2011.04748v1` - [abs](http://arxiv.org/abs/2011.04748v1) - [pdf](http://arxiv.org/pdf/2011.04748v1)

> Spoken language understanding (SLU) systems in conversational AI agents often experience errors in the form of misrecognitions by automatic speech recognition (ASR) or semantic gaps in natural language understanding (NLU). These errors easily translate to user frustrations, particularly so in recurrent events e.g. regularly toggling an appliance, calling a frequent contact, etc. In this work, we propose a query rewriting approach by leveraging users' historically successful interactions as a form of memory. We present a neural retrieval model and a pointer-generator network with hierarchical attention and show that they perform significantly better at the query rewriting task with the aforementioned user memories than without. We also highlight how our approach with the proposed models leverages the structural and semantic diversity in ASR's output towards recovering users' intents.

</details>

<details>

<summary>2020-11-10 00:35:33 - Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video with Applications for Virtual Reality</summary>

- *Alisha Sharma, Ryan Nett, Jonathan Ventura*

- `2010.07704v2` - [abs](http://arxiv.org/abs/2010.07704v2) - [pdf](http://arxiv.org/pdf/2010.07704v2)

> We introduce a convolutional neural network model for unsupervised learning of depth and ego-motion from cylindrical panoramic video. Panoramic depth estimation is an important technology for applications such as virtual reality, 3D modeling, and autonomous robotic navigation. In contrast to previous approaches for applying convolutional neural networks to panoramic imagery, we use the cylindrical panoramic projection which allows for the use of the traditional CNN layers such as convolutional filters and max pooling without modification. Our evaluation of synthetic and real data shows that unsupervised learning of depth and ego-motion on cylindrical panoramic images can produce high-quality depth maps and that an increased field-of-view improves ego-motion estimation accuracy. We create two new datasets to evaluate our approach: a synthetic dataset created using the CARLA simulator, and Headcam, a novel dataset of panoramic video collected from a helmet-mounted camera while biking in an urban setting. We also apply our network to the problem of converting monocular panoramas to stereo panoramas.

</details>

<details>

<summary>2020-11-10 02:31:31 - Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts</summary>

- *Hanmeng Liu, Leyang Cui, Jian Liu, Yue Zhang*

- `2011.04864v1` - [abs](http://arxiv.org/abs/2011.04864v1) - [pdf](http://arxiv.org/pdf/2011.04864v1)

> Natural language inference (NLI) is a fundamental NLP task, investigating the entailment relationship between two texts. Popular NLI datasets present the task at sentence-level. While adequate for testing semantic representations, they fall short for testing contextual reasoning over long texts, which is a natural part of the human inference process. We introduce ConTRoL, a new dataset for ConTextual Reasoning over Long texts. Consisting of 8,325 expert-designed "context-hypothesis" pairs with gold labels, ConTRoL is a passage-level NLI dataset with a focus on complex contextual reasoning types such as logical reasoning. It is derived from competitive selection and recruitment test (verbal reasoning test) for police recruitment, with expert level quality. Compared with previous NLI benchmarks, the materials in ConTRoL are much more challenging, involving a range of reasoning types. Empirical results show that state-of-the-art language models perform by far worse than educated humans. Our dataset can also serve as a testing-set for downstream tasks like Checking Factual Correctness of Summaries.

</details>

<details>

<summary>2020-11-10 04:20:49 - Distance-Based Anomaly Detection for Industrial Surfaces Using Triplet Networks</summary>

- *Tareq Tayeh, Sulaiman Aburakhia, Ryan Myers, Abdallah Shami*

- `2011.04121v2` - [abs](http://arxiv.org/abs/2011.04121v2) - [pdf](http://arxiv.org/pdf/2011.04121v2)

> Surface anomaly detection plays an important quality control role in many manufacturing industries to reduce scrap production. Machine-based visual inspections have been utilized in recent years to conduct this task instead of human experts. In particular, deep learning Convolutional Neural Networks (CNNs) have been at the forefront of these image processing-based solutions due to their predictive accuracy and efficiency. Training a CNN on a classification objective requires a sufficiently large amount of defective data, which is often not available. In this paper, we address that challenge by training the CNN on surface texture patches with a distance-based anomaly detection objective instead. A deep residual-based triplet network model is utilized, and defective training samples are synthesized exclusively from non-defective samples via random erasing techniques to directly learn a similarity metric between the same-class samples and out-of-class samples. Evaluation results demonstrate the approach's strength in detecting different types of anomalies, such as bent, broken, or cracked surfaces, for known surfaces that are part of the training data and unseen novel surfaces.

</details>

<details>

<summary>2020-11-10 04:40:01 - Adversarial Black-Box Attacks On Text Classifiers Using Multi-Objective Genetic Optimization Guided By Deep Networks</summary>

- *Alex Mathai, Shreya Khare, Srikanth Tamilselvam, Senthil Mani*

- `2011.03901v2` - [abs](http://arxiv.org/abs/2011.03901v2) - [pdf](http://arxiv.org/pdf/2011.03901v2)

> We propose a novel genetic-algorithm technique that generates black-box adversarial examples which successfully fool neural network based text classifiers. We perform a genetic search with multi-objective optimization guided by deep learning based inferences and Seq2Seq mutation to generate semantically similar but imperceptible adversaries. We compare our approach with DeepWordBug (DWB) on SST and IMDB sentiment datasets by attacking three trained models viz. char-LSTM, word-LSTM and elmo-LSTM. On an average, we achieve an attack success rate of 65.67% for SST and 36.45% for IMDB across the three models showing an improvement of 49.48% and 101% respectively. Furthermore, our qualitative study indicates that 94% of the time, the users were not able to distinguish between an original and adversarial sample.

</details>

<details>

<summary>2020-11-10 07:16:18 - When Do You Need Billions of Words of Pretraining Data?</summary>

- *Yian Zhang, Alex Warstadt, Haau-Sing Li, Samuel R. Bowman*

- `2011.04946v1` - [abs](http://arxiv.org/abs/2011.04946v1) - [pdf](http://arxiv.org/pdf/2011.04946v1)

> NLP is currently dominated by general-purpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? We adopt four probing methods---classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks---and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.

</details>

<details>

<summary>2020-11-10 10:41:03 - Multi-Loss Weighting with Coefficient of Variations</summary>

- *Rick Groenendijk, Sezer Karaoglu, Theo Gevers, Thomas Mensink*

- `2009.01717v2` - [abs](http://arxiv.org/abs/2009.01717v2) - [pdf](http://arxiv.org/pdf/2009.01717v2)

> Many interesting tasks in machine learning and computer vision are learned by optimising an objective function defined as a weighted linear combination of multiple losses. The final performance is sensitive to choosing the correct (relative) weights for these losses. Finding a good set of weights is often done by adopting them into the set of hyper-parameters, which are set using an extensive grid search. This is computationally expensive. In this paper, we propose a weighting scheme based on the coefficient of variations and set the weights based on properties observed while training the model. The proposed method incorporates a measure of uncertainty to balance the losses, and as a result the loss weights evolve during training without requiring another (learning based) optimisation. In contrast to many loss weighting methods in literature, we focus on single-task multi-loss problems, such as monocular depth estimation and semantic segmentation, and show that multi-task approaches for loss weighting do not work on those single-tasks. The validity of the approach is shown empirically for depth estimation and semantic segmentation on multiple datasets.

</details>

<details>

<summary>2020-11-10 16:18:39 - SeqMobile: A Sequence Based Efficient Android Malware Detection System Using RNN on Mobile Devices</summary>

- *Ruitao Feng, Jing Qiang Lim, Sen Chen, Shang-Wei Lin, Yang Liu*

- `2011.05218v1` - [abs](http://arxiv.org/abs/2011.05218v1) - [pdf](http://arxiv.org/pdf/2011.05218v1)

> With the proliferation of Android malware, the demand for an effective and efficient malware detection system is on the rise. The existing device-end learning based solutions tend to extract limited syntax features (e.g., permissions and API calls) to meet a certain time constraint of mobile devices. However, syntax features lack the semantics which can represent the potential malicious behaviors and further result in more robust model with high accuracy for malware detection. In this paper, we propose an efficient Android malware detection system, named SeqMobile, which adopts behavior-based sequence features and leverages customized deep neural networks on mobile devices instead of the server. Different from the traditional sequence-based approaches on server, to meet the performance demand, SeqMobile accepts three effective performance optimization methods to reduce the time cost. To evaluate the effectiveness and efficiency of our system, we conduct experiments from the following aspects 1) the detection accuracy of different recurrent neural networks; 2) the feature extraction performance on different mobile devices, 3) the detection accuracy and prediction time cost of different sequence lengths. The results unveil that SeqMobile can effectively detect malware with high accuracy. Moreover, our performance optimization methods have proven to improve the performance of training and prediction by at least twofold. Additionally, to discover the potential performance optimization from the SOTA TensorFlow model optimization toolkit for our approach, we also provide an evaluation on the toolkit, which can serve as a guidance for other systems leveraging on sequence-based learning approach. Overall, we conclude that our sequence-based approach, together with our performance optimization methods, enable us to detect malware under the performance demands of mobile devices.

</details>

<details>

<summary>2020-11-10 17:25:27 - Medical Knowledge-enriched Textual Entailment Framework</summary>

- *Shweta Yadav, Vishal Pallagani, Amit Sheth*

- `2011.05257v1` - [abs](http://arxiv.org/abs/2011.05257v1) - [pdf](http://arxiv.org/pdf/2011.05257v1)

> One of the cardinal tasks in achieving robust medical question answering systems is textual entailment. The existing approaches make use of an ensemble of pre-trained language models or data augmentation, often to clock higher numbers on the validation metrics. However, two major shortcomings impede higher success in identifying entailment: (1) understanding the focus/intent of the question and (2) ability to utilize the real-world background knowledge to capture the context beyond the sentence. In this paper, we present a novel Medical Knowledge-Enriched Textual Entailment framework that allows the model to acquire a semantic and global representation of the input medical text with the help of a relevant domain-specific knowledge graph. We evaluate our framework on the benchmark MEDIQA-RQE dataset and manifest that the use of knowledge enriched dual-encoding mechanism help in achieving an absolute improvement of 8.27% over SOTA language models. We have made the source code available here.

</details>

<details>

<summary>2020-11-10 17:38:27 - On Posterior Collapse and Encoder Feature Dispersion in Sequence VAEs</summary>

- *Teng Long, Yanshuai Cao, Jackie Chi Kit Cheung*

- `1911.03976v2` - [abs](http://arxiv.org/abs/1911.03976v2) - [pdf](http://arxiv.org/pdf/1911.03976v2)

> Variational autoencoders (VAEs) hold great potential for modelling text, as they could in theory separate high-level semantic and syntactic properties from local regularities of natural language. Practically, however, VAEs with autoregressive decoders often suffer from posterior collapse, a phenomenon where the model learns to ignore the latent variables, causing the sequence VAE to degenerate into a language model. In this paper, we argue that posterior collapse is in part caused by the lack of dispersion in encoder features. We provide empirical evidence to verify this hypothesis, and propose a straightforward fix using pooling. This simple technique effectively prevents posterior collapse, allowing model to achieve significantly better data log-likelihood than standard sequence VAEs. Comparing to existing work, our proposed method is able to achieve comparable or superior performances while being more computationally efficient.

</details>

<details>

<summary>2020-11-10 18:59:02 - Guarding Serverless Applications with SecLambda</summary>

- *Deepak Sirone Jegan, Liang Wang, Siddhant Bhagat, Thomas Ristenpart, Michael Swift*

- `2011.05322v1` - [abs](http://arxiv.org/abs/2011.05322v1) - [pdf](http://arxiv.org/pdf/2011.05322v1)

> As an emerging application paradigm, serverless computing attracts attention from more and more attackers. Unfortunately, security tools for conventional applications cannot be easily ported to serverless, and existing serverless security solutions are inadequate. In this paper, we present \emph{SecLambda}, an extensible security framework that leverages local function state and global application state to perform sophisticated security tasks to protect an application. We show how SecLambda can be used to achieve control flow integrity, credential protection, and rate limiting in serverless applications. We evaluate the performance overhead and security of SecLambda using realistic open-source applications, and our results suggest that SecLambda can mitigate several attacks while introducing relatively low performance overhead.

</details>

<details>

<summary>2020-11-10 19:09:46 - Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity</summary>

- *Hamza Harkous, Isabel Groves, Amir Saffari*

- `2004.06577v2` - [abs](http://arxiv.org/abs/2004.06577v2) - [pdf](http://arxiv.org/pdf/2004.06577v2)

> End-to-end neural data-to-text (D2T) generation has recently emerged as an alternative to pipeline-based architectures. However, it has faced challenges in generalizing to new domains and generating semantically consistent text. In this work, we present DataTuner, a neural, end-to-end data-to-text generation system that makes minimal assumptions about the data representation and the target domain. We take a two-stage generation-reranking approach, combining a fine-tuned language model with a semantic fidelity classifier. Each of our components is learnt end-to-end without the need for dataset-specific heuristics, entity delexicalization, or post-processing. We show that DataTuner achieves state of the art results on the automated metrics across four major D2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with a fluency assessed by human annotators nearing or exceeding the human-written reference texts. We further demonstrate that the model-based semantic fidelity scorer in DataTuner is a better assessment tool compared to traditional, heuristic-based measures. Our generated text has a significantly better semantic fidelity than the state of the art across all four datasets

</details>

<details>

<summary>2020-11-10 21:24:42 - Deep machine learning-assisted multiphoton microscopy to reduce light exposure and expedite imaging</summary>

- *Stephen McAleer, Alex Fast, Yuntian Xue, Magdalene Seiler, William Tang, Mihaela Balu, Pierre Baldi, Andrew W. Browne*

- `2011.06408v1` - [abs](http://arxiv.org/abs/2011.06408v1) - [pdf](http://arxiv.org/pdf/2011.06408v1)

> Two-photon excitation fluorescence (2PEF) allows imaging of tissue up to about one millimeter in thickness. Typically, reducing fluorescence excitation exposure reduces the quality of the image. However, using deep learning super resolution techniques, these low-resolution images can be converted to high-resolution images. This work explores improving human tissue imaging by applying deep learning to maximize image quality while reducing fluorescence excitation exposure. We analyze two methods: a method based on U-Net, and a patch-based regression method. Both methods are evaluated on a skin dataset and an eye dataset. The eye dataset includes 1200 paired high power and low power images of retinal organoids. The skin dataset contains multiple frames of each sample of human skin. High-resolution images were formed by averaging 70 frames for each sample and low-resolution images were formed by averaging the first 7 and 15 frames for each sample. The skin dataset includes 550 images for each of the resolution levels. We track two measures of performance for the two methods: mean squared error (MSE) and structural similarity index measure (SSIM). For the eye dataset, the patches method achieves an average MSE of 27,611 compared to 146,855 for the U-Net method, and an average SSIM of 0.636 compared to 0.607 for the U-Net method. For the skin dataset, the patches method achieves an average MSE of 3.768 compared to 4.032 for the U-Net method, and an average SSIM of 0.824 compared to 0.783 for the U-Net method. Despite better performance on image quality, the patches method is worse than the U-Net method when comparing the speed of prediction, taking 303 seconds to predict one image compared to less than one second for the U-Net method.

</details>

<details>

<summary>2020-11-10 22:47:14 - Multilingual AMR-to-Text Generation</summary>

- *Angela Fan, Claire Gardent*

- `2011.05443v1` - [abs](http://arxiv.org/abs/2011.05443v1) - [pdf](http://arxiv.org/pdf/2011.05443v1)

> Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. For eighteen languages, based on automatic metrics, our multilingual models surpass baselines that generate into a single language. We analyse the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.

</details>

<details>

<summary>2020-11-11 00:28:40 - ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery</summary>

- *Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Kyle Story, Rose Rustowicz, Cooper Elsworth, Kemen Austin, Andrew Y. Ng*

- `2011.05479v1` - [abs](http://arxiv.org/abs/2011.05479v1) - [pdf](http://arxiv.org/pdf/2011.05479v1)

> Characterizing the processes leading to deforestation is critical to the development and implementation of targeted forest conservation and management policies. In this work, we develop a deep learning model called ForestNet to classify the drivers of primary forest loss in Indonesia, a country with one of the highest deforestation rates in the world. Using satellite imagery, ForestNet identifies the direct drivers of deforestation in forest loss patches of any size. We curate a dataset of Landsat 8 satellite images of known forest loss events paired with driver annotations from expert interpreters. We use the dataset to train and validate the models and demonstrate that ForestNet substantially outperforms other standard driver classification approaches. In order to support future research on automated approaches to deforestation driver classification, the dataset curated in this study is publicly available at https://stanfordmlgroup.github.io/projects/forestnet .

</details>

<details>

<summary>2020-11-11 01:48:09 - Towards Semi-Supervised Semantics Understanding from Speech</summary>

- *Cheng-I Lai, Jin Cao, Sravan Bodapati, Shang-Wen Li*

- `2011.06195v1` - [abs](http://arxiv.org/abs/2011.06195v1) - [pdf](http://arxiv.org/pdf/2011.06195v1)

> Much recent work on Spoken Language Understanding (SLU) falls short in at least one of three ways: models were trained on oracle text input and neglected the Automatics Speech Recognition (ASR) outputs, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. We proposed a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed speech to address these. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU corpus. In parallel, we identified two inadequate settings under which SLU models have been tested: noise-robustness and E2E semantics evaluation. We tested the proposed framework under realistic environmental noises and with a new metric, the slots edit F1 score, on two public SLU corpora. Experiments show that our SLU framework with speech as input can perform on par with those with oracle text as input in semantics understanding, while environmental noises are present, and a limited amount of labeled semantics data is available.

</details>

<details>

<summary>2020-11-11 06:18:31 - Recognizing More Emotions with Less Data Using Self-supervised Transfer Learning</summary>

- *Jonathan Boigne, Biman Liyanage, Ted Östrem*

- `2011.05585v1` - [abs](http://arxiv.org/abs/2011.05585v1) - [pdf](http://arxiv.org/pdf/2011.05585v1)

> We propose a novel transfer learning method for speech emotion recognition allowing us to obtain promising results when only few training data is available. With as low as 125 examples per emotion class, we were able to reach a higher accuracy than a strong baseline trained on 8 times more data. Our method leverages knowledge contained in pre-trained speech representations extracted from models trained on a more general self-supervised task which doesn't require human annotations, such as the wav2vec model. We provide detailed insights on the benefits of our approach by varying the training data size, which can help labeling teams to work more efficiently. We compare performance with other popular methods on the IEMOCAP dataset, a well-benchmarked dataset among the Speech Emotion Recognition (SER) research community. Furthermore, we demonstrate that results can be greatly improved by combining acoustic and linguistic knowledge from transfer learning. We align acoustic pre-trained representations with semantic representations from the BERT model through an attention-based recurrent neural network. Performance improves significantly when combining both modalities and scales with the amount of data. When trained on the full IEMOCAP dataset, we reach a new state-of-the-art of 73.9% unweighted accuracy (UA).

</details>

<details>

<summary>2020-11-11 09:23:20 - Behavior Trees in Action: A Study of Robotics Applications</summary>

- *Razan Ghzouli, Thorsten Berger, Einar Broch Johnsen, Swaib Dragule, Andrzej Wąsowski*

- `2010.06256v2` - [abs](http://arxiv.org/abs/2010.06256v2) - [pdf](http://arxiv.org/pdf/2010.06256v2)

> Autonomous robots combine a variety of skills to form increasingly complex behaviors called missions. While the skills are often programmed at a relatively low level of abstraction, their coordination is architecturally separated and often expressed in higher-level languages or frameworks. Recently, the language of Behavior Trees gained attention among roboticists for this reason. Originally designed for computer games to model autonomous actors, Behavior Trees offer an extensible tree-based representation of missions. However, even though, several implementations of the language are in use, little is known about its usage and scope in the real world. How do behavior trees relate to traditional languages for describing behavior? How are behavior tree concepts used in applications? What are the benefits of using them?   We present a study of the key language concepts in Behavior Trees and their use in real-world robotic applications. We identify behavior tree languages and compare their semantics to the most well-known behavior modeling languages: state and activity diagrams. We mine open source repositories for robotics applications that use the language and analyze this usage. We find that Behavior Trees are a pragmatic language, not fully specified, allowing projects to extend it even for just one model. Behavior trees clearly resemble the models-at-runtime paradigm. We contribute a dataset of real-world behavior models, hoping to inspire the community to use and further develop this language, associated tools, and analysis techniques.

</details>

<details>

<summary>2020-11-11 10:44:52 - Noise Conscious Training of Non Local Neural Network powered by Self Attentive Spectral Normalized Markovian Patch GAN for Low Dose CT Denoising</summary>

- *Sutanu Bera, Prabir Kumar Biswas*

- `2011.05684v1` - [abs](http://arxiv.org/abs/2011.05684v1) - [pdf](http://arxiv.org/pdf/2011.05684v1)

> The explosive rise of the use of Computer tomography (CT) imaging in medical practice has heightened public concern over the patient's associated radiation dose. However, reducing the radiation dose leads to increased noise and artifacts, which adversely degrades the scan's interpretability. Consequently, an advanced image reconstruction algorithm to improve the diagnostic performance of low dose ct arose as the primary concern among the researchers, which is challenging due to the ill-posedness of the problem. In recent times, the deep learning-based technique has emerged as a dominant method for low dose CT(LDCT) denoising. However, some common bottleneck still exists, which hinders deep learning-based techniques from furnishing the best performance. In this study, we attempted to mitigate these problems with three novel accretions. First, we propose a novel convolutional module as the first attempt to utilize neighborhood similarity of CT images for denoising tasks. Our proposed module assisted in boosting the denoising by a significant margin. Next, we moved towards the problem of non-stationarity of CT noise and introduced a new noise aware mean square error loss for LDCT denoising. Moreover, the loss mentioned above also assisted to alleviate the laborious effort required while training CT denoising network using image patches. Lastly, we propose a novel discriminator function for CT denoising tasks. The conventional vanilla discriminator tends to overlook the fine structural details and focus on the global agreement. Our proposed discriminator leverage self-attention and pixel-wise GANs for restoring the diagnostic quality of LDCT images. Our method validated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge performed remarkably better than the existing state of the art method.

</details>

<details>

<summary>2020-11-11 12:35:07 - DeepSim: Semantic similarity metrics for learned image registration</summary>

- *Steffen Czolbe, Oswin Krause, Aasa Feragen*

- `2011.05735v1` - [abs](http://arxiv.org/abs/2011.05735v1) - [pdf](http://arxiv.org/pdf/2011.05735v1)

> We propose a semantic similarity metric for image registration. Existing metrics like euclidean distance or normalized cross-correlation focus on aligning intensity values, giving difficulties with low intensity contrast or noise. Our semantic approach learns dataset-specific features that drive the optimization of a learning-based registration model. Comparing to existing unsupervised and supervised methods across multiple image modalities and applications, we achieve consistently high registration accuracy and faster convergence than state of the art, and the learned invariance to noise gives smoother transformations on low-quality images.

</details>

<details>

<summary>2020-11-11 14:42:17 - ATOM: Commit Message Generation Based on Abstract Syntax Tree and Hybrid Ranking</summary>

- *Shangqing Liu, Cuiyun Gao, Sen Chen, Lun Yiu Nie, Yang Liu*

- `1912.02972v2` - [abs](http://arxiv.org/abs/1912.02972v2) - [pdf](http://arxiv.org/pdf/1912.02972v2)

> Commit messages record code changes (e.g., feature modifications and bug repairs) in natural language, and are useful for program comprehension. Due to the frequent updates of software and time cost, developers are generally unmotivated to write commit messages for code changes. Therefore, automating the message writing process is necessitated. Previous studies on commit message generation have been benefited from generation models or retrieval models, but the code structure of changed code, i.e., AST, which can be important for capturing code semantics, has not been explicitly involved. Moreover, although generation models have the advantages of synthesizing commit messages for new code changes, they are not easy to bridge the semantic gap between code and natural languages which could be mitigated by retrieval models. In this paper, we propose a novel commit message generation model, named ATOM, which explicitly incorporates the abstract syntax tree for representing code changes and integrates both retrieved and generated messages through hybrid ranking. Specifically, the hybrid ranking module can prioritize the most accurate message from both retrieved and generated messages regarding one code change. We evaluate the proposed model ATOM on our dataset crawled from 56 popular Java repositories. Experimental results demonstrate that ATOM increases the state-of-the-art models by 30.72% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate text generation systems). Qualitative analysis also demonstrates the effectiveness of ATOM in generating accurate code commit messages.

</details>

<details>

<summary>2020-11-11 18:22:22 - Real Time Multi-Class Object Detection and Recognition Using Vision Augmentation Algorithm</summary>

- *Al-Akhir Nayan, Joyeta Saha, Ahamad Nokib Mozumder, Khan Raqib Mahmud, Abul Kalam Al Azad*

- `2003.07442v4` - [abs](http://arxiv.org/abs/2003.07442v4) - [pdf](http://arxiv.org/pdf/2003.07442v4)

> The aim of this research is to detect small objects with low resolution and noise. The existing real time object detection algorithm is based on the deep neural network of convolution need to perform multilevel convolution and pooling operations on the entire image to extract a deep semantic characteristic of the image. The detection models perform better for large objects. The features of existing models do not fully represent the essential features of small objects after repeated convolution operations. We have introduced a novel real time detection algorithm which employs upsampling and skip connection to extract multiscale features at different convolution levels in a learning task resulting a remarkable performance in detecting small objects. The detection precision of the model is shown to be higher and faster than that of the state-of-the-art models.

</details>

<details>

<summary>2020-11-11 18:41:07 - Transformers for One-Shot Visual Imitation</summary>

- *Sudeep Dasari, Abhinav Gupta*

- `2011.05970v1` - [abs](http://arxiv.org/abs/2011.05970v1) - [pdf](http://arxiv.org/pdf/2011.05970v1)

> Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a $\sim 2$x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.

</details>

<details>

<summary>2020-11-12 00:49:06 - Algorithms for Causal Reasoning in Probability Trees</summary>

- *Tim Genewein, Tom McGrath, Grégoire Déletang, Vladimir Mikulik, Miljan Martic, Shane Legg, Pedro A. Ortega*

- `2010.12237v2` - [abs](http://arxiv.org/abs/2010.12237v2) - [pdf](http://arxiv.org/pdf/2010.12237v2)

> Probability trees are one of the simplest models of causal generative processes. They possess clean semantics and -- unlike causal Bayesian networks -- they can represent context-specific causal dependencies, which are necessary for e.g. causal induction. Yet, they have received little attention from the AI and ML community. Here we present concrete algorithms for causal reasoning in discrete probability trees that cover the entire causal hierarchy (association, intervention, and counterfactuals), and operate on arbitrary propositional and causal events. Our work expands the domain of causal reasoning to a very general class of discrete stochastic processes.

</details>

<details>

<summary>2020-11-12 01:28:55 - Detecting Adversarial Patches with Class Conditional Reconstruction Networks</summary>

- *Perry Deng, Mohammad Saidur Rahman, Matthew Wright*

- `2011.05850v2` - [abs](http://arxiv.org/abs/2011.05850v2) - [pdf](http://arxiv.org/pdf/2011.05850v2)

> Defending against physical adversarial attacks is a rapidly growing topic in deep learning and computer vision. Prominent forms of physical adversarial attacks, such as overlaid adversarial patches and objects, share similarities with digital attacks, but are easy for humans to notice. This leads us to explore the hypothesis that adversarial detection methods, which have been shown to be ineffective against adaptive digital adversarial examples, can be effective against these physical attacks. We use one such detection method based on autoencoder architectures, and perform adversarial patching experiments on MNIST, SVHN, and CIFAR10 against a CNN architecture and two CapsNet architectures. We also propose two modifications to the EM-Routed CapsNet architecture, Affine Voting and Matrix Capsule Dropout, to improve its classification performance. Our investigation shows that the detector retains some of its effectiveness even against adaptive adversarial patch attacks. In addition, detection performance tends to decrease among all the architectures with the increase of dataset complexity.

</details>

<details>

<summary>2020-11-12 10:44:41 - Analyzing Neural Discourse Coherence Models</summary>

- *Youmna Farag, Josef Valvoda, Helen Yannakoudakis, Ted Briscoe*

- `2011.06306v1` - [abs](http://arxiv.org/abs/2011.06306v1) - [pdf](http://arxiv.org/pdf/2011.06306v1)

> In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.

</details>

<details>

<summary>2020-11-12 12:33:29 - Multi-View Dynamic Heterogeneous Information Network Embedding</summary>

- *Zhenghao Zhang, Jianbin Huang, Qinglin Tan*

- `2011.06346v1` - [abs](http://arxiv.org/abs/2011.06346v1) - [pdf](http://arxiv.org/pdf/2011.06346v1)

> Most existing Heterogeneous Information Network (HIN) embedding methods focus on static environments while neglecting the evolving characteristic of realworld networks. Although several dynamic embedding methods have been proposed, they are merely designed for homogeneous networks and cannot be directly applied in heterogeneous environment. To tackle above challenges, we propose a novel framework for incorporating temporal information into HIN embedding, denoted as Multi-View Dynamic HIN Embedding (MDHNE), which can efficiently preserve evolution patterns of implicit relationships from different views in updating node representations over time. We first transform HIN to a series of homogeneous networks corresponding to different views. Then our proposed MDHNE applies Recurrent Neural Network (RNN) to incorporate evolving pattern of complex network structure and semantic relationships between nodes into latent embedding spaces, and thus the node representations from multiple views can be learned and updated when HIN evolves over time. Moreover, we come up with an attention based fusion mechanism, which can automatically infer weights of latent representations corresponding to different views by minimizing the objective function specific for different mining tasks. Extensive experiments clearly demonstrate that our MDHNE model outperforms state-of-the-art baselines on three real-world dynamic datasets for different network mining tasks.

</details>

<details>

<summary>2020-11-12 12:53:03 - DisenE: Disentangling Knowledge Graph Embeddings</summary>

- *Xiaoyu Kou, Yankai Lin, Yuntao Li, Jiahao Xu, Peng Li, Jie Zhou, Yan Zhang*

- `2010.14730v2` - [abs](http://arxiv.org/abs/2010.14730v2) - [pdf](http://arxiv.org/pdf/2010.14730v2)

> Knowledge graph embedding (KGE), aiming to embed entities and relations into low-dimensional vectors, has attracted wide attention recently. However, the existing research is mainly based on the black-box neural models, which makes it difficult to interpret the learned representation. In this paper, we introduce DisenE, an end-to-end framework to learn disentangled knowledge graph embeddings. Specially, we introduce an attention-based mechanism that enables the model to explicitly focus on relevant components of entity embeddings according to a given relation. Furthermore, we introduce two novel regularizers to encourage each component of the entity representation to independently reflect an isolated semantic aspect. Experimental results demonstrate that our proposed DisenE investigates a perspective to address the interpretability of KGE and is proved to be an effective way to improve the performance of link prediction tasks.

</details>

<details>

<summary>2020-11-12 14:56:15 - Turning Transport Data to Comply with EU Standards while Enabling a Multimodal Transport Knowledge Graph</summary>

- *Mario Scrocca, Marco Comerio, Alessio Carenini, Irene Celino*

- `2011.06423v1` - [abs](http://arxiv.org/abs/2011.06423v1) - [pdf](http://arxiv.org/pdf/2011.06423v1)

> Complying with the EU Regulation on multimodal transportation services requires sharing data on the National Access Points in one of the standards (e.g., NeTEx and SIRI) indicated by the European Commission. These standards are complex and of limited practical adoption. This means that datasets are natively expressed in other formats and require a data translation process for full compliance.   This paper describes the solution to turn the authoritative data of three different transport stakeholders from Italy and Spain into a format compliant with EU standards by means of Semantic Web technologies. Our solution addresses the challenge and also contributes to build a multi-modal transport Knowledge Graph of interlinked and interoperable information that enables intelligent querying and exploration, as well as facilitates the design of added-value services.

</details>

<details>

<summary>2020-11-12 17:41:51 - Unsupervised Domain Adaptation for Visual Navigation</summary>

- *Shangda Li, Devendra Singh Chaplot, Yao-Hung Hubert Tsai, Yue Wu, Louis-Philippe Morency, Ruslan Salakhutdinov*

- `2010.14543v2` - [abs](http://arxiv.org/abs/2010.14543v2) - [pdf](http://arxiv.org/pdf/2010.14543v2)

> Advances in visual navigation methods have led to intelligent embodied navigation agents capable of learning meaningful representations from raw RGB images and perform a wide variety of tasks involving structural and semantic reasoning. However, most learning-based navigation policies are trained and tested in simulation environments. In order for these policies to be practically useful, they need to be transferred to the real-world. In this paper, we propose an unsupervised domain adaptation method for visual navigation. Our method translates the images in the target domain to the source domain such that the translation is consistent with the representations learned by the navigation policy. The proposed method outperforms several baselines across two different navigation tasks in simulation. We further show that our method can be used to transfer the navigation policies learned in simulation to the real world.

</details>

<details>

<summary>2020-11-12 18:21:11 - A partition-based similarity for classification distributions</summary>

- *Hayden S. Helm, Ronak D. Mehta, Brandon Duderstadt, Weiwei Yang, Christoper M. White, Ali Geisa, Joshua T. Vogelstein, Carey E. Priebe*

- `2011.06557v1` - [abs](http://arxiv.org/abs/2011.06557v1) - [pdf](http://arxiv.org/pdf/2011.06557v1)

> Herein we define a measure of similarity between classification distributions that is both principled from the perspective of statistical pattern recognition and useful from the perspective of machine learning practitioners. In particular, we propose a novel similarity on classification distributions, dubbed task similarity, that quantifies how an optimally-transformed optimal representation for a source distribution performs when applied to inference related to a target distribution. The definition of task similarity allows for natural definitions of adversarial and orthogonal distributions. We highlight limiting properties of representations induced by (universally) consistent decision rules and demonstrate in simulation that an empirical estimate of task similarity is a function of the decision rule deployed for inference. We demonstrate that for a given target distribution, both transfer efficiency and semantic similarity of candidate source distributions correlate with empirical task similarity.

</details>

<details>

<summary>2020-11-13 02:14:32 - A GAN-based Approach for Mitigating Inference Attacks in Smart Home Environment</summary>

- *Olakunle Ibitoye, Ashraf Matrawy, M. Omair Shafiq*

- `2011.06725v1` - [abs](http://arxiv.org/abs/2011.06725v1) - [pdf](http://arxiv.org/pdf/2011.06725v1)

> The proliferation of smart, connected, always listening devices have introduced significant privacy risks to users in a smart home environment. Beyond the notable risk of eavesdropping, intruders can adopt machine learning techniques to infer sensitive information from audio recordings on these devices, resulting in a new dimension of privacy concerns and attack variables to smart home users. Techniques such as sound masking and microphone jamming have been effectively used to prevent eavesdroppers from listening in to private conversations. In this study, we explore the problem of adversaries spying on smart home users to infer sensitive information with the aid of machine learning techniques. We then analyze the role of randomness in the effectiveness of sound masking for mitigating sensitive information leakage. We propose a Generative Adversarial Network (GAN) based approach for privacy preservation in smart homes which generates random noise to distort the unwanted machine learning-based inference. Our experimental results demonstrate that GANs can be used to generate more effective sound masking noise signals which exhibit more randomness and effectively mitigate deep learning-based inference attacks while preserving the semantics of the audio samples.

</details>

<details>

<summary>2020-11-13 07:34:57 - Heterogeneous Deep Graph Infomax</summary>

- *Yuxiang Ren, Bo Liu, Chao Huang, Peng Dai, Liefeng Bo, Jiawei Zhang*

- `1911.08538v5` - [abs](http://arxiv.org/abs/1911.08538v5) - [pdf](http://arxiv.org/pdf/1911.08538v5)

> Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node clustering. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph node learning problem. Inspired by the emerging information theoretic-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI) for heterogeneous graph representation learning. We use the meta-path structure to analyze the connections involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI effectively learns high-level node representations that can be utilized in downstream graph-related tasks. Experiment results show that HDGI remarkably outperforms state-of-the-art unsupervised graph representation learning methods on both classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic regression, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models.

</details>

<details>

<summary>2020-11-13 08:54:13 - Shared Prior Learning of Energy-Based Models for Image Reconstruction</summary>

- *Thomas Pinetz, Erich Kobler, Thomas Pock, Alexander Effland*

- `2011.06539v2` - [abs](http://arxiv.org/abs/2011.06539v2) - [pdf](http://arxiv.org/pdf/2011.06539v2)

> We propose a novel learning-based framework for image reconstruction particularly designed for training without ground truth data, which has three major building blocks: energy-based learning, a patch-based Wasserstein loss functional, and shared prior learning. In energy-based learning, the parameters of an energy functional composed of a learned data fidelity term and a data-driven regularizer are computed in a mean-field optimal control problem. In the absence of ground truth data, we change the loss functional to a patch-based Wasserstein functional, in which local statistics of the output images are compared to uncorrupted reference patches. Finally, in shared prior learning, both aforementioned optimal control problems are optimized simultaneously with shared learned parameters of the regularizer to further enhance unsupervised image reconstruction. We derive several time discretization schemes of the gradient flow and verify their consistency in terms of Mosco convergence. In numerous numerical experiments, we demonstrate that the proposed method generates state-of-the-art results for various image reconstruction applications--even if no ground truth images are available for training.

</details>

<details>

<summary>2020-11-13 09:37:10 - Migration-Related Semantic Concepts for the Retrieval of Relevant Video Content</summary>

- *Elejalde Erick, Galanopoulos Damianos, Niederee Claudia, Mezaris Vasileios*

- `2011.06829v1` - [abs](http://arxiv.org/abs/2011.06829v1) - [pdf](http://arxiv.org/pdf/2011.06829v1)

> Migration, and especially irregular migration, is a critical issue for border agencies and society in general. Migration-related situations and decisions are influenced by various factors, including the perceptions about migration routes and target countries. An improved understanding of such factors can be achieved by systematic automated analyses of media and social media channels, and the videos and images published in them. However, the multifaceted nature of migration and the variety of ways migration-related aspects are expressed in images and videos make the finding and automated analysis of migration-related multimedia content a challenging task. We propose a novel approach that effectively bridges the gap between a substantiated domain understanding - encapsulated into a set of Migration-related semantic concepts - and the expression of such concepts in a video, by introducing an advanced video analysis and retrieval method for this purpose.

</details>

<details>

<summary>2020-11-13 14:18:39 - Online Knowledge Distillation via Multi-branch Diversity Enhancement</summary>

- *Zheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan*

- `2010.00795v3` - [abs](http://arxiv.org/abs/2010.00795v3) - [pdf](http://arxiv.org/pdf/2010.00795v3)

> Knowledge distillation is an effective method to transfer the knowledge from the cumbersome teacher model to the lightweight student model. Online knowledge distillation uses the ensembled prediction results of multiple student models as soft targets to train each student model. However, the homogenization problem will lead to difficulty in further improving model performance. In this work, we propose a new distillation method to enhance the diversity among multiple student models. We introduce Feature Fusion Module (FFM), which improves the performance of the attention mechanism in the network by integrating rich semantic information contained in the last block of multiple student models. Furthermore, we use the Classifier Diversification(CD) loss function to strengthen the differences between the student models and deliver a better ensemble result. Extensive experiments proved that our method significantly enhances the diversity among student models and brings better distillation performance. We evaluate our method on three image classification datasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves state-of-the-art performance on these datasets.

</details>

<details>

<summary>2020-11-13 14:50:08 - Learning language variations in news corpora through differential embeddings</summary>

- *Carlos Selmo, Julian F. Martinez, Mariano G. Beiró, J. Ignacio Alvarez-Hamelin*

- `2011.06949v1` - [abs](http://arxiv.org/abs/2011.06949v1) - [pdf](http://arxiv.org/pdf/2011.06949v1)

> There is an increasing interest in the NLP community in capturing variations in the usage of language, either through time (i.e., semantic drift), across regions (as dialects or variants) or in different social contexts (i.e., professional or media technolects). Several successful dynamical embeddings have been proposed that can track semantic change through time. Here we show that a model with a central word representation and a slice-dependent contribution can learn word embeddings from different corpora simultaneously. This model is based on a star-like representation of the slices. We apply it to The New York Times and The Guardian newspapers, and we show that it can capture both temporal dynamics in the yearly slices of each corpus, and language variations between US and UK English in a curated multi-source corpus. We provide an extensive evaluation of this methodology.

</details>

<details>

<summary>2020-11-13 17:35:42 - Yseop at SemEval-2020 Task 5: Cascaded BERT Language Model for Counterfactual Statement Analysis</summary>

- *Hanna Abi Akl, Dominique Mariko, Estelle Labidurie*

- `2005.08519v2` - [abs](http://arxiv.org/abs/2005.08519v2) - [pdf](http://arxiv.org/pdf/2005.08519v2)

> In this paper, we explore strategies to detect and evaluate counterfactual sentences. We describe our system for SemEval-2020 Task 5: Modeling Causal Reasoning in Language: Detecting Counterfactuals. We use a BERT base model for the classification task and build a hybrid BERT Multi-Layer Perceptron system to handle the sequence identification task. Our experiments show that while introducing syntactic and semantic features does little in improving the system in the classification task, using these types of features as cascaded linear inputs to fine-tune the sequence-delimiting ability of the model ensures it outperforms other similar-purpose complex systems like BiLSTM-CRF in the second task. Our system achieves an F1 score of 85.00% in Task 1 and 83.90% in Task 2.

</details>

<details>

<summary>2020-11-13 21:08:36 - Template Guided Text Generation for Task-Oriented Dialogue</summary>

- *Mihir Kale, Abhinav Rastogi*

- `2004.15006v2` - [abs](http://arxiv.org/abs/2004.15006v2) - [pdf](http://arxiv.org/pdf/2004.15006v2)

> Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.

</details>

<details>

<summary>2020-11-13 21:09:54 - Beyond Labeling: Using Clustering to Build Network Behavioral Profiles of Malware Families</summary>

- *Azqa Nadeem, Christian Hammerschmidt, Carlos H. Gañán, Sicco Verwer*

- `1904.01371v3` - [abs](http://arxiv.org/abs/1904.01371v3) - [pdf](http://arxiv.org/pdf/1904.01371v3)

> Malware family labels are known to be inconsistent. They are also black-box since they do not represent the capabilities of malware. The current state-of-the-art in malware capability assessment include mostly manual approaches, which are infeasible due to the ever-increasing volume of discovered malware samples. We propose a novel unsupervised machine learning-based method called MalPaCA, which automates capability assessment by clustering the temporal behavior in malware's network traces. MalPaCA provides meaningful behavioral clusters using only 20 packet headers. Behavioral profiles are generated based on the cluster membership of malware's network traces. A Directed Acyclic Graph shows the relationship between malwares according to their overlapping behaviors. The behavioral profiles together with the DAG provide more insightful characterization of malware than current family designations. We also propose a visualization-based evaluation method for the obtained clusters to assist practitioners in understanding the clustering results. We apply MalPaCA on a financial malware dataset collected in the wild that comprises of 1.1k malware samples resulting in 3.6M packets. Our experiments show that (i) MalPaCA successfully identifies capabilities, such as port scans and reuse of Command and Control servers; (ii) It uncovers multiple discrepancies between behavioral clusters and malware family labels; and (iii) It demonstrates the effectiveness of clustering traces using temporal features by producing an error rate of 8.3%, compared to 57.5% obtained from statistical features.

</details>

<details>

<summary>2020-11-14 21:13:56 - Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving</summary>

- *Michal Uricar, Ganesh Sistu, Hazem Rashed, Antonin Vobecky, Varun Ravi Kumar, Pavel Krizek, Fabian Burger, Senthil Yogamani*

- `1912.02249v3` - [abs](http://arxiv.org/abs/1912.02249v3) - [pdf](http://arxiv.org/pdf/1912.02249v3)

> Wide-angle fisheye cameras are commonly used in automated driving for parking and low-speed navigation tasks. Four of such cameras form a surround-view system that provides a complete and detailed view of the vehicle. These cameras are directly exposed to harsh environmental settings and can get soiled very easily by mud, dust, water, frost. Soiling on the camera lens can severely degrade the visual perception algorithms, and a camera cleaning system triggered by a soiling detection algorithm is increasingly being deployed. While adverse weather conditions, such as rain, are getting attention recently, there is only limited work on general soiling. The main reason is the difficulty in collecting a diverse dataset as it is a relatively rare event. We propose a novel GAN based algorithm for generating unseen patterns of soiled images. Additionally, the proposed method automatically provides the corresponding soiling masks eliminating the manual annotation cost. Augmentation of the generated soiled images for training improves the accuracy of soiling detection tasks significantly by 18% demonstrating its usefulness. The manually annotated soiling dataset and the generated augmentation dataset will be made public. We demonstrate the generalization of our fisheye trained GAN model on the Cityscapes dataset. We provide an empirical evaluation of the degradation of the semantic segmentation algorithm with the soiled data.

</details>

<details>

<summary>2020-11-15 04:51:11 - Image Segmentation Using Deep Learning: A Survey</summary>

- *Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, Demetri Terzopoulos*

- `2001.05566v5` - [abs](http://arxiv.org/abs/2001.05566v5) - [pdf](http://arxiv.org/pdf/2001.05566v5)

> Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.

</details>

<details>

<summary>2020-11-15 22:31:22 - Modeling Task Effects on Meaning Representation in the Brain via Zero-Shot MEG Prediction</summary>

- *Mariya Toneva, Otilia Stretcu, Barnabas Poczos, Leila Wehbe, Tom M. Mitchell*

- `2009.08424v2` - [abs](http://arxiv.org/abs/2009.08424v2) - [pdf](http://arxiv.org/pdf/2009.08424v2)

> How meaning is represented in the brain is still one of the big open questions in neuroscience. Does a word (e.g., bird) always have the same representation, or does the task under which the word is processed alter its representation (answering "can you eat it?" versus "can it fly?")? The brain activity of subjects who read the same word while performing different semantic tasks has been shown to differ across tasks. However, it is still not understood how the task itself contributes to this difference. In the current work, we study Magnetoencephalography (MEG) brain recordings of participants tasked with answering questions about concrete nouns. We investigate the effect of the task (i.e. the question being asked) on the processing of the concrete noun by predicting the millisecond-resolution MEG recordings as a function of both the semantics of the noun and the task. Using this approach, we test several hypotheses about the task-stimulus interactions by comparing the zero-shot predictions made by these hypotheses for novel tasks and nouns not seen during training. We find that incorporating the task semantics significantly improves the prediction of MEG recordings, across participants. The improvement occurs 475-550ms after the participants first see the word, which corresponds to what is considered to be the ending time of semantic processing for a word. These results suggest that only the end of semantic processing of a word is task-dependent, and pose a challenge for future research to formulate new hypotheses for earlier task effects as a function of the task and stimuli.

</details>

<details>

<summary>2020-11-16 01:57:39 - HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification</summary>

- *Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, Mohit Bansal*

- `2011.03088v2` - [abs](http://arxiv.org/abs/2011.03088v2) - [pdf](http://arxiv.org/pdf/2011.03088v2)

> We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes. Moreover, most of the 3/4-hop claims are written in multiple sentences, which adds to the complexity of understanding long-range dependency relations such as coreference. We show that the performance of an existing state-of-the-art semantic-matching model degrades significantly on our dataset as the number of reasoning hops increases, hence demonstrating the necessity of many-hop reasoning to achieve strong results. We hope that the introduction of this challenging dataset and the accompanying evaluation task will encourage research in many-hop fact retrieval and information verification. We make the HoVer dataset publicly available at https://hover-nlp.github.io

</details>

<details>

<summary>2020-11-16 07:00:35 - Retrieving and ranking short medical questions with two stages neural matching model</summary>

- *Xiang Li, Xinyu Fu, Zheng Lu, Ruibin Bai, Uwe Aickelin, Peiming Ge, Gong Liu*

- `2012.01254v1` - [abs](http://arxiv.org/abs/2012.01254v1) - [pdf](http://arxiv.org/pdf/2012.01254v1)

> Internet hospital is a rising business thanks to recent advances in mobile web technology and high demand of health care services. Online medical services become increasingly popular and active. According to US data in 2018, 80 percent of internet users have asked health-related questions online. Numerous data is generated in unprecedented speed and scale. Those representative questions and answers in medical fields are valuable raw data sources for medical data mining. Automated machine interpretation on those sheer amount of data gives an opportunity to assist doctors to answer frequently asked medical-related questions from the perspective of information retrieval and machine learning approaches. In this work, we propose a novel two-stage framework for the semantic matching of query-level medical questions.

</details>

<details>

<summary>2020-11-16 11:43:50 - Modeling the Music Genre Perception across Language-Bound Cultures</summary>

- *Elena V. Epure, Guillaume Salha, Manuel Moussallam, Romain Hennequin*

- `2010.06325v2` - [abs](http://arxiv.org/abs/2010.06325v2) - [pdf](http://arxiv.org/pdf/2010.06325v2)

> The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.

</details>

<details>

<summary>2020-11-16 13:31:33 - Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions</summary>

- *Muhammad N. ElNokrashy, Amr Hendy, Mohamed Abdelghaffar, Mohamed Afify, Ahmed Tawfik, Hany Hassan Awadalla*

- `2011.07933v1` - [abs](http://arxiv.org/abs/2011.07933v1) - [pdf](http://arxiv.org/pdf/2011.07933v1)

> This paper describes our submission to the WMT20 sentence filtering task. We combine scores from (1) a custom LASER built for each source language, (2) a classifier built to distinguish positive and negative pairs by semantic alignment, and (3) the original scores included in the task devkit. For the mBART finetuning setup, provided by the organizers, our method shows 7% and 5% relative improvement over baseline, in sacreBLEU score on the test set for Pashto and Khmer respectively.

</details>

<details>

<summary>2020-11-16 14:46:08 - Comparative Probing of Lexical Semantics Theories for Cognitive Plausibility and Technological Usefulness</summary>

- *António Branco, João Rodrigues, Małgorzata Salawa, Ruben Branco, Chakaveh Saedi*

- `2011.07997v1` - [abs](http://arxiv.org/abs/2011.07997v1) - [pdf](http://arxiv.org/pdf/2011.07997v1)

> Lexical semantics theories differ in advocating that the meaning of words is represented as an inference graph, a feature mapping or a vector space, thus raising the question: is it the case that one of these approaches is superior to the others in representing lexical semantics appropriately? Or in its non antagonistic counterpart: could there be a unified account of lexical semantics where these approaches seamlessly emerge as (partial) renderings of (different) aspects of a core semantic knowledge base?   In this paper, we contribute to these research questions with a number of experiments that systematically probe different lexical semantics theories for their levels of cognitive plausibility and of technological usefulness.   The empirical findings obtained from these experiments advance our insight on lexical semantics as the feature-based approach emerges as superior to the other ones, and arguably also move us closer to finding answers to the research questions above.

</details>

<details>

<summary>2020-11-16 15:27:38 - Fact Checking via Path Embedding and Aggregation</summary>

- *Giuseppe Pirrò*

- `2011.08028v1` - [abs](http://arxiv.org/abs/2011.08028v1) - [pdf](http://arxiv.org/pdf/2011.08028v1)

> Knowledge graphs (KGs) are a useful source of background knowledge to (dis)prove facts of the form (s, p, o). Finding paths between s and o is the cornerstone of several fact-checking approaches. While paths are useful to (visually) explain why a given fact is true or false, it is not completely clear how to identify paths that are most relevant to a fact, encode them and weigh their importance. The goal of this paper is to present the Fact Checking via path Embedding and Aggregation (FEA) system. FEA starts by carefully collecting the paths between s and o that are most semantically related to the domain of p. However, instead of directly working with this subset of all paths, it learns vectorized path representations, aggregates them according to different strategies, and use them to finally (dis)prove a fact. We conducted a large set of experiments on a variety of KGs and found that our hybrid solution brings some benefits in terms of performance.

</details>

<details>

<summary>2020-11-16 17:27:35 - Anonymization of labeled TOF-MRA images for brain vessel segmentation using generative adversarial networks</summary>

- *Tabea Kossen, Pooja Subramaniam, Vince I. Madai, Anja Hennemuth, Kristian Hildebrand, Adam Hilbert, Jan Sobesky, Michelle Livne, Ivana Galinovic, Ahmed A. Khalil, Jochen B. Fiebach, Dietmar Frey*

- `2009.04227v3` - [abs](http://arxiv.org/abs/2009.04227v3) - [pdf](http://arxiv.org/pdf/2009.04227v3)

> Anonymization and data sharing are crucial for privacy protection and acquisition of large datasets for medical image analysis. This is a big challenge, especially for neuroimaging. Here, the brain's unique structure allows for re-identification and thus requires non-conventional anonymization. Generative adversarial networks (GANs) have the potential to provide anonymous images while preserving predictive properties. Analyzing brain vessel segmentation, we trained 3 GANs on time-of-flight (TOF) magnetic resonance angiography (MRA) patches for image-label generation: 1) Deep convolutional GAN, 2) Wasserstein-GAN with gradient penalty (WGAN-GP) and 3) WGAN-GP with spectral normalization (WGAN-GP-SN). The generated image-labels from each GAN were used to train a U-net for segmentation and tested on real data. Moreover, we applied our synthetic patches using transfer learning on a second dataset. For an increasing number of up to 15 patients we evaluated the model performance on real data with and without pre-training. The performance for all models was assessed by the Dice Similarity Coefficient (DSC) and the 95th percentile of the Hausdorff Distance (95HD). Comparing the 3 GANs, the U-net trained on synthetic data generated by the WGAN-GP-SN showed the highest performance to predict vessels (DSC/95HD 0.82/28.97) benchmarked by the U-net trained on real data (0.89/26.61). The transfer learning approach showed superior performance for the same GAN compared to no pre-training, especially for one patient only (0.91/25.68 vs. 0.85/27.36). In this work, synthetic image-label pairs retained generalizable information and showed good performance for vessel segmentation. Besides, we showed that synthetic patches can be used in a transfer learning approach with independent data. This paves the way to overcome the challenges of scarce data and anonymization in medical imaging.

</details>

<details>

<summary>2020-11-16 17:57:08 - Word Rotator's Distance</summary>

- *Sho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki, Kentaro Inui*

- `2004.15003v3` - [abs](http://arxiv.org/abs/2004.15003v3) - [pdf](http://arxiv.org/pdf/2004.15003v3)

> A key principle in assessing textual similarity is measuring the degree of semantic overlap between two texts by considering the word alignment. Such alignment-based approaches are intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. To address this issue, we focus on and demonstrate the fact that the norm of word vectors is a good proxy for word importance, and their angle is a good proxy for word similarity. Alignment-based approaches do not distinguish them, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose a method that first decouples word vectors into their norm and direction, and then computes alignment-based similarity using earth mover's distance (i.e., optimal transport cost), which we refer to as word rotator's distance. Besides, we find how to grow the norm and direction of word vectors (vector converter), which is a new systematic approach derived from sentence-vector estimation methods. On several textual similarity datasets, the combination of these simple proposed methods outperformed not only alignment-based approaches but also strong baselines. The source code is available at https://github.com/eumesy/wrd

</details>

<details>

<summary>2020-11-16 19:48:06 - Semantic Drift in Multilingual Representations</summary>

- *Lisa Beinborn, Rochelle Choenni*

- `1904.10820v4` - [abs](http://arxiv.org/abs/1904.10820v4) - [pdf](http://arxiv.org/pdf/1904.10820v4)

> Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a methodology for comparing languages based on their organization of semantic concepts. We propose to conduct an adapted version of representational similarity analysis of a selected set of concepts in computational multilingual representations. Using this analysis method, we can reconstruct a phylogenetic tree that closely resembles those assumed by linguistic experts. These results indicate that multilingual distributional representations which are only trained on monolingual text and bilingual dictionaries preserve relations between languages without the need for any etymological information. In addition, we propose a measure to identify semantic drift between language families. We perform experiments on word-based and sentence-based multilingual models and provide both quantitative results and qualitative examples. Analyses of semantic drift in multilingual representations can serve two purposes: they can indicate unwanted characteristics of the computational models and they provide a quantitative means to study linguistic phenomena across languages. The code is available at https://github.com/beinborn/SemanticDrift.

</details>

<details>

<summary>2020-11-16 20:30:31 - A Probabilistic Approach in Historical Linguistics Word Order Change in Infinitival Clauses: from Latin to Old French</summary>

- *Olga Scrivner*

- `2011.08262v1` - [abs](http://arxiv.org/abs/2011.08262v1) - [pdf](http://arxiv.org/pdf/2011.08262v1)

> This research offers a new interdisciplinary approach to the field of Linguistics by using Computational Linguistics, NLP, Bayesian Statistics and Sociolinguistics methods. This thesis investigates word order change in infinitival clauses from Object-Verb (OV) to Verb-Object (VO) in the history of Latin and Old French. By applying a variationist approach, I examine a synchronic word order variation in each stage of language change, from which I infer the character, periodization and constraints of diachronic variation. I also show that in discourse-configurational languages, such as Latin and Early Old French, it is possible to identify pragmatically neutral contexts by using information structure annotation. I further argue that by mapping pragmatic categories into a syntactic structure, we can detect how word order change unfolds. For this investigation, the data are extracted from annotated corpora spanning several centuries of Latin and Old French and from additional resources created by using computational linguistic methods. The data are then further codified for various pragmatic, semantic, syntactic and sociolinguistic factors. This study also evaluates previous factors proposed to account for word order alternation and change. I show how information structure and syntactic constraints change over time and propose a method that allows researchers to differentiate a stable word order alternation from alternation indicating a change. Finally, I present a three-stage probabilistic model of word order change, which also conforms to traditional language change patterns.

</details>

<details>

<summary>2020-11-16 20:47:54 - It's a Thin Line Between Love and Hate: Using the Echo in Modeling Dynamics of Racist Online Communities</summary>

- *Eyal Arviv, Simo Hanouna, Oren Tsur*

- `2012.01133v1` - [abs](http://arxiv.org/abs/2012.01133v1) - [pdf](http://arxiv.org/pdf/2012.01133v1)

> The (((echo))) symbol -- triple parenthesis surrounding a name, made it to mainstream social networks in early 2016, with the intensification of the U.S. Presidential race. It was used by members of the alt-right, white supremacists and internet trolls to tag people of Jewish heritage -- a modern incarnation of the infamous yellow badge (Judenstern) used in Nazi-Germany. Tracking this trending meme, its meaning, and its function has proved elusive for its semantic ambiguity (e.g., a symbol for a virtual hug).   In this paper we report of the construction of an appropriate dataset allowing the reconstruction of networks of racist communities and the way they are embedded in the broader community. We combine natural language processing and structural network analysis to study communities promoting hate. In order to overcome dog-whistling and linguistic ambiguity, we propose a multi-modal neural architecture based on a BERT transformer and a BiLSTM network on the tweet level, while also taking into account the users ego-network and meta features. Our multi-modal neural architecture outperforms a set of strong baselines. We further show how the the use of language and network structure in tandem allows the detection of the leaders of the hate communities. We further study the ``intersectionality'' of hate and show that the antisemitic echo correlates with hate speech that targets other minority and protected groups. Finally, we analyze the role IRA trolls assumed in this network as part of the Russian interference campaign. Our findings allow a better understanding of recent manifestations of racism and the dynamics that facilitate it.

</details>

<details>

<summary>2020-11-16 21:16:49 - One Thousand and One Hours: Self-driving Motion Prediction Dataset</summary>

- *John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, Peter Ondruska*

- `2006.14480v2` - [abs](http://arxiv.org/abs/2006.14480v2) - [pdf](http://arxiv.org/pdf/2006.14480v2)

> Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation. The full dataset is available at http://level5.lyft.com/.

</details>

<details>

<summary>2020-11-16 21:31:28 - A Two-Phase Approach for Abstractive Podcast Summarization</summary>

- *Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, Ling Fan*

- `2011.08291v1` - [abs](http://arxiv.org/abs/2011.08291v1) - [pdf](http://arxiv.org/pdf/2011.08291v1)

> Podcast summarization is different from summarization of other data formats, such as news, patents, and scientific papers in that podcasts are often longer, conversational, colloquial, and full of sponsorship and advertising information, which imposes great challenges for existing models. In this paper, we focus on abstractive podcast summarization and propose a two-phase approach: sentence selection and seq2seq learning. Specifically, we first select important sentences from the noisy long podcast transcripts. The selection is based on sentence similarity to the reference to reduce the redundancy and the associated latent topics to preserve semantics. Then the selected sentences are fed into a pre-trained encoder-decoder framework for the summary generation. Our approach achieves promising results regarding both ROUGE-based measures and human evaluations.

</details>

<details>

<summary>2020-11-17 03:53:31 - Sub-clusters of Normal Data for Anomaly Detection</summary>

- *Gahye Lee, Seungkyu Lee*

- `2011.08408v1` - [abs](http://arxiv.org/abs/2011.08408v1) - [pdf](http://arxiv.org/pdf/2011.08408v1)

> Anomaly detection in data analysis is an interesting but still challenging research topic in real world applications. As the complexity of data dimension increases, it requires to understand the semantic contexts in its description for effective anomaly characterization. However, existing anomaly detection methods show limited performances with high dimensional data such as ImageNet. Existing studies have evaluated their performance on low dimensional, clean and well separated data set such as MNIST and CIFAR-10. In this paper, we study anomaly detection with high dimensional and complex normal data. Our observation is that, in general, anomaly data is defined by semantically explainable features which are able to be used in defining semantic sub-clusters of normal data as well. We hypothesize that if there exists reasonably good feature space semantically separating sub-clusters of given normal data, unseen anomaly also can be well distinguished in the space from the normal data. We propose to perform semantic clustering on given normal data and train a classifier to learn the discriminative feature space where anomaly detection is finally performed. Based on our careful and extensive experimental evaluations with MNIST, CIFAR-10, and ImageNet with various combinations of normal and anomaly data, we show that our anomaly detection scheme outperforms state of the art methods especially with high dimensional real world images.

</details>

<details>

<summary>2020-11-17 10:07:55 - An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing</summary>

- *Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, Hinrich Schütze*

- `1904.09447v4` - [abs](http://arxiv.org/abs/1904.09447v4) - [pdf](http://arxiv.org/pdf/1904.09447v4)

> Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well in different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text$\leftrightarrow$graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.

</details>

<details>

<summary>2020-11-17 16:43:56 - Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension</summary>

- *Adyasha Maharana, Mohit Bansal*

- `2004.06076v4` - [abs](http://arxiv.org/abs/2004.06076v4) - [pdf](http://arxiv.org/pdf/2004.06076v4)

> Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves robustness to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.

</details>

<details>

<summary>2020-11-17 19:10:12 - Ginkgo -- A Math Library designed for Platform Portability</summary>

- *Terry Cojean, Yu-Hsiang "Mike" Tsai, Hartwig Anzt*

- `2011.08879v1` - [abs](http://arxiv.org/abs/2011.08879v1) - [pdf](http://arxiv.org/pdf/2011.08879v1)

> The first associations to software sustainability might be the existence of a continuous integration (CI) framework; the existence of a testing framework composed of unit tests, integration tests, and end-to-end tests; and also the existence of software documentation. However, when asking what is a common deathblow for a scientific software product, it is often the lack of platform and performance portability. Against this background, we designed the Ginkgo library with the primary focus on platform portability and the ability to not only port to new hardware architectures, but also achieve good performance. In this paper we present the Ginkgo library design, radically separating algorithms from hardware-specific kernels forming the distinct hardware executors, and report our experience when adding execution backends for NVIDIA, AMD, and Intel GPUs. We also comment on the different levels of performance portability, and the performance we achieved on the distinct hardware backends.

</details>

<details>

<summary>2020-11-17 21:21:37 - Exploring Neural Entity Representations for Semantic Information</summary>

- *Andrew Runge, Eduard Hovy*

- `2011.08951v1` - [abs](http://arxiv.org/abs/2011.08951v1) - [pdf](http://arxiv.org/pdf/2011.08951v1)

> Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task structure, while probing task evaluations often look at only a few attributes and models. We address both of these issues by evaluating a diverse set of eight neural entity embedding methods on a set of simple probing tasks, demonstrating which methods are able to remember words used to describe entities, learn type, relationship and factual information, and identify how frequently an entity is mentioned. We also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets.

</details>

<details>

<summary>2020-11-18 02:03:28 - Weakly-Supervised Reinforcement Learning for Controllable Behavior</summary>

- *Lisa Lee, Benjamin Eysenbach, Ruslan Salakhutdinov, Shixiang Shane Gu, Chelsea Finn*

- `2004.02860v2` - [abs](http://arxiv.org/abs/2004.02860v2) - [pdf](http://arxiv.org/pdf/2004.02860v2)

> Reinforcement learning (RL) is a powerful framework for learning to take actions to solve tasks. However, in many settings, an agent must winnow down the inconceivably large space of all possible tasks to the single task that it is currently being asked to solve. Can we instead constrain the space of tasks to those that are semantically meaningful? In this work, we introduce a framework for using weak supervision to automatically disentangle this semantically meaningful subspace of tasks from the enormous space of nonsensical "chaff" tasks. We show that this learned subspace enables efficient exploration and provides a representation that captures distance between states. On a variety of challenging, vision-based continuous control problems, our approach leads to substantial performance gains, particularly as the complexity of the environment grows.

</details>

<details>

<summary>2020-11-18 02:18:04 - Sequence-Level Mixed Sample Data Augmentation</summary>

- *Demi Guo, Yoon Kim, Alexander M. Rush*

- `2011.09039v1` - [abs](http://arxiv.org/abs/2011.09039v1) - [pdf](http://arxiv.org/pdf/2011.09039v1)

> Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.

</details>

<details>

<summary>2020-11-18 06:33:20 - Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation</summary>

- *Gaurav Kumar Nayak, Konda Reddy Mopuri, Anirban Chakraborty*

- `2011.09113v1` - [abs](http://arxiv.org/abs/2011.09113v1) - [pdf](http://arxiv.org/pdf/2011.09113v1)

> Knowledge Distillation is an effective method to transfer the learning across deep neural networks. Typically, the dataset originally used for training the Teacher model is chosen as the "Transfer Set" to conduct the knowledge transfer to the Student. However, this original training data may not always be freely available due to privacy or sensitivity concerns. In such scenarios, existing approaches either iteratively compose a synthetic set representative of the original training dataset, one sample at a time or learn a generative model to compose such a transfer set. However, both these approaches involve complex optimization (GAN training or several backpropagation steps to synthesize one sample) and are often computationally expensive. In this paper, as a simple alternative, we investigate the effectiveness of "arbitrary transfer sets" such as random noise, publicly available synthetic, and natural datasets, all of which are completely unrelated to the original training dataset in terms of their visual or semantic contents. Through extensive experiments on multiple benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover and validate surprising effectiveness of using arbitrary data to conduct knowledge distillation when this dataset is "target-class balanced". We believe that this important observation can potentially lead to designing baselines for the data-free knowledge distillation task.

</details>

<details>

<summary>2020-11-18 12:09:08 - Efficient image retrieval using multi neural hash codes and bloom filters</summary>

- *Sourin Chakrabarti*

- `2011.03234v2` - [abs](http://arxiv.org/abs/2011.03234v2) - [pdf](http://arxiv.org/pdf/2011.03234v2)

> This paper aims to deliver an efficient and modified approach for image retrieval using multiple neural hash codes and limiting the number of queries using bloom filters by identifying false positives beforehand. Traditional approaches involving neural networks for image retrieval tasks tend to use higher layers for feature extraction. But it has been seen that the activations of lower layers have proven to be more effective in a number of scenarios. In our approach, we have leveraged the use of local deep convolutional neural networks which combines the powers of both the features of lower and higher layers for creating feature maps which are then compressed using PCA and fed to a bloom filter after binary sequencing using a modified multi k-means approach. The feature maps obtained are further used in the image retrieval process in a hierarchical coarse-to-fine manner by first comparing the images in the higher layers for semantically similar images and then gradually moving towards the lower layers searching for structural similarities. While searching, the neural hashes for the query image are again calculated and queried in the bloom filter which tells us whether the query image is absent in the set or maybe present. If the bloom filter doesn't necessarily rule out the query, then it goes into the image retrieval process. This approach can be particularly helpful in cases where the image store is distributed since the approach supports parallel querying.

</details>

<details>

<summary>2020-11-18 14:31:17 - First-Order Rewritability of Frontier-Guarded Ontology-Mediated Queries</summary>

- *Pablo Barcelo, Gerald Berger, Carsten Lutz, Andreas Pieris*

- `2011.09314v1` - [abs](http://arxiv.org/abs/2011.09314v1) - [pdf](http://arxiv.org/pdf/2011.09314v1)

> We focus on ontology-mediated queries (OMQs) based on (frontier-)guarded existential rules and (unions of) conjunctive queries, and we investigate the problem of FO-rewritability, i.e., whether an OMQ can be rewritten as a first-order query. We adopt two different approaches. The first approach employs standard two-way alternating parity tree automata. Although it does not lead to a tight complexity bound, it provides a transparent solution based on widely known tools. The second approach relies on a sophisticated automata model, known as cost automata. This allows us to show that our problem is 2ExpTime-complete. In both approaches, we provide semantic characterizations of FO-rewritability that are of independent interest.

</details>

<details>

<summary>2020-11-18 15:30:08 - SRLGRN: Semantic Role Labeling Graph Reasoning Network</summary>

- *Chen Zheng, Parisa Kordjamshidi*

- `2010.03604v2` - [abs](http://arxiv.org/abs/2010.03604v2) - [pdf](http://arxiv.org/pdf/2010.03604v2)

> This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.

</details>

<details>

<summary>2020-11-18 15:40:13 - Generic Ontology Design Patterns: Roles and Change over Time</summary>

- *Bernd Krieg-Brückner, Till Mossakowski, Mihai Codescu*

- `2011.09353v1` - [abs](http://arxiv.org/abs/2011.09353v1) - [pdf](http://arxiv.org/pdf/2011.09353v1)

> In this chapter we propose Generic Ontology Design Patterns, GODPs, as a methodology for representing and instantiating ontology design patterns in a way that is adaptable, and allows domain experts (and other users) to safely use them without cluttering their ontologies.

</details>

<details>

<summary>2020-11-18 16:47:30 - Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven Acoustic Embedding Selection</summary>

- *Shubhi Tyagi, Marco Nicolis, Jonas Rohnke, Thomas Drugman, Jaime Lorenzo-Trueba*

- `1912.00955v3` - [abs](http://arxiv.org/abs/1912.00955v3) - [pdf](http://arxiv.org/pdf/1912.00955v3)

> Recent advances in Text-to-Speech (TTS) have improved quality and naturalness to near-human capabilities when considering isolated sentences. But something which is still lacking in order to achieve human-like communication is the dynamic variations and adaptability of human speech. This work attempts to solve the problem of achieving a more dynamic and natural intonation in TTS systems, particularly for stylistic speech such as the newscaster speaking style. We propose a novel embedding selection approach which exploits linguistic information, leveraging the speech variability present in the training dataset. We analyze the contribution of both semantic and syntactic features. Our results show that the approach improves the prosody and naturalness for complex utterances as well as in Long Form Reading (LFR).

</details>

<details>

<summary>2020-11-18 17:21:51 - Topology of Word Embeddings: Singularities Reflect Polysemy</summary>

- *Alexander Jakubowski, Milica Gašić, Marcus Zibrowius*

- `2011.09413v1` - [abs](http://arxiv.org/abs/2011.09413v1) - [pdf](http://arxiv.org/pdf/2011.09413v1)

> The manifold hypothesis suggests that word vectors live on a submanifold within their ambient vector space. We argue that we should, more accurately, expect them to live on a pinched manifold: a singular quotient of a manifold obtained by identifying some of its points. The identified, singular points correspond to polysemous words, i.e. words with multiple meanings. Our point of view suggests that monosemous and polysemous words can be distinguished based on the topology of their neighbourhoods. We present two kinds of empirical evidence to support this point of view: (1) We introduce a topological measure of polysemy based on persistent homology that correlates well with the actual number of meanings of a word. (2) We propose a simple, topologically motivated solution to the SemEval-2010 task on Word Sense Induction & Disambiguation that produces competitive results.

</details>

<details>

<summary>2020-11-18 19:17:15 - GRAPHSPY: Fused Program Semantic-Level Embedding via Graph Neural Networks for Dead Store Detection</summary>

- *Yixin Guo, Pengcheng Li, Yingwei Luo, Xiaolin Wang, Zhenlin Wang*

- `2011.09501v1` - [abs](http://arxiv.org/abs/2011.09501v1) - [pdf](http://arxiv.org/pdf/2011.09501v1)

> Production software oftentimes suffers from the issue of performance inefficiencies caused by inappropriate use of data structures, programming abstractions, and conservative compiler optimizations. It is desirable to avoid unnecessary memory operations. However, existing works often use a whole-program fine-grained monitoring method with incredibly high overhead. To this end, we propose a learning-aided approach to identify unnecessary memory operations intelligently with low overhead. By applying several prevalent graph neural network models to extract program semantics with respect to program structure, execution order and dynamic states, we present a novel, hybrid program embedding approach so that to derive unnecessary memory operations through the embedding. We train our model with tens of thousands of samples acquired from a set of real-world benchmarks. Results show that our model achieves 90% of accuracy and incurs only around a half of time overhead of the state-of-art tool.

</details>

<details>

<summary>2020-11-18 21:47:28 - Solving inverse-PDE problems with physics-aware neural networks</summary>

- *Samira Pakravan, Pouria A. Mistani, Miguel Angel Aragon-Calvo, Frederic Gibou*

- `2001.03608v3` - [abs](http://arxiv.org/abs/2001.03608v3) - [pdf](http://arxiv.org/pdf/2001.03608v3)

> We propose a novel composite framework to find unknown fields in the context of inverse problems for partial differential equations (PDEs). We blend the high expressibility of deep neural networks as universal function estimators with the accuracy and reliability of existing numerical algorithms for partial differential equations as custom layers in semantic autoencoders. Our design brings together techniques of computational mathematics, machine learning and pattern recognition under one umbrella to incorporate domain-specific knowledge and physical constraints to discover the underlying hidden fields. The network is explicitly aware of the governing physics through a hard-coded PDE solver layer in contrast to most existing methods that incorporate the governing equations in the loss function or rely on trainable convolutional layers to discover proper discretizations from data. This subsequently focuses the computational load to only the discovery of the hidden fields and therefore is more data efficient. We call this architecture Blended inverse-PDE networks (hereby dubbed BiPDE networks) and demonstrate its applicability for recovering the variable diffusion coefficient in Poisson problems in one and two spatial dimensions, as well as the diffusion coefficient in the time-dependent and nonlinear Burgers' equation in one dimension. We also show that this approach is robust to noise.

</details>

<details>

<summary>2020-11-18 21:53:07 - Semantic CPPS in Industry 4.0</summary>

- *Giuseppe Fenza, Mariacristina Gallo, Vincenzo Loia, Domenico Marinoand Francesco Orciuoli, Alberto Volpe*

- `2011.11395v1` - [abs](http://arxiv.org/abs/2011.11395v1) - [pdf](http://arxiv.org/pdf/2011.11395v1)

> Cyber-Physical Systems (CPS) play a crucial role in the era of the 4thIndustrial Revolution. Recently, the application of the CPS to industrial manufacturing leads to a specialization of them referred as Cyber-Physical Production Systems (CPPS). Among other challenges, CPS and CPPS should be able to address interoperability issues, since one of their intrinsic requirement is the capability to interface and cooperate with other systems. On the other hand, to fully realize theIndustry 4.0 vision, it is required to address horizontal, vertical, and end-to-end integration enabling a complete awareness through the entire supply chain. In this context, Semantic Web standards and technologies may have a promising role to represent manufacturing knowledge in a machine-interpretable way for enabling communications among heterogeneous Industrial assets. This paper proposes an integration of Semantic Web models available at state of the art for implementing a5C architecture mainly targeted to collect and process semantic data stream in a way that would unlock the potentiality of data yield in a smart manufacturing environment. The analysis of key industrial ontologies and semantic technologies allows us to instantiate an example scenario for monitoring Overall Equipment Effectiveness(OEE). The solution uses the SOSA ontology for representing the semantic datastream. Then, C-SPARQL queries are defined for periodically carrying out useful KPIs to address the proposed aim.

</details>

<details>

<summary>2020-11-18 22:33:09 - Predicting metrical patterns in Spanish poetry with language models</summary>

- *Javier de la Rosa, Salvador Ros, Elena González-Blanco*

- `2011.09567v1` - [abs](http://arxiv.org/abs/2011.09567v1) - [pdf](http://arxiv.org/pdf/2011.09567v1)

> In this paper, we compare automated metrical pattern identification systems available for Spanish against extensive experiments done by fine-tuning language models trained on the same task. Despite being initially conceived as a model suitable for semantic tasks, our results suggest that BERT-based models retain enough structural information to perform reasonably well for Spanish scansion.

</details>

<details>

<summary>2020-11-18 23:21:34 - Self-supervised transfer learning of physiological representations from free-living wearable data</summary>

- *Dimitris Spathis, Ignacio Perez-Pozuelo, Soren Brage, Nicholas J. Wareham, Cecilia Mascolo*

- `2011.12121v1` - [abs](http://arxiv.org/abs/2011.12121v1) - [pdf](http://arxiv.org/pdf/2011.12121v1)

> Wearable devices such as smartwatches are becoming increasingly popular tools for objectively monitoring physical activity in free-living conditions. To date, research has primarily focused on the purely supervised task of human activity recognition, demonstrating limited success in inferring high-level health outcomes from low-level signals. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship. In addition, we propose a custom quantile loss function that accounts for the long-tailed HR distribution present in the general population.   We evaluate our model in the largest free-living combined-sensing dataset (comprising >280k hours of wrist accelerometer & wearable ECG data). Our contributions are two-fold: i) the pre-training task creates a model that can accurately forecast HR based only on cheap activity sensors, and ii) we leverage the information captured through this task by proposing a simple method to aggregate the learnt latent representations (embeddings) from the window-level to user-level. Notably, we show that the embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring.

</details>

<details>

<summary>2020-11-19 04:51:13 - Open-sourced Dataset Protection via Backdoor Watermarking</summary>

- *Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, Shu-Tao Xia*

- `2010.05821v3` - [abs](http://arxiv.org/abs/2010.05821v3) - [pdf](http://arxiv.org/pdf/2010.05821v3)

> The rapid development of deep learning has benefited from the release of some high-quality open-sourced datasets ($e.g.$, ImageNet), which allows researchers to easily verify the effectiveness of their algorithms. Almost all existing open-sourced datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes, whereas there is still no good way to protect them. In this paper, we propose a \emph{backdoor embedding based dataset watermarking} method to protect an open-sourced image-classification dataset by verifying whether it is used for training a third-party model. Specifically, the proposed method contains two main processes, including \emph{dataset watermarking} and \emph{dataset verification}. We adopt classical poisoning-based backdoor attacks ($e.g.$, BadNets) for dataset watermarking, ie, generating some poisoned samples by adding a certain trigger ($e.g.$, a local patch) onto some benign samples, labeled with a pre-defined target class. Based on the proposed backdoor-based watermarking, we use a hypothesis test guided method for dataset verification based on the posterior probability generated by the suspicious third-party model of the benign samples and their correspondingly watermarked samples ($i.e.$, images with trigger) on the target class. Experiments on some benchmark datasets are conducted, which verify the effectiveness of the proposed method.

</details>

<details>

<summary>2020-11-19 05:19:46 - Relation Extraction with Contextualized Relation Embedding (CRE)</summary>

- *Xiaoyu Chen, Rohan Badlani*

- `2011.09658v1` - [abs](http://arxiv.org/abs/2011.09658v1) - [pdf](http://arxiv.org/pdf/2011.09658v1)

> Relation extraction is the task of identifying relation instance between two entities given a corpus whereas Knowledge base modeling is the task of representing a knowledge base, in terms of relations between entities. This paper proposes an architecture for the relation extraction task that integrates semantic information with knowledge base modeling in a novel manner. Existing approaches for relation extraction either do not utilize knowledge base modelling or use separately trained KB models for the RE task. We present a model architecture that internalizes KB modeling in relation extraction. This model applies a novel approach to encode sentences into contextualized relation embeddings, which can then be used together with parameterized entity embeddings to score relation instances. The proposed CRE model achieves state of the art performance on datasets derived from The New York Times Annotated Corpus and FreeBase. The source code has been made available.

</details>

<details>

<summary>2020-11-19 07:50:13 - SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy Learning</summary>

- *Albert Zhao, Tong He, Yitao Liang, Haibin Huang, Guy Van den Broeck, Stefano Soatto*

- `1912.02973v2` - [abs](http://arxiv.org/abs/1912.02973v2) - [pdf](http://arxiv.org/pdf/1912.02973v2)

> We describe a policy learning approach to map visual inputs to driving controls conditioned on turning command that leverages side tasks on semantics and object affordances via a learned representation trained for driving. To learn this representation, we train a squeeze network to drive using annotations for the side task as input. This representation encodes the driving-relevant information associated with the side task while ideally throwing out side task-relevant but driving-irrelevant nuisances. We then train a mimic network to drive using only images as input and use the squeeze network's latent representation to supervise the mimic network via a mimicking loss. Notably, we do not aim to achieve the side task nor to learn features for it; instead, we aim to learn, via the mimicking loss, a representation of the side task annotations directly useful for driving. We test our approach using the CARLA simulator. In addition, we introduce a more challenging but realistic evaluation protocol that considers a run that reaches the destination successful only if it does not violate common traffic rules. A video summarizing this work is available at https://youtu.be/ipKAMzmJpMs , and code is available at https://github.com/twsq/sam-driving .

</details>

<details>

<summary>2020-11-19 09:29:51 - Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT</summary>

- *Ruifeng Yuan, Zili Wang, Wenjie Li*

- `2011.09739v1` - [abs](http://arxiv.org/abs/2011.09739v1) - [pdf](http://arxiv.org/pdf/2011.09739v1)

> Most current extractive summarization models generate summaries by selecting salient sentences. However, one of the problems with sentence-level extractive summarization is that there exists a gap between the human-written gold summary and the oracle sentence labels. In this paper, we propose to extract fact-level semantic units for better extractive summarization. We also introduce a hierarchical structure, which incorporates the multi-level of granularities of the textual information into the model. In addition, we incorporate our model with BERT using a hierarchical graph mask. This allows us to combine BERT's ability in natural language understanding and the structural information without increasing the scale of the model. Experiments on the CNN/DaliyMail dataset show that our model achieves state-of-the-art results.

</details>

<details>

<summary>2020-11-19 10:57:43 - Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery</summary>

- *Zhuo Zheng, Yanfei Zhong, Junjue Wang, Ailong Ma*

- `2011.09766v1` - [abs](http://arxiv.org/abs/2011.09766v1) - [pdf](http://arxiv.org/pdf/2011.09766v1)

> Geospatial object segmentation, as a particular semantic segmentation task, always faces with larger-scale variation, larger intra-class variance of background, and foreground-background imbalance in the high spatial resolution (HSR) remote sensing imagery. However, general semantic segmentation methods mainly focus on scale variation in the natural scene, with inadequate consideration of the other two problems that usually happen in the large area earth observation scene. In this paper, we argue that the problems lie on the lack of foreground modeling and propose a foreground-aware relation network (FarSeg) from the perspectives of relation-based and optimization-based foreground modeling, to alleviate the above two problems. From perspective of relation, FarSeg enhances the discrimination of foreground features via foreground-correlated contexts associated by learning foreground-scene relation. Meanwhile, from perspective of optimization, a foreground-aware optimization is proposed to focus on foreground examples and hard examples of background during training for a balanced optimization. The experimental results obtained using a large scale dataset suggest that the proposed method is superior to the state-of-the-art general semantic segmentation methods and achieves a better trade-off between speed and accuracy. Code has been made available at: \url{https://github.com/Z-Zheng/FarSeg}.

</details>

<details>

<summary>2020-11-19 13:02:36 - P2ExNet: Patch-based Prototype Explanation Network</summary>

- *Dominique Mercier, Andreas Dengel, Sheraz Ahmed*

- `2005.02006v2` - [abs](http://arxiv.org/abs/2005.02006v2) - [pdf](http://arxiv.org/pdf/2005.02006v2)

> Deep learning methods have shown great success in several domains as they process a large amount of data efficiently, capable of solving complex classification, forecast, segmentation, and other tasks. However, they come with the inherent drawback of inexplicability limiting their applicability and trustworthiness. Although there exists work addressing this perspective, most of the existing approaches are limited to the image modality due to the intuitive and prominent concepts. Conversely, the concepts in the time-series domain are more complex and non-comprehensive but these and an explanation for the network decision are pivotal in critical domains like medical, financial, or industry. Addressing the need for an explainable approach, we propose a novel interpretable network scheme, designed to inherently use an explainable reasoning process inspired by the human cognition without the need of additional post-hoc explainability methods. Therefore, class-specific patches are used as they cover local concepts relevant to the classification to reveal similarities with samples of the same class. In addition, we introduce a novel loss concerning interpretability and accuracy that constraints P2ExNet to provide viable explanations of the data including relevant patches, their position, class similarities, and comparison methods without compromising accuracy. Analysis of the results on eight publicly available time-series datasets reveals that P2ExNet reaches comparable performance when compared to its counterparts while inherently providing understandable and traceable decisions.

</details>

<details>

<summary>2020-11-19 14:30:43 - Linking OpenStreetMap with Knowledge Graphs -- Link Discovery for Schema-Agnostic Volunteered Geographic Information</summary>

- *Nicolas Tempelmeier, Elena Demidova*

- `2011.05841v3` - [abs](http://arxiv.org/abs/2011.05841v3) - [pdf](http://arxiv.org/pdf/2011.05841v3)

> Representations of geographic entities captured in popular knowledge graphs such as Wikidata and DBpedia are often incomplete. OpenStreetMap (OSM) is a rich source of openly available, volunteered geographic information that has a high potential to complement these representations. However, identity links between the knowledge graph entities and OSM nodes are still rare. The problem of link discovery in these settings is particularly challenging due to the lack of a strict schema and heterogeneity of the user-defined node representations in OSM. In this article, we propose OSM2KG - a novel link discovery approach to predict identity links between OSM nodes and geographic entities in a knowledge graph. The core of the OSM2KG approach is a novel latent, compact representation of OSM nodes that captures semantic node similarity in an embedding. OSM2KG adopts this latent representation to train a supervised model for link prediction and utilises existing links between OSM and knowledge graphs for training. Our experiments conducted on several OSM datasets, as well as the Wikidata and DBpedia knowledge graphs, demonstrate that OSM2KG can reliably discover identity links. OSM2KG achieves an F1 score of 92.05% on Wikidata and of 94.17% on DBpedia on average, which corresponds to a 21.82 percentage points increase in F1 score on Wikidata compared to the best performing baselines.

</details>

<details>

<summary>2020-11-19 16:09:14 - Using Text to Teach Image Retrieval</summary>

- *Haoyu Dong, Ze Wang, Qiang Qiu, Guillermo Sapiro*

- `2011.09928v1` - [abs](http://arxiv.org/abs/2011.09928v1) - [pdf](http://arxiv.org/pdf/2011.09928v1)

> Image retrieval relies heavily on the quality of the data modeling and the distance measurement in the feature space. Building on the concept of image manifold, we first propose to represent the feature space of images, learned via neural networks, as a graph. Neighborhoods in the feature space are now defined by the geodesic distance between images, represented as graph vertices or manifold samples. When limited images are available, this manifold is sparsely sampled, making the geodesic computation and the corresponding retrieval harder. To address this, we augment the manifold samples with geometrically aligned text, thereby using a plethora of sentences to teach us about images. In addition to extensive results on standard datasets illustrating the power of text to help in image retrieval, a new public dataset based on CLEVR is introduced to quantify the semantic similarity between visual data and text data. The experimental results show that the joint embedding manifold is a robust representation, allowing it to be a better basis to perform image retrieval given only an image and a textual instruction on the desired modifications over the image

</details>

<details>

<summary>2020-11-19 17:21:10 - A Bilingual Generative Transformer for Semantic Sentence Embedding</summary>

- *John Wieting, Graham Neubig, Taylor Berg-Kirkpatrick*

- `1911.03895v2` - [abs](http://arxiv.org/abs/1911.03895v2) - [pdf](http://arxiv.org/pdf/1911.03895v2)

> Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks -- contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.

</details>

<details>

<summary>2020-11-19 20:31:05 - Logically Consistent Loss for Visual Question Answering</summary>

- *Anh-Cat Le-Ngo, Truyen Tran, Santu Rana, Sunil Gupta, Svetha Venkatesh*

- `2011.10094v1` - [abs](http://arxiv.org/abs/2011.10094v1) - [pdf](http://arxiv.org/pdf/2011.10094v1)

> Given an image, a back-ground knowledge, and a set of questions about an object, human learners answer the questions very consistently regardless of question forms and semantic tasks. The current advancement in neural-network based Visual Question Answering (VQA), despite their impressive performance, cannot ensure such consistency due to identically distribution (i.i.d.) assumption. We propose a new model-agnostic logic constraint to tackle this issue by formulating a logically consistent loss in the multi-task learning framework as well as a data organisation called family-batch and hybrid-batch. To demonstrate usefulness of this proposal, we train and evaluate MAC-net based VQA machines with and without the proposed logically consistent loss and the proposed data organization. The experiments confirm that the proposed loss formulae and introduction of hybrid-batch leads to more consistency as well as better performance. Though the proposed approach is tested with MAC-net, it can be utilised in any other QA methods whenever the logical consistency between answers exist.

</details>

<details>

<summary>2020-11-20 00:10:51 - Visual Transformers: Token-based Image Representation and Processing for Computer Vision</summary>

- *Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda*

- `2006.03677v4` - [abs](http://arxiv.org/abs/2006.03677v4) - [pdf](http://arxiv.org/pdf/2006.03677v4)

> Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.

</details>

<details>

<summary>2020-11-20 12:06:46 - Towards Abstract Relational Learning in Human Robot Interaction</summary>

- *Mohamadreza Faridghasemnia, Daniele Nardi, Alessandro Saffiotti*

- `2011.10364v1` - [abs](http://arxiv.org/abs/2011.10364v1) - [pdf](http://arxiv.org/pdf/2011.10364v1)

> Humans have a rich representation of the entities in their environment. Entities are described by their attributes, and entities that share attributes are often semantically related. For example, if two books have "Natural Language Processing" as the value of their `title' attribute, we can expect that their `topic' attribute will also be equal, namely, "NLP". Humans tend to generalize such observations, and infer sufficient conditions under which the `topic' attribute of any entity is "NLP". If robots need to interact successfully with humans, they need to represent entities, attributes, and generalizations in a similar way. This ends in a contextualized cognitive agent that can adapt its understanding, where context provides sufficient conditions for a correct understanding. In this work, we address the problem of how to obtain these representations through human-robot interaction. We integrate visual perception and natural language input to incrementally build a semantic model of the world, and then use inductive reasoning to infer logical rules that capture generic semantic relations, true in this model. These relations can be used to enrich the human-robot interaction, to populate a knowledge base with inferred facts, or to remove uncertainty in the robot's sensory inputs.

</details>

<details>

<summary>2020-11-20 15:29:23 - Bridging Scene Understanding and Task Execution with Flexible Simulation Environments</summary>

- *Zachary Ravichandran, J. Daniel Griffith, Benjamin Smith, Costas Frost*

- `2011.10452v1` - [abs](http://arxiv.org/abs/2011.10452v1) - [pdf](http://arxiv.org/pdf/2011.10452v1)

> Significant progress has been made in scene understanding which seeks to build 3D, metric and object-oriented representations of the world. Concurrently, reinforcement learning has made impressive strides largely enabled by advances in simulation. Comparatively, there has been less focus in simulation for perception algorithms. Simulation is becoming increasingly vital as sophisticated perception approaches such as metric-semantic mapping or 3D dynamic scene graph generation require precise 3D, 2D, and inertial information in an interactive environment. To that end, we present TESSE (Task Execution with Semantic Segmentation Environments), an open source simulator for developing scene understanding and task execution algorithms. TESSE has been used to develop state-of-the-art solutions for metric-semantic mapping and 3D dynamic scene graph generation. Additionally, TESSE served as the platform for the GOSEEK Challenge at the International Conference of Robotics and Automation (ICRA) 2020, an object search competition with an emphasis on reinforcement learning. Code for TESSE is available at https://github.com/MIT-TESSE.

</details>

<details>

<summary>2020-11-20 16:31:46 - First Steps: Latent-Space Control with Semantic Constraints for Quadruped Locomotion</summary>

- *Alexander L. Mitchell, Martin Engelcke, Oiwi Parker Jones, David Surovik, Siddhant Gangapurwala, Oliwier Melon, Ioannis Havoutis, Ingmar Posner*

- `2007.01520v2` - [abs](http://arxiv.org/abs/2007.01520v2) - [pdf](http://arxiv.org/pdf/2007.01520v2)

> Traditional approaches to quadruped control frequently employ simplified, hand-derived models. This significantly reduces the capability of the robot since its effective kinematic range is curtailed. In addition, kinodynamic constraints are often non-differentiable and difficult to implement in an optimisation approach. In this work, these challenges are addressed by framing quadruped control as optimisation in a structured latent space. A deep generative model captures a statistical representation of feasible joint configurations, whilst complex dynamic and terminal constraints are expressed via high-level, semantic indicators and represented by learned classifiers operating upon the latent space. As a consequence, complex constraints are rendered differentiable and evaluated an order of magnitude faster than analytical approaches. We validate the feasibility of locomotion trajectories optimised using our approach both in simulation and on a real-world ANYmal quadruped. Our results demonstrate that this approach is capable of generating smooth and realisable trajectories. To the best of our knowledge, this is the first time latent space control has been successfully applied to a complex, real robot platform.

</details>

<details>

<summary>2020-11-20 19:19:48 - ATSal: An Attention Based Architecture for Saliency Prediction in 360 Videos</summary>

- *Yasser Dahou, Marouane Tliba, Kevin McGuinness, Noel O'Connor*

- `2011.10600v1` - [abs](http://arxiv.org/abs/2011.10600v1) - [pdf](http://arxiv.org/pdf/2011.10600v1)

> The spherical domain representation of 360 video/image presents many challenges related to the storage, processing, transmission and rendering of omnidirectional videos (ODV). Models of human visual attention can be used so that only a single viewport is rendered at a time, which is important when developing systems that allow users to explore ODV with head mounted displays (HMD). Accordingly, researchers have proposed various saliency models for 360 video/images. This paper proposes ATSal, a novel attention based (head-eye) saliency model for 360\degree videos. The attention mechanism explicitly encodes global static visual attention allowing expert models to focus on learning the saliency on local patches throughout consecutive frames. We compare the proposed approach to other state-of-the-art saliency models on two datasets: Salient360! and VR-EyeTracking. Experimental results on over 80 ODV videos (75K+ frames) show that the proposed method outperforms the existing state-of-the-art.

</details>

<details>

<summary>2020-11-21 00:45:25 - MacLeR: Machine Learning-based Run-Time Hardware Trojan Detection in Resource-Constrained IoT Edge Devices</summary>

- *Faiq Khalid, Syed Rafay Hasan, Sara Zia, Osman Hasan, Falah Awwad, Muhammad Shafique*

- `2011.11632v1` - [abs](http://arxiv.org/abs/2011.11632v1) - [pdf](http://arxiv.org/pdf/2011.11632v1)

> Traditional learning-based approaches for run-time Hardware Trojan detection require complex and expensive on-chip data acquisition frameworks and thus incur high area and power overhead. To address these challenges, we propose to leverage the power correlation between the executing instructions of a microprocessor to establish a machine learning-based run-time Hardware Trojan (HT) detection framework, called MacLeR. To reduce the overhead of data acquisition, we propose a single power-port current acquisition block using current sensors in time-division multiplexing, which increases accuracy while incurring reduced area overhead. We have implemented a practical solution by analyzing multiple HT benchmarks inserted in the RTL of a system-on-chip (SoC) consisting of four LEON3 processors integrated with other IPs like vga_lcd, RSA, AES, Ethernet, and memory controllers. Our experimental results show that compared to state-of-the-art HT detection techniques, MacLeR achieves 10\% better HT detection accuracy (i.e., 96.256%) while incurring a 7x reduction in area and power overhead (i.e., 0.025% of the area of the SoC and <0.07% of the power of the SoC). In addition, we also analyze the impact of process variation and aging on the extracted power profiles and the HT detection accuracy of MacLeR. Our analysis shows that variations in fine-grained power profiles due to the HTs are significantly higher compared to the variations in fine-grained power profiles caused by the process variations (PV) and aging effects. Moreover, our analysis demonstrates that, on average, the HT detection accuracy drop in MacLeR is less than 1% and 9% when considering only PV and PV with worst-case aging, respectively, which is ~10x less than in the case of the state-of-the-art ML-based HT detection technique.

</details>

<details>

<summary>2020-11-21 03:29:14 - Deep Data Flow Analysis</summary>

- *Chris Cummins, Hugh Leather, Zacharias Fisches, Tal Ben-Nun, Torsten Hoefler, Michael O'Boyle*

- `2012.01470v1` - [abs](http://arxiv.org/abs/2012.01470v1) - [pdf](http://arxiv.org/pdf/2012.01470v1)

> Compiler architects increasingly look to machine learning when building heuristics for compiler optimization. The promise of automatic heuristic design, freeing the compiler engineer from the complex interactions of program, architecture, and other optimizations, is alluring. However, most machine learning methods cannot replicate even the simplest of the abstract interpretations of data flow analysis that are critical to making good optimization decisions. This must change for machine learning to become the dominant technology in compiler heuristics.   To this end, we propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of whole-program semantics for deep learning. To benchmark current and future learning techniques for compiler analyses we introduce an open dataset of 461k Intermediate Representation (IR) files for LLVM, covering five source programming languages, and 15.4M corresponding data flow results. We formulate data flow analysis as an MPNN and show that, using ProGraML, standard analyses can be learned, yielding improved performance on downstream compiler optimization tasks.

</details>

<details>

<summary>2020-11-21 11:33:45 - Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling</summary>

- *Diego Marcheggiani, Ivan Titov*

- `1909.09814v3` - [abs](http://arxiv.org/abs/1909.09814v3) - [pdf](http://arxiv.org/pdf/1909.09814v3)

> Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by `composing' word representations of the first and the last word in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are `decomposed' back into word representations which in turn are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard CoNLL-2005, CoNLL-2012, and FrameNet benchmarks.

</details>

<details>

<summary>2020-11-21 16:37:28 - Evaluating Semantic Accuracy of Data-to-Text Generation with Natural Language Inference</summary>

- *Ondřej Dušek, Zdeněk Kasner*

- `2011.10819v1` - [abs](http://arxiv.org/abs/2011.10819v1) - [pdf](http://arxiv.org/pdf/2011.10819v1)

> A major challenge in evaluating data-to-text (D2T) generation is measuring the semantic accuracy of the generated text, i.e. checking if the output text contains all and only facts supported by the input data. We propose a new metric for evaluating the semantic accuracy of D2T generation based on a neural model pretrained for natural language inference (NLI). We use the NLI model to check textual entailment between the input data and the output text in both directions, allowing us to reveal omissions or hallucinations. Input data are converted to text for NLI using trivial templates. Our experiments on two recent D2T datasets show that our metric can achieve high accuracy in identifying erroneous system outputs.

</details>

<details>

<summary>2020-11-22 01:36:37 - Video SemNet: Memory-Augmented Video Semantic Network</summary>

- *Prashanth Vijayaraghavan, Deb Roy*

- `2011.10909v1` - [abs](http://arxiv.org/abs/2011.10909v1) - [pdf](http://arxiv.org/pdf/2011.10909v1)

> Stories are a very compelling medium to convey ideas, experiences, social and cultural values. Narrative is a specific manifestation of the story that turns it into knowledge for the audience. In this paper, we propose a machine learning approach to capture the narrative elements in movies by bridging the gap between the low-level data representations and semantic aspects of the visual medium. We present a Memory-Augmented Video Semantic Network, called Video SemNet, to encode the semantic descriptors and learn an embedding for the video. The model employs two main components: (i) a neural semantic learner that learns latent embeddings of semantic descriptors and (ii) a memory module that retains and memorizes specific semantic patterns from the video. We evaluate the video representations obtained from variants of our model on two tasks: (a) genre prediction and (b) IMDB Rating prediction. We demonstrate that our model is able to predict genres and IMDB ratings with a weighted F-1 score of 0.72 and 0.63 respectively. The results are indicative of the representational power of our model and the ability of such representations to measure audience engagement.

</details>

<details>

<summary>2020-11-22 12:13:29 - Using ontology embeddings for structural inductive bias in gene expression data analysis</summary>

- *Maja Trębacz, Zohreh Shams, Mateja Jamnik, Paul Scherer, Nikola Simidjievski, Helena Andres Terre, Pietro Liò*

- `2011.10998v1` - [abs](http://arxiv.org/abs/2011.10998v1) - [pdf](http://arxiv.org/pdf/2011.10998v1)

> Stratifying cancer patients based on their gene expression levels allows improving diagnosis, survival analysis and treatment planning. However, such data is extremely highly dimensional as it contains expression values for over 20000 genes per patient, and the number of samples in the datasets is low. To deal with such settings, we propose to incorporate prior biological knowledge about genes from ontologies into the machine learning system for the task of patient classification given their gene expression data. We use ontology embeddings that capture the semantic similarities between the genes to direct a Graph Convolutional Network, and therefore sparsify the network connections. We show this approach provides an advantage for predicting clinical targets from high-dimensional low-sample data.

</details>

<details>

<summary>2020-11-22 13:41:54 - Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings</summary>

- *Brett D. Roads, Bradley C. Love*

- `2011.11015v1` - [abs](http://arxiv.org/abs/2011.11015v1) - [pdf](http://arxiv.org/pdf/2011.11015v1)

> Advances in object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. However, these benchmarks---such as ILSVRC---are relatively task-specific, focusing predominately on predicting class labels. We introduce a publicly-available dataset that embodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ) is composed of human similarity judgments that supplement the ILSVRC validation set. The new dataset supports a range of task and performance metrics, including the evaluation of unsupervised learning algorithms. We demonstrate two methods of assessment: using the similarity judgments directly and using a psychological embedding trained on the similarity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous efforts based on human judgments. Scaling to the full 50,000 image set was made possible through a selective sampling process that used variational Bayesian inference and model ensembles to sample aspects of the embedding space that were most uncertain. This methodological innovation not only enables scaling, but should also improve the quality of solutions by focusing sampling where it is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. One finding is that more complex models that perform better on task-specific benchmarks do not better conform to human semantic judgments. In addition to the human similarity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made publicly available. Collectively, ImageNet-HSJ assets support the appraisal of internal representations and the development of more human-like models.

</details>

<details>

<summary>2020-11-22 18:27:51 - Persuasive Dialogue Understanding: the Baselines and Negative Results</summary>

- *Hui Chen, Deepanway Ghosal, Navonil Majumder, Amir Hussain, Soujanya Poria*

- `2011.09954v2` - [abs](http://arxiv.org/abs/2011.09954v2) - [pdf](http://arxiv.org/pdf/2011.09954v2)

> Persuasion aims at forming one's opinion and action via a series of persuasive messages containing persuader's strategies. Due to its potential application in persuasive dialogue systems, the task of persuasive strategy recognition has gained much attention lately. Previous methods on user intent recognition in dialogue systems adopt recurrent neural network (RNN) or convolutional neural network (CNN) to model context in conversational history, neglecting the tactic history and intra-speaker relation. In this paper, we demonstrate the limitations of a Transformer-based approach coupled with Conditional Random Field (CRF) for the task of persuasive strategy recognition. In this model, we leverage inter- and intra-speaker contextual semantic features, as well as label dependencies to improve the recognition. Despite extensive hyper-parameter optimizations, this architecture fails to outperform the baseline methods. We observe two negative results. Firstly, CRF cannot capture persuasive label dependencies, possibly as strategies in persuasive dialogues do not follow any strict grammar or rules as the cases in Named Entity Recognition (NER) or part-of-speech (POS) tagging. Secondly, the Transformer encoder trained from scratch is less capable of capturing sequential information in persuasive dialogues than Long Short-Term Memory (LSTM). We attribute this to the reason that the vanilla Transformer encoder does not efficiently consider relative position information of sequence elements.

</details>

<details>

<summary>2020-11-22 18:30:23 - Deep learning model trained on mobile phone-acquired frozen section images effectively detects basal cell carcinoma</summary>

- *Junli Cao, B. S., Junyan Wu, M. S., Jing W. Zhang, M. D., Ph. D., Jay J. Ye, M. D., Ph. D., Limin Yu, M. D., M. S*

- `2011.11081v1` - [abs](http://arxiv.org/abs/2011.11081v1) - [pdf](http://arxiv.org/pdf/2011.11081v1)

> Background: Margin assessment of basal cell carcinoma using the frozen section is a common task of pathology intraoperative consultation. Although frequently straight-forward, the determination of the presence or absence of basal cell carcinoma on the tissue sections can sometimes be challenging. We explore if a deep learning model trained on mobile phone-acquired frozen section images can have adequate performance for future deployment. Materials and Methods: One thousand two hundred and forty-one (1241) images of frozen sections performed for basal cell carcinoma margin status were acquired using mobile phones. The photos were taken at 100x magnification (10x objective). The images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone was used for model training. Results: The model uses an image as input and produces a 2-dimensional black and white output of prediction of the same dimension; the areas determined to be basal cell carcinoma were displayed with white color, in a black background. Any output with the number of white pixels exceeding 0.5% of the total number of pixels is deemed positive for basal cell carcinoma. On the test set, the model achieves area under curve of 0.99 for receiver operator curve and 0.97 for precision-recall curve at the pixel level. The accuracy of classification at the slide level is 96%. Conclusions: The deep learning model trained with mobile phone images shows satisfactory performance characteristics, and thus demonstrates the potential for deploying as a mobile phone app to assist in frozen section interpretation in real time.

</details>

<details>

<summary>2020-11-22 20:48:52 - A Tale of Two Linkings: Dynamically Gating between Schema Linking and Structural Linking for Text-to-SQL Parsing</summary>

- *Sanxing Chen, Aidan San, Xiaodong Liu, Yangfeng Ji*

- `2009.14809v2` - [abs](http://arxiv.org/abs/2009.14809v2) - [pdf](http://arxiv.org/pdf/2009.14809v2)

> In Text-to-SQL semantic parsing, selecting the correct entities (tables and columns) for the generated SQL query is both crucial and challenging; the parser is required to connect the natural language (NL) question and the SQL query to the structured knowledge in the database. We formulate two linking processes to address this challenge: schema linking which links explicit NL mentions to the database and structural linking which links the entities in the output SQL with their structural relationships in the database schema. Intuitively, the effectiveness of these two linking processes changes based on the entity being generated, thus we propose to dynamically choose between them using a gating mechanism. Integrating the proposed method with two graph neural network-based semantic parsers together with BERT representations demonstrates substantial gains in parsing accuracy on the challenging Spider dataset. Analyses show that our proposed method helps to enhance the structure of the model output when generating complicated SQL queries and offers more explainable predictions.

</details>

<details>

<summary>2020-11-22 21:51:19 - Employing distributional semantics to organize task-focused vocabulary learning</summary>

- *Haemanth Santhi Ponnusamy, Detmar Meurers*

- `2011.11115v1` - [abs](http://arxiv.org/abs/2011.11115v1) - [pdf](http://arxiv.org/pdf/2011.11115v1)

> How can a learner systematically prepare for reading a book they are interested in? In this paper,we explore how computational linguistic methods such as distributional semantics, morphological clustering, and exercise generation can be combined with graph-based learner models to answer this question both conceptually and in practice. Based on the highly structured learner model and concepts from network analysis, the learner is guided to efficiently explore the targeted lexical space. They practice using multi-gap learning activities generated from the book focused on words that are central to the targeted lexical space. As such the approach offers a unique combination of computational linguistic methods with concepts from network analysis and the tutoring system domain to support learners in achieving their individual, reading task-based learning goals.

</details>

<details>

<summary>2020-11-23 09:01:28 - MEG: Multi-Evidence GNN for Multimodal Semantic Forensics</summary>

- *Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan*

- `2011.11286v1` - [abs](http://arxiv.org/abs/2011.11286v1) - [pdf](http://arxiv.org/pdf/2011.11286v1)

> Fake news often involves semantic manipulations across modalities such as image, text, location etc and requires the development of multimodal semantic forensics for its detection. Recent research has centered the problem around images, calling it image repurposing -- where a digitally unmanipulated image is semantically misrepresented by means of its accompanying multimodal metadata such as captions, location, etc. The image and metadata together comprise a multimedia package. The problem setup requires algorithms to perform multimodal semantic forensics to authenticate a query multimedia package using a reference dataset of potentially related packages as evidences. Existing methods are limited to using a single evidence (retrieved package), which ignores potential performance improvement from the use of multiple evidences. In this work, we introduce a novel graph neural network based model for multimodal semantic forensics, which effectively utilizes multiple retrieved packages as evidences and is scalable with the number of evidences. We compare the scalability and performance of our model against existing methods. Experimental results show that the proposed model outperforms existing state-of-the-art algorithms with an error reduction of up to 25%.

</details>

<details>

<summary>2020-11-23 13:03:10 - A comparative study of semi- and self-supervised semantic segmentation of biomedical microscopy data</summary>

- *Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian Wagner, Ingo Roeder, Nico Scherf*

- `2011.08076v2` - [abs](http://arxiv.org/abs/2011.08076v2) - [pdf](http://arxiv.org/pdf/2011.08076v2)

> In recent years, Convolutional Neural Networks (CNNs) have become the state-of-the-art method for biomedical image analysis. However, these networks are usually trained in a supervised manner, requiring large amounts of labelled training data. These labelled data sets are often difficult to acquire in the biomedical domain. In this work, we validate alternative ways to train CNNs with fewer labels for biomedical image segmentation using. We adapt two semi- and self-supervised image classification methods and analyse their performance for semantic segmentation of biomedical microscopy images.

</details>

<details>

<summary>2020-11-23 13:29:16 - STEPs-RL: Speech-Text Entanglement for Phonetically Sound Representation Learning</summary>

- *Prakamya Mishra*

- `2011.11387v1` - [abs](http://arxiv.org/abs/2011.11387v1) - [pdf](http://arxiv.org/pdf/2011.11387v1)

> In this paper, we present a novel multi-modal deep neural network architecture that uses speech and text entanglement for learning phonetically sound spoken-word representations. STEPs-RL is trained in a supervised manner to predict the phonetic sequence of a target spoken-word using its contextual spoken word's speech and text, such that the model encodes its meaningful latent representations. Unlike existing work, we have used text along with speech for auditory representation learning to capture semantical and syntactical information along with the acoustic and temporal information. The latent representations produced by our model were not only able to predict the target phonetic sequences with an accuracy of 89.47% but were also able to achieve competitive results to textual word representation models, Word2Vec & FastText (trained on textual transcripts), when evaluated on four widely used word similarity benchmark datasets. In addition, investigation of the generated vector space also demonstrated the capability of the proposed model to capture the phonetic structure of the spoken-words. To the best of our knowledge, none of the existing works use speech and text entanglement for learning spoken-word representation, which makes this work first of its kind.

</details>

<details>

<summary>2020-11-23 14:19:30 - Enclave-Aware Compartmentalization and Secure Sharing with Sirius</summary>

- *Zahra Tarkhani, Anil Madhavapeddy*

- `2009.01869v3` - [abs](http://arxiv.org/abs/2009.01869v3) - [pdf](http://arxiv.org/pdf/2009.01869v3)

> Hardware-assisted trusted execution environments (TEEs) are critical building blocks of many modern applications. However, they have a one-way isolation model that introduces a semantic gap between a TEE and its outside world. This lack of information causes an ever-increasing set of attacks on TEE-enabled applications that exploit various insecure interactions with the host OSs, applications, or other enclaves. We introduce Sirius, the first compartmentalization framework that achieves strong isolation and secure sharing in TEE-assisted applications by controlling the dataflows within primary kernel objects (e.g. threads, processes, address spaces, files, sockets, pipes) in both the secure and normal worlds. Sirius replaces ad-hoc interactions in current TEE systems with a principled approach that adds strong inter- and intra-address space isolation and effectively eliminates a wide range of attacks. We evaluate Sirius on ARM platforms and find that it is lightweight ($\approx 15K$ LoC) and only adds $\approx 10.8\%$ overhead to enable TEE support on applications such as httpd, and improves the performance of existing TEE-enabled applications such as the Darknet ML framework and ARM's LibDDSSec by $0.05\%-5.6\%$.

</details>

<details>

<summary>2020-11-23 16:35:43 - Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature Layers</summary>

- *Nina Varney, Vijayan K. Asari, Quinn Graehling*

- `2011.08692v2` - [abs](http://arxiv.org/abs/2011.08692v2) - [pdf](http://arxiv.org/pdf/2011.08692v2)

> We present a method to learn a diverse group of object categories from an unordered point set. We propose our Pyramid Point network, which uses a dense pyramid structure instead of the traditional 'U' shape, typically seen in semantic segmentation networks. This pyramid structure gives a second look, allowing the network to revisit different layers simultaneously, increasing the contextual information by creating additional layers with less noise. We introduce a Focused Kernel Point convolution (FKP Conv), which expands on the traditional point convolutions by adding an attention mechanism to the kernel outputs. This FKP Conv increases our feature quality and allows us to weigh the kernel outputs dynamically. These FKP Convs are the central part of our Recurrent FKP Bottleneck block, which makes up the backbone of our encoder. With this distinct network, we demonstrate competitive performance on three benchmark data sets. We also perform an ablation study to show the positive effects of each element in our FKP Conv.

</details>

<details>

<summary>2020-11-24 01:54:52 - Equivariant Maps for Hierarchical Structures</summary>

- *Renhao Wang, Marjan Albooyeh, Siamak Ravanbakhsh*

- `2006.03627v2` - [abs](http://arxiv.org/abs/2006.03627v2) - [pdf](http://arxiv.org/pdf/2006.03627v2)

> While using invariant and equivariant maps, it is possible to apply deep learning to a range of primitive data structures, a formalism for dealing with hierarchy is lacking. This is a significant issue because many practical structures are hierarchies of simple building blocks; some examples include sequences of sets, graphs of graphs, or multiresolution images. Observing that the symmetry of a hierarchical structure is the "wreath product" of symmetries of the building blocks, we express the equivariant map for the hierarchy using an intuitive combination of the equivariant linear layers of the building blocks. More generally, we show that any equivariant map for the hierarchy has this form. To demonstrate the effectiveness of this approach to model design, we consider its application in the semantic segmentation of point-cloud data. By voxelizing the point cloud, we impose a hierarchy of translation and permutation symmetries on the data and report state-of-the-art on Semantic3D, S3DIS, and vKITTI, that include some of the largest real-world point-cloud benchmarks.

</details>

<details>

<summary>2020-11-24 04:11:13 - A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus</summary>

- *Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas, Vihan Jain, Eugene Ie, Fei Sha*

- `2011.09046v2` - [abs](http://arxiv.org/abs/2011.09046v2) - [pdf](http://arxiv.org/pdf/2011.09046v2)

> Identifying a short segment in a long video that semantically matches a text query is a challenging task that has important application potentials in language-based video search, browsing, and navigation. Typical retrieval systems respond to a query with either a whole video or a pre-defined video segment, but it is challenging to localize undefined segments in untrimmed and unsegmented videos where exhaustively searching over all possible segments is intractable. The outstanding challenge is that the representation of a video must account for different levels of granularity in the temporal domain. To tackle this problem, we propose the HierArchical Multi-Modal EncodeR (HAMMER) that encodes a video at both the coarse-grained clip level and the fine-grained frame level to extract information at different scales based on multiple subtasks, namely, video retrieval, segment temporal localization, and masked language modeling. We conduct extensive experiments to evaluate our model on moment localization in video corpus on ActivityNet Captions and TVR datasets. Our approach outperforms the previous methods as well as strong baselines, establishing new state-of-the-art for this task.

</details>

<details>

<summary>2020-11-24 05:35:46 - Robust Processing-In-Memory Neural Networks via Noise-Aware Normalization</summary>

- *Li-Huang Tsai, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Cheng Juan*

- `2007.03230v2` - [abs](http://arxiv.org/abs/2007.03230v2) - [pdf](http://arxiv.org/pdf/2007.03230v2)

> Analog computing hardwares, such as Processing-in-memory (PIM) accelerators, have gradually received more attention for accelerating the neural network computations. However, PIM accelerators often suffer from intrinsic noise in the physical components, making it challenging for neural network models to achieve the same performance as on the digital hardware. Previous works in mitigating intrinsic noise assumed the knowledge of the noise model, and retraining the neural networks accordingly was required. In this paper, we propose a noise-agnostic method to achieve robust neural network performance against any noise setting. Our key observation is that the degradation of performance is due to the distribution shifts in network activations, which are caused by the noise. To properly track the shifts and calibrate the biased distributions, we propose a "noise-aware" batch normalization layer, which is able to align the distributions of the activations under variational noise inherent in the analog environments. Our method is simple, easy to implement, general to various noise settings, and does not need to retrain the models. We conduct experiments on several tasks in computer vision, including classification, object detection and semantic segmentation. The results demonstrate the effectiveness of our method, achieving robust performance under a wide range of noise settings, more reliable than existing methods. We believe that our simple yet general method can facilitate the adoption of analog computing devices for neural networks.

</details>

<details>

<summary>2020-11-24 06:33:45 - Disentangle-based Continual Graph Representation Learning</summary>

- *Xiaoyu Kou, Yankai Lin, Shaobo Liu, Peng Li, Jie Zhou, Yan Zhang*

- `2010.02565v4` - [abs](http://arxiv.org/abs/2010.02565v4) - [pdf](http://arxiv.org/pdf/2010.02565v4)

> Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.

</details>

<details>

<summary>2020-11-24 06:59:38 - Multiscale Deep Equilibrium Models</summary>

- *Shaojie Bai, Vladlen Koltun, J. Zico Kolter*

- `2006.08656v2` - [abs](http://arxiv.org/abs/2006.08656v2) - [pdf](http://arxiv.org/pdf/2006.08656v2)

> We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only $O(1)$ memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach. The code and pre-trained models are at https://github.com/locuslab/mdeq .

</details>

<details>

<summary>2020-11-24 10:14:58 - Refining Semantic Segmentation with Superpixel by Transparent Initialization and Sparse Encoder</summary>

- *Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley*

- `2010.04363v3` - [abs](http://arxiv.org/abs/2010.04363v3) - [pdf](http://arxiv.org/pdf/2010.04363v3)

> Although deep learning greatly improves the performance of semantic segmentation, its success mainly lies in object central areas without accurate edges. As superpixels are a popular and effective auxiliary to preserve object edges, in this paper, we jointly learn semantic segmentation with trainable superpixels. We achieve it with fully-connected layers with Transparent Initialization (TI) and efficient logit consistency using a sparse encoder. The proposed TI preserves the effects of learned parameters of pretrained networks. This avoids a significant increase of the loss of pretrained networks, which otherwise may be caused by inappropriate parameter initialization of the additional layers. Meanwhile, consistent pixel labels in each superpixel are guaranteed by logit consistency. The sparse encoder with sparse matrix operations substantially reduces both the memory requirement and the computational complexity. We demonstrated the superiority of TI over other parameter initialization methods and tested its numerical stability. The effectiveness of our proposal was validated on PASCAL VOC 2012, ADE20K, and PASCAL Context showing enhanced semantic segmentation edges. With quantitative evaluations on segmentation edges using performance ratio and F-measure, our method outperforms the state-of-the-art.

</details>

<details>

<summary>2020-11-24 10:32:36 - FKAConv: Feature-Kernel Alignment for Point Cloud Convolution</summary>

- *Alexandre Boulch, Gilles Puy, Renaud Marlet*

- `2004.04462v3` - [abs](http://arxiv.org/abs/2004.04462v3) - [pdf](http://arxiv.org/pdf/2004.04462v3)

> Recent state-of-the-art methods for point cloud processing are based on the notion of point convolution, for which several approaches have been proposed. In this paper, inspired by discrete convolution in image processing, we provide a formulation to relate and analyze a number of point convolution methods. We also propose our own convolution variant, that separates the estimation of geometry-less kernel weights and their alignment to the spatial support of features. Additionally, we define a point sampling strategy for convolution that is both effective and fast. Finally, using our convolution and sampling strategy, we show competitive results on classification and semantic segmentation benchmarks while being time and memory efficient.

</details>

<details>

<summary>2020-11-24 13:34:38 - Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning and Machine Learning</summary>

- *Suk Joon Hong, Brandon Bennett*

- `2011.12081v1` - [abs](http://arxiv.org/abs/2011.12081v1) - [pdf](http://arxiv.org/pdf/2011.12081v1)

> The Winograd Schema Challenge (WSC) is a common-sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A thanking domain was defined by key-words, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of Sharma [2019]. Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers (BERT) [Kocijan et al., 2019]. Lastly, in terms of evaluation, we suggest a "robust" accuracy measurement by modifying that of Trichelair et al. [2018]. As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.

</details>

<details>

<summary>2020-11-24 13:54:28 - SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries</summary>

- *Xirong Li, Fangming Zhou, Chaoxi Xu, Jiaqi Ji, Gang Yang*

- `2011.12091v1` - [abs](http://arxiv.org/abs/2011.12091v1) - [pdf](http://arxiv.org/pdf/2011.12091v1)

> Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.

</details>

<details>

<summary>2020-11-24 16:07:18 - Neural Text Classification by Jointly Learning to Cluster and Align</summary>

- *Yekun Chai, Haidong Zhang, Shuo Jin*

- `2011.12184v1` - [abs](http://arxiv.org/abs/2011.12184v1) - [pdf](http://arxiv.org/pdf/2011.12184v1)

> Distributional text clustering delivers semantically informative representations and captures the relevance between each word and semantic clustering centroids. We extend the neural text clustering approach to text classification tasks by inducing cluster centers via a latent variable model and interacting with distributional word embeddings, to enrich the representation of tokens and measure the relatedness between tokens and each learnable cluster centroid. The proposed method jointly learns word clustering centroids and clustering-token alignments, achieving the state of the art results on multiple benchmark datasets and proving that the proposed cluster-token alignment mechanism is indeed favorable to text classification. Notably, our qualitative analysis has conspicuously illustrated that text representations learned by the proposed model are in accord well with our intuition.

</details>

<details>

<summary>2020-11-24 16:19:49 - Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</summary>

- *Yekun Chai, Shuo Jin, Xinwen Hou*

- `2004.08178v5` - [abs](http://arxiv.org/abs/2004.08178v5) - [pdf](http://arxiv.org/pdf/2004.08178v5)

> Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.

</details>

<details>

<summary>2020-11-24 19:52:43 - Quantum Machine Learning Algorithm for Knowledge Graphs</summary>

- *Yunpu Ma, Volker Tresp*

- `2001.01077v2` - [abs](http://arxiv.org/abs/2001.01077v2) - [pdf](http://arxiv.org/pdf/2001.01077v2)

> Semantic knowledge graphs are large-scale triple-oriented databases for knowledge representation and reasoning. Implicit knowledge can be inferred by modeling and reconstructing the tensor representations generated from knowledge graphs. However, as the sizes of knowledge graphs continue to grow, classical modeling becomes increasingly computational resource intensive. This paper investigates how quantum resources can be capitalized to accelerate the modeling of knowledge graphs. In particular, we propose the first quantum machine learning algorithm for making inference on tensorized data, e.g., on knowledge graphs. Since most tensor problems are NP-hard, it is challenging to devise quantum algorithms to support that task. We simplify the problem by making a plausible assumption that the tensor representation of a knowledge graph can be approximated by its low-rank tensor singular value decomposition, which is verified by our experiments. The proposed sampling-based quantum algorithm achieves exponential speedup with a runtime that is polylogarithmic in the dimension of knowledge graph tensor.

</details>

<details>

<summary>2020-11-25 01:31:01 - Emotional Semantics-Preserved and Feature-Aligned CycleGAN for Visual Emotion Adaptation</summary>

- *Sicheng Zhao, Xuanbai Chen, Xiangyu Yue, Chuang Lin, Pengfei Xu, Ravi Krishna, Jufeng Yang, Guiguang Ding, Alberto L. Sangiovanni-Vincentelli, Kurt Keutzer*

- `2011.12470v1` - [abs](http://arxiv.org/abs/2011.12470v1) - [pdf](http://arxiv.org/pdf/2011.12470v1)

> Thanks to large-scale labeled training data, deep neural networks (DNNs) have obtained remarkable success in many vision and multimedia tasks. However, because of the presence of domain shift, the learned knowledge of the well-trained DNNs cannot be well generalized to new domains or datasets that have few labels. Unsupervised domain adaptation (UDA) studies the problem of transferring models trained on one labeled source domain to another unlabeled target domain. In this paper, we focus on UDA in visual emotion analysis for both emotion distribution learning and dominant emotion classification. Specifically, we design a novel end-to-end cycle-consistent adversarial model, termed CycleEmotionGAN++. First, we generate an adapted domain to align the source and target domains on the pixel-level by improving CycleGAN with a multi-scale structured cycle-consistency loss. During the image translation, we propose a dynamic emotional semantic consistency loss to preserve the emotion labels of the source images. Second, we train a transferable task classifier on the adapted domain with feature-level alignment between the adapted and target domains. We conduct extensive UDA experiments on the Flickr-LDL & Twitter-LDL datasets for distribution learning and ArtPhoto & FI datasets for emotion classification. The results demonstrate the significant improvements yielded by the proposed CycleEmotionGAN++ as compared to state-of-the-art UDA approaches.

</details>

<details>

<summary>2020-11-25 02:06:42 - SECNLP: A Survey of Embeddings in Clinical Natural Language Processing</summary>

- *Kalyan KS, S Sangeetha*

- `1903.01039v4` - [abs](http://arxiv.org/abs/1903.01039v4) - [pdf](http://arxiv.org/pdf/1903.01039v4)

> Traditional representations like Bag of words are high dimensional, sparse and ignore the order as well as syntactic and semantic information. Distributed vector representations or embeddings map variable length text to dense fixed length vectors as well as capture the prior knowledge which can transferred to downstream tasks. Even though embedding has become de facto standard for representations in deep learning based NLP tasks in both general and clinical domains, there is no survey paper which presents a detailed review of embeddings in Clinical Natural Language Processing. In this survey paper, we discuss various medical corpora and their characteristics, medical codes and present a brief overview as well as comparison of popular embeddings models. We classify clinical embeddings into nine types and discuss each embedding type in detail. We discuss various evaluation methods followed by possible solutions to various challenges in clinical embeddings. Finally, we conclude with some of the future directions which will advance the research in clinical embeddings.

</details>

<details>

<summary>2020-11-25 05:41:39 - Modeling Functional Similarity in Source Code with Graph-Based Siamese Networks</summary>

- *Nikita Mehrotra, Navdha Agarwal, Piyush Gupta, Saket Anand, David Lo, Rahul Purandare*

- `2011.11228v2` - [abs](http://arxiv.org/abs/2011.11228v2) - [pdf](http://arxiv.org/pdf/2011.11228v2)

> Code clones are duplicate code fragments that share (nearly) similar syntax or semantics. Code clone detection plays an important role in software maintenance, code refactoring, and reuse. A substantial amount of research has been conducted in the past to detect clones. A majority of these approaches use lexical and syntactic information to detect clones. However, only a few of them target semantic clones. Recently, motivated by the success of deep learning models in other fields, including natural language processing and computer vision, researchers have attempted to adopt deep learning techniques to detect code clones. These approaches use lexical information (tokens) and(or) syntactic structures like abstract syntax trees (ASTs) to detect code clones. However, they do not make sufficient use of the available structural and semantic information hence, limiting their capabilities.   This paper addresses the problem of semantic code clone detection using program dependency graphs and geometric neural networks, leveraging the structured syntactic and semantic information. We have developed a prototype tool HOLMES, based on our novel approach, and empirically evaluated it on popular code clone benchmarks. Our results show that HOLMES performs considerably better than the other state-of-the-art tool, TBCCD. We also evaluated HOLMES on unseen projects and performed cross dataset experiments to assess the generalizability of HOLMES. Our results affirm that HOLMES outperforms TBCCD since most of the pairs that HOLMES detected were either undetected or suboptimally reported by TBCCD.

</details>

<details>

<summary>2020-11-25 11:10:37 - The Unreasonable Effectiveness of Encoder-Decoder Networks for Retinal Vessel Segmentation</summary>

- *Björn Browatzki, Jörn-Philipp Lies, Christian Wallraven*

- `2011.12643v1` - [abs](http://arxiv.org/abs/2011.12643v1) - [pdf](http://arxiv.org/pdf/2011.12643v1)

> We propose an encoder-decoder framework for the segmentation of blood vessels in retinal images that relies on the extraction of large-scale patches at multiple image-scales during training. Experiments on three fundus image datasets demonstrate that this approach achieves state-of-the-art results and can be implemented using a simple and efficient fully-convolutional network with a parameter count of less than 0.8M. Furthermore, we show that this framework - called VLight - avoids overfitting to specific training images and generalizes well across different datasets, which makes it highly suitable for real-world applications where robustness, accuracy as well as low inference time on high-resolution fundus images is required.

</details>

<details>

<summary>2020-11-25 11:18:20 - Image Embedded Segmentation: Uniting Supervised and Unsupervised Objectives for Segmenting Histopathological Images</summary>

- *C. T. Sari, C. Sokmensuer, C. Gunduz-Demir*

- `2001.11202v3` - [abs](http://arxiv.org/abs/2001.11202v3) - [pdf](http://arxiv.org/pdf/2001.11202v3)

> This paper presents a new regularization method to train a fully convolutional network for semantic tissue segmentation in histopathological images. This method relies on the benefit of unsupervised learning, in the form of image reconstruction, for network training. To this end, it puts forward an idea of defining a new embedding that allows uniting the main supervised task of semantic segmentation and an auxiliary unsupervised task of image reconstruction into a single one and proposes to learn this united task by a single generative model. This embedding generates an output image by superimposing an input image on its segmentation map. Then, the method learns to translate the input image to this embedded output image using a conditional generative adversarial network, which is known as quite effective for image-to-image translations. This proposal is different than the existing approach that uses image reconstruction for the same regularization purpose. The existing approach considers segmentation and image reconstruction as two separate tasks in a multi-task network, defines their losses independently, and combines them in a joint loss function. However, the definition of such a function requires externally determining right contributions of the supervised and unsupervised losses that yield balanced learning between the segmentation and image reconstruction tasks. The proposed approach provides an easier solution to this problem by uniting these two tasks into a single one, which intrinsically combines their losses. We test our approach on three datasets of histopathological images. Our experiments demonstrate that it leads to better segmentation results in these datasets, compared to its counterparts.

</details>

<details>

<summary>2020-11-25 13:37:01 - GIF: Generative Interpretable Faces</summary>

- *Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael Black, Timo Bolkart*

- `2009.00149v2` - [abs](http://arxiv.org/abs/2009.00149v2) - [pdf](http://arxiv.org/pdf/2009.00149v2)

> Photo-realistic visualization and animation of expressive human faces have been a long standing challenge. 3D face modeling methods provide parametric control but generates unrealistic images, on the other hand, generative 2D models like GANs (Generative Adversarial Networks) output photo-realistic face images, but lack explicit control. Recent methods gain partial control, either by attempting to disentangle different factors in an unsupervised manner, or by adding control post hoc to a pre-trained model. Unconditional GANs, however, may entangle factors that are hard to undo later. We condition our generative model on pre-defined control parameters to encourage disentanglement in the generation process. Specifically, we condition StyleGAN2 on FLAME, a generative 3D face model. While conditioning on FLAME parameters yields unsatisfactory results, we find that conditioning on rendered FLAME geometry and photometric details works well. This gives us a generative 2D face model named GIF (Generative Interpretable Faces) that offers FLAME's parametric control. Here, interpretable refers to the semantic meaning of different parameters. Given FLAME parameters for shape, pose, expressions, parameters for appearance, lighting, and an additional style vector, GIF outputs photo-realistic face images. We perform an AMT based perceptual study to quantitatively and qualitatively evaluate how well GIF follows its conditioning. The code, data, and trained model are publicly available for research purposes at http://gif.is.tue.mpg.de.

</details>

<details>

<summary>2020-11-25 15:18:51 - Tractable Epistemic Reasoning with Functional Fluents, Static Causal Laws and Postdiction</summary>

- *Manfred Eppe*

- `1403.0034v4` - [abs](http://arxiv.org/abs/1403.0034v4) - [pdf](http://arxiv.org/pdf/1403.0034v4)

> We present an epistemic action theory for tractable epistemic reasoning as an extension to the h-approximation (HPX) theory. In contrast to existing tractable approaches, the theory supports functional fluents and postdictive reasoning with static causal laws. We argue that this combination is particularly synergistic because it allows one not only to perform direct postdiction about the conditions of actions, but also indirect postdiction about the conditions of static causal laws. We show that despite the richer expressiveness, the temporal projection problem remains tractable (polynomial), and therefore the planning problem remains in NP. We present the operational semantics of our theory as well as its formulation as Answer Set Programming.

</details>

<details>

<summary>2020-11-25 17:50:56 - DRACO: Weakly Supervised Dense Reconstruction And Canonicalization of Objects</summary>

- *Rahul Sajnani, AadilMehdi Sanchawala, Krishna Murthy Jatavallabhula, Srinath Sridhar, K. Madhava Krishna*

- `2011.12912v1` - [abs](http://arxiv.org/abs/2011.12912v1) - [pdf](http://arxiv.org/pdf/2011.12912v1)

> We present DRACO, a method for Dense Reconstruction And Canonicalization of Object shape from one or more RGB images. Canonical shape reconstruction, estimating 3D object shape in a coordinate space canonicalized for scale, rotation, and translation parameters, is an emerging paradigm that holds promise for a multitude of robotic applications. Prior approaches either rely on painstakingly gathered dense 3D supervision, or produce only sparse canonical representations, limiting real-world applicability. DRACO performs dense canonicalization using only weak supervision in the form of camera poses and semantic keypoints at train time. During inference, DRACO predicts dense object-centric depth maps in a canonical coordinate-space, solely using one or more RGB images of an object. Extensive experiments on canonical shape reconstruction and pose estimation show that DRACO is competitive or superior to fully-supervised methods.

</details>

<details>

<summary>2020-11-25 23:06:53 - Semi-Supervised StyleGAN for Disentanglement Learning</summary>

- *Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath, Anjul Patney, Ankit B. Patel, Anima Anandkumar*

- `2003.03461v3` - [abs](http://arxiv.org/abs/2003.03461v3) - [pdf](http://arxiv.org/pdf/2003.03461v3)

> Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily focusing on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images.

</details>

<details>

<summary>2020-11-26 01:57:24 - The Evolution of Concept-Acquisition based on Developmental Psychology</summary>

- *Hui Wei*

- `2011.13089v1` - [abs](http://arxiv.org/abs/2011.13089v1) - [pdf](http://arxiv.org/pdf/2011.13089v1)

> A conceptual system with rich connotation is key to improving the performance of knowledge-based artificial intelligence systems. While a conceptual system, which has abundant concepts and rich semantic relationships, and is developable, evolvable, and adaptable to multi-task environments, its actual construction is not only one of the major challenges of knowledge engineering, but also the fundamental goal of research on knowledge and conceptualization. Finding a new method to represent concepts and construct a conceptual system will therefore greatly improve the performance of many intelligent systems. Fortunately the core of human cognition is a system with relatively complete concepts and a mechanism that ensures the establishment and development of the system. The human conceptual system can not be achieved immediately, but rather must develop gradually. Developmental psychology carefully observes the process of concept acquisition in humans at the behavioral level, and along with cognitive psychology has proposed some rough explanations of those observations. However, due to the lack of research in aspects such as representation, systematic models, algorithm details and realization, many of the results of developmental psychology have not been applied directly to the building of artificial conceptual systems. For example, Karmiloff-Smith's Representation Redescription (RR) supposition reflects a concept-acquisition process that re-describes a lower level representation of a concept to a higher one. This paper is inspired by this developmental psychology viewpoint. We use an object-oriented approach to re-explain and materialize RR supposition from the formal semantic perspective, because the OO paradigm is a natural way to describe the outside world, and it also has strict grammar regulations.

</details>

<details>

<summary>2020-11-26 09:14:32 - Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer</summary>

- *Joosung Lee*

- `2005.12086v3` - [abs](http://arxiv.org/abs/2005.12086v3) - [pdf](http://arxiv.org/pdf/2005.12086v3)

> Text style transfer is the task that generates a sentence by preserving the content of the input sentence and transferring the style. Most existing studies are progressing on non-parallel datasets because parallel datasets are limited and hard to construct. In this work, we introduce a method that follows two stages in non-parallel datasets. The first stage is to delete attribute markers of a sentence directly through a classifier. The second stage is to generate a transferred sentence by combining the content tokens and the target style. We experiment on two benchmark datasets and evaluate context, style, fluency, and semantic. It is difficult to select the best system using only these automatic metrics, but it is possible to select stable systems. We consider only robust systems in all automatic evaluation metrics to be the minimum conditions that can be used in real applications. Many previous systems are difficult to use in certain situations because performance is significantly lower in several evaluation metrics. However, our system is stable in all automatic evaluation metrics and has results comparable to other models. Also, we compare the performance results of our system and the unstable system through human evaluation. Our code and data are available at the link (https://github.com/rungjoo/Stable-Style-Transformer).

</details>

<details>

<summary>2020-11-26 09:58:20 - SLURP: A Spoken Language Understanding Resource Package</summary>

- *Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, Verena Rieser*

- `2011.13205v1` - [abs](http://arxiv.org/abs/2011.13205v1) - [pdf](http://arxiv.org/pdf/2011.13205v1)

> Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp.

</details>

<details>

<summary>2020-11-26 10:10:57 - Encoding Syntactic Constituency Paths for Frame-Semantic Parsing with Graph Convolutional Networks</summary>

- *Emanuele Bastianelli, Andrea Vanzo, Oliver Lemon*

- `2011.13210v1` - [abs](http://arxiv.org/abs/2011.13210v1) - [pdf](http://arxiv.org/pdf/2011.13210v1)

> We study the problem of integrating syntactic information from constituency trees into a neural model in Frame-semantic parsing sub-tasks, namely Target Identification (TI), FrameIdentification (FI), and Semantic Role Labeling (SRL). We use a Graph Convolutional Network to learn specific representations of constituents, such that each constituent is profiled as the production grammar rule it corresponds to. We leverage these representations to build syntactic features for each word in a sentence, computed as the sum of all the constituents on the path between a word and a task-specific node in the tree, e.g. the target predicate for SRL. Our approach improves state-of-the-art results on the TI and SRL of ~1%and~3.5% points, respectively (+2.5% additional points are gained with BERT as input), when tested on FrameNet 1.5, while yielding comparable results on the CoNLL05 dataset to other syntax-aware systems.

</details>

<details>

<summary>2020-11-26 11:05:52 - MIMOS: A Deterministic Model for the Design and Update of Real-Time Systems</summary>

- *Wang Yi, Morteza Mohaqeqi, Susanne Graf*

- `2011.13234v1` - [abs](http://arxiv.org/abs/2011.13234v1) - [pdf](http://arxiv.org/pdf/2011.13234v1)

> Inspired by the pioneering work of Gilles Kahn on concurrent systems, we propose to model timed systems as a network of software components (implemented as real-time processes or tasks), each of which is specified to compute a collection of functions according to given timing constraints. We present a fixed-point semantics for this model which shows that each system function of such a network computes for a given set of (timed) input streams, a deterministic (timed) output stream. As a desired feature, such a network model can be modified by integrating new components for adding new system functions without changing the existing ones. Additionally, existing components may be replaced also by new ones fulfilling given requirements. Thanks to the deterministic semantics, a model-based approach is enabled for not only building systems but also updating them after deployment, allowing for efficient analysis techniques such as model-in-the-loop simulation to verify the complete behaviour of the updated system.

</details>

<details>

<summary>2020-11-26 13:28:50 - FlexiRepair: Transparent Program Repair with Generic Patches</summary>

- *Anil Koyuncu, Tegawendé F. Bissyandé, Jacques Klein, Yves Le Traon*

- `2011.13280v1` - [abs](http://arxiv.org/abs/2011.13280v1) - [pdf](http://arxiv.org/pdf/2011.13280v1)

> Template-based program repair research is in need for a common ground to express fix patterns in a standard and reusable manner. We propose to build on the concept of generic patch (also known as semantic patch), which is widely used in the Linux community to automate code evolution. We advocate that generic patches could provide at the same time a unified representation and a specification for fix patterns. Generic patches are indeed formally defined, and there exists a robust, industry-adapted, and extensible engine that processes generic patches to perform control-flow code matching and automatically generates concretes patches based on the specified change operations. In this paper, we present the design and implementation of a repair framework, FLEXIREPAIR, that explores generic patches as the core concept. In particular, we show how concretely generic patches can be inferred and applied in a pipeline of Automated Program Repair (APR). With FLEXIREPAIR, we address an urgent challenge in the template-based APR community to separate implementation details from actual scientific contributions by providing an open, transparent and flexible repair pipeline on top of which all advancements in terms of efficiency, efficacy and usability can be measured and assessed rigorously. Furthermore, because the underlying tools and concepts have already been accepted by a wide practitioner community, we expect FLEXIREPAIR's adoption by industry to be facilitated. Preliminary experiments with a prototype FLEXIREPAIR on the IntroClass and CodeFlaws benchmarks suggest that it already constitutes a solid baseline with comparable performance to some of the state of the art.

</details>

<details>

<summary>2020-11-26 15:28:58 - Towards Map-Based Validation of Semantic Segmentation Masks</summary>

- *Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Christian Bauckhage*

- `2011.08008v2` - [abs](http://arxiv.org/abs/2011.08008v2) - [pdf](http://arxiv.org/pdf/2011.08008v2)

> Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness. We propose to validate machine learning models for self-driving vehicles not only with given ground truth labels, but also with additional a-priori knowledge. In particular, we suggest to validate the drivable area in semantic segmentation masks using given street map data. We present first results, which indicate that prediction errors can be uncovered by map-based validation.

</details>

<details>

<summary>2020-11-26 17:26:42 - Depth-Aware Action Recognition: Pose-Motion Encoding through Temporal Heatmaps</summary>

- *Mattia Segu, Federico Pirovano, Gianmario Fumagalli, Amedeo Fabris*

- `2011.13399v1` - [abs](http://arxiv.org/abs/2011.13399v1) - [pdf](http://arxiv.org/pdf/2011.13399v1)

> Most state-of-the-art methods for action recognition rely only on 2D spatial features encoding appearance, motion or pose. However, 2D data lacks the depth information, which is crucial for recognizing fine-grained actions. In this paper, we propose a depth-aware volumetric descriptor that encodes pose and motion information in a unified representation for action classification in-the-wild. Our framework is robust to many challenges inherent to action recognition, e.g. variation in viewpoint, scene, clothing and body shape. The key component of our method is the Depth-Aware Pose Motion representation (DA-PoTion), a new video descriptor that encodes the 3D movement of semantic keypoints of the human body. Given a video, we produce human joint heatmaps for each frame using a state-of-the-art 3D human pose regressor and we give each of them a unique color code according to the relative time in the clip. Then, we aggregate such 3D time-encoded heatmaps for all human joints to obtain a fixed-size descriptor (DA-PoTion), which is suitable for classifying actions using a shallow 3D convolutional neural network (CNN). The DA-PoTion alone defines a new state-of-the-art on the Penn Action Dataset. Moreover, we leverage the intrinsic complementarity of our pose motion descriptor with appearance based approaches by combining it with Inflated 3D ConvNet (I3D) to define a new state-of-the-art on the JHMDB Dataset.

</details>

<details>

<summary>2020-11-27 06:13:56 - Chinese Medical Question Answer Matching Based on Interactive Sentence Representation Learning</summary>

- *Xiongtao Cui, Jungang Han*

- `2011.13573v1` - [abs](http://arxiv.org/abs/2011.13573v1) - [pdf](http://arxiv.org/pdf/2011.13573v1)

> Chinese medical question-answer matching is more challenging than the open-domain question answer matching in English. Even though the deep learning method has performed well in improving the performance of question answer matching, these methods only focus on the semantic information inside sentences, while ignoring the semantic association between questions and answers, thus resulting in performance deficits. In this paper, we design a series of interactive sentence representation learning models to tackle this problem. To better adapt to Chinese medical question-answer matching and take the advantages of different neural network structures, we propose the Crossed BERT network to extract the deep semantic information inside the sentence and the semantic association between question and answer, and then combine with the multi-scale CNNs network or BiGRU network to take the advantage of different structure of neural networks to learn more semantic features into the sentence representation. The experiments on the cMedQA V2.0 and cMedQA V1.0 dataset show that our model significantly outperforms all the existing state-of-the-art models of Chinese medical question answer matching.

</details>

<details>

<summary>2020-11-27 06:21:12 - Learning Relation Prototype from Unlabeled Texts for Long-tail Relation Extraction</summary>

- *Yixin Cao, Jun Kuang, Ming Gao, Aoying Zhou, Yonggang Wen, Tat-Seng Chua*

- `2011.13574v1` - [abs](http://arxiv.org/abs/2011.13574v1) - [pdf](http://arxiv.org/pdf/2011.13574v1)

> Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts.However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lackof sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypesfrom unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient trainingdata. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as theirproximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order andsecond-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs tocorresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or evenunseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information.We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision.Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1% F1 on average). Furtherresults on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study toinvestigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability.Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes will be released later.

</details>

<details>

<summary>2020-11-27 07:18:55 - Can 3D Adversarial Logos Cloak Humans?</summary>

- *Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit Bajaj, Zhangyang Wang*

- `2006.14655v2` - [abs](http://arxiv.org/abs/2006.14655v2) - [pdf](http://arxiv.org/pdf/2006.14655v2)

> With the trend of adversarial attacks, researchers attempt to fool trained object detectors in 2D scenes. Among many of them, an intriguing new form of attack with potential real-world usage is to append adversarial patches (e.g. logos) to images. Nevertheless, much less have we known about adversarial attacks from 3D rendering views, which is essential for the attack to be persistently strong in the physical world. This paper presents a new 3D adversarial logo attack: we construct an arbitrary shape logo from a 2D texture image and map this image into a 3D adversarial logo via a texture mapping called logo transformation. The resulting 3D adversarial logo is then viewed as an adversarial texture enabling easy manipulation of its shape and position. This greatly extends the versatility of adversarial training for computer graphics synthesized imagery. Contrary to the traditional adversarial patch, this new form of attack is mapped into the 3D object world and back-propagates to the 2D image domain through differentiable rendering. In addition, and unlike existing adversarial patches, our new 3D adversarial logo is shown to fool state-of-the-art deep object detectors robustly under model rotations, leading to one step further for realistic attacks in the physical world. Our codes are available at https://github.com/TAMU-VITA/3D_Adversarial_Logo.

</details>

<details>

<summary>2020-11-27 07:33:11 - Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles</summary>

- *Yafu Tian, Alexander Carballo, Ruifeng Li, Kazuya Takeda*

- `2011.13588v1` - [abs](http://arxiv.org/abs/2011.13588v1) - [pdf](http://arxiv.org/pdf/2011.13588v1)

> Rich semantic information extraction plays a vital role on next-generation intelligent vehicles. Currently there is great amount of research focusing on fundamental applications such as 6D pose detection, road scene semantic segmentation, etc. And this provides us a great opportunity to think about how shall these data be organized and exploited.   In this paper we propose road scene graph,a special scene-graph for intelligent vehicles. Different to classical data representation, this graph provides not only object proposals but also their pair-wise relationships. By organizing them in a topological graph, these data are explainable, fully-connected, and could be easily processed by GCNs (Graph Convolutional Networks). Here we apply scene graph on roads using our Road Scene Graph dataset, including the basic graph prediction model. This work also includes experimental evaluations using the proposed model.

</details>

<details>

<summary>2020-11-27 08:00:54 - An anatomically-informed 3D CNN for brain aneurysm classification with weak labels</summary>

- *Tommaso Di Noto, Guillaume Marie, Sébastien Tourbier, Yasser Alemán-Gómez, Guillaume Saliou, Meritxell Bach Cuadra, Patric Hagmann, Jonas Richiardi*

- `2012.08645v1` - [abs](http://arxiv.org/abs/2012.08645v1) - [pdf](http://arxiv.org/pdf/2012.08645v1)

> A commonly adopted approach to carry out detection tasks in medical imaging is to rely on an initial segmentation. However, this approach strongly depends on voxel-wise annotations which are repetitive and time-consuming to draw for medical experts. An interesting alternative to voxel-wise masks are so-called "weak" labels: these can either be coarse or oversized annotations that are less precise, but noticeably faster to create. In this work, we address the task of brain aneurysm detection as a patch-wise binary classification with weak labels, in contrast to related studies that rather use supervised segmentation methods and voxel-wise delineations. Our approach comes with the non-trivial challenge of the data set creation: as for most focal diseases, anomalous patches (with aneurysm) are outnumbered by those showing no anomaly, and the two classes usually have different spatial distributions. To tackle this frequent scenario of inherently imbalanced, spatially skewed data sets, we propose a novel, anatomically-driven approach by using a multi-scale and multi-input 3D Convolutional Neural Network (CNN). We apply our model to 214 subjects (83 patients, 131 controls) who underwent Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) and presented a total of 111 unruptured cerebral aneurysms. We compare two strategies for negative patch sampling that have an increasing level of difficulty for the network and we show how this choice can strongly affect the results. To assess whether the added spatial information helps improving performances, we compare our anatomically-informed CNN with a baseline, spatially-agnostic CNN. When considering the more realistic and challenging scenario including vessel-like negative patches, the former model attains the highest classification results (accuracy$\simeq$95\%, AUROC$\simeq$0.95, AUPR$\simeq$0.71), thus outperforming the baseline.

</details>

<details>

<summary>2020-11-27 08:08:46 - Planting trees at the right places: Recommending suitable sites for growing trees using algorithm fusion</summary>

- *Pushpendra Rana, Lav R Varshney*

- `2009.08002v2` - [abs](http://arxiv.org/abs/2009.08002v2) - [pdf](http://arxiv.org/pdf/2009.08002v2)

> Large-scale planting of trees has been proposed as a low-cost natural solution for carbon mitigation, but is hampered by poor selection of plantation sites, especially in developing countries. To aid in site selection, we develop the ePSA (e-Plantation Site Assistant) recommendation system based on algorithm fusion that combines physics-based/traditional forestry science knowledge with machine learning. ePSA assists forest range officers by identifying blank patches inside forest areas and ranking each such patch based on their tree growth potential. Experiments, user studies, and deployment results characterize the utility of the recommender system in shaping the long-term success of tree plantations as a nature climate solution for carbon mitigation in northern India and beyond.

</details>

<details>

<summary>2020-11-27 10:43:04 - Relation Clustering in Narrative Knowledge Graphs</summary>

- *Simone Mellace, K Vani, Alessandro Antonucci*

- `2011.13647v1` - [abs](http://arxiv.org/abs/2011.13647v1) - [pdf](http://arxiv.org/pdf/2011.13647v1)

> When coping with literary texts such as novels or short stories, the extraction of structured information in the form of a knowledge graph might be hindered by the huge number of possible relations between the entities corresponding to the characters in the novel and the consequent hurdles in gathering supervised information about them. Such issue is addressed here as an unsupervised task empowered by transformers: relational sentences in the original text are embedded (with SBERT) and clustered in order to merge together semantically similar relations. All the sentences in the same cluster are finally summarized (with BART) and a descriptive label extracted from the summary. Preliminary tests show that such clustering might successfully detect similar relations, and provide a valuable preprocessing for semi-supervised approaches.

</details>

<details>

<summary>2020-11-27 11:58:32 - Automated acquisition of structured, semantic models of manipulation activities from human VR demonstration</summary>

- *Andrei Haidu, Michael Beetz*

- `2011.13689v1` - [abs](http://arxiv.org/abs/2011.13689v1) - [pdf](http://arxiv.org/pdf/2011.13689v1)

> In this paper we present a system capable of collecting and annotating, human performed, robot understandable, everyday activities from virtual environments. The human movements are mapped in the simulated world using off-the-shelf virtual reality devices with full body, and eye tracking capabilities. All the interactions in the virtual world are physically simulated, thus movements and their effects are closely relatable to the real world. During the activity execution, a subsymbolic data logger is recording the environment and the human gaze on a per-frame basis, enabling offline scene reproduction and replays. Coupled with the physics engine, online monitors (symbolic data loggers) are parsing (using various grammars) and recording events, actions, and their effects in the simulated world.

</details>

<details>

<summary>2020-11-27 18:46:34 - Teaching the Machine to Explain Itself using Domain Knowledge</summary>

- *Vladimir Balayan, Pedro Saleiro, Catarina Belém, Ludwig Krippahl, Pedro Bizarro*

- `2012.01932v1` - [abs](http://arxiv.org/abs/2012.01932v1) - [pdf](http://arxiv.org/pdf/2012.01932v1)

> Machine Learning (ML) has been increasingly used to aid humans to make better and faster decisions. However, non-technical humans-in-the-loop struggle to comprehend the rationale behind model predictions, hindering trust in algorithmic decision-making systems. Considerable research work on AI explainability attempts to win back trust in AI systems by developing explanation methods but there is still no major breakthrough. At the same time, popular explanation methods (e.g., LIME, and SHAP) produce explanations that are very hard to understand for non-data scientist persona. To address this, we present JOEL, a neural network-based framework to jointly learn a decision-making task and associated explanations that convey domain knowledge. JOEL is tailored to human-in-the-loop domain experts that lack deep technical ML knowledge, providing high-level insights about the model's predictions that very much resemble the experts' own reasoning. Moreover, we collect the domain feedback from a pool of certified experts and use it to ameliorate the model (human teaching), hence promoting seamless and better suited explanations. Lastly, we resort to semantic mappings between legacy expert systems and domain taxonomies to automatically annotate a bootstrap training set, overcoming the absence of concept-based human annotations. We validate JOEL empirically on a real-world fraud detection dataset. We show that JOEL can generalize the explanations from the bootstrap dataset. Furthermore, obtained results indicate that human teaching can further improve the explanations prediction quality by approximately $13.57\%$.

</details>

<details>

<summary>2020-11-27 21:22:14 - Interpretable Multi-Headed Attention for Abstractive Summarization at Controllable Lengths</summary>

- *Ritesh Sarkhel, Moniba Keymanesh, Arnab Nandi, Srinivasan Parthasarathy*

- `2002.07845v2` - [abs](http://arxiv.org/abs/2002.07845v2) - [pdf](http://arxiv.org/pdf/2002.07845v2)

> Abstractive summarization at controllable lengths is a challenging task in natural language processing. It is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand. At the same time, when it comes to trusting machine-generated summaries, explaining how a summary was constructed in human-understandable terms may be critical. We propose Multi-level Summarizer (MLS), a supervised method to construct abstractive summaries of a text document at controllable lengths. The key enabler of our method is an interpretable multi-headed attention mechanism that computes attention distribution over an input document using an array of timestep independent semantic kernels. Each kernel optimizes a human-interpretable syntactic or semantic property. Exhaustive experiments on two low-resource datasets in the English language show that MLS outperforms strong baselines by up to 14.70% in the METEOR score. Human evaluation of the summaries also suggests that they capture the key concepts of the document at various length-budgets.

</details>

<details>

<summary>2020-11-28 15:01:53 - i3DMM: Deep Implicit 3D Morphable Model of Human Heads</summary>

- *Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt*

- `2011.14143v1` - [abs](http://arxiv.org/abs/2011.14143v1) - [pdf](http://arxiv.org/pdf/2011.14143v1)

> We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.

</details>

<details>

<summary>2020-11-29 04:34:39 - Intrinsic Knowledge Evaluation on Chinese Language Models</summary>

- *Zhiruo Wang, Renfen Hu*

- `2011.14277v1` - [abs](http://arxiv.org/abs/2011.14277v1) - [pdf](http://arxiv.org/pdf/2011.14277v1)

> Recent NLP tasks have benefited a lot from pre-trained language models (LM) since they are able to encode knowledge of various aspects. However, current LM evaluations focus on downstream performance, hence lack to comprehensively inspect in which aspect and to what extent have they encoded knowledge. This paper addresses both queries by proposing four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of $39,308$ questions covering both linguistic and world knowledge in Chinese. Throughout experiments, our probes and knowledge data prove to be a reliable benchmark for evaluating pre-trained Chinese LMs. Our work is publicly available at https://github.com/ZhiruoWang/ChnEval.

</details>

<details>

<summary>2020-11-29 04:59:08 - 3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction</summary>

- *Rupal Agravat, Mehul S Raval*

- `2008.11576v2` - [abs](http://arxiv.org/abs/2008.11576v2) - [pdf](http://arxiv.org/pdf/2008.11576v2)

> Glioma, the malignant brain tumor, requires immediate treatment to improve the survival of patients. Gliomas heterogeneous nature makes the segmentation difficult, especially for sub-regions like necrosis, enhancing tumor, non-enhancing tumor, and Edema. Deep neural networks like full convolution neural networks and ensemble of fully convolution neural networks are successful for Glioma segmentation. The paper demonstrates the use of a 3D fully convolution neural network with a three layer encoder decoder approach for layer arrangement. The encoder blocks include the dense modules, and decoder blocks include convolution modules. The input to the network is 3D patches. The loss function combines dice loss and focal loss functions. The validation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing tumor, whole tumor, and tumor core, respectively. The Random Forest Regressor uses shape, volumetric, and age features extracted from ground truth for overall survival prediction. The regressor achieves an accuracy of 44.8% on the validation set.

</details>

<details>

<summary>2020-11-29 10:04:27 - Why Not Simply Translate? A First Swedish Evaluation Benchmark for Semantic Similarity</summary>

- *Tim Isbister, Magnus Sahlgren*

- `2009.03116v2` - [abs](http://arxiv.org/abs/2009.03116v2) - [pdf](http://arxiv.org/pdf/2009.03116v2)

> This paper presents the first Swedish evaluation benchmark for textual semantic similarity. The benchmark is compiled by simply running the English STS-B dataset through the Google machine translation API. This paper discusses potential problems with using such a simple approach to compile a Swedish evaluation benchmark, including translation errors, vocabulary variation, and productive compounding. Despite some obvious problems with the resulting dataset, we use the benchmark to compare the majority of the currently existing Swedish text representations, demonstrating that native models outperform multilingual ones, and that simple bag of words performs remarkably well.

</details>

<details>

<summary>2020-11-29 11:36:13 - Generative Pre-training for Paraphrase Generation by Representing and Predicting Spans in Exemplars</summary>

- *Tien-Cuong Bui, Van-Duc Le, Hai-Thien To, Sang Kyun Cha*

- `2011.14344v1` - [abs](http://arxiv.org/abs/2011.14344v1) - [pdf](http://arxiv.org/pdf/2011.14344v1)

> Paraphrase generation is a long-standing problem and serves an essential role in many natural language processing problems. Despite some encouraging results, recent methods either confront the problem of favoring generic utterance or need to retrain the model from scratch for each new dataset. This paper presents a novel approach to paraphrasing sentences, extended from the GPT-2 model. We develop a template masking technique, named first-order masking, to masked out irrelevant words in exemplars utilizing POS taggers. So that, the paraphrasing task is changed to predicting spans in masked templates. Our proposed approach outperforms competitive baselines, especially in the semantic preservation aspect. To prevent the model from being biased towards a given template, we introduce a technique, referred to as second-order masking, which utilizes Bernoulli distribution to control the visibility of the first-order-masked template's tokens. Moreover, this technique allows the model to provide various paraphrased sentences in testing by adjusting the second-order-masking level. For scale-up objectives, we compare the performance of two alternatives template-selection methods, which shows that they were equivalent in preserving semantic information.

</details>

<details>

<summary>2020-11-29 20:47:06 - Causal Discovery in Physical Systems from Videos</summary>

- *Yunzhu Li, Antonio Torralba, Animashree Anandkumar, Dieter Fox, Animesh Garg*

- `2007.00631v3` - [abs](http://arxiv.org/abs/2007.00631v3) - [pdf](http://arxiv.org/pdf/2007.00631v3)

> Causal discovery is at the core of human cognition. It enables us to reason about the environment and make counterfactual predictions about unseen scenarios that can vastly differ from our previous experiences. We consider the task of causal discovery from videos in an end-to-end fashion without supervision on the ground-truth graph structure. In particular, our goal is to discover the structural dependencies among environmental and object variables: inferring the type and strength of interactions that have a causal effect on the behavior of the dynamical system. Our model consists of (a) a perception module that extracts a semantically meaningful and temporally consistent keypoint representation from images, (b) an inference module for determining the graph distribution induced by the detected keypoints, and (c) a dynamics module that can predict the future by conditioning on the inferred graph. We assume access to different configurations and environmental conditions, i.e., data from unknown interventions on the underlying system; thus, we can hope to discover the correct underlying causal graph without explicit interventions. We evaluate our method in a planar multi-body interaction environment and scenarios involving fabrics of different shapes like shirts and pants. Experiments demonstrate that our model can correctly identify the interactions from a short sequence of images and make long-term future predictions. The causal structure assumed by the model also allows it to make counterfactual predictions and extrapolate to systems of unseen interaction graphs or graphs of various sizes.

</details>

<details>

<summary>2020-11-29 22:51:25 - Improved Semantic Role Labeling using Parameterized Neighborhood Memory Adaptation</summary>

- *Ishan Jindal, Ranit Aharonov, Siddhartha Brahma, Huaiyu Zhu, Yunyao Li*

- `2011.14459v1` - [abs](http://arxiv.org/abs/2011.14459v1) - [pdf](http://arxiv.org/pdf/2011.14459v1)

> Deep neural models achieve some of the best results for semantic role labeling. Inspired by instance-based learning that utilizes nearest neighbors to handle low-frequency context-specific training samples, we investigate the use of memory adaptation techniques in deep neural models. We propose a parameterized neighborhood memory adaptive (PNMA) method that uses a parameterized representation of the nearest neighbors of tokens in a memory of activations and makes predictions based on the most similar samples in the training data. We empirically show that PNMA consistently improves the SRL performance of the base model irrespective of types of word embeddings. Coupled with contextualized word embeddings derived from BERT, PNMA improves over existing models for both span and dependency semantic parsing datasets, especially on out-of-domain text, reaching F1 scores of 80.2, and 84.97 on CoNLL2005, and CoNLL2009 datasets, respectively.

</details>

<details>

<summary>2020-11-30 01:41:15 - Distortion-aware Monocular Depth Estimation for Omnidirectional Images</summary>

- *Hong-Xiang Chen, Kunhong Li, Zhiheng Fu, Mengyi Liu, Zonghao Chen, Yulan Guo*

- `2010.08942v2` - [abs](http://arxiv.org/abs/2010.08942v2) - [pdf](http://arxiv.org/pdf/2010.08942v2)

> A main challenge for tasks on panorama lies in the distortion of objects among images. In this work, we propose a Distortion-Aware Monocular Omnidirectional (DAMO) dense depth estimation network to address this challenge on indoor panoramas with two steps. First, we introduce a distortion-aware module to extract calibrated semantic features from omnidirectional images. Specifically, we exploit deformable convolution to adjust its sampling grids to geometric variations of distorted objects on panoramas and then utilize a strip pooling module to sample against horizontal distortion introduced by inverse gnomonic projection. Second, we further introduce a plug-and-play spherical-aware weight matrix for our objective function to handle the uneven distribution of areas projected from a sphere. Experiments on the 360D dataset show that the proposed method can effectively extract semantic features from distorted panoramas and alleviate the supervision bias caused by distortion. It achieves state-of-the-art performance on the 360D dataset with high efficiency.

</details>

<details>

<summary>2020-11-30 02:07:07 - Training and Inference for Integer-Based Semantic Segmentation Network</summary>

- *Jiayi Yang, Lei Deng, Yukuan Yang, Yuan Xie, Guoqi Li*

- `2011.14504v1` - [abs](http://arxiv.org/abs/2011.14504v1) - [pdf](http://arxiv.org/pdf/2011.14504v1)

> Semantic segmentation has been a major topic in research and industry in recent years. However, due to the computation complexity of pixel-wise prediction and backpropagation algorithm, semantic segmentation has been demanding in computation resources, resulting in slow training and inference speed and large storage space to store models. Existing schemes that speed up segmentation network change the network structure and come with noticeable accuracy degradation. However, neural network quantization can be used to reduce computation load while maintaining comparable accuracy and original network structure. Semantic segmentation networks are different from traditional deep convolutional neural networks (DCNNs) in many ways, and this topic has not been thoroughly explored in existing works. In this paper, we propose a new quantization framework for training and inference of segmentation networks, where parameters and operations are constrained to 8-bit integer-based values for the first time. Full quantization of the data flow and the removal of square and root operations in batch normalization give our framework the ability to perform inference on fixed-point devices. Our proposed framework is evaluated on mainstream semantic segmentation networks like FCN-VGG16 and DeepLabv3-ResNet50, achieving comparable accuracy against floating-point framework on ADE20K dataset and PASCAL VOC 2012 dataset.

</details>

<details>

<summary>2020-11-30 06:27:08 - DiCENet: Dimension-wise Convolutions for Efficient Networks</summary>

- *Sachin Mehta, Hannaneh Hajishirzi, Mohammad Rastegari*

- `1906.03516v3` - [abs](http://arxiv.org/abs/1906.03516v3) - [pdf](http://arxiv.org/pdf/1906.03516v3)

> We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4% higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet. Our source code in PyTorch is open-source and is available at https://github.com/sacmehta/EdgeNets/

</details>

<details>

<summary>2020-11-30 07:11:11 - ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition</summary>

- *Hsin-Pai Cheng, Feng Liang, Meng Li, Bowen Cheng, Feng Yan, Hai Li, Vikas Chandra, Yiran Chen*

- `2011.14584v1` - [abs](http://arxiv.org/abs/2011.14584v1) - [pdf](http://arxiv.org/pdf/2011.14584v1)

> Scale variance among different sizes of body parts and objects is a challenging problem for visual recognition tasks. Existing works usually design dedicated backbone or apply Neural architecture Search(NAS) for each task to tackle this challenge. However, existing works impose significant limitations on the design or search space. To solve these problems, we present ScaleNAS, a one-shot learning method for exploring scale-aware representations. ScaleNAS solves multiple tasks at a time by searching multi-scale feature aggregation. ScaleNAS adopts a flexible search space that allows an arbitrary number of blocks and cross-scale feature fusions. To cope with the high search cost incurred by the flexible space, ScaleNAS employs one-shot learning for multi-scale supernet driven by grouped sampling and evolutionary search. Without further retraining, ScaleNet can be directly deployed for different visual recognition tasks with superior performance. We use ScaleNAS to create high-resolution models for two different tasks, ScaleNet-P for human pose estimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S outperform existing manually crafted and NAS-based methods in both tasks. When applying ScaleNet-P to bottom-up human pose estimation, it surpasses the state-of-the-art HigherHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on COCO test-dev, achieving new state-of-the-art result.

</details>

<details>

<summary>2020-11-30 10:41:50 - UWB @ DIACR-Ita: Lexical Semantic Change Detection with CCA and Orthogonal Transformation</summary>

- *Ondřej Pražák, Pavel Přibáň, Stephen Taylor*

- `2011.14678v1` - [abs](http://arxiv.org/abs/2011.14678v1) - [pdf](http://arxiv.org/pdf/2011.14678v1)

> In this paper, we describe our method for detection of lexical semantic change (i.e., word sense changes over time) for the DIACR-Ita shared task, where we ranked $1^{st}$. We examine semantic differences between specific words in two Italian corpora, chosen from different time periods. Our method is fully unsupervised and language independent. It consists of preparing a semantic vector space for each corpus, earlier and later. Then we compute a linear transformation between earlier and later spaces, using CCA and Orthogonal Transformation. Finally, we measure the cosines between the transformed vectors.

</details>

<details>

<summary>2020-11-30 10:47:45 - UWB at SemEval-2020 Task 1: Lexical Semantic Change Detection</summary>

- *Ondřej Pražák, Pavel Přibáň, Stephen Taylor, Jakub Sido*

- `2012.00004v1` - [abs](http://arxiv.org/abs/2012.00004v1) - [pdf](http://arxiv.org/pdf/2012.00004v1)

> In this paper, we describe our method for the detection of lexical semantic change, i.e., word sense changes over time. We examine semantic differences between specific words in two corpora, chosen from different time periods, for English, German, Latin, and Swedish. Our method was created for the SemEval 2020 Task 1: \textit{Unsupervised Lexical Semantic Change Detection.} We ranked $1^{st}$ in Sub-task 1: binary change detection, and $4^{th}$ in Sub-task 2: ranked change detection. Our method is fully unsupervised and language independent. It consists of preparing a semantic vector space for each corpus, earlier and later; computing a linear transformation between earlier and later spaces, using Canonical Correlation Analysis and Orthogonal Transformation; and measuring the cosines between the transformed vector for the target word from the earlier corpus and the vector for the target word in the later corpus.

</details>

<details>

<summary>2020-11-30 11:00:45 - Two-Level Adversarial Visual-Semantic Coupling for Generalized Zero-shot Learning</summary>

- *Shivam Chandhok, Vineeth N Balasubramanian*

- `2007.07757v2` - [abs](http://arxiv.org/abs/2007.07757v2) - [pdf](http://arxiv.org/pdf/2007.07757v2)

> The performance of generative zero-shot methods mainly depends on the quality of generated features and how well the model facilitates knowledge transfer between visual and semantic domains. The quality of generated features is a direct consequence of the ability of the model to capture the several modes of the underlying data distribution. To address these issues, we propose a new two-level joint maximization idea to augment the generative network with an inference network during training which helps our model capture the several modes of the data and generate features that better represent the underlying data distribution. This provides strong cross-modal interaction for effective transfer of knowledge between visual and semantic domains. Furthermore, existing methods train the zero-shot classifier either on generate synthetic image features or latent embeddings produced by leveraging representation learning. In this work, we unify these paradigms into a single model which in addition to synthesizing image features, also utilizes the representation learning capabilities of the inference network to provide discriminative features for the final zero-shot recognition task. We evaluate our approach on four benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several state-of-the-art methods, and show its performance. We also perform ablation studies to analyze and understand our method more carefully for the Generalized Zero-shot Learning task.

</details>

<details>

<summary>2020-11-30 14:41:44 - Improving Generalization of Transformer for Speech Recognition with Parallel Schedule Sampling and Relative Positional Embedding</summary>

- *Pan Zhou, Ruchao Fan, Wei Chen, Jia Jia*

- `1911.00203v2` - [abs](http://arxiv.org/abs/1911.00203v2) - [pdf](http://arxiv.org/pdf/1911.00203v2)

> Transformer has shown promising results in many sequence to sequence transformation tasks recently. It utilizes a number of feed-forward self-attention layers to replace the recurrent neural networks (RNN) in attention-based encoder decoder (AED) architecture. Self-attention layer learns temporal dependence by incorporating sinusoidal positional embedding of tokens in a sequence for parallel computing. Quicker iteration speed in training than sequential operation of RNN can be obtained. Deeper layers of the transformer also make it perform better than RNN-based AED. However, this parallelization ability is lost when applying scheduled sampling training. Self-attention with sinusoidal positional embedding may cause performance degradations for longer sequences that have similar acoustic or semantic information at different positions as well. To address these problems, we propose to use parallel scheduled sampling (PSS) and relative positional embedding (RPE) to help the transformer generalize to unseen data. Our proposed methods achieve a 7% relative improvement for short utterances and a 70% relative gain for long utterances on a 10,000-hour Mandarin ASR task.

</details>

<details>

<summary>2020-11-30 15:03:47 - A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources</summary>

- *Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, Philip S. Yu*

- `2011.14867v1` - [abs](http://arxiv.org/abs/2011.14867v1) - [pdf](http://arxiv.org/pdf/2011.14867v1)

> Heterogeneous graphs (HGs) also known as heterogeneous information networks have become ubiquitous in real-world scenarios; therefore, HG embedding, which aims to learn representations in a lower-dimension space while preserving the heterogeneous structures and semantics for downstream tasks (e.g., node/graph classification, node clustering, link prediction), has drawn considerable attentions in recent years. In this survey, we perform a comprehensive review of the recent development on HG embedding methods and techniques. We first introduce the basic concepts of HG and discuss the unique challenges brought by the heterogeneity for HG embedding in comparison with homogeneous graph representation learning; and then we systemically survey and categorize the state-of-the-art HG embedding methods based on the information they used in the learning process to address the challenges posed by the HG heterogeneity. In particular, for each representative HG embedding method, we provide detailed introduction and further analyze its pros and cons; meanwhile, we also explore the transformativeness and applicability of different types of HG embedding methods in the real-world industrial environments for the first time. In addition, we further present several widely deployed systems that have demonstrated the success of HG embedding techniques in resolving real-world application problems with broader impacts. To facilitate future research and applications in this area, we also summarize the open-source code, existing graph learning platforms and benchmark datasets. Finally, we explore the additional issues and challenges of HG embedding and forecast the future research directions in this field.

</details>

<details>

<summary>2020-11-30 17:05:58 - Fast, Self Supervised, Fully Convolutional Color Normalization of H&E Stained Images</summary>

- *Abhijeet Patil, Mohd. Talha, Aniket Bhatia, Nikhil Cherian Kurian, Sammed Mangale, Sunil Patel, Amit Sethi*

- `2011.15000v1` - [abs](http://arxiv.org/abs/2011.15000v1) - [pdf](http://arxiv.org/pdf/2011.15000v1)

> Performance of deep learning algorithms decreases drastically if the data distributions of the training and testing sets are different. Due to variations in staining protocols, reagent brands, and habits of technicians, color variation in digital histopathology images is quite common. Color variation causes problems for the deployment of deep learning-based solutions for automatic diagnosis system in histopathology. Previously proposed color normalization methods consider a small patch as a reference for normalization, which creates artifacts on out-of-distribution source images. These methods are also slow as most of the computation is performed on CPUs instead of the GPUs. We propose a color normalization technique, which is fast during its self-supervised training as well as inference. Our method is based on a lightweight fully-convolutional neural network and can be easily attached to a deep learning-based pipeline as a pre-processing block. For classification and segmentation tasks on CAMELYON17 and MoNuSeg datasets respectively, the proposed method is faster and gives a greater increase in accuracy than the state of the art methods.

</details>

<details>

<summary>2020-11-30 17:39:44 - A Framework for Authorial Clustering of Shorter Texts in Latent Semantic Spaces</summary>

- *Rafi Trad, Myra Spiliopoulou*

- `2011.15038v1` - [abs](http://arxiv.org/abs/2011.15038v1) - [pdf](http://arxiv.org/pdf/2011.15038v1)

> Authorial clustering involves the grouping of documents written by the same author or team of authors without any prior positive examples of an author's writing style or thematic preferences. For authorial clustering on shorter texts (paragraph-length texts that are typically shorter than conventional documents), the document representation is particularly important: very high-dimensional feature spaces lead to data sparsity and suffer from serious consequences like the curse of dimensionality, while feature selection may lead to information loss. We propose a high-level framework which utilizes a compact data representation in a latent feature space derived with non-parametric topic modeling. Authorial clusters are identified thereafter in two scenarios: (a) fully unsupervised and (b) semi-supervised where a small number of shorter texts are known to belong to the same author (must-link constraints) or not (cannot-link constraints). We report on experiments with 120 collections in three languages and two genres and show that the topic-based latent feature space provides a promising level of performance while reducing the dimensionality by a factor of 1500 compared to state-of-the-arts. We also demonstrate that, while prior knowledge on the precise number of authors (i.e. authorial clusters) does not contribute much to additional quality, little knowledge on constraints in authorial clusters memberships leads to clear performance improvements in front of this difficult task. Thorough experimentation with standard metrics indicates that there still remains an ample room for improvement for authorial clustering, especially with shorter texts

</details>


## 2020-12

<details>

<summary>2020-12-01 10:55:07 - Introducing Inter-Relatedness between Wikipedia Articles in Explicit Semantic Analysis</summary>

- *Naveen Elango, Pawan Prasad K*

- `2012.00398v1` - [abs](http://arxiv.org/abs/2012.00398v1) - [pdf](http://arxiv.org/pdf/2012.00398v1)

> Explicit Semantic Analysis (ESA) is a technique used to represent a piece of text as a vector in the space of concepts, such as Articles found in Wikipedia. We propose a methodology to incorporate knowledge of Inter-relatedness between Wikipedia Articles to the vectors obtained from ESA using a technique called Retrofitting to improve the performance of subsequent tasks that use ESA to form vector embeddings. Especially we use an undirected Graph to represent this knowledge with nodes as Articles and edges as inter relations between two Articles. Here, we also emphasize how the ESA step could be seen as a predominantly bottom-up approach using a corpus to come up with vector representations and the incorporation of top-down knowledge which is the relations between Articles to further improve it. We test our hypothesis on several smaller subsets of the Wikipedia corpus and show that our proposed methodology leads to decent improvements in performance measures including Spearman's Rank correlation coefficient in most cases.

</details>

<details>

<summary>2020-12-01 12:02:44 - Learning as Abduction: Trainable Natural Logic Theorem Prover for Natural Language Inference</summary>

- *Lasha Abzianidze*

- `2010.15909v2` - [abs](http://arxiv.org/abs/2010.15909v2) - [pdf](http://arxiv.org/pdf/2010.15909v2)

> Tackling Natural Language Inference with a logic-based method is becoming less and less common. While this might have been counterintuitive several decades ago, nowadays it seems pretty obvious. The main reasons for such a conception are that (a) logic-based methods are usually brittle when it comes to processing wide-coverage texts, and (b) instead of automatically learning from data, they require much of manual effort for development. We make a step towards to overcome such shortcomings by modeling learning from data as abduction: reversing a theorem-proving procedure to abduce semantic relations that serve as the best explanation for the gold label of an inference problem. In other words, instead of proving sentence-level inference relations with the help of lexical relations, the lexical relations are proved taking into account the sentence-level inference relations. We implement the learning method in a tableau theorem prover for natural language and show that it improves the performance of the theorem prover on the SICK dataset by 1.4% while still maintaining high precision (>94%). The obtained results are competitive with the state of the art among logic-based systems.

</details>

<details>

<summary>2020-12-01 15:02:11 - SeMantic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web Challenge</summary>

- *Nandana Mihindukulasooriya, Mohnish Dubey, Alfio Gliozzo, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Ricardo Usbeck*

- `2012.00555v1` - [abs](http://arxiv.org/abs/2012.00555v1) - [pdf](http://arxiv.org/pdf/2012.00555v1)

> Each year the International Semantic Web Conference accepts a set of Semantic Web Challenges to establish competitions that will advance the state of the art solutions in any given problem domain. The SeMantic AnsweR Type prediction task (SMART) was part of ISWC 2020 challenges. Question type and answer type prediction can play a key role in knowledge base question answering systems providing insights that are helpful to generate correct queries or rank the answer candidates. More concretely, given a question in natural language, the task of SMART challenge is, to predict the answer type using a target ontology (e.g., DBpedia or Wikidata).

</details>

<details>

<summary>2020-12-01 15:25:47 - Denoising Pre-Training and Data Augmentation Strategies for Enhanced RDF Verbalization with Transformers</summary>

- *Sebastien Montella, Betty Fabre, Tanguy Urvoy, Johannes Heinecke, Lina Rojas-Barahona*

- `2012.00571v1` - [abs](http://arxiv.org/abs/2012.00571v1) - [pdf](http://arxiv.org/pdf/2012.00571v1)

> The task of verbalization of RDF triples has known a growth in popularity due to the rising ubiquity of Knowledge Bases (KBs). The formalism of RDF triples is a simple and efficient way to store facts at a large scale. However, its abstract representation makes it difficult for humans to interpret. For this purpose, the WebNLG challenge aims at promoting automated RDF-to-text generation. We propose to leverage pre-trainings from augmented data with the Transformer model using a data augmentation strategy. Our experiment results show a minimum relative increases of 3.73%, 126.05% and 88.16% in BLEU score for seen categories, unseen entities and unseen categories respectively over the standard training.

</details>

<details>

<summary>2020-12-01 15:53:57 - The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling</summary>

- *Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Evgeny Kharitonov, Alexei Baevski, Ewan Dunbar, Emmanuel Dupoux*

- `2011.11588v2` - [abs](http://arxiv.org/abs/2011.11588v2) - [pdf](http://arxiv.org/pdf/2011.11588v2)

> We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.

</details>

<details>

<summary>2020-12-01 16:37:18 - Overcoming the limitations of patch-based learning to detect cancer in whole slide images</summary>

- *Ozan Ciga, Tony Xu, Sharon Nofech-Mozes, Shawna Noy, Fang-I Lu, Anne L. Martel*

- `2012.00617v1` - [abs](http://arxiv.org/abs/2012.00617v1) - [pdf](http://arxiv.org/pdf/2012.00617v1)

> Whole slide images (WSIs) pose unique challenges when training deep learning models. They are very large which makes it necessary to break each image down into smaller patches for analysis, image features have to be extracted at multiple scales in order to capture both detail and context, and extreme class imbalances may exist. Significant progress has been made in the analysis of these images, thanks largely due to the availability of public annotated datasets. We postulate, however, that even if a method scores well on a challenge task, this success may not translate to good performance in a more clinically relevant workflow. Many datasets consist of image patches which may suffer from data curation bias; other datasets are only labelled at the whole slide level and the lack of annotations across an image may mask erroneous local predictions so long as the final decision is correct. In this paper, we outline the differences between patch or slide-level classification versus methods that need to localize or segment cancer accurately across the whole slide, and we experimentally verify that best practices differ in both cases. We apply a binary cancer detection network on post neoadjuvant therapy breast cancer WSIs to find the tumor bed outlining the extent of cancer, a task which requires sensitivity and precision across the whole slide. We extensively study multiple design choices and their effects on the outcome, including architectures and augmentations. Furthermore, we propose a negative data sampling strategy, which drastically reduces the false positive rate (7% on slide level) and improves each metric pertinent to our problem, with a 15% reduction in the error of tumor extent.

</details>

<details>

<summary>2020-12-01 16:58:01 - Meta-Embeddings for Natural Language Inference and Semantic Similarity tasks</summary>

- *Shree Charran R, Rahul Kumar Dubey*

- `2012.00633v1` - [abs](http://arxiv.org/abs/2012.00633v1) - [pdf](http://arxiv.org/pdf/2012.00633v1)

> Word Representations form the core component for almost all advanced Natural Language Processing (NLP) applications such as text mining, question-answering, and text summarization, etc. Over the last two decades, immense research is conducted to come up with one single model to solve all major NLP tasks. The major problem currently is that there are a plethora of choices for different NLP tasks. Thus for NLP practitioners, the task of choosing the right model to be used itself becomes a challenge. Thus combining multiple pre-trained word embeddings and forming meta embeddings has become a viable approach to improve tackle NLP tasks. Meta embedding learning is a process of producing a single word embedding from a given set of pre-trained input word embeddings. In this paper, we propose to use Meta Embedding derived from few State-of-the-Art (SOTA) models to efficiently tackle mainstream NLP tasks like classification, semantic relatedness, and text similarity. We have compared both ensemble and dynamic variants to identify an efficient approach. The results obtained show that even the best State-of-the-Art models can be bettered. Thus showing us that meta-embeddings can be used for several NLP tasks by harnessing the power of several individual representations.

</details>

<details>

<summary>2020-12-02 14:19:19 - Sequence Generation using Deep Recurrent Networks and Embeddings: A study case in music</summary>

- *Sebastian Garcia-Valencia, Alejandro Betancourt, Juan G. Lalinde-Pulido*

- `2012.01231v1` - [abs](http://arxiv.org/abs/2012.01231v1) - [pdf](http://arxiv.org/pdf/2012.01231v1)

> Automatic generation of sequences has been a highly explored field in the last years. In particular, natural language processing and automatic music composition have gained importance due to the recent advances in machine learning and Neural Networks with intrinsic memory mechanisms such as Recurrent Neural Networks. This paper evaluates different types of memory mechanisms (memory cells) and analyses their performance in the field of music composition. The proposed approach considers music theory concepts such as transposition, and uses data transformations (embeddings) to introduce semantic meaning and improve the quality of the generated melodies. A set of quantitative metrics is presented to evaluate the performance of the proposed architecture automatically, measuring the tonality of the musical compositions.

</details>

<details>

<summary>2020-12-02 15:52:38 - A Computational Approach to Measuring the Semantic Divergence of Cognates</summary>

- *Ana-Sabina Uban, Alina-Maria Ciobanu, Liviu P. Dinu*

- `2012.01288v1` - [abs](http://arxiv.org/abs/2012.01288v1) - [pdf](http://arxiv.org/pdf/2012.01288v1)

> Meaning is the foundation stone of intercultural communication. Languages are continuously changing, and words shift their meanings for various reasons. Semantic divergence in related languages is a key concern of historical linguistics. In this paper we investigate semantic divergence across languages by measuring the semantic similarity of cognate sets in multiple languages. The method that we propose is based on cross-lingual word embeddings. In this paper we implement and evaluate our method on English and five Romance languages, but it can be extended easily to any language pair, requiring only large monolingual corpora for the involved languages and a small bilingual dictionary for the pair. This language-agnostic method facilitates a quantitative analysis of cognates divergence -- by computing degrees of semantic similarity between cognate pairs -- and provides insights for identifying false friends. As a second contribution, we formulate a straightforward method for detecting false friends, and introduce the notion of "soft false friend" and "hard false friend", as well as a measure of the degree of "falseness" of a false friends pair. Additionally, we propose an algorithm that can output suggestions for correcting false friends, which could result in a very helpful tool for language learning or translation.

</details>

<details>

<summary>2020-12-02 16:07:32 - Generating Descriptions for Sequential Images with Local-Object Attention and Global Semantic Context Modelling</summary>

- *Jing Su, Chenghua Lin, Mian Zhou, Qingyun Dai, Haoyu Lv*

- `2012.01295v1` - [abs](http://arxiv.org/abs/2012.01295v1) - [pdf](http://arxiv.org/pdf/2012.01295v1)

> In this paper, we propose an end-to-end CNN-LSTM model for generating descriptions for sequential images with a local-object attention mechanism. To generate coherent descriptions, we capture global semantic context using a multi-layer perceptron, which learns the dependencies between sequential images. A paralleled LSTM network is exploited for decoding the sequence descriptions. Experimental results show that our model outperforms the baseline across three different evaluation metrics on the datasets published by Microsoft.

</details>

<details>

<summary>2020-12-02 16:16:46 - Analyzing Stylistic Variation across Different Political Regimes</summary>

- *Liviu P. Dinu, Ana-Sabina Uban*

- `2012.01305v1` - [abs](http://arxiv.org/abs/2012.01305v1) - [pdf](http://arxiv.org/pdf/2012.01305v1)

> In this article we propose a stylistic analysis of texts written across two different periods, which differ not only temporally, but politically and culturally: communism and democracy in Romania. We aim to analyze the stylistic variation between texts written during these two periods, and determine at what levels the variation is more apparent (if any): at the stylistic level, at the topic level etc. We take a look at the stylistic profile of these texts comparatively, by performing clustering and classification experiments on the texts, using traditional authorship attribution methods and features. To confirm the stylistic variation is indeed an effect of the change in political and cultural environment, and not merely reflective of a natural change in the author's style with time, we look at various stylistic metrics over time and show that the change in style between the two periods is statistically significant. We also perform an analysis of the variation in topic between the two epochs, to compare with the variation at the style level. These analyses show that texts from the two periods can indeed be distinguished, both from the point of view of style and from that of semantic content (topic).

</details>

<details>

<summary>2020-12-02 18:32:14 - A Self-Supervised Feature Map Augmentation (FMA) Loss and Combined Augmentations Finetuning to Efficiently Improve the Robustness of CNNs</summary>

- *Nikhil Kapoor, Chun Yuan, Jonas Löhdefink, Roland Zimmermann, Serin Varghese, Fabian Hüger, Nico Schmidt, Peter Schlicht, Tim Fingscheidt*

- `2012.01386v1` - [abs](http://arxiv.org/abs/2012.01386v1) - [pdf](http://arxiv.org/pdf/2012.01386v1)

> Deep neural networks are often not robust to semantically-irrelevant changes in the input. In this work we address the issue of robustness of state-of-the-art deep convolutional neural networks (CNNs) against commonly occurring distortions in the input such as photometric changes, or the addition of blur and noise. These changes in the input are often accounted for during training in the form of data augmentation. We have two major contributions: First, we propose a new regularization loss called feature-map augmentation (FMA) loss which can be used during finetuning to make a model robust to several distortions in the input. Second, we propose a new combined augmentations (CA) finetuning strategy, that results in a single model that is robust to several augmentation types at the same time in a data-efficient manner. We use the CA strategy to improve an existing state-of-the-art method called stability training (ST). Using CA, on an image classification task with distorted images, we achieve an accuracy improvement of on average 8.94% with FMA and 8.86% with ST absolute on CIFAR-10 and 8.04% with FMA and 8.27% with ST absolute on ImageNet, compared to 1.98% and 2.12%, respectively, with the well known data augmentation method, while keeping the clean baseline performance.

</details>

<details>

<summary>2020-12-02 19:26:35 - CovSegNet: A Multi Encoder-Decoder Architecture for Improved Lesion Segmentation of COVID-19 Chest CT Scans</summary>

- *Tanvir Mahmud, Md Awsafur Rahman, Shaikh Anowarul Fattah, Sun-Yuan Kung*

- `2012.01473v1` - [abs](http://arxiv.org/abs/2012.01473v1) - [pdf](http://arxiv.org/pdf/2012.01473v1)

> Automatic lung lesions segmentation of chest CT scans is considered a pivotal stage towards accurate diagnosis and severity measurement of COVID-19. Traditional U-shaped encoder-decoder architecture and its variants suffer from diminutions of contextual information in pooling/upsampling operations with increased semantic gaps among encoded and decoded feature maps as well as instigate vanishing gradient problems for its sequential gradient propagation that result in sub-optimal performance. Moreover, operating with 3D CT-volume poses further limitations due to the exponential increase of computational complexity making the optimization difficult. In this paper, an automated COVID-19 lesion segmentation scheme is proposed utilizing a highly efficient neural network architecture, namely CovSegNet, to overcome these limitations. Additionally, a two-phase training scheme is introduced where a deeper 2D-network is employed for generating ROI-enhanced CT-volume followed by a shallower 3D-network for further enhancement with more contextual information without increasing computational burden. Along with the traditional vertical expansion of Unet, we have introduced horizontal expansion with multi-stage encoder-decoder modules for achieving optimum performance. Additionally, multi-scale feature maps are integrated into the scale transition process to overcome the loss of contextual information. Moreover, a multi-scale fusion module is introduced with a pyramid fusion scheme to reduce the semantic gaps between subsequent encoder/decoder modules while facilitating the parallel optimization for efficient gradient propagation. Outstanding performances have been achieved in three publicly available datasets that largely outperform other state-of-the-art approaches. The proposed scheme can be easily extended for achieving optimum segmentation performances in a wide variety of applications.

</details>

<details>

<summary>2020-12-02 23:56:34 - SChME at SemEval-2020 Task 1: A Model Ensemble for Detecting Lexical Semantic Change</summary>

- *Maurício Gruppi, Sibel Adali, Pin-Yu Chen*

- `2012.01603v1` - [abs](http://arxiv.org/abs/2012.01603v1) - [pdf](http://arxiv.org/pdf/2012.01603v1)

> This paper describes SChME (Semantic Change Detection with Model Ensemble), a method usedin SemEval-2020 Task 1 on unsupervised detection of lexical semantic change. SChME usesa model ensemble combining signals of distributional models (word embeddings) and wordfrequency models where each model casts a vote indicating the probability that a word sufferedsemantic change according to that feature. More specifically, we combine cosine distance of wordvectors combined with a neighborhood-based metric we named Mapped Neighborhood Distance(MAP), and a word frequency differential metric as input signals to our model. Additionally,we explore alignment-based methods to investigate the importance of the landmarks used in thisprocess. Our results show evidence that the number of landmarks used for alignment has a directimpact on the predictive performance of the model. Moreover, we show that languages that sufferless semantic change tend to benefit from using a large number of landmarks, whereas languageswith more semantic change benefit from a more careful choice of landmark number for alignment.

</details>

<details>

<summary>2020-12-03 00:13:21 - Inverse Visual Question Answering with Multi-Level Attentions</summary>

- *Yaser Alwattar, Yuhong Guo*

- `1909.07583v2` - [abs](http://arxiv.org/abs/1909.07583v2) - [pdf](http://arxiv.org/pdf/1909.07583v2)

> In this paper, we propose a novel deep multi-level attention model to address inverse visual question answering. The proposed model generates regional visual and semantic features at the object level and then enhances them with the answer cue by using attention mechanisms. Two levels of multiple attentions are employed in the model, including the dual attention at the partial question encoding step and the dynamic attention at the next question word generation step. We evaluate the proposed model on the VQA V1 dataset. It demonstrates state-of-the-art performance in terms of multiple commonly used metrics.

</details>

<details>

<summary>2020-12-03 00:30:33 - CorDEL: A Contrastive Deep Learning Approach for Entity Linkage</summary>

- *Zhengyang Wang, Bunyamin Sisman, Hao Wei, Xin Luna Dong, Shuiwang Ji*

- `2009.07203v3` - [abs](http://arxiv.org/abs/2009.07203v3) - [pdf](http://arxiv.org/pdf/2009.07203v3)

> Entity linkage (EL) is a critical problem in data cleaning and integration. In the past several decades, EL has typically been done by rule-based systems or traditional machine learning models with hand-curated features, both of which heavily depend on manual human inputs. With the ever-increasing growth of new data, deep learning (DL) based approaches have been proposed to alleviate the high cost of EL associated with the traditional models. Existing exploration of DL models for EL strictly follows the well-known twin-network architecture. However, we argue that the twin-network architecture is sub-optimal to EL, leading to inherent drawbacks of existing models. In order to address the drawbacks, we propose a novel and generic contrastive DL framework for EL. The proposed framework is able to capture both syntactic and semantic matching signals and pays attention to subtle but critical differences. Based on the framework, we develop a contrastive DL approach for EL, called CorDEL, with three powerful variants. We evaluate CorDEL with extensive experiments conducted on both public benchmark datasets and a real-world dataset. CorDEL outperforms previous state-of-the-art models by 5.2% on public benchmark datasets. Moreover, CorDEL yields a 2.4% improvement over the current best DL model on the real-world dataset, while reducing the number of training parameters by 97.6%.

</details>

<details>

<summary>2020-12-03 01:51:20 - What Makes a Star Teacher? A Hierarchical BERT Model for Evaluating Teacher's Performance in Online Education</summary>

- *Wen Wang, Honglei Zhuang, Mi Zhou, Hanyu Liu, Beibei Li*

- `2012.01633v1` - [abs](http://arxiv.org/abs/2012.01633v1) - [pdf](http://arxiv.org/pdf/2012.01633v1)

> Education has a significant impact on both society and personal life. With the development of technology, online education has been growing rapidly over the past decade. While there are several online education studies on student behavior analysis, the course concept mining, and course recommendations (Feng, Tang, and Liu 2019; Pan et al. 2017), there is little research on evaluating teachers' performance in online education. In this paper, we conduct a systematic study to understand and effectively predict teachers' performance using the subtitles of 1,085 online courses. Our model-free analysis shows that teachers' verbal cues (e.g., question strategy, emotional appealing, and hedging) and their course structure design are both significantly correlated with teachers' performance evaluation. Based on these insights, we then propose a hierarchical course BERT model to predict teachers' performance in online education. Our proposed model can capture the hierarchical structure within each course as well as the deep semantic features extracted from the course content. Experiment results show that our proposed method achieves significant gain over several state-of-the-art methods. Our study provides a significant social impact in helping teachers improve their teaching style and enhance their instructional material design for more effective online teaching in the future.

</details>

<details>

<summary>2020-12-03 01:53:57 - COVID-19 CT Image Synthesis with a Conditional Generative Adversarial Network</summary>

- *Yifan Jiang, Han Chen, Murray Loew, Hanseok Ko*

- `2007.14638v2` - [abs](http://arxiv.org/abs/2007.14638v2) - [pdf](http://arxiv.org/pdf/2007.14638v2)

> Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has spread rapidly since December 2019. Real-time reverse transcription polymerase chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits of quick reporting, a low cost, and high sensitivity for the detection of pulmonary infection. Recently, deep-learning-based computer vision methods have demonstrated great promise for use in medical imaging applications, including X-rays, magnetic resonance imaging, and CT imaging. However, training a deep-learning model requires large volumes of data, and medical staff faces a high risk when collecting COVID-19 CT data due to the high infectivity of the disease. Another issue is the lack of experts available for data labeling. In order to meet the data requirements for COVID-19 CT imaging, we propose a CT image synthesis approach based on a conditional generative adversarial network that can effectively generate high-quality and realistic COVID-19 CT images for use in deep-learning-based medical imaging tasks. Experimental results show that the proposed method outperforms other state-of-the-art image synthesis methods with the generated COVID-19 CT images and indicates promising for various machine learning applications including semantic segmentation and classification.

</details>

<details>

<summary>2020-12-03 02:34:19 - KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation Classification</summary>

- *Chengyu Wang, Minghui Qiu, Jun Huang, Xiaofeng He*

- `2002.10903v2` - [abs](http://arxiv.org/abs/2002.10903v2) - [pdf](http://arxiv.org/pdf/2002.10903v2)

> Lexical relations describe how concepts are semantically related, in the form of relation triples. The accurate prediction of lexical relations between concepts is challenging, due to the sparsity of patterns indicating the existence of such relations. We propose the Knowledge-Enriched Meta-Learning (KEML) framework to address the task of lexical relation classification. In KEML, the LKB-BERT (Lexical Knowledge Base-BERT) model is presented to learn concept representations from massive text corpora, with rich lexical knowledge injected by distant supervision. A probabilistic distribution of auxiliary tasks is defined to increase the model's ability to recognize different types of lexical relations. We further combine a meta-learning process over the auxiliary task distribution and supervised learning to train the neural lexical relation classifier. Experiments over multiple datasets show that KEML outperforms state-of-the-art methods.

</details>

<details>

<summary>2020-12-03 02:47:41 - TURL: Table Understanding through Representation Learning</summary>

- *Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu*

- `2006.14806v2` - [abs](http://arxiv.org/abs/2006.14806v2) - [pdf](http://arxiv.org/pdf/2006.14806v2)

> Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning. Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.

</details>

<details>

<summary>2020-12-03 03:34:05 - GraphPB: Graphical Representations of Prosody Boundary in Speech Synthesis</summary>

- *Aolan Sun, Jianzong Wang, Ning Cheng, Huayi Peng, Zhen Zeng, Lingwei Kong, Jing Xiao*

- `2012.02626v1` - [abs](http://arxiv.org/abs/2012.02626v1) - [pdf](http://arxiv.org/pdf/2012.02626v1)

> This paper introduces a graphical representation approach of prosody boundary (GraphPB) in the task of Chinese speech synthesis, intending to parse the semantic and syntactic relationship of input sequences in a graphical domain for improving the prosody performance. The nodes of the graph embedding are formed by prosodic words, and the edges are formed by the other prosodic boundaries, namely prosodic phrase boundary (PPH) and intonation phrase boundary (IPH). Different Graph Neural Networks (GNN) like Gated Graph Neural Network (GGNN) and Graph Long Short-term Memory (G-LSTM) are utilised as graph encoders to exploit the graphical prosody boundary information. Graph-to-sequence model is proposed and formed by a graph encoder and an attentional decoder. Two techniques are proposed to embed sequential information into the graph-to-sequence text-to-speech model. The experimental results show that this proposed approach can encode the phonetic and prosody rhythm of an utterance. The mean opinion score (MOS) of these GNN models shows comparative results with the state-of-the-art sequence-to-sequence models with better performance in the aspect of prosody. This provides an alternative approach for prosody modelling in end-to-end speech synthesis.

</details>

<details>

<summary>2020-12-03 05:16:08 - Dual Attention Model for Citation Recommendation</summary>

- *Yang Zhang, Qiang Ma*

- `2010.00182v5` - [abs](http://arxiv.org/abs/2010.00182v5) - [pdf](http://arxiv.org/pdf/2010.00182v5)

> Based on an exponentially increasing number of academic articles, discovering and citing comprehensive and appropriate resources has become a non-trivial task. Conventional citation recommender methods suffer from severe information loss. For example, they do not consider the section of the paper that the user is writing and for which they need to find a citation, the relatedness between the words in the local context (the text span that describes a citation), or the importance on each word from the local context. These shortcomings make such methods insufficient for recommending adequate citations to academic manuscripts. In this study, we propose a novel embedding-based neural network called "dual attention model for citation recommendation (DACR)" to recommend citations during manuscript preparation. Our method adapts embedding of three dimensions of semantic information: words in the local context, structural contexts, and the section on which a user is working. A neural network is designed to maximize the similarity between the embedding of the three input (local context words, section and structural contexts) and the target citation appearing in the context. The core of the neural network is composed of self-attention and additive attention, where the former aims to capture the relatedness between the contextual words and structural context, and the latter aims to learn the importance of them. The experiments on real-world datasets demonstrate the effectiveness of the proposed approach.

</details>

<details>

<summary>2020-12-03 05:41:44 - A Study on the Autoregressive and non-Autoregressive Multi-label Learning</summary>

- *Elham J. Barezi, Iacer Calixto, Kyunghyun Cho, Pascale Fung*

- `2012.01711v1` - [abs](http://arxiv.org/abs/2012.01711v1) - [pdf](http://arxiv.org/pdf/2012.01711v1)

> Extreme classification tasks are multi-label tasks with an extremely large number of labels (tags). These tasks are hard because the label space is usually (i) very large, e.g. thousands or millions of labels, (ii) very sparse, i.e. very few labels apply to each input document, and (iii) highly correlated, meaning that the existence of one label changes the likelihood of predicting all other labels. In this work, we propose a self-attention based variational encoder-model to extract the label-label and label-feature dependencies jointly and to predict labels for a given input. In more detail, we propose a non-autoregressive latent variable model and compare it to a strong autoregressive baseline that predicts a label based on all previously generated labels. Our model can therefore be used to predict all labels in parallel while still including both label-label and label-feature dependencies through latent variables, and compares favourably to the autoregressive baseline. We apply our models to four standard extreme classification natural language data sets, and one news videos dataset for automated label detection from a lexicon of semantic concepts. Experimental results show that although the autoregressive models, where use a given order of the labels for chain-order label prediction, work great for the small scale labels or the prediction of the highly ranked label, but our non-autoregressive model surpasses them by around 2% to 6% when we need to predict more labels, or the dataset has a larger number of the labels.

</details>

<details>

<summary>2020-12-03 10:11:03 - CL-IMS @ DIACR-Ita: Volente o Nolente: BERT does not outperform SGNS on Semantic Change Detection</summary>

- *Severin Laicher, Gioia Baldissin, Enrique Castañeda, Dominik Schlechtweg, Sabine Schulte im Walde*

- `2011.07247v2` - [abs](http://arxiv.org/abs/2011.07247v2) - [pdf](http://arxiv.org/pdf/2011.07247v2)

> We present the results of our participation in the DIACR-Ita shared task on lexical semantic change detection for Italian. We exploit Average Pairwise Distance of token-based BERT embeddings between time points and rank 5 (of 8) in the official ranking with an accuracy of $.72$. While we tune parameters on the English data set of SemEval-2020 Task 1 and reach high performance, this does not translate to the Italian DIACR-Ita data set. Our results show that we do not manage to find robust ways to exploit BERT embeddings in lexical semantic change detection.

</details>

<details>

<summary>2020-12-03 12:49:22 - Label Enhanced Event Detection with Heterogeneous Graph Attention Networks</summary>

- *Shiyao Cui, Bowen Yu, Xin Cong, Tingwen Liu, Quangang Li, Jinqiao Shi*

- `2012.01878v1` - [abs](http://arxiv.org/abs/2012.01878v1) - [pdf](http://arxiv.org/pdf/2012.01878v1)

> Event Detection (ED) aims to recognize instances of specified types of event triggers in text. Different from English ED, Chinese ED suffers from the problem of word-trigger mismatch due to the uncertain word boundaries. Existing approaches injecting word information into character-level models have achieved promising progress to alleviate this problem, but they are limited by two issues. First, the interaction between characters and lexicon words is not fully exploited. Second, they ignore the semantic information provided by event labels. We thus propose a novel architecture named Label enhanced Heterogeneous Graph Attention Networks (L-HGAT). Specifically, we transform each sentence into a graph, where character nodes and word nodes are connected with different types of edges, so that the interaction between words and characters is fully reserved. A heterogeneous graph attention networks is then introduced to propagate relational message and enrich information interaction. Furthermore, we convert each label into a trigger-prototype-based embedding, and design a margin loss to guide the model distinguish confusing event labels. Experiments on two benchmark datasets show that our model achieves significant improvement over a range of competitive baseline methods.

</details>

<details>

<summary>2020-12-03 14:26:54 - Drugs4Covid: Drug-driven Knowledge Exploitation based on Scientific Publications</summary>

- *Carlos Badenes-Olmedo, David Chaves-Fraga, MarÍa Poveda-VillalÓn, Ana Iglesias-Molina, Pablo Calleja, Socorro Bernardos, Patricia MartÍn-Chozas, Alba Fernández-Izquierdo, Elvira Amador-Domínguez, Paola Espinoza-Arias, Luis Pozo, Edna Ruckhaus, Esteban González-Guardia, Raquel Cedazo, Beatriz López-Centeno, Oscar Corcho*

- `2012.01953v1` - [abs](http://arxiv.org/abs/2012.01953v1) - [pdf](http://arxiv.org/pdf/2012.01953v1)

> In the absence of sufficient medication for COVID patients due to the increased demand, disused drugs have been employed or the doses of those available were modified by hospital pharmacists. Some evidences for the use of alternative drugs can be found in the existing scientific literature that could assist in such decisions. However, exploiting large corpus of documents in an efficient manner is not easy, since drugs may not appear explicitly related in the texts and could be mentioned under different brand names. Drugs4Covid combines word embedding techniques and semantic web technologies to enable a drug-oriented exploration of large medical literature. Drugs and diseases are identified according to the ATC classification and MeSH categories respectively. More than 60K articles and 2M paragraphs have been processed from the CORD-19 corpus with information of COVID-19, SARS, and other related coronaviruses. An open catalogue of drugs has been created and results are publicly available through a drug browser, a keyword-guided text explorer, and a knowledge graph.

</details>

<details>

<summary>2020-12-03 16:25:07 - Recovering Trajectories of Unmarked Joints in 3D Human Actions Using Latent Space Optimization</summary>

- *Suhas Lohit, Rushil Anirudh, Pavan Turaga*

- `2012.02043v1` - [abs](http://arxiv.org/abs/2012.02043v1) - [pdf](http://arxiv.org/pdf/2012.02043v1)

> Motion capture (mocap) and time-of-flight based sensing of human actions are becoming increasingly popular modalities to perform robust activity analysis. Applications range from action recognition to quantifying movement quality for health applications. While marker-less motion capture has made great progress, in critical applications such as healthcare, marker-based systems, especially active markers, are still considered gold-standard. However, there are several practical challenges in both modalities such as visibility, tracking errors, and simply the need to keep marker setup convenient wherein movements are recorded with a reduced marker-set. This implies that certain joint locations will not even be marked-up, making downstream analysis of full body movement challenging. To address this gap, we first pose the problem of reconstructing the unmarked joint data as an ill-posed linear inverse problem. We recover missing joints for a given action by projecting it onto the manifold of human actions, this is achieved by optimizing the latent space representation of a deep autoencoder. Experiments on both mocap and Kinect datasets clearly demonstrate that the proposed method performs very well in recovering semantics of the actions and dynamics of missing joints. We will release all the code and models publicly.

</details>

<details>

<summary>2020-12-03 17:30:00 - StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation</summary>

- *Zongze Wu, Dani Lischinski, Eli Shechtman*

- `2011.12799v2` - [abs](http://arxiv.org/abs/2011.12799v2) - [pdf](http://arxiv.org/pdf/2011.12799v2)

> We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.

</details>

<details>

<summary>2020-12-03 18:07:28 - BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling</summary>

- *Jing Su, Qingyun Dai, Frank Guerin, Mian Zhou*

- `2012.02128v1` - [abs](http://arxiv.org/abs/2012.02128v1) - [pdf](http://arxiv.org/pdf/2012.02128v1)

> Visual storytelling is a creative and challenging task, aiming to automatically generate a story-like description for a sequence of images. The descriptions generated by previous visual storytelling approaches lack coherence because they use word-level sequence generation methods and do not adequately consider sentence-level dependencies. To tackle this problem, we propose a novel hierarchical visual storytelling framework which separately models sentence-level and word-level semantics. We use the transformer-based BERT to obtain embeddings for sentences and words. We then employ a hierarchical LSTM network: the bottom LSTM receives as input the sentence vector representation from BERT, to learn the dependencies between the sentences corresponding to images, and the top LSTM is responsible for generating the corresponding word vector representations, taking input from the bottom LSTM. Experimental results demonstrate that our model outperforms most closely related baselines under automatic evaluation metrics BLEU and CIDEr, and also show the effectiveness of our method with human evaluation.

</details>

<details>

<summary>2020-12-03 18:59:03 - Space-Time Correspondence as a Contrastive Random Walk</summary>

- *Allan Jabri, Andrew Owens, Alexei A. Efros*

- `2006.14613v2` - [abs](http://arxiv.org/abs/2006.14613v2) - [pdf](http://arxiv.org/pdf/2006.14613v2)

> This paper proposes a simple self-supervised approach for learning a representation for visual correspondence from raw video. We cast correspondence as prediction of links in a space-time graph constructed from video. In this graph, the nodes are patches sampled from each frame, and nodes adjacent in time can share a directed edge. We learn a representation in which pairwise similarity defines transition probability of a random walk, so that long-range correspondence is computed as a walk along the graph. We optimize the representation to place high probability along paths of similarity. Targets for learning are formed without supervision, by cycle-consistency: the objective is to maximize the likelihood of returning to the initial node when walking along a graph constructed from a palindrome of frames. Thus, a single path-level constraint implicitly supervises chains of intermediate comparisons. When used as a similarity metric without adaptation, the learned representation outperforms the self-supervised state-of-the-art on label propagation tasks involving objects, semantic parts, and pose. Moreover, we demonstrate that a technique we call edge dropout, as well as self-supervised adaptation at test-time, further improve transfer for object-centric correspondence.

</details>

<details>

<summary>2020-12-03 19:27:29 - Evolving Character-level Convolutional Neural Networks for Text Classification</summary>

- *Trevor Londt, Xiaoying Gao, Bing Xue, Peter Andreae*

- `2012.02223v1` - [abs](http://arxiv.org/abs/2012.02223v1) - [pdf](http://arxiv.org/pdf/2012.02223v1)

> Character-level convolutional neural networks (char-CNN) require no knowledge of the semantic or syntactic structure of the language they classify. This property simplifies its implementation but reduces its classification accuracy. Increasing the depth of char-CNN architectures does not result in breakthrough accuracy improvements. Research has not established which char-CNN architectures are optimal for text classification tasks. Manually designing and training char-CNNs is an iterative and time-consuming process that requires expert domain knowledge. Evolutionary deep learning (EDL) techniques, including surrogate-based versions, have demonstrated success in automatically searching for performant CNN architectures for image analysis tasks. Researchers have not applied EDL techniques to search the architecture space of char-CNNs for text classification tasks. This article demonstrates the first work in evolving char-CNN architectures using a novel EDL algorithm based on genetic programming, an indirect encoding and surrogate models, to search for performant char-CNN architectures automatically. The algorithm is evaluated on eight text classification datasets and benchmarked against five manually designed CNN architecture and one long short-term memory (LSTM) architecture. Experiment results indicate that the algorithm can evolve architectures that outperform the LSTM in terms of classification accuracy and five of the manually designed CNN architectures in terms of classification accuracy and parameter count.

</details>

<details>

<summary>2020-12-03 19:50:31 - LULC classification by semantic segmentation of satellite images using FastFCN</summary>

- *Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, Amreen Anbar, A. K. M. Nazrul Islam, A. K. M. Mahbubur Rahman*

- `2011.06825v2` - [abs](http://arxiv.org/abs/2011.06825v2) - [pdf](http://arxiv.org/pdf/2011.06825v2)

> This paper analyses how well a Fast Fully Convolutional Network (FastFCN) semantically segments satellite images and thus classifies Land Use/Land Cover(LULC) classes. Fast-FCN was used on Gaofen-2 Image Dataset (GID-2) to segment them in five different classes: BuiltUp, Meadow, Farmland, Water and Forest. The results showed better accuracy (0.93), precision (0.99), recall (0.98) and mean Intersection over Union (mIoU)(0.97) than other approaches like using FCN-8 or eCognition, a readily available software. We presented a comparison between the results. We propose FastFCN to be both faster and more accurate automated method than other existing methods for LULC classification.

</details>

<details>

<summary>2020-12-03 20:29:27 - Technical Report: Refining Case Models Using Cardinality Constraints</summary>

- *Stephan Haarmann, Marco Montali, Mathias Weske*

- `2012.02245v1` - [abs](http://arxiv.org/abs/2012.02245v1) - [pdf](http://arxiv.org/pdf/2012.02245v1)

> Traditionally, business process management focuses on structured, imperative processes. With the increasing importance of knowledge work, semi-structured processes are entering center stage. Existing approaches to modeling knowledge-intensive business processes use data objects but fail to sufficiently take into account data object cardinalities. Hence, they cannot guarantee that cardinality constraints are respected, nor use such constraints to handle concurrency and multiple activity instances during execution. This paper extends an existing case management approach with data object associations and cardinality constraints. The results facilitate a refined data access semantics, lower and upper bounds for process activities, and synchronized processing of multiple data objects. The execution semantics is formally specified using colored Petri nets. The effectiveness of the approach is shown by a compiler translating case models to colored Petri nets and by a dedicated process execution engine.

</details>

<details>

<summary>2020-12-03 21:58:57 - Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric</summary>

- *Ivan P. Yamshchikov, Viacheslav Shibaev, Nikolay Khlebnikov, Alexey Tikhonov*

- `2004.05001v3` - [abs](http://arxiv.org/abs/2004.05001v3) - [pdf](http://arxiv.org/pdf/2004.05001v3)

> The rapid development of such natural language processing tasks as style transfer, paraphrase, and machine translation often calls for the use of semantic similarity metrics. In recent years a lot of methods to measure the semantic similarity of two short texts were developed. This paper provides a comprehensive analysis for more than a dozen of such methods. Using a new dataset of fourteen thousand sentence pairs human-labeled according to their semantic similarity, we demonstrate that none of the metrics widely used in the literature is close enough to human judgment in these tasks. A number of recently proposed metrics provide comparable results, yet Word Mover Distance is shown to be the most reasonable solution to measure semantic similarity in reformulated texts at the moment.

</details>

<details>

<summary>2020-12-04 09:07:13 - Is It a Plausible Colour? UCapsNet for Image Colourisation</summary>

- *Rita Pucci, Christian Micheloni, Gian Luca Foresti, Niki Martinel*

- `2012.02478v1` - [abs](http://arxiv.org/abs/2012.02478v1) - [pdf](http://arxiv.org/pdf/2012.02478v1)

> Human beings can imagine the colours of a grayscale image with no particular effort thanks to their ability of semantic feature extraction. Can an autonomous system achieve that? Can it hallucinate plausible and vibrant colours? This is the colourisation problem. Different from existing works relying on convolutional neural network models pre-trained with supervision, we cast such colourisation problem as a self-supervised learning task. We tackle the problem with the introduction of a novel architecture based on Capsules trained following the adversarial learning paradigm. Capsule networks are able to extract a semantic representation of the entities in the image but loose details about their spatial information, which is important for colourising a grayscale image. Thus our UCapsNet structure comes with an encoding phase that extracts entities through capsules and spatial details through convolutional neural networks. A decoding phase merges the entity features with the spatial features to hallucinate a plausible colour version of the input datum. Results on the ImageNet benchmark show that our approach is able to generate more vibrant and plausible colours than exiting solutions and achieves superior performance than models pre-trained with supervision.

</details>

<details>

<summary>2020-12-04 13:06:27 - EventKG+BT: Generation of Interactive Biography Timelines from a Knowledge Graph</summary>

- *Simon Gottschalk, Elena Demidova*

- `2012.06306v1` - [abs](http://arxiv.org/abs/2012.06306v1) - [pdf](http://arxiv.org/pdf/2012.06306v1)

> Research on notable accomplishments and important events in the life of people of public interest usually requires close reading of long encyclopedic or biographical sources, which is a tedious and time-consuming task. Whereas semantic reference sources, such as the EventKG knowledge graph, provide structured representations of relevant facts, they often include hundreds of events and temporal relations for particular entities. In this paper, we present EventKG+BT - a timeline generation system that creates concise and interactive spatio-temporal representations of biographies from a knowledge graph using distant supervision.

</details>

<details>

<summary>2020-12-04 15:42:08 - Accelerating Road Sign Ground Truth Construction with Knowledge Graph and Machine Learning</summary>

- *Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin*

- `2012.02672v1` - [abs](http://arxiv.org/abs/2012.02672v1) - [pdf](http://arxiv.org/pdf/2012.02672v1)

> Having a comprehensive, high-quality dataset of road sign annotation is critical to the success of AI-based Road Sign Recognition (RSR) systems. In practice, annotators often face difficulties in learning road sign systems of different countries; hence, the tasks are often time-consuming and produce poor results. We propose a novel approach using knowledge graphs and a machine learning algorithm - variational prototyping-encoder (VPE) - to assist human annotators in classifying road signs effectively. Annotators can query the Road Sign Knowledge Graph using visual attributes and receive closest matching candidates suggested by the VPE model. The VPE model uses the candidates from the knowledge graph and a real sign image patch as inputs. We show that our knowledge graph approach can reduce sign search space by 98.9%. Furthermore, with VPE, our system can propose the correct single candidate for 75% of signs in the tested datasets, eliminating the human search effort entirely in those cases.

</details>

<details>

<summary>2020-12-04 17:36:57 - Ultrasound Scatterer Density Classification Using Convolutional Neural Networks by Exploiting Patch Statistics</summary>

- *Ali K. Z. Tehrani, Mina Amiri, Ivan M. Rosado-Mendez, Timothy J. Hall, Hassan Rivaz*

- `2012.02738v1` - [abs](http://arxiv.org/abs/2012.02738v1) - [pdf](http://arxiv.org/pdf/2012.02738v1)

> Quantitative ultrasound (QUS) can reveal crucial information on tissue properties such as scatterer density. If the scatterer density per resolution cell is above or below 10, the tissue is considered as fully developed speckle (FDS) or low-density scatterers (LDS), respectively. Conventionally, the scatterer density has been classified using estimated statistical parameters of the amplitude of backscattered echoes. However, if the patch size is small, the estimation is not accurate. These parameters are also highly dependent on imaging settings. In this paper, we propose a convolutional neural network (CNN) architecture for QUS, and train it using simulation data. We further improve the network performance by utilizing patch statistics as additional input channels. We evaluate the network using simulation data, experimental phantoms and in vivo data. We also compare our proposed network with different classic and deep learning models, and demonstrate its superior performance in classification of tissues with different scatterer density values. The results also show that the proposed network is able to work with different imaging parameters with no need for a reference phantom. This work demonstrates the potential of CNNs in classifying scatterer density in ultrasound images.

</details>

<details>

<summary>2020-12-04 18:28:30 - Delexicalized Paraphrase Generation</summary>

- *Boya Yu, Konstantine Arkoudas, Wael Hamza*

- `2012.02763v1` - [abs](http://arxiv.org/abs/2012.02763v1) - [pdf](http://arxiv.org/pdf/2012.02763v1)

> We present a neural model for paraphrasing and train it to generate delexicalized sentences. We achieve this by creating training data in which each input is paired with a number of reference paraphrases. These sets of reference paraphrases represent a weak type of semantic equivalence based on annotated slots and intents. To understand semantics from different types of slots, other than anonymizing slots, we apply convolutional neural networks (CNN) prior to pooling on slot values and use pointers to locate slots in the output. We show empirically that the generated paraphrases are of high quality, leading to an additional 1.29% exact match on live utterances. We also show that natural language understanding (NLU) tasks, such as intent classification and named entity recognition, can benefit from data augmentation using automatically generated paraphrases.

</details>

<details>

<summary>2020-12-04 18:46:17 - Learning Equivariant Representations</summary>

- *Carlos Esteves*

- `2012.02771v1` - [abs](http://arxiv.org/abs/2012.02771v1) - [pdf](http://arxiv.org/pdf/2012.02771v1)

> State-of-the-art deep learning systems often require large amounts of data and computation. For this reason, leveraging known or unknown structure of the data is paramount. Convolutional neural networks (CNNs) are successful examples of this principle, their defining characteristic being the shift-equivariance. By sliding a filter over the input, when the input shifts, the response shifts by the same amount, exploiting the structure of natural images where semantic content is independent of absolute pixel positions. This property is essential to the success of CNNs in audio, image and video recognition tasks. In this thesis, we extend equivariance to other kinds of transformations, such as rotation and scaling. We propose equivariant models for different transformations defined by groups of symmetries. The main contributions are (i) polar transformer networks, achieving equivariance to the group of similarities on the plane, (ii) equivariant multi-view networks, achieving equivariance to the group of symmetries of the icosahedron, (iii) spherical CNNs, achieving equivariance to the continuous 3D rotation group, (iv) cross-domain image embeddings, achieving equivariance to 3D rotations for 2D inputs, and (v) spin-weighted spherical CNNs, generalizing the spherical CNNs and achieving equivariance to 3D rotations for spherical vector fields. Applications include image classification, 3D shape classification and retrieval, panoramic image classification and segmentation, shape alignment and pose estimation. What these models have in common is that they leverage symmetries in the data to reduce sample and model complexity and improve generalization performance. The advantages are more significant on (but not limited to) challenging tasks where data is limited or input perturbations such as arbitrary rotations are present.

</details>

<details>

<summary>2020-12-04 19:51:24 - On-Device Sentence Similarity for SMS Dataset</summary>

- *Arun D Prabhu, Nikhil Arora, Shubham Vatsal, Gopi Ramena, Sukumar Moharana, Naresh Purre*

- `2012.02819v1` - [abs](http://arxiv.org/abs/2012.02819v1) - [pdf](http://arxiv.org/pdf/2012.02819v1)

> Determining the sentence similarity between Short Message Service (SMS) texts/sentences plays a significant role in mobile device industry. Gauging the similarity between SMS data is thus necessary for various applications like enhanced searching and navigation, clubbing together SMS of similar type when given a custom label or tag is provided by user irrespective of their sender etc. The problem faced with SMS data is its incomplete structure and grammatical inconsistencies. In this paper, we propose a unique pipeline for evaluating the text similarity between SMS texts. We use Part of Speech (POS) model for keyword extraction by taking advantage of the partial structure embedded in SMS texts and similarity comparisons are carried out using statistical methods. The proposed pipeline deals with major semantic variations across SMS data as well as makes it effective for its application on-device (mobile phone). To showcase the capabilities of our work, our pipeline has been designed with an inclination towards one of the possible applications of SMS text similarity discussed in one of the following sections but nonetheless guarantees scalability for other applications as well.

</details>

<details>

<summary>2020-12-05 04:01:02 - Fusion Models for Improved Visual Captioning</summary>

- *Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow*

- `2010.15251v2` - [abs](http://arxiv.org/abs/2010.15251v2) - [pdf](http://arxiv.org/pdf/2010.15251v2)

> Visual captioning aims to generate textual descriptions given images or videos. Traditionally, image captioning models are trained on human annotated datasets such as Flickr30k and MS-COCO, which are limited in size and diversity. This limitation hinders the generalization capabilities of these models while also rendering them liable to making mistakes. Language models can, however, be trained on vast amounts of freely available unlabelled data and have recently emerged as successful language encoders and coherent text generators. Meanwhile, several unimodal and multimodal fusion techniques have been proven to work well for natural language generation and automatic speech recognition. Building on these recent developments, and with the aim of improving the quality of generated captions, the contribution of our work in this paper is two-fold: First, we propose a generic multimodal model fusion framework for caption generation as well as emendation where we utilize different fusion strategies to integrate a pretrained Auxiliary Language Model (AuxLM) within the traditional encoder-decoder visual captioning frameworks. Next, we employ the same fusion strategies to integrate a pretrained Masked Language Model (MLM), namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for emending both syntactic and semantic errors in captions. Our caption emendation experiments on three benchmark image captioning datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the baseline, indicating the usefulness of our proposed multimodal fusion strategies. Further, we perform a preliminary qualitative analysis on the emended captions and identify error categories based on the type of corrections.

</details>

<details>

<summary>2020-12-05 05:03:28 - Neurosymbolic AI for Situated Language Understanding</summary>

- *Nikhil Krishnaswamy, James Pustejovsky*

- `2012.02947v1` - [abs](http://arxiv.org/abs/2012.02947v1) - [pdf](http://arxiv.org/pdf/2012.02947v1)

> In recent years, data-intensive AI, particularly the domain of natural language processing and understanding, has seen significant progress driven by the advent of large datasets and deep neural networks that have sidelined more classic AI approaches to the field. These systems can apparently demonstrate sophisticated linguistic understanding or generation capabilities, but often fail to transfer their skills to situations they have not encountered before. We argue that computational situated grounding provides a solution to some of these learning challenges by creating situational representations that both serve as a formal model of the salient phenomena, and contain rich amounts of exploitable, task-appropriate data for training new, flexible computational models. Our model reincorporates some ideas of classic AI into a framework of neurosymbolic intelligence, using multimodal contextual modeling of interactive situations, events, and object properties. We discuss how situated grounding provides diverse data and multiple levels of modeling for a variety of AI learning challenges, including learning how to interact with object affordances, learning semantics for novel structures and configurations, and transferring such learned knowledge to new objects and situations.

</details>

<details>

<summary>2020-12-05 05:42:46 - Deep Multimodal Fusion by Channel Exchanging</summary>

- *Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, Junzhou Huang*

- `2011.05005v2` - [abs](http://arxiv.org/abs/2011.05005v2) - [pdf](http://arxiv.org/pdf/2011.05005v2)

> Deep multimodal fusion by using multiple sources of data for classification or regression has exhibited a clear advantage over the unimodal counterpart on various applications. Yet, current methods including aggregation-based and alignment-based fusion are still inadequate in balancing the trade-off between inter-modal fusion and intra-modal processing, incurring a bottleneck of performance improvement. To this end, this paper proposes Channel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework that dynamically exchanges channels between sub-networks of different modalities. Specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. The validity of such exchanging process is also guaranteed by sharing convolutional filters yet keeping separate BN layers across modalities, which, as an add-on benefit, allows our multimodal architecture to be almost as compact as a unimodal network. Extensive experiments on semantic segmentation via RGB-D data and image translation through multi-domain input verify the effectiveness of our CEN compared to current state-of-the-art methods. Detailed ablation studies have also been carried out, which provably affirm the advantage of each component we propose. Our code is available at https://github.com/yikaiw/CEN.

</details>

<details>

<summary>2020-12-05 08:59:14 - Contextually Plausible and Diverse 3D Human Motion Prediction</summary>

- *Sadegh Aliakbarian, Fatemeh Sadat Saleh, Lars Petersson, Stephen Gould, Mathieu Salzmann*

- `1912.08521v4` - [abs](http://arxiv.org/abs/1912.08521v4) - [pdf](http://arxiv.org/pdf/1912.08521v4)

> We tackle the task of diverse 3D human motion prediction, that is, forecasting multiple plausible future 3D poses given a sequence of observed 3D poses. In this context, a popular approach consists of using a Conditional Variational Autoencoder (CVAE). However, existing approaches that do so either fail to capture the diversity in human motion, or generate diverse but semantically implausible continuations of the observed motion. In this paper, we address both of these problems by developing a new variational framework that accounts for both diversity and context of the generated future motion. To this end, and in contrast to existing approaches, we condition the sampling of the latent variable that acts as source of diversity on the representation of the past observation, thus encouraging it to carry relevant information. Our experiments demonstrate that our approach yields motions not only of higher quality while retaining diversity, but also that preserve the contextual information contained in the observed 3D pose sequence.

</details>

<details>

<summary>2020-12-05 09:33:43 - A grid-point detection method based on U-net for a structured light system</summary>

- *Dieuthuy Pham, Minhtuan Ha, Changyan Xiao*

- `2012.08641v1` - [abs](http://arxiv.org/abs/2012.08641v1) - [pdf](http://arxiv.org/pdf/2012.08641v1)

> Accurate detection of the feature points of the projected pattern plays an extremely important role in one-shot 3D reconstruction systems, especially for the ones using a grid pattern. To solve this problem, this paper proposes a grid-point detection method based on U-net. A specific dataset is designed that includes the images captured with the two-shot imaging method and the ones acquired with the one-shot imaging method. Among them, the images in the first group after labeled as the ground truth images and the images captured at the same pose with the one-shot method are cut into small patches with the size of 64x64 pixels then feed to the training set. The remaining of the images in the second group is the test set. The experimental results show that our method can achieve a better detecting performance with higher accuracy in comparison with the previous methods.

</details>

<details>

<summary>2020-12-05 12:29:19 - Intrinsic analysis for dual word embedding space models</summary>

- *Mohit Mayank*

- `2012.00728v2` - [abs](http://arxiv.org/abs/2012.00728v2) - [pdf](http://arxiv.org/pdf/2012.00728v2)

> Recent word embeddings techniques represent words in a continuous vector space, moving away from the atomic and sparse representations of the past. Each such technique can further create multiple varieties of embeddings based on different settings of hyper-parameters like embedding dimension size, context window size and training method. One additional variety appears when we especially consider the Dual embedding space techniques which generate not one but two-word embeddings as output. This gives rise to an interesting question - "is there one or a combination of the two word embeddings variety, which works better for a specific task?". This paper tries to answer this question by considering all of these variations. Herein, we compare two classical embedding methods belonging to two different methodologies - Word2Vec from window-based and Glove from count-based. For an extensive evaluation after considering all variations, a total of 84 different models were compared against semantic, association and analogy evaluations tasks which are made up of 9 open-source linguistics datasets. The final Word2vec reports showcase the preference of non-default model for 2 out of 3 tasks. In case of Glove, non-default models outperform in all 3 evaluation tasks.

</details>

<details>

<summary>2020-12-05 18:18:45 - Semantic Segmentation of Medium-Resolution Satellite Imagery using Conditional Generative Adversarial Networks</summary>

- *Aditya Kulkarni, Tharun Mohandoss, Daniel Northrup, Ernest Mwebaze, Hamed Alemohammad*

- `2012.03093v1` - [abs](http://arxiv.org/abs/2012.03093v1) - [pdf](http://arxiv.org/pdf/2012.03093v1)

> Semantic segmentation of satellite imagery is a common approach to identify patterns and detect changes around the planet. Most of the state-of-the-art semantic segmentation models are trained in a fully supervised way using Convolutional Neural Network (CNN). The generalization property of CNN is poor for satellite imagery because the data can be very diverse in terms of landscape types, image resolutions, and scarcity of labels for different geographies and seasons. Hence, the performance of CNN doesn't translate well to images from unseen regions or seasons. Inspired by Conditional Generative Adversarial Networks (CGAN) based approach of image-to-image translation for high-resolution satellite imagery, we propose a CGAN framework for land cover classification using medium-resolution Sentinel-2 imagery. We find that the CGAN model outperforms the CNN model of similar complexity by a significant margin on an unseen imbalanced test dataset.

</details>

<details>

<summary>2020-12-06 22:16:30 - Multimodal Learning for Hateful Memes Detection</summary>

- *Yi Zhou, Zhenhao Chen*

- `2011.12870v3` - [abs](http://arxiv.org/abs/2011.12870v3) - [pdf](http://arxiv.org/pdf/2011.12870v3)

> Memes are used for spreading ideas through social networks. Although most memes are created for humor, some memes become hateful under the combination of pictures and text. Automatically detecting the hateful memes can help reduce their harmful social impact. Unlike the conventional multimodal tasks, where the visual and textual information is semantically aligned, the challenge of hateful memes detection lies in its unique multimodal information. The image and text in memes are weakly aligned or even irrelevant, which requires the model to understand the content and perform reasoning over multiple modalities. In this paper, we focus on multimodal hateful memes detection and propose a novel method that incorporates the image captioning process into the memes detection process. We conduct extensive experiments on multimodal meme datasets and illustrated the effectiveness of our approach. Our model achieves promising results on the Hateful Memes Detection Challenge.

</details>

<details>

<summary>2020-12-07 01:19:08 - Machine Learning Prediction of Gamer's Private Networks</summary>

- *Chris Mazur, Jesse Ayers, Gaetan Hains, Youry Khmelevsky*

- `2012.06480v1` - [abs](http://arxiv.org/abs/2012.06480v1) - [pdf](http://arxiv.org/pdf/2012.06480v1)

> The Gamer's Private Network (GPN) is a client/server technology created by WTFast for making the network performance of online games faster and more reliable. GPN s use middle-mile servers and proprietary algorithms to better connect online video-game players to their game's servers across a wide-area network. Online games are a massive entertainment market and network latency is a key aspect of a player's competitive edge. This market means many different approaches to network architecture are implemented by different competing companies and that those architectures are constantly evolving. Ensuring the optimal connection between a client of WTFast and the online game they wish to play is thus an incredibly difficult problem to automate. Using machine learning, we analyzed historical network data from GPN connections to explore the feasibility of network latency prediction which is a key part of optimization. Our next step will be to collect live data (including client/server load, packet and port information and specific game state information) from GPN Minecraft servers and bots. We will use this information in a Reinforcement Learning model along with predictions about latency to alter the clients' and servers' configurations for optimal network performance. These investigations and experiments will improve the quality of service and reliability of GPN systems.

</details>

<details>

<summary>2020-12-07 02:18:49 - From syntactic structure to semantic relationship: hypernym extraction from definitions by recurrent neural networks using the part of speech information</summary>

- *Yixin Tan, Xiaomeng Wang, Tao Jia*

- `2012.03418v1` - [abs](http://arxiv.org/abs/2012.03418v1) - [pdf](http://arxiv.org/pdf/2012.03418v1)

> The hyponym-hypernym relation is an essential element in the semantic network. Identifying the hypernym from a definition is an important task in natural language processing and semantic analysis. While a public dictionary such as WordNet works for common words, its application in domain-specific scenarios is limited. Existing tools for hypernym extraction either rely on specific semantic patterns or focus on the word representation, which all demonstrate certain limitations.

</details>

<details>

<summary>2020-12-07 03:14:27 - Predict and Use Latent Patterns for Short-Text Conversation</summary>

- *Hung-Ting Chen, Yu-Chieh Chao, Ta-Hsuan Chao, Wei-Yun Ma*

- `2010.13982v2` - [abs](http://arxiv.org/abs/2010.13982v2) - [pdf](http://arxiv.org/pdf/2010.13982v2)

> Many neural network models nowadays have achieved promising performances in Chit-chat settings. The majority of them rely on an encoder for understanding the post and a decoder for generating the response. Without given assigned semantics, the models lack the fine-grained control over responses as the semantic mapping between posts and responses is hidden on the fly within the end-to-end manners. Some previous works utilize sampled latent words as a controllable semantic form to drive the generated response around the work, but few works attempt to use more complex semantic patterns to guide the generation. In this paper, we propose to use more detailed semantic forms, including latent responses and part-of-speech sequences sampled from the corresponding distributions, as the controllable semantics to guide the generation. Our results show that the richer semantics are not only able to provide informative and diverse responses, but also increase the overall performance of response quality, including fluency and coherence.

</details>

<details>

<summary>2020-12-07 05:37:32 - CX DB8: A queryable extractive summarizer and semantic search engine</summary>

- *Allen Roush*

- `2012.03942v1` - [abs](http://arxiv.org/abs/2012.03942v1) - [pdf](http://arxiv.org/pdf/2012.03942v1)

> Competitive Debate's increasingly technical nature has left competitors looking for tools to accelerate evidence production. We find that the unique type of extractive summarization performed by competitive debaters - summarization with a bias towards a particular target meaning - can be performed using the latest innovations in unsupervised pre-trained text vectorization models. We introduce CX_DB8, a queryable word-level extractive summarizer and evidence creation framework, which allows for rapid, biasable summarization of arbitarily sized texts. CX_DB8s usage of the embedding framework Flair means that as the underlying models improve, CX_DB8 will also improve. We observe that CX_DB8 also functions as a semantic search engine, and has application as a supplement to traditional "find" functionality in programs and webpages. CX_DB8 is currently used by competitive debaters and is made available to the public at https://github.com/Hellisotherpeople/CX_DB8

</details>

<details>

<summary>2020-12-07 07:16:47 - Rethinking Learnable Tree Filter for Generic Feature Transform</summary>

- *Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, Nanning Zheng*

- `2012.03482v1` - [abs](http://arxiv.org/abs/2012.03482v1) - [pdf](http://arxiv.org/pdf/2012.03482v1)

> The Learnable Tree Filter presents a remarkable approach to model structure-preserving relations for semantic segmentation. Nevertheless, the intrinsic geometric constraint forces it to focus on the regions with close spatial distance, hindering the effective long-range interactions. To relax the geometric constraint, we give the analysis by reformulating it as a Markov Random Field and introduce a learnable unary term. Besides, we propose a learnable spanning tree algorithm to replace the original non-differentiable one, which further improves the flexibility and robustness. With the above improvements, our method can better capture long-range dependencies and preserve structural details with linear complexity, which is extended to several vision tasks for more generic feature transform. Extensive experiments on object detection/instance segmentation demonstrate the consistent improvements over the original version. For semantic segmentation, we achieve leading performance (82.1% mIoU) on the Cityscapes benchmark without bells-and-whistles. Code is available at https://github.com/StevenGrove/LearnableTreeFilterV2.

</details>

<details>

<summary>2020-12-07 07:56:30 - Analysis of Word Embeddings Using Fuzzy Clustering</summary>

- *Shahin Atakishiyev, Marek Z. Reformat*

- `1907.07672v3` - [abs](http://arxiv.org/abs/1907.07672v3) - [pdf](http://arxiv.org/pdf/1907.07672v3)

> In data dominated systems and applications, a concept of representing words in a numerical format has gained a lot of attention. There are a few approaches used to generate such a representation. An interesting issue that should be considered is the ability of such representations - called embeddings - to imitate human-based semantic similarity between words. In this study, we perform a fuzzy-based analysis of vector representations of words, i.e., word embeddings. We use two popular fuzzy clustering algorithms on count-based word embeddings, known as GloVe, of different dimensionality. Words from WordSim-353, called the gold standard, are represented as vectors and clustered. The results indicate that fuzzy clustering algorithms are very sensitive to high-dimensional data, and parameter tuning can dramatically change their performance. We show that by adjusting the value of the fuzzifier parameter, fuzzy clustering can be successfully applied to vectors of high - up to one hundred - dimensions. Additionally, we illustrate that fuzzy clustering allows to provide interesting results regarding membership of words to different clusters.

</details>

<details>

<summary>2020-12-07 15:57:12 - Vulnerability Forecasting: In theory and practice</summary>

- *Éireann Leverett, Matilda Rhode, Adam Wedgbury*

- `2012.03814v1` - [abs](http://arxiv.org/abs/2012.03814v1) - [pdf](http://arxiv.org/pdf/2012.03814v1)

> Why wait for zero-days when you could predict them in advance? It is possible to predict the volume of CVEs released in the NVD as much as a year in advance. This can be done within 3 percent of the actual value, and different predictive algorithms perform well at different lookahead values. It is also possible to estimate the proportions of that total volumn belonging to specific vendors, software, CVSS scores, or vulnerability types. Strategic patch management should become much easier, with this uncertainty reduction.

</details>

<details>

<summary>2020-12-07 16:06:24 - Public risk perception and emotion on Twitter during the Covid-19 pandemic</summary>

- *Joel Dyer, Blas Kolic*

- `2008.00854v2` - [abs](http://arxiv.org/abs/2008.00854v2) - [pdf](http://arxiv.org/pdf/2008.00854v2)

> Successful navigation of the Covid-19 pandemic is predicated on public cooperation with safety measures and appropriate perception of risk, in which emotion and attention play important roles. Signatures of public emotion and attention are present in social media data, thus natural language analysis of this text enables near-to-real-time monitoring of indicators of public risk perception. We compare key epidemiological indicators of the progression of the pandemic with indicators of the public perception of the pandemic constructed from ~20 million unique Covid-19-related tweets from 12 countries posted between 10th March -- 14th June 2020. We find evidence of psychophysical numbing: Twitter users increasingly fixate on mortality, but in a decreasingly emotional and increasingly analytic tone. Semantic network analysis based on word co-occurrences reveals changes in the emotional framing of Covid-19 casualties that are consistent with this hypothesis. We also find that the average attention afforded to national Covid-19 mortality rates is modelled accurately with the Weber-Fechner and power law functions of sensory perception. Our parameter estimates for these models are consistent with estimates from psychological experiments, and indicate that users in this dataset exhibit differential sensitivity by country to the national Covid-19 death rates. Our work illustrates the potential utility of social media for monitoring public risk perception and guiding public communication during crisis scenarios.

</details>

<details>

<summary>2020-12-07 18:33:45 - Generating Word and Document Embeddings for Sentiment Analysis</summary>

- *Cem Rıfkı Aydın, Tunga Güngör, Ali Erkan*

- `2001.01269v2` - [abs](http://arxiv.org/abs/2001.01269v2) - [pdf](http://arxiv.org/pdf/2001.01269v2)

> Sentiments of words differ from one corpus to another. Inducing general sentiment lexicons for languages and using them cannot, in general, produce meaningful results for different domains. In this paper, we combine contextual and supervised information with the general semantic representations of words occurring in the dictionary. Contexts of words help us capture the domain-specific information and supervised scores of words are indicative of the polarities of those words. When we combine supervised features of words with the features extracted from their dictionary definitions, we observe an increase in the success rates. We try out the combinations of contextual, supervised, and dictionary-based approaches, and generate original vectors. We also combine the word2vec approach with hand-crafted features. We induce domain-specific sentimental vectors for two corpora, which are the movie domain and the Twitter datasets in Turkish. When we thereafter generate document vectors and employ the support vector machines method utilising those vectors, our approaches perform better than the baseline studies for Turkish with a significant margin. We evaluated our models on two English corpora as well and these also outperformed the word2vec approach. It shows that our approaches are cross-domain and portable to other languages.

</details>

<details>

<summary>2020-12-07 18:42:38 - MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation</summary>

- *Saim Wani, Shivansh Patel, Unnat Jain, Angel X. Chang, Manolis Savva*

- `2012.03912v1` - [abs](http://arxiv.org/abs/2012.03912v1) - [pdf](http://arxiv.org/pdf/2012.03912v1)

> Navigation tasks in photorealistic 3D environments are challenging because they require perception and effective planning under partial observability. Recent work shows that map-like memory is useful for long-horizon navigation tasks. However, a focused investigation of the impact of maps on navigation tasks of varying complexity has not yet been performed. We propose the multiON task, which requires navigation to an episode-specific sequence of objects in a realistic environment. MultiON generalizes the ObjectGoal navigation task and explicitly tests the ability of navigation agents to locate previously observed goal objects. We perform a set of multiON experiments to examine how a variety of agent models perform across a spectrum of navigation task complexities. Our experiments show that: i) navigation performance degrades dramatically with escalating task complexity; ii) a simple semantic map agent performs surprisingly well relative to more complex neural image feature map agents; and iii) even oracle map agents achieve relatively low performance, indicating the potential for future work in training embodied navigation agents using maps. Video summary: https://youtu.be/yqTlHNIcgnY

</details>

<details>

<summary>2020-12-07 20:04:39 - Generating unseen complex scenes: are we there yet?</summary>

- *Arantxa Casanova, Michal Drozdzal, Adriana Romero-Soriano*

- `2012.04027v1` - [abs](http://arxiv.org/abs/2012.04027v1) - [pdf](http://arxiv.org/pdf/2012.04027v1)

> Although recent complex scene conditional generation models generate increasingly appealing scenes, it is very hard to assess which models perform better and why. This is often due to models being trained to fit different data splits, and defining their own experimental setups. In this paper, we propose a methodology to compare complex scene conditional generation models, and provide an in-depth analysis that assesses the ability of each model to (1) fit the training distribution and hence perform well on seen conditionings, (2) to generalize to unseen conditionings composed of seen object combinations, and (3) generalize to unseen conditionings composed of unseen object combinations. As a result, we observe that recent methods are able to generate recognizable scenes given seen conditionings, and exploit compositionality to generalize to unseen conditionings with seen object combinations. However, all methods suffer from noticeable image quality degradation when asked to generate images from conditionings composed of unseen object combinations. Moreover, through our analysis, we identify the advantages of different pipeline components, and find that (1) encouraging compositionality through instance-wise spatial conditioning normalizations increases robustness to both types of unseen conditionings, (2) using semantically aware losses such as the scene-graph perceptual similarity helps improve some dimensions of the generation process, and (3) enhancing the quality of generated masks and the quality of the individual objects are crucial steps to improve robustness to both types of unseen conditionings.

</details>

<details>

<summary>2020-12-07 21:19:24 - Dragonblood is Still Leaking: Practical Cache-based Side-Channel in the Wild</summary>

- *Daniel De Almeida Braga, Pierre-Alain Fouque, Mohamed Sabt*

- `2012.02745v2` - [abs](http://arxiv.org/abs/2012.02745v2) - [pdf](http://arxiv.org/pdf/2012.02745v2)

> Recently, the Dragonblood attacks have attracted new interests on the security of WPA-3 implementation and in particular on the Dragonfly code deployed on many open-source libraries. One attack concerns the protection of users passwords during authentication. In the Password Authentication Key Exchange (PAKE) protocol called Dragonfly, the secret, namely the password, is mapped to an elliptic curve point. This operation is sensitive, as it involves the secret password, and therefore its resistance against side-channel attacks is of utmost importance. Following the initial disclosure of Dragonblood, we notice that this particular attack has been partially patched by only a few implementations.   In this work, we show that the patches implemented after the disclosure of Dragonblood are insufficient. We took advantage of state-of-the-art techniques to extend the original attack, demonstrating that we are able to recover the password with only a third of the measurements needed in Dragonblood attack. We mainly apply our attack on two open-source projects: iwd (iNet Wireless Daemon) and FreeRADIUS, in order underline the practicability of our attack. Indeed, the iwd package, written by Intel, is already deployed in the Arch Linux distribution, which is well-known among security experts, and aims to offer an alternative to wpa\_supplicant. As for FreeRADIUS, it is widely deployed and well-maintained upstream open-source project. We publish a full Proof of Concept of our attack, and actively participated in the process of patching the vulnerable code. Here, in a backward compatibility perspective, we advise the use of a branch-free implementation as a mitigation technique, as what was used in hostapd, due to its quite simplicity and its negligible incurred overhead.

</details>

<details>

<summary>2020-12-08 02:22:09 - Improving Human-Labeled Data through Dynamic Automatic Conflict Resolution</summary>

- *David Q. Sun, Hadas Kotek, Christopher Klein, Mayank Gupta, William Li, Jason D. Williams*

- `2012.04169v1` - [abs](http://arxiv.org/abs/2012.04169v1) - [pdf](http://arxiv.org/pdf/2012.04169v1)

> This paper develops and implements a scalable methodology for (a) estimating the noisiness of labels produced by a typical crowdsourcing semantic annotation task, and (b) reducing the resulting error of the labeling process by as much as 20-30% in comparison to other common labeling strategies. Importantly, this new approach to the labeling process, which we name Dynamic Automatic Conflict Resolution (DACR), does not require a ground truth dataset and is instead based on inter-project annotation inconsistencies. This makes DACR not only more accurate but also available to a broad range of labeling tasks. In what follows we present results from a text classification task performed at scale for a commercial personal assistant, and evaluate the inherent ambiguity uncovered by this annotation strategy as compared to other common labeling strategies.

</details>

<details>

<summary>2020-12-08 03:01:50 - Neural Image Compression and Explanation</summary>

- *Xiang Li, Shihao Ji*

- `1908.08988v2` - [abs](http://arxiv.org/abs/1908.08988v2) - [pdf](http://arxiv.org/pdf/1908.08988v2)

> Explaining the prediction of deep neural networks (DNNs) and semantic image compression are two active research areas of deep learning with a numerous of applications in decision-critical systems, such as surveillance cameras, drones and self-driving cars, where interpretable decision is critical and storage/network bandwidth is limited. In this paper, we propose a novel end-to-end Neural Image Compression and Explanation (NICE) framework that learns to (1) explain the predictions of convolutional neural networks (CNNs), and (2) subsequently compress the input images for efficient storage or transmission. Specifically, NICE generates a sparse mask over an input image by attaching a stochastic binary gate to each pixel of the image, whose parameters are learned through the interaction with the CNN classifier to be explained. The generated mask is able to capture the saliency of each pixel measured by its influence to the final prediction of CNN; it can also be used to produce a mixed-resolution image, where important pixels maintain their original high resolution and insignificant background pixels are subsampled to a low resolution. The produced images achieve a high compression rate (e.g., about 0.6x of original image file size), while retaining a similar classification accuracy. Extensive experiments across multiple image classification benchmarks demonstrate the superior performance of NICE compared to the state-of-the-art methods in terms of explanation quality and semantic image compression rate. Our code is available at: https://github.com/lxuniverse/NICE.

</details>

<details>

<summary>2020-12-08 03:46:19 - VAE-Info-cGAN: Generating Synthetic Images by Combining Pixel-level and Feature-level Geospatial Conditional Inputs</summary>

- *Xuerong Xiao, Swetava Ganguli, Vipul Pandey*

- `2012.04196v1` - [abs](http://arxiv.org/abs/2012.04196v1) - [pdf](http://arxiv.org/pdf/2012.04196v1)

> Training robust supervised deep learning models for many geospatial applications of computer vision is difficult due to dearth of class-balanced and diverse training data. Conversely, obtaining enough training data for many applications is financially prohibitive or may be infeasible, especially when the application involves modeling rare or extreme events. Synthetically generating data (and labels) using a generative model that can sample from a target distribution and exploit the multi-scale nature of images can be an inexpensive solution to address scarcity of labeled data. Towards this goal, we present a deep conditional generative model, called VAE-Info-cGAN, that combines a Variational Autoencoder (VAE) with a conditional Information Maximizing Generative Adversarial Network (InfoGAN), for synthesizing semantically rich images simultaneously conditioned on a pixel-level condition (PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC can only vary in the channel dimension from the synthesized image and is meant to be a task-specific input. The FLC is modeled as an attribute vector in the latent space of the generated image which controls the contributions of various characteristic attributes germane to the target distribution. An interpretation of the attribute vector to systematically generate synthetic images by varying a chosen binary macroscopic feature is explored. Experiments on a GPS trajectories dataset show that the proposed model can accurately generate various forms of spatio-temporal aggregates across different geographic locations while conditioned only on a raster representation of the road network. The primary intended application of the VAE-Info-cGAN is synthetic data (and label) generation for targeted data augmentation for computer vision-based modeling of problems relevant to geospatial analysis and remote sensing.

</details>

<details>

<summary>2020-12-08 04:21:40 - A Topological Method for Comparing Document Semantics</summary>

- *Yuqi Kong, Fanchao Meng, Benjamin Carterette*

- `2012.04203v1` - [abs](http://arxiv.org/abs/2012.04203v1) - [pdf](http://arxiv.org/pdf/2012.04203v1)

> Comparing document semantics is one of the toughest tasks in both Natural Language Processing and Information Retrieval. To date, on one hand, the tools for this task are still rare. On the other hand, most relevant methods are devised from the statistic or the vector space model perspectives but nearly none from a topological perspective. In this paper, we hope to make a different sound. A novel algorithm based on topological persistence for comparing semantics similarity between two documents is proposed. Our experiments are conducted on a document dataset with human judges' results. A collection of state-of-the-art methods are selected for comparison. The experimental results show that our algorithm can produce highly human-consistent results, and also beats most state-of-the-art methods though ties with NLTK.

</details>

<details>

<summary>2020-12-08 05:36:08 - Task-Aware Variational Adversarial Active Learning</summary>

- *Kwanyoung Kim, Dongwon Park, Kwang In Kim, Se Young Chun*

- `2002.04709v2` - [abs](http://arxiv.org/abs/2002.04709v2) - [pdf](http://arxiv.org/pdf/2002.04709v2)

> Often, labeling large amount of data is challenging due to high labeling cost limiting the application domain of deep learning techniques. Active learning (AL) tackles this by querying the most informative samples to be annotated among unlabeled pool. Two promising directions for AL that have been recently explored are task-agnostic approach to select data points that are far from the current labeled pool and task-aware approach that relies on the perspective of task model. Unfortunately, the former does not exploit structures from tasks and the latter does not seem to well-utilize overall data distribution. Here, we propose task-aware variational adversarial AL (TA-VAAL) that modifies task-agnostic VAAL, that considered data distribution of both label and unlabeled pools, by relaxing task learning loss prediction to ranking loss prediction and by using ranking conditional generative adversarial network to embed normalized ranking loss information on VAAL. Our proposed TA-VAAL outperforms state-of-the-arts on various benchmark datasets for classifications with balanced / imbalanced labels as well as semantic segmentation and its task-aware and task-agnostic AL properties were confirmed with our in-depth analyses.

</details>

<details>

<summary>2020-12-08 07:11:35 - Variational Interaction Information Maximization for Cross-domain Disentanglement</summary>

- *HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, Kee-Eung Kim*

- `2012.04251v1` - [abs](http://arxiv.org/abs/2012.04251v1) - [pdf](http://arxiv.org/pdf/2012.04251v1)

> Cross-domain disentanglement is the problem of learning representations partitioned into domain-invariant and domain-specific representations, which is a key to successful domain transfer or measuring semantic distance between two domains. Grounded in information theory, we cast the simultaneous learning of domain-invariant and domain-specific representations as a joint objective of multiple information constraints, which does not require adversarial training or gradient reversal layers. We derive a tractable bound of the objective and propose a generative model named Interaction Information Auto-Encoder (IIAE). Our approach reveals insights on the desirable representation for cross-domain disentanglement and its connection to Variational Auto-Encoder (VAE). We demonstrate the validity of our model in the image-to-image translation and the cross-domain retrieval tasks. We further show that our model achieves the state-of-the-art performance in the zero-shot sketch based image retrieval task, even without external knowledge. Our implementation is publicly available at: https://github.com/gr8joo/IIAE

</details>

<details>

<summary>2020-12-08 13:31:49 - Formatting the Landscape: Spatial conditional GAN for varying population in satellite imagery</summary>

- *Tomas Langer, Natalia Fedorova, Ron Hagensieker*

- `2101.05069v1` - [abs](http://arxiv.org/abs/2101.05069v1) - [pdf](http://arxiv.org/pdf/2101.05069v1)

> Climate change is expected to reshuffle the settlement landscape: forcing people in affected areas to migrate, to change their lifeways, and continuing to affect demographic change throughout the world. Changes to the geographic distribution of population will have dramatic impacts on land use and land cover and thus constitute one of the major challenges of planning for climate change scenarios. In this paper, we explore a generative model framework for generating satellite imagery conditional on gridded population distributions. We make additions to the existing ALAE architecture, creating a spatially conditional version: SCALAE. This method allows us to explicitly disentangle population from the model's latent space and thus input custom population forecasts into the generated imagery. We postulate that such imagery could then be directly used for land cover and land use change estimation using existing frameworks, as well as for realistic visualisation of expected local change. We evaluate the model by comparing pixel and semantic reconstructions, as well as calculate the standard FID metric. The results suggest the model captures population distributions accurately and delivers a controllable method to generate realistic satellite imagery.

</details>

<details>

<summary>2020-12-08 20:51:56 - Graph-Based Generative Representation Learning of Semantically and Behaviorally Augmented Floorplans</summary>

- *Vahid Azizi, Muhammad Usman, Honglu Zhou, Petros Faloutsos, Mubbasir Kapadia*

- `2012.04735v1` - [abs](http://arxiv.org/abs/2012.04735v1) - [pdf](http://arxiv.org/pdf/2012.04735v1)

> Floorplans are commonly used to represent the layout of buildings. In computer aided-design (CAD) floorplans are usually represented in the form of hierarchical graph structures. Research works towards computational techniques that facilitate the design process, such as automated analysis and optimization, often use simple floorplan representations that ignore the semantics of the space and do not take into account usage related analytics. We present a floorplan embedding technique that uses an attributed graph to represent the geometric information as well as design semantics and behavioral features of the inhabitants as node and edge attributes. A Long Short-Term Memory (LSTM) Variational Autoencoder (VAE) architecture is proposed and trained to embed attributed graphs as vectors in a continuous space. A user study is conducted to evaluate the coupling of similar floorplans retrieved from the embedding space with respect to a given input (e.g., design layout). The qualitative, quantitative and user-study evaluations show that our embedding framework produces meaningful and accurate vector representations for floorplans. In addition, our proposed model is a generative model. We studied and showcased its effectiveness for generating new floorplans. We also release the dataset that we have constructed and which, for each floorplan, includes the design semantics attributes as well as simulation generated human behavioral features for further study in the community.

</details>

<details>

<summary>2020-12-09 03:10:36 - Towards Coinductive Models for Natural Language Understanding. Bringing together Deep Learning and Deep Semantics</summary>

- *Wlodek W. Zadrozny*

- `2012.05715v1` - [abs](http://arxiv.org/abs/2012.05715v1) - [pdf](http://arxiv.org/pdf/2012.05715v1)

> This article contains a proposal to add coinduction to the computational apparatus of natural language understanding. This, we argue, will provide a basis for more realistic, computationally sound, and scalable models of natural language dialogue, syntax and semantics. Given that the bottom up, inductively constructed, semantic and syntactic structures are brittle, and seemingly incapable of adequately representing the meaning of longer sentences or realistic dialogues, natural language understanding is in need of a new foundation. Coinduction, which uses top down constraints, has been successfully used in the design of operating systems and programming languages. Moreover, implicitly it has been present in text mining, machine translation, and in some attempts to model intensionality and modalities, which provides evidence that it works. This article shows high level formalizations of some of such uses.   Since coinduction and induction can coexist, they can provide a common language and a conceptual model for research in natural language understanding. In particular, such an opportunity seems to be emerging in research on compositionality. This article shows several examples of the joint appearance of induction and coinduction in natural language processing. We argue that the known individual limitations of induction and coinduction can be overcome in empirical settings by a combination of the the two methods. We see an open problem in providing a theory of their joint use.

</details>

<details>

<summary>2020-12-09 03:37:30 - SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data</summary>

- *Shaoli Huang, Xinchao Wang, Dacheng Tao*

- `2012.04846v1` - [abs](http://arxiv.org/abs/2012.04846v1) - [pdf](http://arxiv.org/pdf/2012.04846v1)

> Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly based on the mixture proportion of image pixels. As the main discriminative information of a fine-grained image usually resides in subtle regions, methods along this line are prone to heavy label noise in fine-grained recognition. We propose in this paper a novel scheme, termed as Semantically Proportional Mixing (SnapMix), which exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. SnapMix generates the target label for a mixed image by estimating its intrinsic semantic composition, and allows for asymmetric mixing operations and ensures semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches on various datasets and under different network depths. Furthermore, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a solid baseline for fine-grained recognition. Our code is available at https://github.com/Shaoli-Huang/SnapMix.git.

</details>

<details>

<summary>2020-12-09 09:11:45 - Improving Gradient Flow with Unrolled Highway Expectation Maximization</summary>

- *Chonghyuk Song, Eunseok Kim, Inwook Shim*

- `2012.04926v1` - [abs](http://arxiv.org/abs/2012.04926v1) - [pdf](http://arxiv.org/pdf/2012.04926v1)

> Integrating model-based machine learning methods into deep neural architectures allows one to leverage both the expressive power of deep neural nets and the ability of model-based methods to incorporate domain-specific knowledge. In particular, many works have employed the expectation maximization (EM) algorithm in the form of an unrolled layer-wise structure that is jointly trained with a backbone neural network. However, it is difficult to discriminatively train the backbone network by backpropagating through the EM iterations as they are prone to the vanishing gradient problem. To address this issue, we propose Highway Expectation Maximization Networks (HEMNet), which is comprised of unrolled iterations of the generalized EM (GEM) algorithm based on the Newton-Rahpson method. HEMNet features scaled skip connections, or highways, along the depths of the unrolled architecture, resulting in improved gradient flow during backpropagation while incurring negligible additional computation and memory costs compared to standard unrolled EM. Furthermore, HEMNet preserves the underlying EM procedure, thereby fully retaining the convergence properties of the original EM algorithm. We achieve significant improvement in performance on several semantic segmentation benchmarks and empirically show that HEMNet effectively alleviates gradient decay.

</details>

<details>

<summary>2020-12-09 09:34:25 - AMVNet: Assertion-based Multi-View Fusion Network for LiDAR Semantic Segmentation</summary>

- *Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Widjaja, Dhananjai Sharma, Zhuang Jie Chong*

- `2012.04934v1` - [abs](http://arxiv.org/abs/2012.04934v1) - [pdf](http://arxiv.org/pdf/2012.04934v1)

> In this paper, we present an Assertion-based Multi-View Fusion network (AMVNet) for LiDAR semantic segmentation which aggregates the semantic features of individual projection-based networks using late fusion. Given class scores from different projection-based networks, we perform assertion-guided point sampling on score disagreements and pass a set of point-level features for each sampled point to a simple point head which refines the predictions. This modular-and-hierarchical late fusion approach provides the flexibility of having two independent networks with a minor overhead from a light-weight network. Such approaches are desirable for robotic systems, e.g. autonomous vehicles, for which the computational and memory resources are often limited. Extensive experiments show that AMVNet achieves state-of-the-art results in both the SemanticKITTI and nuScenes benchmark datasets and that our approach outperforms the baseline method of combining the class scores of the projection-based networks.

</details>

<details>

<summary>2020-12-09 11:34:35 - Label Confusion Learning to Enhance Text Classification Models</summary>

- *Biyang Guo, Songqiao Han, Xiao Han, Hailiang Huang, Ting Lu*

- `2012.04987v1` - [abs](http://arxiv.org/abs/2012.04987v1) - [pdf](http://arxiv.org/pdf/2012.04987v1)

> Representing a true label as a one-hot vector is a common practice in training text classification models. However, the one-hot representation may not adequately reflect the relation between the instances and labels, as labels are often not completely independent and instances may relate to multiple labels in practice. The inadequate one-hot representations tend to train the model to be over-confident, which may result in arbitrary prediction and model overfitting, especially for confused datasets (datasets with very similar labels) or noisy datasets (datasets with labeling errors). While training models with label smoothing (LS) can ease this problem in some degree, it still fails to capture the realistic relation among labels. In this paper, we propose a novel Label Confusion Model (LCM) as an enhancement component to current popular text classification models. LCM can learn label confusion to capture semantic overlap among labels by calculating the similarity between instances and labels during training and generate a better label distribution to replace the original one-hot label vector, thus improving the final classification performance. Extensive experiments on five text classification benchmark datasets reveal the effectiveness of LCM for several widely used deep learning classification models. Further experiments also verify that LCM is especially helpful for confused or noisy datasets and superior to the label smoothing method.

</details>

<details>

<summary>2020-12-09 11:59:58 - Tracking Interaction States for Multi-Turn Text-to-SQL Semantic Parsing</summary>

- *Run-Ze Wang, Zhen-Hua Ling, Jing-Bo Zhou, Yu Hu*

- `2012.04995v1` - [abs](http://arxiv.org/abs/2012.04995v1) - [pdf](http://arxiv.org/pdf/2012.04995v1)

> The task of multi-turn text-to-SQL semantic parsing aims to translate natural language utterances in an interaction into SQL queries in order to answer them using a database which normally contains multiple table schemas. Previous studies on this task usually utilized contextual information to enrich utterance representations and to further influence the decoding process. While they ignored to describe and track the interaction states which are determined by history SQL queries and are related with the intent of current utterance. In this paper, two kinds of interaction states are defined based on schema items and SQL keywords separately. A relational graph neural network and a non-linear layer are designed to update the representations of these two states respectively. The dynamic schema-state and SQL-state representations are then utilized to decode the SQL query corresponding to current utterance. Experimental results on the challenging CoSQL dataset demonstrate the effectiveness of our proposed method, which achieves better performance than other published methods on the task leaderboard.

</details>

<details>

<summary>2020-12-09 13:53:18 - Deep Mixtures of Unigrams for uncovering Topics in Textual Data</summary>

- *Cinzia Viroli, Laura Anderlucci*

- `1902.06615v2` - [abs](http://arxiv.org/abs/1902.06615v2) - [pdf](http://arxiv.org/pdf/1902.06615v2)

> Mixtures of Unigrams are one of the simplest and most efficient tools for clustering textual data, as they assume that documents related to the same topic have similar distributions of terms, naturally described by Multinomials. When the classification task is particularly challenging, such as when the document-term matrix is high-dimensional and extremely sparse, a more composite representation can provide better insight on the grouping structure. In this work, we developed a deep version of mixtures of Unigrams for the unsupervised classification of very short documents with a large number of terms, by allowing for models with further deeper latent layers; the proposal is derived in a Bayesian framework. The behaviour of the Deep Mixtures of Unigrams is empirically compared with that of other traditional and state-of-the-art methods, namely $k$-means with cosine distance, $k$-means with Euclidean distance on data transformed according to Semantic Analysis, Partition Around Medoids, Mixture of Gaussians on semantic-based transformed data, hierarchical clustering according to Ward's method with cosine dissimilarity, Latent Dirichlet Allocation, Mixtures of Unigrams estimated via the EM algorithm, Spectral Clustering and Affinity Propagation clustering. The performance is evaluated in terms of both correct classification rate and Adjusted Rand Index. Simulation studies and real data analysis prove that going deep in clustering such data highly improves the classification accuracy.

</details>

<details>

<summary>2020-12-10 03:49:23 - Developing Motion Code Embedding for Action Recognition in Videos</summary>

- *Maxat Alibayev, David Paulius, Yu Sun*

- `2012.05438v1` - [abs](http://arxiv.org/abs/2012.05438v1) - [pdf](http://arxiv.org/pdf/2012.05438v1)

> In this work, we propose a motion embedding strategy known as motion codes, which is a vectorized representation of motions based on a manipulation's salient mechanical attributes. These motion codes provide a robust motion representation, and they are obtained using a hierarchy of features called the motion taxonomy. We developed and trained a deep neural network model that combines visual and semantic features to identify the features found in our motion taxonomy to embed or annotate videos with motion codes. To demonstrate the potential of motion codes as features for machine learning tasks, we integrated the extracted features from the motion embedding model into the current state-of-the-art action recognition model. The obtained model achieved higher accuracy than the baseline model for the verb classification task on egocentric videos from the EPIC-KITCHENS dataset.

</details>

<details>

<summary>2020-12-10 05:16:18 - Beyond Class-Conditional Assumption: A Primary Attempt to Combat Instance-Dependent Label Noise</summary>

- *Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, Pheng-Ann Heng*

- `2012.05458v1` - [abs](http://arxiv.org/abs/2012.05458v1) - [pdf](http://arxiv.org/pdf/2012.05458v1)

> Supervised learning under label noise has seen numerous advances recently, while existing theoretical findings and empirical results broadly build up on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label. In this work, we present a theoretical hypothesis testing and prove that noise in real-world dataset is unlikely to be CCN, which confirms that label noise should depend on the instance and justifies the urgent need to go beyond the CCN assumption.The theoretical results motivate us to study the more general and practical-relevant instance-dependent noise (IDN). To stimulate the development of theory and methodology on IDN, we formalize an algorithm to generate controllable IDN and present both theoretical and empirical evidence to show that IDN is semantically meaningful and challenging. As a primary attempt to combat IDN, we present a tiny algorithm termed self-evolution average label (SEAL), which not only stands out under IDN with various noise fractions, but also improves the generalization on real-world noise benchmark Clothing1M. Our code is released. Notably, our theoretical analysis in Section 2 provides rigorous motivations for studying IDN, which is an important topic that deserves more research attention in future.

</details>

<details>

<summary>2020-12-10 06:51:13 - A Practical Approach towards Causality Mining in Clinical Text using Active Transfer Learning</summary>

- *Musarrat Hussain, Fahad Ahmed Satti, Jamil Hussain, Taqdir Ali, Syed Imran Ali, Hafiz Syed Muhammad Bilal, Gwang Hoon Park, Sungyoung Lee*

- `2012.07563v1` - [abs](http://arxiv.org/abs/2012.07563v1) - [pdf](http://arxiv.org/pdf/2012.07563v1)

> Objective: Causality mining is an active research area, which requires the application of state-of-the-art natural language processing techniques. In the healthcare domain, medical experts create clinical text to overcome the limitation of well-defined and schema driven information systems. The objective of this research work is to create a framework, which can convert clinical text into causal knowledge. Methods: A practical approach based on term expansion, phrase generation, BERT based phrase embedding and semantic matching, semantic enrichment, expert verification, and model evolution has been used to construct a comprehensive causality mining framework. This active transfer learning based framework along with its supplementary services, is able to extract and enrich, causal relationships and their corresponding entities from clinical text. Results: The multi-model transfer learning technique when applied over multiple iterations, gains performance improvements in terms of its accuracy and recall while keeping the precision constant. We also present a comparative analysis of the presented techniques with their common alternatives, which demonstrate the correctness of our approach and its ability to capture most causal relationships. Conclusion: The presented framework has provided cutting-edge results in the healthcare domain. However, the framework can be tweaked to provide causality detection in other domains, as well. Significance: The presented framework is generic enough to be utilized in any domain, healthcare services can gain massive benefits due to the voluminous and various nature of its data. This causal knowledge extraction framework can be used to summarize clinical text, create personas, discover medical knowledge, and provide evidence to clinical decision making.

</details>

<details>

<summary>2020-12-10 07:57:44 - Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation</summary>

- *Daizong Liu, Shuangjie Xu, Xiao-Yang Liu, Zichuan Xu, Wei Wei, Pan Zhou*

- `2012.05499v1` - [abs](http://arxiv.org/abs/2012.05499v1) - [pdf](http://arxiv.org/pdf/2012.05499v1)

> This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusion and missing. Without online learning and fine-tuning, our STG-Net achieves state-of-the-art performance on four large benchmarks (DAVIS, YouTube-VOS, SegTrack-v2, and YouTube-Objects), demonstrating the effectiveness of the proposed approach.

</details>

<details>

<summary>2020-12-10 11:03:37 - Learning event representations for temporal segmentation of image sequences by dynamic graph embedding</summary>

- *Mariella Dimiccoli, Herwig Wendt*

- `1910.03483v3` - [abs](http://arxiv.org/abs/1910.03483v3) - [pdf](http://arxiv.org/pdf/1910.03483v3)

> Recently, self-supervised learning has proved to be effective to learn representations of events suitable for temporal segmentation in image sequences, where events are understood as sets of temporally adjacent images that are semantically perceived as a whole. However, although this approach does not require expensive manual annotations, it is data hungry and suffers from domain adaptation problems. As an alternative, in this work, we propose a novel approach for learning event representations named Dynamic Graph Embedding (DGE). The assumption underlying our model is that a sequence of images can be represented by a graph that encodes both semantic and temporal similarity. The key novelty of DGE is to learn jointly the graph and its graph embedding. At its core, DGE works by iterating over two steps: 1) updating the graph representing the semantic and temporal similarity of the data based on the current data representation, and 2) updating the data representation to take into account the current data graph structure. The main advantage of DGE over state-of-the-art self-supervised approaches is that it does not require any training set, but instead learns iteratively from the data itself a low-dimensional embedding that reflects their temporal and semantic similarity. Experimental results on two benchmark datasets of real image sequences captured at regular time intervals demonstrate that the proposed DGE leads to event representations effective for temporal segmentation. In particular, it achieves robust temporal segmentation on the EDUBSeg and EDUBSeg-Desc benchmark datasets, outperforming the state of the art. Additional experiments on two Human Motion Segmentation benchmark datasets demonstrate the generalization capabilities of the proposed DGE.

</details>

<details>

<summary>2020-12-10 15:18:15 - Interpreting Neural Networks as Gradual Argumentation Frameworks (Including Proof Appendix)</summary>

- *Nico Potyka*

- `2012.05738v1` - [abs](http://arxiv.org/abs/2012.05738v1) - [pdf](http://arxiv.org/pdf/2012.05738v1)

> We show that an interesting class of feed-forward neural networks can be understood as quantitative argumentation frameworks. This connection creates a bridge between research in Formal Argumentation and Machine Learning. We generalize the semantics of feed-forward neural networks to acyclic graphs and study the resulting computational and semantical properties in argumentation graphs. As it turns out, the semantics gives stronger guarantees than existing semantics that have been tailor-made for the argumentation setting. From a machine-learning perspective, the connection does not seem immediately helpful. While it gives intuitive meaning to some feed-forward-neural networks, they remain difficult to understand due to their size and density. However, the connection seems helpful for combining background knowledge in form of sparse argumentation networks with dense neural networks that have been trained for complementary purposes and for learning the parameters of quantitative argumentation frameworks in an end-to-end fashion from data.

</details>

<details>

<summary>2020-12-10 15:39:51 - MeshWalker: Deep Mesh Understanding by Random Walks</summary>

- *Alon Lahav, Ayellet Tal*

- `2006.05353v3` - [abs](http://arxiv.org/abs/2006.05353v3) - [pdf](http://arxiv.org/pdf/2006.05353v3)

> Most attempts to represent 3D shapes for deep learning have focused on volumetric grids, multi-view images and point clouds. In this paper we look at the most popular representation of 3D shapes in computer graphics - a triangular mesh - and ask how it can be utilized within deep learning. The few attempts to answer this question propose to adapt convolutions & pooling to suit Convolutional Neural Networks (CNNs). This paper proposes a very different approach, termed MeshWalker, to learn the shape directly from a given mesh. The key idea is to represent the mesh by random walks along the surface, which "explore" the mesh's geometry and topology. Each walk is organized as a list of vertices, which in some manner imposes regularity on the mesh. The walk is fed into a Recurrent Neural Network (RNN) that "remembers" the history of the walk. We show that our approach achieves state-of-the-art results for two fundamental shape analysis tasks: shape classification and semantic segmentation. Furthermore, even a very small number of examples suffices for learning. This is highly important, since large datasets of meshes are difficult to acquire.

</details>

<details>

<summary>2020-12-10 18:07:30 - Giving Semantics to Program-Counter Labels via Secure Effects</summary>

- *Andrew K. Hirsch, Ethan Cecchetti*

- `2010.13191v2` - [abs](http://arxiv.org/abs/2010.13191v2) - [pdf](http://arxiv.org/pdf/2010.13191v2)

> Type systems designed for information-flow control commonly use a program-counter label to track the sensitivity of the context and rule out data leakage arising from effectful computation in a sensitive context. Currently, type-system designers reason about this label informally except in security proofs, where they use ad-hoc techniques. We develop a framework based on monadic semantics for effects to give semantics to program-counter labels. This framework leads to three results about program-counter labels. First, we develop a new proof technique for noninterference, the core security theorem for information-flow control in effectful languages. Second, we unify notions of security for different types of effects, including state, exceptions, and nontermination. Finally, we formalize the folklore that program-counter labels are a lower bound on effects. We show that, while not universally true, this folklore has a good semantic foundation.

</details>

<details>

<summary>2020-12-10 18:18:34 - GNN-XML: Graph Neural Networks for Extreme Multi-label Text Classification</summary>

- *Daoming Zong, Shiliang Sun*

- `2012.05860v1` - [abs](http://arxiv.org/abs/2012.05860v1) - [pdf](http://arxiv.org/pdf/2012.05860v1)

> Extreme multi-label text classification (XMTC) aims to tag a text instance with the most relevant subset of labels from an extremely large label set. XMTC has attracted much recent attention due to massive label sets yielded by modern applications, such as news annotation and product recommendation. The main challenges of XMTC are the data scalability and sparsity, thereby leading to two issues: i) the intractability to scale to the extreme label setting, ii) the presence of long-tailed label distribution, implying that a large fraction of labels have few positive training instances. To overcome these problems, we propose GNN-XML, a scalable graph neural network framework tailored for XMTC problems. Specifically, we exploit label correlations via mining their co-occurrence patterns and build a label graph based on the correlation matrix. We then conduct the attributed graph clustering by performing graph convolution with a low-pass graph filter to jointly model label dependencies and label features, which induces semantic label clusters. We further propose a bilateral-branch graph isomorphism network to decouple representation learning and classifier learning for better modeling tail labels. Experimental results on multiple benchmark datasets show that GNN-XML significantly outperforms state-of-the-art methods while maintaining comparable prediction efficiency and model size.

</details>

<details>

<summary>2020-12-10 18:58:10 - Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation</summary>

- *Hugues Thomas, Ben Agro, Mona Gridseth, Jian Zhang, Timothy D. Barfoot*

- `2012.05897v1` - [abs](http://arxiv.org/abs/2012.05897v1) - [pdf](http://arxiv.org/pdf/2012.05897v1)

> We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.

</details>

<details>

<summary>2020-12-10 20:29:34 - Multilingual Transfer Learning for QA Using Translation as Data Augmentation</summary>

- *Mihaela Bornea, Lin Pan, Sara Rosenthal, Radu Florian, Avirup Sil*

- `2012.05958v1` - [abs](http://arxiv.org/abs/2012.05958v1) - [pdf](http://arxiv.org/pdf/2012.05958v1)

> Prior work on multilingual question answering has mostly focused on using large multilingual pre-trained language models (LM) to perform zero-shot language-wise learning: train a QA model on English and test on other languages. In this work, we explore strategies that improve cross-lingual transfer by bringing the multilingual embeddings closer in the semantic space. Our first strategy augments the original English training data with machine translation-generated data. This results in a corpus of multilingual silver-labeled QA pairs that is 14 times larger than the original training set. In addition, we propose two novel strategies, language adversarial training and language arbitration framework, which significantly improve the (zero-resource) cross-lingual transfer performance and result in LM embeddings that are less language-variant. Empirically, we show that the proposed models outperform the previous zero-shot baseline on the recently introduced multilingual MLQA and TyDiQA datasets.

</details>

<details>

<summary>2020-12-10 21:34:59 - m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks</summary>

- *Salman Maqbool, Aqsa Riaz, Hasan Sajid, Osman Hasan*

- `2008.10134v2` - [abs](http://arxiv.org/abs/2008.10134v2) - [pdf](http://arxiv.org/pdf/2008.10134v2)

> Autonomous surgical procedures, in particular minimal invasive surgeries, are the next frontier for Artificial Intelligence research. However, the existing challenges include precise identification of the human anatomy and the surgical settings, and modeling the environment for training of an autonomous agent. To address the identification of human anatomy and the surgical settings, we propose a deep learning based semantic segmentation algorithm to identify and label the tissues and organs in the endoscopic video feed of the human torso region. We present an annotated dataset, m2caiSeg, created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene. We propose and train a deep convolutional neural network for the semantic segmentation task. To cater for the low quantity of annotated data, we use unsupervised pre-training and data augmentation. The trained model is evaluated on an independent test set of the proposed dataset. We obtained a F1 score of 0.33 while using all the labeled categories for the semantic segmentation task. Secondly, we labeled all instruments into an 'Instruments' superclass to evaluate the model's performance on discerning the various organs and obtained a F1 score of 0.57. We propose a new dataset and a deep learning method for pixel level identification of various organs and instruments in a endoscopic surgical scene. Surgical scene understanding is one of the first steps towards automating surgical procedures.

</details>

<details>

<summary>2020-12-10 21:50:35 - Strong Admissibility for Abstract Dialectical Frameworks</summary>

- *Atefeh Keshavarzi Zafarghandi, Rineke Verbrugge, Bart Verheij*

- `2012.05997v1` - [abs](http://arxiv.org/abs/2012.05997v1) - [pdf](http://arxiv.org/pdf/2012.05997v1)

> Abstract dialectical frameworks (ADFs) have been introduced as a formalism for modeling and evaluating argumentation allowing general logical satisfaction conditions. Different criteria used to settle the acceptance of arguments are called semantics. Semantics of ADFs have so far mainly been defined based on the concept of admissibility. However, the notion of strongly admissible semantics studied for abstract argumentation frameworks has not yet been introduced for ADFs. In the current work we present the concept of strong admissibility of interpretations for ADFs. Further, we show that strongly admissible interpretations of ADFs form a lattice with the grounded interpretation as top element.

</details>

<details>

<summary>2020-12-11 01:01:45 - SKATE: A Natural Language Interface for Encoding Structured Knowledge</summary>

- *Clifton McFate, Aditya Kalyanpur, Dave Ferrucci, Andrea Bradshaw, Ariel Diertani, David Melville, Lori Moon*

- `2010.10597v2` - [abs](http://arxiv.org/abs/2010.10597v2) - [pdf](http://arxiv.org/pdf/2010.10597v2)

> In Natural Language (NL) applications, there is often a mismatch between what the NL interface is capable of interpreting and what a lay user knows how to express. This work describes a novel natural language interface that reduces this mismatch by refining natural language input through successive, automatically generated semi-structured templates. In this paper we describe how our approach, called SKATE, uses a neural semantic parser to parse NL input and suggest semi-structured templates, which are recursively filled to produce fully structured interpretations. We also show how SKATE integrates with a neural rule-generation model to interactively suggest and acquire commonsense knowledge. We provide a preliminary coverage analysis of SKATE for the task of story understanding, and then describe a current business use-case of the tool in a specific domain: COVID-19 policy design.

</details>

<details>

<summary>2020-12-11 03:44:58 - Knowledge of Uncertain Worlds: Programming with Logical Constraints</summary>

- *Yanhong A. Liu, Scott D. Stoller*

- `1910.10346v3` - [abs](http://arxiv.org/abs/1910.10346v3) - [pdf](http://arxiv.org/pdf/1910.10346v3)

> Programming with logic for sophisticated applications must deal with recursion and negation, which together have created significant challenges in logic, leading to many different, conflicting semantics of rules. This paper describes a unified language, DA logic, for design and analysis logic, based on the unifying founded semantics and constraint semantics, that support the power and ease of programming with different intended semantics. The key idea is to provide meta-constraints, supports the use of uncertain information in the form of either undefined values or possible combinations of values or both, and promote the use of knowledge units that can be instantiated by any new predicates, including predicates with additional arguments.

</details>

<details>

<summary>2020-12-11 05:43:12 - Classifying Breast Histopathology Images with a Ductal Instance-Oriented Pipeline</summary>

- *Beibin Li, Ezgi Mercan, Sachin Mehta, Stevan Knezevich, Corey W. Arnold, Donald L. Weaver, Joann G. Elmore, Linda G. Shapiro*

- `2012.06136v1` - [abs](http://arxiv.org/abs/2012.06136v1) - [pdf](http://arxiv.org/pdf/2012.06136v1)

> In this study, we propose the Ductal Instance-Oriented Pipeline (DIOP) that contains a duct-level instance segmentation model, a tissue-level semantic segmentation model, and three-levels of features for diagnostic classification. Based on recent advancements in instance segmentation and the Mask R-CNN model, our duct-level segmenter tries to identify each ductal individual inside a microscopic image; then, it extracts tissue-level information from the identified ductal instances. Leveraging three levels of information obtained from these ductal instances and also the histopathology image, the proposed DIOP outperforms previous approaches (both feature-based and CNN-based) in all diagnostic tasks; for the four-way classification task, the DIOP achieves comparable performance to general pathologists in this unique dataset. The proposed DIOP only takes a few seconds to run in the inference time, which could be used interactively on most modern computers. More clinical explorations are needed to study the robustness and generalizability of this system in the future.

</details>

<details>

<summary>2020-12-11 10:52:04 - Improving Zero Shot Learning Baselines with Commonsense Knowledge</summary>

- *Abhinaba Roy, Deepanway Ghosal, Erik Cambria, Navonil Majumder, Rada Mihalcea, Soujanya Poria*

- `2012.06236v1` - [abs](http://arxiv.org/abs/2012.06236v1) - [pdf](http://arxiv.org/pdf/2012.06236v1)

> Zero shot learning -- the problem of training and testing on a completely disjoint set of classes -- relies greatly on its ability to transfer knowledge from train classes to test classes. Traditionally semantic embeddings consisting of human defined attributes (HA) or distributed word embeddings (DWE) are used to facilitate this transfer by improving the association between visual and semantic embeddings. In this paper, we take advantage of explicit relations between nodes defined in ConceptNet, a commonsense knowledge graph, to generate commonsense embeddings of the class labels by using a graph convolution network-based autoencoder. Our experiments performed on three standard benchmark datasets surpass the strong baselines when we fuse our commonsense embeddings with existing semantic embeddings i.e. HA and DWE.

</details>

<details>

<summary>2020-12-11 12:31:50 - Analysis of Feature Representations for Anomalous Sound Detection</summary>

- *Robert Müller, Steffen Illium, Fabian Ritz, Kyrill Schmid*

- `2012.06282v1` - [abs](http://arxiv.org/abs/2012.06282v1) - [pdf](http://arxiv.org/pdf/2012.06282v1)

> In this work, we thoroughly evaluate the efficacy of pretrained neural networks as feature extractors for anomalous sound detection. In doing so, we leverage the knowledge that is contained in these neural networks to extract semantically rich features (representations) that serve as input to a Gaussian Mixture Model which is used as a density estimator to model normality. We compare feature extractors that were trained on data from various domains, namely: images, environmental sounds and music. Our approach is evaluated on recordings from factory machinery such as valves, pumps, sliders and fans. All of the evaluated representations outperform the autoencoder baseline with music based representations yielding the best performance in most cases. These results challenge the common assumption that closely matching the domain of the feature extractor and the downstream task results in better downstream task performance.

</details>

<details>

<summary>2020-12-11 14:32:37 - Type-Centric Kotlin Compiler Fuzzing: Preserving Test Program Correctness by Preserving Types</summary>

- *Daniil Stepanov, Marat Akhin, Mikhail Belyaev*

- `2012.06382v1` - [abs](http://arxiv.org/abs/2012.06382v1) - [pdf](http://arxiv.org/pdf/2012.06382v1)

> Kotlin is a relatively new programming language from JetBrains: its development started in 2010 with release 1.0 done in early 2016. The Kotlin compiler, while slowly and steadily becoming more and more mature, still crashes from time to time on the more tricky input programs, not least because of the complexity of its features and their interactions. This makes it a great target for fuzzing, even the basic forms of which can find a significant number of Kotlin compiler crashes.   There is a problem with fuzzing, however, closely related to the cause of the crashes: generating a random, non-trivial and semantically valid Kotlin program is hard. In this paper, we talk about type-centric compiler fuzzing in the form of type-centric enumeration, an approach inspired by skeletal program enumeration and based on a combination of generative and mutation-based fuzzing, which solves this problem by focusing on program types. After creating the skeleton program, we fill the typed holes with fragments of suitable type, created via generation and enhanced by semantic-aware mutation.   We implemented this approach in our Kotlin compiler fuzzing framework called Backend Bug Finder (BBF) and did an extensive evaluation, not only testing the real-world feasibility of our approach, but also comparing it to other compiler fuzzing techniques. The results show our approach to be significantly better compared to other fuzzing approaches at generating semantically valid Kotlin programs, while creating more interesting crash-inducing inputs at the same time. We managed to find more than 50 previously unknown compiler crashes, of which 18 were considered important after their triage by the compiler team.

</details>

<details>

<summary>2020-12-11 17:16:57 - Adaptive Self-training for Few-shot Neural Sequence Labeling</summary>

- *Yaqing Wang, Subhabrata Mukherjee, Haoda Chu, Yuancheng Tu, Ming Wu, Jing Gao, Ahmed Hassan Awadallah*

- `2010.03680v2` - [abs](http://arxiv.org/abs/2010.03680v2) - [pdf](http://arxiv.org/pdf/2010.03680v2)

> Sequence labeling is an important technique employed for many Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER), slot tagging for dialog systems and semantic parsing. Large-scale pre-trained language models obtain very good performance on these tasks when fine-tuned on large amounts of task-specific labeled data. However, such large-scale labeled datasets are difficult to obtain for several tasks and domains due to the high cost of human annotation as well as privacy and data access constraints for sensitive user applications. This is exacerbated for sequence labeling tasks requiring such annotations at token-level. In this work, we develop techniques to address the label scarcity challenge for neural sequence labeling models. Specifically, we develop self-training and meta-learning techniques for training neural sequence taggers with few labels. While self-training serves as an effective mechanism to learn from large amounts of unlabeled data -- meta-learning helps in adaptive sample re-weighting to mitigate error propagation from noisy pseudo-labels. Extensive experiments on six benchmark datasets including two for massive multilingual NER and four slot tagging datasets for task-oriented dialog systems demonstrate the effectiveness of our method. With only 10 labeled examples for each class for each task, our method obtains 10% improvement over state-of-the-art systems demonstrating its effectiveness for the low-resource setting.

</details>

<details>

<summary>2020-12-11 17:19:18 - IBIR: Bug Report driven Fault Injection</summary>

- *Ahmed Khanfir, Anil Koyuncu, Mike Papadakis, Maxime Cordy, Tegawendé F. Bissyandé, Jacques Klein, Yves Le Traon*

- `2012.06506v1` - [abs](http://arxiv.org/abs/2012.06506v1) - [pdf](http://arxiv.org/pdf/2012.06506v1)

> Much research on software engineering and software testing relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it "blindly" injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce IBIR, a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by retargeting a bug report driven automated program repair system, i.e., reversing its code transformation templates. IBIR is further appealing in practice since it requires deep knowledge of neither of the code nor the tests, but just of the program's relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess IBIR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant, estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, IBIR injects faults that couple with the real ones in 36% of the cases, while mutants from mutation testing inject less than 1%. Overall, IBIR targets real functionality and injects realistic and diverse faults.

</details>

<details>

<summary>2020-12-11 17:48:56 - Software Language Comprehension using a Program-Derived Semantics Graph</summary>

- *Roshni G. Iyer, Yizhou Sun, Wei Wang, Justin Gottschlich*

- `2004.00768v3` - [abs](http://arxiv.org/abs/2004.00768v3) - [pdf](http://arxiv.org/pdf/2004.00768v3)

> Traditional code transformation structures, such as abstract syntax trees (ASTs), conteXtual flow graphs (XFGs), and more generally, compiler intermediate representations (IRs), may have limitations in extracting higher-order semantics from code. While work has already begun on higher-order semantics lifting (e.g., Aroma's simplified parse tree (SPT), verified lifting's lambda calculi, and Halide's intentional domain specific language (DSL)), research in this area is still immature. To continue to advance this research, we present the program-derived semantics graph, a new graphical structure to capture semantics of code. The PSG is designed to provide a single structure for capturing program semantics at multiple levels of abstraction. The PSG may be in a class of emerging structural representations that cannot be built from a traditional set of predefined rules and instead must be learned. In this paper, we describe the PSG and its fundamental structural differences compared to state-of-the-art structures. Although our exploration into the PSG is in its infancy, our early results and architectural analysis indicate it is a promising new research direction to automatically extract program semantics.

</details>

<details>

<summary>2020-12-11 22:37:22 - Cooperative Location Privacy in Vehicular Networks: Why Simple Mix-zones are not Enough</summary>

- *Mohammad Khodaei, Panos Papadimitratos*

- `2012.06666v1` - [abs](http://arxiv.org/abs/2012.06666v1) - [pdf](http://arxiv.org/pdf/2012.06666v1)

> Vehicular communications disclose rich information about the vehicles and their whereabouts. Pseudonymous authentication secures communication while enhancing user privacy. To enhance location privacy, cryptographic mix-zones were proposed to facilitate vehicles covertly transition to new ephemeral credentials. The resilience to (syntactic and semantic) pseudonym linking (attacks) highly depends on the geometry of the mix-zones, mobility patterns, vehicle density, and arrival rates. We introduce a tracking algorithm for linking pseudonyms before and after a cryptographically protected mix-zone. Our experimental results show that an eavesdropper, leveraging standardized vehicular communication messages and road layout, could successfully link 73% of pseudonyms during non-rush hours and 62% of pseudonyms during rush hours after vehicles change their pseudonyms in a mix-zone. To mitigate such inference attacks, we present a novel cooperative mix-zone scheme that enhances user privacy regardless of the vehicle mobility patterns, vehicle density, and arrival rate to the mix-zone. A subset of vehicles, termed relaying vehicles, are selected to be responsible for emulating non-existing vehicles. Such vehicles cooperatively disseminate decoy traffic without affecting safety-critical operations: with 50% of vehicles as relaying vehicles, the probability of linking pseudonyms (for the entire interval) drops from 68% to 18%. On average, this imposes 28 ms extra computation overhead, per second, on the Roadside Units (RSUs) and 4.67 ms extra computation overhead, per second, on the (relaying) vehicle side; it also introduces 1.46 KB/sec extra communication overhead by (relaying) vehicles and 45 KB/sec by RSUs for the dissemination of decoy traffic. Thus, user privacy is enhanced at the cost of low computation and communication overhead.

</details>

<details>

<summary>2020-12-11 22:58:05 - Synonymy = Translational Equivalence</summary>

- *Bradley Hauer, Grzegorz Kondrak*

- `2004.13886v2` - [abs](http://arxiv.org/abs/2004.13886v2) - [pdf](http://arxiv.org/pdf/2004.13886v2)

> Synonymy and translational equivalence are the relations of sameness of meaning within and across languages. As the principal relations in wordnets and multi-wordnets, they are vital to computational lexical semantics, yet the field suffers from the absence of a common formal framework to define their properties and relationship. This paper proposes a unifying treatment of these two relations, which is validated by experiments on existing resources. In our view, synonymy and translational equivalence are simply different types of semantic identity. The theory establishes a solid foundation for critically re-evaluating prior work in cross-lingual semantics, and facilitating the creation, verification, and amelioration of lexical resources.

</details>

<details>

<summary>2020-12-11 23:03:11 - Semantics of the Black-Box: Can knowledge graphs help make deep learning systems more interpretable and explainable?</summary>

- *Manas Gaur, Keyur Faldu, Amit Sheth*

- `2010.08660v4` - [abs](http://arxiv.org/abs/2010.08660v4) - [pdf](http://arxiv.org/pdf/2010.08660v4)

> The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. The DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, signal processing, and human-computer interactions. However, the Black-Box nature of DL models and their over-reliance on massive amounts of data condensed into labels and dense representations poses challenges for interpretability and explainability of the system. Furthermore, DLs have not yet been proven in their ability to effectively utilize relevant domain knowledge and experience critical to human understanding. This aspect is missing in early data-focused approaches and necessitated knowledge-infused learning and other strategies to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL methods using knowledge-infused learning, which is one of the strategies. We then discuss how this makes a fundamental difference in the interpretability and explainability of current approaches, and illustrate it with examples from natural language processing for healthcare and education applications.

</details>

<details>

<summary>2020-12-12 02:50:57 - Weakly-supervised Object Localization for Few-shot Learning and Fine-grained Few-shot Learning</summary>

- *Xiaojian He, Jinfu Lin, Junming Shen*

- `2003.00874v3` - [abs](http://arxiv.org/abs/2003.00874v3) - [pdf](http://arxiv.org/pdf/2003.00874v3)

> Few-shot learning (FSL) aims to learn novel visual categories from very few samples, which is a challenging problem in real-world applications. Many methods of few-shot classification work well on general images to learn global representation. However, they can not deal with fine-grained categories well at the same time due to a lack of subtle and local information. We argue that localization is an efficient approach because it directly provides the discriminative regions, which is critical for both general classification and fine-grained classification in a low data regime. In this paper, we propose a Self-Attention Based Complementary Module (SAC Module) to fulfill the weakly-supervised object localization, and more importantly produce the activated masks for selecting discriminative deep descriptors for few-shot classification. Based on each selected deep descriptor, Semantic Alignment Module (SAM) calculates the semantic alignment distance between the query and support images to boost classification performance. Extensive experiments show our method outperforms the state-of-the-art methods on benchmark datasets under various settings, especially on the fine-grained few-shot tasks. Besides, our method achieves superior performance over previous methods when training the model on miniImageNet and evaluating it on the different datasets, demonstrating its superior generalization capacity. Extra visualization shows the proposed method can localize the key objects more interval.

</details>

<details>

<summary>2020-12-12 08:21:08 - SenSeNet: Neural Keyphrase Generation with Document Structure</summary>

- *Yichao Luo, Zhengyan Li, Bingning Wang, Xiaoyu Xing, Qi Zhang, Xuanjing Huang*

- `2012.06754v1` - [abs](http://arxiv.org/abs/2012.06754v1) - [pdf](http://arxiv.org/pdf/2012.06754v1)

> Keyphrase Generation (KG) is the task of generating central topics from a given document or literary work, which captures the crucial information necessary to understand the content. Documents such as scientific literature contain rich meta-sentence information, which represents the logical-semantic structure of the documents. However, previous approaches ignore the constraints of document logical structure, and hence they mistakenly generate keyphrases from unimportant sentences. To address this problem, we propose a new method called Sentence Selective Network (SenSeNet) to incorporate the meta-sentence inductive bias into KG. In SenSeNet, we use a straight-through estimator for end-to-end training and incorporate weak supervision in the training of the sentence selection module. Experimental results show that SenSeNet can consistently improve the performance of major KG models based on seq2seq framework, which demonstrate the effectiveness of capturing structural information and distinguishing the significance of sentences in KG task.

</details>

<details>

<summary>2020-12-13 08:22:33 - KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning</summary>

- *Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, Lejian Liao*

- `2012.07000v1` - [abs](http://arxiv.org/abs/2012.07000v1) - [pdf](http://arxiv.org/pdf/2012.07000v1)

> Reasoning is a critical ability towards complete visual understanding. To develop machine with cognition-level visual understanding and reasoning abilities, the visual commonsense reasoning (VCR) task has been introduced. In VCR, given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. The methods adopting the powerful BERT model as the backbone for learning joint representation of image content and natural language have shown promising improvements on VCR. However, none of the existing methods have utilized commonsense knowledge in visual commonsense reasoning, which we believe will be greatly helpful in this task. With the support of commonsense knowledge, complex questions even if the required information is not depicted in the image can be answered with cognitive reasoning. Therefore, we incorporate commonsense knowledge into the cross-modal BERT, and propose a novel Knowledge Enhanced Visual-and-Linguistic BERT (KVL-BERT for short) model. Besides taking visual and linguistic contents as input, external commonsense knowledge extracted from ConceptNet is integrated into the multi-layer Transformer. In order to reserve the structural information and semantic representation of the original sentence, we propose using relative position embedding and mask-self-attention to weaken the effect between the injected commonsense knowledge and other unrelated components in the input sequence. Compared to other task-specific models and general task-agnostic pre-training models, our KVL-BERT outperforms them by a large margin.

</details>

<details>

<summary>2020-12-13 08:35:37 - C2C-GenDA: Cluster-to-Cluster Generation for Data Augmentation of Slot Filling</summary>

- *Yutai Hou, Sanyuan Chen, Wanxiang Che, Cheng Chen, Ting Liu*

- `2012.07004v1` - [abs](http://arxiv.org/abs/2012.07004v1) - [pdf](http://arxiv.org/pdf/2012.07004v1)

> Slot filling, a fundamental module of spoken language understanding, often suffers from insufficient quantity and diversity of training data. To remedy this, we propose a novel Cluster-to-Cluster generation framework for Data Augmentation (DA), named C2C-GenDA. It enlarges the training set by reconstructing existing utterances into alternative expressions while keeping semantic. Different from previous DA works that reconstruct utterances one by one independently, C2C-GenDA jointly encodes multiple existing utterances of the same semantics and simultaneously decodes multiple unseen expressions. Jointly generating multiple new utterances allows to consider the relations between generated instances and encourages diversity. Besides, encoding multiple existing utterances endows C2C with a wider view of existing expressions, helping to reduce generation that duplicates existing data. Experiments on ATIS and Snips datasets show that instances augmented by C2C-GenDA improve slot filling by 7.99 (11.9%) and 5.76 (13.6%) F-scores respectively, when there are only hundreds of training utterances.

</details>

<details>

<summary>2020-12-13 09:46:24 - Iterative Utterance Segmentation for Neural Semantic Parsing</summary>

- *Yinuo Guo, Zeqi Lin, Jian-Guang Lou, Dongmei Zhang*

- `2012.07019v1` - [abs](http://arxiv.org/abs/2012.07019v1) - [pdf](http://arxiv.org/pdf/2012.07019v1)

> Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions, and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo 63.1 to 81.2, Formulas 59.7 to 72.7, ComplexWebQuestions 27.1 to 56.3.

</details>

<details>

<summary>2020-12-13 11:07:30 - Threat Detection and Investigation with System-level Provenance Graphs: A Survey</summary>

- *Zhenyuan Li, Qi Alfred Chen, Runqing Yang, Yan Chen*

- `2006.01722v3` - [abs](http://arxiv.org/abs/2006.01722v3) - [pdf](http://arxiv.org/pdf/2006.01722v3)

> With the development of information technology, the border of the cyberspace gets much broader, exposing more and more vulnerabilities to attackers. Traditional mitigation-based defence strategies are challenging to cope with the current complicated situation. Security practitioners urgently need better tools to describe and modelling attacks for defence.   The provenance graph seems like an ideal method for threat modelling with powerful semantic expression ability and attacks historic correlation ability. In this paper, we firstly introduce the basic concepts about system-level provenance graph and proposed typical system architecture for provenance graph-based threat detection and investigation. A comprehensive provenance graph-based threat detection system can be divided into three modules, namely, "data collection module", "data management module", and "threat detection modules". Each module contains several components and involves many research problem. We systematically analyzed the algorithms and design details involved. By comparison, we give the strategy of technology selection. Moreover, we pointed out the shortcomings of the existing work for future improvement.

</details>

<details>

<summary>2020-12-14 08:00:26 - Adversarial Training against Location-Optimized Adversarial Patches</summary>

- *Sukrut Rao, David Stutz, Bernt Schiele*

- `2005.02313v2` - [abs](http://arxiv.org/abs/2005.02313v2) - [pdf](http://arxiv.org/pdf/2005.02313v2)

> Deep neural networks have been shown to be susceptible to adversarial examples -- small, imperceptible changes constructed to cause mis-classification in otherwise highly accurate image classifiers. As a practical alternative, recent work proposed so-called adversarial patches: clearly visible, but adversarially crafted rectangular patches in images. These patches can easily be printed and applied in the physical world. While defenses against imperceptible adversarial examples have been studied extensively, robustness against adversarial patches is poorly understood. In this work, we first devise a practical approach to obtain adversarial patches while actively optimizing their location within the image. Then, we apply adversarial training on these location-optimized adversarial patches and demonstrate significantly improved robustness on CIFAR10 and GTSRB. Additionally, in contrast to adversarial training on imperceptible adversarial examples, our adversarial patch training does not reduce accuracy.

</details>

<details>

<summary>2020-12-14 09:15:15 - HR-Depth: High Resolution Self-Supervised Monocular Depth Estimation</summary>

- *Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, Lina Liu, Yong Liu, Xinxin Chen, Yi Yuan*

- `2012.07356v1` - [abs](http://arxiv.org/abs/2012.07356v1) - [pdf](http://arxiv.org/pdf/2012.07356v1)

> Self-supervised learning shows great potential in monoculardepth estimation, using image sequences as the only source ofsupervision. Although people try to use the high-resolutionimage for depth estimation, the accuracy of prediction hasnot been significantly improved. In this work, we find thecore reason comes from the inaccurate depth estimation inlarge gradient regions, making the bilinear interpolation er-ror gradually disappear as the resolution increases. To obtainmore accurate depth estimation in large gradient regions, itis necessary to obtain high-resolution features with spatialand semantic information. Therefore, we present an improvedDepthNet, HR-Depth, with two effective strategies: (1) re-design the skip-connection in DepthNet to get better high-resolution features and (2) propose feature fusion Squeeze-and-Excitation(fSE) module to fuse feature more efficiently.Using Resnet-18 as the encoder, HR-Depth surpasses all pre-vious state-of-the-art(SoTA) methods with the least param-eters at both high and low resolution. Moreover, previousstate-of-the-art methods are based on fairly complex and deepnetworks with a mass of parameters which limits their realapplications. Thus we also construct a lightweight networkwhich uses MobileNetV3 as encoder. Experiments show thatthe lightweight network can perform on par with many largemodels like Monodepth2 at high-resolution with only20%parameters. All codes and models will be available at https://github.com/shawLyu/HR-Depth.

</details>

<details>

<summary>2020-12-14 09:41:33 - Swapping Autoencoder for Deep Image Manipulation</summary>

- *Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, Richard Zhang*

- `2007.00653v2` - [abs](http://arxiv.org/abs/2007.00653v2) - [pdf](http://arxiv.org/pdf/2007.00653v2)

> Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping Autoencoder, a deep model designed specifically for image manipulation, rather than random sampling. The key idea is to encode an image with two independent components and enforce that any swapped combination maps to a realistic image. In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of an image. As our method is trained with an encoder, finding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, it can be used to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efficient compared to recent generative models.

</details>

<details>

<summary>2020-12-14 10:30:51 - Towards localisation of keywords in speech using weak supervision</summary>

- *Kayode Olaleye, Benjamin van Niekerk, Herman Kamper*

- `2012.07396v1` - [abs](http://arxiv.org/abs/2012.07396v1) - [pdf](http://arxiv.org/pdf/2012.07396v1)

> Developments in weakly supervised and self-supervised models could enable speech technology in low-resource settings where full transcriptions are not available. We consider whether keyword localisation is possible using two forms of weak supervision where location information is not provided explicitly. In the first, only the presence or absence of a word is indicated, i.e. a bag-of-words (BoW) labelling. In the second, visual context is provided in the form of an image paired with an unlabelled utterance; a model then needs to be trained in a self-supervised fashion using the paired data. For keyword localisation, we adapt a saliency-based method typically used in the vision domain. We compare this to an existing technique that performs localisation as a part of the network architecture. While the saliency-based method is more flexible (it can be applied without architectural restrictions), we identify a critical limitation when using it for keyword localisation. Of the two forms of supervision, the visually trained model performs worse than the BoW-trained model. We show qualitatively that the visually trained model sometimes locate semantically related words, but this is not consistent. While our results show that there is some signal allowing for localisation, it also calls for other localisation methods better matched to these forms of weak supervision.

</details>

<details>

<summary>2020-12-14 11:37:39 - Cross Layer Attacks and How to Use Them (for DNS Cache Poisoning, Device Tracking and More)</summary>

- *Amit Klein*

- `2012.07432v1` - [abs](http://arxiv.org/abs/2012.07432v1) - [pdf](http://arxiv.org/pdf/2012.07432v1)

> We analyze the prandom pseudo random number generator (PRNG) in use in the Linux kernel (which is the kernel of the Linux operating system, as well as of Android) and demonstrate that this PRNG is weak. The prandom PRNG is in use by many "consumers" in the Linux kernel. We focused on three consumers at the network level -- the UDP source port generation algorithm, the IPv6 flow label generation algorithm and the IPv4 ID generation algorithm. The flawed prandom PRNG is shared by all these consumers, which enables us to mount "cross layer attacks" against the Linux kernel. In these attacks, we infer the internal state of the prandom PRNG from one OSI layer, and use it to either predict the values of the PRNG employed by the other OSI layer, or to correlate it to an internal state of the PRNG inferred from the other protocol.   Using this approach we can mount a very efficient DNS cache poisoning attack against Linux. We collect TCP/IPv6 flow label values, or UDP source ports, or TCP/IPv4 IP ID values, reconstruct the internal PRNG state, then predict an outbound DNS query UDP source port, which speeds up the attack by a factor of x3000 to x6000. This attack works remotely, but can also be mounted locally, across Linux users and across containers, and (depending on the stub resolver) can poison the cache with an arbitrary DNS record. Additionally, we can identify and track Linux and Android devices -- we collect TCP/IPv6 flow label values and/or UDP source port values and/or TCP/IPv4 ID fields, reconstruct the PRNG internal state and correlate this new state to previously extracted PRNG states to identify the same device.

</details>

<details>

<summary>2020-12-14 12:50:52 - Reasoning about disclosure in data integration in the presence of source constraints</summary>

- *Michael Benedikt, Pierre Bourhis, Louis Jachiet, Michaël Thomazo*

- `1906.00624v2` - [abs](http://arxiv.org/abs/1906.00624v2) - [pdf](http://arxiv.org/pdf/1906.00624v2)

> Data integration systems allow users to access data sitting in multiple sources by means of queries over a global schema, related to the sources via mappings. Data sources often contain sensitive information, and thus an analysis is needed to verify that a schema satisfies a privacy policy, given as a set of queries whose answers should not be accessible to users. Such an analysis should take into account not only knowledge that an attacker may have about the mappings, but also what they may know about the semantics of the sources. In this paper, we show that source constraints can have a dramatic impact on disclosure analysis. We study the problem of determining whether a given data integration system discloses a source query to an attacker in the presence of constraints, providing both lower and upper bounds on source-aware disclosure analysis.

</details>

<details>

<summary>2020-12-14 14:16:45 - Variational Auto-encoder Based Bayesian Poisson Tensor Factorization for Sparse and Imbalanced Count Data</summary>

- *Yuan Jin, Ming Liu, Yunfeng Li, Ruohua Xu, Lan Du, Longxiang Gao, Yong Xiang*

- `1910.05570v2` - [abs](http://arxiv.org/abs/1910.05570v2) - [pdf](http://arxiv.org/pdf/1910.05570v2)

> Non-negative tensor factorization models enable predictive analysis on count data. Among them, Bayesian Poisson-Gamma models can derive full posterior distributions of latent factors and are less sensitive to sparse count data. However, current inference methods for these Bayesian models adopt restricted update rules for the posterior parameters. They also fail to share the update information to better cope with the data sparsity. Moreover, these models are not endowed with a component that handles the imbalance in count data values. In this paper, we propose a novel variational auto-encoder framework called VAE-BPTF which addresses the above issues. It uses multi-layer perceptron networks to encode and share complex update information. The encoded information is then reweighted per data instance to penalize common data values before aggregated to compute the posterior parameters for the latent factors. Under synthetic data evaluation, VAE-BPTF tended to recover the right number of latent factors and posterior parameter values. It also outperformed current models in both reconstruction errors and latent factor (semantic) coherence across five real-world datasets. Furthermore, the latent factors inferred by VAE-BPTF are perceived to be meaningful and coherent under a qualitative analysis.

</details>

<details>

<summary>2020-12-14 15:12:16 - TreeCaps: Tree-Based Capsule Networks for Source Code Processing</summary>

- *Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang*

- `2009.09777v4` - [abs](http://arxiv.org/abs/2009.09777v4) - [pdf](http://arxiv.org/pdf/2009.09777v4)

> Recently program learning techniques have been proposed to process source code based on syntactical structures (e.g., Abstract Syntax Trees) and/or semantic information (e.g., Dependency Graphs). Although graphs may be better at capturing various viewpoints of code semantics than trees, constructing graph inputs from code needs static code semantic analysis that may not be accurate and introduces noise during learning. Although syntax trees are precisely defined according to the language grammar and easier to construct and process than graphs, previous tree-based learning techniques have not been able to learn semantic information from trees to achieve better accuracy than graph-based techniques. We propose a new learning technique, named TreeCaps, by fusing together capsule networks with tree-based convolutional neural networks, to achieve learning accuracy higher than existing graph-based techniques while it is based only on trees. TreeCaps introduces novel variable-to-static routing algorithms into the capsule networks to compensate for the loss of previous routing algorithms. Aside from accuracy, we also find that TreeCaps is the most robust to withstand those semantic-preserving program transformations that change code syntax without modifying the semantics. Evaluated on a large number of Java and C/C++ programs, TreeCaps models outperform prior deep learning models of program source code, in terms of both accuracy and robustness for program comprehension tasks such as code functionality classification and function name prediction

</details>

<details>

<summary>2020-12-14 16:56:29 - Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers</summary>

- *Raphaël Barman, Maud Ehrmann, Simon Clematide, Sofia Ares Oliveira, Frédéric Kaplan*

- `2002.06144v4` - [abs](http://arxiv.org/abs/2002.06144v4) - [pdf](http://arxiv.org/pdf/2002.06144v4)

> The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. If the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of finer-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. In this context, we introduce a multimodal approach for the semantic segmentation of historical newspapers that combines visual and textual features. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate, among others, the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to high material variance.

</details>

<details>

<summary>2020-12-14 21:40:59 - Fixing Multiple Type Errors in Model Transformations with Alternative Oracles to Test Cases</summary>

- *Zahra VaraminyBahnemiry, Jessie Galasso, Houari Sahraoui*

- `2012.07953v1` - [abs](http://arxiv.org/abs/2012.07953v1) - [pdf](http://arxiv.org/pdf/2012.07953v1)

> This paper addresses the issue of correcting type errors in model transformations in realistic scenarios where neither predefined patches nor behavior-safe guards such as test suites are available. Instead of using predefined patches targeting isolated errors of specific categories, we propose to explore the space of possible patches by combining basic edit operations for model transformation programs. To guide the search, we define two families of objectives: one to limit the number of type errors and the other to preserve the transformation behavior. To approximate the latter, we study two objectives: minimizing the number of changes and keeping the changes local. Additionally, we define four heuristics to refine candidate patches to increase the likelihood of correcting type errors while preserving the transformation behavior. We implemented our approach for the ATL language using the evolutionary algorithm NSGA-II, and performed an evaluation based on three published case studies. The evaluation results show that our approach was able to automatically correct on average more than82% of type errors for two cases and more than 56% for the third case.

</details>

<details>

<summary>2020-12-14 22:27:10 - Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering</summary>

- *Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric Nyberg, Alessandro Oltramari*

- `2011.03863v2` - [abs](http://arxiv.org/abs/2011.03863v2) - [pdf](http://arxiv.org/pdf/2011.03863v2)

> Recent developments in pre-trained neural language modeling have led to leaps in accuracy on commonsense question-answering benchmarks. However, there is increasing concern that models overfit to specific tasks, without learning to utilize external knowledge or perform general semantic reasoning. In contrast, zero-shot evaluations have shown promise as a more robust measure of a model's general reasoning abilities. In this paper, we propose a novel neuro-symbolic framework for zero-shot question answering across commonsense tasks. Guided by a set of hypotheses, the framework studies how to transform various pre-existing knowledge resources into a form that is most effective for pre-training models. We vary the set of language models, training regimes, knowledge sources, and data generation strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies. We provide empirical results across five commonsense question-answering tasks with data generated from five external knowledge resources. We show that, while an individual knowledge graph is better suited for specific tasks, a global knowledge graph brings consistent gains across different tasks. In addition, both preserving the structure of the task as well as generating fair and informative questions help language models learn more effectively.

</details>

<details>

<summary>2020-12-14 23:09:10 - Discovering Airline-Specific Business Intelligence from Online Passenger Reviews: An Unsupervised Text Analytics Approach</summary>

- *Sharan Srinivas, Surya Ramachandiran*

- `2012.08000v1` - [abs](http://arxiv.org/abs/2012.08000v1) - [pdf](http://arxiv.org/pdf/2012.08000v1)

> To understand the important dimensions of service quality from the passenger's perspective and tailor service offerings for competitive advantage, airlines can capitalize on the abundantly available online customer reviews (OCR). The objective of this paper is to discover company- and competitor-specific intelligence from OCR using an unsupervised text analytics approach. First, the key aspects (or topics) discussed in the OCR are extracted using three topic models - probabilistic latent semantic analysis (pLSA) and two variants of Latent Dirichlet allocation (LDA-VI and LDA-GS). Subsequently, we propose an ensemble-assisted topic model (EA-TM), which integrates the individual topic models, to classify each review sentence to the most representative aspect. Likewise, to determine the sentiment corresponding to a review sentence, an ensemble sentiment analyzer (E-SA), which combines the predictions of three opinion mining methods (AFINN, SentiStrength, and VADER), is developed. An aspect-based opinion summary (AOS), which provides a snapshot of passenger-perceived strengths and weaknesses of an airline, is established by consolidating the sentiments associated with each aspect. Furthermore, a bi-gram analysis of the labeled OCR is employed to perform root cause analysis within each identified aspect. A case study involving 99,147 airline reviews of a US-based target carrier and four of its competitors is used to validate the proposed approach. The results indicate that a cost- and time-effective performance summary of an airline and its competitors can be obtained from OCR. Finally, besides providing theoretical and managerial implications based on our results, we also provide implications for post-pandemic preparedness in the airline industry considering the unprecedented impact of coronavirus disease 2019 (COVID-19) and predictions on similar pandemics in the future.

</details>

<details>

<summary>2020-12-14 23:40:39 - Generalizing Emergent Communication</summary>

- *Thomas A. Unger, Elia Bruni*

- `2001.01772v3` - [abs](http://arxiv.org/abs/2001.01772v3) - [pdf](http://arxiv.org/pdf/2001.01772v3)

> We converted the recently developed BabyAI grid world platform to a sender/receiver setup in order to test the hypothesis that established deep reinforcement learning techniques are sufficient to incentivize the emergence of a grounded discrete communication protocol between generalized agents. This is in contrast to previous experiments that employed straight-through estimation or specialized inductive biases. Our results show that these can indeed be avoided, by instead providing proper environmental incentives. Moreover, they show that a longer interval between communications incentivized more abstract semantics. In some cases, the communicating agents adapted to new environments more quickly than a monolithic agent, showcasing the potential of emergent communication for transfer learning and generalization in general.

</details>

<details>

<summary>2020-12-15 11:19:38 - RGPNet: A Real-Time General Purpose Semantic Segmentation</summary>

- *Elahe Arani, Shabbir Marzban, Andrei Pata, Bahram Zonooz*

- `1912.01394v2` - [abs](http://arxiv.org/abs/1912.01394v2) - [pdf](http://arxiv.org/pdf/1912.01394v2)

> We propose a real-time general purpose semantic segmentation architecture, RGPNet, which achieves significant performance gain in complex environments. RGPNet consists of a light-weight asymmetric encoder-decoder and an adaptor. The adaptor helps preserve and refine the abstract concepts from multiple levels of distributed representations between the encoder and decoder. It also facilitates the gradient flow from deeper layers to shallower layers. Our experiments demonstrate that RGPNet can generate segmentation results in real-time with comparable accuracy to the state-of-the-art non-real-time heavy models. Moreover, towards green AI, we show that using an optimized label-relaxation technique with progressive resizing can reduce the training time by up to 60% while preserving the performance. We conclude that RGPNet obtains a better speed-accuracy trade-off across multiple datasets.

</details>

<details>

<summary>2020-12-15 13:01:26 - *-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task</summary>

- *Dmitry Tsarkov, Tibor Tihon, Nathan Scales, Nikola Momchev, Danila Sinopalnikov, Nathanael Schärli*

- `2012.08266v1` - [abs](http://arxiv.org/abs/2012.08266v1) - [pdf](http://arxiv.org/pdf/2012.08266v1)

> We present *-CFQ ("star-CFQ"): a suite of large-scale datasets of varying scope based on the CFQ semantic parsing benchmark, designed for principled investigation of the scalability of machine learning systems in a realistic compositional task setting. Using this suite, we conduct a series of experiments investigating the ability of Transformers to benefit from increased training size under conditions of fixed computational cost. We show that compositional generalization remains a challenge at all training sizes, and we show that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data. We further show that while additional training data from a related domain improves the accuracy in data-starved situations, this improvement is limited and diminishes as the distance from the related domain to the target domain increases.

</details>

<details>

<summary>2020-12-15 16:25:23 - Artificial Neural Networks for Sensor Data Classification on Small Embedded Systems</summary>

- *Marcus Venzke, Daniel Klisch, Philipp Kubik, Asad Ali, Jesper Dell Missier, Volker Turau*

- `2012.08403v1` - [abs](http://arxiv.org/abs/2012.08403v1) - [pdf](http://arxiv.org/pdf/2012.08403v1)

> In this paper we investigate the usage of machine learning for interpreting measured sensor values in sensor modules. In particular we analyze the potential of artificial neural networks (ANNs) on low-cost micro-controllers with a few kilobytes of memory to semantically enrich data captured by sensors. The focus is on classifying temporal data series with a high level of reliability. Design and implementation of ANNs are analyzed considering Feed Forward Neural Networks (FFNNs) and Recurrent Neural Networks (RNNs). We validate the developed ANNs in a case study of optical hand gesture recognition on an 8-bit micro-controller. The best reliability was found for an FFNN with two layers and 1493 parameters requiring an execution time of 36 ms. We propose a workflow to develop ANNs for embedded devices.

</details>

<details>

<summary>2020-12-15 16:53:28 - Knowledge Graphs and Natural-Language Processing</summary>

- *Andreas L Opdahl*

- `2101.06111v1` - [abs](http://arxiv.org/abs/2101.06111v1) - [pdf](http://arxiv.org/pdf/2101.06111v1)

> Emergency-relevant data comes in many varieties. It can be high volume and high velocity, and reaction times are critical, calling for efficient and powerful techniques for data analysis and management. Knowledge graphs represent data in a rich, flexible, and uniform way that is well matched with the needs of emergency management. They build on existing standards, resources, techniques, and tools for semantic data and computing. This chapter explains the most important semantic technologies and how they support knowledge graphs. We proceed to discuss their benefits and challenges and give examples of relevant semantic data sources and vocabularies. Natural-language texts -- in particular those collected from social media such as Twitter -- is a type of data source that poses particular analysis challenges. We therefore include an overview of techniques for processing natural-language texts.

</details>

<details>

<summary>2020-12-15 17:23:15 - Remote Sensing Image Scene Classification with Deep Neural Networks in JPEG 2000 Compressed Domain</summary>

- *Akshara Preethy Byju, Gencer Sumbul, Begüm Demir, Lorenzo Bruzzone*

- `2006.11529v2` - [abs](http://arxiv.org/abs/2006.11529v2) - [pdf](http://arxiv.org/pdf/2006.11529v2)

> To reduce the storage requirements, remote sensing (RS) images are usually stored in compressed format. Existing scene classification approaches using deep neural networks (DNNs) require to fully decompress the images, which is a computationally demanding task in operational applications. To address this issue, in this paper we propose a novel approach to achieve scene classification in JPEG 2000 compressed RS images. The proposed approach consists of two main steps: i) approximation of the finer resolution sub-bands of reversible biorthogonal wavelet filters used in JPEG 2000; and ii) characterization of the high-level semantic content of approximated wavelet sub-bands and scene classification based on the learnt descriptors. This is achieved by taking codestreams associated with the coarsest resolution wavelet sub-band as input to approximate finer resolution sub-bands using a number of transposed convolutional layers. Then, a series of convolutional layers models the high-level semantic content of the approximated wavelet sub-band. Thus, the proposed approach models the multiresolution paradigm given in the JPEG 2000 compression algorithm in an end-to-end trainable unified neural network. In the classification stage, the proposed approach takes only the coarsest resolution wavelet sub-bands as input, thereby reducing the time required to apply decoding. Experimental results performed on two benchmark aerial image archives demonstrate that the proposed approach significantly reduces the computational time with similar classification accuracies when compared to traditional RS scene classification approaches (which requires full image decompression).

</details>

<details>

<summary>2020-12-15 17:55:53 - Rule Extraction from Binary Neural Networks with Convolutional Rules for Model Validation</summary>

- *Sophie Burkhardt, Jannis Brugger, Nicolas Wagner, Zahra Ahmadi, Kristian Kersting, Stefan Kramer*

- `2012.08459v1` - [abs](http://arxiv.org/abs/2012.08459v1) - [pdf](http://arxiv.org/pdf/2012.08459v1)

> Most deep neural networks are considered to be black boxes, meaning their output is hard to interpret. In contrast, logical expressions are considered to be more comprehensible since they use symbols that are semantically close to natural language instead of distributed representations. However, for high-dimensional input data such as images, the individual symbols, i.e. pixels, are not easily interpretable. We introduce the concept of first-order convolutional rules, which are logical rules that can be extracted using a convolutional neural network (CNN), and whose complexity depends on the size of the convolutional filter and not on the dimensionality of the input. Our approach is based on rule extraction from binary neural networks with stochastic local search. We show how to extract rules that are not necessarily short, but characteristic of the input, and easy to visualize. Our experiments show that the proposed approach is able to model the functionality of the neural network while at the same time producing interpretable logical rules.

</details>

<details>

<summary>2020-12-15 20:08:19 - Semantic Annotation for Tabular Data</summary>

- *Udayan Khurana, Sainyam Galhotra*

- `2012.08594v1` - [abs](http://arxiv.org/abs/2012.08594v1) - [pdf](http://arxiv.org/pdf/2012.08594v1)

> Detecting semantic concept of columns in tabular data is of particular interest to many applications ranging from data integration, cleaning, search to feature engineering and model building in machine learning. Recently, several works have proposed supervised learning-based or heuristic pattern-based approaches to semantic type annotation. Both have shortcomings that prevent them from generalizing over a large number of concepts or examples. Many neural network based methods also present scalability issues. Additionally, none of the known methods works well for numerical data. We propose $C^2$, a column to concept mapper that is based on a maximum likelihood estimation approach through ensembles. It is able to effectively utilize vast amounts of, albeit somewhat noisy, openly available table corpora in addition to two popular knowledge graphs to perform effective and efficient concept prediction for structured data. We demonstrate the effectiveness of $C^2$ over available techniques on 9 datasets, the most comprehensive comparison on this topic so far.

</details>

<details>

<summary>2020-12-16 03:01:25 - Adversarial Training with Fast Gradient Projection Method against Synonym Substitution based Text Attacks</summary>

- *Xiaosen Wang, Yichen Yang, Yihe Deng, Kun He*

- `2008.03709v4` - [abs](http://arxiv.org/abs/2008.03709v4) - [pdf](http://arxiv.org/pdf/2008.03709v4)

> Adversarial training is the most empirically successful approach in improving the robustness of deep neural networks for image classification.For text classification, however, existing synonym substitution based adversarial attacks are effective but not efficient to be incorporated into practical text adversarial training. Gradient-based attacks, which are very efficient for images, are hard to be implemented for synonym substitution based text attacks due to the lexical, grammatical and semantic constraints and the discrete text input space. Thereby, we propose a fast text adversarial attack method called Fast Gradient Projection Method (FGPM) based on synonym substitution, which is about 20 times faster than existing text attack methods and could achieve similar attack performance. We then incorporate FGPM with adversarial training and propose a text defense method called Adversarial Training with FGPM enhanced by Logit pairing (ATFL). Experiments show that ATFL could significantly improve the model robustness and block the transferability of adversarial examples.

</details>

<details>

<summary>2020-12-16 10:17:53 - A Hybrid Graph Neural Network Approach for Detecting PHP Vulnerabilities</summary>

- *Rishi Rabheru, Hazim Hanif, Sergio Maffeis*

- `2012.08835v1` - [abs](http://arxiv.org/abs/2012.08835v1) - [pdf](http://arxiv.org/pdf/2012.08835v1)

> This paper presents DeepTective, a deep learning approach to detect vulnerabilities in PHP source code. Our approach implements a novel hybrid technique that combines Gated Recurrent Units and Graph Convolutional Networks to detect SQLi, XSS and OSCI vulnerabilities leveraging both syntactic and semantic information. We evaluate DeepTective and compare it to the state of the art on an established synthetic dataset and on a novel real-world dataset collected from GitHub. Experimental results show that DeepTective achieves near perfect classification on the synthetic dataset, and an F1 score of 88.12% on the realistic dataset, outperforming related approaches. We validate DeepTective in the wild by discovering 4 novel vulnerabilities in established WordPress plugins.

</details>

<details>

<summary>2020-12-16 13:11:30 - R$^2$-Net: Relation of Relation Learning Network for Sentence Semantic Matching</summary>

- *Kun Zhang, Le Wu, Guangyi Lv, Meng Wang, Enhong Chen, Shulan Ruan*

- `2012.08920v1` - [abs](http://arxiv.org/abs/2012.08920v1) - [pdf](http://arxiv.org/pdf/2012.08920v1)

> Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences. Recently, deep neural networks have achieved impressive performance in this area, especially BERT. Despite the effectiveness of these models, most of them treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels. To address this problem, we propose a Relation of Relation Learning Network (R2-Net) for sentence semantic matching. Specifically, we first employ BERT to encode the input sentences from a global perspective. Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective. To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding R2-Net to consider more about labels. Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model. As a byproduct, we have released the codes to facilitate other researches.

</details>

<details>

<summary>2020-12-16 13:40:20 - Summarizing Unstructured Logs in Online Services</summary>

- *Weibin Meng, Federico Zaiter, Yuheng Huang, Ying Liu, Shenglin Zhang, Yuzhe Zhang, Yichen Zhu, Tianke Zhang, En Wang, Zuomin Ren, Feng Wang, Shimin Tao, Dan Pei*

- `2012.08938v1` - [abs](http://arxiv.org/abs/2012.08938v1) - [pdf](http://arxiv.org/pdf/2012.08938v1)

> Logs are one of the most valuable data sources for managing large-scale online services. After a failure is detected/diagnosed/predicted, operators still have to inspect the raw logs to gain a summarized view before take actions. However, manual or rule-based log summarization has become inefficient and ineffective. In this work, we propose LogSummary, an automatic, unsupervised end-to-end log summarization framework for online services. LogSummary obtains the summarized triples of important logs for a given log sequence. It integrates a novel information extraction method taking both semantic information and domain knowledge into consideration, with a new triple ranking approach using the global knowledge learned from all logs. Given the lack of a publicly-available gold standard for log summarization, we have manually labelled the summaries of four open-source log datasets and made them publicly available. The evaluation on these datasets as well as the case studies on real-world logs demonstrate that LogSummary produces a highly representative (average ROUGE F1 score of 0.741) summaries. We have packaged LogSummary into an open-source toolkit and hope that it can benefit for future NLP-powered summarization works.

</details>

<details>

<summary>2020-12-16 17:15:51 - AMR Quality Rating with a Lightweight CNN</summary>

- *Juri Opitz*

- `2005.12187v2` - [abs](http://arxiv.org/abs/2005.12187v2) - [pdf](http://arxiv.org/pdf/2005.12187v2)

> Structured semantic sentence representations such as Abstract Meaning Representations (AMRs) are potentially useful in various NLP tasks. However, the quality of automatic parses can vary greatly and jeopardizes their usefulness. This can be mitigated by models that can accurately rate AMR quality in the absence of costly gold data, allowing us to inform downstream systems about an incorporated parse's trustworthiness or select among different candidate parses.   In this work, we propose to transfer the AMR graph to the domain of images. This allows us to create a simple convolutional neural network (CNN) that imitates a human judge tasked with rating graph quality. Our experiments show that the method can rate quality more accurately than strong baselines, in several quality dimensions. Moreover, the method proves to be efficient and reduces the incurred energy consumption.

</details>

<details>

<summary>2020-12-16 20:14:41 - S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds</summary>

- *Ran Cheng, Christopher Agia, Yuan Ren, Xinhai Li, Liu Bingbing*

- `2012.09242v1` - [abs](http://arxiv.org/abs/2012.09242v1) - [pdf](http://arxiv.org/pdf/2012.09242v1)

> With the increasing reliance of self-driving and similar robotic systems on robust 3D vision, the processing of LiDAR scans with deep convolutional neural networks has become a trend in academia and industry alike. Prior attempts on the challenging Semantic Scene Completion task - which entails the inference of dense 3D structure and associated semantic labels from "sparse" representations - have been, to a degree, successful in small indoor scenes when provided with dense point clouds or dense depth maps often fused with semantic segmentation maps from RGB images. However, the performance of these systems drop drastically when applied to large outdoor scenes characterized by dynamic and exponentially sparser conditions. Likewise, processing of the entire sparse volume becomes infeasible due to memory limitations and workarounds introduce computational inefficiency as practitioners are forced to divide the overall volume into multiple equal segments and infer on each individually, rendering real-time performance impossible. In this work, we formulate a method that subsumes the sparsity of large-scale environments and present S3CNet, a sparse convolution based neural network that predicts the semantically completed scene from a single, unified LiDAR point cloud. We show that our proposed method outperforms all counterparts on the 3D task, achieving state-of-the art results on the SemanticKITTI benchmark. Furthermore, we propose a 2D variant of S3CNet with a multi-view fusion strategy to complement our 3D network, providing robustness to occlusions and extreme sparsity in distant regions. We conduct experiments for the 2D semantic scene completion task and compare the results of our sparse 2D network against several leading LiDAR segmentation models adapted for bird's eye view segmentation on two open-source datasets.

</details>

<details>

<summary>2020-12-16 23:09:53 - Generate and Verify: Semantically Meaningful Formal Analysis of Neural Network Perception Systems</summary>

- *Chris R. Serrano, Pape M. Sylla, Michael A. Warren*

- `2012.09313v1` - [abs](http://arxiv.org/abs/2012.09313v1) - [pdf](http://arxiv.org/pdf/2012.09313v1)

> Testing remains the primary method to evaluate the accuracy of neural network perception systems. Prior work on the formal verification of neural network perception models has been limited to notions of local adversarial robustness for classification with respect to individual image inputs. In this work, we propose a notion of global correctness for neural network perception models performing regression with respect to a generative neural network with a semantically meaningful latent space. That is, against an infinite set of images produced by a generative model over an interval of its latent space, we employ neural network verification to prove that the model will always produce estimates within some error bound of the ground truth. Where the perception model fails, we obtain semantically meaningful counter-examples which carry information on concrete states of the system of interest that can be used programmatically without human inspection of corresponding generated images. Our approach, Generate and Verify, provides a new technique to gather insight into the failure cases of neural network perception systems and provide meaningful guarantees of correct behavior in safety critical applications.

</details>

<details>

<summary>2020-12-17 02:18:41 - Visual Pivoting for (Unsupervised) Entity Alignment</summary>

- *Fangyu Liu, Muhao Chen, Dan Roth, Nigel Collier*

- `2009.13603v2` - [abs](http://arxiv.org/abs/2009.13603v2) - [pdf](http://arxiv.org/pdf/2009.13603v2)

> This work studies the use of visual semantic representations to align entities in heterogeneous knowledge graphs (KGs). Images are natural components of many existing KGs. By combining visual knowledge with other auxiliary information, we show that the proposed new approach, EVA, creates a holistic entity representation that provides strong signals for cross-graph entity alignment. Besides, previous entity alignment methods require human labelled seed alignment, restricting availability. EVA provides a completely unsupervised solution by leveraging the visual similarity of entities to create an initial seed dictionary (visual pivots). Experiments on benchmark data sets DBP15k and DWY15k show that EVA offers state-of-the-art performance on both monolingual and cross-lingual entity alignment tasks. Furthermore, we discover that images are particularly useful to align long-tail KG entities, which inherently lack the structural contexts necessary for capturing the correspondences.

</details>

<details>

<summary>2020-12-17 03:46:09 - DecAug: Out-of-Distribution Generalization via Decomposed Feature Representation and Semantic Augmentation</summary>

- *Haoyue Bai, Rui Sun, Lanqing Hong, Fengwei Zhou, Nanyang Ye, Han-Jia Ye, S. -H. Gary Chan, Zhenguo Li*

- `2012.09382v1` - [abs](http://arxiv.org/abs/2012.09382v1) - [pdf](http://arxiv.org/pdf/2012.09382v1)

> While deep learning demonstrates its strong ability to handle independent and identically distributed (IID) data, it often suffers from out-of-distribution (OoD) generalization, where the test data come from another distribution (w.r.t. the training one). Designing a general OoD generalization framework to a wide range of applications is challenging, mainly due to possible correlation shift and diversity shift in the real world. Most of the previous approaches can only solve one specific distribution shift, such as shift across domains or the extrapolation of correlation. To address that, we propose DecAug, a novel decomposed feature representation and semantic augmentation approach for OoD generalization. DecAug disentangles the category-related and context-related features. Category-related features contain causal information of the target object, while context-related features describe the attributes, styles, backgrounds, or scenes, causing distribution shifts between training and test data. The decomposition is achieved by orthogonalizing the two gradients (w.r.t. intermediate features) of losses for predicting category and context labels. Furthermore, we perform gradient-based augmentation on context-related features to improve the robustness of the learned representations. Experimental results show that DecAug outperforms other state-of-the-art methods on various OoD datasets, which is among the very few methods that can deal with different types of OoD generalization challenges.

</details>

<details>

<summary>2020-12-17 04:54:16 - MASKER: Masked Keyword Regularization for Reliable Text Classification</summary>

- *Seung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, Jinwoo Shin*

- `2012.09392v1` - [abs](http://arxiv.org/abs/2012.09392v1) - [pdf](http://arxiv.org/pdf/2012.09392v1)

> Pre-trained language models have achieved state-of-the-art accuracies on various text classification tasks, e.g., sentiment analysis, natural language inference, and semantic textual similarity. However, the reliability of the fine-tuned text classifiers is an often underlooked performance criterion. For instance, one may desire a model that can detect out-of-distribution (OOD) samples (drawn far from training distribution) or be robust against domain shifts. We claim that one central obstacle to the reliability is the over-reliance of the model on a limited number of keywords, instead of looking at the whole context. In particular, we find that (a) OOD samples often contain in-distribution keywords, while (b) cross-domain samples may not always contain keywords; over-relying on the keywords can be problematic for both cases. In light of this observation, we propose a simple yet effective fine-tuning method, coined masked keyword regularization (MASKER), that facilitates context-based prediction. MASKER regularizes the model to reconstruct the keywords from the rest of the words and make low-confidence predictions without enough context. When applied to various pre-trained language models (e.g., BERT, RoBERTa, and ALBERT), we demonstrate that MASKER improves OOD detection and cross-domain generalization without degrading classification accuracy. Code is available at https://github.com/alinlab/MASKER.

</details>

<details>

<summary>2020-12-17 07:51:05 - DataProVe: A Data Protection Policy and System Architecture Verification Tool</summary>

- *Vinh Thong Ta*

- `2008.08936v4` - [abs](http://arxiv.org/abs/2008.08936v4) - [pdf](http://arxiv.org/pdf/2008.08936v4)

> In this paper, we propose a tool, called DataProVe, for specifying high-level data protection policies and system architectures, as well as verifying the conformance between them in a fully automated way. The syntax of the policies and the architectures is based on semi-formal languages, and the automated verification engine relies on logic and resolution based proofs. The functionality and operation of the tool are presented using different examples.

</details>

<details>

<summary>2020-12-17 11:38:42 - Probabilistic Deep Learning for Instance Segmentation</summary>

- *Josef Lorenz Rumberger, Lisa Mais, Dagmar Kainmueller*

- `2008.10678v2` - [abs](http://arxiv.org/abs/2008.10678v2) - [pdf](http://arxiv.org/pdf/2008.10678v2)

> Probabilistic convolutional neural networks, which predict distributions of predictions instead of point estimates, led to recent advances in many areas of computer vision, from image reconstruction to semantic segmentation. Besides state of the art benchmark results, these networks made it possible to quantify local uncertainties in the predictions. These were used in active learning frameworks to target the labeling efforts of specialist annotators or to assess the quality of a prediction in a safety-critical environment. However, for instance segmentation problems these methods are not frequently used so far. We seek to close this gap by proposing a generic method to obtain model-inherent uncertainty estimates within proposal-free instance segmentation models. Furthermore, we analyze the quality of the uncertainty estimates with a metric adapted from semantic segmentation. We evaluate our method on the BBBC010 C.\ elegans dataset, where it yields competitive performance while also predicting uncertainty estimates that carry information about object-level inaccuracies like false splits and false merges. We perform a simulation to show the potential use of such uncertainty estimates in guided proofreading.

</details>

<details>

<summary>2020-12-17 13:12:57 - A framework to semantify BPMN models using DEMO business transaction pattern</summary>

- *Sérgio Guerreiro, Pedro Sousa*

- `2012.09557v1` - [abs](http://arxiv.org/abs/2012.09557v1) - [pdf](http://arxiv.org/pdf/2012.09557v1)

> BPMN is a specification language widely used by industry and researchers for business process modeling and execution. It defines clearly how to articulate its concepts, but do not provide mechanism to represent the semantics of the produced models. This paper addresses the problem of how to improve the expressiveness of BPMN models, proposing a definition for the semantics of a business process within a BPMN model, and improving the completeness of the models in a systematic manner, so that models can describe far more situations with few extra managed complexity. We conceive a framework based on the business transaction patterns available in the enterprise ontology body of knowledge to prescribe the foundations of semantic BPMN models. A tool has been developed to automate the framework. Then, two industrial proof-of-concepts are used to measure its coverage, both positive and negative, and to argue about our proposal's usefulness. After that, the proposal is compared with others using a systematic literature review. A full BPMN pattern is proposed encompassing the happy flow, the declinations, the rejections and the revocations, without adding any new element to the BPMN specification. A software tool has been developed, and made publicly available, to support the automatic generation of the BPMN models from the proposed patterns. Our semantified BPMN pattern allowed the identification of a large amount of implicit, and other not implemented, situations in both proof-of-concepts. It is concluded that the usage of a semantic solution, grounded on a sound pattern, allows the systematic enrichment of the BPMN models with a bounded effort. Moreover, to simplify the BPMN executable models' implementation, its elements could be classified as implicit, explicit, or not implemented yet. Finally, related work indicates that this work is demanded, but no full solutions are available.

</details>

<details>

<summary>2020-12-17 14:34:41 - Multi-View Collaborative Network Embedding</summary>

- *Sezin Kircali Ata, Yuan Fang, Min Wu, Jiaqi Shi, Chee Keong Kwoh, Xiaoli Li*

- `2005.08189v2` - [abs](http://arxiv.org/abs/2005.08189v2) - [pdf](http://arxiv.org/pdf/2005.08189v2)

> Real-world networks often exist with multiple views, where each view describes one type of interaction among a common set of nodes. For example, on a video-sharing network, while two user nodes are linked if they have common favorite videos in one view, they can also be linked in another view if they share common subscribers. Unlike traditional single-view networks, multiple views maintain different semantics to complement each other. In this paper, we propose MANE, a multi-view network embedding approach to learn low-dimensional representations. Similar to existing studies, MANE hinges on diversity and collaboration - while diversity enables views to maintain their individual semantics, collaboration enables views to work together. However, we also discover a novel form of second-order collaboration that has not been explored previously, and further unify it into our framework to attain superior node representations. Furthermore, as each view often has varying importance w.r.t. different nodes, we propose MANE+, an attention-based extension of MANE to model node-wise view importance. Finally, we conduct comprehensive experiments on three public, real-world multi-view networks, and the results demonstrate that our models consistently outperform state-of-the-art approaches.

</details>

<details>

<summary>2020-12-17 18:41:44 - Enhancing Fiber Orientation Distributions using convolutional Neural Networks</summary>

- *Oeslle Lucena, Sjoerd B. Vos, Vejay Vakharia, John Duncan, Keyoumars Ashkan, Rachel Sparks, Sebastien Ourselin*

- `2008.05409v2` - [abs](http://arxiv.org/abs/2008.05409v2) - [pdf](http://arxiv.org/pdf/2008.05409v2)

> Accurate local fiber orientation distribution (FOD) modeling based on diffusion magnetic resonance imaging (dMRI) capable of resolving complex fiber configurations benefits from specific acquisition protocols that sample a high number of gradient directions (b-vecs), a high maximum b-value(b-vals), and multiple b-values (multi-shell). However, acquisition time is limited in a clinical setting and commercial scanners may not provide such dMRI sequences. Therefore, dMRI is often acquired as single-shell (single b-value). In this work, we learn improved FODs for commercially acquired MRI. We evaluate patch-based 3D convolutional neural networks (CNNs)on their ability to regress multi-shell FOD representations from single-shell representations, where the representation is a spherical harmonics obtained from constrained spherical deconvolution (CSD) to model FODs. We evaluate U-Net and HighResNet 3D CNN architectures on data from the Human Connectome Project and an in-house dataset. We evaluate how well each CNN model can resolve local fiber orientation 1) when training and testing on datasets with the same dMRI acquisition protocol; 2) when testing on a dataset with a different dMRI acquisition protocol than used to train the CNN models; and 3) when testing on a dataset with a fewer number of gradient directions than used to train the CNN models. Our approach may enable robust CSD model estimation on single-shell dMRI acquisition protocols with few gradient directions, reducing acquisition times, facilitating translation of improved FOD estimation to time-limited clinical environments.

</details>

<details>

<summary>2020-12-18 07:04:04 - Semantics and explanation: why counterfactual explanations produce adversarial examples in deep neural networks</summary>

- *Kieran Browne, Ben Swift*

- `2012.10076v1` - [abs](http://arxiv.org/abs/2012.10076v1) - [pdf](http://arxiv.org/pdf/2012.10076v1)

> Recent papers in explainable AI have made a compelling case for counterfactual modes of explanation. While counterfactual explanations appear to be extremely effective in some instances, they are formally equivalent to adversarial examples. This presents an apparent paradox for explainability researchers: if these two procedures are formally equivalent, what accounts for the explanatory divide apparent between counterfactual explanations and adversarial examples? We resolve this paradox by placing emphasis back on the semantics of counterfactual expressions. Producing satisfactory explanations for deep learning systems will require that we find ways to interpret the semantics of hidden layer representations in deep neural networks.

</details>

<details>

<summary>2020-12-18 10:00:51 - Contrastive Multiview Coding</summary>

- *Yonglong Tian, Dilip Krishnan, Phillip Isola*

- `1906.05849v5` - [abs](http://arxiv.org/abs/1906.05849v5) - [pdf](http://arxiv.org/pdf/1906.05849v5)

> Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a "dog" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.

</details>

<details>

<summary>2020-12-18 11:17:15 - JeSemE: A Website for Exploring Diachronic Changes in Word Meaning and Emotion</summary>

- *Johannes Hellrich, Sven Buechel, Udo Hahn*

- `1807.04148v2` - [abs](http://arxiv.org/abs/1807.04148v2) - [pdf](http://arxiv.org/pdf/1807.04148v2)

> We here introduce a substantially extended version of JeSemE, an interactive website for visually exploring computationally derived time-variant information on word meanings and lexical emotions assembled from five large diachronic text corpora. JeSemE is designed for scholars in the (digital) humanities as an alternative to consulting manually compiled, printed dictionaries for such information (if available at all). This tool uniquely combines state-of-the-art distributional semantics with a nuanced model of human emotions, two information streams we deem beneficial for a data-driven interpretation of texts in the humanities.

</details>

<details>

<summary>2020-12-18 12:45:03 - Exact Reduction of Huge Action Spaces in General Reinforcement Learning</summary>

- *Sultan Javed Majeed, Marcus Hutter*

- `2012.10200v1` - [abs](http://arxiv.org/abs/2012.10200v1) - [pdf](http://arxiv.org/pdf/2012.10200v1)

> The reinforcement learning (RL) framework formalizes the notion of learning with interactions. Many real-world problems have large state-spaces and/or action-spaces such as in Go, StarCraft, protein folding, and robotics or are non-Markovian, which cause significant challenges to RL algorithms. In this work we address the large action-space problem by sequentializing actions, which can reduce the action-space size significantly, even down to two actions at the expense of an increased planning horizon. We provide explicit and exact constructions and equivalence proofs for all quantities of interest for arbitrary history-based processes. In the case of MDPs, this could help RL algorithms that bootstrap. In this work we show how action-binarization in the non-MDP case can significantly improve Extreme State Aggregation (ESA) bounds. ESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a fixed-sized non-Markovian state-space with the help of a surrogate Markovian process. On the upside, ESA enjoys similar optimality guarantees as Markovian models do. But a downside is that the size of the aggregated state-space becomes exponential in the size of the action-space. In this work, we patch this issue by binarizing the action-space. We provide an upper bound on the number of states of this binarized ESA that is logarithmic in the original action-space size, a double-exponential improvement.

</details>

<details>

<summary>2020-12-18 12:48:59 - Ultra-Fast, Low-Storage, Highly Effective Coarse-grained Selection in Retrieval-based Chatbot by Using Deep Semantic Hashing</summary>

- *Tian Lan, Xian-Ling Mao, Xiaoyan Gao, Wei Wei, Heyan Huang*

- `2012.09647v2` - [abs](http://arxiv.org/abs/2012.09647v2) - [pdf](http://arxiv.org/pdf/2012.09647v2)

> We study the coarse-grained selection module in retrieval-based chatbot. Coarse-grained selection is a basic module in a retrieval-based chatbot, which constructs a rough candidate set from the whole database to speed up the interaction with customers. So far, there are two kinds of approaches for coarse-grained selection module: (1) sparse representation; (2) dense representation. To the best of our knowledge, there is no systematic comparison between these two approaches in retrieval-based chatbots, and which kind of method is better in real scenarios is still an open question. In this paper, we first systematically compare these two methods from four aspects: (1) effectiveness; (2) index stoarge; (3) search time cost; (4) human evaluation. Extensive experiment results demonstrate that dense representation method significantly outperforms the sparse representation, but costs more time and storage occupation. In order to overcome these fatal weaknesses of dense representation method, we propose an ultra-fast, low-storage, and highly effective Deep Semantic Hashing Coarse-grained selection method, called DSHC model. Specifically, in our proposed DSHC model, a hashing optimizing module that consists of two autoencoder models is stacked on a trained dense representation model, and three loss functions are designed to optimize it. The hash codes provided by hashing optimizing module effectively preserve the rich semantic and similarity information in dense vectors. Extensive experiment results prove that, our proposed DSHC model can achieve much faster speed and lower storage than sparse representation, with limited performance loss compared with dense representation. Besides, our source codes have been publicly released for future research.

</details>

<details>

<summary>2020-12-18 14:59:34 - Temporal Bilinear Encoding Network of Audio-Visual Features at Low Sampling Rates</summary>

- *Feiyan Hu, Eva Mohedano, Noel O'Connor, Kevin McGuinness*

- `2012.10283v1` - [abs](http://arxiv.org/abs/2012.10283v1) - [pdf](http://arxiv.org/pdf/2012.10283v1)

> Current deep learning based video classification architectures are typically trained end-to-end on large volumes of data and require extensive computational resources. This paper aims to exploit audio-visual information in video classification with a 1 frame per second sampling rate. We propose Temporal Bilinear Encoding Networks (TBEN) for encoding both audio and visual long range temporal information using bilinear pooling and demonstrate bilinear pooling is better than average pooling on the temporal dimension for videos with low sampling rate. We also embed the label hierarchy in TBEN to further improve the robustness of the classifier. Experiments on the FGA240 fine-grained classification dataset using TBEN achieve a new state-of-the-art (hit@1=47.95%). We also exploit the possibility of incorporating TBEN with multiple decoupled modalities like visual semantic and motion features: experiments on UCF101 sampled at 1 FPS achieve close to state-of-the-art accuracy (hit@1=91.03%) while requiring significantly less computational resources than competing approaches for both training and prediction.

</details>

<details>

<summary>2020-12-18 15:53:50 - Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training</summary>

- *Peng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Cicero Nogueira dos Santos, Bing Xiang*

- `2012.10309v1` - [abs](http://arxiv.org/abs/2012.10309v1) - [pdf](http://arxiv.org/pdf/2012.10309v1)

> Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model (MLM). However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, Generation-Augmented Pre-training (GAP), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. GAP MODEL is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage GAP MODEL as a representation encoder obtain new state-of-the-art results on both SPIDER and CRITERIA-TO-SQL benchmarks.

</details>

<details>

<summary>2020-12-18 15:57:10 - Towards Formally Verified Compilation of Tag-Based Policy Enforcement</summary>

- *CHR Chhak, Andrew Tolmach, Sean Anderson*

- `2012.10313v1` - [abs](http://arxiv.org/abs/2012.10313v1) - [pdf](http://arxiv.org/pdf/2012.10313v1)

> Hardware-assisted reference monitoring is receiving increasing attention as a way to improve the security of existing software. One example is the PIPE architecture extension, which attaches metadata tags to register and memory values and executes tag-based rules at each machine instruction to enforce a software-defined security policy. To use PIPE effectively, engineers should be able to write security policies in terms of source-level concepts like functions, local variables, and structured control operators, which are not visible at machine level. It is the job of the compiler to generate PIPE-aware machine code that enforces these source-level policies. The compiler thus becomes part of the monitored system's trusted computing base -- and hence a prime candidate for verification.   To formalize compiler correctness in this setting, we extend the source language semantics with its own form of user-specified tag-based monitoring, and show that the compiler preserves that monitoring behavior. The challenges of compilation include mapping source-level monitoring policies to instruction-level tag rules, preserving fail-stop behaviors, and satisfying the surprisingly complex preconditions for conventional optimizations. In this paper, we describe the design and verification of Tagine, a small prototype compiler that translates a simple tagged WHILE language to a tagged register transfer language and performs simple optimizations. Tagine is based on the RTLgen and Deadcode phases of the CompCert compiler, and hence is written and verified in Coq. This work is a first step toward verification of a full-scale compiler for a realistic tagged source language.

</details>

<details>

<summary>2020-12-18 20:57:34 - An Infrastructure for Faithful Execution of Remote Attestation Protocols</summary>

- *Adam Petz, Perry Alexander*

- `2012.10511v1` - [abs](http://arxiv.org/abs/2012.10511v1) - [pdf](http://arxiv.org/pdf/2012.10511v1)

> Remote attestation is an emerging technology for establishing trust in a remote computing system. Copland is a domain-specific language for specifying layered attestation protocols, characterizing attestation-relevant system events, and describing evidence bundling. In this work we formally define and verify a Copland Compiler and Copland Virtual Machine for executing Copland protocols. The compiler translates Copland into instructions that are executed on the virtual machine. The compiler and virtual machine are implemented as monadic, functional programs in the Coq proof assistant and verified with respect to the Copland event and evidence semantics. In addition we introduce the Attestation Manager Monad as an environment for managing Copland term execution providing support for managing nonces, binding results of Copland protocols to variables, and appraising evidence results.

</details>

<details>

<summary>2020-12-18 21:37:37 - Achieving Operational Scalability Using Razee Continuous Deployment Model and Kubernetes Operators</summary>

- *Srini Bhagavan, Saravanan Balasubramanian, Prasad Reddy Annem, Thuan Ngo, Arun Soundararaj*

- `2012.10526v1` - [abs](http://arxiv.org/abs/2012.10526v1) - [pdf](http://arxiv.org/pdf/2012.10526v1)

> Recent advancements in the cloud computing domain have resulted in huge strides toward simplifying the procurement of hardware and software for diverse needs. By moving enterprise workloads to managed cloud offerings (private, public, hybrid), customers are delegating mundane tasks and labor-intensive maintenance activities related to network connectivity, procurement of cloud resource, application deployment, software patches, and upgrades, etc., This often translates to benefits such as high availability and reduced cost. The popularity of container and micro-services-based deployment has made Kubernetes the de-facto standard to deliver applications. However, even with Kubernetes orchestration, cloud service providers frequently have operational scalability issues due to lack of Continuous Integration and Continuous Deployment (CICD) automation and increased demand for human operators when managing a large number of software deployments across multiple data centers/availability zones. Kubernetes solves this in a novel way by creating and managing custom applications using Operators. Agile methodology advocates incremental CICD which are adopted by cloud providers. However, ironically, it is this same continuous delivery feature of application updates, Kubernetes cluster upgrades, etc., that is also a bane to cloud providers. In this paper, we will demonstrate the use of IBM open-source project Razee as a scalable continuous deployment framework to deploy open-source RStudio and Nginx Operators. We will discuss how IBM Watson SaaS application Operator, Blockchain applications, and Kubernetes resources updates, etc., can be deployed similarly and the use of Operators to perform application life cycle management. We assert that using Razee in conjunction with Operators on Kubernetes simplifies application life cycle management and increases scalability.

</details>

<details>

<summary>2020-12-18 22:19:57 - Biomedical Knowledge Graph Refinement and Completion using Graph Representation Learning and Top-K Similarity Measure</summary>

- *Islam Akef Ebeid, Majdi Hassan, Tingyi Wanyan, Jack Roper, Abhik Seal, Ying Ding*

- `2012.10540v1` - [abs](http://arxiv.org/abs/2012.10540v1) - [pdf](http://arxiv.org/pdf/2012.10540v1)

> Knowledge Graphs have been one of the fundamental methods for integrating heterogeneous data sources. Integrating heterogeneous data sources is crucial, especially in the biomedical domain, where central data-driven tasks such as drug discovery rely on incorporating information from different biomedical databases. These databases contain various biological entities and relations such as proteins (PDB), genes (Gene Ontology), drugs (DrugBank), diseases (DDB), and protein-protein interactions (BioGRID). The process of semantically integrating heterogeneous biomedical databases is often riddled with imperfections. The quality of data-driven drug discovery relies on the accuracy of the mining methods used and the data's quality as well. Thus, having complete and refined biomedical knowledge graphs is central to achieving more accurate drug discovery outcomes. Here we propose using the latest graph representation learning and embedding models to refine and complete biomedical knowledge graphs. This preliminary work demonstrates learning discrete representations of the integrated biomedical knowledge graph Chem2Bio2RD [3]. We perform a knowledge graph completion and refinement task using a simple top-K cosine similarity measure between the learned embedding vectors to predict missing links between drugs and targets present in the data. We show that this simple procedure can be used alternatively to binary classifiers in link prediction.

</details>

<details>

<summary>2020-12-18 23:46:56 - AGenT Zero: Zero-shot Automatic Multiple-Choice Question Generation for Skill Assessments</summary>

- *Eric Li, Jingyi Su, Hao Sheng, Lawrence Wai*

- `2012.01186v2` - [abs](http://arxiv.org/abs/2012.01186v2) - [pdf](http://arxiv.org/pdf/2012.01186v2)

> Multiple-choice questions (MCQs) offer the most promising avenue for skill evaluation in the era of virtual education and job recruiting, where traditional performance-based alternatives such as projects and essays have become less viable, and grading resources are constrained. The automated generation of MCQs would allow assessment creation at scale. Recent advances in natural language processing have given rise to many complex question generation methods. However, the few methods that produce deployable results in specific domains require a large amount of domain-specific training data that can be very costly to acquire. Our work provides an initial foray into MCQ generation under high data-acquisition cost scenarios by strategically emphasizing paraphrasing the question context (compared to the task). In addition to maintaining semantic similarity between the question-answer pairs, our pipeline, which we call AGenT Zero, consists of only pre-trained models and requires no fine-tuning, minimizing data acquisition costs for question generation. AGenT Zero successfully outperforms other pre-trained methods in fluency and semantic similarity. Additionally, with some small changes, our assessment pipeline can be generalized to a broader question and answer space, including short answer or fill in the blank questions.

</details>

<details>

<summary>2020-12-19 00:10:05 - Taking Control of Intra-class Variation in Conditional GANs Under Weak Supervision</summary>

- *Richard T. Marriott, Sami Romdhani, Liming Chen*

- `1811.11296v2` - [abs](http://arxiv.org/abs/1811.11296v2) - [pdf](http://arxiv.org/pdf/1811.11296v2)

> Generative Adversarial Networks (GANs) are able to learn mappings between simple, relatively low-dimensional, random distributions and points on the manifold of realistic images in image-space. The semantics of this mapping, however, are typically entangled such that meaningful image properties cannot be controlled independently of one another. Conditional GANs (cGANs) provide a potential solution to this problem, allowing specific semantics to be enforced during training. This solution, however, depends on the availability of precise labels, which are sometimes difficult or near impossible to obtain, e.g. labels representing lighting conditions or describing the background. In this paper we introduce a new formulation of the cGAN that is able to learn disentangled, multivariate models of semantically meaningful variation and which has the advantage of requiring only the weak supervision of binary attribute labels. For example, given only labels of ambient / non-ambient lighting, our method is able to learn multivariate lighting models disentangled from other factors such as the identity and pose. We coin the method intra-class variation isolation (IVI) and the resulting network the IVI-GAN. We evaluate IVI-GAN on the CelebA dataset and on synthetic 3D morphable model data, learning to disentangle attributes such as lighting, pose, expression, and even the background.

</details>

<details>

<summary>2020-12-19 11:55:46 - FraCaS: Temporal Analysis</summary>

- *Jean-Philippe Bernardy, Stergios Chatzikyriakidis*

- `2012.10668v1` - [abs](http://arxiv.org/abs/2012.10668v1) - [pdf](http://arxiv.org/pdf/2012.10668v1)

> In this paper, we propose an implementation of temporal semantics which is suitable for inference problems. This implementation translates syntax trees to logical formulas, suitable for consumption by the Coq proof assistant. We support several phenomena including: temporal references, temporal adverbs, aspectual classes and progressives. We apply these semantics to the complete FraCaS testsuite. We obtain an accuracy of 81 percent overall and 73 percent for problems explicitly marked as related to temporal reference.

</details>

<details>

<summary>2020-12-19 22:29:49 - Probabilistic Dependency Graphs</summary>

- *Oliver Richardson, Joseph Y Halpern*

- `2012.10800v1` - [abs](http://arxiv.org/abs/2012.10800v1) - [pdf](http://arxiv.org/pdf/2012.10800v1)

> We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed graphical models. PDGs can capture inconsistent beliefs in a natural way and are more modular than Bayesian Networks (BNs), in that they make it easier to incorporate new information and restructure the representation. We show by example how PDGs are an especially natural modeling tool. We provide three semantics for PDGs, each of which can be derived from a scoring function (on joint distributions over the variables in the network) that can be viewed as representing a distribution's incompatibility with the PDG. For the PDG corresponding to a BN, this function is uniquely minimized by the distribution the BN represents, showing that PDG semantics extend BN semantics. We show further that factor graphs and their exponential families can also be faithfully represented as PDGs, while there are significant barriers to modeling a PDG with a factor graph.

</details>

<details>

<summary>2020-12-19 23:23:40 - Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection</summary>

- *Yikang Li, Pulkit Goel, Varsha Kuppur Rajendra, Har Simrat Singh, Jonathan Francis, Kaixin Ma, Eric Nyberg, Alessandro Oltramari*

- `2012.10813v1` - [abs](http://arxiv.org/abs/2012.10813v1) - [pdf](http://arxiv.org/pdf/2012.10813v1)

> Conditional text generation has been a challenging task that is yet to see human-level performance from state-of-the-art models. In this work, we specifically focus on the Commongen benchmark, wherein the aim is to generate a plausible sentence for a given set of input concepts. Despite advances in other tasks, large pre-trained language models that are fine-tuned on this dataset often produce sentences that are syntactically correct but qualitatively deviate from a human understanding of common sense. Furthermore, generated sequences are unable to fulfill such lexical requirements as matching part-of-speech and full concept coverage. In this paper, we explore how commonsense knowledge graphs can enhance model performance, with respect to commonsense reasoning and lexically-constrained decoding. We propose strategies for enhancing the semantic correctness of the generated text, which we accomplish through: extracting commonsense relations from Conceptnet, injecting these relations into the Unified Language Model (UniLM) through attention mechanisms, and enforcing the aforementioned lexical requirements through output constraints. By performing several ablations, we find that commonsense injection enables the generation of sentences that are more aligned with human understanding, while remaining compliant with lexical requirements.

</details>

<details>

<summary>2020-12-20 02:02:39 - Hierarchical Metadata-Aware Document Categorization under Weak Supervision</summary>

- *Yu Zhang, Xiusi Chen, Yu Meng, Jiawei Han*

- `2010.13556v2` - [abs](http://arxiv.org/abs/2010.13556v2) - [pdf](http://arxiv.org/pdf/2010.13556v2)

> Categorizing documents into a given label hierarchy is intuitively appealing due to the ubiquity of hierarchical topic structures in massive text corpora. Although related studies have achieved satisfying performance in fully supervised hierarchical document classification, they usually require massive human-annotated training data and only utilize text information. However, in many domains, (1) annotations are quite expensive where very few training samples can be acquired; (2) documents are accompanied by metadata information. Hence, this paper studies how to integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. We develop HiMeCat, an embedding-based generative framework for our task. Specifically, we propose a novel joint representation learning module that allows simultaneous modeling of category dependencies, metadata information and textual semantics, and we introduce a data augmentation module that hierarchically synthesizes training documents to complement the original, small-scale training set. Our experiments demonstrate a consistent improvement of HiMeCat over competitive baselines and validate the contribution of our representation learning and data augmentation modules.

</details>

<details>

<summary>2020-12-20 13:58:52 - Explaining Black-box Models for Biomedical Text Classification</summary>

- *Milad Moradi, Matthias Samwald*

- `2012.10928v1` - [abs](http://arxiv.org/abs/2012.10928v1) - [pdf](http://arxiv.org/pdf/2012.10928v1)

> In this paper, we propose a novel method named Biomedical Confident Itemsets Explanation (BioCIE), aiming at post-hoc explanation of black-box machine learning models for biomedical text classification. Using sources of domain knowledge and a confident itemset mining method, BioCIE discretizes the decision space of a black-box into smaller subspaces and extracts semantic relationships between the input text and class labels in different subspaces. Confident itemsets discover how biomedical concepts are related to class labels in the black-box's decision space. BioCIE uses the itemsets to approximate the black-box's behavior for individual predictions. Optimizing fidelity, interpretability, and coverage measures, BioCIE produces class-wise explanations that represent decision boundaries of the black-box. Results of evaluations on various biomedical text classification tasks and black-box models demonstrated that BioCIE can outperform perturbation-based and decision set methods in terms of producing concise, accurate, and interpretable explanations. BioCIE improved the fidelity of instance-wise and class-wise explanations by 11.6% and 7.5%, respectively. It also improved the interpretability of explanations by 8%. BioCIE can be effectively used to explain how a black-box biomedical text classification model semantically relates input texts to class labels. The source code and supplementary material are available at https://github.com/mmoradi-iut/BioCIE.

</details>

<details>

<summary>2020-12-20 14:42:04 - Lexicographic Logic: a Many-valued Logic for Preference Representation</summary>

- *Angelos Charalambidis, Giorgos Papadimitriou, Panos Rondogiannis, Antonis Troumpoukis*

- `2012.10940v1` - [abs](http://arxiv.org/abs/2012.10940v1) - [pdf](http://arxiv.org/pdf/2012.10940v1)

> Logical formalisms provide a natural and concise means for specifying and reasoning about preferences. In this paper, we propose lexicographic logic, an extension of classical propositional logic that can express a variety of preferences, most notably lexicographic ones. The proposed logic supports a simple new connective whose semantics can be defined in terms of finite lists of truth values. We demonstrate that, despite the well-known theoretical limitations that pose barriers to the quantitative representation of lexicographic preferences, there exists a subset of the rational numbers over which the proposed new connective can be naturally defined. Lexicographic logic can be used to define in a simple way some well-known preferential operators, like "$A$ and if possible $B$", and "$A$ or failing that $B$". Moreover, many other hierarchical preferential operators can be defined using a systematic approach. We argue that the new logic is an effective formalism for ranking query results according to the satisfaction level of user preferences.

</details>

<details>

<summary>2020-12-20 17:23:48 - Methodology for Mining, Discovering and Analyzing Semantic Human Mobility Behaviors</summary>

- *Clement Moreau, Thomas Devogele, Laurent Etienne, Veronika Peralta, Cyril de Runz*

- `2012.04767v2` - [abs](http://arxiv.org/abs/2012.04767v2) - [pdf](http://arxiv.org/pdf/2012.04767v2)

> Various institutes produce large semantic datasets containing information regarding daily activities and human mobility. The analysis and understanding of such data are crucial for urban planning, socio-psychology, political sciences, and epidemiology. However, none of the typical data mining processes have been customized for the thorough analysis of semantic mobility sequences to translate data into understandable behaviors. Based on an extended literature review, we propose a novel methodological pipeline called simba (Semantic Indicators for Mobility and Behavior Analysis), for mining and analyzing semantic mobility sequences to identify coherent information and human behaviors. A framework for semantic sequence mobility analysis and clustering explicability based on integrating different complementary statistical indicators and visual tools is implemented. To validate this methodology, we used a large set of real daily mobility sequences obtained from a household travel survey. Complementary knowledge is automatically discovered in the proposed method.

</details>

<details>

<summary>2020-12-20 20:13:02 - KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA</summary>

- *Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus Rohrbach*

- `2012.11014v1` - [abs](http://arxiv.org/abs/2012.11014v1) - [pdf](http://arxiv.org/pdf/2012.11014v1)

> One of the most challenging question types in VQA is when answering the question requires outside knowledge not present in the image. In this work we study open-domain knowledge, the setting when the knowledge required to answer a question is not given/annotated, neither at training nor test time. We tap into two types of knowledge representations and reasoning. First, implicit knowledge which can be learned effectively from unsupervised language pre-training and supervised training data with transformer-based models. Second, explicit, symbolic knowledge encoded in knowledge bases. Our approach combines both - exploiting the powerful implicit reasoning of transformer models for answer prediction, and integrating symbolic representations from a knowledge graph, while never losing their explicit semantics to an implicit embedding. We combine diverse sources of knowledge to cover the wide variety of knowledge needed to solve knowledge-based questions. We show our approach, KRISP (Knowledge Reasoning with Implicit and Symbolic rePresentations), significantly outperforms state-of-the-art on OK-VQA, the largest available dataset for open-domain knowledge-based VQA. We show with extensive ablations that while our model successfully exploits implicit knowledge reasoning, the symbolic answer module which explicitly connects the knowledge graph to the answer vocabulary is critical to the performance of our method and generalizes to rare answers.

</details>

<details>

<summary>2020-12-21 07:03:43 - A complete, parallel and autonomous photonic neural network in a semiconductor multimode laser</summary>

- *Xavier Porte, Anas Skalli, Nasibeh Haghighi, Stephan Reitzenstein, James A. Lott, Daniel Brunner*

- `2012.11153v1` - [abs](http://arxiv.org/abs/2012.11153v1) - [pdf](http://arxiv.org/pdf/2012.11153v1)

> Neural networks are one of the disruptive computing concepts of our time. However, they fundamentally differ from classical, algorithmic computing in a number of fundamental aspects. These differences result in equally fundamental, severe and relevant challenges for neural network computing using current computing substrates. Neural networks urge for parallelism across the entire processor and for a co-location of memory and arithmetic, i.e. beyond von Neumann architectures. Parallelism in particular made photonics a highly promising platform, yet until now scalable and integratable concepts are scarce. Here, we demonstrate for the first time how a fully parallel and fully implemented photonic neural network can be realized using spatially distributed modes of an efficient and fast semiconductor laser. Importantly, all neural network connections are realized in hardware, and our processor produces results without pre- or post-processing. 130+ nodes are implemented in a large-area vertical cavity surface emitting laser, input and output weights are realized via the complex transmission matrix of a multimode fiber and a digital micro-mirror array, respectively. We train the readout weights to perform 2-bit header recognition, a 2-bit XOR and 2-bit digital analog conversion, and obtain < 0.9 10^-3 and 2.9 10^-2 error rates for digit recognition and XOR, respectively. Finally, the digital analog conversion can be realized with a standard deviation of only 5.4 10^-2. Our system is scalable to much larger sizes and to bandwidths in excess of 20 GHz.

</details>

<details>

<summary>2020-12-21 07:43:31 - MT-Teql: Evaluating and Augmenting Consistency of Text-to-SQL Models with Metamorphic Testing</summary>

- *Pingchuan Ma, Shuai Wang*

- `2012.11163v1` - [abs](http://arxiv.org/abs/2012.11163v1) - [pdf](http://arxiv.org/pdf/2012.11163v1)

> Text-to-SQL is a task to generate SQL queries from human utterances. However, due to the variation of natural language, two semantically equivalent utterances may appear differently in the lexical level. Likewise, user preferences (e.g., the choice of normal forms) can lead to dramatic changes in table structures when expressing conceptually identical schemas. Envisioning the general difficulty for text-to-SQL models to preserve prediction consistency against linguistic and schema variations, we propose MT-Teql, a Metamorphic Testing-based framework for systematically evaluating and augmenting the consistency of TExt-to-SQL models. Inspired by the principles of software metamorphic testing, MT-Teql delivers a model-agnostic framework which implements a comprehensive set of metamorphic relations (MRs) to conduct semantics-preserving transformations toward utterances and schemas. Model Inconsistency can be exposed when the original and transformed inputs induce different SQL queries. In addition, we leverage the transformed inputs to retrain models for further model robustness boost. Our experiments show that our framework exposes thousands of prediction errors from SOTA models and enriches existing datasets by order of magnitude, eliminating over 40% inconsistency errors without compromising standard accuracy.

</details>

<details>

<summary>2020-12-21 08:01:04 - An End-to-End Document-Level Neural Discourse Parser Exploiting Multi-Granularity Representations</summary>

- *Ke Shi, Zhengyuan Liu, Nancy F. Chen*

- `2012.11169v1` - [abs](http://arxiv.org/abs/2012.11169v1) - [pdf](http://arxiv.org/pdf/2012.11169v1)

> Document-level discourse parsing, in accordance with the Rhetorical Structure Theory (RST), remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granularity across syntax and semantics, and in turn incorporate such representations in an end-to-end encoder-decoder neural architecture for more resourceful discourse processing. In particular, we first use a pre-trained contextual language model that embodies high-order and long-range dependency to enable finer-grain semantic, syntactic, and organizational representations. We further encode such representations with boundary and hierarchical information to obtain more refined modeling for document-level discourse processing. Experimental results show that our parser achieves the state-of-the-art performance, approaching human-level performance on the benchmarked RST dataset.

</details>

<details>

<summary>2020-12-21 15:34:08 - Generalized Adversarially Learned Inference</summary>

- *Yatin Dandi, Homanga Bharadhwaj, Abhishek Kumar, Piyush Rai*

- `2006.08089v3` - [abs](http://arxiv.org/abs/2006.08089v3) - [pdf](http://arxiv.org/pdf/2006.08089v3)

> Allowing effective inference of latent vectors while training GANs can greatly increase their applicability in various downstream tasks. Recent approaches, such as ALI and BiGAN frameworks, develop methods of inference of latent variables in GANs by adversarially training an image generator along with an encoder to match two joint distributions of image and latent vector pairs. We generalize these approaches to incorporate multiple layers of feedback on reconstructions, self-supervision, and other forms of supervision based on prior or learned knowledge about the desired solutions. We achieve this by modifying the discriminator's objective to correctly identify more than two joint distributions of tuples of an arbitrary number of random variables consisting of images, latent vectors, and other variables generated through auxiliary tasks, such as reconstruction and inpainting or as outputs of suitable pre-trained models. We design a non-saturating maximization objective for the generator-encoder pair and prove that the resulting adversarial game corresponds to a global optimum that simultaneously matches all the distributions. Within our proposed framework, we introduce a novel set of techniques for providing self-supervised feedback to the model based on properties, such as patch-level correspondence and cycle consistency of reconstructions. Through comprehensive experiments, we demonstrate the efficacy, scalability, and flexibility of the proposed approach for a variety of tasks.

</details>

<details>

<summary>2020-12-21 18:21:09 - Privacy Interpretation of Behavioural-based Anomaly Detection Approaches</summary>

- *Muhammad Imran Khan, Simon Foley, Barry O'Sullivan*

- `2012.11541v1` - [abs](http://arxiv.org/abs/2012.11541v1) - [pdf](http://arxiv.org/pdf/2012.11541v1)

> This paper proposes the notion of 'Privacy-Anomaly Detection' and considers the question of whether behavioural-based anomaly detection approaches can have a privacy semantic interpretation and whether the detected anomalies can be related to the conventional (formal) definitions of privacy semantics such as k-anonymity. The idea is to learn the user's past querying behaviour in terms of privacy and then identifying deviations from past behaviour in order to detect privacy violations. Privacy attacks, violations of formal privacy definition, based on a sequence of SQL queries (query correlations) are also considered in the paper and it is shown that interactive querying settings are vulnerable to privacy attacks based on query sequences. Investigation on whether these types of privacy attacks can potentially manifest themselves as anomalies, specifically as privacy-anomalies was carried out. It is shown that in this paper that behavioural-based anomaly detection approaches have the potential to detect privacy attacks based on query sequences (violation of formal privacy definition) as privacy-anomalies.

</details>

<details>

<summary>2020-12-21 23:14:18 - SChuBERT: Scholarly Document Chunks with BERT-encoding boost Citation Count Prediction</summary>

- *Thomas van Dongen, Gideon Maillette de Buy Wenniger, Lambert Schomaker*

- `2012.11740v1` - [abs](http://arxiv.org/abs/2012.11740v1) - [pdf](http://arxiv.org/pdf/2012.11740v1)

> Predicting the number of citations of scholarly documents is an upcoming task in scholarly document processing. Besides the intrinsic merit of this information, it also has a wider use as an imperfect proxy for quality which has the advantage of being cheaply available for large volumes of scholarly documents. Previous work has dealt with number of citations prediction with relatively small training data sets, or larger datasets but with short, incomplete input text. In this work we leverage the open access ACL Anthology collection in combination with the Semantic Scholar bibliometric database to create a large corpus of scholarly documents with associated citation information and we propose a new citation prediction model called SChuBERT. In our experiments we compare SChuBERT with several state-of-the-art citation prediction models and show that it outperforms previous methods by a large margin. We also show the merit of using more training data and longer input for number of citations prediction.

</details>

<details>

<summary>2020-12-21 23:48:06 - Contraband Materials Detection Within Volumetric 3D Computed Tomography Baggage Security Screening Imagery</summary>

- *Qian Wang, Toby P. Breckon*

- `2012.11753v1` - [abs](http://arxiv.org/abs/2012.11753v1) - [pdf](http://arxiv.org/pdf/2012.11753v1)

> Automatic prohibited object detection within 2D/3D X-ray Computed Tomography (CT) has been studied in literature to enhance the aviation security screening at checkpoints. Deep Convolutional Neural Networks (CNN) have demonstrated superior performance in 2D X-ray imagery. However, there exists very limited proof of how deep neural networks perform in materials detection within volumetric 3D CT baggage screening imagery. We attempt to close this gap by applying Deep Neural Networks in 3D contraband substance detection based on their material signatures. Specifically, we formulate it as a 3D semantic segmentation problem to identify material types for all voxels based on which contraband materials can be detected. To this end, we firstly investigate 3D CNN based semantic segmentation algorithms such as 3D U-Net and its variants. In contrast to the original dense representation form of volumetric 3D CT data, we propose to convert the CT volumes into sparse point clouds which allows the use of point cloud processing approaches such as PointNet++ towards more efficient processing. Experimental results on a publicly available dataset (NEU ATR) demonstrate the effectiveness of both 3D U-Net and PointNet++ in materials detection in 3D CT imagery for baggage security screening.

</details>

<details>

<summary>2020-12-22 02:55:04 - Semi-Supervised Disentangled Framework for Transferable Named Entity Recognition</summary>

- *Zhifeng Hao, Di Lv, Zijian Li, Ruichu Cai, Wen Wen, Boyan Xu*

- `2012.11805v1` - [abs](http://arxiv.org/abs/2012.11805v1) - [pdf](http://arxiv.org/pdf/2012.11805v1)

> Named entity recognition (NER) for identifying proper nouns in unstructured text is one of the most important and fundamental tasks in natural language processing. However, despite the widespread use of NER models, they still require a large-scale labeled data set, which incurs a heavy burden due to manual annotation. Domain adaptation is one of the most promising solutions to this problem, where rich labeled data from the relevant source domain are utilized to strengthen the generalizability of a model based on the target domain. However, the mainstream cross-domain NER models are still affected by the following two challenges (1) Extracting domain-invariant information such as syntactic information for cross-domain transfer. (2) Integrating domain-specific information such as semantic information into the model to improve the performance of NER. In this study, we present a semi-supervised framework for transferable NER, which disentangles the domain-invariant latent variables and domain-specific latent variables. In the proposed framework, the domain-specific information is integrated with the domain-specific latent variables by using a domain predictor. The domain-specific and domain-invariant latent variables are disentangled using three mutual information regularization terms, i.e., maximizing the mutual information between the domain-specific latent variables and the original embedding, maximizing the mutual information between the domain-invariant latent variables and the original embedding, and minimizing the mutual information between the domain-specific and domain-invariant latent variables. Extensive experiments demonstrated that our model can obtain state-of-the-art performance with cross-domain and cross-lingual NER benchmark data sets.

</details>

<details>

<summary>2020-12-22 03:03:36 - Learning Disentangled Semantic Representation for Domain Adaptation</summary>

- *Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, Zhifeng Hao*

- `2012.11807v1` - [abs](http://arxiv.org/abs/2012.11807v1) - [pdf](http://arxiv.org/pdf/2012.11807v1)

> Domain adaptation is an important but challenging task. Most of the existing domain adaptation methods struggle to extract the domain-invariant representation on the feature space with entangling domain information and semantic information. Different from previous efforts on the entangled feature space, we aim to extract the domain invariant semantic information in the latent disentangled semantic representation (DSR) of the data. In DSR, we assume the data generation process is controlled by two independent sets of variables, i.e., the semantic latent variables and the domain latent variables. Under the above assumption, we employ a variational auto-encoder to reconstruct the semantic latent variables and domain latent variables behind the data. We further devise a dual adversarial network to disentangle these two sets of reconstructed latent variables. The disentangled semantic latent variables are finally adapted across the domains. Experimental studies testify that our model yields state-of-the-art performance on several domain adaptation benchmark datasets.

</details>

<details>

<summary>2020-12-22 04:43:20 - Multimodal Research in Vision and Language: A Review of Current and Emerging Trends</summary>

- *Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar, Soujanya Poria, Roger Zimmermann, Amir Zadeh*

- `2010.09522v2` - [abs](http://arxiv.org/abs/2010.09522v2) - [pdf](http://arxiv.org/pdf/2010.09522v2)

> Deep Learning and its applications have cascaded impactful research and development with a diverse range of modalities present in the real-world data. More recently, this has enhanced research interests in the intersection of the Vision and Language arena with its numerous applications and fast-paced growth. In this paper, we present a detailed overview of the latest trends in research pertaining to visual and language modalities. We look at its applications in their task formulations and how to solve various problems related to semantic perception and content generation. We also address task-specific trends, along with their evaluation strategies and upcoming challenges. Moreover, we shed some light on multi-disciplinary patterns and insights that have emerged in the recent past, directing this field towards more modular and transparent intelligent systems. This survey identifies key trends gravitating recent literature in VisLang research and attempts to unearth directions that the field is heading towards.

</details>

<details>

<summary>2020-12-22 11:21:09 - Knowledge Graphs Evolution and Preservation -- A Technical Report from ISWS 2019</summary>

- *Nacira Abbas, Kholoud Alghamdi, Mortaza Alinam, Francesca Alloatti, Glenda Amaral, Claudia d'Amato, Luigi Asprino, Martin Beno, Felix Bensmann, Russa Biswas, Ling Cai, Riley Capshaw, Valentina Anita Carriero, Irene Celino, Amine Dadoun, Stefano De Giorgis, Harm Delva, John Domingue, Michel Dumontier, Vincent Emonet, Marieke van Erp, Paola Espinoza Arias, Omaima Fallatah, Sebastián Ferrada, Marc Gallofré Ocaña, Michalis Georgiou, Genet Asefa Gesese, Frances Gillis-Webber, Francesca Giovannetti, Marìa Granados Buey, Ismail Harrando, Ivan Heibi, Vitor Horta, Laurine Huber, Federico Igne, Mohamad Yaser Jaradeh, Neha Keshan, Aneta Koleva, Bilal Koteich, Kabul Kurniawan, Mengya Liu, Chuangtao Ma, Lientje Maas, Martin Mansfield, Fabio Mariani, Eleonora Marzi, Sepideh Mesbah, Maheshkumar Mistry, Alba Catalina Morales Tirado, Anna Nguyen, Viet Bach Nguyen, Allard Oelen, Valentina Pasqual, Heiko Paulheim, Axel Polleres, Margherita Porena, Jan Portisch, Valentina Presutti, Kader Pustu-Iren, Ariam Rivas Mendez, Soheil Roshankish, Sebastian Rudolph, Harald Sack, Ahmad Sakor, Jaime Salas, Thomas Schleider, Meilin Shi, Gianmarco Spinaci, Chang Sun, Tabea Tietz, Molka Tounsi Dhouib, Alessandro Umbrico, Wouter van den Berg, Weiqin Xu*

- `2012.11936v1` - [abs](http://arxiv.org/abs/2012.11936v1) - [pdf](http://arxiv.org/pdf/2012.11936v1)

> One of the grand challenges discussed during the Dagstuhl Seminar "Knowledge Graphs: New Directions for Knowledge Representation on the Semantic Web" and described in its report is that of a: "Public FAIR Knowledge Graph of Everything: We increasingly see the creation of knowledge graphs that capture information about the entirety of a class of entities. [...] This grand challenge extends this further by asking if we can create a knowledge graph of "everything" ranging from common sense concepts to location based entities. This knowledge graph should be "open to the public" in a FAIR manner democratizing this mass amount of knowledge." Although linked open data (LOD) is one knowledge graph, it is the closest realisation (and probably the only one) to a public FAIR Knowledge Graph (KG) of everything. Surely, LOD provides a unique testbed for experimenting and evaluating research hypotheses on open and FAIR KG. One of the most neglected FAIR issues about KGs is their ongoing evolution and long term preservation. We want to investigate this problem, that is to understand what preserving and supporting the evolution of KGs means and how these problems can be addressed. Clearly, the problem can be approached from different perspectives and may require the development of different approaches, including new theories, ontologies, metrics, strategies, procedures, etc. This document reports a collaborative effort performed by 9 teams of students, each guided by a senior researcher as their mentor, attending the International Semantic Web Research School (ISWS 2019). Each team provides a different perspective to the problem of knowledge graph evolution substantiated by a set of research questions as the main subject of their investigation. In addition, they provide their working definition for KG preservation and evolution.

</details>

<details>

<summary>2020-12-22 11:45:03 - Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing</summary>

- *Ruisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu Chen, Kai Yu*

- `2005.13485v3` - [abs](http://arxiv.org/abs/2005.13485v3) - [pdf](http://arxiv.org/pdf/2005.13485v3)

> One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.

</details>

<details>

<summary>2020-12-22 12:27:45 - A Hierarchical Reasoning Graph Neural Network for The Automatic Scoring of Answer Transcriptions in Video Job Interviews</summary>

- *Kai Chen, Meng Niu, Qingcai Chen*

- `2012.11960v1` - [abs](http://arxiv.org/abs/2012.11960v1) - [pdf](http://arxiv.org/pdf/2012.11960v1)

> We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition (ASR) transcriptions in the asynchronous video job interview (AVI). The key challenge is how to construct the dependency relation between questions and answers, and conduct the semantic level interaction for each question-answer (QA) pair. However, most of the recent studies in AVI focus on how to represent questions and answers better, but ignore the dependency information and interaction between them, which is critical for QA evaluation. In this work, we propose a Hierarchical Reasoning Graph Neural Network (HRGNN) for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the dependency information of sentences in or between the question and the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit encoder to represent the temporal question-answer pairs for the final prediction. Empirical results conducted on CHNAT (a real-world dataset) validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.

</details>

<details>

<summary>2020-12-22 14:43:28 - Quantum Cognitive Triad. Semantic geometry of context representation</summary>

- *Ilya A. Surov*

- `2002.11195v2` - [abs](http://arxiv.org/abs/2002.11195v2) - [pdf](http://arxiv.org/pdf/2002.11195v2)

> The paper describes an algorithm for semantic representation of behavioral contexts relative to a dichotomic decision alternative. The contexts are represented as quantum qubit states in two-dimensional Hilbert space visualized as points on the Bloch sphere. The azimuthal coordinate of this sphere functions as a one-dimensional semantic space in which the contexts are accommodated according to their subjective relevance to the considered uncertainty. The contexts are processed in triples defined by knowledge of a subject about a binary situational factor. The obtained triads of context representations function as stable cognitive structure at the same time allowing a subject to model probabilistically-variative behavior. The developed algorithm illustrates an approach for quantitative subjectively-semantic modeling of behavior based on conceptual and mathematical apparatus of quantum theory.

</details>

<details>

<summary>2020-12-22 14:56:54 - Event-Driven Query Expansion</summary>

- *Guy D. Rosin, Ido Guy, Kira Radinsky*

- `2012.12065v1` - [abs](http://arxiv.org/abs/2012.12065v1) - [pdf](http://arxiv.org/pdf/2012.12065v1)

> A significant number of event-related queries are issued in Web search. In this paper, we seek to improve retrieval performance by leveraging events and specifically target the classic task of query expansion. We propose a method to expand an event-related query by first detecting the events related to it. Then, we derive the candidates for expansion as terms semantically related to both the query and the events. To identify the candidates, we utilize a novel mechanism to simultaneously embed words and events in the same vector space. We show that our proposed method of leveraging events improves query expansion performance significantly compared with state-of-the-art methods on various newswire TREC datasets.

</details>

<details>

<summary>2020-12-22 17:14:19 - Latent Feature Representation via Unsupervised Learning for Pattern Discovery in Massive Electron Microscopy Image Volumes</summary>

- *Gary B Huang, Huei-Fang Yang, Shin-ya Takemura, Pat Rivlin, Stephen M Plaza*

- `2012.12175v1` - [abs](http://arxiv.org/abs/2012.12175v1) - [pdf](http://arxiv.org/pdf/2012.12175v1)

> We propose a method to facilitate exploration and analysis of new large data sets. In particular, we give an unsupervised deep learning approach to learning a latent representation that captures semantic similarity in the data set. The core idea is to use data augmentations that preserve semantic meaning to generate synthetic examples of elements whose feature representations should be close to one another.   We demonstrate the utility of our method applied to nano-scale electron microscopy data, where even relatively small portions of animal brains can require terabytes of image data. Although supervised methods can be used to predict and identify known patterns of interest, the scale of the data makes it difficult to mine and analyze patterns that are not known a priori. We show the ability of our learned representation to enable query by example, so that if a scientist notices an interesting pattern in the data, they can be presented with other locations with matching patterns. We also demonstrate that clustering of data in the learned space correlates with biologically-meaningful distinctions. Finally, we introduce a visualization tool and software ecosystem to facilitate user-friendly interactive analysis and uncover interesting biological patterns. In short, our work opens possible new avenues in understanding of and discovery in large data sets, arising in domains such as EM analysis.

</details>

<details>

<summary>2020-12-22 21:34:02 - Multi-Head Self-Attention with Role-Guided Masks</summary>

- *Dongsheng Wang, Casper Hansen, Lucas Chaves Lima, Christian Hansen, Maria Maistro, Jakob Grue Simonsen, Christina Lioma*

- `2012.12366v1` - [abs](http://arxiv.org/abs/2012.12366v1) - [pdf](http://arxiv.org/pdf/2012.12366v1)

> The state of the art in learning meaningful semantic representations of words is the Transformer model and its attention mechanisms. Simply put, the attention mechanisms learn to attend to specific parts of the input dispensing recurrence and convolutions. While some of the learned attention heads have been found to play linguistically interpretable roles, they can be redundant or prone to errors. We propose a method to guide the attention heads towards roles identified in prior work as important. We do this by defining role-specific masks to constrain the heads to attend to specific parts of the input, such that different heads are designed to play different roles. Experiments on text classification and machine translation using 7 different datasets show that our method outperforms competitive attention-based, CNN, and RNN baselines.

</details>

<details>

<summary>2020-12-23 07:11:30 - GAHNE: Graph-Aggregated Heterogeneous Network Embedding</summary>

- *Xiaohe Li, Lijie Wen, Chen Qian, Jianmin Wang*

- `2012.12517v1` - [abs](http://arxiv.org/abs/2012.12517v1) - [pdf](http://arxiv.org/pdf/2012.12517v1)

> The real-world networks often compose of different types of nodes and edges with rich semantics, widely known as heterogeneous information network (HIN). Heterogeneous network embedding aims to embed nodes into low-dimensional vectors which capture rich intrinsic information of heterogeneous networks. However, existing models either depend on manually designing meta-paths, ignore mutual effects between different semantics, or omit some aspects of information from global networks. To address these limitations, we propose a novel Graph-Aggregated Heterogeneous Network Embedding (GAHNE), which is designed to extract the semantics of HINs as comprehensively as possible to improve the results of downstream tasks based on graph convolutional neural networks. In GAHNE model, we develop several mechanisms that can aggregate semantic representations from different single-type sub-networks as well as fuse the global information into final embeddings. Extensive experiments on three real-world HIN datasets show that our proposed model consistently outperforms the existing state-of-the-art methods.

</details>

<details>

<summary>2020-12-23 07:47:13 - The Translucent Patch: A Physical and Universal Attack on Object Detectors</summary>

- *Alon Zolfi, Moshe Kravchik, Yuval Elovici, Asaf Shabtai*

- `2012.12528v1` - [abs](http://arxiv.org/abs/2012.12528v1) - [pdf](http://arxiv.org/pdf/2012.12528v1)

> Physical adversarial attacks against object detectors have seen increasing success in recent years. However, these attacks require direct access to the object of interest in order to apply a physical patch. Furthermore, to hide multiple objects, an adversarial patch must be applied to each object. In this paper, we propose a contactless translucent physical patch containing a carefully constructed pattern, which is placed on the camera's lens, to fool state-of-the-art object detectors. The primary goal of our patch is to hide all instances of a selected target class. In addition, the optimization method used to construct the patch aims to ensure that the detection of other (untargeted) classes remains unharmed. Therefore, in our experiments, which are conducted on state-of-the-art object detection models used in autonomous driving, we study the effect of the patch on the detection of both the selected target class and the other classes. We show that our patch was able to prevent the detection of 42.27% of all stop sign instances while maintaining high (nearly 80%) detection of the other classes.

</details>

<details>

<summary>2020-12-23 09:58:38 - ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation</summary>

- *Guodong Zeng, Till D. Lerch, Florian Schmaranzer, Guoyan Zheng, Juergen Burger, Kate Gerber, Moritz Tannast, Klaus Siebenrock, Nicolas Gerber*

- `2012.12570v1` - [abs](http://arxiv.org/abs/2012.12570v1) - [pdf](http://arxiv.org/pdf/2012.12570v1)

> Unsupervised domain adaptation (UDA) for cross-modality medical image segmentation has shown great progress by domain-invariant feature learning or image appearance translation. Adapted feature learning usually cannot detect domain shifts at the pixel level and is not able to achieve good results in dense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN, translates images into different styles with good appearance, despite its population, its semantic consistency is hardly to maintain and results in poor cross-modality segmentation. In this paper, we propose intra- and cross-modality semantic consistency (ICMSC) for UDA and our key insight is that the segmentation of synthesised images in different styles should be consistent. Specifically, our model consists of an image translation module and a domain-specific segmentation module. The image translation module is a standard CycleGAN, while the segmentation module contains two domain-specific segmentation networks. The intra-modality semantic consistency (IMSC) forces the reconstructed image after a cycle to be segmented in the same way as the original input image, while the cross-modality semantic consistency (CMSC) encourages the synthesized images after translation to be segmented exactly the same as before translation. Comprehensive experimental results on cross-modality hip joint bone segmentation show the effectiveness of our proposed method, which achieves an average DICE of 81.61% on the acetabulum and 88.16% on the proximal femur, outperforming other state-of-the-art methods. It is worth to note that without UDA, a model trained on CT for hip joint bone segmentation is non-transferable to MRI and has almost zero-DICE segmentation.

</details>

<details>

<summary>2020-12-23 10:34:38 - Stability in Abstract Argumentation</summary>

- *Jean-Guy Mailly, Julien Rossit*

- `2012.12588v1` - [abs](http://arxiv.org/abs/2012.12588v1) - [pdf](http://arxiv.org/pdf/2012.12588v1)

> The notion of stability in a structured argumentation setup characterizes situations where the acceptance status associated with a given literal will not be impacted by any future evolution of this setup. In this paper, we abstract away from the logical structure of arguments, and we transpose this notion of stability to the context of Dungean argumentation frameworks. In particular, we show how this problem can be translated into reasoning with Argument-Incomplete AFs. Then we provide preliminary complexity results for stability under four prominent semantics, in the case of both credulous and skeptical reasoning. Finally, we illustrate to what extent this notion can be useful with an application to argument-based negotiation.

</details>

<details>

<summary>2020-12-23 15:03:21 - Facebook Ad Engagement in the Russian Active Measures Campaign of 2016</summary>

- *Mirela Silva, Luiz Giovanini, Juliana Fernandes, Daniela Oliveira, Catia S. Silva*

- `2012.11690v2` - [abs](http://arxiv.org/abs/2012.11690v2) - [pdf](http://arxiv.org/pdf/2012.11690v2)

> This paper examines 3,517 Facebook ads created by Russia's Internet Research Agency (IRA) between June 2015 and August 2017 in its active measures disinformation campaign targeting the 2016 U.S. general election. We aimed to unearth the relationship between ad engagement (as measured by ad clicks) and 41 features related to ads' metadata, sociolinguistic structures, and sentiment. Our analysis was three-fold: (i) understand the relationship between engagement and features via correlation analysis; (ii) find the most relevant feature subsets to predict engagement via feature selection; and (iii) find the semantic topics that best characterize the dataset via topic modeling. We found that ad expenditure, text size, ad lifetime, and sentiment were the top features predicting users' engagement to the ads. Additionally, positive sentiment ads were more engaging than negative ads, and sociolinguistic features (e.g., use of religion-relevant words) were identified as highly important in the makeup of an engaging ad. Linear SVM and Logistic Regression classifiers achieved the highest mean F-scores (93.6% for both models), determining that the optimal feature subset contains 12 and 6 features, respectively. Finally, we corroborate the findings of related works that the IRA specifically targeted Americans on divisive ad topics (e.g., LGBT rights, African American reparations).

</details>

<details>

<summary>2020-12-23 22:54:43 - Low-latency Perception in Off-Road Dynamical Low Visibility Environments</summary>

- *Nelson Alves, Marco Ruiz, Marco Reis, Tiago Cajahyba, Davi Oliveira, Ana Barreto, Eduardo F. Simas Filho, Wagner L. A. de Oliveira, Leizer Schnitman, Roberto L. S. Monteiro*

- `2012.13014v1` - [abs](http://arxiv.org/abs/2012.13014v1) - [pdf](http://arxiv.org/pdf/2012.13014v1)

> This work proposes a perception system for autonomous vehicles and advanced driver assistance specialized on unpaved roads and off-road environments. In this research, the authors have investigated the behavior of Deep Learning algorithms applied to semantic segmentation of off-road environments and unpaved roads under differents adverse conditions of visibility. Almost 12,000 images of different unpaved and off-road environments were collected and labeled. It was assembled an off-road proving ground exclusively for its development. The proposed dataset also contains many adverse situations such as rain, dust, and low light. To develop the system, we have used convolutional neural networks trained to segment obstacles and areas where the car can pass through. We developed a Configurable Modular Segmentation Network (CMSNet) framework to help create different architectures arrangements and test them on the proposed dataset. Besides, we also have ported some CMSNet configurations by removing and fusing many layers using TensorRT, C++, and CUDA to achieve embedded real-time inference and allow field tests. The main contributions of this work are: a new dataset for unpaved roads and off-roads environments containing many adverse conditions such as night, rain, and dust; a CMSNet framework; an investigation regarding the feasibility of applying deep learning to detect region where the vehicle can pass through when there is no clear boundary of the track; a study of how our proposed segmentation algorithms behave in different severity levels of visibility impairment; and an evaluation of field tests carried out with semantic segmentation architectures ported for real-time inference.

</details>

<details>

<summary>2020-12-24 01:27:31 - A Generalized A* Algorithm for Finding Globally Optimal Paths in Weighted Colored Graphs</summary>

- *Jaein Lim, Panagiotis Tsiotras*

- `2012.13057v1` - [abs](http://arxiv.org/abs/2012.13057v1) - [pdf](http://arxiv.org/pdf/2012.13057v1)

> Both geometric and semantic information of the search space is imperative for a good plan. We encode those properties in a weighted colored graph (geometric information in terms of edge weight and semantic information in terms of edge and vertex color), and propose a generalized A* to find the shortest path among the set of paths with minimal inclusion of low-ranked color edges. We prove the completeness and optimality of this Class-Ordered A* (COA*) algorithm with respect to the hereto defined notion of optimality. The utility of COA* is numerically validated in a ternary graph with feasible, infeasible, and unknown vertices and edges for the cases of a 2D mobile robot, a 3D robotic arm, and a 5D robotic arm with limited sensing capabilities. We compare the results of COA* to that of the regular A* algorithm, the latter of which finds the shortest path regardless of uncertainty, and we show that the COA* dominates the A* solution in terms of finding less uncertain paths.

</details>

<details>

<summary>2020-12-24 04:59:45 - THUIR@COLIEE-2020: Leveraging Semantic Understanding and Exact Matching for Legal Case Retrieval and Entailment</summary>

- *Yunqiu Shao, Bulou Liu, Jiaxin Mao, Yiqun Liu, Min Zhang, Shaoping Ma*

- `2012.13102v1` - [abs](http://arxiv.org/abs/2012.13102v1) - [pdf](http://arxiv.org/pdf/2012.13102v1)

> In this paper, we present our methodologies for tackling the challenges of legal case retrieval and entailment in the Competition on Legal Information Extraction / Entailment 2020 (COLIEE-2020). We participated in the two case law tasks, i.e., the legal case retrieval task and the legal case entailment task. Task 1 (the retrieval task) aims to automatically identify supporting cases from the case law corpus given a new case, and Task 2 (the entailment task) to identify specific paragraphs that entail the decision of a new case in a relevant case. In both tasks, we employed the neural models for semantic understanding and the traditional retrieval models for exact matching. As a result, our team (TLIR) ranked 2nd among all of the teams in Task 1 and 3rd among teams in Task 2. Experimental results suggest that combing models of semantic understanding and exact matching benefits the legal case retrieval task while the legal case entailment task relies more on semantic understanding.

</details>

<details>

<summary>2020-12-24 17:24:54 - A Context Aware Approach for Generating Natural Language Attacks</summary>

- *Rishabh Maheshwary, Saket Maheshwary, Vikram Pudi*

- `2012.13339v1` - [abs](http://arxiv.org/abs/2012.13339v1) - [pdf](http://arxiv.org/pdf/2012.13339v1)

> We study an important task of attacking natural language processing models in a black box setting. We propose an attack strategy that crafts semantically similar adversarial examples on text classification and entailment tasks. Our proposed attack finds candidate words by considering the information of both the original word and its surrounding context. It jointly leverages masked language modelling and next sentence prediction for context understanding. In comparison to attacks proposed in prior literature, we are able to generate high quality adversarial examples that do significantly better both in terms of success rate and word perturbation percentage.

</details>

<details>

<summary>2020-12-25 05:15:51 - Manhattan Room Layout Reconstruction from a Single 360 image: A Comparative Study of State-of-the-art Methods</summary>

- *Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu, Derek Hoiem*

- `1910.04099v3` - [abs](http://arxiv.org/abs/1910.04099v3) - [pdf](http://arxiv.org/pdf/1910.04099v3)

> Recent approaches for predicting layouts from 360 panoramas produce excellent results. These approaches build on a common framework consisting of three steps: a pre-processing step based on edge-based alignment, prediction of layout elements, and a post-processing step by fitting a 3D layout to the layout elements. Until now, it has been difficult to compare the methods due to multiple different design decisions, such as the encoding network (e.g. SegNet or ResNet), type of elements predicted (e.g. corners, wall/floor boundaries, or semantic segmentation), or method of fitting the 3D layout. To address this challenge, we summarize and describe the common framework, the variants, and the impact of the design decisions. For a complete evaluation, we also propose extended annotations for the Matterport3D dataset [3], and introduce two depth-based evaluation metrics.

</details>

<details>

<summary>2020-12-25 06:36:11 - Brain-inspired Search Engine Assistant based on Knowledge Graph</summary>

- *Xuejiao Zhao, Huanhuan Chen, Zhenchang Xing, Chunyan Miao*

- `2012.13529v1` - [abs](http://arxiv.org/abs/2012.13529v1) - [pdf](http://arxiv.org/pdf/2012.13529v1)

> Search engines can quickly response a hyperlink list according to query keywords. However, when a query is complex, developers need to repeatedly refine the search keywords and open a large number of web pages to find and summarize answers. Many research works of question and answering (Q and A) system attempt to assist search engines by providing simple, accurate and understandable answers. However, without original semantic contexts, these answers lack explainability, making them difficult for users to trust and adopt. In this paper, a brain-inspired search engine assistant named DeveloperBot based on knowledge graph is proposed, which aligns to the cognitive process of human and has the capacity to answer complex queries with explainability. Specifically, DeveloperBot firstly constructs a multi-layer query graph by splitting a complex multi-constraint query into several ordered constraints. Then it models the constraint reasoning process as subgraph search process inspired by the spreading activation model of cognitive science. In the end, novel features of the subgraph will be extracted for decision-making. The corresponding reasoning subgraph and answer confidence will be derived as explanations. The results of the decision-making demonstrate that DeveloperBot can estimate the answers and answer confidences with high accuracy. We implement a prototype and conduct a user study to evaluate whether and how the direct answers and the explanations provided by DeveloperBot can assist developers' information needs.

</details>

<details>

<summary>2020-12-25 14:40:54 - Generating Semantically Valid Adversarial Questions for TableQA</summary>

- *Yi Zhu, Yiwei Zhou, Menglin Xia*

- `2005.12696v3` - [abs](http://arxiv.org/abs/2005.12696v3) - [pdf](http://arxiv.org/pdf/2005.12696v3)

> Adversarial attack on question answering systems over tabular data (TableQA) can help evaluate to what extent they can understand natural language questions and reason with tables. However, generating natural language adversarial questions is difficult, because even a single character swap could lead to huge semantic difference in human perception. In this paper, we propose SAGE (Semantically valid Adversarial GEnerator), a Wasserstein sequence-to-sequence model for TableQA white-box attack. To preserve meaning of original questions, we apply minimum risk training with SIMILE and entity delexicalization. We use Gumbel-Softmax to incorporate adversarial loss for end-to-end training. Our experiments show that SAGE outperforms existing local attack models on semantic validity and fluency while achieving a good attack success rate. Finally, we demonstrate that adversarial training with SAGE augmented data can improve performance and robustness of TableQA systems.

</details>

<details>

<summary>2020-12-25 21:02:37 - Toward a Knowledge-based Personalised Recommender System for Mobile App Development</summary>

- *Bilal Abu-Salih, Hamad Alsawalqah, Basima Elshqeirat, Tomayess Issa, Pornpit Wongthongtham*

- `1909.03733v3` - [abs](http://arxiv.org/abs/1909.03733v3) - [pdf](http://arxiv.org/pdf/1909.03733v3)

> Over the last few years, the arena of mobile application development has expanded considerably beyond the balance of the world\'s software markets. With the growing number of mobile software companies, and the mounting sophistication of smartphones\' technology, developers have been building several categories of applications on dissimilar platforms. However, developers confront several challenges through the implementation of mobile application projects. In particular, there is a lack of consolidated systems that are competent to provide developers with personalised services promptly and efficiently. Hence, it is essential to develop tailored systems which can recommend appropriate tools, IDEs, platforms, software components and other correlated artifacts to mobile application developers. This paper proposes a new recommender system framework comprising a fortified set of techniques that are designed to provide mobile app developers with a distinctive platform to browse and search for the personalised artifacts. The proposed system make use of ontology and semantic web technology as well as machine learning techniques. In particular, the new RS framework comprises the following components; (i) domain knowledge inference module: including various semantic web technologies and lightweight ontologies; (ii) profiling and preferencing: a new proposed time-aware multidimensional user modelling; (iii) query expansion: to improve and enhance the retrieved results by semantically augmenting users\' query; and (iv) recommendation and information filtration: to make use of the aforementioned components to provide personalised services to the designated users and to answer a user\'s query with the minimum mismatches.

</details>

<details>

<summary>2020-12-26 08:21:42 - Entity Recognition and Relation Extraction from Scientific and Technical Texts in Russian</summary>

- *Elena Bruches, Alexey Pauls, Tatiana Batura, Vladimir Isachenko*

- `2011.09817v3` - [abs](http://arxiv.org/abs/2011.09817v3) - [pdf](http://arxiv.org/pdf/2011.09817v3)

> This paper is devoted to the study of methods for information extraction (entity recognition and relation classification) from scientific texts on information technology. Scientific publications provide valuable information into cutting-edge scientific advances, but efficient processing of increasing amounts of data is a time-consuming task. In this paper, several modifications of methods for the Russian language are proposed. It also includes the results of experiments comparing a keyword extraction method, vocabulary method, and some methods based on neural networks. Text collections for these tasks exist for the English language and are actively used by the scientific community, but at present, such datasets in Russian are not publicly available. In this paper, we present a corpus of scientific texts in Russian, RuSERRC. This dataset consists of 1600 unlabeled documents and 80 labeled with entities and semantic relations (6 relation types were considered). The dataset and models are available at https://github.com/iis-research-team. We hope they can be useful for research purposes and development of information extraction systems.

</details>

<details>

<summary>2020-12-27 02:43:08 - An Action Language for Multi-Agent Domains: Foundations</summary>

- *Chitta Baral, Gregory Gelfond, Enrico Pontelli, Tran Cao Son*

- `1511.01960v3` - [abs](http://arxiv.org/abs/1511.01960v3) - [pdf](http://arxiv.org/pdf/1511.01960v3)

> In multi-agent domains (MADs), an agent's action may not just change the world and the agent's knowledge and beliefs about the world, but also may change other agents' knowledge and beliefs about the world and their knowledge and beliefs about other agents' knowledge and beliefs about the world. The goals of an agent in a multi-agent world may involve manipulating the knowledge and beliefs of other agents' and again, not just their knowledge/belief about the world, but also their knowledge about other agents' knowledge about the world. Our goal is to present an action language (mA+) that has the necessary features to address the above aspects in representing and RAC in MADs. mA+ allows the representation of and reasoning about different types of actions that an agent can perform in a domain where many other agents might be present -- such as world-altering actions, sensing actions, and announcement/communication actions. It also allows the specification of agents' dynamic awareness of action occurrences which has future implications on what agents' know about the world and other agents' knowledge about the world. mA+ considers three different types of awareness: full-, partial- awareness, and complete oblivion of an action occurrence and its effects. This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios in MADs. The semantics of mA+ relies on the notion of state, which is described by a pointed Kripke model and is used to encode the agent's knowledge and the real state of the world. It is defined by a transition function that maps pairs of actions and states into sets of states. We illustrate properties of the action theories, including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally, we relate mA+ to other related formalisms that contribute to RAC in MADs.

</details>

<details>

<summary>2020-12-27 06:13:29 - Multi-Channel Sequential Behavior Networks for User Modeling in Online Advertising</summary>

- *Iyad Batal, Akshay Soni*

- `2012.15728v1` - [abs](http://arxiv.org/abs/2012.15728v1) - [pdf](http://arxiv.org/pdf/2012.15728v1)

> Multiple content providers rely on native advertisement for revenue by placing ads within the organic content of their pages. We refer to this setting as ``queryless'' to differentiate from search advertisement where a user submits a search query and gets back related ads. Understanding user intent is critical because relevant ads improve user experience and increase the likelihood of delivering clicks that have value to our advertisers.   This paper presents Multi-Channel Sequential Behavior Network (MC-SBN), a deep learning approach for embedding users and ads in a semantic space in which relevance can be evaluated. Our proposed user encoder architecture summarizes user activities from multiple input channels--such as previous search queries, visited pages, or clicked ads--into a user vector. It uses multiple RNNs to encode sequences of event sessions from the different channels and then applies an attention mechanism to create the user representation. A key property of our approach is that user vectors can be maintained and updated incrementally, which makes it feasible to be deployed for large-scale serving. We conduct extensive experiments on real-world datasets. The results demonstrate that MC-SBN can improve the ranking of relevant ads and boost the performance of both click prediction and conversion prediction in the queryless native advertising setting.

</details>

<details>

<summary>2020-12-27 06:19:20 - My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism</summary>

- *Swapnil Parekh, Yaman Kumar Singla, Changyou Chen, Junyi Jessy Li, Rajiv Ratn Shah*

- `2012.13872v1` - [abs](http://arxiv.org/abs/2012.13872v1) - [pdf](http://arxiv.org/pdf/2012.13872v1)

> Significant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. However, little research has been put to understand and interpret the black-box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capability raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi-modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. We utilize recent advances in interpretability to find the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. We find that the systems tested consider essays not as a piece of prose having the characteristics of natural flow of speech and grammatical structure, but as `word-soups' where a few words are much more important than the other words. Removing the context surrounding those few important words causes the prose to lose the flow of speech and grammar, however has little impact on the predicted score. We also find that since the models are not semantically grounded with world-knowledge and common sense, adding false facts such as ``the world is flat'' actually increases the score instead of decreasing it.

</details>

<details>

<summary>2020-12-27 13:26:11 - Adaptive Convolution for Semantic Role Labeling</summary>

- *Kashif Munir, Hai Zhao, Zuchao Li*

- `2012.13939v1` - [abs](http://arxiv.org/abs/2012.13939v1) - [pdf](http://arxiv.org/pdf/2012.13939v1)

> Semantic role labeling (SRL) aims at elaborating the meaning of a sentence by forming a predicate-argument structure. Recent researches depicted that the effective use of syntax can improve SRL performance. However, syntax is a complicated linguistic clue and is hard to be effectively applied in a downstream task like SRL. This work effectively encodes syntax using adaptive convolution which endows strong flexibility to existing convolutional networks. The existing CNNs may help in encoding a complicated structure like syntax for SRL, but it still has shortcomings. Contrary to traditional convolutional networks that use same filters for different inputs, adaptive convolution uses adaptively generated filters conditioned on syntactically informed inputs. We achieve this with the integration of a filter generation network which generates the input specific filters. This helps the model to focus on important syntactic features present inside the input, thus enlarging the gap between syntax-aware and syntax-agnostic SRL systems. We further study a hashing technique to compress the size of the filter generation network for SRL in terms of trainable parameters. Experiments on CoNLL-2009 dataset confirm that the proposed model substantially outperforms most previous SRL systems for both English and Chinese languages

</details>

<details>

<summary>2020-12-28 13:56:13 - Multi-span Style Extraction for Generative Reading Comprehension</summary>

- *Junjie Yang, Zhuosheng Zhang, Hai Zhao*

- `2009.07382v2` - [abs](http://arxiv.org/abs/2009.07382v2) - [pdf](http://arxiv.org/pdf/2009.07382v2)

> Generative machine reading comprehension (MRC) requires a model to generate well-formed answers. For this type of MRC, answer generation method is crucial to the model performance. However, generative models, which are supposed to be the right model for the task, in generally perform poorly. At the same time, single-span extraction models have been proven effective for extractive MRC, where the answer is constrained to a single span in the passage. Nevertheless, they generally suffer from generating incomplete answers or introducing redundant words when applied to the generative MRC. Thus, we extend the single-span extraction method to multi-span, proposing a new framework which enables generative MRC to be smoothly solved as multi-span extraction. Thorough experiments demonstrate that this novel approach can alleviate the dilemma between generative models and single-span models and produce answers with better-formed syntax and semantics.

</details>

<details>

<summary>2020-12-28 16:55:19 - Commonsense Visual Sensemaking for Autonomous Driving: On Generalised Neurosymbolic Online Abduction Integrating Vision and Semantics</summary>

- *Jakob Suchan, Mehul Bhatt, Srikrishna Varadarajan*

- `2012.14359v1` - [abs](http://arxiv.org/abs/2012.14359v1) - [pdf](http://arxiv.org/pdf/2012.14359v1)

> We demonstrate the need and potential of systematically integrated vision and semantics solutions for visual sensemaking in the backdrop of autonomous driving. A general neurosymbolic method for online visual sensemaking using answer set programming (ASP) is systematically formalised and fully implemented. The method integrates state of the art in visual computing, and is developed as a modular framework that is generally usable within hybrid architectures for realtime perception and control. We evaluate and demonstrate with community established benchmarks KITTIMOD, MOT-2017, and MOT-2020. As use-case, we focus on the significance of human-centred visual sensemaking -- e.g., involving semantic representation and explainability, question-answering, commonsense interpolation -- in safety-critical autonomous driving situations. The developed neurosymbolic framework is domain-independent, with the case of autonomous driving designed to serve as an exemplar for online visual sensemaking in diverse cognitive interaction settings in the backdrop of select human-centred AI technology design considerations.   Keywords: Cognitive Vision, Deep Semantics, Declarative Spatial Reasoning, Knowledge Representation and Reasoning, Commonsense Reasoning, Visual Abduction, Answer Set Programming, Autonomous Driving, Human-Centred Computing and Design, Standardisation in Driving Technology, Spatial Cognition and AI.

</details>

<details>

<summary>2020-12-28 18:48:35 - Disentangling semantics in language through VAEs and a certain architectural choice</summary>

- *Ghazi Felhi, Joseph Le Roux, Djamé Seddah*

- `2012.13031v2` - [abs](http://arxiv.org/abs/2012.13031v2) - [pdf](http://arxiv.org/pdf/2012.13031v2)

> We present an unsupervised method to obtain disentangled representations of sentences that single out semantic content. Using modified Transformers as building blocks, we train a Variational Autoencoder to translate the sentence to a fixed number of hierarchically structured latent variables. We study the influence of each latent variable in generation on the dependency structure of sentences, and on the predicate structure it yields when passed through an Open Information Extraction model. Our model could separate verbs, subjects, direct objects, and prepositional objects into latent variables we identified. We show that varying the corresponding latent variables results in varying these elements in sentences, and that swapping them between couples of sentences leads to the expected partial semantic swap.

</details>

<details>

<summary>2020-12-29 01:09:57 - On Deep Learning Techniques to Boost Monocular Depth Estimation for Autonomous Navigation</summary>

- *Raul de Queiroz Mendes, Eduardo Godinho Ribeiro, Nicolas dos Santos Rosa, Valdir Grassi Jr*

- `2010.06626v2` - [abs](http://arxiv.org/abs/2010.06626v2) - [pdf](http://arxiv.org/pdf/2010.06626v2)

> Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.

</details>

<details>

<summary>2020-12-29 02:36:19 - Formal Statement of the Decision-making Support Problem in the Management of Municipal Social and Economic Development</summary>

- *Anatoly Sidorov, Maria Shishanina*

- `2012.14573v1` - [abs](http://arxiv.org/abs/2012.14573v1) - [pdf](http://arxiv.org/pdf/2012.14573v1)

> This article deals with the process of managing the social and economic development of municipal formations. It highlights characteristics and key issues that arise during management at the municipal level. In order to minimize the impact of the described issues, it is suggested to consider municipal social and economic development as a semistructured system which is modelled using a semantic network. As a result, it is concluded that a rating of indicators for assessing social and economic development needs to be created in order to determine the effectiveness and correlation with the targeted indicators.

</details>

<details>

<summary>2020-12-29 08:01:50 - DEGAS: Differentiable Efficient Generator Search</summary>

- *Sivan Doveh, Raja Giryes*

- `1912.00606v3` - [abs](http://arxiv.org/abs/1912.00606v3) - [pdf](http://arxiv.org/pdf/1912.00606v3)

> Network architecture search (NAS) achieves state-of-the-art results in various tasks such as classification and semantic segmentation. Recently, a reinforcement learning-based approach has been proposed for Generative Adversarial Networks (GANs) search. In this work, we propose an alternative strategy for GAN search by using a method called DEGAS (Differentiable Efficient GenerAtor Search), which focuses on efficiently finding the generator in the GAN. Our search algorithm is inspired by the differential architecture search strategy and the Global Latent Optimization (GLO) procedure. This leads to both an efficient and stable GAN search. After the generator architecture is found, it can be plugged into any existing framework for GAN training. For CTGAN, which we use in this work, the new model outperforms the original inception score results by 0.25 for CIFAR-10 and 0.77 for STL. It also gets better results than the RL based GAN search methods in shorter search time.

</details>

<details>

<summary>2020-12-29 14:28:07 - Generating Adversarial Examples in Chinese Texts Using Sentence-Pieces</summary>

- *Linyang Li, Yunfan Shao, Demin Song, Xipeng Qiu, Xuanjing Huang*

- `2012.14769v1` - [abs](http://arxiv.org/abs/2012.14769v1) - [pdf](http://arxiv.org/pdf/2012.14769v1)

> Adversarial attacks in texts are mostly substitution-based methods that replace words or characters in the original texts to achieve success attacks. Recent methods use pre-trained language models as the substitutes generator. While in Chinese, such methods are not applicable since words in Chinese require segmentations first. In this paper, we propose a pre-train language model as the substitutes generator using sentence-pieces to craft adversarial examples in Chinese. The substitutions in the generated adversarial examples are not characters or words but \textit{'pieces'}, which are more natural to Chinese readers. Experiments results show that the generated adversarial samples can mislead strong target models and remain fluent and semantically preserved.

</details>

<details>

<summary>2020-12-29 14:47:35 - A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation</summary>

- *Jiangnan Li, Zheng Lin, Peng Fu, Qingyi Si, Weiping Wang*

- `2012.14781v1` - [abs](http://arxiv.org/abs/2012.14781v1) - [pdf](http://arxiv.org/pdf/2012.14781v1)

> Emotion Recognition in Conversation (ERC) is a more challenging task than conventional text emotion recognition. It can be regarded as a personalized and interactive emotion recognition task, which is supposed to consider not only the semantic information of text but also the influences from speakers. The current method models speakers' interactions by building a relation between every two speakers. However, this fine-grained but complicated modeling is computationally expensive, hard to extend, and can only consider local context. To address this problem, we simplify the complicated modeling to a binary version: Intra-Speaker and Inter-Speaker dependencies, without identifying every unique speaker for the targeted speaker. To better achieve the simplified interaction modeling of speakers in Transformer, which shows excellent ability to settle long-distance dependency, we design three types of masks and respectively utilize them in three independent Transformer blocks. The designed masks respectively model the conventional context modeling, Intra-Speaker dependency, and Inter-Speaker dependency. Furthermore, different speaker-aware information extracted by Transformer blocks diversely contributes to the prediction, and therefore we utilize the attention mechanism to automatically weight them. Experiments on two ERC datasets indicate that our model is efficacious to achieve better performance.

</details>

<details>

<summary>2020-12-29 16:36:49 - DRS at MRP 2020: Dressing up Discourse Representation Structures as Graphs</summary>

- *Lasha Abzianidze, Johan Bos, Stephan Oepen*

- `2012.14837v1` - [abs](http://arxiv.org/abs/2012.14837v1) - [pdf](http://arxiv.org/pdf/2012.14837v1)

> Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.

</details>

<details>

<summary>2020-12-29 17:04:10 - The Parallel Meaning Bank: A Framework for Semantically Annotating Multiple Languages</summary>

- *Lasha Abzianidze, Rik van Noord, Chunliu Wang, Johan Bos*

- `2012.14854v1` - [abs](http://arxiv.org/abs/2012.14854v1) - [pdf](http://arxiv.org/pdf/2012.14854v1)

> This paper gives a general description of the ideas behind the Parallel Meaning Bank, a framework with the aim to provide an easy way to annotate compositional semantics for texts written in languages other than English. The annotation procedure is semi-automatic, and comprises seven layers of linguistic information: segmentation, symbolisation, semantic tagging, word sense disambiguation, syntactic structure, thematic role labelling, and co-reference. New languages can be added to the meaning bank as long as the documents are based on translations from English, but also introduce new interesting challenges on the linguistics assumptions underlying the Parallel Meaning Bank.

</details>

<details>

<summary>2020-12-30 03:50:16 - Word Embedding based New Corpus for Low-resourced Language: Sindhi</summary>

- *Wazir Ali, Jay Kumar, Junyu Lu, Zenglin Xu*

- `1911.12579v3` - [abs](http://arxiv.org/abs/1911.12579v3) - [pdf](http://arxiv.org/pdf/1911.12579v3)

> Representing words and phrases into dense vectors of real numbers which encode semantic and syntactic properties is a vital constituent in natural language processing (NLP). The success of neural network (NN) models in NLP largely rely on such dense word representations learned on the large unlabeled corpus. Sindhi is one of the rich morphological language, spoken by large population in Pakistan and India lacks corpora which plays an essential role of a test-bed for generating word embeddings and developing language independent NLP systems. In this paper, a large corpus of more than 61 million words is developed for low-resourced Sindhi language for training neural word embeddings. The corpus is acquired from multiple web-resources using web-scrappy. Due to the unavailability of open source preprocessing tools for Sindhi, the prepossessing of such large corpus becomes a challenging problem specially cleaning of noisy data extracted from web resources. Therefore, a preprocessing pipeline is employed for the filtration of noisy text. Afterwards, the cleaned vocabulary is utilized for training Sindhi word embeddings with state-of-the-art GloVe, Skip-Gram (SG), and Continuous Bag of Words (CBoW) word2vec algorithms. The intrinsic evaluation approach of cosine similarity matrix and WordSim-353 are employed for the evaluation of generated Sindhi word embeddings. Moreover, we compare the proposed word embeddings with recently revealed Sindhi fastText (SdfastText) word representations. Our intrinsic evaluation results demonstrate the high quality of our generated Sindhi word embeddings using SG, CBoW, and GloVe as compare to SdfastText word representations.

</details>

<details>

<summary>2020-12-30 07:49:00 - Enhancing Pre-trained Language Model with Lexical Simplification</summary>

- *Rongzhou Bao, Jiayi Wang, Zhuosheng Zhang, Hai Zhao*

- `2012.15070v1` - [abs](http://arxiv.org/abs/2012.15070v1) - [pdf](http://arxiv.org/pdf/2012.15070v1)

> For both human readers and pre-trained language models (PrLMs), lexical diversity may lead to confusion and inaccuracy when understanding the underlying semantic meanings of given sentences. By substituting complex words with simple alternatives, lexical simplification (LS) is a recognized method to reduce such lexical diversity, and therefore to improve the understandability of sentences. In this paper, we leverage LS and propose a novel approach which can effectively improve the performance of PrLMs in text classification. A rule-based simplification process is applied to a given sentence. PrLMs are encouraged to predict the real label of the given sentence with auxiliary inputs from the simplified version. Using strong PrLMs (BERT and ELECTRA) as baselines, our approach can still further improve the performance in various text classification tasks.

</details>

<details>

<summary>2020-12-30 19:28:53 - MRI brain tumor segmentation and uncertainty estimation using 3D-UNet architectures</summary>

- *Laura Mora Ballestar, Veronica Vilaplana*

- `2012.15294v1` - [abs](http://arxiv.org/abs/2012.15294v1) - [pdf](http://arxiv.org/pdf/2012.15294v1)

> Automation of brain tumor segmentation in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task. However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is especially critical in medical diagnosis. This work studies 3D encoder-decoder architectures trained with patch-based techniques to reduce memory consumption and decrease the effect of unbalanced data. The different trained models are then used to create an ensemble that leverages the properties of each model, thus increasing the performance. We also introduce voxel-wise uncertainty information, both epistemic and aleatoric using test-time dropout (TTD) and data-augmentation (TTA) respectively. In addition, a hybrid approach is proposed that helps increase the accuracy of the segmentation. The model and uncertainty estimation measurements proposed in this work have been used in the BraTS'20 Challenge for task 1 and 3 regarding tumor segmentation and uncertainty estimation.

</details>

<details>

<summary>2020-12-30 22:17:15 - ConfigFix: Interactive Configuration Conflict Resolution for the Linux Kernel</summary>

- *Patrick Franz, Thorsten Berger, Ibrahim Fayaz, Sarah Nadi, Evgeny Groshev*

- `2012.15342v1` - [abs](http://arxiv.org/abs/2012.15342v1) - [pdf](http://arxiv.org/pdf/2012.15342v1)

> Highly configurable systems are highly complex systems, with the Linux kernel arguably being one of the most well-known ones. Since 2007, it has been a frequent target of the research community, conducting empirical studies and building dedicated methods and tools for analyzing, configuring, testing, optimizing, and maintaining the kernel in the light of its vast configuration space. However, despite a large body of work, mainly bug fixes that were the result of such research made it back into the kernel's source tree. Unfortunately, Linux users still struggle with kernel configuration and resolving configuration conflicts, since the kernel largely lacks automated support. Additionally, there are technical and community requirements for supporting automated conflict resolution in the kernel, such as, for example, using a pure C-based solution that uses only compatible third-party libraries (if any). With the aim of contributing back to the Linux community, we present CONFIGFIX, a tooling that we integrated with the kernel configurator, that is purely implemented in C, and that is finally a working solution able to produce fixes for configuration conflicts. In this experience report, we describe our experiences ranging over a decade of building upon the large body of work from research on the Linux kernel configuration mechanisms as well as how we designed and realized CONFIGFIX while adhering to the Linux kernel's community requirements and standards. While CONFIGFIX helps Linux kernel users obtaining their desired configuration, the sound semantic abstraction we implement provides the basis for many of the above techniques supporting kernel configuration, helping researchers and kernel developers.

</details>

<details>

<summary>2020-12-30 22:52:29 - Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings</summary>

- *Jacob Turton, David Vinson, Robert Elliott Smith*

- `2012.15353v1` - [abs](http://arxiv.org/abs/2012.15353v1) - [pdf](http://arxiv.org/pdf/2012.15353v1)

> Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing. Importantly, they allow the creation of word embeddings that capture important semantic information about words in context. However, as single entities, these embeddings are difficult to interpret and the models used to create them have been described as opaque. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. Unfortunately, the space only exists for a small dataset of 535 words, limiting its uses. Previous work (Utsumi, 2018, 2020, Turton, Vinson & Smith, 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. This provides contextualised Binder embeddings, which can aid in understanding semantic differences between words in context. It additionally provides insights into how semantic features are represented across the different layers of the BERT model.

</details>

<details>

<summary>2020-12-31 01:02:40 - Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing</summary>

- *Xi Victoria Lin, Richard Socher, Caiming Xiong*

- `2012.12627v2` - [abs](http://arxiv.org/abs/2012.12627v2) - [pdf](http://arxiv.org/pdf/2012.12627v2)

> We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset of the fields are augmented with cell values mentioned in the question. The hybrid sequence is encoded by BERT with minimal subsequent layers and the text-DB contextualization is realized via the fine-tuned deep attention in BERT. Combined with a pointer-generator decoder with schema-consistency driven search space pruning, BRIDGE attained state-of-the-art performance on popular cross-DB text-to-SQL benchmarks, Spider (71.1\% dev, 67.5\% test with ensemble model) and WikiSQL (92.6\% dev, 91.9\% test). Our analysis shows that BRIDGE effectively captures the desired cross-modal dependencies and has the potential to generalize to more text-DB related tasks. Our implementation is available at \url{https://github.com/salesforce/TabularSemanticParsing}.

</details>

<details>

<summary>2020-12-31 03:24:34 - Verb Knowledge Injection for Multilingual Event Processing</summary>

- *Olga Majewska, Ivan Vulić, Goran Glavaš, Edoardo M. Ponti, Anna Korhonen*

- `2012.15421v1` - [abs](http://arxiv.org/abs/2012.15421v1) - [pdf](http://arxiv.org/pdf/2012.15421v1)

> In parallel to their overwhelming success across NLP tasks, language ability of deep Transformer networks, pretrained via language modeling (LM) objectives has undergone extensive scrutiny. While probing revealed that these models encode a range of syntactic and semantic properties of a language, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic knowledge. In this paper, we target one such area of their deficiency, verbal reasoning. We investigate whether injecting explicit information on verbs' semantic-syntactic behaviour improves the performance of LM-pretrained Transformers in event extraction tasks -- downstream tasks for which accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (dubbed verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages: we investigate (1) zero-shot language transfer with multilingual Transformers as well as (2) transfer via (noisy automatic) translation of English verb-based lexical constraints. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when verb adapters are trained on noisily translated constraints.

</details>

<details>

<summary>2020-12-31 05:28:38 - Text-Free Image-to-Speech Synthesis Using Learned Segmental Units</summary>

- *Wei-Ning Hsu, David Harwath, Christopher Song, James Glass*

- `2012.15454v1` - [abs](http://arxiv.org/abs/2012.15454v1) - [pdf](http://arxiv.org/pdf/2012.15454v1)

> In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.

</details>

<details>

<summary>2020-12-31 06:34:00 - Generalized Octave Convolutions for Learned Multi-Frequency Image Compression</summary>

- *Mohammad Akbari, Jie Liang, Jingning Han, Chengjie Tu*

- `2002.10032v3` - [abs](http://arxiv.org/abs/2002.10032v3) - [pdf](http://arxiv.org/pdf/2002.10032v3)

> Learned image compression has recently shown the potential to outperform the standard codecs. State-of-the-art rate-distortion (R-D) performance has been achieved by context-adaptive entropy coding approaches in which hyperprior and autoregressive models are jointly utilized to effectively capture the spatial dependencies in the latent representations. However, the latents are feature maps of the same spatial resolution in previous works, which contain some redundancies that affect the R-D performance. In this paper, we propose the first learned multi-frequency image compression and entropy coding approach that is based on the recently developed octave convolutions to factorize the latents into high and low frequency (resolution) components, where the low frequency is represented by a lower resolution. Therefore, its spatial redundancy is reduced, which improves the R-D performance. Novel generalized octave convolution and octave transposed-convolution architectures with internal activation layers are also proposed to preserve more spatial structure of the information. Experimental results show that the proposed scheme not only outperforms all existing learned methods as well as standard codecs such as the next-generation video coding standard VVC (4:2:0) on the Kodak dataset in both PSNR and MS-SSIM. We also show that the proposed generalized octave convolution can improve the performance of other auto-encoder-based computer vision tasks such as semantic segmentation and image denoising.

</details>

<details>

<summary>2020-12-31 10:58:37 - Discovering Dialog Structure Graph for Open-Domain Dialog Generation</summary>

- *Jun Xu, Zeyang Lei, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu*

- `2012.15543v1` - [abs](http://arxiv.org/abs/2012.15543v1) - [pdf](http://arxiv.org/pdf/2012.15543v1)

> Learning interpretable dialog structure from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. In this paper, we conduct unsupervised discovery of dialog structure from chitchat corpora, and then leverage it to facilitate dialog generation in downstream systems. To this end, we present a Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover a unified human-readable dialog structure. The structure is a two-layer directed graph that contains session-level semantics in the upper-layer vertices, utterance-level semantics in the lower-layer vertices, and edges among these semantic vertices. In particular, we integrate GNN into DVAE to fine-tune utterance-level semantics for more effective recognition of session-level semantic vertex. Furthermore, to alleviate the difficulty of discovering a large number of utterance-level semantics, we design a coupling mechanism that binds each utterance-level semantic vertex with a distinct phrase to provide prior semantics. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure, and the use of dialog structure graph as background knowledge can facilitate a graph grounded conversational system to conduct coherent multi-turn dialog generation.

</details>

<details>

<summary>2020-12-31 14:50:42 - Leveraging Audio Gestalt to Predict Media Memorability</summary>

- *Lorin Sweeney, Graham Healy, Alan F. Smeaton*

- `2012.15635v1` - [abs](http://arxiv.org/abs/2012.15635v1) - [pdf](http://arxiv.org/pdf/2012.15635v1)

> Memorability determines what evanesces into emptiness, and what worms its way into the deepest furrows of our minds. It is the key to curating more meaningful media content as we wade through daily digital torrents. The Predicting Media Memorability task in MediaEval 2020 aims to address the question of media memorability by setting the task of automatically predicting video memorability. Our approach is a multimodal deep learning-based late fusion that combines visual, semantic, and auditory features. We used audio gestalt to estimate the influence of the audio modality on overall video memorability, and accordingly inform which combination of features would best predict a given video's memorability scores.

</details>

<details>

<summary>2020-12-31 14:58:01 - TexSmart: A Text Understanding System for Fine-Grained NER and Enhanced Semantic Analysis</summary>

- *Haisong Zhang, Lemao Liu, Haiyun Jiang, Yangming Li, Enbo Zhao, Kun Xu, Linfeng Song, Suncong Zheng, Botong Zhou, Jianchen Zhu, Xiao Feng, Tao Chen, Tao Yang, Dong Yu, Feng Zhang, Zhanhui Kang, Shuming Shi*

- `2012.15639v1` - [abs](http://arxiv.org/abs/2012.15639v1) - [pdf](http://arxiv.org/pdf/2012.15639v1)

> This technique report introduces TexSmart, a text understanding system that supports fine-grained named entity recognition (NER) and enhanced semantic analysis functionalities. Compared to most previous publicly available text understanding systems and tools, TexSmart holds some unique features. First, the NER function of TexSmart supports over 1,000 entity types, while most other public tools typically support several to (at most) dozens of entity types. Second, TexSmart introduces new semantic analysis functions like semantic expansion and deep semantic representation, that are absent in most previous systems. Third, a spectrum of algorithms (from very fast algorithms to those that are relatively slow but more accurate) are implemented for one function in TexSmart, to fulfill the requirements of different academic and industrial applications. The adoption of unsupervised or weakly-supervised algorithms is especially emphasized, with the goal of easily updating our models to include fresh data with less human annotation efforts.   The main contents of this report include major functions of TexSmart, algorithms for achieving these functions, how to use the TexSmart toolkit and Web APIs, and evaluation results of some key algorithms.

</details>

<details>

<summary>2020-12-31 18:34:29 - UCCA's Foundational Layer: Annotation Guidelines v2.1</summary>

- *Omri Abend, Nathan Schneider, Dotan Dvir, Jakob Prange, Ari Rappoport*

- `2012.15810v1` - [abs](http://arxiv.org/abs/2012.15810v1) - [pdf](http://arxiv.org/pdf/2012.15810v1)

> This is the annotation manual for Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), specifically the Foundational Layer. UCCA is a graph-based semantic annotation scheme based on typological linguistic principles. It has been applied to several languages; for ease of exposition these guidelines give examples mainly in English. New annotators may wish to start with the tutorial on the UCCA framework (Abend et al., 2020). Further resources are available at the project homepage: https://universalconceptualcognitiveannotation.github.io

</details>

