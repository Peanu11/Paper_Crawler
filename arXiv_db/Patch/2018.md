# 2018

## TOC

- [2018-01](#2018-01)
- [2018-02](#2018-02)
- [2018-03](#2018-03)
- [2018-04](#2018-04)
- [2018-05](#2018-05)
- [2018-06](#2018-06)
- [2018-07](#2018-07)
- [2018-08](#2018-08)
- [2018-09](#2018-09)
- [2018-10](#2018-10)
- [2018-11](#2018-11)
- [2018-12](#2018-12)

## 2018-01

<details>

<summary>2018-01-05 09:37:18 - VulDeePecker: A Deep Learning-Based System for Vulnerability Detection</summary>

- *Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, Yuyi Zhong*

- `1801.01681v1` - [abs](http://arxiv.org/abs/1801.01681v1) - [pdf](http://arxiv.org/pdf/1801.01681v1)

> The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were "silently" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.

</details>

<details>

<summary>2018-01-11 21:38:21 - Implicit LOD using points ordering for processing and visualisation in Point Cloud Servers</summary>

- *Rémi Cura, Julien Perret, Nicolas Paparoditis*

- `1602.06920v3` - [abs](http://arxiv.org/abs/1602.06920v3) - [pdf](http://arxiv.org/pdf/1602.06920v3)

> Lidar datasets now commonly reach Billions of points and are very dense. Using these point cloud becomes challenging, as the high number of points is intractable for most applications and for visualisation.In this work we propose a new paradigm to easily get a portable geometric Level Of Details (LOD) inside a Point Cloud Server.The main idea is to not store the LOD information in an external additional file, but instead to store it implicitly by exploiting the order of the points.The point cloud is divided into groups (patches). These patches are ordered so that their order gradually provides more and more details on the patch. We demonstrate the interest of our method with several classical uses of LOD, such as visualisation of massive point cloud, algorithm acceleration, fast density peak detection and correction.

</details>

<details>

<summary>2018-01-15 09:03:27 - Attack Potential in Impact and Complexity</summary>

- *Luca Allodi, Fabio Massacci*

- `1801.04703v1` - [abs](http://arxiv.org/abs/1801.04703v1) - [pdf](http://arxiv.org/pdf/1801.04703v1)

> Vulnerability exploitation is reportedly one of the main attack vectors against computer systems. Yet, most vulnerabilities remain unexploited by attackers. It is therefore of central importance to identify vulnerabilities that carry a high `potential for attack'. In this paper we rely on Symantec data on real attacks detected in the wild to identify a trade-off in the Impact and Complexity of a vulnerability, in terms of attacks that it generates; exploiting this effect, we devise a readily computable estimator of the vulnerability's Attack Potential that reliably estimates the expected volume of attacks against the vulnerability. We evaluate our estimator performance against standard patching policies by measuring foiled attacks and demanded workload expressed as the number of vulnerabilities entailed to patch. We show that our estimator significantly improves over standard patching policies by ruling out low-risk vulnerabilities, while maintaining invariant levels of coverage against attacks in the wild. Our estimator can be used as a first aid for vulnerability prioritisation to focus assessment efforts on high-potential vulnerabilities.

</details>

<details>

<summary>2018-01-23 20:33:00 - Whose Hands Are in the Finnish Cookie Jar?</summary>

- *Jukka Ruohonen, Ville Leppänen*

- `1801.07759v1` - [abs](http://arxiv.org/abs/1801.07759v1) - [pdf](http://arxiv.org/pdf/1801.07759v1)

> Web cookies are ubiquitously used to track and profile the behavior of users. Although there is a solid empirical foundation for understanding the use of cookies in the global world wide web, thus far, limited attention has been devoted for country-specific and company-level analysis of cookies. To patch this limitation in the literature, this paper investigates persistent third-party cookies used in the Finnish web. The exploratory results reveal some similarities and interesting differences between the Finnish and the global web---in particular, popular Finnish web sites are mostly owned by media companies, which have established their distinct partnerships with online advertisement companies. The results reported can be also reflected against current and future privacy regulation in the European Union.

</details>

<details>

<summary>2018-01-30 00:16:05 - Mix-and-Match Tuning for Self-Supervised Semantic Segmentation</summary>

- *Xiaohang Zhan, Ziwei Liu, Ping Luo, Xiaoou Tang, Chen Change Loy*

- `1712.00661v3` - [abs](http://arxiv.org/abs/1712.00661v3) - [pdf](http://arxiv.org/pdf/1712.00661v3)

> Deep convolutional networks for semantic image segmentation typically require large-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training. To reduce annotation efforts, self-supervised semantic segmentation is recently proposed to pre-train a network without any human-provided labels. The key of this new form of learning is to design a proxy task (e.g. image colorization), from which a discriminative loss can be formulated on unlabeled data. Many proxy tasks, however, lack the critical supervision signals that could induce discriminative representation for the target image segmentation task. Thus self-supervision's performance is still far from that of supervised pre-training. In this study, we overcome this limitation by incorporating a "mix-and-match" (M&M) tuning stage in the self-supervision pipeline. The proposed approach is readily pluggable to many self-supervision methods and does not use more annotated samples than the original process. Yet, it is capable of boosting the performance of target image segmentation task to surpass fully-supervised pre-trained counterpart. The improvement is made possible by better harnessing the limited pixel-wise annotations in the target dataset. Specifically, we first introduce the "mix" stage, which sparsely samples and mixes patches from the target set to reflect rich and diverse local patch statistics of target images. A "match" stage then forms a class-wise connected graph, which can be used to derive a strong triplet-based discriminative loss for fine-tuning the network. Our paradigm follows the standard practice in existing self-supervised studies and no extra data or label is required. With the proposed M&M approach, for the first time, a self-supervision method can achieve comparable or even better performance compared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 dataset and CityScapes dataset.

</details>


## 2018-02

<details>

<summary>2018-02-01 22:11:50 - Snort Intrusion Detection System with Intel Software Guard Extension (Intel SGX)</summary>

- *Dmitrii Kuvaiskii, Somnath Chakrabarti, Mona Vij*

- `1802.00508v1` - [abs](http://arxiv.org/abs/1802.00508v1) - [pdf](http://arxiv.org/pdf/1802.00508v1)

> Network Function Virtualization (NFV) promises the benefits of reduced infrastructure, personnel, and management costs by outsourcing network middleboxes to the public or private cloud. Unfortunately, running network functions in the cloud entails security challenges, especially for complex stateful services. In this paper, we describe our experiences with hardening the king of middleboxes - Intrusion Detection Systems (IDS) - using Intel Software Guard Extensions (Intel SGX) technology. Our IDS secured using Intel SGX, called SEC-IDS, is an unmodified Snort 3 with a DPDK network layer that achieves 10Gbps line rate. SEC-IDS guarantees computational integrity by running all Snort code inside an Intel SGX enclave. At the same time, SEC-IDS achieves near-native performance, with throughput close to 100 percent of vanilla Snort 3, by retaining network I/O outside of the enclave. Our experiments indicate that performance is only constrained by the modest Enclave Page Cache size available on current Intel SGX Skylake based E3 Xeon platforms. Finally, we kept the porting effort minimal by using the Graphene-SGX library OS. Only 27 Lines of Code (LoC) were modified in Snort and 178 LoC in Graphene-SGX itself.

</details>

<details>

<summary>2018-02-05 20:39:55 - Dissection of a Bug Dataset: Anatomy of 395 Patches from Defects4J</summary>

- *Victor Sobreira, Thomas Durieux, Fernanda Madeiral, Martin Monperrus, Marcelo A. Maia*

- `1801.06393v3` - [abs](http://arxiv.org/abs/1801.06393v3) - [pdf](http://arxiv.org/pdf/1801.06393v3)

> Well-designed and publicly available datasets of bugs are an invaluable asset to advance research fields such as fault localization and program repair as they allow directly and fairly comparison between competing techniques and also the replication of experiments. These datasets need to be deeply understood by researchers: the answer for questions like "which bugs can my technique handle?" and "for which bugs is my technique effective?" depends on the comprehension of properties related to bugs and their patches. However, such properties are usually not included in the datasets, and there is still no widely adopted methodology for characterizing bugs and patches. In this work, we deeply study 395 patches of the Defects4J dataset. Quantitative properties (patch size and spreading) were automatically extracted, whereas qualitative ones (repair actions and patterns) were manually extracted using a thematic analysis-based approach. We found that 1) the median size of Defects4J patches is four lines, and almost 30% of the patches contain only addition of lines; 2) 92% of the patches change only one file, and 38% has no spreading at all; 3) the top-3 most applied repair actions are addition of method calls, conditionals, and assignments, occurring in 77% of the patches; and 4) nine repair patterns were found for 95% of the patches, where the most prevalent, appearing in 43% of the patches, is on conditional blocks. These results are useful for researchers to perform advanced analysis on their techniques' results based on Defects4J. Moreover, our set of properties can be used to characterize and compare different bug datasets.

</details>

<details>

<summary>2018-02-07 17:41:25 - Learning One Convolutional Layer with Overlapping Patches</summary>

- *Surbhi Goel, Adam Klivans, Raghu Meka*

- `1802.02547v1` - [abs](http://arxiv.org/abs/1802.02547v1) - [pdf](http://arxiv.org/pdf/1802.02547v1)

> We give the first provably efficient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially overlapping) patches. Additionally, our algorithm requires only mild conditions on the underlying distribution. We prove that our framework captures commonly used schemes from computer vision, including one-dimensional and two-dimensional "patch and stride" convolutions.   Our algorithm-- $Convotron$ -- is inspired by recent work applying isotonic regression to learning neural networks. Convotron uses a simple, iterative update rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as opposed to the realizable setting). In contrast to gradient descent, Convotron requires no special initialization or learning-rate tuning to converge to the global optimum.   We also point out that learning one hidden convolutional layer with respect to a Gaussian distribution and just $one$ disjoint patch $P$ (the other patches may be arbitrary) is $easy$ in the following sense: Convotron can efficiently recover the hidden weight vector by updating $only$ in the direction of $P$.

</details>

<details>

<summary>2018-02-07 21:43:25 - Expressive power of recurrent neural networks</summary>

- *Valentin Khrulkov, Alexander Novikov, Ivan Oseledets*

- `1711.00811v2` - [abs](http://arxiv.org/abs/1711.00811v2) - [pdf](http://arxiv.org/pdf/1711.00811v2)

> Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks -- namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition -- has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks -- ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT- and TT-Networks. We also implement the recurrent TT-Networks and provide numerical evidence of their expressivity.

</details>

<details>

<summary>2018-02-10 06:35:23 - Aurora: Providing Trusted System Services for Enclaves On an Untrusted System</summary>

- *Hongliang Liang, Mingyu Li, Qiong Zhang, Yue Yu, Lin Jiang, Yixiu Chen*

- `1802.03530v1` - [abs](http://arxiv.org/abs/1802.03530v1) - [pdf](http://arxiv.org/pdf/1802.03530v1)

> Intel SGX provisions shielded executions for security-sensitive computation, but lacks support for trusted system services (TSS), such as clock, network and filesystem. This makes \textit{enclaves} vulnerable to Iago attacks~\cite{DBLP:conf/asplos/CheckowayS13} in the face of a powerful malicious system. To mitigate this problem, we present Aurora, a novel architecture that provides TSSes via a secure channel between enclaves and devices on top of an untrusted system, and implement two types of TSSes, i.e. clock and end-to-end network. We evaluate our solution by porting SQLite and OpenSSL into Aurora, experimental results show that SQLite benefits from a \textit{microsecond} accuracy trusted clock and OpenSSL gains end-to-end secure network with about 1ms overhead.

</details>

<details>

<summary>2018-02-11 01:33:33 - Understanding Convolutional Networks with APPLE : Automatic Patch Pattern Labeling for Explanation</summary>

- *Sandeep Konam, Ian Quah, Stephanie Rosenthal, Manuela Veloso*

- `1802.03675v1` - [abs](http://arxiv.org/abs/1802.03675v1) - [pdf](http://arxiv.org/pdf/1802.03675v1)

> With the success of deep learning, recent efforts have been focused on analyzing how learned networks make their classifications. We are interested in analyzing the network output based on the network structure and information flow through the network layers. We contribute an algorithm for 1) analyzing a deep network to find neurons that are 'important' in terms of the network classification outcome, and 2)automatically labeling the patches of the input image that activate these important neurons. We propose several measures of importance for neurons and demonstrate that our technique can be used to gain insight into, and explain how a network decomposes an image to make its final classification.

</details>

<details>

<summary>2018-02-19 02:05:54 - Attributed Hierarchical Port Graphs and Applications</summary>

- *Nneka Chinelo Ene, Maribel Fernández, Bruno Pinaud*

- `1802.06492v1` - [abs](http://arxiv.org/abs/1802.06492v1) - [pdf](http://arxiv.org/pdf/1802.06492v1)

> We present attributed hierarchical port graphs (AHP) as an extension of port graphs that aims at facilitating the design of modular port graph models for complex systems. AHP consist of a number of interconnected layers, where each layer defines a port graph whose nodes may link to layers further down the hierarchy; attributes are used to store user-defined data as well as visualisation and run-time system parameters. We also generalise the notion of strategic port graph rewriting (a particular kind of graph transformation system, where port graph rewriting rules are controlled by user-defined strategies) to deal with AHP following the Single Push-out approach. We outline examples of application in two areas: functional programming and financial modelling.

</details>

<details>

<summary>2018-02-26 15:23:53 - VAE with a VampPrior</summary>

- *Jakub M. Tomczak, Max Welling*

- `1705.07120v5` - [abs](http://arxiv.org/abs/1705.07120v5) - [pdf](http://arxiv.org/pdf/1705.07120v5)

> Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.

</details>

<details>

<summary>2018-02-28 16:03:45 - Learning Discriminative Multilevel Structured Dictionaries for Supervised Image Classification</summary>

- *Jeremy Aghaei Mazaheri, Elif Vural, Claude Labit, Christine Guillemot*

- `1802.10497v1` - [abs](http://arxiv.org/abs/1802.10497v1) - [pdf](http://arxiv.org/pdf/1802.10497v1)

> Sparse representations using overcomplete dictionaries have proved to be a powerful tool in many signal processing applications such as denoising, super-resolution, inpainting, compression or classification. The sparsity of the representation very much depends on how well the dictionary is adapted to the data at hand. In this paper, we propose a method for learning structured multilevel dictionaries with discriminative constraints to make them well suited for the supervised pixelwise classification of images. A multilevel tree-structured discriminative dictionary is learnt for each class, with a learning objective concerning the reconstruction errors of the image patches around the pixels over each class-representative dictionary. After the initial assignment of the class labels to image pixels based on their sparse representations over the learnt dictionaries, the final classification is achieved by smoothing the label image with a graph cut method and an erosion method. Applied to a common set of texture images, our supervised classification method shows competitive results with the state of the art.

</details>

<details>

<summary>2018-02-28 17:08:26 - When is a Convolutional Filter Easy To Learn?</summary>

- *Simon S. Du, Jason D. Lee, Yuandong Tian*

- `1709.06129v2` - [abs](http://arxiv.org/abs/1709.06129v2) - [pdf](http://arxiv.org/pdf/1709.06129v2)

> We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.

</details>


## 2018-03

<details>

<summary>2018-03-01 12:49:11 - LaVAN: Localized and Visible Adversarial Noise</summary>

- *Danny Karmon, Daniel Zoran, Yoav Goldberg*

- `1801.02608v2` - [abs](http://arxiv.org/abs/1801.02608v2) - [pdf](http://arxiv.org/pdf/1801.02608v2)

> Most works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates.

</details>

<details>

<summary>2018-03-02 12:59:50 - Towards a Question Answering System over the Semantic Web</summary>

- *Dennis Diefenbach, Andreas Both, Kamal Singh, Pierre Maret*

- `1803.00832v1` - [abs](http://arxiv.org/abs/1803.00832v1) - [pdf](http://arxiv.org/pdf/1803.00832v1)

> Thanks to the development of the Semantic Web, a lot of new structured data has become available on the Web in the form of knowledge bases (KBs). Making this valuable data accessible and usable for end-users is one of the main goals of Question Answering (QA) over KBs. Most current QA systems query one KB, in one language (namely English). The existing approaches are not designed to be easily adaptable to new KBs and languages. We first introduce a new approach for translating natural language questions to SPARQL queries. It is able to query several KBs simultaneously, in different languages, and can easily be ported to other KBs and languages. In our evaluation, the impact of our approach is proven using 5 different well-known and large KBs: Wikidata, DBpedia, MusicBrainz, DBLP and Freebase as well as 5 different languages namely English, German, French, Italian and Spanish. Second, we show how we integrated our approach, to make it easily accessible by the research community and by end-users. To summarize, we provided a conceptional solution for multilingual, KB-agnostic Question Answering over the Semantic Web. The provided first approximation validates this concept.

</details>

<details>

<summary>2018-03-06 01:35:01 - Learning Filter Bank Sparsifying Transforms</summary>

- *Luke Pfister, Yoram Bresler*

- `1803.01980v1` - [abs](http://arxiv.org/abs/1803.01980v1) - [pdf](http://arxiv.org/pdf/1803.01980v1)

> Data is said to follow the transform (or analysis) sparsity model if it becomes sparse when acted on by a linear operator called a sparsifying transform. Several algorithms have been designed to learn such a transform directly from data, and data-adaptive sparsifying transforms have demonstrated excellent performance in signal restoration tasks. Sparsifying transforms are typically learned using small sub-regions of data called patches, but these algorithms often ignore redundant information shared between neighboring patches.   We show that many existing transform and analysis sparse representations can be viewed as filter banks, thus linking the local properties of patch-based model to the global properties of a convolutional model. We propose a new transform learning framework where the sparsifying transform is an undecimated perfect reconstruction filter bank. Unlike previous transform learning algorithms, the filter length can be chosen independently of the number of filter bank channels. Numerical results indicate filter bank sparsifying transforms outperform existing patch-based transform learning for image denoising while benefiting from additional flexibility in the design process.

</details>

<details>

<summary>2018-03-06 08:28:18 - The Earth ain't Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera</summary>

- *Junaid Ahmed Ansari, Sarthak Sharma, Anshuman Majumdar, J. Krishna Murthy, K. Madhava Krishna*

- `1803.02057v1` - [abs](http://arxiv.org/abs/1803.02057v1) - [pdf](http://arxiv.org/pdf/1803.02057v1)

> Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but most such demonstrations have been confined to plain roads. We demonstrate, to the best of our knowledge, the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands as well. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.

</details>

<details>

<summary>2018-03-10 05:31:31 - Large-Scale Analysis of Framework-Specific Exceptions in Android Apps</summary>

- *Lingling Fan, Ting Su, Sen Chen, Guozhu Meng, Yang Liu, Lihua Xu, Geguang Pu, Zhendong Su*

- `1801.07009v3` - [abs](http://arxiv.org/abs/1801.07009v3) - [pdf](http://arxiv.org/pdf/1801.07009v3)

> Mobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist -- this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community.

</details>

<details>

<summary>2018-03-14 22:56:31 - Machine learning-assisted virtual patching of web applications</summary>

- *Gustavo Betarte, Eduardo Giménez, Rodrigo Martínez, Álvaro Pardo*

- `1803.05529v1` - [abs](http://arxiv.org/abs/1803.05529v1) - [pdf](http://arxiv.org/pdf/1803.05529v1)

> Web applications are permanently being exposed to attacks that exploit their vulnerabilities. In this work we investigate the application of machine learning techniques to leverage Web Application Firewall (WAF), a technology that is used to detect and prevent attacks. We propose a combined approach of machine learning models, based on one-class classification and n-gram analysis, to enhance the detection and accuracy capabilities of MODSECURITY, an open source and widely used WAF. The results are promising and outperform MODSECURITY when configured with the OWASP Core Rule Set, the baseline configuration setting of a widely deployed, rule-based WAF technology. The proposed solution, combining both approaches, allow us to deploy a WAF when no training data for the application is available (using one-class classification), and an improved one using n-grams when training data is available.

</details>

<details>

<summary>2018-03-20 01:54:20 - Acoustic feature learning using cross-domain articulatory measurements</summary>

- *Qingming Tang, Weiran Wang, Karen Livescu*

- `1803.06805v2` - [abs](http://arxiv.org/abs/1803.06805v2) - [pdf](http://arxiv.org/pdf/1803.06805v2)

> Previous work has shown that it is possible to improve speech recognition by learning acoustic features from paired acoustic-articulatory data, for example by using canonical correlation analysis (CCA) or its deep extensions. One limitation of this prior work is that the learned feature models are difficult to port to new datasets or domains, and articulatory data is not available for most speech corpora. In this work we study the problem of acoustic feature learning in the setting where we have access to an external, domain-mismatched dataset of paired speech and articulatory measurements, either with or without labels. We develop methods for acoustic feature learning in these settings, based on deep variational CCA and extensions that use both source and target domain data and labels. Using this approach, we improve phonetic recognition accuracies on both TIMIT and Wall Street Journal and analyze a number of design choices.

</details>

<details>

<summary>2018-03-20 17:17:39 - Stacked Neural Networks for end-to-end ciliary motion analysis</summary>

- *Charles Lu, M. Marx, M. Zahid, C. W. Lo, C. Chennubhotla, S. P. Quinn*

- `1803.07534v1` - [abs](http://arxiv.org/abs/1803.07534v1) - [pdf](http://arxiv.org/pdf/1803.07534v1)

> Cilia are hairlike structures protruding from nearly every cell in the body. Diseases known as ciliopathies, where cilia function is disrupted, can result in a wide spectrum of disorders. However, most techniques for assessing ciliary motion rely on manual identification and tracking of cilia; this process is laborious and error-prone, and does not scale well. Even where automated ciliary motion analysis tools exist, their applicability is limited. Here, we propose an end-to-end computational machine learning pipeline that automatically identifies regions of cilia from videos, extracts patches of cilia, and classifies patients as exhibiting normal or abnormal ciliary motion. In particular, we demonstrate how convolutional LSTM are able to encode complex features while remaining sensitive enough to differentiate between a variety of motion patterns. Our framework achieves 90% with only a few hundred training epochs. We find that the combination of segmentation and classification networks in a single pipeline yields performance comparable to existing computational pipelines, while providing the additional benefit of an end-to-end, fully-automated analysis toolbox for ciliary motion.

</details>

<details>

<summary>2018-03-27 10:20:03 - ΔBreakpad: Diversified Binary Crash Reporting</summary>

- *Bert Abrath, Bart Coppens, Mohit Mishra, Jens Van den Broeck, Bjorn De Sutter*

- `1705.00713v3` - [abs](http://arxiv.org/abs/1705.00713v3) - [pdf](http://arxiv.org/pdf/1705.00713v3)

> This paper introduces {\Delta}Breakpad. It extends the Breakpad crash reporting system to handle software diversity effectively and efficiently by replicating and patching the debug information of diversified software versions. Simple adaptations to existing open source compiler tools are presented that on the one hand introduce significant amounts of diversification in the code and stack layout of ARMv7 binaries to mitigate the widespread deployment of code injection and code reuse attacks, while on the other hand still supporting accurate crash reporting. An evaluation on SPEC2006 benchmarks demonstrates that the corresponding computational, storage, and communication overheads are small.

</details>

<details>

<summary>2018-03-28 17:07:18 - Pattern Analysis with Layered Self-Organizing Maps</summary>

- *David Friedlander*

- `1803.08996v2` - [abs](http://arxiv.org/abs/1803.08996v2) - [pdf](http://arxiv.org/pdf/1803.08996v2)

> This paper defines a new learning architecture, Layered Self-Organizing Maps (LSOMs), that uses the SOM and supervised-SOM learning algorithms. The architecture is validated with the MNIST database of hand-written digit images. LSOMs are similar to convolutional neural nets (covnets) in the way they sample data, but different in the way they represent features and learn. LSOMs analyze (or generate) image patches with maps of exemplars determined by the SOM learning algorithm rather than feature maps from filter-banks learned via backprop.   LSOMs provide an alternative to features derived from covnets. Multi-layer LSOMs are trained bottom-up, without the use of backprop and therefore may be of interest as a model of the visual cortex. The results show organization at multiple levels. The algorithm appears to be resource efficient in learning, classifying and generating images. Although LSOMs can be used for classification, their validation accuracy for these exploratory runs was well below the state of the art. The goal of this article is to define the architecture and display the structures resulting from its application to the MNIST images.

</details>

<details>

<summary>2018-03-28 22:14:26 - Improvements to context based self-supervised learning</summary>

- *T. Nathan Mundhenk, Daniel Ho, Barry Y. Chen*

- `1711.06379v3` - [abs](http://arxiv.org/abs/1711.06379v3) - [pdf](http://arxiv.org/pdf/1711.06379v3)

> We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and "linear tests" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci.llnl.gov/selfsupervised/.

</details>

<details>

<summary>2018-03-30 09:51:04 - Contrast-Oriented Deep Neural Networks for Salient Object Detection</summary>

- *Guanbin Li, Yizhou Yu*

- `1803.11395v1` - [abs](http://arxiv.org/abs/1803.11395v1) - [pdf](http://arxiv.org/pdf/1803.11395v1)

> Deep convolutional neural networks have become a key element in the recent breakthrough of salient object detection. However, existing CNN-based methods are based on either patch-wise (region-wise) training and inference or fully convolutional networks. Methods in the former category are generally time-consuming due to severe storage and computational redundancies among overlapping patches. To overcome this deficiency, methods in the second category attempt to directly map a raw input image to a predicted dense saliency map in a single network forward pass. Though being very efficient, it is arduous for these methods to detect salient objects of different scales or salient regions with weak semantic information. In this paper, we develop hybrid contrast-oriented deep neural networks to overcome the aforementioned limitations. Each of our deep networks is composed of two complementary components, including a fully convolutional stream for dense prediction and a segment-level spatial pooling stream for sparse saliency inference. We further propose an attentional module that learns weight maps for fusing the two saliency predictions from these two streams. A tailored alternate scheme is designed to train these deep networks by fine-tuning pre-trained baseline models. Finally, a customized fully connected CRF model incorporating a salient contour feature embedding can be optionally applied as a post-processing step to improve spatial coherence and contour positioning in the fused result from these two streams. Extensive experiments on six benchmark datasets demonstrate that our proposed model can significantly outperform the state of the art in terms of all popular evaluation metrics.

</details>


## 2018-04

<details>

<summary>2018-04-04 12:00:22 - Analysing and Patching SPEKE in ISO/IEC</summary>

- *Feng Hao, Roberto Metere, Siamak F. Shahandashti, Changyu Dong*

- `1802.04900v2` - [abs](http://arxiv.org/abs/1802.04900v2) - [pdf](http://arxiv.org/pdf/1802.04900v2)

> Simple Password Exponential Key Exchange (SPEKE) is a well-known Password Authenticated Key Exchange (PAKE) protocol that has been used in Blackberry phones for secure messaging and Entrust's TruePass end-to-end web products. It has also been included into international standards such as ISO/IEC 11770-4 and IEEE P1363.2. In this paper, we analyse the SPEKE protocol as specified in the ISO/IEC and IEEE standards. We identify that the protocol is vulnerable to two new attacks: an impersonation attack that allows an attacker to impersonate a user without knowing the password by launching two parallel sessions with the victim, and a key-malleability attack that allows a man-in-the-middle (MITM) to manipulate the session key without being detected by the end users. Both attacks have been acknowledged by the technical committee of ISO/IEC SC 27, and ISO/IEC 11770-4 revised as a result. We propose a patched SPEKE called P-SPEKE and present a formal analysis in the Applied Pi Calculus using ProVerif to show that the proposed patch prevents both attacks. The proposed patch has been included into the latest revision of ISO/IEC 11770-4 published in 2017.

</details>

<details>

<summary>2018-04-06 22:06:25 - Optimisation of Least Squares Algorithm: A Study of Frame Based Programming Techniques in Horizontal Networks</summary>

- *C. P. E. Agbachi*

- `1804.05665v1` - [abs](http://arxiv.org/abs/1804.05665v1) - [pdf](http://arxiv.org/pdf/1804.05665v1)

> Least squares estimation, a regression technique based on minimisation of residuals, has been invaluable in bringing the best fit solutions to parameters in science and engineering. However, in dynamic environments such as in Geomatics Engineering, formation of these equations can be very challenging. And these constraints are ported and apparent in most program models, requiring users at ease with the subject matter. This paper reviews the methods of least squares approximation and examines a one-step automated approach, with error analysis, through the instrumentality of frames, object oriented programming.

</details>

<details>

<summary>2018-04-11 13:28:07 - Data Augmentation by Pairing Samples for Images Classification</summary>

- *Hiroshi Inoue*

- `1801.02929v2` - [abs](http://arxiv.org/abs/1801.02929v2) - [pdf](http://arxiv.org/pdf/1801.02929v2)

> Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate $N^2$ new samples from $N$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.

</details>

<details>

<summary>2018-04-19 11:40:23 - Unsupervised Prostate Cancer Detection on H&E using Convolutional Adversarial Autoencoders</summary>

- *Wouter Bulten, Geert Litjens*

- `1804.07098v1` - [abs](http://arxiv.org/abs/1804.07098v1) - [pdf](http://arxiv.org/pdf/1804.07098v1)

> We propose an unsupervised method using self-clustering convolutional adversarial autoencoders to classify prostate tissue as tumor or non-tumor without any labeled training data. The clustering method is integrated into the training of the autoencoder and requires only little post-processing. Our network trains on hematoxylin and eosin (H&E) input patches and we tested two different reconstruction targets, H&E and immunohistochemistry (IHC). We show that antibody-driven feature learning using IHC helps the network to learn relevant features for the clustering task. Our network achieves a F1 score of 0.62 using only a small set of validation labels to assign classes to clusters.

</details>


## 2018-05

<details>

<summary>2018-05-05 07:53:47 - Process Algebraic Architectural Description Languages: Generalizing Component-Oriented Mismatch Detection in the Presence of Nonsynchronous Communications</summary>

- *Marco Bernardo, Edoardo Bontà, Alessandro Aldini*

- `1805.11676v1` - [abs](http://arxiv.org/abs/1805.11676v1) - [pdf](http://arxiv.org/pdf/1805.11676v1)

> In the original paper, we showed how to enhance the expressiveness of a typical process algebraic architectural description language by including the capability of representing nonsynchronous communications. In particular, we extended the language by means of additional qualifiers enabling the designer to distinguish among synchronous, semi-synchronous, and asynchronous ports. Moreover, we showed how to modify techniques for detecting coordination mismatches such as the compatibility check for star topologies and the interoperability check for cycle topologies, in such a way that those two checks are applicable also in the presence of nonsynchronous communications. In this addendum, we generalize those results by showing that it is possible to verify in a component-oriented way an arbitrary property of a certain class (not only deadlock) over an entire architectural type having an arbitrary topology (not only stars and cycles) by considering also behavioral variations, exogenous variations, endogenous variations, and multiplicity variations, so to deal with the possible presence of nonsynchronous communications. The proofs are at the basis of some results mentioned in the book "A Process Algebraic Approach to Software Architecture Design" by Alessandro Aldini, Marco Bernardo, and Flavio Corradini, published by Springer in 2010.

</details>

<details>

<summary>2018-05-08 15:14:20 - Fast Feature Extraction with CNNs with Pooling Layers</summary>

- *Christian Bailer, Tewodros Habtegebrial, Kiran varanasi, Didier Stricker*

- `1805.03096v1` - [abs](http://arxiv.org/abs/1805.03096v1) - [pdf](http://arxiv.org/pdf/1805.03096v1)

> In recent years, many publications showed that convolutional neural network based features can have a superior performance to engineered features. However, not much effort was taken so far to extract local features efficiently for a whole image. In this paper, we present an approach to compute patch-based local feature descriptors efficiently in presence of pooling and striding layers for whole images at once. Our approach is generic and can be applied to nearly all existing network architectures. This includes networks for all local feature extraction tasks like camera calibration, Patchmatching, optical flow estimation and stereo matching. In addition, our approach can be applied to other patch-based approaches like sliding window object detection and recognition. We complete our paper with a speed benchmark of popular CNN based feature extraction approaches applied on a whole image, with and without our speedup, and example code (for Torch) that shows how an arbitrary CNN architecture can be easily converted by our approach.

</details>

<details>

<summary>2018-05-11 09:02:29 - Incentivized Delivery Network of IoT Software Updates Based on Trustless Proof-of-Distribution</summary>

- *Oded Leiba, Yechiav Yitzchak, Ron Bitton, Asaf Nadler, Asaf Shabtai*

- `1805.04282v1` - [abs](http://arxiv.org/abs/1805.04282v1) - [pdf](http://arxiv.org/pdf/1805.04282v1)

> The prevalence of IoT devices makes them an ideal target for attackers. To reduce the risk of attacks vendors routinely deliver security updates (patches) for their devices. The delivery of security updates becomes challenging due to the issue of scalability as the number of devices may grow much quicker than vendors' distribution systems. Previous studies have suggested a permissionless and decentralized blockchain-based network in which nodes can host and deliver security updates, thus the addition of new nodes scales out the network. However, these studies do not provide an incentive for nodes to join the network, making it unlikely for nodes to freely contribute their hosting space, bandwidth, and computation resources. In this paper, we propose a novel decentralized IoT software update delivery network in which participating nodes referred to as distributors) are compensated by vendors with digital currency for delivering updates to devices. Upon the release of a new security update, a vendor will make a commitment to provide digital currency to distributors that deliver the update; the commitment will be made with the use of smart contracts, and hence will be public, binding, and irreversible. The smart contract promises compensation to any distributor that provides proof-of-distribution, which is unforgeable proof that a single update was delivered to a single device. A distributor acquires the proof-of-distribution by exchanging a security update for a device signature using the Zero-Knowledge Contingent Payment (ZKCP) trustless data exchange protocol. Eliminating the need for trust between the security update distributor and the security consumer (IoT device) by providing fair compensation, can significantly increase the number of distributors, thus facilitating rapid scale out.

</details>

<details>

<summary>2018-05-16 20:53:21 - End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition</summary>

- *Samet Oymak, Mahdi Soltanolkotabi*

- `1805.06523v1` - [abs](http://arxiv.org/abs/1805.06523v1) - [pdf](http://arxiv.org/pdf/1805.06523v1)

> In this paper we study the problem of learning the weights of a deep convolutional neural network. We consider a network where convolutions are carried out over non-overlapping patches with a single kernel in each layer. We develop an algorithm for simultaneously learning all the kernels from the training data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based on a rank-1 tensor decomposition. We theoretically investigate DeepTD under a realizable model for the training data where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted convolutional kernels. We show that DeepTD is data-efficient and provably works as soon as the sample size exceeds the total number of convolutional weights in the network. We carry out a variety of numerical experiments to investigate the effectiveness of DeepTD and verify our theoretical findings.

</details>

<details>

<summary>2018-05-17 09:19:35 - Evolutionary RL for Container Loading</summary>

- *S Saikia, R Verma, P Agarwal, G Shroff, L Vig, A Srinivasan*

- `1805.06664v1` - [abs](http://arxiv.org/abs/1805.06664v1) - [pdf](http://arxiv.org/pdf/1805.06664v1)

> Loading the containers on the ship from a yard, is an impor- tant part of port operations. Finding the optimal sequence for the loading of containers, is known to be computationally hard and is an example of combinatorial optimization, which leads to the application of simple heuristics in practice. In this paper, we propose an approach which uses a mix of Evolutionary Strategies and Reinforcement Learning (RL) tech- niques to find an approximation of the optimal solution. The RL based agent uses the Policy Gradient method, an evolutionary reward strategy and a Pool of good (not-optimal) solutions to find the approximation. We find that the RL agent learns near-optimal solutions that outperforms the heuristic solutions. We also observe that the RL agent assisted with a pool generalizes better for unseen problems than an RL agent without a pool. We present our results on synthetic data as well as on subsets of real-world problems taken from container terminal. The results validate that our approach does comparatively better than the heuristics solutions available, and adapts to unseen problems better.

</details>

<details>

<summary>2018-05-21 02:23:49 - Learning Device Models with Recurrent Neural Networks</summary>

- *John Clemens*

- `1805.07869v1` - [abs](http://arxiv.org/abs/1805.07869v1) - [pdf](http://arxiv.org/pdf/1805.07869v1)

> Recurrent neural networks (RNNs) are powerful constructs capable of modeling complex systems, up to and including Turing Machines. However, learning such complex models from finite training sets can be difficult. In this paper we empirically show that RNNs can learn models of computer peripheral devices through input and output state observation. This enables automated development of functional software-only models of hardware devices. Such models are applicable to any number of tasks, including device validation, driver development, code de-obfuscation, and reverse engineering. We show that the same RNN structure successfully models six different devices from simple test circuits up to a 16550 UART serial port, and verify that these models are capable of producing equivalent output to real hardware.

</details>

<details>

<summary>2018-05-24 18:50:36 - A Bug Bounty Perspective on the Disclosure of Web Vulnerabilities</summary>

- *Jukka Ruohonen, Luca Allodi*

- `1805.09850v1` - [abs](http://arxiv.org/abs/1805.09850v1) - [pdf](http://arxiv.org/pdf/1805.09850v1)

> Bug bounties have become increasingly popular in recent years. This paper discusses bug bounties by framing these theoretically against so-called platform economy. Empirically the interest is on the disclosure of web vulnerabilities through the Open Bug Bounty (OBB) platform between 2015 and late 2017. According to the empirical results based on a dataset covering nearly 160 thousand web vulnerabilities, (i) OBB has been successful as a community-based platform for the dissemination of web vulnerabilities. The platform has also attracted many productive hackers, (ii) but there exists a large productivity gap, which likely relates to (iii) a knowledge gap and the use of automated tools for web vulnerability discovery. While the platform (iv) has been exceptionally fast to evaluate new vulnerability submissions, (v) the patching times of the web vulnerabilities disseminated have been long. With these empirical results and the accompanying theoretical discussion, the paper contributes to the small but rapidly growing amount of research on bug bounties. In addition, the paper makes a practical contribution by discussing the business models behind bug bounties from the viewpoints of platforms, ecosystems, and vulnerability markets.

</details>

<details>

<summary>2018-05-25 07:36:50 - User Interface Design Smell: Automatic Detection and Refactoring of Blob Listeners</summary>

- *Arnaud Blouin, Valéria Lelli, Benoit Baudry, Fabien Coulon*

- `1703.10674v3` - [abs](http://arxiv.org/abs/1703.10674v3) - [pdf](http://arxiv.org/pdf/1703.10674v3)

> User Interfaces (UIs) intensively rely on event-driven programming: widgets send UI events, which capture users' interactions, to dedicated objects called controllers. Controllers use several UI listeners that handle these events to produce UI commands. First, we reveal the presence of design smells in the code that describes and controls UIs. Second, we demonstrate that specific code analyses are necessary to analyze and refactor UI code, because of its coupling with the rest of the code. We conducted an empirical study on four large Java Swing and SWT open-source software systems. We study to what extent the number of UI commands that a UI listener can produce has an impact on the change- and fault-proneness of the UI listener code. We develop a static code analysis for detecting UI commands in the code. We identify a new type of design smell, called Blob Listener that characterizes UI listeners that can produce more than two UI commands. We propose a systematic static code analysis procedure that searches for Blob Listeners that we implement in InspectorGuidget. We conducted experiments on the four software systems for which we manually identified 53 instances of Blob Listener. InspectorGuidget successfully detected 52 Blob Listeners out of 53. The results exhibit a precision of 81.25% and a recall of 98.11%. We then developed a semi-automatically and behavior-preserving refactoring process to remove Blob Listeners. 49.06% of the 53 Blob Listeners were automatically refactored. Patches for JabRef, and FreeCol have been accepted and merged. Discussions with developers of the four software systems assess the relevance of the Blob Listener. This work shows that UI code also suffers from design smells that have to be identified and characterized. We argue that studies have to be conducted to find other UI design smells and tools that analyze UI code must be developed.

</details>

<details>

<summary>2018-05-28 17:01:37 - Flexible and accurate inference and learning for deep generative models</summary>

- *Eszter Vertes, Maneesh Sahani*

- `1805.11051v1` - [abs](http://arxiv.org/abs/1805.11051v1) - [pdf](http://arxiv.org/pdf/1805.11051v1)

> We introduce a new approach to learning in hierarchical latent-variable generative models called the "distributed distributional code Helmholtz machine", which emphasises flexibility and accuracy in the inferential process. In common with the original Helmholtz machine and later variational autoencoder algorithms (but unlike adverserial methods) our approach learns an explicit inference or "recognition" model to approximate the posterior distribution over the latent variables. Unlike in these earlier methods, the posterior representation is not limited to a narrow tractable parameterised form (nor is it represented by samples). To train the generative and recognition models we develop an extended wake-sleep algorithm inspired by the original Helmholtz Machine. This makes it possible to learn hierarchical latent models with both discrete and continuous variables, where an accurate posterior representation is essential. We demonstrate that the new algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets.

</details>

<details>

<summary>2018-05-29 09:15:26 - CNN-Based Detection of Generic Constrast Adjustment with JPEG Post-processing</summary>

- *Mauro Barni, Andrea Costanzo, Ehsan Nowroozi, Benedetta Tondi*

- `1805.11318v1` - [abs](http://arxiv.org/abs/1805.11318v1) - [pdf](http://arxiv.org/pdf/1805.11318v1)

> Detection of contrast adjustments in the presence of JPEG postprocessing is known to be a challenging task. JPEG post processing is often applied innocently, as JPEG is the most common image format, or it may correspond to a laundering attack, when it is purposely applied to erase the traces of manipulation. In this paper, we propose a CNN-based detector for generic contrast adjustment, which is robust to JPEG compression. The proposed system relies on a patch-based Convolutional Neural Network (CNN), trained to distinguish pristine images from contrast adjusted images, for some selected adjustment operators of different nature. Robustness to JPEG compression is achieved by training the CNN with JPEG examples, compressed over a range of Quality Factors (QFs). Experimental results show that the detector works very well and scales well with respect to the adjustment type, yielding very good performance under a large variety of unseen tonal adjustments.

</details>

<details>

<summary>2018-05-30 15:47:05 - The Coming Era of AlphaHacking? A Survey of Automatic Software Vulnerability Detection, Exploitation and Patching Techniques</summary>

- *Tiantian Ji, Yue Wu, Chang Wang, Xi Zhang, Zhongru Wang*

- `1805.11001v2` - [abs](http://arxiv.org/abs/1805.11001v2) - [pdf](http://arxiv.org/pdf/1805.11001v2)

> With the success of the Cyber Grand Challenge (CGC) sponsored by DARPA, the topic of Autonomous Cyber Reasoning System (CRS) has recently attracted extensive attention from both industry and academia. Utilizing automated system to detect, exploit and patch software vulnerabilities seems so attractive because of its scalability and cost-efficiency compared with the human expert based solution. In this paper, we give an extensive survey of former representative works related to the underlying technologies of a CRS, including vulnerability detection, exploitation and patching. As an important supplement, we then review several pioneer studies that explore the potential of machine learning technologies in this field, and point out that the future development of Autonomous CRS is inseparable from machine learning.

</details>


## 2018-06

<details>

<summary>2018-06-01 00:10:04 - Sea surface temperature prediction and reconstruction using patch-level neural network representations</summary>

- *Said Ouala, Cedric Herzet, Ronan Fablet*

- `1806.00144v1` - [abs](http://arxiv.org/abs/1806.00144v1) - [pdf](http://arxiv.org/pdf/1806.00144v1)

> The forecasting and reconstruction of ocean and atmosphere dynamics from satellite observation time series are key challenges. While model-driven representations remain the classic approaches, data-driven representations become more and more appealing to benefit from available large-scale observation and simulation datasets. In this work we investigate the relevance of recently introduced bilinear residual neural network representations, which mimic numerical integration schemes such as Runge-Kutta, for the forecasting and assimilation of geophysical fields from satellite-derived remote sensing data. As a case-study, we consider satellite-derived Sea Surface Temperature time series off South Africa, which involves intense and complex upper ocean dynamics. Our numerical experiments demonstrate that the proposed patch-level neural-network-based representations outperform other data-driven models, including analog schemes, both in terms of forecasting and missing data interpolation performance with a relative gain up to 50\% for highly dynamic areas.

</details>

<details>

<summary>2018-06-01 16:35:08 - A Classification approach towards Unsupervised Learning of Visual Representations</summary>

- *Aditya Vora*

- `1806.00428v1` - [abs](http://arxiv.org/abs/1806.00428v1) - [pdf](http://arxiv.org/pdf/1806.00428v1)

> In this paper, we present a technique for unsupervised learning of visual representations. Specifically, we train a model for foreground and background classification task, in the process of which it learns visual representations. Foreground and background patches for training come af- ter mining for such patches from hundreds and thousands of unlabelled videos available on the web which we ex- tract using a proposed patch extraction algorithm. With- out using any supervision, with just using 150, 000 unla- belled videos and the PASCAL VOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP which is close to the best performing unsupervised feature learn- ing technique whereas better than many other proposed al- gorithms. The code for patch extraction is implemented in Matlab and available open source at the following link .

</details>

<details>

<summary>2018-06-01 23:29:24 - Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps</summary>

- *Simon S. Du, Surbhi Goel*

- `1805.07798v2` - [abs](http://arxiv.org/abs/1805.07798v2) - [pdf](http://arxiv.org/pdf/1805.07798v2)

> We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned. Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks. Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems. We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.

</details>

<details>

<summary>2018-06-05 12:41:14 - Deep Gaussian Processes with Convolutional Kernels</summary>

- *Vinayak Kumar, Vaibhav Singh, P. K. Srijith, Andreas Damianou*

- `1806.01655v1` - [abs](http://arxiv.org/abs/1806.01655v1) - [pdf](http://arxiv.org/pdf/1806.01655v1)

> Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative to standard parametric deep learning models. A DGP is formed by stacking multiple GPs resulting in a well-regularized composition of functions. The Bayesian framework that equips the model with attractive properties, such as implicit capacity control and predictive uncertainty, makes it at the same time challenging to combine with a convolutional structure. This has hindered the application of DGPs in computer vision tasks, an area where deep parametric models (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such as radial basis functions (RBFs) are insufficient for handling pixel variability in raw images. In this paper, we build on the recent convolutional GP to develop Convolutional DGP (CDGP) models which effectively capture image level features through the use of convolution kernels, therefore opening up the way for applying DGPs to computer vision tasks. Our model learns local spatial influence and outperforms strong GP based baselines on multi-class image classification. We also consider various constructions of convolution kernel over the image patches, analyze the computational trade-offs and provide an efficient framework for convolutional DGP models. The experimental results on image data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate the effectiveness of the proposed approaches.

</details>

<details>

<summary>2018-06-11 12:45:13 - Securing the Internet of Things in the Age of Machine Learning and Software-defined Networking</summary>

- *Francesco Restuccia, Salvatore D'Oro, Tommaso Melodia*

- `1803.05022v2` - [abs](http://arxiv.org/abs/1803.05022v2) - [pdf](http://arxiv.org/pdf/1803.05022v2)

> The Internet of Things (IoT) realizes a vision where billions of interconnected devices are deployed just about everywhere, from inside our bodies to the most remote areas of the globe. As the IoT will soon pervade every aspect of our lives and will be accessible from anywhere, addressing critical IoT security threats is now more important than ever. Traditional approaches where security is applied as an afterthought and as a "patch" against known attacks are insufficient. Indeed, next-generation IoT challenges will require a new secure-by-design vision, where threats are addressed proactively and IoT devices learn to dynamically adapt to different threats. To this end, machine learning and software-defined networking will be key to provide both reconfigurability and intelligence to the IoT devices. In this paper, we first provide a taxonomy and survey the state of the art in IoT security research, and offer a roadmap of concrete research challenges related to the application of machine learning and software-defined networking to address existing and next-generation IoT security threats.

</details>

<details>

<summary>2018-06-11 23:44:45 - Understanding Patch-Based Learning by Explaining Predictions</summary>

- *Christopher Anders, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller*

- `1806.06926v1` - [abs](http://arxiv.org/abs/1806.06926v1) - [pdf](http://arxiv.org/pdf/1806.06926v1)

> Deep networks are able to learn highly predictive models of video data. Due to video length, a common strategy is to train them on small video snippets. We apply the deep Taylor / LRP technique to understand the deep network's classification decisions, and identify a "border effect": a tendency of the classifier to look mainly at the bordering frames of the input. This effect relates to the step size used to build the video snippet, which we can then tune in order to improve the classifier's accuracy without retraining the model. To our knowledge, this is the the first work to apply the deep Taylor / LRP technique on any video analyzing neural network.

</details>

<details>

<summary>2018-06-12 03:59:47 - Sound Patch Generation for Vulnerabilities</summary>

- *Zhen Huang, David Lie*

- `1711.11136v2` - [abs](http://arxiv.org/abs/1711.11136v2) - [pdf](http://arxiv.org/pdf/1711.11136v2)

> Security vulnerabilities are among the most critical software defects in existence. As such, they require patches that are correct and quickly deployed. This motivates an automatic patch generation method that emphasizes both soundness and wide applicability. To address this challenge, we propose Senx, which uses three novel patch generation techniques to create patches for out-of-bounds read/write vulnerabilities. Senx uses symbolic execution to extract expressions from the source code of a target application to synthesize patches. To reduce the runtime overhead of patches, it uses loop cloning and access range analysis to analyze loops involved in these vulnerabilities and elevate patches outside of loops. For vulnerabilities that span multiple functions, Senx uses expression translation to translate expressions and place them in a function scope where all values are available to create the patch. This enables Senx to patch vulnerabilities with complex loops and interprocedural dependencies that previous semantics-based patch generation systems cannot handle.   We have implemented a prototype using this approach. Our evaluation shows that the patches generated by Senx successfully fix 76% of 42 real-world vulnerabilities from 11 applications including various tools or libraries for manipulating graphics/media files, a programming language interpreter, a relational database engine, a collection of programming tools for creating and managing binary programs, and a collection of basic file, shell, and text manipulation tools. All patches that Senx produces are sound, and Senx correctly aborts patch generations in cases where its analysis will fall short.

</details>

<details>

<summary>2018-06-12 15:42:21 - Production-Driven Patch Generation and Validation</summary>

- *Thomas Durieux, Youssef Hamadi, Martin Monperrus*

- `1609.06848v2` - [abs](http://arxiv.org/abs/1609.06848v2) - [pdf](http://arxiv.org/pdf/1609.06848v2)

> We envision a world where the developer would receive each morning in her GitHub dashboard a list of potential patches that fix certain production failures. For this, we propose a novel program repair scheme, with the unique feature of being applicable to production directly. We present the design and implementation of a prototype system for Java, called Itzal, that performs patch generation for uncaught exceptions in production. We have performed two empirical experiments to validate our system: the first one on 34 failures from 14 different software applications, the second one on 16 seeded failures in 3 real open-source e-commerce applications for which we have set up a realistic user traffic. This validates the novel and disruptive idea of using program repair directly in production.

</details>

<details>

<summary>2018-06-17 14:13:14 - Mind the gap: quantification of incomplete ablation patterns after pulmonary vein isolation using minimum path search</summary>

- *Marta Nuñez-Garcia, Oscar Camara, Mark D. O'Neill, Reza Razavi, Henry Chubb, Constantine Butakoff*

- `1806.06387v1` - [abs](http://arxiv.org/abs/1806.06387v1) - [pdf](http://arxiv.org/pdf/1806.06387v1)

> Pulmonary vein isolation (PVI) is a common procedure for the treatment of atrial fibrillation (AF). A successful isolation produces a continuous lesion (scar) completely encircling the veins that stops activation waves from propagating to the atrial body. Unfortunately, the encircling lesion is often incomplete, becoming a combination of scar and gaps of healthy tissue. These gaps are potential causes of AF recurrence, which requires a redo of the isolation procedure. Late-gadolinium enhanced cardiac magnetic resonance (LGE-CMR) is a non-invasive method that may also be used to detect gaps, but it is currently a time-consuming process, prone to high inter-observer variability. In this paper, we present a method to semi-automatically identify and quantify ablation gaps. Gap quantification is performed through minimum path search in a graph where every node is a scar patch and the edges are the geodesic distances between patches. We propose the Relative Gap Measure (RGM) to estimate the percentage of gap around a vein, which is defined as the ratio of the overall gap length and the total length of the path that encircles the vein. Additionally, an advanced version of the RGM has been developed to integrate gap quantification estimates from different scar segmentation techniques into a single figure-of-merit. Population-based statistical and regional analysis of gap distribution was performed using a standardised parcellation of the left atrium. We have evaluated our method on synthetic and clinical data from 50 AF patients who underwent PVI with radiofrequency ablation. The population-based analysis concluded that the left superior PV is more prone to lesion gaps while the left inferior PV tends to have less gaps (p<0.05 in both cases), in the processed data. This type of information can be very useful for the optimization and objective assessment of PVI interventions.

</details>

<details>

<summary>2018-06-18 09:28:52 - Learning from Outside the Viability Kernel: Why we Should Build Robots that can Fall with Grace</summary>

- *Steve Heim, Alexander Spröwitz*

- `1806.06569v1` - [abs](http://arxiv.org/abs/1806.06569v1) - [pdf](http://arxiv.org/pdf/1806.06569v1)

> Despite impressive results using reinforcement learning to solve complex problems from scratch, in robotics this has still been largely limited to model-based learning with very informative reward functions. One of the major challenges is that the reward landscape often has large patches with no gradient, making it difficult to sample gradients effectively. We show here that the robot state-initialization can have a more important effect on the reward landscape than is generally expected. In particular, we show the counter-intuitive benefit of including initializations that are unviable, in other words initializing in states that are doomed to fail.

</details>

<details>

<summary>2018-06-24 18:11:02 - Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations</summary>

- *Ju-chieh Chou, Cheng-chieh Yeh, Hung-yi Lee, Lin-shan Lee*

- `1804.02812v2` - [abs](http://arxiv.org/abs/1804.02812v2) - [pdf](http://arxiv.org/pdf/1804.02812v2)

> Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations.

</details>

<details>

<summary>2018-06-27 02:55:25 - The Impact of Human Factors on the Participation Decision of Reviewers in Modern Code Review</summary>

- *Shade Ruangwan, Patanamon Thongtanunam, Akinori Ihara, Kenichi Matsumoto*

- `1806.10277v1` - [abs](http://arxiv.org/abs/1806.10277v1) - [pdf](http://arxiv.org/pdf/1806.10277v1)

> Modern Code Review (MCR) plays a key role in software quality practices. In MCR process, a new patch (i.e., a set of code changes) is encouraged to be examined by reviewers in order to identify weaknesses in source code prior to an integration into main software repositories. To mitigate the risk of having future defects, prior work suggests that MCR should be performed with sufficient review participation. Indeed, recent work shows that a low number of participated reviewers is associated with poor software quality. However, there is a likely case that a new patch still suffers from poor review participation even though reviewers were invited. Hence, in this paper, we set out to investigate the factors that are associated with the participation decision of an invited reviewer. Through a case study of 230,090 patches spread across the Android, LibreOffice, OpenStack and Qt systems, we find that (1) 16%-66% of patches have at least one invited reviewer who did not respond to the review invitation; (2) human factors play an important role in predicting whether or not an invited reviewer will participate in a review; (3) a review participation rate of an invited reviewers and code authoring experience of an invited reviewer are highly associated with the participation decision of an invited reviewer. These results can help practitioners better understand about how human factors associate with the participation decision of reviewers and serve as guidelines for inviting reviewers, leading to a better inviting decision and a better reviewer participation.

</details>

<details>

<summary>2018-06-27 22:22:22 - On Reliability of Patch Correctness Assessment</summary>

- *Xuan Bach D. Le, Lingfeng Bao, David Lo, Xin Xia, Shanping Li*

- `1805.05983v2` - [abs](http://arxiv.org/abs/1805.05983v2) - [pdf](http://arxiv.org/pdf/1805.05983v2)

> Current state-of-the-art automatic software repair (ASR) techniques rely heavily on incomplete specifications, e.g., test suites, to generate repairs. This, however, may render ASR tools to generate incorrect repairs that do not generalize. To assess patch correctness, researchers have been following two typical ways separately: (1) Automated annotation, wherein patches are automatically labeled by an independent test suite (ITS) - a patch passing the ITS is regarded as correct or generalizable, and incorrect otherwise, (2) Author annotation, wherein authors of ASR techniques annotate correctness labels of patches generated by their and competing tools by themselves. While automated annotation fails to prove that a patch is actually correct, author annotation is prone to subjectivity. This concern has caused an on-going debate on appropriate ways to assess the effectiveness of numerous ASR techniques proposed recently. To address this concern, we propose to assess reliability of author and automated annotations on patch correctness assessment. We do this by first constructing a gold set of correctness labels for 189 randomly selected patches generated by 8 state-of-the-art ASR techniques through a user study involving 35 professional developers as independent annotators. By measuring inter-rater agreement as a proxy for annotation quality - as commonly done in the literature - we demonstrate that our constructed gold set is on par with other high-quality gold sets. We then compare labels generated by author and automated annotations with this gold set to assess reliability of the patch assessment methodologies. We subsequently report several findings and highlight implications for future studies.

</details>

<details>

<summary>2018-06-30 01:26:51 - Mammography Assessment using Multi-Scale Deep Classifiers</summary>

- *Ulzee An, Khader Shameer, Lakshmi Subramanian*

- `1807.03095v1` - [abs](http://arxiv.org/abs/1807.03095v1) - [pdf](http://arxiv.org/pdf/1807.03095v1)

> Applying deep learning methods to mammography assessment has remained a challenging topic. Dense noise with sparse expressions, mega-pixel raw data resolution, lack of diverse examples have all been factors affecting performance. The lack of pixel-level ground truths have especially limited segmentation methods in pushing beyond approximately bounding regions. We propose a classification approach grounded in high performance tissue assessment as an alternative to all-in-one localization and assessment models that is also capable of pinpointing the causal pixels. First, the objective of the mammography assessment task is formalized in the context of local tissue classifiers. Then, the accuracy of a convolutional neural net is evaluated on classifying patches of tissue with suspicious findings at varying scales, where highest obtained AUC is above $0.9$. The local evaluations of one such expert tissue classifier is used to augment the results of a heatmap regression model and additionally recover the exact causal regions at high resolution as a saliency image suitable for clinical settings.

</details>


## 2018-07

<details>

<summary>2018-07-05 05:50:11 - Impact of Continuous Integration on Code Reviews</summary>

- *Mohammad Masudur Rahman, Chanchal K. Roy*

- `1807.01851v1` - [abs](http://arxiv.org/abs/1807.01851v1) - [pdf](http://arxiv.org/pdf/1807.01851v1)

> Peer code review and continuous integration often interleave with each other in the modern software quality management. Although several studies investigate how non-technical factors (e.g., reviewer workload), developer participation and even patch size affect the code review process, the impact of continuous integration on code reviews is not yet properly understood. In this paper, we report an exploratory study using 578K automated build entries where we investigate the impact of automated builds on the code reviews. Our investigation suggests that successfully passed builds are more likely to encourage new code review participation in a pull request. Frequently built projects are found to be maintaining a steady level of reviewing activities over the years, which was quite missing from the rarely built projects. Experiments with 26,516 automated build entries reported that our proposed model can identify 64% of the builds that triggered new code reviews later.

</details>

<details>

<summary>2018-07-06 02:59:54 - Homodyne-detector-blinding attack in continuous-variable quantum key distribution</summary>

- *Hao Qin, Rupesh Kumar, Vadim Makarov, Romain Alléaume*

- `1805.01620v2` - [abs](http://arxiv.org/abs/1805.01620v2) - [pdf](http://arxiv.org/pdf/1805.01620v2)

> We propose an efficient strategy to attack a continuous-variable quantum key distribution (CV-QKD) system, that we call homodyne detector blinding. This attack strategy takes advantage of a generic vulnerability of homodyne receivers: a bright light pulse sent on the signal port can lead to a saturation of the detector electronics. While detector saturation has already been proposed to attack CV-QKD, the attack we study in this paper has the additional advantage of not requiring an eavesdropper to be phase locked with the homodyne receiver. We show that under certain conditions, an attacker can use a simple laser, incoherent with the homodyne receiver, to generate bright pulses and bias the excess noise to arbitrary small values, fully comprising CV-QKD security. These results highlight the feasibility and the impact of the detector blinding attack. We finally discuss how to design countermeasures in order to protect against this attack.

</details>

<details>

<summary>2018-07-07 16:55:04 - Patchwork Kriging for Large-scale Gaussian Process Regression</summary>

- *Chiwoo Park, Daniel Apley*

- `1701.06655v4` - [abs](http://arxiv.org/abs/1701.06655v4) - [pdf](http://arxiv.org/pdf/1701.06655v4)

> This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the boundary accuracy of existing local partitioned GP methods. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches.

</details>

<details>

<summary>2018-07-09 14:06:24 - Ultra-Large Repair Search Space with Automatically Mined Templates: the Cardumen Mode of Astor</summary>

- *Matias Martinez, Martin Monperrus*

- `1712.03854v2` - [abs](http://arxiv.org/abs/1712.03854v2) - [pdf](http://arxiv.org/pdf/1712.03854v2)

> Astor is a program repair library which has different modes. In this paper, we present the Cardumen mode of Astor, a repair approach based mined templates that has an ultra-large search space. We evaluate the capacity of Cardumen to discover test-suite adequate patches (aka plausible patches) over the 356 real bugs from Defects4J. Cardumen finds 8935 patches over 77 bugs of Defects4J. This is the largest number of automatically synthesized patches ever reported, all patches being available in an open-science repository. Moreover, Cardumen identifies 8 unique patches, that are patches for Defects4J bugs that were never repaired in the whole history of program repair.

</details>

<details>

<summary>2018-07-10 07:57:41 - Practical Program Repair via Bytecode Mutation</summary>

- *Ali Ghanbari, Lingming Zhang*

- `1807.03512v1` - [abs](http://arxiv.org/abs/1807.03512v1) - [pdf](http://arxiv.org/pdf/1807.03512v1)

> Software debugging is tedious, time-consuming, and even error-prone by itself. So, various automated debugging techniques have been proposed in the literature to facilitate the debugging process. Automated Program Repair (APR) is one of the most recent advances in automated debugging, and can directly produce patches for buggy programs with minimal human intervention. Although various advanced APR techniques (including those that are either search-based or semantic-based) have been proposed, the simplistic mutation-based APR technique, which simply uses pre-defined mutation operators (e.g., changing a>=b into a>b) to mutate programs for finding patches, has not yet been thoroughly studied. In this paper, we implement the first practical bytecode-level APR technique, PraPR, and present the first extensive study on fixing real-world bugs (e.g., Defects4J bugs) using bytecode mutation. The experimental results show that surprisingly even PraPR with only the basic traditional mutators can produce genuine patches for 18 bugs. Furthermore, with our augmented mutators, PraPR is able to produce genuine patches for 43 bugs, significantly outperforming state-of-the-art APR. It is also an order of magnitude faster, indicating a promising future for bytecode-mutation-based APR.

</details>

<details>

<summary>2018-07-10 17:02:16 - Speculative Buffer Overflows: Attacks and Defenses</summary>

- *Vladimir Kiriansky, Carl Waldspurger*

- `1807.03757v1` - [abs](http://arxiv.org/abs/1807.03757v1) - [pdf](http://arxiv.org/pdf/1807.03757v1)

> Practical attacks that exploit speculative execution can leak confidential information via microarchitectural side channels. The recently-demonstrated Spectre attacks leverage speculative loads which circumvent access checks to read memory-resident secrets, transmitting them to an attacker using cache timing or other covert communication channels.   We introduce Spectre1.1, a new Spectre-v1 variant that leverages speculative stores to create speculative buffer overflows. Much like classic buffer overflows, speculative out-of-bounds stores can modify data and code pointers. Data-value attacks can bypass some Spectre-v1 mitigations, either directly or by redirecting control flow. Control-flow attacks enable arbitrary speculative code execution, which can bypass fence instructions and all other software mitigations for previous speculative-execution attacks. It is easy to construct return-oriented-programming (ROP) gadgets that can be used to build alternative attack payloads.   We also present Spectre1.2: on CPUs that do not enforce read/write protections, speculative stores can overwrite read-only data and code pointers to breach sandboxes.   We highlight new risks posed by these vulnerabilities, discuss possible software mitigations, and sketch microarchitectural mechanisms that could serve as hardware defenses. We have not yet evaluated the performance impact of our proposed software and hardware mitigations. We describe the salient vulnerability features and additional hypothetical attack scenarios only to the detail necessary to guide hardware and software vendors in threat analysis and mitigations. We advise users to refer to more user-friendly vendor recommendations for mitigations against speculative buffer overflows or available patches.

</details>

<details>

<summary>2018-07-11 09:06:20 - Medusa: A Scalable Interconnect for Many-Port DNN Accelerators and Wide DRAM Controller Interfaces</summary>

- *Yongming Shen, Tianchu Ji, Michael Ferdman, Peter Milder*

- `1807.04013v1` - [abs](http://arxiv.org/abs/1807.04013v1) - [pdf](http://arxiv.org/pdf/1807.04013v1)

> To cope with the increasing demand and computational intensity of deep neural networks (DNNs), industry and academia have turned to accelerator technologies. In particular, FPGAs have been shown to provide a good balance between performance and energy efficiency for accelerating DNNs. While significant research has focused on how to build efficient layer processors, the computational building blocks of DNN accelerators, relatively little attention has been paid to the on-chip interconnects that sit between the layer processors and the FPGA's DRAM controller.   We observe a disparity between DNN accelerator interfaces, which tend to comprise many narrow ports, and FPGA DRAM controller interfaces, which tend to be wide buses. This mismatch causes traditional interconnects to consume significant FPGA resources. To address this problem, we designed Medusa: an optimized FPGA memory interconnect which transposes data in the interconnect fabric, tailoring the interconnect to the needs of DNN layer processors. Compared to a traditional FPGA interconnect, our design can reduce LUT and FF use by 4.7x and 6.0x, and improves frequency by 1.8x.

</details>

<details>

<summary>2018-07-12 15:42:18 - A segmentation-free isogeometric extended mortar contact method</summary>

- *Thang Xuan Duong, Laura De Lorenzis, Roger A. Sauer*

- `1712.01179v2` - [abs](http://arxiv.org/abs/1712.01179v2) - [pdf](http://arxiv.org/pdf/1712.01179v2)

> This paper presents a new isogeometric mortar contact formulation based on an extended finite element interpolation to capture physical pressure discontinuities at the contact boundary. The so called two-half-pass algorithm is employed, which leads to an unbiased formulation and, when applied to the mortar setting, has the additional advantage that the mortar coupling term is no longer present in the contact forces. As a result, the computationally expensive segmentation at overlapping master-slave element boundaries, usually required in mortar methods (although often simplified with loss of accuracy), is not needed from the outset. For the numerical integration of general contact problems, the so-called refined boundary quadrature is employed, which is based on adaptive partitioning of contact elements along the contact boundary. The contact patch test shows that the proposed formulation passes the test without using either segmentation or refined boundary quadrature. Several numerical examples are presented to demonstrate the robustness and accuracy of the proposed formulation.

</details>

<details>

<summary>2018-07-12 16:14:24 - Symbolic Verification of Cache Side-channel Freedom</summary>

- *Sudipta Chattopadhyay, Abhik Roychoudhury*

- `1807.04701v1` - [abs](http://arxiv.org/abs/1807.04701v1) - [pdf](http://arxiv.org/pdf/1807.04701v1)

> Cache timing attacks allow third-party observers to retrieve sensitive information from program executions. But, is it possible to automatically check the vulnerability of a program against cache timing attacks and then, automatically shield program executions against these attacks? For a given program, a cache configuration and an attack model, our CACHEFIX framework either verifies the cache side-channel freedom of the program or synthesizes a series of patches to ensure cache side-channel freedom during program execution. At the core of our framework is a novel symbolic verification technique based on automated abstraction refinement of cache semantics. The power of such a framework is to allow symbolic reasoning over counterexample traces and to combine it with runtime monitoring for eliminating cache side channels during program execution. Our evaluation with routines from OpenSSL, libfixedtimefixedpoint, GDK and FourQlib libraries reveals that our CACHEFIX approach (dis)proves cache sidechannel freedom within an average of 75 seconds. Besides, in all except one case, CACHEFIX synthesizes all patches within 20 minutes to ensure cache side-channel freedom of the respective routines during execution.

</details>

<details>

<summary>2018-07-20 16:56:40 - Spectre Returns! Speculation Attacks using the Return Stack Buffer</summary>

- *Esmaeil Mohammadian Koruyeh, Khaled Khasawneh, Chengyu Song, Nael Abu-Ghazaleh*

- `1807.07940v1` - [abs](http://arxiv.org/abs/1807.07940v1) - [pdf](http://arxiv.org/pdf/1807.07940v1)

> The recent Spectre attacks exploit speculative execution, a pervasively used feature of modern microprocessors, to allow the exfiltration of sensitive data across protection boundaries. In this paper, we introduce a new Spectre-class attack that we call SpectreRSB. In particular, rather than exploiting the branch predictor unit, SpectreRSB exploits the return stack buffer (RSB), a common predictor structure in modern CPUs used to predict return addresses. We show that both local attacks (within the same process such as Spectre 1) and attacks on SGX are possible by constructing proof of concept attacks. We also analyze additional types of the attack on the kernel or across address spaces and show that under some practical and widely used conditions they are possible. Importantly, none of the known defenses including Retpoline and Intel's microcode patches stop all SpectreRSB attacks. We believe that future system developers should be aware of this vulnerability and consider it in developing defenses against speculation attacks. In particular, on Core-i7 Skylake and newer processors (but not on Intel's Xeon processor line), a patch called RSB refilling is used to address a vulnerability when the RSB underfills; this defense interferes with SpectreRSB's ability to launch attacks that switch into the kernel. We recommend that this patch should be used on all machines to protect against SpectreRSB.

</details>

<details>

<summary>2018-07-20 17:45:14 - On the Automatic Generation of Medical Imaging Reports</summary>

- *Baoyu Jing, Pengtao Xie, Eric Xing*

- `1711.08195v3` - [abs](http://arxiv.org/abs/1711.08195v3) - [pdf](http://arxiv.org/pdf/1711.08195v3)

> Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time- consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the re- ports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the pre- diction of tags and the generation of para- graphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.

</details>

<details>

<summary>2018-07-23 13:31:51 - 3D Convolutional Neural Networks for Tumor Segmentation using Long-range 2D Context</summary>

- *Pawel Mlynarski, Hervé Delingette, Antonio Criminisi, Nicholas Ayache*

- `1807.08599v1` - [abs](http://arxiv.org/abs/1807.08599v1) - [pdf](http://arxiv.org/pdf/1807.08599v1)

> We present an efficient deep learning approach for the challenging task of tumor segmentation in multisequence MR images. In recent years, Convolutional Neural Networks (CNN) have achieved state-of-the-art performances in a large variety of recognition tasks in medical imaging. Because of the considerable computational cost of CNNs, large volumes such as MRI are typically processed by subvolumes, for instance slices (axial, coronal, sagittal) or small 3D patches. In this paper we introduce a CNN-based model which efficiently combines the advantages of the short-range 3D context and the long-range 2D context. To overcome the limitations of specific choices of neural network architectures, we also propose to merge outputs of several cascaded 2D-3D models by a voxelwise voting strategy. Furthermore, we propose a network architecture in which the different MR sequences are processed by separate subnetworks in order to be more robust to the problem of missing MR sequences. Finally, a simple and efficient algorithm for training large CNN models is introduced. We evaluate our method on the public benchmark of the BRATS 2017 challenge on the task of multiclass segmentation of malignant brain tumors. Our method achieves good performances and produces accurate segmentations with median Dice scores of 0.918 (whole tumor), 0.883 (tumor core) and 0.854 (enhancing core). Our approach can be naturally applied to various tasks involving segmentation of lesions or organs.

</details>

<details>

<summary>2018-07-27 03:08:32 - Identifying Patch Correctness in Test-Based Program Repair</summary>

- *Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang*

- `1706.09120v3` - [abs](http://arxiv.org/abs/1706.09120v3) - [pdf](http://arxiv.org/pdf/1706.09120v3)

> Test-based automatic program repair has attracted a lot of attention in recent years. However, the test suites in practice are often too weak to guarantee correctness and existing approaches often generate a large number of incorrect patches.   To reduce the number of incorrect patches generated, we propose a novel approach that heuristically determines the correctness of the generated patches. The core idea is to exploit the behavior similarity of test case executions. The passing tests on original and patched programs are likely to behave similarly while the failing tests on original and patched programs are likely to behave differently. Also, if two tests exhibit similar runtime behavior, the two tests are likely to have the same test results. Based on these observations, we generate new test inputs to enhance the test suites and use their behavior similarity to determine patch correctness.   Our approach is evaluated on a dataset consisting of 139 patches generated from existing program repair systems including jGenProg, Nopol, jKali, ACS and HDRepair. Our approach successfully prevented 56.3\% of the incorrect patches to be generated, without blocking any correct patches.

</details>

<details>

<summary>2018-07-29 21:45:35 - Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer Segmentation in Whole-slide Images</summary>

- *Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Zeya Wang, Wei Dai, Eric P. Xing*

- `1807.11113v1` - [abs](http://arxiv.org/abs/1807.11113v1) - [pdf](http://arxiv.org/pdf/1807.11113v1)

> Convolutional neural networks have led to significant breakthroughs in the domain of medical image analysis. However, the task of breast cancer segmentation in whole-slide images (WSIs) is still underexplored. WSIs are large histopathological images with extremely high resolution. Constrained by the hardware and field of view, using high-magnification patches can slow down the inference process and using low-magnification patches can cause the loss of information. In this paper, we aim to achieve two seemingly conflicting goals for breast cancer segmentation: accurate and fast prediction. We propose a simple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this task. Motivated by the zoom-in operation of a pathologist using a digital microscope, RAZN learns a policy network to decide whether zooming is required in a given region of interest. Because the zoom-in action is selective, RAZN is robust to unbalanced and noisy ground truth labels and can efficiently reduce overfitting. We evaluate our method on a public breast cancer dataset. RAZN outperforms both single-scale and multi-scale baseline approaches, achieving better accuracy at low inference cost.

</details>

<details>

<summary>2018-07-30 11:06:31 - Towards an automated approach for bug fix pattern detection</summary>

- *Fernanda Madeiral, Thomas Durieux, Victor Sobreira, Marcelo Maia*

- `1807.11286v1` - [abs](http://arxiv.org/abs/1807.11286v1) - [pdf](http://arxiv.org/pdf/1807.11286v1)

> The characterization of bug datasets is essential to support the evaluation of automatic program repair tools. In a previous work, we manually studied almost 400 human-written patches (bug fixes) from the Defects4J dataset and annotated them with properties, such as repair patterns. However, manually finding these patterns in different datasets is tedious and time-consuming. To address this activity, we designed and implemented PPD, a detector of repair patterns in patches, which performs source code change analysis at abstract-syntax tree level. In this paper, we report on PPD and its evaluation on Defects4J, where we compare the results from the automated detection with the results from the previous manual analysis. We found that PPD has overall precision of 91% and overall recall of 92%, and we conclude that PPD has the potential to detect as many repair patterns as human manual analysis.

</details>


## 2018-08

<details>

<summary>2018-08-01 14:49:30 - A monolithic approach to fluid-structure interaction based on a hybrid Eulerian-ALE fluid domain decomposition involving cut elements</summary>

- *Benedikt Schott, Christoph Ager, Wolfgang A. Wall*

- `1808.00343v1` - [abs](http://arxiv.org/abs/1808.00343v1) - [pdf](http://arxiv.org/pdf/1808.00343v1)

> A novel method for complex fluid-structure interaction (FSI) involving large structural deformation and motion is proposed. The new approach is based on a hybrid fluid formulation that combines the advantages of purely Eulerian (fixed-grid) and Arbitrary-Lagrangian-Eulerian (ALE moving mesh) formulations in the context of FSI. The structure - as commonly given in Lagrangian description - is surrounded by a fine resolved layer of fluid elements based on an ALE-framework. This ALE-fluid patch, which is embedded in an Eulerian background fluid domain, follows the deformation and motion of the structural interface. This approximation technique is not limited to Finite Element Methods, but can can also be realized within other frameworks like Finite Volume or Discontinuous Galerkin Methods. In this work, the surface coupling between the two disjoint fluid subdomains is imposed weakly using a stabilized Nitsche's technique in a Cut Finite Element Method (CutFEM) framework. At the fluid-solid interface, standard weak coupling of node-matching or non-matching finite element approximations can be utilized. As the fluid subdomains can be meshed independently, a sufficient mesh quality in the vicinity of the common fluid-structure interface can be assured. To our knowledge the proposed method is the only method (despite some overlapping domain decomposition approaches that suffer from other issues) that allows for capturing boundary layers and flow detachment via appropriate grids around largely moving and deforming bodies and is able to do this e.g. without the necessity of costly remeshing procedures. In addition it might also help to safe computational costs as now background grids can be much coarser. Various FSI-cases of rising complexity conclude the work.

</details>

<details>

<summary>2018-08-01 15:07:08 - A Virtual Element Method for 2D linear elastic fracture analysis</summary>

- *Vien Minh Nguyen-Thanh, Xiaoying Zhuang, Hung Nguyen-Xuan, Timon Rabczuk, Peter Wriggers*

- `1808.00355v1` - [abs](http://arxiv.org/abs/1808.00355v1) - [pdf](http://arxiv.org/pdf/1808.00355v1)

> This paper presents the Virtual Element Method (VEM) for the modeling of crack propagation in 2D within the context of linear elastic fracture mechanics (LEFM). By exploiting the advantage of mesh flexibility in the VEM, we establish an adaptive mesh refinement strategy based on the superconvergent patch recovery for triangular, quadrilateral as well as for arbitrary polygonal meshes. For the local stiffness matrix in VEM, we adopt a stabilization term which is stable for both isotropic scaling and ratio. Stress intensity factors (SIFs) of a polygonal mesh are discussed and solved by using the interaction domain integral. The present VEM formulations are finally tested and validated by studying its convergence rate for both continuous and discontinuous problems, and are compared with the optimal convergence rate in the conventional Finite Element Method (FEM). Furthermore, the adaptive mesh refinement strategies used to effectively predict the crack growth with the existence of hanging nodes in nonconforming elements are examined.

</details>

<details>

<summary>2018-08-03 08:07:32 - CAKE: Compact and Accurate K-dimensional representation of Emotion</summary>

- *Corentin Kervadec, Valentin Vielzeuf, Stéphane Pateux, Alexis Lechervy, Frédéric Jurie*

- `1807.11215v2` - [abs](http://arxiv.org/abs/1807.11215v2) - [pdf](http://arxiv.org/pdf/1807.11215v2)

> Numerous models describing the human emotional states have been built by the psychology community. Alongside, Deep Neural Networks (DNN) are reaching excellent performances and are becoming interesting features extraction tools in many computer vision tasks.Inspired by works from the psychology community, we first study the link between the compact two-dimensional representation of the emotion known as arousal-valence, and discrete emotion classes (e.g. anger, happiness, sadness, etc.) used in the computer vision community. It enables to assess the benefits -- in terms of discrete emotion inference -- of adding an extra dimension to arousal-valence (usually named dominance). Building on these observations, we propose CAKE, a 3-dimensional representation of emotion learned in a multi-domain fashion, achieving accurate emotion recognition on several public datasets. Moreover, we visualize how emotions boundaries are organized inside DNN representations and show that DNNs are implicitly learning arousal-valence-like descriptions of emotions. Finally, we use the CAKE representation to compare the quality of the annotations of different public datasets.

</details>

<details>

<summary>2018-08-07 04:08:39 - Survey of Automated Vulnerability Detection and Exploit Generation Techniques in Cyber Reasoning Systems</summary>

- *Teresa Nicole Brooks*

- `1702.06162v4` - [abs](http://arxiv.org/abs/1702.06162v4) - [pdf](http://arxiv.org/pdf/1702.06162v4)

> Software is everywhere, from mission critical systems such as industrial power stations, pacemakers and even household appliances. This growing dependence on technology and the increasing complexity software has serious security implications as it means we are potentially surrounded by software that contain exploitable vulnerabilities. These challenges have made binary analysis an important area of research in computer science and has emphasized the need for building automated analysis systems that can operate at scale, speed and efficacy; all while performing with the skill of a human expert. Though great progress has been made in this area of research, there remains limitations and open challenges to be addressed. Recognizing this need, DARPA sponsored the Cyber Grand Challenge (CGC), a competition to showcase the current state of the art in systems that perform; automated vulnerability detection, exploit generation and software patching. This paper is a survey of the vulnerability detection and exploit generation techniques, underlying technologies and related works of two of the winning systems Mayhem and Mechanical Phish.

</details>

<details>

<summary>2018-08-10 09:30:52 - Out of the Black Box: Properties of deep neural networks and their applications</summary>

- *Nizar Ouarti, David Carmona*

- `1808.04433v1` - [abs](http://arxiv.org/abs/1808.04433v1) - [pdf](http://arxiv.org/pdf/1808.04433v1)

> Deep neural networks are powerful machine learning approaches that have exhibited excellent results on many classification tasks. However, they are considered as black boxes and some of their properties remain to be formalized. In the context of image recognition, it is still an arduous task to understand why an image is recognized or not. In this study, we formalize some properties shared by eight state-of-the-art deep neural networks in order to grasp the principles allowing a given deep neural network to classify an image. Our results, tested on these eight networks, show that an image can be sub-divided into several regions (patches) responding at different degrees of probability (local property). With the same patch, some locations in the image can answer two (or three) orders of magnitude higher than other locations (spatial property). Some locations are activators and others inhibitors (activation-inhibition property). The repetition of the same patch can increase (or decrease) the probability of recognition of an object (cumulative property). Furthermore, we propose a new approach called Deepception that exploits these properties to deceive a deep neural network. We obtain for the VGG-VDD-19 neural network a fooling ratio of 88\%. Thanks to our "Psychophysics" approach, no prior knowledge on the networks architectures is required.

</details>

<details>

<summary>2018-08-22 04:40:36 - Maximising Throughput in a Complex Coal Export System</summary>

- *Mateus Rocha de Paula, Natashia Boland, Andreas Ernst, Alexandre Mendes, Martin Savelsbergh*

- `1808.06044v2` - [abs](http://arxiv.org/abs/1808.06044v2) - [pdf](http://arxiv.org/pdf/1808.06044v2)

> The Port of Newcastle features three coal export terminals, operating primarily in cargo assembly mode, that share a rail network on their inbound side, and a channel on their outbound side. Maximising throughput at a single coal terminal, taking into account its layout, its equipment, and its operating policies, is already challenging, but maximising throughput of the Hunter Valley coal export system as a whole requires that terminals and inbound and outbound shared resources be considered simultaneously. Existing approaches to do so either lack realism or are too computationally demanding to be useful as an everyday planning tool. We present a parallel genetic algorithm to optimise the integrated system. The algorithm models activities in continuous time, can handle practical planning horizons efficiently, and generates solutions that match or improve solutions obtained with the state-of-the-art solvers, whilst vastly outperforming them both in memory usage and running time.

</details>

<details>

<summary>2018-08-28 11:34:10 - CFAAR: Control Flow Alteration to Assist Repair</summary>

- *Chadi Trad, Rawad Abou Assi, Wes Masri, Fadi Zaraket*

- `1808.09229v1` - [abs](http://arxiv.org/abs/1808.09229v1) - [pdf](http://arxiv.org/pdf/1808.09229v1)

> We present CFAAR, a program repair assistance technique that operates by selectively altering the outcome of suspicious predicates in order to yield expected behavior. CFAAR is applicable to defects that are repairable by negating predicates under specific conditions. CFAAR proceeds as follows: 1) it identifies predicates such that negating them at given instances would make the failing tests exhibit correct behavior; 2) for each candidate predicate, it uses the program state information to build a classifier that dictates when the predicate should be negated; 3) for each classifier, it leverages a Decision Tree to synthesize a patch to be presented to the developer. We evaluated our toolset using 149 defects from the IntroClass and Siemens benchmarks. CFAAR identified 91 potential candidate defects and generated plausible patches for 41 of them. Twelve of the patches are believed to be correct, whereas the rest provide repair assistance to the developer.

</details>

<details>

<summary>2018-08-28 14:49:19 - Exploiting the Shape of CAN Data for In-Vehicle Intrusion Detection</summary>

- *Zachariah Tyree, Robert A. Bridges, Frank L. Combs, Michael R. Moore*

- `1808.10840v1` - [abs](http://arxiv.org/abs/1808.10840v1) - [pdf](http://arxiv.org/pdf/1808.10840v1)

> Modern vehicles rely on scores of electronic control units (ECUs) broadcasting messages over a few controller area networks (CANs). Bereft of security features, in-vehicle CANs are exposed to cyber manipulation and multiple researches have proved viable, life-threatening cyber attacks. Complicating the issue, CAN messages lack a common mapping of functions to commands, so packets are observable but not easily decipherable. We present a transformational approach to CAN IDS that exploits the geometric properties of CAN data to inform two novel detectors--one based on distance from a learned, lower dimensional manifold and the other on discontinuities of the manifold over time. Proof-of-concept tests are presented by implementing a potential attack approach on a driving vehicle. The initial results suggest that (1) the first detector requires additional refinement but does hold promise; (2) the second detector gives a clear, strong indicator of the attack; and (3) the algorithms keep pace with high-speed CAN messages. As our approach is data-driven it provides a vehicle-agnostic IDS that eliminates the need to reverse engineer CAN messages and can be ported to an after-market plugin.

</details>

<details>

<summary>2018-08-29 06:06:58 - Precise Condition Synthesis for Program Repair</summary>

- *Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, Lu Zhang*

- `1608.07754v5` - [abs](http://arxiv.org/abs/1608.07754v5) - [pdf](http://arxiv.org/pdf/1608.07754v5)

> Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches.   In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an "if" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects.   We develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 17 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 73.9%, which is significantly higher than previous approaches, which are usually less than 40%.

</details>

<details>

<summary>2018-08-29 09:21:22 - Chest X-ray Inpainting with Deep Generative Models</summary>

- *Ecem Sogancioglu, Shi Hu, Davide Belli, Bram van Ginneken*

- `1809.01471v1` - [abs](http://arxiv.org/abs/1809.01471v1) - [pdf](http://arxiv.org/pdf/1809.01471v1)

> Generative adversarial networks have been successfully applied to inpainting in natural images. However, the current state-of-the-art models have not yet been widely adopted in the medical imaging domain. In this paper, we investigate the performance of three recently published deep learning based inpainting models: context encoders, semantic image inpainting, and the contextual attention model, applied to chest x-rays, as the chest exam is the most commonly performed radiological procedure. We train these generative models on 1.2M 128 $\times$ 128 patches from 60K healthy x-rays, and learn to predict the center 64 $\times$ 64 region in each patch. We test the models on both the healthy and abnormal radiographs. We evaluate the results by visual inspection and comparing the PSNR scores. The outputs of the models are in most cases highly realistic. We show that the methods have potential to enhance and detect abnormalities. In addition, we perform a 2AFC observer study and show that an experienced human observer performs poorly in detecting inpainted regions, particularly those generated by the contextual attention model.

</details>

<details>

<summary>2018-08-29 12:22:07 - Vulnerable Open Source Dependencies: Counting Those That Matter</summary>

- *Ivan Pashchenko, Henrik Plate, Serena Elisa Ponta, Antonino Sabetta, Fabio Massacci*

- `1808.09753v1` - [abs](http://arxiv.org/abs/1808.09753v1) - [pdf](http://arxiv.org/pdf/1808.09753v1)

> BACKGROUND: Vulnerable dependencies are a known problem in today's open-source software ecosystems because OSS libraries are highly interconnected and developers do not always update their dependencies. AIMS: In this paper we aim to present a precise methodology, that combines the code-based analysis of patches with information on build, test, update dates, and group extracted from the very code repository, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. METHOD: To understand the industrial impact of the proposed methodology, we considered the 200 most popular OSS Java libraries used by SAP in its own software. Our analysis included 10905 distinct GAVs (group, artifact, version) when considering all the library versions. RESULTS: We found that about 20% of the dependencies affected by a known vulnerability are not deployed, and therefore, they do not represent a danger to the analyzed library because they cannot be exploited in practice. Developers of the analyzed libraries are able to fix (and actually responsible for) 82% of the deployed vulnerable dependencies. The vast majority (81%) of vulnerable dependencies may be fixed by simply updating to a new version, while 1% of the vulnerable dependencies in our sample are halted, and therefore, potentially require a costly mitigation strategy. CONCLUSIONS: Our case study shows that the correct counting allows software development companies to receive actionable information about their library dependencies, and therefore, correctly allocate costly development and audit resources, which is spent inefficiently in case of distorted measurements.

</details>


## 2018-09

<details>

<summary>2018-09-07 19:41:23 - Kernel Graph Convolutional Neural Networks</summary>

- *Giannis Nikolentzos, Polykarpos Meladianos, Antoine Jean-Pierre Tixier, Konstantinos Skianis, Michalis Vazirgiannis*

- `1710.10689v2` - [abs](http://arxiv.org/abs/1710.10689v2) - [pdf](http://arxiv.org/pdf/1710.10689v2)

> Graph kernels have been successfully applied to many graph classification problems. Typically, a kernel is first designed, and then an SVM classifier is trained based on the features defined implicitly by this kernel. This two-stage approach decouples data representation from learning, which is suboptimal. On the other hand, Convolutional Neural Networks (CNNs) have the capability to learn their own features directly from the raw data during training. Unfortunately, they cannot handle irregular data such as graphs. We address this challenge by using graph kernels to embed meaningful local neighborhoods of the graphs in a continuous vector space. A set of filters is then convolved with these patches, pooled, and the output is then passed to a feedforward network. With limited parameter tuning, our approach outperforms strong baselines on 7 out of 10 benchmark datasets.

</details>

<details>

<summary>2018-09-21 09:31:45 - Exemplar-based synthesis of geology using kernel discrepancies and generative neural networks</summary>

- *Shing Chan, Ahmed H. Elsheikh*

- `1809.07748v2` - [abs](http://arxiv.org/abs/1809.07748v2) - [pdf](http://arxiv.org/pdf/1809.07748v2)

> We propose a framework for synthesis of geological images based on an exemplar image. We synthesize new realizations such that the discrepancy in the patch distribution between the realizations and the exemplar image is minimized. Such discrepancy is quantified using a kernel method for two-sample test called maximum mean discrepancy. To enable fast synthesis, we train a generative neural network in an offline phase to sample realizations efficiently during deployment, while also providing a parametrization of the synthesis process. We assess the framework on a classical binary image representing channelized subsurface reservoirs, finding that the method reproduces the visual patterns and spatial statistics (image histogram and two-point probability functions) of the exemplar image.

</details>

<details>

<summary>2018-09-21 09:59:16 - Towards a Mini-App for Smoothed Particle Hydrodynamics at Exascale</summary>

- *Danilo Guerrera, Rubén M. Cabezón, Jean-Guillaume Piccinali, Aurélien Cavelan, Florina M. Ciorba, David Imbert, Lucio Mayer, Darren Reed*

- `1809.08013v1` - [abs](http://arxiv.org/abs/1809.08013v1) - [pdf](http://arxiv.org/pdf/1809.08013v1)

> The smoothed particle hydrodynamics (SPH) technique is a purely Lagrangian method, used in numerical simulations of fluids in astrophysics and computational fluid dynamics, among many other fields. SPH simulations with detailed physics represent computationally-demanding calculations. The parallelization of SPH codes is not trivial due to the absence of a structured grid. Additionally, the performance of the SPH codes can be, in general, adversely impacted by several factors, such as multiple time-stepping, long-range interactions, and/or boundary conditions. This work presents insights into the current performance and functionalities of three SPH codes: SPHYNX, ChaNGa, and SPH-flow. These codes are the starting point of an interdisciplinary co-design project, SPH-EXA, for the development of an Exascale-ready SPH mini-app. To gain such insights, a rotating square patch test was implemented as a common test simulation for the three SPH codes and analyzed on two modern HPC systems. Furthermore, to stress the differences with the codes stemming from the astrophysics community (SPHYNX and ChaNGa), an additional test case, the Evrard collapse, has also been carried out. This work extrapolates the common basic SPH features in the three codes for the purpose of consolidating them into a pure-SPH, Exascale-ready, optimized, mini-app. Moreover, the outcome of this serves as direct feedback to the parent codes, to improve their performance and overall scalability.

</details>

<details>

<summary>2018-09-21 18:52:49 - (k,p)-Planarity: A Relaxation of Hybrid Planarity</summary>

- *Emilio Di Giacomo, William J. Lenhart, Giuseppe Liotta, Timothy W. Randolph, Alessandra Tappini*

- `1806.11413v2` - [abs](http://arxiv.org/abs/1806.11413v2) - [pdf](http://arxiv.org/pdf/1806.11413v2)

> We present a new model for hybrid planarity that relaxes existing hybrid representations. A graph $G = (V,E)$ is $(k,p)$-planar if $V$ can be partitioned into clusters of size at most $k$ such that $G$ admits a drawing where: (i) each cluster is associated with a closed, bounded planar region, called a cluster region; (ii) cluster regions are pairwise disjoint, (iii) each vertex $v \in V$ is identified with at most $p$ distinct points, called \emph{ports}, on the boundary of its cluster region; (iv) each inter-cluster edge $(u,v) \in E$ is identified with a Jordan arc connecting a port of $u$ to a port of $v$; (v) inter-cluster edges do not cross or intersect cluster regions except at their endpoints. We first tightly bound the number of edges in a $(k,p)$-planar graph with $p<k$. We then prove that $(4,1)$-planarity testing and $(2,2)$-planarity testing are NP-complete problems. Finally, we prove that neither the class of $(2,2)$-planar graphs nor the class of $1$-planar graphs contains the other, indicating that the $(k,p)$-planar graphs are a large and novel class.

</details>

<details>

<summary>2018-09-24 09:06:37 - Object segmentation in depth maps with one user click and a synthetically trained fully convolutional network</summary>

- *Matthieu Grard, Romain Brégier, Florian Sella, Emmanuel Dellandréa, Liming Chen*

- `1801.01281v2` - [abs](http://arxiv.org/abs/1801.01281v2) - [pdf](http://arxiv.org/pdf/1801.01281v2)

> With more and more household objects built on planned obsolescence and consumed by a fast-growing population, hazardous waste recycling has become a critical challenge. Given the large variability of household waste, current recycling platforms mostly rely on human operators to analyze the scene, typically composed of many object instances piled up in bulk. Helping them by robotizing the unitary extraction is a key challenge to speed up this tedious process. Whereas supervised deep learning has proven very efficient for such object-level scene understanding, e.g., generic object detection and segmentation in everyday scenes, it however requires large sets of per-pixel labeled images, that are hardly available for numerous application contexts, including industrial robotics. We thus propose a step towards a practical interactive application for generating an object-oriented robotic grasp, requiring as inputs only one depth map of the scene and one user click on the next object to extract. More precisely, we address in this paper the middle issue of object seg-mentation in top views of piles of bulk objects given a pixel location, namely seed, provided interactively by a human operator. We propose a twofold framework for generating edge-driven instance segments. First, we repurpose a state-of-the-art fully convolutional object contour detector for seed-based instance segmentation by introducing the notion of edge-mask duality with a novel patch-free and contour-oriented loss function. Second, we train one model using only synthetic scenes, instead of manually labeled training data. Our experimental results show that considering edge-mask duality for training an encoder-decoder network, as we suggest, outperforms a state-of-the-art patch-based network in the present application context.

</details>

<details>

<summary>2018-09-28 21:52:56 - Predicting Destinations by Nearest Neighbor Search on Training Vessel Routes</summary>

- *Valentin Roşca, Emanuel Onica, Paul Diac, Ciprian Amariei*

- `1810.00096v1` - [abs](http://arxiv.org/abs/1810.00096v1) - [pdf](http://arxiv.org/pdf/1810.00096v1)

> The DEBS Grand Challenge 2018 is set in the context of maritime route prediction. Vessel routes are modeled as streams of Automatic Identification System (AIS) data points selected from real-world tracking data. The challenge requires to correctly estimate the destination ports and arrival times of vessel trips, as early as possible. Our proposed solution partitions the training vessel routes by reported destination port and uses a nearest neighbor search to find the training routes that are closer to the query AIS point. Particular improvements have been included as well, such as a way to avoid changing the predicted ports frequently within one query route and automating the parameters tuning by the use of a genetic algorithm. This leads to significant improvements on the final score.

</details>

<details>

<summary>2018-09-30 00:24:22 - Community-Based Security for the Internet of Things</summary>

- *Quanyan Zhu, Stefan Rass, Peter Schartner*

- `1810.00281v1` - [abs](http://arxiv.org/abs/1810.00281v1) - [pdf](http://arxiv.org/pdf/1810.00281v1)

> With more and more devices becoming connectable to the internet, the number of services but also a lot of threats increases dramatically. Security is often a secondary matter behind functionality and comfort, but the problem has already been recognized. Still, with many IoT devices being deployed already, security will come step-by-step and through updates, patches and new versions of apps and IoT software. While these updates can be safely retrieved from app stores, the problems kick in via jailbroken devices and with the variety of untrusted sources arising on the internet. Since hacking is typically a community effort? these days, security could be a community goal too. The challenges are manifold, and one reason for weak or absent security on IoT devices is their weak computational power. In this chapter, we discuss a community based security mechanism in which devices mutually aid each other in secure software management. We discuss game-theoretic methods of community formation and light-weight cryptographic means to accomplish authentic software deployment inside the IoT device community.

</details>


## 2018-10

<details>

<summary>2018-10-04 08:53:42 - Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling</summary>

- *Jaejin Cho, Murali Karthick Baskar, Ruizhi Li, Matthew Wiesner, Sri Harish Mallidi, Nelson Yalta, Martin Karafiat, Shinji Watanabe, Takaaki Hori*

- `1810.03459v1` - [abs](http://arxiv.org/abs/1810.03459v1) - [pdf](http://arxiv.org/pdf/1810.03459v1)

> Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multi-lingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.

</details>

<details>

<summary>2018-10-08 11:33:13 - Identification of promoted eclipse unstable interfaces using clone detection technique</summary>

- *Simon Kawuma, Evarist Nabaasa*

- `1810.03381v1` - [abs](http://arxiv.org/abs/1810.03381v1) - [pdf](http://arxiv.org/pdf/1810.03381v1)

> The Eclipse framework is a popular and widely used framework that has been evolving for over a decade. The framework provides both stable interfaces (APIs) and unstable interfaces (non-APIs). Despite being discouraged by Eclipse, client developers often use non-APIs which may cause their systems to fail when ported to new framework releases. To overcome this problem, Eclipse interface producers may promote unstable interfaces to APIs. However, client developers have no assistance to aid them to identify the promoted unstable interfaces in the Eclipse framework. We aim to help API users identify promoted unstable interfaces. We used the clone detection technique to identify promoted unstable interfaces as the framework evolves. Our empirical investigation on 16 Eclipse major releases presents the following observations. First, we have discovered that there exists over 60% non-API methods of the total interfaces in each of the analyzed 16 Eclipse releases. Second, we have discovered that the percentage of promoted non-APIs identified through clone detection ranges from 0.20% to 10.38%.

</details>

<details>

<summary>2018-10-12 15:14:00 - Grand Challenge: Real-time Destination and ETA Prediction for Maritime Traffic</summary>

- *Oleh Bodunov, Florian Schmidt, André Martin, Andrey Brito, Christof Fetzer*

- `1810.05567v1` - [abs](http://arxiv.org/abs/1810.05567v1) - [pdf](http://arxiv.org/pdf/1810.05567v1)

> In this paper, we present our approach for solving the DEBS Grand Challenge 2018. The challenge asks to provide a prediction for (i) a destination and the (ii) arrival time of ships in a streaming-fashion using Geo-spatial data in the maritime context. Novel aspects of our approach include the use of ensemble learning based on Random Forest, Gradient Boosting Decision Trees (GBDT), XGBoost Trees and Extremely Randomized Trees (ERT) in order to provide a prediction for a destination while for the arrival time, we propose the use of Feed-forward Neural Networks. In our evaluation, we were able to achieve an accuracy of 97% for the port destination classification problem and 90% (in mins) for the ETA prediction.

</details>

<details>

<summary>2018-10-13 06:35:18 - Human-competitive Patches in Automatic Program Repair with Repairnator</summary>

- *Martin Monperrus, Simon Urli, Thomas Durieux, Matias Martinez, Benoit Baudry, Lionel Seinturier*

- `1810.05806v1` - [abs](http://arxiv.org/abs/1810.05806v1) - [pdf](http://arxiv.org/pdf/1810.05806v1)

> Repairnator is a bot. It constantly monitors software bugs discovered during continuous integration of open-source software and tries to fix them automatically. If it succeeds to synthesize a valid patch, Repairnator proposes the patch to the human developers, disguised under a fake human identity. To date, Repairnator has been able to produce 5 patches that were accepted by the human developers and permanently merged in the code base. This is a milestone for human-competitiveness in software engineering research on automatic program repair.

</details>

<details>

<summary>2018-10-16 14:21:18 - Optimizing AIREBO: Navigating the Journey from Complex Legacy Code to High Performance</summary>

- *Markus Höhnerbach, Paolo Bientinesi*

- `1810.07026v1` - [abs](http://arxiv.org/abs/1810.07026v1) - [pdf](http://arxiv.org/pdf/1810.07026v1)

> Despite initiatives to improve the quality of scientific codes, there still is a large presence of legacy code. Such code often needs to implement a lot of functionality under time constrains, sacrificing quality. Additionally, quality is rarely improved by optimizations for new architectures. This development model leads to code that is increasingly difficult to work with. Our suggested solution includes complexity-reducing refactoring and hardware abstraction. We focus on the AIREBO potential from LAMMPS, where the challenge is that any potential kernel is rather large and complex, hindering systematic optimization. This issue is common to codes that model multiple physical phenomena. We present our journey from the C++ port of a previous Fortran code to performance-portable, KNC-hybrid, vectorized, scalable, optimized code supporting full and reduced precision. The journey includes extensive testing that fixed bugs in the original code. Large-scale, full-precision runs sustain speedups of more than 4x (KNL) and 3x (Skylake).

</details>

<details>

<summary>2018-10-24 15:07:16 - Entropy in Quantum Information Theory -- Communication and Cryptography</summary>

- *Christian Majenz*

- `1810.10436v1` - [abs](http://arxiv.org/abs/1810.10436v1) - [pdf](http://arxiv.org/pdf/1810.10436v1)

> In this Thesis, several results in quantum information theory are collected, most of which use entropy as the main mathematical tool. *While a direct generalization of the Shannon entropy to density matrices, the von Neumann entropy behaves differently. A long-standing open question is, whether there are quantum analogues of unconstrained non-Shannon type inequalities. Here, a new constrained non-von-Neumann type inequality is proven, a step towards a conjectured unconstrained inequality by Linden and Winter. *IID quantum state merging can be optimally achieved using the decoupling technique. The one-shot results by Berta et al. and Anshu at al., however, had to bring in additional mathematical machinery. We introduce a natural generalized decoupling paradigm, catalytic decoupling, that can reproduce the aforementioned results when used analogously to the application of standard decoupling in the asymptotic case. *Port based teleportation, a variant of standard quantum teleportation protocol, cannot be implemented perfectly. We prove several lower bounds on the necessary number of output ports N to achieve port based teleportation for given error and input dimension, showing that N diverges uniformly in the dimension of the teleported quantum system, for vanishing error. As a byproduct, a new lower bound for the size of the program register for an approximate universal programmable quantum processor is derived. *In the last part, we give a new definition for information-theoretic quantum non-malleability, strengthening the previous definition by Ambainis et al. We show that quantum non-malleability implies secrecy, analogous to quantum authentication. Furthermore, non-malleable encryption schemes can be used as a primitive to build authenticating encryption schemes. We also show that the strong notion of authentication recently proposed by Garg et al. can be fulfilled using 2-designs.

</details>

<details>

<summary>2018-10-24 20:43:26 - Alleviating Patch Overfitting with Automatic Test Generation: A Study of Feasibility and Effectiveness for the Nopol Repair System</summary>

- *Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux, Martin Monperrus*

- `1810.10614v1` - [abs](http://arxiv.org/abs/1810.10614v1) - [pdf](http://arxiv.org/pdf/1810.10614v1)

> Among the many different kinds of program repair techniques, one widely studied family of techniques is called test suite based repair. However, test suites are in essence input-output specifications and are thus typically inadequate for completely specifying the expected behavior of the program under repair. Consequently, the patches generated by test suite based repair techniques can just overfit to the used test suite, and fail to generalize to other tests. We deeply analyze the overfitting problem in program repair and give a classification of this problem. This classification will help the community to better understand and design techniques to defeat the overfitting problem. We further propose and evaluate an approach called UnsatGuided, which aims to alleviate the overfitting problem for synthesis-based repair techniques with automatic test case generation. The approach uses additional automatically generated tests to strengthen the repair constraint used by synthesis-based repair techniques. We analyze the effectiveness of UnsatGuided: 1) analytically with respect to alleviating two different kinds of overfitting issues; 2) empirically based on an experiment over the 224 bugs of the Defects4J repository. The main result is that automatic test generation is effective in alleviating one kind of overfitting issue--regression introduction, but due to oracle problem, has minimal positive impact on alleviating the other kind of overfitting issue--incomplete fixing.

</details>


## 2018-11

<details>

<summary>2018-11-01 13:40:55 - BENGAL: An Automatic Benchmark Generator for Entity Recognition and Linking</summary>

- *Axel-Cyrille Ngonga Ngomo, Michael Röder, Diego Moussallem, Ricardo Usbeck, René Speck*

- `1710.08691v3` - [abs](http://arxiv.org/abs/1710.08691v3) - [pdf](http://arxiv.org/pdf/1710.08691v3)

> The manual creation of gold standards for named entity recognition and entity linking is time- and resource-intensive. Moreover, recent works show that such gold standards contain a large proportion of mistakes in addition to being difficult to maintain. We hence present BENGAL, a novel automatic generation of such gold standards as a complement to manually created benchmarks. The main advantage of our benchmarks is that they can be readily generated at any time. They are also cost-effective while being guaranteed to be free of annotation errors. We compare the performance of 11 tools on benchmarks in English generated by BENGAL and on 16benchmarks created manually. We show that our approach can be ported easily across languages by presenting results achieved by 4 tools on both Brazilian Portuguese and Spanish. Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing benchmarks. Our approach is open-source. Our experimental results are available at http://faturl.com/bengalexpinlg and the code at https://github.com/dice-group/BENGAL.

</details>

<details>

<summary>2018-11-02 01:54:47 - Batch Normalization Sampling</summary>

- *Zhaodong Chen, Lei Deng, Guoqi Li, Jiawei Sun, Xing Hu, Xin Ma, Yuan Xie*

- `1810.10962v2` - [abs](http://arxiv.org/abs/1810.10962v2) - [pdf](http://arxiv.org/pdf/1810.10962v2)

> Deep Neural Networks (DNNs) thrive in recent years in which Batch Normalization (BN) plays an indispensable role. However, it has been observed that BN is costly due to the reduction operations. In this paper, we propose alleviating this problem through sampling only a small fraction of data for normalization at each iteration. Specifically, we model it as a statistical sampling problem and identify that by sampling less correlated data, we can largely reduce the requirement of the number of data for statistics estimation in BN, which directly simplifies the reduction operations. Based on this conclusion, we propose two sampling strategies, "Batch Sampling" (randomly select several samples from each batch) and "Feature Sampling" (randomly select a small patch from each feature map of all samples), that take both computational efficiency and sample correlation into consideration. Furthermore, we introduce an extremely simple variant of BN, termed as Virtual Dataset Normalization (VDN), that can normalize the activations well with few synthetical random samples. All the proposed methods are evaluated on various datasets and networks, where an overall training speedup by up to 20% on GPU is practically achieved without the support of any specialized libraries, and the loss on accuracy and convergence rate are negligible. Finally, we extend our work to the "micro-batch normalization" problem and yield comparable performance with existing approaches at the case of tiny batch size.

</details>

<details>

<summary>2018-11-02 22:52:22 - Neural Task Representations as Weak Supervision for Model Agnostic Cross-Lingual Transfer</summary>

- *Sujay Kumar Jauhar, Michael Gamon, Patrick Pantel*

- `1811.01115v1` - [abs](http://arxiv.org/abs/1811.01115v1) - [pdf](http://arxiv.org/pdf/1811.01115v1)

> Natural language processing is heavily Anglo-centric, while the demand for models that work in languages other than English is greater than ever. Yet, the task of transferring a model from one language to another can be expensive in terms of annotation costs, engineering time and effort. In this paper, we present a general framework for easily and effectively transferring neural models from English to other languages. The framework, which relies on task representations as a form of weak supervision, is model and task agnostic, meaning that many existing neural architectures can be ported to other languages with minimal effort. The only requirement is unlabeled parallel data, and a loss defined over task representations. We evaluate our framework by transferring an English sentiment classifier to three different languages. On a battery of tests, we show that our models outperform a number of strong baselines and rival state-of-the-art results, which rely on more complex approaches and significantly more resources and data. Additionally, we find that the framework proposed in this paper is able to capture semantically rich and meaningful representations across languages, despite the lack of direct supervision.

</details>

<details>

<summary>2018-11-03 18:58:50 - On GANs and GMMs</summary>

- *Eitan Richardson, Yair Weiss*

- `1805.12462v2` - [abs](http://arxiv.org/abs/1805.12462v2) - [pdf](http://arxiv.org/pdf/1805.12462v2)

> A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution ("mode collapse") and that using the learned models for anything other than generating samples may be very difficult. In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images.

</details>

<details>

<summary>2018-11-04 14:31:05 - Automatic Repair of Real Bugs in Java: A Large-Scale Experiment on the Defects4J Dataset</summary>

- *Matias Martinez, Thomas Durieux, Romain Sommerard, Jifeng Xuan, Martin Monperrus*

- `1811.02429v1` - [abs](http://arxiv.org/abs/1811.02429v1) - [pdf](http://arxiv.org/pdf/1811.02429v1)

> Defects4J is a large, peer-reviewed, structured dataset of real-world Java bugs. Each bug in Defects4J comes with a test suite and at least one failing test case that triggers the bug. In this paper, we report on an experiment to explore the effectiveness of automatic test-suite based repair on Defects4J. The result of our experiment shows that the considered state-of-the-art repair methods can generate patches for 47 out of 224 bugs. However, those patches are only test-suite adequate, which means that they pass the test suite and may potentially be incorrect beyond the test-suite satisfaction correctness criterion. We have manually analyzed 84 different patches to assess their real correctness. In total, 9 real Java bugs can be correctly repaired with test-suite based repair. This analysis shows that test-suite based repair suffers from under-specified bugs, for which trivial or incorrect patches still pass the test suite. With respect to practical applicability, it takes on average 14.8 minutes to find a patch. The experiment was done on a scientific grid, totaling 17.6 days of computation time. All the repair systems and experimental results are publicly available on Github in order to facilitate future research on automatic repair.

</details>

<details>

<summary>2018-11-06 11:26:35 - Defeating the Downgrade Attack on Identity Privacy in 5G</summary>

- *Mohsin Khan, Philip Ginzboorg, Kimmo Järvinen, Valtteri Niemi*

- `1811.02293v1` - [abs](http://arxiv.org/abs/1811.02293v1) - [pdf](http://arxiv.org/pdf/1811.02293v1)

> 3GPP Release 15, the first 5G standard, includes protection of user identity privacy against IMSI catchers. These protection mechanisms are based on public key encryption. Despite this protection, IMSI catching is still possible in LTE networks which opens the possibility of a downgrade attack on user identity privacy, where a fake LTE base station obtains the identity of a 5G user equipment. We propose (i) to use an existing pseudonym-based solution to protect user identity privacy of 5G user equipment against IMSI catchers in LTE and (ii) to include a mechanism for updating LTE pseudonyms in the public key encryption based 5G identity privacy procedure. The latter helps to recover from a loss of synchronization of LTE pseudonyms. Using this mechanism, pseudonyms in the user equipment and home network are automatically synchronized when the user equipment connects to 5G. Our mechanisms utilize existing LTE and 3GPP Release 15 messages and require modifications only in the user equipment and home network in order to provide identity privacy. Additionally, lawful interception requires minor patching in the serving network.

</details>

<details>

<summary>2018-11-09 00:23:48 - AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume Segmentation of Head and Neck Anatomy</summary>

- *Wentao Zhu, Yufang Huang, Liang Zeng, Xuming Chen, Yong Liu, Zhen Qian, Nan Du, Wei Fan, Xiaohui Xie*

- `1808.05238v2` - [abs](http://arxiv.org/abs/1808.05238v2) - [pdf](http://arxiv.org/pdf/1808.05238v2)

> Methods: Our deep learning model, called AnatomyNet, segments OARs from head and neck CT images in an end-to-end fashion, receiving whole-volume HaN CT images as input and generating masks of all OARs of interest in one shot. AnatomyNet is built upon the popular 3D U-net architecture, but extends it in three important ways: 1) a new encoding scheme to allow auto-segmentation on whole-volume CT images instead of local patches or subsets of slices, 2) incorporating 3D squeeze-and-excitation residual blocks in encoding layers for better feature representation, and 3) a new loss function combining Dice scores and focal loss to facilitate the training of the neural model. These features are designed to address two main challenges in deep-learning-based HaN segmentation: a) segmenting small anatomies (i.e., optic chiasm and optic nerves) occupying only a few slices, and b) training with inconsistent data annotations with missing ground truth for some anatomical structures.   Results: We collected 261 HaN CT images to train AnatomyNet, and used MICCAI Head and Neck Auto Segmentation Challenge 2015 as a benchmark dataset to evaluate the performance of AnatomyNet. The objective is to segment nine anatomies: brain stem, chiasm, mandible, optic nerve left, optic nerve right, parotid gland left, parotid gland right, submandibular gland left, and submandibular gland right. Compared to previous state-of-the-art results from the MICCAI 2015 competition, AnatomyNet increases Dice similarity coefficient by 3.3% on average. AnatomyNet takes about 0.12 seconds to fully segment a head and neck CT image of dimension 178 x 302 x 225, significantly faster than previous methods. In addition, the model is able to process whole-volume CT images and delineate all OARs in one pass, requiring little pre- or post-processing. https://github.com/wentaozhu/AnatomyNet-for-anatomical-segmentation.git.

</details>

<details>

<summary>2018-11-10 07:59:04 - Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs</summary>

- *Jifeng Xuan, Matias Martinez, Favio Demarco, Maxime Clément, Sebastian Lamelas, Thomas Durieux, Daniel Le Berre, Martin Monperrus*

- `1811.04211v1` - [abs](http://arxiv.org/abs/1811.04211v1) - [pdf](http://arxiv.org/pdf/1811.04211v1)

> We propose NOPOL, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of NOPOL consists of three major phases. First, NOPOL employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, NOPOL encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem, then a feasible solution to the SMT instance is translated back into a code patch. We evaluate NOPOL on 22 real-world bugs (16 bugs with buggy IF conditions and 6 bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy IF conditions and missing preconditions. We illustrate the capabilities and limitations of NOPOL using case studies of real bug fixes.

</details>

<details>

<summary>2018-11-14 16:58:25 - Blockchain-based Firmware Update Scheme Tailored for Autonomous Vehicles</summary>

- *Mohamed Baza, Mahmoud Nabil, Noureddine Lasla, Kemal Fidan, Mohamed Mahmoud, Mohamed Abdallah*

- `1811.05905v1` - [abs](http://arxiv.org/abs/1811.05905v1) - [pdf](http://arxiv.org/pdf/1811.05905v1)

> Recently, Autonomous Vehicles (AVs) have gained extensive attention from both academia and industry. AVs are a complex system composed of many subsystems, making them a typical target for attackers. Therefore, the firmware of the different subsystems needs to be updated to the latest version by the manufacturer to fix bugs and introduce new features, e.g., using security patches. In this paper, we propose a distributed firmware update scheme for the AVs' subsystems, leveraging blockchain and smart contract technology. A consortium blockchain made of different AVs manufacturers is used to ensure the authenticity and integrity of firmware updates. Instead of depending on centralized third parties to distribute the new updates, we enable AVs, namely distributors, to participate in the distribution process and we take advantage of their mobility to guarantee high availability and fast delivery of the updates. To incentivize AVs to distribute the updates, a reward system is established that maintains a credit reputation for each distributor account in the blockchain. A zero-knowledge proof protocol is used to exchange the update in return for a proof of distribution in a trust-less environment. Moreover, we use attribute-based encryption (ABE) scheme to ensure that only authorized AVs will be able to download and use a new update. Our analysis indicates that the additional cryptography primitives and exchanged transactions do not affect the operation of the AVs network. Also, our security analysis demonstrates that our scheme is efficient and secure against different attacks.

</details>

<details>

<summary>2018-11-15 04:15:34 - Plan Interdiction Games</summary>

- *Yevgeniy Vorobeychik, Michael Pritchard*

- `1811.06162v1` - [abs](http://arxiv.org/abs/1811.06162v1) - [pdf](http://arxiv.org/pdf/1811.06162v1)

> We propose a framework for cyber risk assessment and mitigation which models attackers as formal planners and defenders as interdicting such plans. We illustrate the value of plan interdiction problems by first modeling network cyber risk through the use of formal planning, and subsequently formalizing an important question of prioritizing vulnerabilities for patching in the plan interdiction framework. In particular, we show that selectively patching relatively few vulnerabilities allows a network administrator to significantly reduce exposure to cyber risk. More broadly, we have developed a number of scalable approaches for plan interdiction problems, making especially significant advances when attack plans involve uncertainty about system dynamics. However, important open problems remain, including how to effectively capture information asymmetry between the attacker and defender, how to best model dynamics in the attacker-defender interaction, and how to develop scalable algorithms for solving associated plan interdiction games.

</details>

<details>

<summary>2018-11-15 17:49:50 - Development and Validation of a Deep Learning Algorithm for Improving Gleason Scoring of Prostate Cancer</summary>

- *Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan, Chen, Ellery Wulczyn, Fraser Tan, Niels Olson, Jenny L. Smith, Arash Mohtashamian, James H. Wren, Greg S. Corrado, Robert MacDonald, Lily H. Peng, Mahul B. Amin, Andrew J. Evans, Ankur R. Sangoi, Craig H. Mermel, Jason D. Hipp, Martin C. Stumpe*

- `1811.06497v1` - [abs](http://arxiv.org/abs/1811.06497v1) - [pdf](http://arxiv.org/pdf/1811.06497v1)

> For prostate cancer patients, the Gleason score is one of the most important prognostic factors, potentially determining treatment independent of the stage. However, Gleason scoring is based on subjective microscopic examination of tumor morphology and suffers from poor reproducibility. Here we present a deep learning system (DLS) for Gleason scoring whole-slide images of prostatectomies. Our system was developed using 112 million pathologist-annotated image patches from 1,226 slides, and evaluated on an independent validation dataset of 331 slides, where the reference standard was established by genitourinary specialist pathologists. On the validation dataset, the mean accuracy among 29 general pathologists was 0.61. The DLS achieved a significantly higher diagnostic accuracy of 0.70 (p=0.002) and trended towards better patient risk stratification in correlations to clinical follow-up data. Our approach could improve the accuracy of Gleason scoring and subsequent therapy decisions, particularly where specialist expertise is unavailable. The DLS also goes beyond the current Gleason system to more finely characterize and quantitate tumor morphology, providing opportunities for refinement of the Gleason system itself.

</details>

<details>

<summary>2018-11-18 02:39:45 - GLStyleNet: Higher Quality Style Transfer Combining Global and Local Pyramid Features</summary>

- *Zhizhong Wang, Lei Zhao, Wei Xing, Dongming Lu*

- `1811.07260v1` - [abs](http://arxiv.org/abs/1811.07260v1) - [pdf](http://arxiv.org/pdf/1811.07260v1)

> Recent studies using deep neural networks have shown remarkable success in style transfer especially for artistic and photo-realistic images. However, the approaches using global feature correlations fail to capture small, intricate textures and maintain correct texture scales of the artworks, and the approaches based on local patches are defective on global effect. In this paper, we present a novel feature pyramid fusion neural network, dubbed GLStyleNet, which sufficiently takes into consideration multi-scale and multi-level pyramid features by best aggregating layers across a VGG network, and performs style transfer hierarchically with multiple losses of different scales. Our proposed method retains high-frequency pixel information and low frequency construct information of images from two aspects: loss function constraint and feature fusion. Our approach is not only flexible to adjust the trade-off between content and style, but also controllable between global and local. Compared to state-of-the-art methods, our method can transfer not just large-scale, obvious style cues but also subtle, exquisite ones, and dramatically improves the quality of style transfer. We demonstrate the effectiveness of our approach on portrait style transfer, artistic style transfer, photo-realistic style transfer and Chinese ancient painting style transfer tasks. Experimental results indicate that our unified approach improves image style transfer quality over previous state-of-the-art methods, while also accelerating the whole process in a certain extent. Our code is available at https://github.com/EndyWon/GLStyleNet.

</details>

<details>

<summary>2018-11-20 12:56:39 - A Semi-supervised Spatial Spectral Regularized Manifold Local Scaling Cut With HGF for Dimensionality Reduction of Hyperspectral Images</summary>

- *Ramanarayan Mohanty, SL Happy, Aurobinda Routray*

- `1811.08223v1` - [abs](http://arxiv.org/abs/1811.08223v1) - [pdf](http://arxiv.org/pdf/1811.08223v1)

> Hyperspectral images (HSI) contain a wealth of information over hundreds of contiguous spectral bands, making it possible to classify materials through subtle spectral discrepancies. However, the classification of this rich spectral information is accompanied by the challenges like high dimensionality, singularity, limited training samples, lack of labeled data samples, heteroscedasticity and nonlinearity. To address these challenges, we propose a semi-supervised graph based dimensionality reduction method named `semi-supervised spatial spectral regularized manifold local scaling cut' (S3RMLSC). The underlying idea of the proposed method is to exploit the limited labeled information from both the spectral and spatial domains along with the abundant unlabeled samples to facilitate the classification task by retaining the original distribution of the data. In S3RMLSC, a hierarchical guided filter (HGF) is initially used to smoothen the pixels of the HSI data to preserve the spatial pixel consistency. This step is followed by the construction of linear patches from the nonlinear manifold by using the maximal linear patch (MLP) criterion. Then the inter-patch and intra-patch dissimilarity matrices are constructed in both spectral and spatial domains by regularized manifold local scaling cut (RMLSC) and neighboring pixel manifold local scaling cut (NPMLSC) respectively. Finally, we obtain the projection matrix by optimizing the updated semi-supervised spatial-spectral between-patch and total-patch dissimilarity. The effectiveness of the proposed DR algorithm is illustrated with publicly available real-world HSI datasets.

</details>

<details>

<summary>2018-11-20 16:00:22 - Automatic Test Improvement with DSpot: a Study with Ten Mature Open-Source Projects</summary>

- *Benjamin Danglot, Oscar Luis Vera-Pérez, Benoit Baudry, Martin Monperrus*

- `1811.08330v1` - [abs](http://arxiv.org/abs/1811.08330v1) - [pdf](http://arxiv.org/pdf/1811.08330v1)

> In the literature, there is a rather clear segregation between manually written tests by developers and automatically generated ones. In this paper, we explore a third solution: to automatically improve existing test cases written by developers. We present the concept, design, and implementation of a system called \dspot, that takes developer-written test cases as input (junit tests in Java) and synthesizes improved versions of them as output. Those test improvements are given back to developers as patches or pull requests, that can be directly integrated in the main branch of the test code base. We have evaluated DSpot in a deep, systematic manner over 40 real-world unit test classes from 10 notable and open-source software projects. We have amplified all test methods from those 40 unit test classes. In 26/40 cases, DSpot is able to automatically improve the test under study, by triggering new behaviors and adding new valuable assertions. Next, for ten projects under consideration, we have proposed a test improvement automatically synthesized by \dspot to the lead developers. In total, 13/19 proposed test improvements were accepted by the developers and merged into the main code base. This shows that DSpot is capable of automatically improving unit-tests in real-world, large-scale Java software.

</details>

<details>

<summary>2018-11-24 16:30:01 - How to Design a Program Repair Bot? Insights from the Repairnator Project</summary>

- *Simon Urli, Zhongxing Yu, Lionel Seinturier, Martin Monperrus*

- `1811.09852v1` - [abs](http://arxiv.org/abs/1811.09852v1) - [pdf](http://arxiv.org/pdf/1811.09852v1)

> Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a " program repair bot " and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 317 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 17 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.

</details>

<details>

<summary>2018-11-28 15:05:22 - Cluster-Based Learning from Weakly Labeled Bags in Digital Pathology</summary>

- *Shazia Akbar, Anne L. Martel*

- `1812.00884v1` - [abs](http://arxiv.org/abs/1812.00884v1) - [pdf](http://arxiv.org/pdf/1812.00884v1)

> To alleviate the burden of gathering detailed expert annotations when training deep neural networks, we propose a weakly supervised learning approach to recognize metastases in microscopic images of breast lymph nodes. We describe an alternative training loss which clusters weakly labeled bags in latent space to inform relevance of patch-instances during training of a convolutional neural network. We evaluate our method on the Camelyon dataset which contains high-resolution digital slides of breast lymph nodes, where labels are provided at the image-level and only subsets of patches are made available during training.

</details>


## 2018-12

<details>

<summary>2018-12-01 06:27:20 - A Tight Approximation for Co-flow Scheduling for Minimizing Total Weighted Completion Time</summary>

- *Sungjin Im, Manish Purohit*

- `1707.04331v2` - [abs](http://arxiv.org/abs/1707.04331v2) - [pdf](http://arxiv.org/pdf/1707.04331v2)

> Co-flows model a modern scheduling setting that is commonly found in a variety of applications in distributed and cloud computing. In co-flow scheduling, there are $m$ input ports and $m$ output ports. Each co-flow $j \in J$ can be represented by a bipartite graph between the input and output ports, where each edge $(i,o)$ with demand $d_{i,o}^j$ means that $d_{i,o}^j$ units of packets must be delivered from port $i$ to port $o$. To complete co-flow $j$, we must satisfy all of its demands. Due to capacity constraints, a port can only transmit (or receive) one unit of data in unit time. A feasible schedule at each time $t$ must therefore be a bipartite matching.   We consider co-flow scheduling and seek to optimize the popular objective of total weighted completion time. Our main result is a $(2+\epsilon)$-approximation for this problem, which is essentially tight, as the problem is hard to approximate within a factor of $(2 - \epsilon)$. This improves upon the previous best known 4-approximation. Further, our result holds even when jobs have release times without any loss in the approximation guarantee. The key idea of our approach is to construct a continuous-time schedule using a configuration linear program and interpret each job's completion time therein as the job's deadline. The continuous-time schedule serves as a witness schedule meeting the discovered deadlines, which allows us to reduce the problem to a deadline-constrained scheduling problem.   * This result is flawed; see the first page for the details.

</details>

<details>

<summary>2018-12-01 12:19:48 - When a Patch is Not Enough - HardFails: Software-Exploitable Hardware Bugs</summary>

- *Ghada Dessouky, David Gens, Patrick Haney, Garrett Persyn, Arun Kanuparthi, Hareesh Khattri, Jason M. Fung, Ahmad-Reza Sadeghi, Jeyavijayan Rajendran*

- `1812.00197v1` - [abs](http://arxiv.org/abs/1812.00197v1) - [pdf](http://arxiv.org/pdf/1812.00197v1)

> In this paper, we take a deep dive into microarchitectural security from a hardware designer's perspective by reviewing the existing approaches to detect hardware vulnerabilities during the design phase. We show that a protection gap currently exists in practice that leaves chip designs vulnerable to software-based attacks. In particular, existing verification approaches fail to detect specific classes of vulnerabilities, which we call HardFails: these bugs evade detection by current verification techniques while being exploitable from software. We demonstrate such vulnerabilities in real-world SoCs using RISC-V to showcase and analyze concrete instantiations of HardFails. Patching these hardware bugs may not always be possible and can potentially result in a product recall. We base our findings on two extensive case studies: the recent Hack@DAC 2018 hardware security competition, where 54 independent teams of researchers competed world-wide over a period of 12 weeks to catch inserted security bugs in SoC RTL designs, and an in-depth systematic evaluation of state-of-the-art verification approaches. Our findings indicate that even combinations of techniques will miss high-impact bugs due to the large number of modules with complex interdependencies and fundamental limitations of current detection approaches. We also craft a real-world software attack that exploits one of the RTL bugs from Hack@DAC that evaded detection and discuss novel approaches to mitigate the growing problem of cross-layer bugs at design time.

</details>

<details>

<summary>2018-12-02 15:40:30 - Dynamic Patch Generation for Null Pointer Exceptions using Metaprogramming</summary>

- *Thomas Durieux, Benoit Cornu, Lionel Seinturier, Martin Monperrus*

- `1812.00409v1` - [abs](http://arxiv.org/abs/1812.00409v1) - [pdf](http://arxiv.org/pdf/1812.00409v1)

> Null pointer exceptions (NPE) are the number one cause of uncaught crashing exceptions in production. In this paper, we aim at exploring the search space of possible patches for null pointer exceptions with metaprogramming. Our idea is to transform the program under repair with automated code transformation, so as to obtain a metaprogram. This metaprogram contains automatically injected hooks, that can be activated to emulate a null pointer exception patch. This enables us to perform a fine-grain analysis of the runtime context of null pointer exceptions. We set up an experiment with 16 real null pointer exceptions that have happened in the field. We compare the effectiveness of our metaprogramming approach against simple templates for repairing null pointer exceptions.

</details>

<details>

<summary>2018-12-04 17:07:27 - An Idea to Increase the Security of EAP-MD5 Protocol Against Dictionary Attack</summary>

- *Behrooz Khadem, Siavosh Abedi, Isa Sa-adatyar*

- `1812.01533v1` - [abs](http://arxiv.org/abs/1812.01533v1) - [pdf](http://arxiv.org/pdf/1812.01533v1)

> IEEE 802.1X is an international standard for Port-based Network Access Control which provides authentication for devices applicant of either local network or wireless local network. This standard defines the packing of EAP protocol on IEEE 802. In this standard, authentication protocols become a complementary part of network security. There is a variety in EAP family protocols, regarding their speed and security. One of the fastest of these protocols is EAP-MD5 which is the main subject of this paper. Moreover, in order to improve EAP-MD5 security, a series of attacks against it have been investigated. In this paper at first EAP-MD5 protocol is introduced briefly and a series of the dictionary attacks against it are described. Then, based on observed weaknesses, by proposing an appropriate idea while maintaining the speed of execution, its security against dictionary attack is improved.

</details>

<details>

<summary>2018-12-08 08:24:49 - Production-Driven Patch Generation</summary>

- *Thomas Durieux, Youssef Hamadi, Martin Monperrus*

- `1812.04475v1` - [abs](http://arxiv.org/abs/1812.04475v1) - [pdf](http://arxiv.org/pdf/1812.04475v1)

> We present an original concept for patch generation: we propose to do it directly in production. Our idea is to generate patches on-the-fly based on automated analysis of the failure context. By doing this in production, the repair process has complete access to the system state at the point of failure. We propose to perform live regression testing of the generated patches directly on the production traffic, by feeding a sandboxed version of the application with a copy of the production traffic, the 'shadow traffic'. Our concept widens the applicability of program repair, because it removes the requirements of having a failing test case.

</details>

<details>

<summary>2018-12-11 02:30:27 - Code-less Patching for Heap Vulnerabilities Using Targeted Calling Context Encoding</summary>

- *Qiang Zeng, Golam Kayas, Emil Mohammed, Lannan Luo, Xiaojiang Du, Junghwan Rhee*

- `1812.04191v1` - [abs](http://arxiv.org/abs/1812.04191v1) - [pdf](http://arxiv.org/pdf/1812.04191v1)

> Exploitation of heap vulnerabilities has been on the rise, leading to many devastating attacks. Conventional heap patch generation is a lengthy procedure, requiring intensive manual efforts. Worse, fresh patches tend to harm system dependability, hence deterring users from deploying them. We propose a heap patching system that simultaneously has the following prominent advantages: (1) generating patches without manual efforts; (2) installing patches without altering the code (so called code-less patching); (3) handling various heap vulnerability types; (4) imposing a very low overhead; and (5) no dependency on specific heap allocators. As a separate contribution, we propose targeted calling context encoding, which is a suite of algorithms for optimizing calling context encoding, an important technique with applications in many areas. The system properly combines heavyweight offline attack analysis with lightweight online defense generation, and provides a new countermeasure against heap attacks. The evaluation shows that the system is effective and efficient.

</details>

<details>

<summary>2018-12-12 11:39:38 - On the potential for open-endedness in neural networks</summary>

- *Nicholas Guttenberg, Nathaniel Virgo, Alexandra Penn*

- `1812.04907v1` - [abs](http://arxiv.org/abs/1812.04907v1) - [pdf](http://arxiv.org/pdf/1812.04907v1)

> Natural evolution gives the impression of leading to an open-ended process of increasing diversity and complexity. If our goal is to produce such open-endedness artificially, this suggests an approach driven by evolutionary metaphor. On the other hand, techniques from machine learning and artificial intelligence are often considered too narrow to provide the sort of exploratory dynamics associated with evolution. In this paper, we hope to bridge that gap by reviewing common barriers to open-endedness in the evolution-inspired approach and how they are dealt with in the evolutionary case - collapse of diversity, saturation of complexity, and failure to form new kinds of individuality. We then show how these problems map onto similar issues in the machine learning approach, and discuss how the same insights and solutions which alleviated those barriers in evolutionary approaches can be ported over. At the same time, the form these issues take in the machine learning formulation suggests new ways to analyze and resolve barriers to open-endedness. Ultimately, we hope to inspire researchers to be able to interchangeably use evolutionary and gradient-descent-based machine learning methods to approach the design and creation of open-ended systems.

</details>

<details>

<summary>2018-12-12 16:36:14 - Thwarting Adversarial Examples: An $L_0$-RobustSparse Fourier Transform</summary>

- *Mitali Bafna, Jack Murtagh, Nikhil Vyas*

- `1812.05013v1` - [abs](http://arxiv.org/abs/1812.05013v1) - [pdf](http://arxiv.org/pdf/1812.05013v1)

> We give a new algorithm for approximating the Discrete Fourier transform of an approximately sparse signal that has been corrupted by worst-case $L_0$ noise, namely a bounded number of coordinates of the signal have been corrupted arbitrarily. Our techniques generalize to a wide range of linear transformations that are used in data analysis such as the Discrete Cosine and Sine transforms, the Hadamard transform, and their high-dimensional analogs. We use our algorithm to successfully defend against well known $L_0$ adversaries in the setting of image classification. We give experimental results on the Jacobian-based Saliency Map Attack (JSMA) and the Carlini Wagner (CW) $L_0$ attack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch on the ImageNet dataset.

</details>

<details>

<summary>2018-12-17 16:13:42 - BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity</summary>

- *Alexey A. Gritsenko, Alex D'Amour, James Atwood, Yoni Halpern, D. Sculley*

- `1812.06869v1` - [abs](http://arxiv.org/abs/1812.06869v1) - [pdf](http://arxiv.org/pdf/1812.06869v1)

> We introduce the BriarPatch, a pixel-space intervention that obscures sensitive attributes from representations encoded in pre-trained classifiers. The patches encourage internal model representations not to encode sensitive information, which has the effect of pushing downstream predictors towards exhibiting demographic parity with respect to the sensitive information. The net result is that these BriarPatches provide an intervention mechanism available at user level, and complements prior research on fair representations that were previously only applicable by model developers and ML experts.

</details>

<details>

<summary>2018-12-17 17:44:10 - CT-Wasm: Type-Driven Secure Cryptography for the Web Ecosystem</summary>

- *Conrad Watt, John Renner, Natalie Popescu, Sunjay Cauligi, Deian Stefan*

- `1808.01348v5` - [abs](http://arxiv.org/abs/1808.01348v5) - [pdf](http://arxiv.org/pdf/1808.01348v5)

> A significant amount of both client and server-side cryptography is implemented in JavaScript. Despite widespread concerns about its security, no other language has been able to match the convenience that comes from its ubiquitous support on the "web ecosystem" - the wide variety of technologies that collectively underpins the modern World Wide Web. With the new introduction of the WebAssembly bytecode language (Wasm) into the web ecosystem, we have a unique opportunity to advance a principled alternative to existing JavaScript cryptography use cases which does not compromise this convenience.   We present Constant-Time WebAssembly (CT-Wasm), a type-driven strict extension to WebAssembly which facilitates the verifiably secure implementation of cryptographic algorithms. CT-Wasm's type system ensures that code written in CT-Wasm is both information flow secure and resistant to timing side channel attacks; like base Wasm, these guarantees are verifiable in linear time. Building on an existing Wasm mechanization, we mechanize the full CT-Wasm specification, prove soundness of the extended type system, implement a verified type checker, and give several proofs of the language's security properties.   We provide two implementations of CT-Wasm: an OCaml reference interpreter and a native implementation for Node.js and Chromium that extends Google's V8 engine. We also implement a CT-Wasm to Wasm rewrite tool that allows developers to reap the benefits of CT-Wasm's type system today, while developing cryptographic algorithms for base Wasm environments. We evaluate the language, our implementations, and supporting tools by porting several cryptographic primitives - Salsa20, SHA-256, and TEA - and the full TweetNaCl library. We find that CT-Wasm is fast, expressive, and generates code that we experimentally measure to be constant-time.

</details>

<details>

<summary>2018-12-17 19:28:30 - Multi Instance Learning For Unbalanced Data</summary>

- *Mark Kozdoba, Edward Moroshko, Lior Shani, Takuya Takagi, Takashi Katoh, Shie Mannor, Koby Crammer*

- `1812.07010v1` - [abs](http://arxiv.org/abs/1812.07010v1) - [pdf](http://arxiv.org/pdf/1812.07010v1)

> In the context of Multi Instance Learning, we analyze the Single Instance (SI) learning objective. We show that when the data is unbalanced and the family of classifiers is sufficiently rich, the SI method is a useful learning algorithm. In particular, we show that larger data imbalance, a quality that is typically perceived as negative, in fact implies a better resilience of the algorithm to the statistical dependencies of the objects in bags. In addition, our results shed new light on some known issues with the SI method in the setting of linear classifiers, and we show that these issues are significantly less likely to occur in the setting of neural networks. We demonstrate our results on a synthetic dataset, and on the COCO dataset for the problem of patch classification with weak image level labels derived from captions.

</details>

<details>

<summary>2018-12-18 15:02:26 - Impact of Tool Support in Patch Construction</summary>

- *Anil Koyuncu, Tegawendé F. Bissyandé, Dongsun Kim, Jacques Klein, Martin Monperrus, Yves Le Traon*

- `1812.07416v1` - [abs](http://arxiv.org/abs/1812.07416v1) - [pdf](http://arxiv.org/pdf/1812.07416v1)

> In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches.

</details>

<details>

<summary>2018-12-19 08:49:50 - Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method</summary>

- *Haruki Imai, Samuel Matzek, Tung D. Le, Yasushi Negishi, Kiyokuni Kawachiya*

- `1812.07816v1` - [abs](http://arxiv.org/abs/1812.07816v1) - [pdf](http://arxiv.org/pdf/1812.07816v1)

> Deep neural network models used for medical image segmentation are large because they are trained with high-resolution three-dimensional (3D) images. Graphics processing units (GPUs) are widely used to accelerate the trainings. However, the memory on a GPU is not large enough to train the models. A popular approach to tackling this problem is patch-based method, which divides a large image into small patches and trains the models with these small patches. However, this method would degrade the segmentation quality if a target object spans multiple patches. In this paper, we propose a novel approach for 3D medical image segmentation that utilizes the data-swapping, which swaps out intermediate data from GPU memory to CPU memory to enlarge the effective GPU memory size, for training high-resolution 3D medical images without patching. We carefully tuned parameters in the data-swapping method to obtain the best training performance for 3D U-Net, a widely used deep neural network model for medical image segmentation. We applied our tuning to train 3D U-Net with full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result, communication overhead, which is the most important issue, was reduced by 17.1%. Compared with the patch-based method for patches of 128 x 128 x 128 voxels, our training for full-size images achieved improvement on the mean Dice score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core sub-region, respectively. The total training time was reduced from 164 hours to 47 hours, resulting in 3.53 times of acceleration.

</details>

<details>

<summary>2018-12-21 15:44:59 - Deep Graph Infomax</summary>

- *Petar Veličković, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, R Devon Hjelm*

- `1809.10341v2` - [abs](http://arxiv.org/abs/1809.10341v2) - [pdf](http://arxiv.org/pdf/1809.10341v2)

> We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.

</details>

<details>

<summary>2018-12-24 13:30:40 - Divide et Impera: MemoryRanger Runs Drivers in Isolated Kernel Spaces</summary>

- *Igor Korkin*

- `1812.09920v1` - [abs](http://arxiv.org/abs/1812.09920v1) - [pdf](http://arxiv.org/pdf/1812.09920v1)

> One of the main issues in the OS security is to provide trusted code execution in an untrusted environment. During executing, kernel-mode drivers allocate and process memory data: OS internal structures, users private information, and sensitive data of third-party drivers. All this data and the drivers code can be tampered with by kernel-mode malware. Microsoft security experts integrated new features to fill this gap, but they are not enough: allocated data can be stolen and patched and the drivers code can be dumped without any security reaction. The proposed hypervisor-based system (MemoryRanger) tackles this issue by executing drivers in separate kernel enclaves with specific memory attributes. MemoryRanger protects code and data using Intel VT-x and EPT features with low performance degradation on Windows 10 x64.

</details>

<details>

<summary>2018-12-26 07:35:24 - Multi-resolution neural networks for tracking seismic horizons from few training images</summary>

- *Bas Peters, Justin Granek, Eldad Haber*

- `1812.11092v1` - [abs](http://arxiv.org/abs/1812.11092v1) - [pdf](http://arxiv.org/pdf/1812.11092v1)

> Detecting a specific horizon in seismic images is a valuable tool for geological interpretation. Because hand-picking the locations of the horizon is a time-consuming process, automated computational methods were developed starting three decades ago. Older techniques for such picking include interpolation of control points however, in recent years neural networks have been used for this task. Until now, most networks trained on small patches from larger images. This limits the networks ability to learn from large-scale geologic structures. Moreover, currently available networks and training strategies require label patches that have full and continuous annotations, which are also time-consuming to generate.   We propose a projected loss-function for training convolutional networks with a multi-resolution structure, including variants of the U-net. Our networks learn from a small number of large seismic images without creating patches. The projected loss-function enables training on labels with just a few annotated pixels and has no issue with the other unknown label pixels. Training uses all data without reserving some for validation. Only the labels are split into training/testing. Contrary to other work on horizon tracking, we train the network to perform non-linear regression, and not classification. As such, we propose labels as the convolution of a Gaussian kernel and the known horizon locations that indicate uncertainty in the labels. The network output is the probability of the horizon location. We demonstrate the proposed computational ingredients on two different datasets, for horizon extrapolation and interpolation. We show that the predictions of our methodology are accurate even in areas far from known horizon locations because our learning strategy exploits all data in large seismic images.

</details>

<details>

<summary>2018-12-26 21:05:27 - A Greedy Approach to $\ell_{0,\infty}$ Based Convolutional Sparse Coding</summary>

- *Elad Plaut, Raja Giryes*

- `1812.10538v1` - [abs](http://arxiv.org/abs/1812.10538v1) - [pdf](http://arxiv.org/pdf/1812.10538v1)

> Sparse coding techniques for image processing traditionally rely on a processing of small overlapping patches separately followed by averaging. This has the disadvantage that the reconstructed image no longer obeys the sparsity prior used in the processing. For this purpose convolutional sparse coding has been introduced, where a shift-invariant dictionary is used and the sparsity of the recovered image is maintained. Most such strategies target the $\ell_0$ "norm" or the $\ell_1$ norm of the whole image, which may create an imbalanced sparsity across various regions in the image. In order to face this challenge, the $\ell_{0,\infty}$ "norm" has been proposed as an alternative that "operates locally while thinking globally". The approaches taken for tackling the non-convexity of these optimization problems have been either using a convex relaxation or local pursuit algorithms. In this paper, we present an efficient greedy method for sparse coding and dictionary learning, which is specifically tailored to $\ell_{0,\infty}$, and is based on matching pursuit. We demonstrate the usage of our approach in salt-and-pepper noise removal and image inpainting. A code package which reproduces the experiments presented in this work is available at https://web.eng.tau.ac.il/~raja

</details>

<details>

<summary>2018-12-27 03:15:30 - Sanctorum: A lightweight security monitor for secure enclaves</summary>

- *Ilia Lebedev, Kyle Hogan, Jules Drean, David Kohlbrenner, Dayeol Lee, Krste Asanović, Dawn Song, Srinivas Devadas*

- `1812.10605v1` - [abs](http://arxiv.org/abs/1812.10605v1) - [pdf](http://arxiv.org/pdf/1812.10605v1)

> Enclaves have emerged as a particularly compelling primitive to implement trusted execution environments: strongly isolated sensitive user-mode processes in a largely untrusted software environment. While the threat models employed by various enclave systems differ, the high-level guarantees they offer are essentially the same: attestation of an enclave's initial state, as well as a guarantee of enclave integrity and privacy in the presence of an adversary.   This work describes Sanctorum, a small trusted code base (TCB), consisting of a generic enclave-capable system, which is sufficient to implement secure enclaves akin to the primitive offered by Intel's SGX. While enclaves may be implemented via unconditionally trusted hardware and microcode, as it is the case in SGX, we employ a smaller TCB principally consisting of authenticated, privileged software, which may be replaced or patched as needed. Sanctorum implements a formally verified specification for generic enclaves on an in-order multiprocessor system meeting baseline security requirements, e.g., the MIT Sanctum processor and the Keystone enclave framework. Sanctorum requires trustworthy hardware including a random number generator, a private cryptographic key pair derived via a secure bootstrapping protocol, and a robust isolation primitive to safeguard sensitive information. Sanctorum's threat model is informed by the threat model of the isolation primitive, and is suitable for adding enclaves to a variety of processor systems.

</details>

<details>

<summary>2018-12-30 15:38:25 - Monte-Carlo Sampling applied to Multiple Instance Learning for Histological Image Classification</summary>

- *Marc Combalia, Veronica Vilaplana*

- `1812.11560v1` - [abs](http://arxiv.org/abs/1812.11560v1) - [pdf](http://arxiv.org/pdf/1812.11560v1)

> We propose a patch sampling strategy based on a sequential Monte-Carlo method for high resolution image classification in the context of Multiple Instance Learning. When compared with grid sampling and uniform sampling techniques, it achieves higher generalization performance. We validate the strategy on two artificial datasets and two histological datasets for breast cancer and sun exposure classification.

</details>

<details>

<summary>2018-12-31 03:01:40 - Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities</summary>

- *Martin White, Michele Tufano, Matias Martinez, Martin Monperrus, Denys Poshyvanyk*

- `1707.04742v2` - [abs](http://arxiv.org/abs/1707.04742v2) - [pdf](http://arxiv.org/pdf/1707.04742v2)

> In the field of automated program repair, the redundancy assumption claims large programs contain the seeds of their own repair. However, most redundancy-based program repair techniques do not reason about the repair ingredients---the code that is reused to craft a patch. We aim to reason about the repair ingredients by using code similarities to prioritize and transform statements in a codebase for patch generation. Our approach, DeepRepair, relies on deep learning to reason about code similarities. Code fragments at well-defined levels of granularity in a codebase can be sorted according to their similarity to suspicious elements (i.e., code elements that contain suspicious statements) and statements can be transformed by mapping out-of-scope identifiers to similar identifiers in scope. We examined these new search strategies for patch generation with respect to effectiveness from the viewpoint of a software maintainer. Our comparative experiments were executed on six open-source Java projects including 374 buggy program revisions and consisted of 19,949 trials spanning 2,616 days of computation time. DeepRepair's search strategy using code similarities generally found compilable ingredients faster than the baseline, jGenProg, but this improvement neither yielded test-adequate patches in fewer attempts (on average) nor found significantly more patches than the baseline. Although the patch counts were not statistically different, there were notable differences between the nature of DeepRepair patches and baseline patches. The results demonstrate that our learning-based approach finds patches that cannot be found by existing redundancy-based repair techniques.

</details>

